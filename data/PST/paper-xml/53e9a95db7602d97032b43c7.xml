<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Beyond Clicks: Query Reformulation as a Predictor of Search Satisfaction</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Ahmed</forename><surname>Hassan</surname></persName>
							<email>hassanam@microsoft.com</email>
						</author>
						<author>
							<persName><forename type="first">Xiaolin</forename><surname>Shi</surname></persName>
							<email>xishi@microsoft.com</email>
						</author>
						<author>
							<persName><forename type="first">Nick</forename><surname>Craswell</surname></persName>
							<email>nickcr@microsoft.com</email>
						</author>
						<author>
							<persName><forename type="first">Bill</forename><surname>Ramsey</surname></persName>
						</author>
						<author>
							<affiliation key="aff0">
								<address>
									<settlement>San Francisco</settlement>
									<region>CA</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff1">
								<orgName type="department">Microsoft Research One Microsoft Way Redmond</orgName>
								<address>
									<postCode>98052</postCode>
									<region>WA</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff2">
								<orgName type="institution">Microsoft</orgName>
								<address>
									<addrLine>Bing One Microsoft Way Redmond</addrLine>
									<postCode>98052</postCode>
									<region>WA</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Beyond Clicks: Query Reformulation as a Predictor of Search Satisfaction</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">21481C52B66347B403DA964EB7EA9948</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.3" ident="GROBID" when="2023-07-28T13:19+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>H.3.3 [Information Storage and Retrieval]: Information Search and Retrieval -selection process</term>
					<term>search process Re-querying behavior</term>
					<term>success prediction</term>
					<term>search tasks</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>To understand whether a user is satisfied with the current search results, implicit behavior is a useful data source, with clicks being the best-known implicit signal. However, it is possible for a nonclicking user to be satisfied and a clicking user to be dissatisfied. Here we study additional implicit signals based on the relationship between the user's current query and the next query, such as their textual similarity and the inter-query time. Using a large unlabeled dataset, a labeled dataset of queries and a labeled dataset of user tasks, we analyze the relationship between these signals. We identify an easily-implemented rule that indicates dissatisfaction: that a similar query issued within a time interval that is short enough (such as five minutes) implies dissatisfaction. By incorporating additional query-based features in the model, we show that a query-based model (with no click information) can indicate satisfaction more accurately than click-based models. The best model uses both query and click features. In addition, by comparing query sequences in successful tasks and unsuccessful tasks, we observe that search success is an incremental process for successful tasks with multiple queries.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">INTRODUCTION</head><p>Search is an interactive process that starts with a user submitting a query to a search system. Depending on the results returned by the system and the user's information need, the user may click zero or more results and may submit zero or more follow-up queries. Search log data can provide implicit feedback from which the search system can identify which results are relevant for particular queries <ref type="bibr" target="#b1">[2]</ref>. It can also provide insights into retrieval performance. Given search logs, models of searcher satisfaction can be developed at the query level or at the session/task level <ref type="bibr" target="#b15">[16]</ref>.</p><p>A user's search activity has one or more queries and zero or more clicks on results. Such activity is motivated by one or more higherlevel goals, which we call tasks, although tasks are not our focus of study in this paper. Instead, we focus on query-level satisfaction. To understand the difference between query-level and task-level success, consider the task of booking a holiday. The user might enter a query "expedia" with navigational intent. In that case, reaching the Expedia site constitutes query-level success without necessarily indicating task-level success, since we do not know if the user's task was completed. The notion of satisfaction at the query level could have many scenarios and aspects relating to the quality and usefulness of search results. These include but are not limited to: successful navigation to a known item, finding the answer to a question, learning about a new topic, finding the required information without clicking (i.e. good abandonment) <ref type="bibr" target="#b29">[30]</ref>, or gathering evidence that the required item/information doesn't exist. Rather than studying these separately, or modeling degrees of success, we follow state of the art work on search success (e.g. <ref type="bibr" target="#b0">[1]</ref>, <ref type="bibr" target="#b17">[18]</ref>) by choosing a simple view that satisfaction is binary: if a user is satisfied with the current results then the query is a successful one; otherwise, it is not. Click-based metrics have been widely used as a way to predict whether a given user is satisfied with the search results or not. Clicking on a result page does not necessarily indicate that the query was successful if taken out of context. To better understand this, consider the example in Figure <ref type="figure" target="#fig_0">1</ref>. We see an example of a user submitting the query "greenfield, mn accident". Apparently, the user is looking for information about an accident that took place in Greenfield, MN. The user clicks a result, dwells for 36 seconds, then types a second query "woman dies in a fatal accident in greenfield, minnesota" and clicks another result. The first clicked result is from 2010, the second is from July 2012, and the documents describe different incidents. A likely interpretation of this is that the user was looking for the 2012 accident, and failed to find it on the first query, especially because the queries were submitted just one day after the 2012 accident. The 2012 document was not even in the top-20 results of the first query. The 2010 document does not mention the 2012 accident. Had we seen only the first query and click, we might have thought the user was satisfied. In this work, we introduce and evaluate models of query-level satisfaction that consider the next query submitted by the user and not only based on whether a user clicks on a result or not. Given a stream of queries submitted by a user, we only consider the immediate follow-up query. This means we make minimal use of other queries, but also we are using the query that best reflects the user's reaction to the current query. The next query may be a manual reformulation of the current query because the user is dissatisfied with the current query results (e.g. "greenfield, mn accident"  "woman dies in a fatal accident in greenfield, mn"). If the user is satisfied, the next query might be a related query on the same topic (e.g., "best gre practice tests"  "gre powerpre"), or a new query on a different topic. Hence, we build models to predict the current query success using query pair information, click information or both. More specifically, we try to answer the following research questions:</p><p>Research Question 1: What is the correlation between click signals and query pair features such as overlap and inter-query time? If both are indicators of satisfaction, there should be some correlation.</p><p>Research Question 2: Can we accurately predict user satisfaction using query pair data alone? Research Question 3: Can we improve query success prediction using both click and query pair signals?</p><p>Research Question 4: Using search tasks which may have more than two queries, how does our query-level prediction relate to tasklevel success? The remainder of this paper is structured as follows. Section 2 describes related work. A formal problem definition is given in Section 3. Section 4 motivates this research by conducting a large scale exploratory analysis of user behavior logs, considering the correlation between click-based and query-based satisfaction indicators. In Section 5, we present a method for identifying query reformulation behavior, which is used to predict query-level satisfaction In Section 6. In Section 7, we present the experiments performed to evaluate model effectiveness and discusses query reformulation patterns in search tasks. We conclude in Section 8.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">RELATED WORK</head><p>There are four areas of work related to the research presented in this paper: (i) query document relevance, (ii) search satisfaction, success, and frustration, (iii) search tasks boundary identification, and (iv) query refinement. We cover each of these areas in turn.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Query Document Relevance</head><p>State of the art measurement in information retrieval uses a test collection comprising a document corpus, query topics and relevance judgment. These are then used with relevance metrics such as MAP and discounted cumulative gain (DCG) <ref type="bibr" target="#b20">[21]</ref>. This process requires costly manual judgments of the relevance of documents in the search result list to individual queries. Previous work has also estimated query document relevance using models derived from user click behavior <ref type="bibr" target="#b1">[2]</ref>[8] <ref type="bibr" target="#b27">[28]</ref>. Other research work has used the click patterns to compare different ranking functions, i.e. to derive a metric <ref type="bibr" target="#b7">[8]</ref>[11] <ref type="bibr" target="#b23">[24]</ref>. Even though Click data is very useful for predicting query document relevance, it is also very noisy. Some of the reasons behind that are document snippets that do not accurately represent the content, and the bias resulting from the position of the document in the result set <ref type="bibr" target="#b23">[24]</ref>. Our work shows that using information about the next query submitted by the user can allow us to filter out noisy click signals that are not indicative of query success.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Search Satisfaction</head><p>Extensive literature exists on deriving indicators of task success or failure from online user behavior. Fox et al. <ref type="bibr" target="#b12">[13]</ref> used an instrumented browser to determine whether there was an association between explicit ratings of user satisfaction and implicit measures of user interest and identified the measures that were most strongly associated with user satisfaction. They found that there was a link between user activity and satisfaction ratings, and that clickthrough, dwell time, and session termination activity combined to make good predictors of satisfaction for Web pages. For example, they found out that a short dwell time is an indicator of dissatisfaction, while long dwell time is correlated more with satisfaction. Behavioral patterns were also used to predict user satisfaction for search sessions. Huffman and Hochster <ref type="bibr" target="#b20">[21]</ref> found a relatively strong correlation with session satisfaction using a linear model encompassing the relevance of the first three results returned for the first query in a search task, whether the information need was navigational, and the number of events in the session. Hassan et al. <ref type="bibr" target="#b19">[20]</ref> developed models of user behavior to accurately estimate search success on a session level, independent of the relevance of documents retrieved by the search engine. Ageev et al. <ref type="bibr" target="#b0">[1]</ref> propose a formalization of different types of success for informational search, and presented a scalable game-like infrastructure for crowdsourcing search behavior studies, specifically targeted towards capturing and evaluating successful search strategies on informational tasks with known intent. They show that their model can predict search success effectively on their data and on a separate set of log data comprising search engine sessions. Feild et al. <ref type="bibr" target="#b11">[12]</ref> developed methods to predict user frustration. They assigned users difficult information seeking tasks and monitored their degree of frustration via query logs and physical sensors. Our work is different from this work in that we focus on querylevel satisfaction. However, we also try to understand the difference between query-level and task-level satisfaction and we study the patterns of query sequences that form a task.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Search Task Boundary Identification</head><p>The problem of classifying the boundaries of the user search tasks within sessions in web search logs has been widely addressed before. Boldi et al. <ref type="bibr" target="#b6">[7]</ref> presented the concept of the query-flow graph. A query-flow graph represents chains of related queries in query logs. They use this model for finding logical session boundaries and query recommendation. Ozmutlu <ref type="bibr" target="#b33">[34]</ref> proposes a method for identifying new topics in search logs. He demonstrates that time interval, search pattern and position of a query in a user session, are effective on shifting to a new topic. Radlinski and Joachims <ref type="bibr" target="#b34">[35]</ref> study sequences of related queries (query chains). They use that to generate new types of preference judgments from search engine logs to learn better ranked retrieval functions.</p><p>Arlitt <ref type="bibr" target="#b2">[3]</ref> found session boundaries using a calculated timeout threshold. Murray et al. <ref type="bibr" target="#b30">[31]</ref> extended this work by using hierarchical clustering to find better timeout values to detect session boundaries. Jones and Klinkner <ref type="bibr" target="#b25">[26]</ref> also addressed the problem of classifying the boundaries of the goals and missions in search logs. They showed that using features like edit distance and common words achieves considerably better results compared to timeouts. Lucchese et al. <ref type="bibr" target="#b30">[31]</ref> uses a similar set of features as <ref type="bibr" target="#b25">[26]</ref>, but uses clustering to group queries in the same task together as opposed to identifying task boundary as in <ref type="bibr" target="#b25">[26]</ref>. In this work, we present better models for predicting the relation between pairs of queries and we use it toward a higher level goal, which is predicting query level success.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.4">Query Refinement</head><p>Existing research has studied how web search engines can propose reformulations, but has given less attention to how people perform query reformulations. Most of the research on manual query reformulation has focused on building taxonomies of query reformulation. These taxonomies are generally constructed by examining a small set of query logs. Anick <ref type="bibr" target="#b2">[3]</ref> classified a random sample of 100 reformulations by hand into eleven categories. Jensen et al. <ref type="bibr" target="#b22">[23]</ref> identified 6 different kinds of reformulation states (New, Assistance, Content Change, Generalization, Reformulation, and Specialization) and provides heuristics for identifying them. They use them to predict when a user is most receptive to automatic query suggestions. The same categories were used in several other studies Error! Reference source not found. <ref type="bibr" target="#b28">[29]</ref>.</p><p>Huang and Efthimis <ref type="bibr" target="#b18">[19]</ref> proposed another reformulation taxonomy. Their taxonomy was lexical in nature (e.g., word reorder, adding words, removing words, etc.). They also proposed the use of regular expressions to identify them. While studying refinding behavior, Teevan et al. <ref type="bibr" target="#b35">[36]</ref> constructed a taxonomy of query re-finding by manually examining query logs, and implemented algorithms identify repeat queries, equal click queries and overlapping click queries. None has built an automatic classifier distinguishing reformulation queries from other. Heuristics and regular expressions have been used in <ref type="bibr" target="#b18">[19]</ref> and <ref type="bibr" target="#b22">[23]</ref> to identify different types of reformulations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">PROBLEM DEFINITION</head><p>We start by defining some terms that will be used through-out the paper:</p><p>Definition: A Search Session is group of queries and clicks demarcated with a 30-minute inactivity timeout, such as that used in previous work <ref type="bibr" target="#b34">[35]</ref>.</p><p>Definition: A SAT (Satisfied) Query is a query where the information need of the searching user has been successfully addressed.</p><p>Definition: A DSAT (disatisfied) Query is a query where the information need of the searching user has not been successfully addressed.</p><p>Definition: Query Reformulation is the act of submitting a Next Query Q2 to modify a previous search query Q1 in hope of retrieving better results.</p><p>Assume we have a stream of queries submitted to a search engine.</p><p>In response to each query, the engine returns a search results page.</p><p>The user may decide to click on one or more elements on the page, reformulate the query, or end the search. So given a query Q1, clicks on Q1's results, and the next query Q2, our objective is to predict whether the user was satisfied with Q1 or not (i.e. Q1 was successful, we use the terms satisfied and successful interchangeably throughout the paper). To build toward this goal, we start with a large-scale motivating exploratory analysis of search logs (Section 4), build methods for predicting query reformulation (Section 5), and build methods for query success prediction (Section 6).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">CLICKS AND NEXT QUERY: A LARGE SCALE EXPLORATORY ANALYSIS</head><p>We begin with some motivating exploratory analysis of user behavior logs, considering the correlation between click-based and query-based satisfaction indicators. With one week of activity from a large number of users, we identify all query pairs such that a single user entered Q1 then Q2 with no intervening queries. 67% of the queries in the dataset had a next query.</p><p>For each pair we are interested in the user's query-level satisfaction with Q1. Since this dataset has no relevance judgments of any kind we use clicks as a satisfaction indicator. For a large set of pairs, we can calculate a clickthrough rate (CTR) that is the proportion of pairs where Q1 has at least one click. Since for some clicks the user backs out immediately, we also calculate CTR-30, which is the proportion of pairs where Q1 has at least one click with a dwell time of 30 seconds or greater (we see no further search activity for at least 30 seconds). Previous work has shown that dwell time exceeding 30 seconds is highly correlated with satisfaction <ref type="bibr" target="#b12">[13]</ref>.</p><p>Clicks are a noisy indicator of relevance, but for a very large set of pairs a higher CTR and higher CTR-30 is some indication of greater satisfaction with Q1.</p><p>Our query-based satisfaction indicators are based on query similarity and time between queries. Here we say that Q1 and Q2 overlap if, after lowercasing, tokenization, and removing stopwords, the queries have at least one token in common. Consider the query Q1 "la map", where the user's intention is to find maps of Louisiana (abbreviated as LA). If the results of Q1 consist of maps of Los Angeles, then Q2 is more likely to reformulate the query, for example "Louisiana map". Issuing another "map" query would be less likely if Q1 returned relevant results. In this case, reformulation is an indicator of dissatisfaction. In this section, we use word overlap as a proxy for reformulation (i.e. Q2 is considered a reformulation of Q1 if they have at least one non-stop-word term overlap). Note, we later build on this intuition by considering richer notions of Q1-Q2 similarity. Our other indicator of satisfaction is the time between Q1 and Q2.</p><p>Using our previous example, if Q1 has the wrong maps, Q2 may show up sooner as the user searches for the right ones. More precisely, we characterize the time between queries as either quick (less than or equal to 5 minutes) or non-quick (greater than 5 minutes). This threshold was tuned using the dataset described in Section 5. Note, we later build richer models that do not use any hard thresholds on time between queries. Now let us reconsider the maps search, if Q1 has the right maps, we have more chance of ceasing search activity for 5 minutes or more. In this case, a quick Q2 is an indication of dissatisfaction. We note that a low CTR-30 and a quick Q2 are both associated with quick user interactions, so should be correlated in our analysis, though they use quite different thresholds (30 seconds and 5 minutes).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">CTR Analysis of Query Pairs</head><p>We analyze the CTRs of various sets of pairs. We show CTR relative to the CTR of all pairs. Reading the first row of Table <ref type="table" target="#tab_0">1</ref>, the pairs with Q1-Q2 overlap had CTR that was 21% below average (relative), while non-overlapping pairs had 11% above average CTR. Quick pairs had 29% below average CTR, with non-quick pairs being 25% above average. The remaining cells show interactions. Lowest CTR is found in quick overlapping pairs (-39%). Interestingly, all three values in the non-quick row are similar, indicating that for pairs with 5 or more intervening minutes overlap is not such a useful indicator of dissatisfaction. Table <ref type="table" target="#tab_1">2</ref> presents the same analysis but for CTR-30. As before, although overlap and quick seem like good dissatisfaction indicators on their own (-12% and -7%), there are interactions between the two, and it is really pairs that are quick and overlapping that are the interesting case, with CTR-30 that is 30% below average.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Query Pair Examples</head><p>Via manual sorting and grouping of the query pairs, we can find some illustrative examples of agreement and disagreement between our satisfaction indicators. For example, in pairs where Q1 is "chicago tribune" we see a high CTR-30, and relatively few cases where Q2 is quick and overlapping. These all indicate query-level success and we agree, it seems like successful navigational behavior. By contrast, it is possible for a single user session to confound all our indicators. A user searching for "how tall is X" for many celebrities named X will be typing many overlapping queries in quick succession. If the search engine has the factoid answer on the results page, the user also does not need to click (good abandonment). To identify that the user is actually satisfied at each query, and indeed we think they saw the factoid answer, we will need a more nuanced definition of query similarity, as will be presented later in the paper. The surprisingly high CTR of overlapping non-quick pairs could also be related to our simple definition of similarity. We observed high CTR, high overlap rate, low quick rate for pairs where Q1 is "christmas crafts for kids". In this case, the user may have query-level SAT but naturally carries on and searches for related queries such as "easy snowman christmas crafts". Pairs where Q1 is "chiropractor" have a relatively low CTR-30 and relatively high chance of being followed by a quick and overlapping Q2. The most frequent overlapping Q2 cases are "chiropractor directory", "what is a chiropractor", "chiropractor  <ref type="table" target="#tab_0">1</ref> and<ref type="table" target="#tab_1">2</ref> are affected by cases such as good abandonment and the general noisiness of click data. This highlights the importance of now moving to datasets with explicit success judgments.</p><p>In the next sections, we build on the observations, from the large scale log analysis described in this section, to build richer models of query success prediction using both click data and query reformulation data. The analysis is this section highlighted the relation between query reformulation and click through rate, where click through rate is used as a proxy for success. The analysis also emphasized the importance of building more nuance query reformulation prediction models (Section 5), richer query success prediction models (Section 6), and datasets with explicit success judgments (Section 7).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">QUERY REFORMULATION PREDICTION</head><p>Query Reformulation is the act of submitting a query Q2 to modify a previous search query Q1 in hope of retrieving better results. Hence, query reformulation is considered an indication of dissatisfaction with the previous query. For Q2 to be considered a reformulation of Q1, both queries must be intended to satisfy the same information need. Note that a related query on the same topic addressing a different information need is not considered as query reformulation for our purpose (e.g., "best gre practice tests"  "gre powerpre"). In this section, we propose methods for automatically predicting whether the current query is a reformulation of the previous query.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Query Normalization</head><p>We perform standard normalization where we replace all letters with their corresponding lower case representation. We also replace all runs of whitespace characters with a single space and remove any leading or trailing spaces.</p><p>In addition to the standard normalization, we also break queries that do not respect word boundaries into words. Word breaking is a well-studied topic that has proved to be useful for many natural language processing applications. This becomes a frequent problems with queries when users do not observe the correct word boundaries (for example: "southjeseycraigslist" for "south jersey craiglist") or when users are searching for a part of a URL (for example "quincycollege" for "quincy college"). We used a freely available word breaker Web service available 1 that has been described at <ref type="bibr" target="#b36">[37]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Queries to Keywords</head><p>Lexical similarity between queries has been often used to identify related queries <ref type="bibr" target="#b23">[24]</ref>. The problem with lexical similarity is that it introduces many false negatives (e.g. synonyms), but this can be handled by other features as we will describe later. More seriously, it introduces many false positives. Take the following query pair as an example Q1: "weather in new york city" and Q2: "hotels in new york city". 80% of the words are shared between Q1 and Q2. Hence, any lexical similarity feature would predict the user submitted Q2 as a rewrite of Q1. What we would like to do is to have a query representation that recognizes that the first query has two keywords: "weather" and "new york city" and the second has also two keywords "hotels" and "new York city" and that only 50% of the keywords are shared between the queries.</p><p>To build such a representation, we segment each query into keywords. Query segmentation is the process of taking a user's search query and dividing the tokens into individual phrases or semantic units <ref type="bibr" target="#b5">[6]</ref>. Consider a query 𝑥 = {𝑥 1 , 𝑥 2 , … 𝑥 𝑛 } consisting of 𝑛 tokens. Query segmentation is the process of finding a mapping: 𝑥 → 𝑦 ∈ 𝑌 𝑛 , where 𝑦 is a segmentation from the set 𝑌 𝑛 . Many approaches to query segmentation have been presented in recent research. Some of them pose the problem as a supervised learning problem <ref type="bibr">[6] [43]</ref>. Many of the supervised methods though use expensive features that are difficult to re-implement.</p><p>On the other hand many unsupervised methods for query segmentation have also been proposed <ref type="bibr">[14][27]</ref>. Most of these methods use only raw web n-gram frequencies and are very easy to re-implement. Additionally, Hagen et al. <ref type="bibr" target="#b14">[15]</ref> have shown that these methods can achieve segmentation accuracy comparable to current state-of-the-art techniques using supervised learning. We opt for the unsupervised techniques to perform query segmentation. More specifically, we adopt the mutual information method (MI) used throughout the literature. A segmentation 𝑆 for a query 𝑞 is obtained by computing the pointwise mutual information score for each pair of consecutive words. More formally, for a query 𝑥 = {𝑥 1 , 𝑥 2 , … 𝑥 𝑛 }:</p><formula xml:id="formula_0">𝑃𝑀𝐼(𝑥 𝑖 , 𝑥 𝑖+1 ) = 𝑙𝑜𝑔 𝑝(𝑥 𝑖 , 𝑥 𝑖+1 ) 𝑝(𝑥 𝑖 ). 𝑝(𝑥 𝑖+1 )</formula><p>where 𝑝(𝑥 𝑖 , 𝑥 𝑖+1 ) is the joint probability of occurrence of the bigram (𝑥 𝑖 , 𝑥 𝑖+1 ) and 𝑝(𝑥 𝑖 ) and 𝑝(𝑥 𝑖+1 ) are the individual occurrence probabilities of the two tokens 𝑥 𝑖 and 𝑥 𝑖+1 . A segment break is introduced whenever the point wise mutual information between two consecutive words drops below a certain threshold τ. The threshold we used, τ = 0.895 , was selected to maximize the break accuracy <ref type="bibr" target="#b23">[24]</ref> on the Bergsma-Wang-Corpus <ref type="bibr" target="#b5">[6]</ref>. In our implementation, the probabilities for all words and n-grams have been computed using the freely available Microsoft Web N-Gram Service <ref type="bibr" target="#b19">[20]</ref>. Table <ref type="table" target="#tab_3">3</ref> shows different examples of queries, and the corresponding phrases.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3">Matching Keywords</head><p>Two keywords may have full term overlap, partial term overlap, or no direct overlap yet are semantically similar. To capture phrase similarity, we define four different ways of matching phrases ranked from the most to the least strict:</p><p>1-Exact Match: The two phrases match exactly. 2-Approximate Match: To capture spelling variants and misspelling, we allow two keywords to match if the Levenshtein edit distance between them is less than 2. 3-Semantic Match: We compute the keyword similarity by measuring the semantic similarity between the two phrases representing the keywords. Let 𝑄 = 𝑞 1 … 𝑞 𝐽 be one phrase and 𝑆 = 𝑠 1 … 𝑠 𝐼 be another, the semantic similarity between these two phrases can be measured based on WordNet <ref type="bibr" target="#b31">[32]</ref>.</p><p>WordNet is a large lexical database of English. Nouns, verbs, adjectives and adverbs are grouped into sets of cognitive synonyms (synsets), each expressing a distinct concept <ref type="bibr" target="#b31">[32]</ref>. Synsets are interlinked by means of conceptual-semantic and lexical relations.</p><p>To capture semantic variants, we match two terms if their similarity according to the WordNet Wu and Palmer measure (wup) is greater than 0.5. The Wu and Palmer measure <ref type="bibr" target="#b41">[42]</ref> calculates relatedness by considering the depths of the two synsets in the WordNet taxonomies, along with the depth of the Least Common Subsumer (LCS). The measure is computed as follows:</p><formula xml:id="formula_1">𝑤𝑢𝑝(𝑡 𝑖 , 𝑡 𝑗 ) = 2 * 𝑑𝑒𝑝𝑡ℎ(𝐿𝐶𝑆) 𝑑𝑒𝑝𝑡ℎ(𝑡 𝑖 ) + 𝑑𝑒𝑝𝑡ℎ(𝑡 𝑗 )</formula><p>where the depth of any synset in WordNet is the length of the path connecting it to the root node plus one.</p><p>To measure the similarity between the two phrases, we calculate the number of matched terms between 𝑄 and 𝑆 and divide it by the sum of the to the of matched terms between 𝑄 and 𝑆, the terms in 𝑄 that did not match any terms in 𝑆 and the terms in 𝑆 that did not match any terms in 𝑄. This is similar to computing the Jaccard distance between the terms in 𝑄 that did not match any terms in 𝑄. This is similar to calculated the Jaccard distance between 𝑄 and 𝑆 except that terms are considered identically if they can be matched using the Wu and Palmer measure described earlier.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.4">Features</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.4.1">Textual Features</head><p>Jones and Klinkner <ref type="bibr" target="#b23">[24]</ref> showed that word and character edit features are very useful for identifying same task queries. The intuition behind this is that sequence queries which have many words and/or characters in common tend to be related. We repurpose those features for detecting satisfaction. The features they used are:</p><p>normalized Levenshtein edit distance -1 if lev &gt; 2, 0 otherwise num. characters in common starting from the left num. characters in common starting from the right num. words in common starting from the left num. words in common starting from the right num. words in common -Jaccard distance between sets of words </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.4.2">Keyword Features</head><p>As we explained earlier the word and character edit features capture similarity between many pairs of queries. However, they also tend to misclassify many other pairs especially when the two queries share many words yet have different intents. We used the keyword representation of queries described in Section 5. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.4.3">Other Features</head><p>Other features, that have been also used in <ref type="bibr" target="#b23">[24]</ref>, include temporal features:</p><p>time between queries in seconds time between queries as a binary feature (5 mins, -30 mins, 60 mins, 120 mins) and search results feature:</p><p>cosine distance between vectors derived from the first 10 search results for the query terms.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">QUERY SUCCESS PREDICTION</head><p>Now that we can predict whether a query Q2 is a reformulation of the previous query Q1 using the methods from the previous section, we move on to addressing the main problem of this study. The problem we are trying to solve in this section is given a query Q1, can we predict whether the user was satisfied with Q1's results or not using information about the next query Q2 and the clicks on Q1's results if any. We discussed earlier how search can be viewed as an interactive process that involves the user and the search engine. When a user submits a query Q, sometimes, the user gets satisfying results and ends her search, moves on to another unrelated search, or moves on to another related search but with a different information need. On the other hand, when the user does not get a satisfying result, the user may abandon her search or try to reformulate the query in hope of getting better results. Previous work often assumes that a click is a strong evidence of satisfaction. This has been evident in the use of click through rate (CTR), long dwell time clicks and other similar variants as features to predict query success and query-URL relevance. However, there are many cases where users click on a result and later find out that it is not relevant. Previous work has shown that the probability of click is influenced by a document's position in the results page <ref type="bibr" target="#b9">[10]</ref> which results in more clicks for highly ranked results even if they are not relevant. It has also been shown that the "attractiveness" of the title and snippet of the result may lead to a user to clicking on this result <ref type="bibr" target="#b25">[26]</ref>, only to later find out that it is not relevant. An example of such behavior has been shown earlier in Figure <ref type="figure" target="#fig_0">1</ref>.</p><p>Hence, we build models to improve query success prediction using both click information and query reformulation. We build and test the following prediction systems: System 1. Clicks Only: A query Q is successful if it receives at least one click System 2. SAT Clicks Only: The dwell time is the time the user spends on the results page. It has been shown in previous work that longer dwell time is highly correlated with success <ref type="bibr">[11][19]</ref>. We consider a query Q successful if it received a click with dwell time greater than τ. Previous work has often used the threshold of 30 seconds to identify successful clicks <ref type="bibr" target="#b13">[14]</ref>. We experiment with this threshold and several other thresholds. System 3. Reformulation Only: We completely ignore clicks and predict the success of the current query based on the next query only. We use all the features from the previous section to predict whether a query is successful or not. This method assumes that users will always reformulate their queries when not successful. System 4. Reformulation + Clicks (2 stages): Even though, information about the next query is useful for predicting the user satisfaction with the current query, using it alone is problematic because some users simply give up and abandon their searches without reformulation when not satisfied. Using click information, in addition to information about the next query can help us identify these cases. In this approach, we classify the query as unsuccessful if the user reformulates their queries as predicted by the system in the previous section. If the next query was predicted as not being a reformulation of the current query, then the click information is used. We try both system 1 (Clicks Only) and system 2 (SAT Clicks Only) for this purpose. System 5. Reformulation + Clicks (classifier): Instead of adopting a staged approach as in the previous system, we learn a classifier using both the next query information and the click information simultaneously. We use all the features from the previous section, as well as whether the query received a click or not and the max dwell time if it did receive a click as features to predict whether the current query has been successful or not.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.">EXPERIMENTS AND RESULTS</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.1">Data</head><p>Our data consists of several thousands of query pairs randomly sampled from the queries submitted to a commercial search engine during a week in mid-2011. Every record in our data consisted of a consecutive query pair (𝑄 𝑖 , 𝑄 𝑖+1 ) submitted to the search engine by the same user and in the same session (i.e. within less than 30 minutes of idle time, the 30 minutes threshold has been frequently used in previous work, e.g. <ref type="bibr" target="#b38">[39]</ref>). Identical queries were excluded from the data. All data in the session to which the sampled query pair belongs were recorded. In addition to queries, the data contained a timestamp for each page view, all elements shown in response to that query (e.g. Web results, answers, etc.), and visited Web page or clicked answers. Intranet and secure URL visits were excluded. Any personally identifiable information was removed from the data prior to analysis. A group of annotators were instructed to exhaustively examine each session and "re-enact" the user's experience. The annotators inspected the entire search results page for each of 𝑄 𝑖 and 𝑄 𝑖+1 , including URLs, page titles, relevant snippets, and other features .. They were also shown clicks to aid them in their judgments. Additionally, they were also shown queries and clicks before and after the query pair of interest. They were asked to then use their assessment of the user's objectives to determine whether the user was satisfied with 𝑄 𝑖 's results. Different judges were also asked to determine whether 𝑄 𝑖+1 is a reformulation of 𝑄 𝑖 . Each query pair was labeled by at least three judges and the majority vote among judges was used. Because the number of positive instances is much smaller than the number of negative instances, we use all positive instances and an equal number of randomly selected negative instances leaving us with approximately 6000 query pairs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.2">Predicting Query Reformulation</head><p>In this section we describe the experiments we conducted to evaluate the reformulation prediction system. We perform experiments using the data described in the previous section. We compare the performance of four different systems: System 1. Heuristics: The first system is a heuristic that does not need any training data. It simply computes the similarity between two queries as the percentage of common words to the length of the longer query in terms of the number of words. When finding common words, it allows two words to be matched if their Levenshtein edit distance is less than or equals 2. The second query is predicted to be a reformulation of the first if 𝑠𝑖𝑚𝑖𝑙𝑎𝑟𝑖𝑡𝑦 ≥ τ 𝑠𝑖𝑚 and the 𝑡𝑖𝑚𝑒_𝑑𝑖𝑓𝑓𝑒𝑟𝑒𝑛𝑐𝑒 ≤ τ 𝑡𝑖𝑚𝑒 mins. The two thresholds were set to 0.35 and 5 minutes respectively using grid search to maximize accuracy over the training data. We present the results of this baseline to provide a simple method that is easy to reimplement and does not need any training data and can be easily used in future research.</p><p>System 2. Textual: The second system uses the textual features from previous work that have been described in Section 5.4.1 and the temporal and results features described in Section 5.4.3. System 3. Keywords: The third method uses the keyword features that we presented in Section 5.4.2 and the temporal and results features described in Section 5.4.3. System 4: All: Finally the last system uses both the textual features, the keyword features and the temporal and results features.</p><p>All the last three methods use the temporal and search results similarity features that have been described in Section 5.4.3. We do not report the contribution of temporal and web search results features here due to space limitation and because they have already been studied in previous work <ref type="bibr" target="#b25">[26]</ref>. The accuracy, positive (reformulation) F1, and negative (nonreformulation) F1 for the four methods are shown in Table <ref type="table" target="#tab_5">4</ref>. The results show that the keyword features outperform the textual features. Combining them together results in a small gain over using the keyword features only. The keyword features were able to achieve higher precision rates while not sacrificing recall because they were more effective in eliminating false reformulation cases.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.3">Predicting Query Success</head><p>Next, we move on to the experiments we conducted to predict query success using both click information and information about the next query. The results are shown in Table <ref type="table" target="#tab_7">5</ref>. The first row corresponds to the "Click Only" method which predicts a query as successful if it received one or more clicks. The following three rows correspond to systems that use click information only but also takes dwell time into consideration to identify satisfied clicks ("SAT Click Only").</p><p>The three systems require a certain amount of dwell time for a click to count (10, 30, and 50 seconds) respectively. Next we move on to the "Reformulation Only" method which completely ignores the clicks and predicts a query as successful if the next query is not a reformulation of the current query. The following 4 rows correspond to the "Reformulation + Clicks (2 stages)" methods that predict a query as successful if the next query is not a reformulation of the current query (as the previous one) and the query received at least one click (with different thresholds on dwell time as before).</p><p>Finally, the last system ("Reformulation + Clicks (classifier)") learns a classifier where the reformulation features, from Section 5.4, and click features to predict query success. We evaluate the methods using the following metrics accuracy, precision, recall and F1 measure of the "Satisfied Queries" class (SAT Precision, SAT Recall and SAT F1), and precision, recall and F1 measure of the "Dissatisfied Queries" class (DSAT Precision, DSAT Recall and DSAT F1). We notice from the results in Table <ref type="table" target="#tab_7">5</ref> that the "Clicks Only" method performs poorly with low accuracy and very low SAT precision and DSAT recall. This confirms our hypothesis that many queries that receive a click still end up being unsuccessful. As we introduce a threshold on the dwell time of the clicks for them to be considered, the performance improves (rows 2 -4). As we increase the dwell time threshold, we see an improved accuracy and precision but at the expense of recall. This is expected since the higher the threshold the more likely that a click becomes a true successful click. However as we increase this threshold, we also miss many successful clicks with shorter dwell time. Using dwell time thresholds of less than 10 seconds or more than 50 seconds did not result in any performance improvement.</p><p>Interestingly, when we only use the reformulation signal to predict success ("Reformulation Only"), we achieve better performance than using clicks only. Notice however that this is limited to the cases where a next query exists. As shown in Section 4, these correspond to two thirds of the queries as estimated using millions of queries submitted to a commercial search engine. Because we predict any query that has no reformulation as successful, we end up getting very high SAT recall and DSAT precision. The SAT precision and DSAT recall are much lower though.</p><p>In rows 6-9, we combine the reformulation signal and the click signal by a simple rule that assumes that successful queries should receive a click (with certain dwell time) and should not be reformulated. This improves the SAT precision and DSAT recall compared to using reformulation only because some DSAT queries are not followed by reformulation. This comes at the expense of the SAT recall and DSAT precision as expected. Like the clicks only cases, increasing the threshold on the dwell time improves most metrics except for the SAT recall. Finally, when we allow the learner to learn a classifier using both the reformulation and the click features, we get the best performance in terms of accuracy SAT F1 and DSAT F1.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.4">Query Sequences and Search Tasks</head><p>In this section, we investigate how the user reformulation behavior is related to the success of the entire search task. We obtained labeled search tasks from the authors of the study described in <ref type="bibr" target="#b16">[17]</ref>. Each task is labeled as either satisfied or not by the user performing the search. To gather this data, they deployed a plugin that detected when a user submits a query to any of the three major search engines (Google, Bing, and Yahoo). Users were instructed to submit a satisfaction rating at the end of their search task, where a search task is defined as a single information need that may result in one or more queries <ref type="bibr" target="#b25">[26]</ref>.The data gathered during that study provided in-situ judgments of satisfaction direct from searchers at the point of task termination. In this dataset, we have 7,628 tasks that were labeled by 218 users. Among these tasks, 98% are tasks with fewer or equal to 5 queries. Table <ref type="table" target="#tab_6">6</ref> shows the counts of tasks that are successful or unsuccessful with lengths (i.e. the numbers of queries) from 1 to 5.</p><p>From Table <ref type="table" target="#tab_6">6</ref>, we see that not only the number of tasks is skewly distributed with regard to the task length, the probability of being successful is not uniformly distributed for tasks with different lengths either: the probability for a task to be successful decreases as its length increases. This fact tells us that, in a search task, having more reformulations does not increase the probability of being successful. But rather, having more reformulations is a strong indictation of a possibly difficult task and thus the task has a higher probability of failure.</p><p>After examining the relationship between the number of re-queries and the probability for a task to be successful, we control the length of tasks and further examine the different patterns presented by query sequences in successful tasks and unsuccessful tasks of the same length. We compute the similarity between pairs of consecutive queries and compare these similarities for both succesful and unsucesful tasks. The result of this experiment is shown in Figure <ref type="figure" target="#fig_3">2</ref>. The figure shows the textual similarity and keyword similarity between consecutive queries in tasks of different lengths with 2 queries, 3 queries, 4 queries and 5 queries.</p><p>From Figure <ref type="figure" target="#fig_3">2</ref>, we see that the relationships between consecutive queries in successful tasks and unsuccessful tasks are showing different patterns. Especially, we notice that the last pair of queries in a successful task is much less similar to each other than an unsuccessful task, using both text and keyword similarity. This is also the case for tasks as short as having a length if 2 (i.e. having two queries only ). We also see that the textual and keyword similarity between the only two queries are significantly higher for unsuccessful tasks than successful tasks. This observation tells us that there is significant difference between re-query behavior in a successful task and in an unsuccessful task.</p><p>In order to get a better understanding on why successful tasks and unsuccessful tasks are showing such distinct patterns as shown in Figure <ref type="figure" target="#fig_3">2</ref>, we examine a few individual examples. We find that it is often the case that, in a successful task, the user adopts some completely new term(s) or novel information from the landing page(s) of URL(s) she clicks after issuing the previous queries in the same task. Figure <ref type="figure" target="#fig_2">3</ref>(a) shows an example of a sequence of queries in a successful task. The term "powerprep" in the last query is from the landing page the user clicks after issuing the second last query, and it doesn't appear in any of the previous queries in the same task. On the other hand, we observe that for most unsuccessful tasks, the users are not able to get much information from their previous queries, and thus the terms used for the entire tasks are very similar to each other. Figure <ref type="figure" target="#fig_2">3</ref>(b) gives an example of an unsuccessful task, in which although the user has a few clicks, she does not get much useful information from those clicks to reformulate the query and achieve success. We also observe that the average time gap between consecutive queries of successful tasks is significantly longer than unsuccessful tasks (i.e. 91.10 seconds vs. 73.07 seconds with p-value &lt; 0.05). This fact further supports our hypothesis that in a successful task, a user tends to spend more time to gather useful information in each intermediate step before reaching full success eventually. In summary, in this part we investigate the relationship between query reformulation patterns in search tasks. We have the following two major observations regarding users' different re-query behavior in successful tasks and unsuccessful tasks. The first observation is that unsuccessful tasks tend to have more re-queries than successful tasks. Which means, having more re-queries in a task doesn't infer a higher probability of success instead, it is a strong indictation of a possibly difficult task and the probability of failure is higher.</p><p>The second observation is more interesting, as we see the pairwise query similarity of successful tasks and unsuccessful tasks are showing very different patterns. In particular, queries of an unsuccessful task tend to be more similar to each other than queries of a successful task. This suggest that queries in a successful task are more likely to be intended to cover different aspects of the information need while queries in an unsuccessful tasks are more likely to be trying to express the same information in different ways.</p><p>By investigating individual examples, we find that in successful tasks users are more likely to reformulate their queries with the novel information gained from the results of previous queries. This suggests that users are learning useful information from the results of previous queries and using it to formulate the following queries. However, in unsuccessful tasks, users usually don't gain much novel or useful information from the previous queries and thus the re-queries tend to be similar to each other. This fact suggests that for a task with multiple queries, search success is an incremental process. This also agrees with a similar assumption used in previous work <ref type="bibr" target="#b10">[11]</ref>, where session utility model was presented that is based on the assumption that users reach partial satisfaction at the intermediate steps of a successful task.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8.">CONCLUSIONS</head><p>Previous models of query-level satisfaction have focused on clickthrough data as the main signal for predicting query success or query document relevance. Clickthrough information has also been shown to be noisy due to snippet bias and result presentation bias. This bias leads users to clicking on results because they think they are relevant (perceived relevance) only to find out later that some of these clicks are not relevant (Actual relevance). In this work, we addressed this shortcoming by introducing models of query-level success that draw conclusions on query success based on information about the next query submitted by the user in addition to click information. The following query submitted by a user in a search session reveals information about the user's intent as well as the user's satisfaction with the current search results. Through experimentation via labeled query pairs drawn from logs of a commercial search engine, we show that our proposed models can accurately identify query reformulations by users dissatisfied with the results of their current query. We also showed that our proposed models can predict query-level satisfaction more accurately than baselines that use clickthrough features only. Additionally, we studied the relation between sequences of queries in satisfied vs. dissatisfied search tasks. We observe that search success is an incremental process for most multi-query tasks. That is, in a successful task, a user is more likely to gain some novel information from the search results of intermediate queries before they reach final success. We used three datasets through this paper: a dataset of reformulation pairs, one-week worth of query logs to conduct the exploratory motivating study, and search tasks data for the task analysis. To reproduce these experiments, the reader may use any search engine log data, such as the AOL data, to conduct the exploratory analysis. For the query success experiments, query pairs sampled from the same log data should be judged by annotators to label satisfaction and reformulation. Finally, for tasklevel data, the reader may use the data in <ref type="bibr" target="#b0">[1]</ref>, or collect similar data using human annotators on in-situ judgments. Although the user has a few clicks after some of the queries, the terms used in all queries are very similar.  </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 .</head><label>1</label><figDesc>Figure 1. The next query is evidence of dissatisfaction even though the original query received a long dwell time click (&gt; 30 seconds).</figDesc><graphic coords="2,61.56,57.84,225.00,143.28" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head></head><label></label><figDesc>(a) Example: a successful task with 4 queries (queries are shown on the left side and clicks on the right side). The last query has a new term that comes from a landing page of a URL the user clicks after issuing the third query. (b) Example: an unsuccessful task with 4 queries (queries are shown on the left side and clicks on the right side).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: Examples of query sequences of a successful task and an unsuccessful task.</figDesc><graphic coords="9,324.84,208.56,227.04,58.44" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: Different patterns of text similarity and keyword similarity of successful tasks versus unsuccessful tasks.</figDesc><graphic coords="9,48.06,208.86,117.61,125.32" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 . Relative CTR for different subsets of pairs, using word overlap and a 5 minute time threshold</head><label>1</label><figDesc></figDesc><table><row><cell></cell><cell>overall</cell><cell>non-overlap</cell><cell>overlap</cell></row><row><cell>overall</cell><cell>0%</cell><cell>11%</cell><cell>-21%</cell></row><row><cell>non-quick</cell><cell>25%</cell><cell>24%</cell><cell>29%</cell></row><row><cell>quick</cell><cell>-29%</cell><cell>-17%</cell><cell>-39%</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 . Relative CTR-30 for different subsets of pairs, using word overlap and a 5 minute time threshold</head><label>2</label><figDesc></figDesc><table><row><cell></cell><cell>overall</cell><cell>non-overlap</cell><cell>overlap</cell></row><row><cell>overall</cell><cell>0%</cell><cell>6%</cell><cell>-12%</cell></row><row><cell>non-quick</cell><cell>6%</cell><cell>-1%</cell><cell>40%</cell></row><row><cell>quick</cell><cell>-7%</cell><cell>20%</cell><cell>-30%</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head></head><label></label><figDesc>Many other Q2 cases add a location, for example "chiropractor pittsburgh". If the relevance of Q1 improved, by adding informative results for the user to click, we might see higher CTR-30 and fewer quick overlapping follow up queries. If Q1 relevance was improved by adding an inline answer to the "what is" question, requiring no clicks, then CTR-30 would give an incorrect indication that users were dissatisfied. However, we note that our click-based analysis in Tables</figDesc><table /><note><p>1 http://web-ngram.research.microsoft.com/info/ salary" and "chiropractor school".</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 3 . Examples of queries, and the corresponding segmentation into keywords. Different tokens in a keyword are separated by "_"</head><label>3</label><figDesc></figDesc><table><row><cell>Query</cell><cell>Phrases and Keywords</cell></row><row><cell>hotels in new york city</cell><cell>hotels in new_york_city</cell></row><row><cell>hyundai roadside assistance phone</cell><cell>hyundai roadside_assistance</cell></row><row><cell>number</cell><cell>phone_number</cell></row><row><cell>kodak easyshare recharger chord</cell><cell>kodak_easyshare echarger_cord</cell></row><row><cell>user reviews for apple iphone</cell><cell>user_reviews for apple_iphone</cell></row><row><cell>user reviews for apple ipad</cell><cell>user_reviews for apple_ipad</cell></row><row><cell>tommy bhama perfume</cell><cell>tommy_bhama perfume</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 4 . Heuristics vs. Textual vs. Keyword Features for Reformulation Prediction</head><label>4</label><figDesc></figDesc><table><row><cell></cell><cell>Accuracy</cell><cell>Reform. F1</cell><cell>No-Reform. F1</cell></row><row><cell>Heuristic</cell><cell>77.10%</cell><cell>75.60%</cell><cell>76.08%</cell></row><row><cell>Textual</cell><cell>82.90%</cell><cell>71.75%</cell><cell>87.75%</cell></row><row><cell>Keywords</cell><cell>85.80%</cell><cell>77.30%</cell><cell>88.15%</cell></row><row><cell>All</cell><cell>87.15%</cell><cell>81.06%</cell><cell>89.86%</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 6 : Counts of successful and unsuccessful tasks with lengths from 1 to 5.</head><label>6</label><figDesc></figDesc><table><row><cell></cell><cell>Len. 1</cell><cell>Len. 2</cell><cell cols="3">Len. 3 Len. 4 Len. 5</cell></row><row><cell>SAT</cell><cell>4,581</cell><cell>1,163</cell><cell>421</cell><cell>172</cell><cell>97</cell></row><row><cell>DSAT</cell><cell>521</cell><cell>275</cell><cell>129</cell><cell>61</cell><cell>39</cell></row><row><cell>SAT</cell><cell>89.79</cell><cell>80.88%</cell><cell>76.51</cell><cell>73.82</cell><cell>71.32</cell></row><row><cell>Prob.</cell><cell>%</cell><cell></cell><cell>%</cell><cell>%</cell><cell>%</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 5 . Query Success Prediction Performance</head><label>5</label><figDesc></figDesc><table><row><cell></cell><cell></cell><cell>Accuracy</cell><cell>SAT</cell><cell>SAT</cell><cell>DSAT</cell><cell>DSAT</cell><cell>SAT</cell><cell>DSAT</cell></row><row><cell></cell><cell></cell><cell></cell><cell>Precision</cell><cell>Recall</cell><cell>Precision</cell><cell>Recall</cell><cell>F1</cell><cell>F1</cell></row><row><cell>1</cell><cell>Clicks Only</cell><cell>38.86%</cell><cell>35.88%</cell><cell>64.21%</cell><cell>46.77%</cell><cell>21.51%</cell><cell>46.04%</cell><cell>29.47%</cell></row><row><cell>2</cell><cell>Sat Clicks Only (τ =10s)</cell><cell>51.20%</cell><cell>42.19%</cell><cell>54.34%</cell><cell>61.10%</cell><cell>49.05%</cell><cell>47.50%</cell><cell>54.42%</cell></row><row><cell>3</cell><cell>Sat Click Only (τ =30s)</cell><cell>56.07%</cell><cell>46.22%</cell><cell>49.87%</cell><cell>63.75%</cell><cell>60.31%</cell><cell>47.97%</cell><cell>61.98%</cell></row><row><cell>4</cell><cell>Sat Click Only (τ =50s)</cell><cell>60.61%</cell><cell>51.80%</cell><cell>43.55%</cell><cell>65.18%</cell><cell>72.28%</cell><cell>47.32%</cell><cell>68.54%</cell></row><row><cell>5</cell><cell>Reformulation Only</cell><cell>79.17%</cell><cell>64.79%</cell><cell>97.16%</cell><cell>97.58%</cell><cell>68.41%</cell><cell>77.74%</cell><cell>80.43%</cell></row><row><cell>6</cell><cell>Reformulation + Clicks (2 stages)</cell><cell>73.17%</cell><cell>68.70%</cell><cell>62.37%</cell><cell>75.78%</cell><cell>80.56%</cell><cell>65.38%</cell><cell>78.10%</cell></row><row><cell>7</cell><cell>Reformulation + SAT Click (τ =10s) (2 stages)</cell><cell>73.22%</cell><cell>73.85%</cell><cell>52.76%</cell><cell>72.97%</cell><cell>87.22%</cell><cell>61.55%</cell><cell>79.46%</cell></row><row><cell>8</cell><cell>Reformulation + SAT Click (τ =30s) (2 stages)</cell><cell>73.01%</cell><cell>76.51%</cell><cell>48.42%</cell><cell>71.80%</cell><cell>89.83%</cell><cell>59.31%</cell><cell>79.81%</cell></row><row><cell>9</cell><cell>Reformulation + SAT Click (τ =50s) (2 stages)</cell><cell>71.99%</cell><cell>78.92%</cell><cell>42.37%</cell><cell>70.06%</cell><cell>92.26%</cell><cell>55.14%</cell><cell>79.64%</cell></row><row><cell>10</cell><cell>Reformulation + Clicks (Classifier)</cell><cell>84.23%</cell><cell>77.74%</cell><cell>81.19%</cell><cell>88.53%</cell><cell>86.04%</cell><cell>79.43%</cell><cell>87.27%</cell></row></table></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Find it if you can: a game for modeling different types of web search success using interaction data</title>
		<author>
			<persName><forename type="first">M</forename><surname>Ageev</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Lagun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Agichtein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. SIGIR</title>
		<meeting>SIGIR</meeting>
		<imprint>
			<date type="published" when="2011">2011</date>
			<biblScope unit="page" from="345" to="354" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Improving web search ranking by incorporating user behavior information</title>
		<author>
			<persName><forename type="first">E</forename><surname>Agichtein</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Brill</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">T</forename><surname>Dumais</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">T</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. SIGIR</title>
		<meeting>SIGIR</meeting>
		<imprint>
			<date type="published" when="2006">2006</date>
			<biblScope unit="page" from="19" to="26" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Using terminological feedback for web search refinement: a log-based study</title>
		<author>
			<persName><forename type="first">P</forename><surname>Anick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. SIGIR</title>
		<meeting>SIGIR</meeting>
		<imprint>
			<date type="published" when="2003">2003</date>
			<biblScope unit="page" from="88" to="95" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Characterizing Web user sessions</title>
		<author>
			<persName><forename type="first">M</forename><surname>Arlitt</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM SIGMETRICS Performance Eval Review</title>
		<imprint>
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="50" to="63" />
			<date type="published" when="2000">2000</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Information retrieval as statistical translation</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">L</forename><surname>Berger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Lafferty</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. SIGIR</title>
		<meeting>SIGIR</meeting>
		<imprint>
			<date type="published" when="1999">1999</date>
			<biblScope unit="page" from="222" to="229" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Learning Noun Phrase Query Segmentation</title>
		<author>
			<persName><forename type="first">S</forename><surname>Bergsma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wang</forename><forename type="middle">Q I</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. EMNLP</title>
		<meeting>EMNLP</meeting>
		<imprint>
			<date type="published" when="2007">2007</date>
			<biblScope unit="page" from="819" to="826" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">The query-flow graph: model and applications</title>
		<author>
			<persName><forename type="first">P</forename><surname>Boldi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Bonchi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Castillo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Donato</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Gionis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Vigna</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. CIKM</title>
		<meeting>CIKM</meeting>
		<imprint>
			<date type="published" when="2008">2008</date>
			<biblScope unit="page" from="609" to="618" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Evaluating search engines by modeling the relationship between relevance and clicks</title>
		<author>
			<persName><forename type="first">B</forename><surname>Carterette</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Jones</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. NIPS</title>
		<meeting>NIPS</meeting>
		<imprint>
			<date type="published" when="2007">2007</date>
			<biblScope unit="page" from="217" to="224" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">An experimental comparison of click position-bias models</title>
		<author>
			<persName><forename type="first">N</forename><surname>Craswell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Zoeter</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Taylor</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Ramsey</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. WSDM</title>
		<meeting>WSDM</meeting>
		<imprint>
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">A model to estimate intrinsic document relevance from the clickthrough logs of a web search engine</title>
		<author>
			<persName><forename type="first">G</forename><surname>Dupret</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Liao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. WSDM</title>
		<meeting>WSDM</meeting>
		<imprint>
			<date type="published" when="2010">2010</date>
			<biblScope unit="page" from="181" to="190" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Web search engine evaluation using clickthrough data and a user model</title>
		<author>
			<persName><forename type="first">G</forename><surname>Dupret</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Murdock</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Piwowarski</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">WWW workshop on Query Log Analysis: Social and Technological Challenges</title>
		<imprint>
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Predicting searcher frustration</title>
		<author>
			<persName><forename type="first">H</forename><surname>Feild</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Allan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Jones</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. SIGIR</title>
		<meeting>SIGIR</meeting>
		<imprint>
			<date type="published" when="2010">2010</date>
			<biblScope unit="page" from="34" to="41" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Evaluating implicit measures to improve the search experience</title>
		<author>
			<persName><forename type="first">S</forename><surname>Fox</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Karnawat</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Mydland</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">T</forename><surname>Dumais</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>White</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM TOIS</title>
		<imprint>
			<biblScope unit="volume">23</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="147" to="168" />
			<date type="published" when="2005">2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Query segmentation revisited</title>
		<author>
			<persName><forename type="first">M</forename><surname>Hagen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Potthast</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Stein</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Bräutigam</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. WWW</title>
		<meeting>WWW</meeting>
		<imprint>
			<date type="published" when="2011">2011</date>
			<biblScope unit="page" from="97" to="106" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">The Power of Naïve Query Segmentation</title>
		<author>
			<persName><forename type="first">M</forename><surname>Hagen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Potthast</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Stein</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Bräutigam</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. SIGIR</title>
		<meeting>SIGIR</meeting>
		<imprint>
			<date type="published" when="2010">2010</date>
			<biblScope unit="page" from="797" to="798" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Beyond DCG: user behavior as a predictor of a successful search</title>
		<author>
			<persName><forename type="first">A</forename><surname>Hassan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">L</forename><surname>Klinkner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. WSDM</title>
		<meeting>WSDM</meeting>
		<imprint>
			<date type="published" when="2010">2010</date>
			<biblScope unit="page" from="221" to="230" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">A task level user satisfaction model and its application on improving relevance estimation</title>
		<author>
			<persName><forename type="first">A</forename><surname>Hassan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>He</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. CIKM</title>
		<meeting>CIKM</meeting>
		<imprint>
			<date type="published" when="2011">2011</date>
			<biblScope unit="page" from="125" to="134" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">A semi-supervised approach to modeling web search satisfaction</title>
		<author>
			<persName><forename type="first">A</forename><surname>Hassan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. SIGIR</title>
		<meeting>SIGIR</meeting>
		<imprint>
			<date type="published" when="2012">2012</date>
			<biblScope unit="page" from="275" to="284" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Analyzing and evaluating query reformulation strategies in web search logs</title>
		<author>
			<persName><forename type="first">J</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Efthimis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of CIKM</title>
		<meeting>of CIKM</meeting>
		<imprint>
			<date type="published" when="2009">2009</date>
			<biblScope unit="page" from="77" to="86" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Exploring Web Scale Language Models for Search Query Processing</title>
		<author>
			<persName><forename type="first">J</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Miao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Behr</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. WWW</title>
		<meeting>WWW</meeting>
		<imprint>
			<date type="published" when="2010">2010</date>
			<biblScope unit="page" from="451" to="460" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">How well does result relevance predict session satisfaction?</title>
		<author>
			<persName><forename type="first">S</forename><surname>Huffman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Hochster</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. SIGIR</title>
		<meeting>SIGIR</meeting>
		<imprint>
			<date type="published" when="2007">2007</date>
			<biblScope unit="page" from="567" to="574" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Cumulated gain-based evaluation of IR techniques</title>
		<author>
			<persName><forename type="first">K</forename><surname>Jarvelin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Kekalainen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM TOIS</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="422" to="446" />
			<date type="published" when="2002">2002</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">Patterns and transitions of query reformulation during web searching</title>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">J</forename><surname>Jansen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Spink</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">Evaluating search engines using clickthrough data</title>
		<author>
			<persName><forename type="first">T</forename><surname>Joachims</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2002">2002</date>
		</imprint>
		<respStmt>
			<orgName>Department of Computer Science, Cornell University</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Accurately interpreting clickthrough data as implicit feedback</title>
		<author>
			<persName><forename type="first">T</forename><surname>Joachims</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Granka</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Hembrooke</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Gay</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. SIGIR</title>
		<meeting>SIGIR</meeting>
		<imprint>
			<date type="published" when="2005">2005</date>
			<biblScope unit="page" from="154" to="161" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Beyond the session timeout: automatic hierarchical segmentation of search topics in query logs</title>
		<author>
			<persName><forename type="first">R</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">L</forename><surname>Klinkner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. CIKM</title>
		<meeting>CIKM</meeting>
		<imprint>
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Generating Query Substitutions</title>
		<author>
			<persName><forename type="first">R</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Rey</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Madani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Greiner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. WWW</title>
		<meeting>WWW</meeting>
		<imprint>
			<date type="published" when="2006">2006</date>
			<biblScope unit="page" from="387" to="396" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Click data as implicit relevance feedback in web search</title>
		<author>
			<persName><forename type="first">S</forename><surname>Jung</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">L</forename><surname>Herlocker</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Webster</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Information Processing and Management (IPM)</title>
		<imprint>
			<biblScope unit="volume">43</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="791" to="807" />
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Patterns of search: analyzing and modeling Web query refinement</title>
		<author>
			<persName><forename type="first">T</forename><surname>Lau</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Horvitz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">User Modeling &apos;99</title>
		<imprint>
			<date type="published" when="1999">1999</date>
			<biblScope unit="page" from="119" to="128" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Good abandonment in mobile and PC internet search</title>
		<author>
			<persName><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">B</forename><surname>Huffman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Tokuda</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. SIGIR</title>
		<meeting>SIGIR</meeting>
		<imprint>
			<date type="published" when="2009">2009</date>
			<biblScope unit="page" from="43" to="50" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Identifying Task-based Sessions in Search Engine Query Logs</title>
		<author>
			<persName><forename type="first">C</forename><surname>Lucchese</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Orlando</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Perego</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Silvestri</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Tolomei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. WSDM</title>
		<meeting>WSDM</meeting>
		<imprint>
			<date type="published" when="2011">2011. 2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Wordnet: a lexical database for English</title>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">A</forename><surname>Miller</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Commun. ACM</title>
		<imprint>
			<biblScope unit="volume">38</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="39" to="41" />
			<date type="published" when="1995">1995</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Identification of User Sessions with Hierarchical Agglomerative Clustering</title>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">C</forename><surname>Murray</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Chowdhury</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ASIS&amp;T &apos;06</title>
		<imprint>
			<date type="published" when="2006">2006</date>
			<biblScope unit="volume">43</biblScope>
			<biblScope unit="page" from="1" to="5" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Automatic new topic identification using multiple linear regression</title>
		<author>
			<persName><forename type="first">S</forename><surname>Ozmutlu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Information Processing and Management</title>
		<imprint>
			<biblScope unit="volume">42</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="934" to="950" />
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Query chains: learning to rank from implicit feedback</title>
		<author>
			<persName><forename type="first">F</forename><surname>Radlinski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Joachims</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">KDD</title>
		<editor>
			<persName><forename type="first">R</forename><surname>Grossman</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">R</forename><surname>Bayardo</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">K</forename><forename type="middle">P</forename><surname>Bennett</surname></persName>
		</editor>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2005">2005</date>
			<biblScope unit="page" from="239" to="248" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Information Re-Retrieval: Repeat Queries in Yahoo&apos;s Logs</title>
		<author>
			<persName><forename type="first">J</forename><surname>Teevan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Adar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Potts</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. SIGIR</title>
		<meeting>SIGIR</meeting>
		<imprint>
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Feature-Rich Part-of-Speech Tagging with a Cyclic Dependency Network</title>
		<author>
			<persName><forename type="first">K</forename><surname>Toutanova</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Klein</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Manning</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Singer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. HLT-NAACL</title>
		<meeting>HLT-NAACL</meeting>
		<imprint>
			<date type="published" when="2003">2003</date>
			<biblScope unit="page" from="252" to="259" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Web Scale NLP: A Case Study on URL Word Breaking</title>
		<author>
			<persName><forename type="first">K</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Thrasher</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Hsu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc.WWW</title>
		<meeting>.WWW</meeting>
		<imprint>
			<date type="published" when="2011">2011</date>
			<biblScope unit="page" from="357" to="366" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">The demographics of Web search</title>
		<author>
			<persName><forename type="first">I</forename><surname>Weber</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Castillo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. SIGIR</title>
		<meeting>SIGIR</meeting>
		<imprint>
			<date type="published" when="2010">2010</date>
			<biblScope unit="page" from="523" to="530" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Investigating behavioral variability in Web search</title>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">W</forename><surname>White</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">M</forename><surname>Drucker</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. WWW</title>
		<meeting>WWW</meeting>
		<imprint>
			<date type="published" when="2007">2007</date>
			<biblScope unit="page" from="21" to="30" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Characterizing and predicting search engine switching behaviour</title>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">W</forename><surname>White</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Dumais</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. CIKM</title>
		<meeting>CIKM</meeting>
		<imprint>
			<date type="published" when="2009">2009</date>
			<biblScope unit="page" from="87" to="96" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Verb semantics and lexical selection</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Palmer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ACL</title>
		<meeting>ACL</meeting>
		<imprint>
			<date type="published" when="1994">1994</date>
			<biblScope unit="page" from="133" to="138" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Query Segmentation Using Conditional Random Fields</title>
		<author>
			<persName><forename type="first">X</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Shi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Workshop on Keyword Search on Structured Data (KEYS)</title>
		<meeting>the Workshop on Keyword Search on Structured Data (KEYS)</meeting>
		<imprint>
			<date type="published" when="2009">2009</date>
			<biblScope unit="page" from="21" to="26" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Domain knowledge, search behavior, and search effectiveness of engineering and science students</title>
		<author>
			<persName><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">G B</forename><surname>Anghelescu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Yuan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Inf. Res</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page">217</biblScope>
			<date type="published" when="2005">2005</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
