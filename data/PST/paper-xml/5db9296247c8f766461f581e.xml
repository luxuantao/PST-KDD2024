<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">PubMedQA: A Dataset for Biomedical Research Question Answering</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Qiao</forename><surname>Jin</surname></persName>
							<email>qiao.jin@pitt.edu</email>
						</author>
						<author>
							<persName><forename type="first">Bhuwan</forename><surname>Dhingra</surname></persName>
							<email>bdhingra@cs.cmu.edu</email>
						</author>
						<author>
							<persName><forename type="first">Zhengping</forename><surname>Liu</surname></persName>
							<email>zliu@pitt.edu</email>
						</author>
						<author>
							<persName><forename type="first">William</forename><forename type="middle">W</forename><surname>Cohen</surname></persName>
							<email>wcohen@google.com</email>
						</author>
						<author>
							<persName><forename type="first">Google</forename><surname>Ai</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Xinghua</forename><surname>Lu</surname></persName>
							<email>xinghua@pitt.edu</email>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="institution">University of Pittsburgh</orgName>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff1">
								<orgName type="institution">Carnegie Mellon University</orgName>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff2">
								<orgName type="institution">University of Pittsburgh</orgName>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff3">
								<orgName type="institution">University of Pittsburgh</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">PubMedQA: A Dataset for Biomedical Research Question Answering</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-01-03T08:59+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>We introduce PubMedQA, a novel biomedical question answering (QA) dataset collected from PubMed abstracts. The task of Pub-MedQA is to answer research questions with yes/no/maybe (e.g.: Do preoperative statins reduce atrial fibrillation after coronary artery bypass grafting?) using the corresponding abstracts. PubMedQA has 1k expert-annotated, 61.2k unlabeled and 211.3k artificially generated QA instances. Each PubMedQA instance is composed of (1) a question which is either an existing research article title or derived from one, (2) a context which is the corresponding abstract without its conclusion, (3) a long answer, which is the conclusion of the abstract and, presumably, answers the research question, and (4) a yes/no/maybe answer which summarizes the conclusion. Pub-MedQA is the first QA dataset where reasoning over biomedical research texts, especially their quantitative contents, is required to answer the questions. Our best performing model, multi-phase fine-tuning of BioBERT with long answer bag-of-word statistics as additional supervision, achieves 68.1% accuracy, compared to single human performance of 78.0% accuracy and majority-baseline of 55.2% accuracy, leaving much room for improvement. PubMedQA is publicly available at https://pubmedqa.github.io.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>A long-term goal of natural language understanding is to build intelligent systems that can reason and infer over natural language. The question answering (QA) task, in which models learn how to answer questions, is often used as a benchmark for quantitatively measuring the reasoning and inferring abilities of such intelligent systems.</p><p>While many large-scale annotated general domain QA datasets have been introduced <ref type="bibr" target="#b19">(Rajpurkar et al., 2016;</ref><ref type="bibr" target="#b9">Lai et al., 2017;</ref><ref type="bibr" target="#b7">Ko?isk?</ref> Question: Do preoperative statins reduce atrial fibrillation after coronary artery bypass grafting? Context: (Objective) Recent studies have demonstrated that statins have pleiotropic effects, including anti-inflammatory effects and atrial fibrillation (AF) preventive effects [...] (Methods) 221 patients underwent CABG in our hospital from 2004 to 2007. 14 patients with preoperative AF and 4 patients with concomitant valve surgery [...] (Results) The overall incidence of postoperative AF was 26%. Postoperative AF was significantly lower in the Statin group compared with the Non-statin group (16% versus 33%, p=0.005). Multivariate analysis demonstrated that independent predictors of AF <ref type="bibr">[...]</ref> Long Answer: (Conclusion) Our study indicated that preoperative statin therapy seems to reduce AF development after CABG. Answer: yes Figure <ref type="figure">1</ref>: An instance <ref type="bibr" target="#b21">(Sakamoto et al., 2011)</ref> of Pub-MedQA dataset: Question is the original question title; Context includes the structured abstract except its conclusive part, which serves as the Long Answer; Human experts annotated the Answer yes. Supporting fact for the answer is highlighted. <ref type="bibr">et al., 2018;</ref><ref type="bibr" target="#b23">Yang et al., 2018;</ref><ref type="bibr" target="#b8">Kwiatkowski et al., 2019)</ref>, the largest annotated biomedical QA dataset, BioASQ <ref type="bibr" target="#b22">(Tsatsaronis et al., 2015)</ref> has less than 3k training instances, most of which are simple factual questions. Some works proposed automatically constructed biomedical QA datasets <ref type="bibr" target="#b14">(Pampari et al., 2018;</ref><ref type="bibr" target="#b15">Pappas et al., 2018;</ref><ref type="bibr" target="#b6">Kim et al., 2018)</ref>, which have much larger sizes. However, questions of these datasets are mostly factoid, whose answers can be extracted in the contexts without much reasoning.</p><p>In this paper, we aim at building a biomedical QA dataset which (1) has substantial instances with some expert annotations and (2) requires reasoning over the contexts to answer the questions. For this, we turn to the PubMed 1 , a search engine providing access to over 25 million references of 2568 biomedical articles. We found that around 760k articles in PubMed use questions as their titles. Among them, the abstracts of about 120k articles are written in a structured style -meaning they have subsections of "Introduction", "Results" etc. Conclusive parts of the abstracts, often in "Conclusions", are the authors' answers to the question title. Other abstract parts can be viewed as the contexts for giving such answers. This pattern perfectly fits the scheme of QA, but modeling it as abstractive QA, where models learn to generate the conclusions, will result in an extremely hard task due to the variability of writing styles.</p><p>Interestingly, more than half of the question titles of PubMed articles can be briefly answered by yes/no/maybe, which is significantly higher than the proportions of such questions in other datasets, e.g.: just 1% in Natural Questions <ref type="bibr" target="#b8">(Kwiatkowski et al., 2019)</ref> and 6% in HotpotQA <ref type="bibr" target="#b23">(Yang et al., 2018)</ref>. Instead of using conclusions to answer the questions, we explore answering them with yes/no/maybe and treat the conclusions as a long answer for additional supervision.</p><p>To this end, we present PubMedQA, a biomedical QA dataset for answering research questions using yes/no/maybe. We collected all PubMed articles with question titles, and manually labeled 1k of them for cross-validation and testing. An example is shown in Fig. <ref type="figure">1</ref>. The rest of yes/no/answerable QA instances compose of the unlabeled subset which can be used for semisupervised learning. Further, we automatically convert statement titles of 211.3k PubMed articles to questions and label them with yes/no answers using a simple heuristic. These artificially generated instances can be used for pre-training. Unlike other QA datasets in which questions are asked by crowd-workers for existing contexts <ref type="bibr" target="#b19">(Rajpurkar et al., 2016;</ref><ref type="bibr" target="#b23">Yang et al., 2018;</ref><ref type="bibr" target="#b7">Ko?isk? et al., 2018)</ref>, in PubMedQA contexts are generated to answer the questions and both are written by the same authors. This consistency assures that contexts are perfectly related to the questions, thus making PubMedQA an ideal benchmark for testing scientific reasoning abilities.</p><p>As an attempt to solve PubMedQA and provide a strong baseline, we fine-tune BioBERT <ref type="bibr" target="#b10">(Lee et al., 2019)</ref> on different subsets in a multi-phase style with additional supervision of long answers. Though this model generates decent results and vastly outperforms other baselines, it's still much worse than the single-human performance, leaving significant room for future improvements.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Works</head><p>Biomedical QA: Expert-annotated biomedical QA datasets are limited by scale due to the difficulty of annotations. In 2006 and 2007, TREC<ref type="foot" target="#foot_0">2</ref> held QA challenges on genomics corpus <ref type="bibr" target="#b4">(Hersh et al., 2006</ref><ref type="bibr" target="#b3">(Hersh et al., , 2007))</ref>, where the task is to retrieve relevant documents for 36 and 38 topic questions, respectively. QA4MRE <ref type="bibr" target="#b16">(Pe?as et al., 2013)</ref> included a QA task about Alzheimer's disease <ref type="bibr" target="#b13">(Morante et al., 2012)</ref>. This dataset has 40 QA instances and the task is to answer a question related to a given document using one of five answer choices. The QA task of BioASQ <ref type="bibr" target="#b22">(Tsatsaronis et al., 2015)</ref> has phases of (a) retrieve question-related documents and (b) using related documents as contexts to answer yes/no, factoid, list or summary questions. BioASQ 2019 has a training set of 2,747 QA instances and a test set of 500 instances.</p><p>Several large-scale automatically collected biomedical QA datasets have been introduced: emrQA <ref type="bibr" target="#b14">(Pampari et al., 2018)</ref> is an extractive QA dataset for electronic medical records (EHR) built by re-purposing existing annotations on EHR corpora. BioRead <ref type="bibr" target="#b15">(Pappas et al., 2018)</ref> and BMKC <ref type="bibr" target="#b6">(Kim et al., 2018)</ref> both collect cloze-style QA instances by masking biomedical named entities in sentences of research articles and using other parts of the same article as context.</p><p>Yes/No QA: Datasets such as HotpotQA <ref type="bibr" target="#b23">(Yang et al., 2018)</ref>, Natural Questions <ref type="bibr" target="#b8">(Kwiatkowski et al., 2019)</ref>, ShARC <ref type="bibr" target="#b20">(Saeidi et al., 2018)</ref> and BioASQ <ref type="bibr" target="#b22">(Tsatsaronis et al., 2015)</ref> contain yes/no questions as well as other types of questions. BoolQ <ref type="bibr" target="#b1">(Clark et al., 2019)</ref> specifically focuses on naturally occurring yes/no questions, and those questions are shown to be surprisingly difficult to answer. We add a "maybe" choice in PubMedQA to cover uncertain instances.</p><p>Typical neural approaches to answering yes/no questions involve encoding both the question and context, and decoding the encoding to a class output, which is similar to the well-studied natural language inference (NLI) task.</p><p>Recent breakthroughs of pre-trained language models like ELMo <ref type="bibr" target="#b17">(Peters et al., 2018)</ref> and BERT <ref type="bibr" target="#b2">(Devlin et al., 2018)</ref> show significant performance im-  provements on NLI tasks. In this work, we use domain specific versions of them to set baseline performance on PubMedQA.</p><p>3 PubMedQA Dataset</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Data Collection</head><p>PubMedQA is split into three subsets: labeled, unlabeled and artificially generated. They are denoted as PQA-L(abeled), PQA-U(nlabeled) and PQA-A(rtificial), respectively. We show the architecture of PubMedQA dataset in Fig. <ref type="figure" target="#fig_0">2</ref>. Collection of PQA-L and PQA-U: PubMed articles which have i) a question mark in the titles and ii) a structured abstract with conclusive part are collected and denoted as pre-PQA-U. Now each instance has 1) a question which is the original title 2) a context which is the structured abstract without the conclusive part and 3) a long answer which is the conclusive part of the abstract. Two annotators 3 labeled 1k instances from pre-PQA-U with yes/no/maybe to build PQA-L using Algorithm 1. The annotator 1 doesn't need to do much reasoning to annotate since the long answer is available. We denote this reasoning-free setting. However, the annotator 2 cannot use the long answer, so reasoning over the context is required for 3 Both are qualified M.D. candidates.</p><formula xml:id="formula_0">Statistic PQA-L PQA-U PQA-A Number of</formula><p>Algorithm 1 PQA-L data collection procedure</p><formula xml:id="formula_1">Input: pre-PQA-U ReasoningFreeAnnotation ? {} ReasoningRequiredAnnotation ? {} GroundTruthLabel ? {} while not finished do</formula><p>Randomly sample an instance inst from pre-PQA-U if inst is not yes/no/maybe answerable then Remove inst and continue to next iteration end if Annotator 1 annotates inst with l1 ? {yes, no, maybe} using question, context and long answer Annotator 2 annotates inst with l2 ? {yes, no, maybe} using question and context if l1 = l2 then la ? l1 else</p><p>Annotator 1 and Annotator 2 discuss for an agreement annotation la if not ?la then Remove inst and continue to next iteration</p><formula xml:id="formula_2">end if end if ReasoningFreeAnnotation[inst] ? l1 ReasoningRequiredAnnotation[inst] ? l2 GroundTruthLabel[inst] ? la end while</formula><p>annotation. We denote such setting as reasoningrequired setting. Note that the annotation process might assign wrong labels when both annotator 1 and annotator 2 make a same mistake, but considering human performance in ?5.1, such error rate could be as low as 1%<ref type="foot" target="#foot_1">4</ref> . 500 randomly sampled PQA-L instances are used for 10-fold cross validation and the rest 500 instances consist of Pub-MedQA test set.</p><p>Further, we include the unlabeled instances in pre-PQA-U with yes/no/maybe answerable questions to build PQA-U. For this, we use a simple rule-based method which removes all questions started with interrogative words (i.e. wh-words) or involving selections from multiple entities. This results in over 93% agreement with annotator 1 in identifying the questions that can be answered by yes/no/maybe.  statement titles are converted to questions by simply moving or adding copulas ("is", "are") or auxiliary verbs ("does", "do") in the front and further revising for coherence (e.g.: adding a question mark). We generate the yes/no answer according to negation status of the VB. Several examples are shown in Table <ref type="table" target="#tab_3">2</ref>. We collected 211.3k instances for PQA-A, of which 200k randomly sampled instances are for training and the rest 11.3k instances are for validation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Characteristics</head><p>We show the basic statistics of three PubMedQA subsets in Table <ref type="table" target="#tab_2">1</ref>.</p><p>Instance Topics: PubMed abstracts are manually annotated by medical librarians with Medical Subject Headings (MeSH) 6 , which is a controlled vocabulary designed to describe the topics of biomedical texts. We use MeSH terms to represent abstract topics, and visualize their distribution in Fig. <ref type="figure" target="#fig_1">3</ref>  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Evaluation Settings</head><p>The main metrics of PubMedQA are accuracy and macro-F1 on PQA-L test set using question and context as input. We denote prediction using question and context as a reasoning-required setting, because under this setting answers are not directly expressed in the input and reasoning over the contexts is required to answer the question. Additionally, long answers are available at training time, so generation or prediction of them can be used as an auxiliary task in this setting.</p><p>A parallel setting, where models can use question and long answer to predict yes/no/maybe answer, is denoted as reasoning-free setting since yes/no/maybe are usually explicitly expressed in the long answers (i.e.: conclusions of the abstracts). Obviously, it's a much easier setting which can be exploited for bootstrapping PQA-U.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Methods</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Fine-tuning BioBERT</head><p>We fine-tune BioBERT <ref type="bibr" target="#b10">(Lee et al., 2019)</ref> on Pub-MedQA as a baseline. BioBERT is initialized with BERT <ref type="bibr" target="#b2">(Devlin et al., 2018)</ref> and further pretrained on PubMed abstracts and PMC 7 articles. Expectedly, it vastly outperforms BERT in various biomedical NLP tasks. We denote the original transformer weights of BioBERT as ? 0 .</p><p>While fine-tuning, we feed PubMedQA questions and contexts (or long answers), separated 7 https://www.ncbi.nlm.nih.gov/pmc/ by the special [SEP] token, to BioBERT. The yes/no/maybe labels are predicted using the special [CLS] embedding using a softmax function. Cross-entropy loss of predicted and true label distribution is denoted as L QA .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Long Answer as Additional Supervision</head><p>Under reasoning-required setting, long answers are available in training but not inference phase. We use them as an additional signal for training: similar to <ref type="bibr" target="#b11">Ma et al. (2018)</ref> regularizing neural machine translation models with binary bag-of-word (BoW) statistics, we fine-tune BioBERT with an auxiliary task of predicting the binary BoW statistics of the long answers, also using the special [CLS] embedding. We minimize binary crossentropy loss of this auxiliary task:</p><formula xml:id="formula_3">L BoW = - 1 N i b i log bi + (1 -b i )log(1 -bi )</formula><p>where b i and bi are ground-truth and predicted probability of whether token i is in the long answers (i.e.: b i ? {0, 1} and bi ? [0, 1]), and N is the BoW vocabulary size. The total loss is:</p><formula xml:id="formula_4">L = L QA + ?L BoW</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Bootstrapping (Reasoning-free)</head><p>Training (Reasoning-required)</p><formula xml:id="formula_5">(q A , c A ), l A (q U , c U ), l U pseudo (q L , c L ), l L (q A , a A ), l A (q L , a L ), l L (q U , a U ) ? 0 ? 0 ? I ? II ? F ? B 1 ? B 2</formula><p>Fine-tuning Supervision Pseudolabeling eq. ( <ref type="formula">1</ref>) eq. ( <ref type="formula">2</ref>) eq. ( <ref type="formula">3</ref>) eq. ( <ref type="formula" target="#formula_10">5</ref>) eq. ( <ref type="formula">6</ref>) eq. ( <ref type="formula" target="#formula_9">4</ref>)</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Phase I</head><p>Phase II</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Final Phase</head><p>Figure <ref type="figure">5</ref>: Multi-phase fine-tuning architecture. Notations and equations are described in ?4.3.</p><p>In reasoning-free setting which we use for bootstrapping, the regularization coefficient ? is set to 0 because answers are directly used as input.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Multi-phase Fine-tuning Schedule</head><p>Since PQA-A and PQA-U have different properties from the ultimate test set of PQA-L, BioBERT is fine-tuned in a multi-phase style on different subsets. Fig. <ref type="figure">5</ref> shows the architecture of this training schedule. We use q, c, a, l to denote question, context, long answer and yes/no/maybe label of instances, respectively. Their source subsets are indexed by the superscripts of A for PQA-A, U for PQA-U and L for PQA-L.</p><p>Phase I Fine-tuning on PQA-A: PQA-A is automatically collected whose questions and labels are artificially generated. As a result, questions of PQA-A might differ a lot from those of PQA-U and PQA-L, and it only has yes/no labels with a very imbalanced distribution (92.8% yes v.s. 7.2% no). Despite these drawbacks, PQA-A has substantial training instances so models could still benefit from it as a pre-training step. Thus, in Phase I of multi-phase fine-tuning, we initialize BioBERT with ? 0 , and fine-tune it on PQA-A using question and context as input:</p><formula xml:id="formula_6">? I ? argmin ? L(BioBERT ? (q A , c A ), l A ) (1)</formula><p>Phase II Fine-tuning on Bootstrapped PQA-U: To fully utilize the unlabeled instances in PQA-U, we exploit the easiness of reasoning-free setting to pseudo-label these instances with a bootstrapping strategy: first, we initialize BioBERT with ? 0 , and fine-tune it on PQA-A using question and long answer (reasoning-free),</p><formula xml:id="formula_7">? B 1 ? argmin ? L(BioBERT ? (q A , a A ), l A ) (2)</formula><p>then we further fine-tune BioBERT ? B 1 on PQA-L, also under the reasoning-free setting:</p><formula xml:id="formula_8">? B 2 ? argmin ? L(BioBERT ? (q L , a L ), l L ) (3)</formula><p>We pseudo-label PQA-U instances using the most confident predictions of BioBERT ? B 2 for each class. Confidence is simply defined by the corresponding softmax probability and then we label a subset which has the same proportions of yes/no/maybe labels as those in the PQA-L:</p><formula xml:id="formula_9">l U pseudo ? BioBERT ? B 2 (q U , a U )<label>(4)</label></formula><p>In phase II, we fine-tune BioBERT ? I on the bootstrapped PQA-U using question and context (under reasoning-required setting):</p><formula xml:id="formula_10">? II ? argmin ? L(BioBERT ? (q U , c U ), l U pseudo )<label>(5)</label></formula><p>Final Phase Fine-tuning on PQA-L: In the final phase, we fine-tune BioBERT ? II on PQA-L:</p><formula xml:id="formula_11">? F ? argmin ? L(BioBERT ? (q L , c L ), l L ) (6)</formula><p>Final predictions on instances of PQA-L validation and test sets are made using BioBERT ? F :</p><formula xml:id="formula_12">l pred = BioBERT ? F (q L , c L )</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Compared Models</head><p>Majority: The majority (about 55%) of the instances have the label "yes". We use a trivial baseline denoted as Majority where we simply predict "yes" for all instances, regardless of the question and context. Shallow Features: For each instance, we include the following shallow features: 1) TF-IDF statistics of the question 2) TF-IDF statistics of the context/long answer and 3) sum of IDF of the overlapping non-stop words between the question and the context/long answer. To allow multi-phase fine-tuning, we apply a feed-forward neural network on the shallow features instead of using a logistic classifier.</p><p>BiLSTM: We simply concatenate the question and context/long answer with learnable segment embeddings appended to the biomedical word2vec embeddings <ref type="bibr" target="#b18">(Pyysalo et al., 2013)</ref> of each token. The concatenated sentence is then fed to a biL-STM, and the final hidden states of the forward and backward network are used for classifying the yes/no/maybe label.</p><p>ESIM with BioELMo: Following the state-ofthe-art recurrent architecture of NLI <ref type="bibr" target="#b17">(Peters et al., 2018)</ref>, we use pre-trained biomedical contextualized embeddings BioELMo <ref type="bibr" target="#b5">(Jin et al., 2019)</ref> for word representations. Then we apply the ESIM model <ref type="bibr" target="#b0">(Chen et al., 2016)</ref>, where a biLSTM is used to encode the question and context/long answer, followed by an attentional local inference layer and a biLSTM inference composition layer. After pooling, a softmax output unit is applied for predicting the yes/no/maybe label. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5">Compared Training Schedules</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Experiments</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Human Performance</head><p>Human performance is measured during the annotation: As shown in Algorithm 1, annotations of annotator 1 and annotator 2 are used to calculate reasoning-free and reasoning-required human performance, respectively, against the discussed ground truth labels. Human performance on the test set of PQA-L is shown in Table <ref type="table">4</ref>. We only test single-annotator performance due to limited resources. <ref type="bibr" target="#b8">Kwiatkowski et al. (2019)</ref> show that an ensemble of annotators perform significantly better than single-annotator, so the results reported in Table <ref type="table">4</ref> are the lower bounds of human performance. Under reasoning-free setting where the annotator can see the conclusions, a single human achieves 90.4% accuracy and 84.2% macro-F1. Under reasoning-required setting, the task be-comes much harder, but it's still possible for humans to solve: a single annotator can get 78.0% accuracy and 72.2% macro-F1.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Setting</head><p>Accuracy (%) Macro-F1 (%)</p><p>Reasoning-Free 90.40 84.18 Reasoning-Required 78.00 72.19</p><p>Table <ref type="table">4</ref>: Human performance (single-annotator).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Main Results</head><p>We report the test set performance of different models and training schedules in Table <ref type="table" target="#tab_7">5</ref>. In general, multi-phase fine-tuning of BioBERT with additional supervision outperforms other baselines by large margins, but the results are still much worse than just single-human performance.</p><p>Comparison of Models: A trend of BioBERT &gt; ESIM w/ BioELMo &gt; BiLSTM &gt; shallow features &gt; majority, conserves across different training schedules on both accuracy and macro-F1. Fine-tuned BioBERT is better than state-of-theart recurrent model of ESIM w/ BioELMo, probably because BioELMo weights are fixed while all BioBERT parameters can be fine-tuned, which better benefit from the pre-training settings.</p><p>Comparison of Training Schedules: Multiphase fine-tuning setting gets 5 out of 9 modelwise best accuracy/macro-F1. Due to lack of annotated data, training only on the PQA-L (final phase only) generates similar results as the majority baseline. In phase I + Final setting where models are pre-trained on PQA-A, we observe significant improvements on accuracy and macro-F1 and some models even achieve their best accuracy under this setting. This indicates that a hard task with limited training instances can be at least partially solved by pre-training on a large automatically collected dataset when the tasks are similarly formatted. Improvements are also observed in phase II + Final setting, though less significant than those of phase I + Final. As expected, multi-phase finetuning schedule is better than single-phase, due to different properties of the subsets.</p><p>Additional Supervision: Despite its simplicity, the auxiliary task of long answer BoW prediction clearly improves the performance: most results (28/40) are better with such additional supervision than without.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3">Intermediate Results</head><p>In this section we show the intermediate results of multi-phase fine-tuning schedule.</p><p>Phase I: Results are shown in Table <ref type="table" target="#tab_8">6</ref>. Phase I is fine-tuning on PQA-A using question and context. Since PQA-A is imbalanced due to its collection process, a trivial majority baseline gets 92.76% accuracy. Other models have better accuracy and especially macro-F1 than majority baseline. Finetuned BioBERT performs best. Bootstrapping: Results are shown in Table <ref type="table">7</ref>.</p><p>Bootstrapping is a three-step process: fine-tuning on PQA-A, then on PQA-L and pseudo-labeling PQA-U. All three steps are using question and long answer as input. Expectedly, models perform better in this reasoning-free setting than they do in reasoning-required setting (for PQA-A, Eq. 2 results in Table <ref type="table">7</ref> are better than the performance in Table <ref type="table" target="#tab_8">6</ref>; for PQA-L, Eq. 3 results in Table <ref type="table">7</ref> are better than the performance in Table <ref type="table" target="#tab_7">5</ref>).</p><p>Phase II: Results are shown in Table <ref type="table" target="#tab_9">8</ref>. In Phase II, since each model is fine-tuned on its own pseudo-labeled PQA-U instances, results are not comparable between models. While the ablation study in Table <ref type="table" target="#tab_7">5</ref> clearly shows that Phase II is helpful, performance in Phase II doesn't necessarily correlate with final performance on PQA-L.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Conclusion</head><p>We present PubMedQA, a novel dataset aimed at biomedical research question answering using yes/no/maybe, where complex quantitative reasoning is required to solve the task. PubMedQA has substantial automatically collected instances as well as the largest size of expert annotated yes/no/maybe questions in biomedical domain.</p><p>We provide a strong baseline using multi-phase fine-tuning of BioBERT with long answer as additional supervision, but it's still much worse than just single human performance.</p><p>There are several interesting future directions to explore on PubMedQA, e.g.: (1) about 21% of PubMedQA contexts contain no natural language descriptions of numbers, so how to properly handle these numbers is worth studying; (2) we use binary BoW statistics prediction as a simple demonstration for additional supervision of long answers. Learning a harder but more informative auxiliary task of long answer generation might lead to further improvements.</p><p>Articles of PubMedQA are biased towards clinical study-related topics (described in Appendix B), so PubMedQA has the potential to assist evidence-based medicine, which seeks to make clinical decisions based on evidence of high quality clinical studies. Generally, PubMedQA can serve as a benchmark for testing scientific reasoning abilities of machine reading comprehension models.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: Architecture of PubMedQA dataset. Pub-MedQA is split into three subsets, PQA-A(rtificial), PQA-U(nlabeled) and PQA-L(abeled).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: MeSH topic distribution of PubMedQA.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 4 :</head><label>4</label><figDesc>Figure 4: Proportional relationships between corresponded question types, reasoning types, and whether the text interpretations of numbers exist in contexts.</figDesc><graphic url="image-1.png" coords="4,306.40,191.96,216.59,128.08" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Final</head><label></label><figDesc>Phase Only: Under this setting, we train models only on PQA-L. It's an extremely low resources setting where there are only 450 training instances in each fold of cross-validation. Phase I + Final Phase: Under this setting, we skip the training on bootstrapped PQA-U. Models are first fine-tuned on PQA-A, and then fine-tuned on PQA-L. Phase II + Final Phase: Under this setting, we skip the training on PQA-A. Models are first fine-tuned on bootstrapped PQA-U, and then finetuned on PQA-L. Single-phase Training: Instead of training a model sequentially on different splits, under single-phase training setting we train the model on the combined training set of all PQA splits: PQA-A, bootstrapped PQA-U and PQA-L.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>PQA-Artificial (211.3k) Ori. Question Title PQA-Unlabeled (61.2k)</head><label></label><figDesc></figDesc><table><row><cell>Ori. Title -&gt; Question</cell><cell>Ori. Question Title</cell></row><row><cell>Structured Context</cell><cell></cell></row><row><cell>(Ori. Abstract</cell><cell></cell></row><row><cell>w/o conclusion)</cell><cell></cell></row><row><cell>Long Answer</cell><cell></cell></row><row><cell>(Conclusion)</cell><cell></cell></row><row><cell>Generated yes/no</cell><cell>Yes/no/maybe</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>PQA-Labeled (1k)</head><label></label><figDesc></figDesc><table><row><cell>Structured Context</cell><cell>Structured Context</cell></row><row><cell>(Ori. Abstract</cell><cell>(Ori. Abstract</cell></row><row><cell>w/o conclusion)</cell><cell>w/o conclusion)</cell></row><row><cell>Long Answer</cell><cell>Long Answer</cell></row><row><cell>(Conclusion)</cell><cell>(Conclusion)</cell></row><row><cell>Unlabeled</cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 1 :</head><label>1</label><figDesc>PubMedQA dataset statistics.</figDesc><table><row><cell>QA pairs</cell><cell>1.0k</cell><cell>61.2k</cell><cell>211.3k</cell></row><row><cell>Prop. of yes (%)</cell><cell>55.2</cell><cell>-</cell><cell>92.8</cell></row><row><cell>Prop. of no (%)</cell><cell>33.8</cell><cell>-</cell><cell>7.2</cell></row><row><cell>Prop. of maybe (%)</cell><cell>11.0</cell><cell>-</cell><cell>0.0</cell></row><row><cell>Avg. question length</cell><cell>14.4</cell><cell>15.0</cell><cell>16.3</cell></row><row><cell>Avg. context length</cell><cell>238.9</cell><cell>237.3</cell><cell>238.0</cell></row><row><cell>Avg. long answer length</cell><cell>43.2</cell><cell>45.9</cell><cell>41.0</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 2 :</head><label>2</label><figDesc>Examples of automatically generated instances for PQA-A. Original statement titles are converted to questions and answers are automatically generated according to the negation status.</figDesc><table><row><cell>Collection of PQA-A: Motivated by the recent</cell></row><row><cell>successes of large-scale pre-training from ELMo</cell></row><row><cell>(Peters et al., 2018) and BERT (Devlin et al.,</cell></row><row><cell>2018), we use a simple heuristic to collect many</cell></row><row><cell>noisily-labeled instances to build PQA-A for pre-</cell></row><row><cell>training. Towards this end, we use PubMed arti-</cell></row><row><cell>cles with 1) a statement title which has POS tag-</cell></row><row><cell>ging structures of NP-(VBP/VBZ) 5 and 2) a struc-</cell></row><row><cell>tured abstract including a conclusive part. The</cell></row></table><note><p><p><p>Spontaneous electrocardiogram alterations predict ventricular fibrillation in Brugada syndrome. Do spontaneous electrocardiogram alterations predict ventricular fibrillation in Brugada syndrome? yes 92.8</p>Liver grafts from selected older donors do not have significantly more ischaemia reperfusion injury.</p>Do liver grafts from selected older donors have significantly more ischaemia reperfusion injury?</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head></head><label></label><figDesc>Does reducing spasticity translate into functional benefit? Does ibuprofen increase perioperative blood loss during hip arthroplasty? Is a therapy good/necessary? 26.0 Should circumcision be performed in childhood? Is external palliative radiotherapy for gallbladder carcinoma effective? Is a statement true? 18.0 Sternal fracture in growing children: A rare and often overlooked fracture? Xanthogranulomatous cholecystitis: a premalignant condition?</figDesc><table><row><cell>Question Type</cell><cell>%</cell><cell>Example Questions</cell></row><row><cell cols="3">Does a factor influence the output? 36.5 Is a factor related to the output? 18.0 Can PRISM predict length of PICU stay?</cell></row><row><cell></cell><cell></cell><cell>Is trabecular bone related to primary stability of miniscrews?</cell></row><row><cell>Reasoning Type</cell><cell>%</cell><cell>Example Snippet in Context</cell></row><row><cell>Inter-group comparison</cell><cell cols="2">57.5 [...] Postoperative AF was significantly lower in the Statin group compared</cell></row><row><cell></cell><cell></cell><cell>with the Non-statin group (16% versus 33%, p=0.005). [...]</cell></row><row><cell>Interpreting subgroup statistics</cell><cell cols="2">16.5 [...] 57% of patients were of lower socioeconomic status and they had more</cell></row><row><cell></cell><cell></cell><cell>health problems, less functioning, and more symptoms [...]</cell></row><row><cell cols="3">Interpreting (single) group statistics 16.0 [...] A total of 4 children aged 5-14 years with a sternal fracture were treated</cell></row><row><cell></cell><cell></cell><cell>in 2 years, 2 children were hospitalized for pain management and [...]</cell></row><row><cell>Text Interpretations of Numbers</cell><cell>%</cell><cell>Example Snippet in Context</cell></row><row><cell cols="3">Existing interpretations of numbers 75.5 [...] Postoperative AF was significantly lower in the Statin group compared</cell></row><row><cell></cell><cell></cell><cell>with the Non-statin group (16% versus 33%, p=0.005). [...]</cell></row><row><cell>No interpretations (numbers only)</cell><cell cols="2">21.0 [...] 30-day mortality was 12.4% in those aged&lt;70 years and 22% in</cell></row><row><cell></cell><cell></cell><cell>those&gt;70 years (p&lt;0.001). [...]</cell></row><row><cell>No numbers (texts only)</cell><cell>3.5</cell><cell>[...</cell></row></table><note><p>] The halofantrine therapeutic dose group showed loss and distortion of inner hair cells and inner phalangeal cells [...] Table 3: Summary of PubMedQA question types, reasoning types and whether there are text descriptions of the statistics in context. Colored texts are matched key phrases (sentences) between types and examples.</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head></head><label></label><figDesc>? 46.70 ? 67.24 ? 46.21 ? 66.44 ? 51.41 ? 68.08 ? 52.72 ?</figDesc><table><row><cell>Model</cell><cell cols="2">Final Phase Only</cell><cell cols="2">Single-phase</cell><cell cols="4">Phase I + Final Phase II + Final</cell><cell cols="2">Multi-phase</cell></row><row><cell></cell><cell>Acc</cell><cell>F1</cell><cell>Acc</cell><cell>F1</cell><cell>Acc</cell><cell>F1</cell><cell>Acc</cell><cell>F1</cell><cell>Acc</cell><cell>F1</cell></row><row><cell>Majority</cell><cell>55.20</cell><cell>23.71</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell>Human (single)</cell><cell>78.00</cell><cell>72.19</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell>w/o A.S.</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Shallow Features</cell><cell>53.88</cell><cell>36.12</cell><cell>57.58</cell><cell>31.47</cell><cell>57.48</cell><cell>37.24</cell><cell>56.28</cell><cell>40.88</cell><cell>53.50</cell><cell>39.33</cell></row><row><cell>BiLSTM</cell><cell>55.16</cell><cell>23.97</cell><cell>55.46</cell><cell>39.70</cell><cell>58.44</cell><cell>40.67</cell><cell>52.98</cell><cell>33.84</cell><cell>59.82</cell><cell>41.86</cell></row><row><cell>ESIM w/ BioELMo</cell><cell>53.90</cell><cell>32.40</cell><cell>61.28</cell><cell>42.99</cell><cell>61.96</cell><cell>43.32</cell><cell>60.34</cell><cell>44.38</cell><cell>62.08</cell><cell>45.75</cell></row><row><cell>BioBERT</cell><cell>56.98</cell><cell>28.50</cell><cell>66.44</cell><cell>47.25</cell><cell>66.90</cell><cell>46.16</cell><cell>66.08</cell><cell>50.84</cell><cell>67.66</cell><cell>52.41</cell></row><row><cell>w/ A.S.</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Shallow Features</cell><cell>53.60</cell><cell>35.92</cell><cell>57.30</cell><cell>30.45</cell><cell>55.82</cell><cell>35.09</cell><cell>56.46  ?</cell><cell>40.76</cell><cell cols="2">55.06  ? 40.67  ?</cell></row><row><cell>BiLSTM</cell><cell>55.22  ?</cell><cell>23.86</cell><cell cols="6">55.96  ? 40.26  ? 61.06  ? 41.18  ? 54.12  ? 34.11  ?</cell><cell>58.86</cell><cell>41.06</cell></row><row><cell cols="2">ESIM w/ BioELMo 53.96  ?</cell><cell>31.07</cell><cell cols="4">62.68  ? 43.59  ? 63.72  ? 47.04  ?</cell><cell>60.16</cell><cell cols="3">45.81  ? 63.72  ? 47.90  ?</cell></row><row><cell>BioBERT</cell><cell>57.28  ?</cell><cell>28.70  ?</cell><cell>66.66</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 5 :</head><label>5</label><figDesc>Main results on PQA-L test set under reasoning-required setting. A.S.: additional supervision. ? with A.S. is better than without A.S. Underlined numbers are model-wise best performance, and bolded numbers are global best performance. All numbers are percentages.</figDesc><table><row><cell>Model</cell><cell cols="2">w/o A.S.</cell><cell cols="2">w/ A.S.</cell></row><row><cell></cell><cell>Acc</cell><cell>F1</cell><cell>Acc</cell><cell>F1</cell></row><row><cell>Majority</cell><cell cols="2">92.76 48.12</cell><cell>-</cell><cell>-</cell></row><row><cell>Shallow Features</cell><cell cols="4">93.01 54.59 93.05 55.12</cell></row><row><cell>BiLSTM</cell><cell cols="4">94.59 73.40 94.45 71.81</cell></row><row><cell cols="5">ESIM w/ BioELMo 94.82 74.01 95.04 75.22</cell></row><row><cell>BioBERT</cell><cell cols="4">96.50 84.65 96.40 83.76</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>Table 6 :</head><label>6</label><figDesc>Results of Phase I (eq. 1). Experiments are on PQA-A under reasoning-required setting. A.S.: additional supervision.</figDesc><table><row><cell>Model</cell><cell>Eq. 2</cell><cell></cell><cell cols="2">Eq. 3</cell></row><row><cell></cell><cell>Acc</cell><cell>F1</cell><cell>Acc</cell><cell>F1</cell></row><row><cell>Majority</cell><cell cols="2">92.76 48.12</cell><cell>55.20</cell><cell>23.71</cell></row><row><cell>Human (single)</cell><cell>-</cell><cell>-</cell><cell cols="2">90.40  ? 84.18  ?</cell></row><row><cell>Shallow Features</cell><cell cols="2">93.11 56.11</cell><cell>54.44</cell><cell>38.63</cell></row><row><cell>BiLSTM</cell><cell cols="2">95.97 83.70</cell><cell>71.46</cell><cell>50.93</cell></row><row><cell cols="3">ESIM w/ BioELMo 97.01 88.47</cell><cell>74.06</cell><cell>58.53</cell></row><row><cell>BioBERT</cell><cell cols="2">98.28 93.17</cell><cell>80.80</cell><cell>63.50</cell></row><row><cell cols="5">Table 7: Bootstrapping results. Experiments are on</cell></row><row><cell cols="5">PQA-A (eq. 2) and PQA-L (eq. 3) under reasoning-</cell></row><row><cell>free setting.</cell><cell></cell><cell></cell><cell></cell></row></table><note><p>? Reasoning-free human performance.</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head>Table 8 :</head><label>8</label><figDesc>Phase II results (eq. 5). Experiments are on pseudo-labeled PQA-U under reasoning-required setting. A.S.: additional supervision.</figDesc><table><row><cell>Model</cell><cell cols="2">w/o A.S.</cell><cell cols="2">w/ A.S.</cell></row><row><cell></cell><cell>Acc</cell><cell>F1</cell><cell>Acc</cell><cell>F1</cell></row><row><cell>Majority</cell><cell cols="2">55.10 23.68</cell><cell>-</cell><cell>-</cell></row><row><cell>Shallow Features</cell><cell cols="4">76.66 66.12 77.71 67.97</cell></row><row><cell>Majority</cell><cell cols="2">56.53 24.07</cell><cell>-</cell><cell>-</cell></row><row><cell>BiLSTM</cell><cell cols="4">85.33 81.32 85.68 81.87</cell></row><row><cell>Majority</cell><cell cols="2">55.10 23.68</cell><cell>-</cell><cell>-</cell></row><row><cell cols="5">ESIM w/ BioELMo 78.47 63.32 79.62 64.91</cell></row><row><cell>Majority</cell><cell cols="2">54.82 24.87</cell><cell>-</cell><cell>-</cell></row><row><cell>BioBERT</cell><cell cols="4">80.93 68.84 81.02 70.04</cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_0"><p>https://trec.nist.gov/</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4" xml:id="foot_1"><p>Roughly half of the products of two annotator error rates.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="5" xml:id="foot_2"><p>Using Stanford CoreNLP parser (Manning et al., 2014).</p></note>
		</body>
		<back>

			<div type="acknowledgement">
<div><head n="7">Acknowledgement</head><p>We are grateful for the anonymous reviewers of EMNLP who gave us very valuable comments and suggestions.</p></div>
			</div>			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>A Yes/no/maybe Answerability Not all naturally occuring question titles from PubMed are answerable by yes/no/maybe. The first step of annotating PQA-L (as shown in algorithm 1) from pre-PQA-U is to manually identify questions that can be answered using yes/no/maybe. We labeled 1091 (about 50.2%) of 2173 question titles as unanswerable. For example, those questions cannot be answered by yes/no/maybe:</p><p>? "Critical Overview of HER2 Assessement in Bladder Cancer: What Is Missing for a Better Therapeutic Approach?" (wh-question)</p><p>? "Otolaryngology externships and the match: Productive or futile?" (multiple choices)</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B Over-represented Topics</head><p>Clinical study-related topics are over-represented in PubMedQA: we found proportions of MeSH terms like:</p><p>? "Pregnancy Outcome"</p><p>? "Socioeconomic Factors"</p><p>? "Risk Assessment"</p><p>? "Survival Analysis"</p><p>? "Prospective Studies"</p><p>? "Case-Control Studies"</p><p>? "Reference Values" are significantly higher in the PubMedQA articles than those in 200k most recent general PubMed articles (significance is defined by p &lt; 0.05 in twoproportion z-test).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C Annotation Criteria</head><p>Strictly speaking, most yes/no/maybe research questions can be answered by "maybe" since there will always be some conditions where one statement is true and vice versa. However, the task will be trivial in this case. Instead, we annotate a question using "yes" if the experiments and results in the paper indicate it, so the answer is not universal but context-dependent. Given a question like "Do patients benefit from drug X?": certainly not all patients will benefit from it, but if there is a significant difference in an outcome between the experimental and control group, the answer will be "yes". If there is not, the answer will be "no".</p><p>"Maybe" is annotated when (1) the paper discusses conditions where the answer is True and conditions where the answer is False or (2) more than one intervention/observation/etc. is asked, and the answer is True for some but False for the others (e.g.: "Do Disease A, Disease B and/or Disease C benefit from drug X?"). To model uncertainty of the answer, we don't strictly follow the logic calculations where such questions can always be answered by either "yes" or "no".</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<author>
			<persName><forename type="first">Qian</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaodan</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhenhua</forename><surname>Ling</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Si</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hui</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Diana</forename><surname>Inkpen</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1609.06038</idno>
		<title level="m">Enhanced lstm for natural language inference</title>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Boolq: Exploring the surprising difficulty of natural yes/no questions</title>
		<author>
			<persName><forename type="first">Christopher</forename><surname>Clark</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kenton</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ming-Wei</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tom</forename><surname>Kwiatkowski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><surname>Collins</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kristina</forename><surname>Toutanova</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</title>
		<title level="s">Long Papers</title>
		<meeting>the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<author>
			<persName><forename type="first">Jacob</forename><surname>Devlin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ming-Wei</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kenton</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kristina</forename><surname>Toutanova</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1810.04805</idno>
		<title level="m">Bert: Pre-training of deep bidirectional transformers for language understanding</title>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">Trec 2007 genomics track overview</title>
		<author>
			<persName><forename type="first">William</forename><surname>Hersh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aaron</forename><surname>Cohen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lynn</forename><surname>Ruslen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Phoebe</forename><surname>Roberts</surname></persName>
		</author>
		<idno>TREC 2007</idno>
		<imprint>
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Trec 2006 genomics track overview</title>
		<author>
			<persName><forename type="first">William</forename><surname>Hersh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aaron</forename><forename type="middle">M</forename><surname>Cohen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Phoebe</forename><surname>Roberts</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hari</forename><forename type="middle">Krishna</forename><surname>Rekapalli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">TREC</title>
		<imprint>
			<date type="published" when="2006">2006. 2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<author>
			<persName><forename type="first">Qiao</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bhuwan</forename><surname>Dhingra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">William</forename><forename type="middle">W</forename><surname>Cohen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xinghua</forename><surname>Lu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1904.02181</idno>
		<title level="m">Probing biomedical embeddings from language models</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">A pilot study of biomedical text comprehension using an attention-based deep neural reader: Design and experimental analysis</title>
		<author>
			<persName><forename type="first">Seongsoon</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Donghyeon</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yonghwa</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kyubum</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Byounggun</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Minji</forename><surname>Jeon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jihye</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aik</forename><surname>Choon Tan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jaewoo</forename><surname>Kang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">JMIR medical informatics</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="issue">1</biblScope>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">The narrativeqa reading comprehension challenge</title>
		<author>
			<persName><forename type="first">Tom??</forename><surname>Ko?isk?</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jonathan</forename><surname>Schwarz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Phil</forename><surname>Blunsom</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chris</forename><surname>Dyer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Karl</forename><forename type="middle">Moritz</forename><surname>Hermann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G?abor</forename><surname>Melis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Edward</forename><surname>Grefenstette</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Transactions of the Association of Computational Linguistics</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page" from="317" to="328" />
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<author>
			<persName><forename type="first">Tom</forename><surname>Kwiatkowski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jennimaria</forename><surname>Palomaki</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Olivia</forename><surname>Rhinehart</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><surname>Collins</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ankur</forename><surname>Parikh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chris</forename><surname>Alberti</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Danielle</forename><surname>Epstein</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Illia</forename><surname>Polosukhin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Matthew</forename><surname>Kelcey</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jacob</forename><surname>Devlin</surname></persName>
		</author>
		<title level="m">Natural questions: a benchmark for question answering research</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<author>
			<persName><forename type="first">Guokun</forename><surname>Lai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qizhe</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hanxiao</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yiming</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Eduard</forename><surname>Hovy</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1704.04683</idno>
		<title level="m">Race: Large-scale reading comprehension dataset from examinations</title>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<author>
			<persName><forename type="first">Jinhyuk</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wonjin</forename><surname>Yoon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sungdong</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Donghyeon</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sunkyu</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chan</forename><surname>Ho</surname></persName>
		</author>
		<author>
			<persName><forename type="first">So</forename></persName>
		</author>
		<author>
			<persName><forename type="first">Jaewoo</forename><surname>Kang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1901.08746</idno>
		<title level="m">Biobert: pre-trained biomedical language representation model for biomedical text mining</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<author>
			<persName><forename type="first">Shuming</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xu</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yizhong</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Junyang</forename><surname>Lin</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1805.04871</idno>
		<title level="m">Bag-of-words as target for neural machine translation</title>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">The Stanford CoreNLP natural language processing toolkit</title>
		<author>
			<persName><forename type="first">D</forename><surname>Christopher</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mihai</forename><surname>Manning</surname></persName>
		</author>
		<author>
			<persName><forename type="first">John</forename><surname>Surdeanu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jenny</forename><surname>Bauer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Steven</forename><forename type="middle">J</forename><surname>Finkel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><surname>Bethard</surname></persName>
		</author>
		<author>
			<persName><surname>Mc-Closky</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Association for Computational Linguistics (ACL) System Demonstrations</title>
		<imprint>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="55" to="60" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Machine reading of biomedical texts about alzheimers disease</title>
		<author>
			<persName><forename type="first">Roser</forename><surname>Morante</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Martin</forename><surname>Krallinger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alfonso</forename><surname>Valencia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Walter</forename><surname>Daelemans</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CLEF 2012 Conference and Labs of the Evaluation Forum-Question Answering For Machine Reading Evaluation (QA4MRE)</title>
		<imprint>
			<publisher>Rome/Forner</publisher>
			<date type="published" when="2012">2012</date>
			<biblScope unit="page" from="1" to="14" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<author>
			<persName><forename type="first">Anusri</forename><surname>Pampari</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Preethi</forename><surname>Raghavan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jennifer</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jian</forename><surname>Peng</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1809.00732</idno>
		<title level="m">emrqa: A large corpus for question answering on electronic medical records</title>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Bioread: A new dataset for biomedical reading comprehension</title>
		<author>
			<persName><forename type="first">Dimitris</forename><surname>Pappas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Eleventh International Conference on Language Resources and Evaluation</title>
		<meeting>the Eleventh International Conference on Language Resources and Evaluation</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page">2018</biblScope>
		</imprint>
	</monogr>
	<note>Ion Androutsopoulos, and Haris Papageorgiou</note>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Qa4mre 2011-2013: Overview of question answering for machine reading evaluation</title>
		<author>
			<persName><forename type="first">Anselmo</forename><surname>Pe?as</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Eduard</forename><surname>Hovy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pamela</forename><surname>Forner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">?lvaro</forename><surname>Rodrigo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Richard</forename><surname>Sutcliffe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Roser</forename><surname>Morante</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference of the Cross-Language Evaluation Forum for European Languages</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2013">2013</date>
			<biblScope unit="page" from="303" to="320" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<author>
			<persName><forename type="first">Mark</forename><surname>Matthew E Peters</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mohit</forename><surname>Neumann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Matt</forename><surname>Iyyer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christopher</forename><surname>Gardner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kenton</forename><surname>Clark</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Luke</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><surname>Zettlemoyer</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1802.05365</idno>
		<title level="m">Deep contextualized word representations</title>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">Distributional semantics resources for biomedical text processing</title>
		<author>
			<persName><forename type="first">Sampo</forename><surname>Pyysalo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Filip</forename><surname>Ginter</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hans</forename><surname>Moen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tapio</forename><surname>Salakoski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sophia</forename><surname>Ananiadou</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<author>
			<persName><forename type="first">Pranav</forename><surname>Rajpurkar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jian</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Konstantin</forename><surname>Lopyrev</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Percy</forename><surname>Liang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1606.05250</idno>
		<title level="m">Squad: 100,000+ questions for machine comprehension of text</title>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<author>
			<persName><forename type="first">Marzieh</forename><surname>Saeidi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Max</forename><surname>Bartolo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Patrick</forename><surname>Lewis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sameer</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tim</forename><surname>Rockt?schel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mike</forename><surname>Sheldon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Guillaume</forename><surname>Bouchard</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sebastian</forename><surname>Riedel</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1809.01494</idno>
		<title level="m">Interpretation of natural language rules in conversational machine reading</title>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Do preoperative statins reduce atrial fibrillation after coronary artery bypass grafting?</title>
		<author>
			<persName><forename type="first">Hiroaki</forename><surname>Sakamoto</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yasunori</forename><surname>Watanabe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Masataka</forename><surname>Satou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Annals of thoracic and cardiovascular surgery</title>
		<imprint>
			<biblScope unit="volume">17</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="376" to="382" />
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">An overview of the bioasq large-scale biomedical semantic indexing and question answering competition</title>
		<author>
			<persName><forename type="first">George</forename><surname>Tsatsaronis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Georgios</forename><surname>Balikas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Prodromos</forename><surname>Malakasiotis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ioannis</forename><surname>Partalas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Matthias</forename><surname>Zschunke</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dirk</forename><surname>Michael R Alvers</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Anastasia</forename><surname>Weissenborn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sergios</forename><surname>Krithara</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dimitris</forename><surname>Petridis</surname></persName>
		</author>
		<author>
			<persName><surname>Polychronopoulos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">BMC bioinformatics</title>
		<imprint>
			<biblScope unit="volume">16</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page">138</biblScope>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<author>
			<persName><forename type="first">Zhilin</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Peng</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Saizheng</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">William</forename><forename type="middle">W</forename><surname>Cohen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ruslan</forename><surname>Salakhutdinov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christopher</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1809.09600</idno>
		<title level="m">Hotpotqa: A dataset for diverse, explainable multi-hop question answering</title>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
