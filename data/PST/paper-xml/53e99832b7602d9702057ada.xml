<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Quality-Aware Images</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><roleName>Member, IEEE</roleName><forename type="first">Zhou</forename><surname>Wang</surname></persName>
							<email>zhouwang@ieee.org</email>
						</author>
						<author>
							<persName><forename type="first">Guixing</forename><surname>Wu</surname></persName>
							<email>g2wu@bbcr.uwaterloo.ca</email>
						</author>
						<author>
							<persName><roleName>Member, IEEE</roleName><forename type="first">Hamid</forename><forename type="middle">Rahim</forename><surname>Sheikh</surname></persName>
							<email>hamid.sheikh@ieee.org</email>
						</author>
						<author>
							<persName><roleName>Senior Member, IEEE</roleName><forename type="first">Eero</forename><forename type="middle">P</forename><surname>Simoncelli</surname></persName>
							<email>eero.simoncelli@nyu.edu</email>
						</author>
						<author>
							<persName><roleName>Senior Member, IEEE</roleName><forename type="first">En-Hui</forename><surname>Yang</surname></persName>
							<email>ehyang@bbcr.uwaterloo.ca</email>
						</author>
						<author>
							<persName><roleName>Fellow, IEEE</roleName><forename type="first">Alan</forename><forename type="middle">Conrad</forename><surname>Bovik</surname></persName>
							<email>bovik@ece.utexas.edu</email>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="institution">New York University</orgName>
								<address>
									<postCode>10012</postCode>
									<settlement>New York</settlement>
									<region>NY</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff1">
								<orgName type="department">Department of Electrical Engineering</orgName>
								<orgName type="institution">The University of Texas at Arlington</orgName>
								<address>
									<postCode>76019</postCode>
									<settlement>Arlington</settlement>
									<region>TX</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff2">
								<orgName type="department">Center for Neural Science and the Courant Institute for Mathematical Sciences</orgName>
								<orgName type="institution">New York University</orgName>
								<address>
									<postCode>10012</postCode>
									<settlement>New York</settlement>
									<region>NY</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff3">
								<orgName type="department">Department of Electrical and Computer En-gineering</orgName>
								<orgName type="institution">University of Waterloo</orgName>
								<address>
									<postCode>N2L 3G1</postCode>
									<settlement>Waterloo</settlement>
									<region>ON</region>
									<country key="CA">Canada</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff4">
								<orgName type="institution">Texas Instruments, Inc</orgName>
								<address>
									<postCode>75243</postCode>
									<settlement>Dallas</settlement>
									<region>TX</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff5">
								<orgName type="department">Department of Electrical and Computer Engi-neering</orgName>
								<orgName type="institution">The University of Texas at Austin</orgName>
								<address>
									<postCode>78712</postCode>
									<settlement>Austin</settlement>
									<region>TX</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Quality-Aware Images</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">61AE02AFA5802BB2C0C817902EF5EBF3</idno>
					<idno type="DOI">10.1109/TIP.2005.864165</idno>
					<note type="submission">received August 30, 2004; revised May 31, 2005.</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.3" ident="GROBID" when="2023-07-28T05:34+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Generalized Gaussian density (GGD)</term>
					<term>image communication</term>
					<term>image quality assessment</term>
					<term>image watermarking</term>
					<term>information hiding</term>
					<term>natural image statistics</term>
					<term>quality-aware image</term>
					<term>reduced-reference image quality assessment</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>We propose the concept of quality-aware image, in which certain extracted features of the original (high-quality) image are embedded into the image data as invisible hidden messages. When a distorted version of such an image is received, users can decode the hidden messages and use them to provide an objective measure of the quality of the distorted image. To demonstrate the idea, we build a practical quality-aware image encoding, decoding and quality analysis system, 1 which employs: 1) a novel reduced-reference image quality assessment algorithm based on a statistical model of natural images and 2) a previously developed quantization watermarking-based data hiding technique in the wavelet transform domain.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>is expensive and time-consuming. On the other hand, most objective image/video quality assessment methods proposed in the literature <ref type="bibr" target="#b0">[1]</ref>- <ref type="bibr" target="#b2">[3]</ref> are not applicable in this scenario because they are full-reference (FR) methods that require access to the original images as references. Therefore, it is highly desirable to develop quality assessment algorithms that do not require full access to the reference images.</p><p>Unfortunately, no-reference (NR) or "blind" image quality assessment is an extremely difficult task. Most proposed NR quality metrics are designed for one or a set of predefined specific distortion types <ref type="bibr" target="#b3">[4]</ref>- <ref type="bibr" target="#b9">[10]</ref> that may not be generalized for evaluating images degraded with other types of distortions. Moreover, knowledge of the distortions that arise between the original and corrupted images is in general not available to image quality assessment systems. Thus, it is desirable to have a more general image quality assessment system that is applicable to a wide variety of distortions. However, to the best of our knowledge, no such method has been proposed and extensively tested.</p><p>One interesting recent development in image/video quality assessment research is to design reduced-reference (RR) methods for quality assessment <ref type="bibr" target="#b1">[2]</ref>, <ref type="bibr" target="#b2">[3]</ref>. These methods do not require full access to reference images, but only needs partial information, in the form of a set of extracted features. Conceptually, RR methods make the quality assessment task easier than NR methods by paying the additional cost of transmitting side information to the users. The standard deployment of an RR method requires the side information to be sent through an ancillary data channel <ref type="bibr" target="#b2">[3]</ref>. However, this restricts the application scope of the method because an additional data channel may be inconvenient or expensive to provide. An alternative solution would be to send the side information in the same channel as the images being transmitted. For example, the side information can be included as a component of the image data structure (e.g., as part of the header of the image format). However, this strategy would be difficult to implement in existing large-scale, heterogeneous networks such as the Internet, because it requires all the users in the communication network to adopt a new image format, or amend all the existing image formats to allow the side information to be included. Besides, lossy data transmission and typical image format conversion may cause loss of the original image headers.</p><p>In this paper, we propose the concept of quality-aware image, in which extracted features of the reference image are embedded as hidden messages. When a distorted version of such an image is received, the users can decode the hidden messages and use them to help evaluate the quality of the distorted image using an RR quality assessment method. There are several advantages of this approach.</p><p>• It uses an RR method that makes the image quality assessment task feasible (as compared to FR and NR methods). • It does not affect the conventional usage of the image data because the data hiding process causes only invisible changes to the image. • It does not require a separate data channel to transmit the side information. • It allows the image data to be stored, converted and distributed using any existing or user-defined formats without losing the functionality of "quality-awareness," provided the hidden messages are not corrupted during lossy format conversion. • It provides the users with a chance to partially "repair" the received distorted images by making use of the embedded features. This study is largely inspired by <ref type="bibr" target="#b10">[11]</ref>- <ref type="bibr" target="#b12">[13]</ref>, where a pseudorandom bit sequence or a watermark image is hidden inside the image being transmitted. The bit-error rate or the degradation of the watermark image measured at the receiver side is then used as an indication of the quality degradation of the host image signal. These methods are perhaps the first attempts to use information hiding technologies for the estimation of image quality degradation. Nevertheless, strictly speaking, these methods are not image quality assessment methods because no extracted features about either the reference or the distorted images are actually used in the quality evaluation process. Instead, the distortion processes that occur in the distortion channel are gauged, in the hope that such estimated channel distortion would correlate well with perceptual image degradation incurred during transmission through the channel. However, such a connection is obscured by the nature (e.g., complexity) of the image signals and the types of image distortions, which have variable effects on perceived image quality. In addition, these methods provide no clue about how the received distorted images can be repaired.</p><p>Information hiding or digital watermarking has been an active research area in the last decade. Traditionally, these techniques have been designed for security-related applications such as copyright protection and data authentication. Recently, researchers have attempted to broaden their application scope to nonsecurity oriented applications <ref type="bibr" target="#b13">[14]</ref>, <ref type="bibr" target="#b14">[15]</ref>. Quality-aware images mainly belong to the second category (see Section V for discussions), and they bring about new challenges in the selection and design of information hiding techniques.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>II. QUALITY-AWARE IMAGE</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Framework</head><p>A system diagram of quality-aware image encoding, decoding and quality analysis system is shown in Fig. <ref type="figure" target="#fig_0">1</ref>. A feature extraction process is first applied to the original image, which is assumed to have perfect quality. The quality-aware image is obtained by embedding these features as invisible messages into the original image. The quality-aware image may then pass through a "distortion process" before it reaches the receiver side. Here the "distortion process" is general in concept. It can be a distortion channel in an image communication system, with possibly lossy compression, noise contamination and/or postprocessing involved. It can also be any other processes that may alter the image.</p><p>At the receiver side, the hidden messages are first decoded from the distorted quality-aware image. In order for correct decoding of the messages, the key for information embedding and decoding is shared between the sender and the receiver. Depending on the application environment, there may be different ways to distribute the embedding key. One simple solution is to attach the key to the decoder software and/or publish the key, so that it can be easily obtained by all potential users of quality-aware images. Note that the key is independent of the image and can be the same for all quality-aware images, so it does not need to be transmitted with the image data. The decoded messages are translated back to the features about the reference image. Next, another feature extraction procedure corresponding to the one at the sender side is applied to the distorted image. The resulting features are then compared with those of the reference image to yield a quality score for the distorted quality-aware image.</p><p>In order to improve robustness, error detection/correction coding techniques may be applied before the information embedding process. Nevertheless, the hidden messages may still be decoded incorrectly when the distortions are extremely severe. At the receiver side, the system must be able to detect such situations (based on the error detection and correction code) and report a failure message, instead of a quality score.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Design Considerations</head><p>Designing an effective quality-aware image system is a challenging task. On the one hand, in order to provide effective quality prediction, the RR quality assessment system desires to know as much information as possible about the reference image. Therefore, the information hiding system would need to embed a fairly large amount of information. On the other hand, in order for the hidden messages to be invisible and for these messages to survive a wide variety and degree of distortions, the amount of information that can be embedded is limited. The RR quality assessment system must observe this limit and carefully select a set of features that can be encoded within the limit. These features must be highly relevant to image quality degradations. They must also provide an efficient summary about the reference image.</p><p>Another issue that may need to be considered is that many data hiding techniques tend to change certain statistical features of the original image (e.g. <ref type="bibr" target="#b15">[16]</ref> and <ref type="bibr" target="#b16">[17]</ref>). This could potentially conflict with quality assessment systems because these systems may rely on the way that these statistical features change as an indication of quality degradation.</p><p>To summarize, a successful quality-aware image system must provide a good trade-off between data hiding load, embedding distortion, robustness, and the accuracy of image quality prediction.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Simple Example</head><p>Perhaps the simplest way to implement a quality-aware image system is to embed a certain number of (perhaps randomly selected) reference image pixels as hidden messages. For synchronization purpose, the positions of theses pixels also need to be embedded. At the receiver side, the decoded reference image pixels are compared with the corresponding distorted image pixels, and certain distortion/quality metric, such as mean-squared error (MSE) and peak signal-to-noise ratio (PSNR), are estimated.</p><p>Such a system, although simple, is quite weak in several aspects. First, it requires a high data hiding rate. For example, for a 512 512, 8 bits/pixel gray scale image, to embed 1% of the image pixels (together with 2 9 bits for encoding each pixel position) requires a total of 68 146 bits, a heavy load for most robust information hiding systems. Second, such a small number of pixels is unlikely to allow accurate estimation of the distortion metrics, unless the distortion between the reference and distorted images is independently and identically distributed noise. The obvious drawbacks of this simple example lead us to consider image features that are more efficient in summarizing image information and more effective in evaluating image quality.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>III. IMPLEMENTATION</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. RR Quality Assessment</head><p>Here, we propose a new RR quality assessment method based on statistics computed for natural images in the wavelet transform domain. Wavelet transforms provide a convenient framework for localized representation of signals simultaneously in space and frequency. They have been widely used to model the processing in the early stages of biological visual systems and have also become the preferred form of representations for many image processing and computer vision algorithms. In recent years, natural image statistics have played an important role in the understanding of sensory neural behaviors of the human visual system <ref type="bibr" target="#b17">[18]</ref>. In the image processing literature, statistical prior models of natural images have been employed as fundamental ingredients in a large number of image coding and estimation algorithms (e.g., <ref type="bibr" target="#b18">[19]</ref>- <ref type="bibr" target="#b20">[21]</ref>). They have also been used for image quality assessment purposes (e.g., <ref type="bibr" target="#b7">[8]</ref>).</p><p>Fig. <ref type="figure" target="#fig_1">2</ref> shows the histograms of the coefficients computed from one of the wavelet subbands in a steerable pyramid decomposition <ref type="bibr" target="#b21">[22]</ref> (a type of redundant wavelet transform that avoids aliasing in subbands). It has been pointed out that the marginal distributions of such oriented bandpass filter responses of natural images are highly kurtotic [with sharp peaks at zero and much longer tails than Gaussian density, as demonstrated in Fig. <ref type="figure" target="#fig_1">2(a)</ref>] and have a number of important implications to sensory neural coding of natural visual scene <ref type="bibr" target="#b22">[23]</ref>. In <ref type="bibr" target="#b23">[24]</ref> and <ref type="bibr" target="#b24">[25]</ref>, it was demonstrated that many natural looking texture images can be synthesized by matching the histograms of the filter responses of a set of well-selected bandpass filters. Psychophysical visual sensitivity to histogram changes of wavelet-textures had also been studied (e.g., <ref type="bibr" target="#b25">[26]</ref>, <ref type="bibr" target="#b26">[27]</ref>). In Fig. <ref type="figure" target="#fig_1">2</ref>, it can be seen that the marginal distribution of the wavelet coefficients changes in different ways for different types of image distortions. Such histogram changes in images contaminated with white Gaussian noise have been observed previously and used for image denoising <ref type="bibr" target="#b18">[19]</ref>, <ref type="bibr" target="#b19">[20]</ref>.</p><p>Let and denote the probability density functions of the wavelet coefficients (assumed to be independently and identically distributed) in the same subband of two images, respectively. Let be a set of randomly and independently selected coefficients. The log-likelihoods of being drawn from and are and ( <ref type="formula">1</ref>)</p><p>respectively. Now, assume that is the true probability density distribution of the coefficients. Based on the law of large numbers, when is large, the difference of the log-likelihoods (or, equivalently, the log-likelihood-ratio) between and asymptotically approaches the Kullback-Leibler distance <ref type="bibr" target="#b27">[28]</ref> (KLD) between and</p><p>(2)</p><p>In previous work, a number of authors have pointed out the relationship between KLD and log-likelihood function and used KLD to compare images, mainly for classification and retrieval purposes <ref type="bibr" target="#b28">[29]</ref>- <ref type="bibr" target="#b31">[32]</ref>. KLD has also been used to quantify the distributions of image pixel intensity values for the evaluation of compressed image quality <ref type="bibr" target="#b32">[33]</ref>, <ref type="bibr" target="#b33">[34]</ref>. Here, we use KLD to quantify the difference between wavelet coefficient distributions of a perfect quality reference image and a distorted image [denoted later on as and , respectively)]. To make an effective estimation, the coefficient histograms for both images must be available. The latter can be easily computed from the received distorted image. The difficulty is in obtaining the coefficient histogram of the reference image at the receiver side. Transmitting all the histogram bins as hidden messages would result in either a heavy data load (when the bin step size is fine) or weaker statistical characterization (when the bin step size is coarse).</p><p>One important discovery in the literature of natural image statistics is that the marginal distribution of the coefficients in individual wavelet subbands can be well-fitted with a two-parameter generalized Gaussian density (GGD) model <ref type="bibr" target="#b34">[35]</ref> (3) where (for ) is the Gamma function. One fitting example is shown in Fig. <ref type="figure" target="#fig_1">2</ref>(a) as the dashed line. This model provides a very efficient means to summarize the coefficient histogram of the reference image, so that only two model parameters need to be transmitted to the receiver as hidden messages. This model has been explicitly used in previous work for image compression <ref type="bibr" target="#b20">[21]</ref> and texture image retrieval <ref type="bibr" target="#b31">[32]</ref>. In addition to the fitting parameters and , we also embed the prediction error as a third parameter, which is defined as the KLD between and</p><p>(4)</p><p>In practice, this quantity has to be evaluated numerically using histograms <ref type="bibr" target="#b4">(5)</ref> where and are the normalized heights of the th histogram bins, and is the number of bins in the histograms.</p><p>At the receiver side, we wish to compute an approximation to (2), the KLD between the probability distribution of the original image and that of the distorted image . Since we do not have the probability distribution of the original image, we replace the expectation over with an expectation over the model density</p><formula xml:id="formula_0">(6) (7)</formula><p>The second term is simply the KLD between the original probability distribution and the model (4), which is embedded in the image by the encoder. The first term is the KLD between and <ref type="bibr" target="#b7">(8)</ref> This is computed at the receiver side from the histogram bins of the distorted wavelet coefficients [analogous to <ref type="bibr" target="#b4">(5)</ref>]. Note that, unlike the encoding side, we avoid fitting with a GGD model, which may not be appropriate for the distorted data. Finally, the overall distortion between the distorted and reference images is defined as <ref type="bibr" target="#b8">(9)</ref> where is the number of subbands, and are the probability density functions of the th subbands in the reference and distorted images, respectively, is the estimation of the KLD between and , and is a constant used to control the scale of the distortion measure (but has no impact on the performance of the algorithm).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Feature Extraction</head><p>Fig. <ref type="figure" target="#fig_2">3</ref> illustrates our implementation of the feature extraction system at the encoder side. We first apply a three-scale four-orientation steerable pyramid transform <ref type="bibr" target="#b21">[22]</ref> to decompose the image into 12 oriented subbands (four for each scale) and the highpass and lowpass residuals, as demonstrated in Fig. <ref type="figure" target="#fig_3">4</ref>. For  each subband, the histogram of the coefficients is computed and then its feature parameters are estimated using a gradient descent algorithm to minimize the KLD between and . Six of the 12 oriented subbands (as shown in Fig. <ref type="figure" target="#fig_3">4</ref>) are selected for feature extraction. The major criterion for selecting these subbands is to reduce the data rate of RR features while at the same time, maintain the quality prediction performance. Specifically, in the Fourier domain, the adjacent steerable pyramid subbands (in both scale and orientation) have significant overlaps, but there is essentially no overlap between nonadjacent subbands. Therefore, the six subbands marked in Fig. <ref type="figure" target="#fig_3">4</ref> are selected to reduce the use of redundant information. Furthermore, in our tests, selecting the other six oriented subbands or all the 12 oriented subbands gives similar overall performance of image quality prediction.</p><p>The extracted scalar features are quantized to finite precision. Both and are quantized into 8-bit precision, and is represented using 11-bit floating point, with 8 bits for mantissa and 3 bits for exponent. These quantization precision parameters were hand picked to represent the features in a limited number of bits while maintaining a reasonable approximation of the features. The final result is a total of bits that are embedded into the image.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Information Embedding</head><p>To embed the extracted features into the image, we choose to use an existing dithered uniform scalar quantization watermarking method in the wavelet transform domain. This method is a simple case of the class of quantization-index-modulation information embedding techniques <ref type="bibr" target="#b35">[36]</ref>, which allow for "blind" decoding (decoding does not require the access to the reference image) and achieve a good tradeoff between data-hiding rate and robustness. The information embedding system is illustrated in Fig. <ref type="figure" target="#fig_4">5</ref>.</p><p>We first use a five-scale separable wavelet transform (specifically, a quadrature mirror filter transform <ref type="bibr" target="#b36">[37]</ref>) to decompose the reference image into 16 subbands, including the horizontal, vertical and diagonal subbands at each scale, and a low frequency residual band. In order to embed one bit of information into a wavelet coefficient , the coefficient is altered according to the following rule: <ref type="bibr" target="#b9">(10)</ref> where is the altered coefficient, is a base quantization operator with quantization step size , and is a dithering operator defined as if if <ref type="bibr" target="#b10">(11)</ref> At the receiver side, a distorted coefficient is obtained and used to estimate the embedded bit based on the minimum distance criterion <ref type="bibr" target="#b11">(12)</ref> We embed the hidden messages into the horizontal, vertical and diagonal subbands at the fifth scale (counted from fine to coarse) of the wavelet decomposition. We choose to use these low-frequency components because they usually have high signal energy and are less likely to be significantly altered during typical image processing operations. Moreover, such a selection avoids conflict with the proposed RR quality assessment method, which is based on detecting the statistical changes of the wavelet coefficients at the finer scales. To further improve robustness, two error detection/correction techniques are employed. First, a 16-bit cyclic redundancy check (CRC) code <ref type="bibr" target="#b37">[38]</ref> is computed and attached to the 162 information bits. Second, the resulting 178 bits are further encoded using a binary <ref type="bibr" target="#b14">(15,</ref><ref type="bibr" target="#b4">5,</ref><ref type="bibr" target="#b6">7)</ref> BCH code <ref type="bibr" target="#b37">[38]</ref>, which can correct up to 3 bits of errors out of every 15 bits. As a result, a total of 540 bits are generated. The same number of wavelet coefficients are randomly selected from the fifth scale of the wavelet transform, and every bit is encoded into one coefficient using <ref type="bibr" target="#b9">(10)</ref>. The positions of the coefficients are shared between the sender and receiver as the embedding key.</p><p>At the receiver side, we first apply the same wavelet transform to the received image. The embedded 540 bits are then extracted from the corresponding wavelet coefficients using ( <ref type="formula">12</ref>), and decoded with the BCH system. The decoded 178 bits are split into the corresponding 162 information bits and 16 CRC bits. We then calculate a new set of CRC bits using the decoded information bits and compare them with the decoded CRC bits. If any of the CRC bit is incorrect, the system reports a failure message. Otherwise, the extracted 162 information bits are converted back into scalar features about the reference image and relayed to the quality assessment system. Finally, a quality score of the distorted image is reported.</p><p>In several cases, a failure message may be reported. It could be that the received image is not a quality-aware image (no side information has been embedded) or the embedded information is desynchronized (e.g., by image editing). It could also be that the image quality degradation is very severe, such that the embedded information cannot be completely recovered. It is often useful to distinguish between the two cases, because in the latter case, a failure message can serve as an indication of low image quality. One way to make such a distinction is to look at the percentage of correct CRC bits because statistically only in the latter case, may be significantly higher than 50%. Following the general idea of <ref type="bibr" target="#b10">[11]</ref>- <ref type="bibr" target="#b12">[13]</ref>, one can take an even further step to use as an important factor for the prediction of image quality at very low quality range, although the accuracy may be complicated by the nature (e.g. complexity) of the images being evaluated.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>IV. TEST</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Performance of Quality Assessment</head><p>In order to evaluate and compare the performance of image quality assessment algorithms, we built a large image database (the LIVE image database, available online <ref type="bibr" target="#b38">[39]</ref>) and conducted an extensive subjective experiment to assess the quality of the images in the database. The database contains 29 high-resolution (typically 768 512) original images altered with five types of distortions at different distortion levels. The distorted images were divided into seven datasets. Datasets 1 (87 images) and 2 (82 images) are JPEG2000 compressed images; Datasets 3 (87 images) and 4 (88 images) are JPEG compressed images; and Datasets 5-7 (each containing 145 images) are distorted with white Gaussian noise, Gaussian blur, and transmission errors in the JPEG2000 bitstream using a fast-fading Rayleigh channel model, respectively. Subjects were asked to provide their perception of quality on a continuous linear scale and each image was rated by 20-25 subjects. The raw scores for each subject were converted into Z-scores and rescaled within each dataset to fill the range from 1 to 100. Mean opinion score and the standard deviation between subjective scores were then computed for each image.</p><p>Three measures are computed to quantify the performance of the proposed quality assessment method. First, following the procedure given in the Video Quality Experts Group (VQEG) Phase I FR-TV test <ref type="bibr" target="#b40">[42]</ref>, we use a logistic function to provide a nonlinear mapping between the objective and subjective scores <ref type="bibr" target="#b12">(13)</ref> where is the objective score and , , , and are the model parameters, which are found numerically using a nonlinear regression process with MATLAB optimization toolbox. After the nonlinear mapping, the correlation coefficient between the predicted and true subjective scores is calculated to evaluate prediction accuracy. Second, the Spearman rank-order correlation coefficient is employed to evaluate prediction monotonicity. Finally, to evaluate prediction consistency, the outlier ratio is used, which is defined as the percentage of predictions outside the range of 2 standard deviations between subjective scores.</p><p>To the best of our knowledge, no other RR method has been proposed that: 1) aims for general-purpose image quality assessment (as opposed to distortion-or application-specific) and 2) uses such small amount of information about the reference image as compared to the proposed method. Therefore, we compare the proposed method with a set of general-purpose FR models as well as application-specific NR models. These models include PSNR (FR), Lubin's Sarnoff model (FR) [40], <ref type="bibr" target="#b41">[43]</ref>, <ref type="bibr" target="#b42">[44]</ref>, the mean structural similarity index (MSSIM, FR) <ref type="bibr" target="#b39">[41]</ref>, the JPEG quality index by Wang et al. (NR) <ref type="bibr" target="#b6">[7]</ref>, and the JPEG2000 quality assessment method by Sheikh et al. (NR) <ref type="bibr" target="#b7">[8]</ref>.</p><p>Although such comparison is unfair to one method or another in different aspects, it provides a useful indication about the relative performance of the proposed method. The performance evaluation results of all methods are summarized in Table <ref type="table" target="#tab_0">I</ref>. It can be seen that the proposed method performs quite well for a wide range of distortion types. Specifically, for five of the seven datasets, it gives better prediction accuracy (higher correlation coefficients), better prediction monotonicity (higher Spearman rank-order correlation coefficients) and better prediction consistency (lower outlier ratios) than PSNR, which is the most widely used FR image quality metric in the image processing literature. In comparison with the NR models, the proposed method is inferior to Wang et al.'s method for the JPEG datasets (JPEG compressed images have distinct blocking effect, which is readily detected by an application-specific NR method), and performs better than Sheikh et al.'s method for the JPEG2000 datasets. Note that these application-specific NR methods are not applicable to other types of image distortions. A more complete test may include other distortion types (including mixed distortions) as well as validations across different distortion types, but the current testing results lead us to believe that the proposed method is a reasonable and useful choice for quality-aware image systems. It needs to be emphasized that none of the other methods being compared, or any other method we are aware of, can be used in this scenario.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Robustness of Information Embedding</head><p>The information embedding system is tested with four distortion types: JPEG2000 compression, JPEG compression, white Gaussian noise contamination, and Gaussian blur. For convenience, we define the distortion levels as compression bit rate (bits/pixel) for JPEG2000 compression, quality factor (which controls the quantization step of discrete cosine transform coefficients) for JPEG compression, noise standard deviation for white noise contamination, and standard deviation of blurring filter for Gaussian blur, respectively. The same 29 original images in the LIVE database <ref type="bibr" target="#b38">[39]</ref> are used for the test. We first generate ten quality-aware images (each uses a different randomly generated embedding key) for each of the test images. For any given distortion type and level, we distort the 290 quality-aware images accordingly and check if the hidden messages can be correctly decoded (by comparing the CRC bits, see Section III-B).</p><p>Since the RR quality assessment system can provide useful quality prediction only when the hidden messages are fully recovered, we use correct decoding rate (defined as the percentage of the images whose embedded messages are completely recovered) as the criterion for evaluating the robustness of the system. The test results are shown in Fig. <ref type="figure" target="#fig_5">6</ref>, which covers the transition range (from %0 to 100% correct decoding rate) of distortion levels for each distortion type.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>V. CONCLUSION</head><p>The major contributions of the paper include the following. 1) introduction of the concept of quality-aware image, and discussion of its design considerations; 2) implementation of a practical quality-aware image encoding, decoding and quality analysis system; 3) development of a simple and effective RR image quality assessment algorithm based on a wavelet-domain statistical model of natural images; 4) expansion of the application scope of information hiding technologies. Like other FR and RR approaches, the proposed quality assessment method assumes the existence of a perfect-quality reference image. In the case that this assumption does not hold, only an NR method can provide useful quality evaluation of the images.</p><p>In the future, the research initiated in this paper can be extended in several directions: The algorithm presented in Section III is only a specific implementation of the general framework of quality-aware image system (Fig. <ref type="figure" target="#fig_0">1</ref>). The current method can be improved in many ways. For example, different RR image quality assessment algorithms could be employed. The improved algorithms may include more statistical image features (e.g., joint statistics of wavelet coefficients), which may lead to better quality prediction accuracy. For another example, different information hiding techniques could be used to enhance the robustness to a broader range of distortion types. The current method is sensitive to geometric transformations, gain attack and perhaps some other types of malicious attacks. The general concept of quality-aware images does not exclude itself from being employed in security-related applications. For example, in a pay-per-view scenario, an image could be paid according to its quality degradation. However, given the limited capability of the existing robust image watermarking techniques (including the one we are currently using), we propose to use it mainly for nonsecurity oriented applications, in which nobody will benefit from "removing" or "destroying" the embedded information, and therefore, the images are less likely to encounter malicious attacks (though the precise definition of malicious attacks could vary for different application environment). This is different from security-related applications such as copyright protection, where robustness to malicious attacks <ref type="bibr" target="#b43">[45]</ref> is an essential issue.</p><p>The general approach may also be used beyond the scope of image quality assessment. For example, suppose an image is subject to a number of distortion stages. One can embed the quality scores measured at the intermediate stages into the image as additional hidden messages. The end receiver can then trace back to find the critical processing stages that have caused significant quality degradations. Inspired by the work of using data hiding techniques for error concealment (e.g., <ref type="bibr" target="#b44">[46]</ref> and <ref type="bibr" target="#b45">[47]</ref>), we can have another interesting application of the embedded features, which we refer to as "self-repairing images." The idea is to "repair" a distorted image by forcing some of its statistical properties to match those of the original image. Similar idea has been successfully used for texture synthesis (e.g., <ref type="bibr" target="#b23">[24]</ref>, <ref type="bibr" target="#b24">[25]</ref>, and <ref type="bibr" target="#b46">[48]</ref>). Finally, the principle idea may be applied to other types of signals to create quality-aware (and possibly self-repairing) video, audio, and multimedia, etc.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 1 .</head><label>1</label><figDesc>Fig. 1. Quality-aware image encoding, decoding, and quality analysis system.</figDesc><graphic coords="2,303.60,65.50,246.00,200.00" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 2 .</head><label>2</label><figDesc>Fig. 2. Comparisons of wavelet coefficient histograms (solid curves) calculated from the same horizontal subband in the steerable pyramid decomposition [22]. (a) Original (reference) "buildings" image (cropped for visibility). (b) JPEG2000 compressed image. (c) White Gaussian noise contaminated image. (d) Gaussian blurred image. The histogram of the original image coefficients is well fitted by a generalized Gaussian density model (dashed curves).</figDesc><graphic coords="3,54.48,65.44,484.00,221.00" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 3 .</head><label>3</label><figDesc>Fig. 3. Feature extraction system at the encoder side.</figDesc><graphic coords="5,69.00,65.80,455.00,110.00" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Fig. 4 .</head><label>4</label><figDesc>Fig. 4. Steerable pyramid decomposition [22] of image (highpass residual band not shown). A set of selected subbands (marked with dashed boxes) are used for GGD feature extraction.</figDesc><graphic coords="5,51.06,211.12,228.00,317.00" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Fig. 5 .</head><label>5</label><figDesc>Fig. 5. Information embedding system.</figDesc><graphic coords="6,46.62,65.34,234.00,282.00" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Fig. 6 .</head><label>6</label><figDesc>Fig. 6. Robustness test of the information embedding system. The quality/distortion level is defined as (a) bit rate (bits/pixel) for JPEG2000 compressed images; (b) quality factor for JPEG compressed images; (c) noise standard deviation for white noise contaminated images; and (d) standard deviation (pixels) of blurring filter for Gaussian blurred images.</figDesc><graphic coords="8,43.14,66.14,504.00,127.00" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0"><head></head><label></label><figDesc></figDesc><graphic coords="7,127.14,102.82,339.00,218.00" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>TABLE I PERFORMANCE</head><label>I</label><figDesc>EVALUATION OF IMAGE QUALITY MEASURES USING THE LIVE DATABASE [39]. JP2: JPEG2000 DATASET; JPG: JPEG DATASET; NOISE: WHITE GAUSSIAN NOISE DATASET; BLUR: GAUSSIAN BLUR DATASET; ERROR: TRANSMISSION ERROR DATASET</figDesc><table /></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>ACKNOWLEDGMENT</head><p>The authors would like to thank Dr. J. Portilla, Dr. H. Farid, and the anonymous reviewers for valuable comments.</p></div>
			</div>


			<div type="funding">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>The work of Z. Wang and E. P. Simoncelli was supported in part by the Howard Hughes Medical Institute. The work of G. Wu and E.-H. Yang was supported in part by the National Sciences and Engineering Research Council of Canada, in part by the Premier's Research Excellence Award, Canada Foundation for Innovation, in part by the Ontario Distinguished Research Award, and in part by the Canada Research Chairs Program. The work of H. R. Sheikh and A. C. Bovik was supported in part by the National Science Foundation.</p></div>
			</div>

			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>He is currently affiliated with Texas Instrument, Inc., Dallas, TX. His research interests include fullreference and no-reference quality assessment, application of natural scene statistics models and human visual system models for solving image and video processing problems, and image and video codecs and their embedded implementation. </p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Perceptual criteria for image quality evaluation</title>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">N</forename><surname>Pappas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">J</forename><surname>Safranek</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Handbook of Image and Video Processing</title>
		<editor>
			<persName><forename type="first">A</forename><surname>Bovik</surname></persName>
		</editor>
		<meeting><address><addrLine>New York</addrLine></address></meeting>
		<imprint>
			<publisher>Academic</publisher>
			<date type="published" when="2000">2000</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">The Video Quality Experts Group</title>
		<author>
			<persName><surname>Vqeg</surname></persName>
		</author>
		<ptr target="http://www.vqeg.org/" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Objective video quality assessment</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">R</forename><surname>Sheikh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">C</forename><surname>Bovik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The Handbook of Video Databases: Design and Applications</title>
		<editor>
			<persName><forename type="first">B</forename><surname>Furht</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">O</forename><surname>Marqueseds</surname></persName>
		</editor>
		<meeting><address><addrLine>Boca Raton, FL</addrLine></address></meeting>
		<imprint>
			<publisher>CRC</publisher>
			<date type="published" when="2003">2003</date>
			<biblScope unit="page" from="1041" to="1078" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">A generalized block-edge impairment metric for video coding</title>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">R</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Yuen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Signal Process. Lett</title>
		<imprint>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="317" to="320" />
			<date type="published" when="1997-11">Nov. 1997</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Blind measurement of blocking artifacts in images</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">C</forename><surname>Bovik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">L</forename><surname>Evans</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Int. Conf. Image Process</title>
		<meeting>IEEE Int. Conf. Image ess</meeting>
		<imprint>
			<date type="published" when="2000-09">Sep. 2000</date>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page" from="981" to="984" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Vision-model-based impairment metric to evaluate blocking artifact in digital video</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">R</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Winkler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE</title>
		<meeting>IEEE</meeting>
		<imprint>
			<date type="published" when="2002-01">Jan. 2002</date>
			<biblScope unit="volume">90</biblScope>
			<biblScope unit="page" from="154" to="169" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">No-reference perceptual quality assessment of JPEG compressed images</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">R</forename><surname>Sheikh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">C</forename><surname>Bovik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Int. Conf. Image Processing</title>
		<meeting>IEEE Int. Conf. Image essing<address><addrLine>Rochester, NY</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2002-09">Sep. 2002</date>
			<biblScope unit="page" from="477" to="480" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">No-reference quality assessment using natural scene statistics: JPEG2000</title>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">R</forename><surname>Sheikh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">C</forename><surname>Bovik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Cormack</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Image Process</title>
		<imprint>
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="1918" to="1927" />
			<date type="published" when="2005-11">Nov. 2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Perceptual blur and ringing metrics: application to JPEG2000</title>
		<author>
			<persName><forename type="first">P</forename><surname>Marziliano</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Dufaux</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Winkler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Ebrahimi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Signal Process. Image Commun</title>
		<imprint>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="page" from="163" to="172" />
			<date type="published" when="2004-02">Feb. 2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Blind image quality assessment</title>
		<author>
			<persName><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Int. Conf. Image Process</title>
		<meeting>IEEE Int. Conf. Image ess</meeting>
		<imprint>
			<date type="published" when="2002-09">Sep. 2002</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="449" to="452" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Objective measurement scheme for perceived picture quality degradation caused by MPEG encoding without any reference pictures</title>
		<author>
			<persName><forename type="first">O</forename><surname>Sugimoto</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Kawada</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Wada</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Matsumoto</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. SPIE</title>
		<meeting>SPIE</meeting>
		<imprint>
			<date type="published" when="2001">2001</date>
			<biblScope unit="volume">4310</biblScope>
			<biblScope unit="page" from="932" to="939" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">A comparison between an objective quality measure and the mean annoyance values of watermarked videos</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">C Q</forename><surname>Farias</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">K</forename><surname>Mitra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Carli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Neri</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Int. Conf. Image Processing</title>
		<meeting>IEEE Int. Conf. Image essing<address><addrLine>Rochester, NY</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2002-09">Sep. 2002</date>
			<biblScope unit="page" from="469" to="472" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Blind quality assessment system for multimedia communications using tracing watermarking</title>
		<author>
			<persName><forename type="first">P</forename><surname>Campisi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Carli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Giunta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Neri</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Signal Process</title>
		<imprint>
			<biblScope unit="volume">51</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="996" to="1002" />
			<date type="published" when="2003-04">Apr. 2003</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">What is the future for watermarking? (Part I)</title>
		<author>
			<persName><forename type="first">M</forename><surname>Barni</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Signal Process. Mag</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="issue">9</biblScope>
			<biblScope unit="page" from="55" to="59" />
			<date type="published" when="2003-09">Sep. 2003</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">What is the future for watermarking? (Part II)</title>
	</analytic>
	<monogr>
		<title level="j">IEEE Signal Process. Mag</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="53" to="57" />
			<date type="published" when="2003-11">Nov. 2003</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Attacks on steganographic systems</title>
		<author>
			<persName><forename type="first">A</forename><surname>Westfeld</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Pfitzmann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. 3rd Int. Workshop Inf. Hiding, Dresden</title>
		<meeting>3rd Int. Workshop Inf. Hiding, Dresden</meeting>
		<imprint>
			<date type="published" when="1999">Germany, 1999</date>
			<biblScope unit="page" from="61" to="75" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Detecting hidden messages using higher-order statistics and support vector machines</title>
		<author>
			<persName><forename type="first">S</forename><surname>Lyu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Farid</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. 5th Int. Workshop Inf. Hiding</title>
		<meeting>5th Int. Workshop Inf. Hiding<address><addrLine>Noordwijkerhout, The Netherlands</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2002">2002</date>
			<biblScope unit="page" from="340" to="354" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Natural image statistics and neural representation</title>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">P</forename><surname>Simoncelli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Olshausen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Ann. Rev. Neurosci</title>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="page" from="1193" to="1216" />
			<date type="published" when="2001-05">May 2001</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Noise removal via Bayesian wavelet coring</title>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">P</forename><surname>Simoncelli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">H</forename><surname>Adelson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. 3rd Int. Conf. Image Processing</title>
		<meeting>3rd Int. Conf. Image essing</meeting>
		<imprint>
			<date type="published" when="1996-09">Sep. 1996</date>
			<biblScope unit="volume">I</biblScope>
			<biblScope unit="page" from="379" to="382" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Analysis of multiresolution image denoising schemes using a generalized Gaussian and complexity priors</title>
		<author>
			<persName><forename type="first">P</forename><surname>Moulin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Inf. Theory</title>
		<imprint>
			<biblScope unit="volume">45</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="909" to="919" />
			<date type="published" when="1999-04">Apr. 1999</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Image compression via joint statistical characterization in the wavelet domain</title>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">W</forename><surname>Buccigrossi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">P</forename><surname>Simoncelli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Image Process</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page" from="1688" to="1701" />
			<date type="published" when="1999-12">Dec. 1999</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Shiftable multi-scale transforms</title>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">P</forename><surname>Simoncelli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">T</forename><surname>Freeman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">H</forename><surname>Adelson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">J</forename><surname>Heeger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Inf. Theory</title>
		<imprint>
			<biblScope unit="volume">38</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="587" to="607" />
			<date type="published" when="1992-03">Mar. 1992</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">What is the goal of sensory coding?</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">J</forename><surname>Field</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural Comput</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="559" to="601" />
			<date type="published" when="1994">1994</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Pyramid-based texture analysis/synthesis</title>
		<author>
			<persName><forename type="first">D</forename><surname>Heeger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Bergen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ACM SIGGRAPH</title>
		<meeting>ACM SIGGRAPH</meeting>
		<imprint>
			<date type="published" when="1995-08">Aug. 1995</date>
			<biblScope unit="page" from="229" to="238" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">FRAME: Filters, random fields and maximum entropy-toward a unified theory for texture modeling</title>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">C</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><forename type="middle">N</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Mumford</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Int. J. Comp. Vis</title>
		<imprint>
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="1" to="20" />
			<date type="published" when="1998">1998</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Histogram contrast analysis and the visual segregation of iid textures</title>
		<author>
			<persName><forename type="first">C</forename><surname>Chubb</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Econopouly</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">S</forename><surname>Landy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Opt. Soc. Amer. A</title>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="issue">9</biblScope>
			<biblScope unit="page" from="2350" to="2374" />
			<date type="published" when="1994">1994</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Sensitivity to contrast histogram differences in synthetic wavelet-textures</title>
		<author>
			<persName><forename type="first">F</forename><forename type="middle">A A</forename><surname>Kingdom</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Hayes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">J</forename><surname>Field</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Vis. Res</title>
		<imprint>
			<biblScope unit="volume">41</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="585" to="598" />
			<date type="published" when="2001">2001</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<title level="m" type="main">Elements of Information Theory</title>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">M</forename><surname>Cover</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">A</forename><surname>Thomas</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1991">1991</date>
			<publisher>Wiley</publisher>
			<pubPlace>New York</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Multiscale branch and bound image database search</title>
		<author>
			<persName><forename type="first">J.-Y</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">A</forename><surname>Bouman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">P</forename><surname>Allebach</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. SPIE/IS&amp;T Conf. Storage Retrieval Image Video Databases V</title>
		<meeting>SPIE/IS&amp;T Conf. Storage Retrieval Image Video Databases V</meeting>
		<imprint>
			<date type="published" when="1997-02">Feb. 1997</date>
			<biblScope unit="volume">3022</biblScope>
			<biblScope unit="page" from="133" to="144" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Texture recognition using a non parametric multi-scale statistical model</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">De</forename><surname>Bonet</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Viola</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conf. Comput. Vis</title>
		<meeting>IEEE Conf. Comput. Vis</meeting>
		<imprint>
			<date type="published" when="1998-06">Jun. 1998</date>
			<biblScope unit="page" from="641" to="647" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">A probabilistic architecture for content-based image retrieval</title>
		<author>
			<persName><forename type="first">N</forename><surname>Vasconcelos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Lippman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conf. Comput. Vis</title>
		<meeting>IEEE Conf. Comput. Vis</meeting>
		<imprint>
			<date type="published" when="2000-06">Jun. 2000</date>
			<biblScope unit="page" from="216" to="221" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Wavelet-based texture retrieval using generalized gaussian density and Kullback-Leibler distance</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">N</forename><surname>Do</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Vetterli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Image Process</title>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="146" to="158" />
			<date type="published" when="2002-02">Feb. 2002</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Statistical evaluation of image quality measures</title>
		<author>
			<persName><forename type="first">İ</forename><surname>Avcibas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">¸</forename></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Sankur</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Sayood</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Electron. Imag</title>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="page" from="206" to="223" />
			<date type="published" when="2002-04">Apr. 2002</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Performance of the Kullback-Leibler information gain for predicting image fidelity</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">A</forename><surname>Garcia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Fdez-Valdivia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Rodriguez-Sánchez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><forename type="middle">R</forename><surname>Fdez</surname></persName>
		</author>
		<author>
			<persName><surname>Vidal</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Int. Conf. Pattern Recognition</title>
		<meeting>IEEE Int. Conf. Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2002">2002</date>
			<biblScope unit="volume">III</biblScope>
			<biblScope unit="page" from="843" to="848" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Multifrequency channel decomposition of images and wavelet models</title>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">G</forename><surname>Mallat</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Acoust., Speech, Signal Process</title>
		<imprint>
			<biblScope unit="volume">37</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="2091" to="2110" />
			<date type="published" when="1989-12">Dec. 1989</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Quantization index modulation: a class of provably good methods for digital watermarking and information embedding</title>
		<author>
			<persName><forename type="first">B</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Wornell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Inf. Theory</title>
		<imprint>
			<biblScope unit="volume">47</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="1423" to="1443" />
			<date type="published" when="2001-05">May 2001</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Subband transforms</title>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">P</forename><surname>Simoncelli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">H</forename><surname>Adelson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Subband Image Coding</title>
		<editor>
			<persName><forename type="first">J</forename><forename type="middle">W</forename><surname>Woods</surname></persName>
		</editor>
		<meeting><address><addrLine>Norwell, MA</addrLine></address></meeting>
		<imprint>
			<publisher>Kluwer</publisher>
			<date type="published" when="1990">1990</date>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page" from="143" to="192" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<monogr>
		<title level="m" type="main">Source and Channel Coding: An Algorithmic Approach</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">B</forename><surname>Anderson</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1991">1991</date>
			<publisher>Kluwer</publisher>
			<pubPlace>Norwell, MA</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<monogr>
		<title level="m" type="main">Image and Video Quality Assessment Research at LIVE</title>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">R</forename><surname>Sheikh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">C</forename><surname>Bovik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">K</forename><surname>Cormack</surname></persName>
		</author>
		<ptr target="http://live.ece.utexas.edu/research/quality/" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Image quality assessment: from error visibility to structural similarity</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">C</forename><surname>Bovik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">R</forename><surname>Sheikh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">P</forename><surname>Simoncelli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Image Process</title>
		<imprint>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="600" to="612" />
			<date type="published" when="2004-04">Apr. 2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<monogr>
		<title level="m" type="main">Final report from the video quality experts group on the validation of objective models of video quality assessment</title>
		<author>
			<persName><surname>Vqeg</surname></persName>
		</author>
		<ptr target="http://www.vqeg.org/" />
		<imprint>
			<date type="published" when="2000-04">2000. Apr</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">The use of psychophysical data and models in the analysis of display system performance</title>
		<author>
			<persName><forename type="first">J</forename><surname>Lubin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Digital Images and Human Vision</title>
		<editor>
			<persName><forename type="first">B</forename><surname>Watson</surname></persName>
		</editor>
		<meeting><address><addrLine>Cambridge, MA</addrLine></address></meeting>
		<imprint>
			<publisher>MIT Press</publisher>
			<date type="published" when="1993">1993</date>
			<biblScope unit="page" from="163" to="178" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">A visual discrimination mode for image system design and evaluation</title>
	</analytic>
	<monogr>
		<title level="m">Visual Models for Target Detection and Recognition</title>
		<editor>
			<persName><forename type="first">E</forename><surname>Peli</surname></persName>
		</editor>
		<meeting><address><addrLine>Singapore</addrLine></address></meeting>
		<imprint>
			<publisher>World Scientific</publisher>
			<date type="published" when="1995">1995</date>
			<biblScope unit="page" from="207" to="220" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Watermarking schemes evaluation</title>
		<author>
			<persName><forename type="first">F</forename><forename type="middle">A P</forename><surname>Petitcolas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Signal Process. Mag</title>
		<imprint>
			<biblScope unit="volume">17</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="58" to="64" />
			<date type="published" when="2000-09">Sep. 2000</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Error concealment of digital images using data hiding</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">presented at the 9th DSP Workshop</title>
		<meeting><address><addrLine>Hunt, TX</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2000-10">Oct. 2000</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Error concealment using data hiding</title>
		<author>
			<persName><forename type="first">P</forename><surname>Yin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">H</forename><surname>Yu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Int. Conf. Acoustics, Speech, Signal Processing</title>
		<meeting>IEEE Int. Conf. Acoustics, Speech, Signal essing<address><addrLine>Salt Lake City, UT</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2001-05">May 2001</date>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page" from="1453" to="1456" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">A parametric texture model based on joint statistics of complex wavelet coefficients</title>
		<author>
			<persName><forename type="first">J</forename><surname>Portilla</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">P</forename><surname>Simoncelli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Int. J. Comput. Vis</title>
		<imprint>
			<biblScope unit="volume">40</biblScope>
			<biblScope unit="page" from="49" to="71" />
			<date type="published" when="2000-12">Dec. 2000</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
