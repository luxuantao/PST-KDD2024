<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Change Detection Based on Artificial Intelligence: State-of-the-Art and Challenges</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date type="published" when="2020-05-25">25 May 2020</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Wenzhong</forename><surname>Shi</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Land Surveying and Geo-Informatics</orgName>
								<orgName type="institution">The Hong Kong Polytechnic University</orgName>
								<address>
									<addrLine>Hung Hom</addrLine>
									<settlement>Hong Kong</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Min</forename><surname>Zhang</surname></persName>
							<email>rzhang@cumt.edu.cn</email>
							<idno type="ORCID">0000-0003-1643-5271</idno>
							<affiliation key="aff0">
								<orgName type="department">Department of Land Surveying and Geo-Informatics</orgName>
								<orgName type="institution">The Hong Kong Polytechnic University</orgName>
								<address>
									<addrLine>Hung Hom</addrLine>
									<settlement>Hong Kong</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="department">School of Remote Sensing and Information Engineering</orgName>
								<orgName type="institution">Wuhan University</orgName>
								<address>
									<postCode>430079</postCode>
									<settlement>Wuhan</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Rui</forename><surname>Zhang</surname></persName>
							<idno type="ORCID">0000-0003-1643-5271</idno>
							<affiliation key="aff0">
								<orgName type="department">Department of Land Surveying and Geo-Informatics</orgName>
								<orgName type="institution">The Hong Kong Polytechnic University</orgName>
								<address>
									<addrLine>Hung Hom</addrLine>
									<settlement>Hong Kong</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
							<affiliation key="aff2">
								<orgName type="department">School of Environment Science and Spatial Informatics</orgName>
								<orgName type="institution">China University of Mining and Technology</orgName>
								<address>
									<postCode>221116</postCode>
									<settlement>Xuzhou</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Shanxiong</forename><surname>Chen</surname></persName>
							<email>shanxiongchen@whu.edu.cn</email>
							<idno type="ORCID">0000-0002-9235-6340</idno>
							<affiliation key="aff0">
								<orgName type="department">Department of Land Surveying and Geo-Informatics</orgName>
								<orgName type="institution">The Hong Kong Polytechnic University</orgName>
								<address>
									<addrLine>Hung Hom</addrLine>
									<settlement>Hong Kong</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="department">School of Remote Sensing and Information Engineering</orgName>
								<orgName type="institution">Wuhan University</orgName>
								<address>
									<postCode>430079</postCode>
									<settlement>Wuhan</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Zhao</forename><surname>Zhan</surname></persName>
							<email>zhanzhao@whu.edu.cn</email>
							<affiliation key="aff0">
								<orgName type="department">Department of Land Surveying and Geo-Informatics</orgName>
								<orgName type="institution">The Hong Kong Polytechnic University</orgName>
								<address>
									<addrLine>Hung Hom</addrLine>
									<settlement>Hong Kong</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="department">School of Remote Sensing and Information Engineering</orgName>
								<orgName type="institution">Wuhan University</orgName>
								<address>
									<postCode>430079</postCode>
									<settlement>Wuhan</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Change Detection Based on Artificial Intelligence: State-of-the-Art and Challenges</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2020-05-25">25 May 2020</date>
						</imprint>
					</monogr>
					<idno type="MD5">13337C796A44F3CB1B1C42CFC9CFE083</idno>
					<idno type="DOI">10.3390/rs12101688</idno>
					<note type="submission">Received: 13 April 2020; Accepted: 20 May 2020;</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.3" ident="GROBID" when="2023-07-27T08:21+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>artificial intelligence</term>
					<term>change detection</term>
					<term>remote sensing</term>
					<term>deep learning</term>
					<term>neural network</term>
					<term>unsupervised learning</term>
					<term>SAR</term>
					<term>hyperspectral</term>
					<term>multispectral</term>
					<term>street view</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Change detection based on remote sensing (RS) data is an important method of detecting changes on the Earth's surface and has a wide range of applications in urban planning, environmental monitoring, agriculture investigation, disaster assessment, and map revision. In recent years, integrated artificial intelligence (AI) technology has become a research focus in developing new change detection methods. Although some researchers claim that AI-based change detection approaches outperform traditional change detection approaches, it is not immediately obvious how and to what extent AI can improve the performance of change detection. This review focuses on the state-of-the-art methods, applications, and challenges of AI for change detection. Specifically, the implementation process of AI-based change detection is first introduced. Then, the data from different sensors used for change detection, including optical RS data, synthetic aperture radar (SAR) data, street view images, and combined heterogeneous data, are presented, and the available open datasets are also listed. The general frameworks of AI-based change detection methods are reviewed and analyzed systematically, and the unsupervised schemes used in AI-based change detection are further analyzed. Subsequently, the commonly used networks in AI for change detection are described. From a practical point of view, the application domains of AI-based change detection methods are classified based on their applicability. Finally, the major challenges and prospects of AI for change detection are discussed and delineated, including (a) heterogeneous big data processing, (b) unsupervised AI, and (c) the reliability of AI. This review will be beneficial for researchers in understanding this field.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Change detection is the process of identifying differences in the state of an object or phenomenon by observing it at different times <ref type="bibr" target="#b0">[1]</ref>. It is one of the major problems in earth observation and has been extensively researched in recent decades. Multi-temporal RS data, such as satellite imagery and aerial imagery, can provide abundant information to identify land use and land cover (LULC) differences in a specific area across a period of time. This is very crucial in various applications, such as urban planning, environmental monitoring, agriculture investigation, disaster assessment, and map revision.</p><p>With the ongoing development of Earth observation techniques, huge amounts of RS data with a high spectral-spatial-temporal resolution are now available, which brings new requirements of Existing change detection reviews have focused mainly on the design of change detection techniques in multi-temporal hyperspectral images (HSIs) and high-spatial-resolution images <ref type="bibr" target="#b0">[1]</ref><ref type="bibr" target="#b1">[2]</ref><ref type="bibr">[3]</ref><ref type="bibr" target="#b3">[4]</ref>. The techniques they reviewed are mainly traditional change detection approaches, which can be generally summarized into the following groups:  Visual analysis: the change map is obtained by manual interpretation, which can provide highly reliable results based on expert knowledge but is time-consuming and labor-intensive;  aAlgebra-based methods: the change map is obtained by performing algebraic operation or transformation on multi-temporal data, such as image differencing, image regression, image ratioing, and change vector analysis (CVA);  Transformation: data reduction methods, such as principle component analysis (PCA), Tasseled Cap (KT), multivariate alteration detection (MAD), Gramm-Schmidt (GS), and Chi-Square, are used to suppress correlated information and highlight variance in multi-temporal data;  Classification-based methods: changes are identified by comparing multiple classification maps (i.e., post-classification comparison), or using a trained classifier to directly classify data from multiple periods (i.e., multidate classification or direct classification);  Advanced models: advanced models, such as the Li-Strahler reflectance model, the spectral mixture model, and the biophysical parameter method, are used to convert the spectral reflectance values of multi-period data into physically based parameters or fractions to perform change analysis, and this is more intuitive and has physical meaning, but it is complicated and time-consuming;  Others: hybrid approaches and others, such as knowledge-based, spatial-statistics-based, and integrated GIS and RS methods, are used.</p><p>According to the detection unit, these methods can also be classified based on pixel-level, feature-level, object-level, and three-dimensions (3D) object-level, and have been systematically reviewed in the literature <ref type="bibr" target="#b4">[5]</ref><ref type="bibr" target="#b5">[6]</ref><ref type="bibr" target="#b6">[7]</ref>. Due to the rapid development of computer technology, the research of traditional change detection approaches has turned to integrating AI techniques. In both traditional change detection flow and AI-based flow, the first step is data acquisition and the aim of change detection is to obtain the change detection map for various applications; after preparing the data, traditional approaches typically consist of two steps, including a homogenization process and a change detection process, while the AI-based approaches generally require an extra training set Existing change detection reviews have focused mainly on the design of change detection techniques in multi-temporal hyperspectral images (HSIs) and high-spatial-resolution images <ref type="bibr" target="#b0">[1]</ref><ref type="bibr" target="#b1">[2]</ref><ref type="bibr">[3]</ref><ref type="bibr" target="#b3">[4]</ref>. The techniques they reviewed are mainly traditional change detection approaches, which can be generally summarized into the following groups:</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>•</head><p>Visual analysis: the change map is obtained by manual interpretation, which can provide highly reliable results based on expert knowledge but is time-consuming and labor-intensive; According to the detection unit, these methods can also be classified based on pixel-level, feature-level, object-level, and three-dimensions (3D) object-level, and have been systematically reviewed in the literature <ref type="bibr" target="#b4">[5]</ref><ref type="bibr" target="#b5">[6]</ref><ref type="bibr" target="#b6">[7]</ref>. Due to the rapid development of computer technology, the research of traditional change detection approaches has turned to integrating AI techniques. In both traditional change detection flow and AI-based flow, the first step is data acquisition and the aim of change 1.</p><p>The implementation process of AI-based change detection is introduced, and we summarize common implementation strategies that can help beginners understand this research field; 2.</p><p>We present the data from different sensors used for AI-based change detection in detail, mainly including optical RS data, SAR data, street-view images, and combined heterogeneous data. More practically, we list the available open datasets with annotations, which can be used as benchmarks for training and evaluating AI models in future change detection studies; 3.</p><p>By systematically reviewing and analyzing the process of AI-based change detection methods, we summarize their general frameworks in a practical way, which can help to design change detection approaches in the future. Furthermore, the unsupervised schemes used in AI-based change detection are analyzed to help address the problem of lack of training samples in practical applications; 4.</p><p>We describe the commonly used networks in AI for change detection. Analyzing their applicability is helpful for the selection of AI models in practical applications; 5.</p><p>We provide the application of AI-based change detection in various fields, and subdivide it into different data types, which helps those interested in these areas to find relevant AI-based change detection approaches; 6.</p><p>We delineate and discuss the challenges and prospects of AI for change detection from three major directions, i.e., heterogeneous big data processing, unsupervised AI, and the reliability of AI, providing a useful reference for future research.</p><p>The rest of the paper is organized as follows. We introduce the implementation process of AI-based change detection in Section 2; and we list data sources used for change detection in Section 3;</p><p>The review of general frameworks and commonly used networks in AI for change detection are presented in Sections 4 and 5, respectively; in Section 6, we summarize the various applications of the methods; after discussing the challenges and opportunities of AI-based change detection in Section 7, we draw conclusions of this review in Section 8.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Implementation Process of AI-Based Change Detection</head><p>Figure <ref type="figure" target="#fig_0">1</ref> illustrates the general flow of AI-based change detection. The key is to obtain a high-performance trained AI model. In detail, as presented in Figure <ref type="figure" target="#fig_3">2</ref>, the implementation process of AI-based change detection includes the following four main steps:</p><p>1.</p><p>Homogenization: Due to differences in illumination and atmospheric conditions, seasons, and sensor attitudes at the time of acquisition, multi-period data usually need to be homogenized before change detection. Geometric and radiometric correction are two commonly used methods <ref type="bibr" target="#b13">[14,</ref><ref type="bibr" target="#b14">15]</ref>. The former aims to geometrically align two or more given pieces of data, which can be achieved through registration or co-registration. Given two period data, only when they are overlaid can the comparison between corresponding positions be meaningful <ref type="bibr">[16]</ref>. The latter aims to eliminate radiance or reflectance differences caused by the digitalization process of sensors and atmospheric attenuation distortion caused by absorption and scattering in the atmosphere <ref type="bibr" target="#b3">[4]</ref>, which helps to reduce false alarms caused by these radiation errors in change detection. For heterogeneous data, a special AI model structure can be designed for feature space transformation to achieve change detection (see Section 4.1.2); 2.</p><p>Training set generation: To develop the AI model, a large, high-quality training set is required that can help algorithms to understand that certain patterns or series of outcomes come with a given question. Multi-period data are labeled or annotated using certain techniques (e.g., manual annotation <ref type="bibr" target="#b16">[17]</ref>, pre-classification <ref type="bibr" target="#b17">[18]</ref>, use of thematic data <ref type="bibr" target="#b18">[19]</ref>) to make it easy for the AI model to learn the characteristics of the changed objects. Figure <ref type="figure" target="#fig_3">2</ref> presents an annotated example for building change detection, which is composed of two-period RS images and a corresponding ground truth labeled with building changes at the pixel level. Based on the ground truth, i.e., prior knowledge, the AI model can be trained in a supervised manner.  The above steps provide a general implementation process of AI-based change detection, but the structure of the AI model is diverse and needs to be well designed according to different application situations and the training data, which will be introduced in Sections 4 and 5. It is worth mentioning that existing mature frameworks such as TensorFlow <ref type="bibr" target="#b24">[25]</ref>, Keras <ref type="bibr" target="#b25">[26]</ref>, Pytorch <ref type="bibr" target="#b26">[27]</ref>, and Caffe <ref type="bibr" target="#b27">[28]</ref>, help researchers more easily realize the design, training, and deployment of AI models, and their development documents provide detailed introductions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Data Sources for Change Detection</head><p>With the development of data acquisition platforms such as satellites, drones, and ground survey vehicles, the massive multi-source RS data that they produce bring forth new application requirements for land change monitoring. In particular, multi-sensor high-spatial and high-temporalresolution data require more automated and robust change detection methods to reduce the cost of manual interpretation. By summarizing the data types used for change detection, we can deeply analyze the applicability of existing change detection methods to the data. In this paper, the types of data used for change detection are divided into optical RS images, SAR images, and street view images. It should be noted that street view images are usually not used as RS data but as auxiliary data <ref type="bibr" target="#b28">[29]</ref><ref type="bibr" target="#b29">[30]</ref><ref type="bibr" target="#b30">[31]</ref>, so it is not common in the RS community. Still, there are overlapping ideas for change detection. In this paper, street view images are treated as a kind of RS data in a broad sense and reviewed, because they can provide street-level observation data. Moreover, the methods combining heterogeneous data for change detection are summarized and analyzed. Examples of different data sources are shown in Figure <ref type="figure" target="#fig_5">3</ref>. Optical RS and SAR images are gathered with passive and active sensors, respectively, covering different electromagnetic spectral ranges. Other data sources, such as digital elevation models (DEMs), geographic information system (GIS) data, and point cloud data, can provide valuable supplementary attributes. Overhead remote sensing collects information over large spatial areas, but its time resolution is relatively low. Street view images can provide nearly real-time information at street-level. For detailed descriptions of optical RS images, SAR images, street view images, and combined heterogeneous data, please refer to Section 3.1. In addition, Section 3.2 lists existing open datasets for change detection tasks that can be employed as benchmarks for future research. The above steps provide a general implementation process of AI-based change detection, but the structure of the AI model is diverse and needs to be well designed according to different application situations and the training data, which will be introduced in Sections 4 and 5. It is worth mentioning that existing mature frameworks such as TensorFlow <ref type="bibr" target="#b24">[25]</ref>, Keras <ref type="bibr" target="#b25">[26]</ref>, Pytorch <ref type="bibr" target="#b26">[27]</ref>, and Caffe <ref type="bibr" target="#b27">[28]</ref>, help researchers more easily realize the design, training, and deployment of AI models, and their development documents provide detailed introductions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Data Sources for Change Detection</head><p>With the development of data acquisition platforms such as satellites, drones, and ground survey vehicles, the massive multi-source RS data that they produce bring forth new application requirements for land change monitoring. In particular, multi-sensor high-spatial and high-temporal-resolution data require more automated and robust change detection methods to reduce the cost of manual interpretation. By summarizing the data types used for change detection, we can deeply analyze the applicability of existing change detection methods to the data. In this paper, the types of data used for change detection are divided into optical RS images, SAR images, and street view images. It should be noted that street view images are usually not used as RS data but as auxiliary data <ref type="bibr" target="#b28">[29]</ref><ref type="bibr" target="#b29">[30]</ref><ref type="bibr" target="#b30">[31]</ref>, so it is not common in the RS community. Still, there are overlapping ideas for change detection. In this paper, street view images are treated as a kind of RS data in a broad sense and reviewed, because they can provide street-level observation data. Moreover, the methods combining heterogeneous data for change detection are summarized and analyzed. Examples of different data sources are shown in Figure <ref type="figure" target="#fig_5">3</ref>. Optical RS and SAR images are gathered with passive and active sensors, respectively, covering different electromagnetic spectral ranges. Other data sources, such as digital elevation models (DEMs), geographic information system (GIS) data, and point cloud data, can provide valuable supplementary attributes. Overhead remote sensing collects information over large spatial areas, but its time resolution is relatively low. Street view images can provide nearly real-time information at street-level. For detailed descriptions of optical RS images, SAR images, street view images, and combined heterogeneous data, please refer to Section 3.1. In addition, Section 3.2 lists existing open datasets for change detection tasks that can be employed as benchmarks for future research. geographic information system (GIS) data (from OpenStreetMap <ref type="bibr" target="#b31">[32]</ref>); (e) Point cloud data (from International Society for Photogrammetry and Remote Sensing (ISPRS) benchmarks <ref type="bibr" target="#b32">[33]</ref>); (f) Street view image (from Cityscapes datasets <ref type="bibr" target="#b33">[34]</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Data Used for Change Detection</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.1.">Optical RS Images</head><p>Optical RS images can be divided into hyperspectral, multispectral, and panchromatic images according to the number of bands. HSIs are volumetric image cubes that consist of hundreds of spectral bands. They have narrow bands over a wide portion of the electromagnetic spectrum; the band range is generally less than 10 nm. Multispectral images typically contain multiple bands but fewer than 15 bands. The spectral resolution of multispectral images is in the range of 0.1 times the wavelengths. Panchromatic images have a single band that is formed by using the total light energy in the visible spectrum (instead of partitioning it into different spectra). A side-by-side example of hyperspectral, multispectral, and panchromatic images is shown in Figure <ref type="figure" target="#fig_8">4</ref>. HSIs are gathered by the AVIRIS sensor <ref type="bibr" target="#b34">[35]</ref>; multispectral and panchromatic images are taken from the Quickbird satellite. The second row of Figure <ref type="figure" target="#fig_8">4</ref> shows the spectral intensity of a selected image pixel and the spectral resolution of the three types of images. Therefore, images with different number of bands, reflecting the spectral resolution, require different methods for change detection. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Data Used for Change Detection</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.1.">Optical RS Images</head><p>Optical RS images can be divided into hyperspectral, multispectral, and panchromatic images according to the number of bands. HSIs are volumetric image cubes that consist of hundreds of spectral bands. They have narrow bands over a wide portion of the electromagnetic spectrum; the band range is generally less than 10 nm. Multispectral images typically contain multiple bands but fewer than 15 bands. The spectral resolution of multispectral images is in the range of 0.1 times the wavelengths. Panchromatic images have a single band that is formed by using the total light energy in the visible spectrum (instead of partitioning it into different spectra). A side-by-side example of hyperspectral, multispectral, and panchromatic images is shown in Figure <ref type="figure" target="#fig_8">4</ref>. HSIs are gathered by the AVIRIS sensor <ref type="bibr" target="#b34">[35]</ref>; multispectral and panchromatic images are taken from the Quickbird satellite. The second row of Figure <ref type="figure" target="#fig_8">4</ref> shows the spectral intensity of a selected image pixel and the spectral resolution of the three types of images. Therefore, images with different number of bands, reflecting the spectral resolution, require different methods for change detection.</p><p>HSIs have hundreds or even thousands of continuous and narrow bands, which can provide abundant spectral and spatial information. Multi-temporal HSIs are of great significance in distinguishing the subtle changes in ground objects through their high-dimensional feature information. The detailed information on spectral changes presents promising change detection performance. However, it increases the redundancy of the data and makes it difficult to interpret. Moreover, due to the generally low spatial resolution of HSIs, the textures around the pixels are vague, and mixed pixels occupy a large proportion. Change detection methods for HSIs must address the problems of high dimensionality, mixed pixels, high computational cost, and a limited training dataset. Effective AI algorithms can be employed to solve these problems and have been proved to achieve satisfactory performance <ref type="bibr" target="#b35">[36]</ref><ref type="bibr" target="#b36">[37]</ref><ref type="bibr" target="#b37">[38]</ref><ref type="bibr" target="#b38">[39]</ref>.</p><p>Multispectral images can be acquired economically and stably, with spatial resolution ranging from low to high. They can provide rich colors, textures, and other properties. Images with high spatial resolution or very high spatial resolution (10 to 100 cm/pixel) can also reflect the structure information of the ground objects <ref type="bibr" target="#b39">[40]</ref>. Consequently, they are widely used for change detection. Specifically, the most commonly used types of multispectral images for AI-based change detection methods are derived from the Landsat series of satellites  and the Sentinel series of satellites <ref type="bibr" target="#b65">[66,</ref><ref type="bibr" target="#b66">67]</ref>, due to their low acquisition cost and high time and space coverage. In addition, other satellites, such as Quickbird <ref type="bibr" target="#b67">[68]</ref><ref type="bibr" target="#b68">[69]</ref><ref type="bibr" target="#b69">[70]</ref><ref type="bibr">[71]</ref><ref type="bibr" target="#b71">[72]</ref><ref type="bibr" target="#b72">[73]</ref><ref type="bibr" target="#b73">[74]</ref>, SPOT series <ref type="bibr" target="#b74">[75]</ref><ref type="bibr" target="#b75">[76]</ref><ref type="bibr" target="#b76">[77]</ref><ref type="bibr" target="#b77">[78]</ref>, Gaofen series <ref type="bibr" target="#b13">[14,</ref><ref type="bibr" target="#b78">79,</ref><ref type="bibr" target="#b79">80]</ref>, Worldview series <ref type="bibr" target="#b80">[81]</ref><ref type="bibr" target="#b81">[82]</ref><ref type="bibr" target="#b82">[83]</ref><ref type="bibr">[84]</ref><ref type="bibr">[85]</ref>, provide high and very high spatial resolution images, and various aircrafts provide very high spatial resolution aerial images <ref type="bibr" target="#b19">[20,</ref><ref type="bibr" target="#b85">[86]</ref><ref type="bibr" target="#b86">[87]</ref><ref type="bibr" target="#b87">[88]</ref><ref type="bibr" target="#b88">[89]</ref><ref type="bibr" target="#b89">[90]</ref><ref type="bibr" target="#b90">[91]</ref><ref type="bibr" target="#b91">[92]</ref><ref type="bibr" target="#b92">[93]</ref><ref type="bibr">[94]</ref>, allowing the change detection results to retain more details of the changes.  HSIs have hundreds or even thousands of continuous and narrow bands, which can provide abundant spectral and spatial information. Multi-temporal HSIs are of great significance in distinguishing the subtle changes in ground objects through their high-dimensional feature information. The detailed information on spectral changes presents promising change detection performance. However, it increases the redundancy of the data and makes it difficult to interpret. Moreover, due to the generally low spatial resolution of HSIs, the textures around the pixels are vague, and mixed pixels occupy a large proportion. Change detection methods for HSIs must address the problems of high dimensionality, mixed pixels, high computational cost, and a limited training dataset. Effective AI algorithms can be employed to solve these problems and have been proved to achieve satisfactory performance <ref type="bibr" target="#b35">[36]</ref><ref type="bibr" target="#b36">[37]</ref><ref type="bibr" target="#b37">[38]</ref><ref type="bibr" target="#b38">[39]</ref>.</p><p>Multispectral images can be acquired economically and stably, with spatial resolution ranging from low to high. They can provide rich colors, textures, and other properties. Images with high spatial resolution or very high spatial resolution (10 to 100 cm/pixel) can also reflect the structure information of the ground objects <ref type="bibr" target="#b39">[40]</ref>. Consequently, they are widely used for change detection. Specifically, the most commonly used types of multispectral images for AI-based change detection methods are derived from the Landsat series of satellites  and the Sentinel series of satellites <ref type="bibr" target="#b65">[66,</ref><ref type="bibr" target="#b66">67]</ref>, due to their low acquisition cost and high time and space coverage. In addition, other satellites, such as Quickbird <ref type="bibr" target="#b67">[68]</ref><ref type="bibr" target="#b68">[69]</ref><ref type="bibr" target="#b69">[70]</ref><ref type="bibr">[71]</ref><ref type="bibr" target="#b71">[72]</ref><ref type="bibr" target="#b72">[73]</ref><ref type="bibr" target="#b73">[74]</ref>, SPOT series <ref type="bibr" target="#b74">[75]</ref><ref type="bibr" target="#b75">[76]</ref><ref type="bibr" target="#b76">[77]</ref><ref type="bibr" target="#b77">[78]</ref>, Gaofen series <ref type="bibr" target="#b13">[14,</ref><ref type="bibr" target="#b78">79,</ref><ref type="bibr" target="#b79">80]</ref>, Worldview series[81-85], provide high and very high spatial resolution images, and various aircrafts provide very high spatial A panchromatic image has only one band (i.e., black and white band), and usually contains a couple of hundred nanometer bandwidth. The bandwidth enables it to hold a high signal-noise ratio, making the panchromatic data available at a high and very high spatial resolution. Therefore, panchromatic images are usually fused with multispectral images to obtain richer spectral information and spatial information for change detection with high and very high spatial resolution. In addition, they can be used directly for change detection <ref type="bibr" target="#b94">[95]</ref>.</p><p>Optical RS images are widely utilized for change detection as they provide abundant spectral and spatial information. However, optical sensors rely upon the sun's illumination and the used wavelength is close to visible light or 1 mm. Therefore, they are often affected by solar radiation and clouds. SAR, on the other hand, uses a wavelength of 1 cm to 1 m and has its own illumination source. Thus, it can image at both day and night, in almost all weather conditions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.2.">SAR Images</head><p>SAR is a technique which uses signal processing to improve the resolution beyond the limitation of physical antenna aperture <ref type="bibr" target="#b95">[96]</ref>. The sensor is mounted on an aircraft or a satellite, and is used to make a high-resolution image of the earth's surface. SAR is independent of atmospheric and sunlight condition, so it has become a valuable source of information in change detection. With the development of SAR imaging technology, multi-platform, multi-band, multi-polarization SAR images provide more abundant data sources for change detection tasks. However, SAR images always suffer from the effect of speckle noises, which results in a more difficult process of change detection than optical RS images. Their three key problems include: (1) suppressing speckle noise; (2) designing a change metric or a change indicator; and (3) using a threshold or a classifier based on a change metric to generate a final change map. Change detection methods using AI techniques, especially an autoencoder (AE) <ref type="bibr" target="#b96">[97]</ref><ref type="bibr" target="#b97">[98]</ref><ref type="bibr" target="#b98">[99]</ref><ref type="bibr" target="#b99">[100]</ref><ref type="bibr" target="#b100">[101]</ref><ref type="bibr" target="#b101">[102]</ref><ref type="bibr" target="#b102">[103]</ref><ref type="bibr">[104]</ref><ref type="bibr" target="#b104">[105]</ref><ref type="bibr" target="#b105">[106]</ref><ref type="bibr" target="#b106">[107]</ref> and a convolutional neural network (CNN) <ref type="bibr" target="#b107">[108]</ref><ref type="bibr" target="#b108">[109]</ref><ref type="bibr" target="#b109">[110]</ref><ref type="bibr" target="#b110">[111]</ref><ref type="bibr" target="#b111">[112]</ref><ref type="bibr" target="#b112">[113]</ref><ref type="bibr" target="#b113">[114]</ref>, to suppress speckle noise and extract features has been proven to be the state of the art. In the overall process and framework of methods, they are similar to the methods based on optical RS images, and the detailed framework and AI model introduction are analyzed in Sections 4 and 5.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.3.">Street View Images</head><p>Unlike optical RS and SAR images, street view images are captured at eye-level instead of overhead. They provide more detailed information in relatively small areas and at more observation angles, which can be used for dynamic or real-time change detection. Change detection based on street view images focuses on changes in the dynamic urban visual landscape, such as the addition or removal of specific landmarks, pedestrians, vehicles, and other roadside buildings.</p><p>A critical challenge is on how to identify noisy changes caused by various illuminations, camera viewpoints, occlusions, and shadows in detecting changes using street view images. These noisy changes are interwoven with semantic changes, making it difficult to define and measure the wanted semantic changes in street view images. Thus, using AI algorithms, mainly CNN <ref type="bibr" target="#b114">[115]</ref><ref type="bibr" target="#b115">[116]</ref><ref type="bibr" target="#b116">[117]</ref><ref type="bibr" target="#b117">[118]</ref><ref type="bibr" target="#b118">[119]</ref><ref type="bibr" target="#b119">[120]</ref>, to learn deep features for change detection, requires street view images that have been spatially registered.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.4.">Combining Heterogeneous Data</head><p>According to whether the source of multi-period data is the same, the change detection methods can be divided into homogeneous data change detection and heterogeneous data change detection. Homogeneous data comes from the same type of sensors, and they have the same properties, spectral distribution, and feature space, while heterogeneous data come from different types of sensors, they have different properties and feature spaces, so they cannot be analyzed directly for difference image. Although change detection using heterogeneous data is more challenging, it has fewer restrictions on the type of input data and can be used in more situations. Different sensors can complement each other to provide richer information on ground objects. For example, using optical RS images and SAR images for change detection, the former can provide rich texture information, while the latter can be acquired without atmospheric restrictions. It can be used for emergency change detection in areas where data are insufficient or disasters occur. Much work on this issue has been proposed that uses AI methods to detect changes in SAR and optical RS images <ref type="bibr">[16,</ref><ref type="bibr" target="#b65">66,</ref><ref type="bibr" target="#b120">[121]</ref><ref type="bibr">[122]</ref><ref type="bibr" target="#b122">[123]</ref><ref type="bibr" target="#b123">[124]</ref><ref type="bibr" target="#b124">[125]</ref><ref type="bibr" target="#b125">[126]</ref><ref type="bibr" target="#b126">[127]</ref>. In addition, GIS maps <ref type="bibr" target="#b127">[128]</ref>, point cloud data <ref type="bibr" target="#b90">[91]</ref>, DEMs <ref type="bibr" target="#b128">[129,</ref><ref type="bibr" target="#b129">130]</ref>, and digital surface models (DSMs) <ref type="bibr" target="#b130">[131]</ref> are used in combination with optical RS images or SAR images for change detection. These different data types can satisfy different application requirements and are selected according to the actual situation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Open Data Sets</head><p>Currently, there are some freely available data sets for change detection, which can be used as benchmark datasets for AI training and accuracy evaluation in future research. Detailed information is presented in Table <ref type="table" target="#tab_1">1</ref>.</p><p>It can be seen that the amount of open datasets that can be used for change detection tasks is small, and some of them have small data sizes. At present, there is still a lack of large SAR datasets that can be used for AI training. Most AI-based change detection methods are based on several SAR data sets that contain limited types of changes, e.g., the Bern dataset, the Ottawa dataset, the Yellow River dataset, and the Mexico dataset <ref type="bibr">[24,</ref><ref type="bibr" target="#b102">103]</ref>, which cannot meet the needs of change detection in areas with complex land cover and various change types. Moreover, their labels are not freely available. Street-view datasets are generally used for research of AI-based change detection methods in computer vision (CV). In CV, change detection based on pictures or video is also a hot research field, and the basic idea is consistent with that based on RS data. Therefore, in addition to street view image datasets, several video datasets in CV can also be used for research on AI-based change detection methods, such as CDNet 2012 <ref type="bibr" target="#b131">[132]</ref> and CDNet 2014 <ref type="bibr" target="#b132">[133]</ref>. Since they belong to the research field of video analysis, this paper will not review them in more detail. Those interested can refer to <ref type="bibr" target="#b131">[132]</ref><ref type="bibr" target="#b132">[133]</ref><ref type="bibr" target="#b133">[134]</ref>. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Type Data Set Description</head><p>Optical RS Hyperspectral change detection dataset <ref type="bibr" target="#b134">[135]</ref> 3 different hyperspectral scenes acquired by AVIRIS or HYPERION sensor, with 224 or 242 spectral bands, labeled 5 types of changes related with crop transitions at pixel level.</p><p>River HSIs dataset <ref type="bibr" target="#b38">[39]</ref> 2 HSIs in Jiangsu province, China, with 198 bands, labeled as changed and unchanged at pixel level.</p><p>HRSCD <ref type="bibr" target="#b135">[136]</ref> 291 co-registered pairs of RGB aerial images, with pixel-level change and land cover annotations, providing hierarchical level change labels, for example, level 1 labels include five classes: no information, artificial surfaces, agricultural areas, forests, wetlands, and water. WHU building dataset <ref type="bibr" target="#b87">[88]</ref> 2-period aerial images containing 12,796 buildings, provided along with building vector and raster maps. SZTAKI Air change benchmark <ref type="bibr" target="#b136">[137,</ref><ref type="bibr" target="#b137">138]</ref> 13 aerial image pairs with 1.5 m spatial resolution, labeled as changed and unchanged at pixel level.</p><p>OSCD <ref type="bibr" target="#b138">[139]</ref> 24 pairs of multispectral images acquired by Sentinel-2, labeled as changed and unchanged at pixel level.</p><p>Change detection dataset <ref type="bibr" target="#b139">[140]</ref> 4 pairs of multispectral images with different spatial resolutions, labeled as changed and unchanged at pixel level.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>MtS-WH [141]</head><p>2 large-size VHR images acquired by IKONOS sensors, with 4 bands and 1 m spatial resolution, labeled 5 types of changes (i.e., parking, sparse houses, residential region, and vegetation region) at scene level.</p><p>ABCD <ref type="bibr" target="#b91">[92]</ref> 16,950 pairs of RGB aerial images for detecting washed buildings by tsunami, labeled damaged buildings at scene level.</p><p>xBD <ref type="bibr" target="#b141">[142]</ref> Pre-and post-disaster satellite imageries for building damage assessment, with over 850,000 building polygons from 6 disaster types, labeled at pixel level with 4 damage scales.</p><p>AICD <ref type="bibr" target="#b142">[143]</ref> 1000 pairs of synthetic aerial images with artificial changes generated with a rendering engine, labeled as changed and unchanged at pixel level.</p><p>Database of synthetic and real images <ref type="bibr" target="#b143">[144]</ref> 24,000 synthetic images and 16,000 fragments of real season-varying RS images obtained by Google Earth, labeled as changed and unchanged at pixel level. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Type Data Set Description</head><p>Street view VL-CMU-CD <ref type="bibr" target="#b144">[145]</ref> 1362 co-registered pairs of RGB and depth images, labeled ground truth change (e.g., bin, sign, vehicle, refuse, construction, traffic cone, person/cycle, barrier)</p><p>and sky masks at pixel level.</p><p>PCD 2015 <ref type="bibr" target="#b118">[119]</ref> 200 panoramic image pairs in "TSUNAMI" and "GSV" subset, with the size of 224 × 1024 pixels, label as changed and unchanged at pixel level.</p><p>Change detection dataset <ref type="bibr" target="#b145">[146]</ref> Image sequences of city streets captured by a vehicle-mounted camera at two different time points, with the size of 5000 × 2500 pixels, labeled 3D scene structure changes at pixel level.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">General AI-Based Change Detection Frameworks</head><p>The input of the change detection task is multi-temporal data, which are homogeneous or heterogeneous data in two or more periods. According to the deep feature extraction or latent feature representation learning process of the bi-temporal data, the AI-based change detection frameworks can be summarized into three types: single-stream, double-stream, and multi-model integrated.</p><p>In addition, we further analyze their unsupervised scheme in these frameworks, which is a very important and challenging research issue in AI.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Single-Stream Framework</head><p>There are two main types of single-stream framework structures for AI-based change detection, as shown in Figure <ref type="figure" target="#fig_10">5</ref>, namely a direct classification structure and a mapping transformationbased structure.</p><p>vegetation region) at scene level.</p><p>ABCD <ref type="bibr" target="#b91">[92]</ref> 16,950 pairs of RGB aerial images for detecting washed buildings by tsunami, labeled damaged buildings at scene level.</p><p>xBD <ref type="bibr" target="#b141">[142]</ref> Pre-and post-disaster satellite imageries for building damage assessment, with over 850,000 building polygons from 6 disaster types, labeled at pixel level with 4 damage scales.</p><p>AICD <ref type="bibr" target="#b142">[143]</ref> 1000 pairs of synthetic aerial images with artificial changes generated with a rendering engine, labeled as changed and unchanged at pixel level.</p><p>Database of synthetic and real images <ref type="bibr" target="#b143">[144]</ref> 24,000 synthetic images and 16,000 fragments of real season-varying RS images obtained by Google Earth, labeled as changed and unchanged at pixel level.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Street view</head><p>VL-CMU-CD <ref type="bibr" target="#b144">[145]</ref> 1362 co-registered pairs of RGB and depth images, labeled ground truth change (e.g., bin, sign, vehicle, refuse, construction, traffic cone, person/cycle, barrier) and sky masks at pixel level.</p><p>PCD 2015 <ref type="bibr" target="#b118">[119]</ref> 200 panoramic image pairs in "TSUNAMI" and "GSV" subset, with the size of 224 × 1024 pixels, label as changed and unchanged at pixel level.</p><p>Change detection dataset <ref type="bibr" target="#b145">[146]</ref> Image sequences of city streets captured by a vehicle-mounted camera at two different time points, with the size of 5000 × 2500 pixels, labeled 3D scene structure changes at pixel level.</p><p>They usually only need a core AI model to achieve change detection, so they can be regarded as a single-stream structure. It is worth noting that, in practice, some studies have made improvements based on these structures to meet specific change detection purposes, and a detailed analysis is given below. They usually only need a core AI model to achieve change detection, so they can be regarded as a single-stream structure. It is worth noting that, in practice, some studies have made improvements based on these structures to meet specific change detection purposes, and a detailed analysis is given below.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.1.">Direct Classification Structure</head><p>The direct classification methods use various data processing approaches to fuse the two or more periods of data into intermediate data, and a single AI-based classifier is then used to perform feature learning and achieve two or multiple classifications of the fusion data. That is, as shown in Figure <ref type="figure" target="#fig_10">5a</ref>, this structure converts the change detection task into a classification task, also known as a two-channel structure in some of the literature <ref type="bibr" target="#b107">[108,</ref><ref type="bibr" target="#b146">147]</ref>. Its two key research issues are the choice of data fusion approach and AI-based classifier.</p><p>To obtain the fusion data from multi-period data, the two most common approaches are using change analysis methods and direct concatenation. Change analysis methods, such as CVA [47], differencing by log-ratio operator <ref type="bibr" target="#b17">[18,</ref><ref type="bibr" target="#b147">148]</ref> or change measures <ref type="bibr" target="#b102">[103,</ref><ref type="bibr" target="#b148">149]</ref>, are able to directly provide change intensity information (i.e., the difference data) in multitemporal data, which can highlight change information and facilitate change detection. The direct concatenation method can retain all the information of the multi-period data, so the change information is extracted by the subsequent classifier. In general, the one-dimensional input data is directly concatenated [24,42,101,150-152], while the two-dimensional data is concatenated by channel <ref type="bibr" target="#b110">[111,</ref><ref type="bibr" target="#b111">112,</ref><ref type="bibr" target="#b152">153,</ref><ref type="bibr" target="#b153">154]</ref>. Moreover, the fusion of original data and difference data <ref type="bibr" target="#b20">[21,</ref><ref type="bibr" target="#b98">99]</ref> is another good strategy, which can keep all the information while highlighting the difference information.</p><p>The classifier uses AI techniques to classify the fused data into two types (i.e., changed or unchanged) or multiple types (different types of changes) <ref type="bibr" target="#b76">[77]</ref>. Its performance and related training data are the key to finally obtaining satisfactory change maps. More details are reviewed in Section 5.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.2.">Mapping Transformation-Based Structure</head><p>The mapping transformation-based framework structure is usually used to detect changes in different domains or heterogeneous data. Its main idea is to use the AI method to learn the feature mapping transformation, and use it to perform feature transformation on one kind of data, as shown in Figure <ref type="figure" target="#fig_10">5b</ref>. The transformed features correspond to the features of another kind of data. In short, it transforms data from one feature space to another feature space. Finally, the change map can be obtained by performing decision analysis on the corresponding features of the two kinds of data. In [16], a mapping neural network (MNN) is designed to learn the mapping function between the multi-spatial-resolution data, and the feature similarity analysis is then implemented to build a change map. This method also achieves change detection between SAR and optical images. In <ref type="bibr" target="#b59">[60]</ref>, the authors use an ANN to achieve relative radiometric normalization, and then detect changes of the two-period data under the same radiation condition. Moreover, using this idea of mapping transformation, several improved structures have been proposed for detecting changes in heterogeneous data <ref type="bibr" target="#b120">[121]</ref><ref type="bibr">[122]</ref><ref type="bibr" target="#b122">[123]</ref><ref type="bibr" target="#b123">[124]</ref> or different domain data <ref type="bibr" target="#b9">[10]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Double-Stream Framework</head><p>Since the change detection task is usually based on two periods of data, that is, two inputs, the double-stream structure is very common for change detection and can be summarized into three types, as shown in Figure <ref type="figure" target="#fig_11">6</ref>. They are a siamese structure, a transfer learning-based structure, and a post-classification structure.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.1.">Siamese Structure</head><p>As shown in Figure <ref type="figure" target="#fig_12">6a</ref>, the Siamese structure generally consists of two sub-networks with the same structure, i.e., feature extractors, which convert the input two-period data into feature maps. Finally, the change map is obtained by using change analysis (i.e., decision maker). The main advantage of this structure is that its two sub-networks are directly trained at the same time to learn the deep features of the input two-period data.</p><p>According to whether the weights of sub-networks are shared, this can be divided into the pure-Siamese structure <ref type="bibr" target="#b21">[22,</ref><ref type="bibr" target="#b67">68,</ref><ref type="bibr">94,</ref><ref type="bibr" target="#b116">117,</ref><ref type="bibr" target="#b154">155,</ref><ref type="bibr" target="#b155">156]</ref> and the pseudo-Siamese structure <ref type="bibr" target="#b78">[79,</ref><ref type="bibr" target="#b108">109,</ref><ref type="bibr" target="#b156">157,</ref><ref type="bibr" target="#b157">158]</ref>. The main difference is that the former sub-network extracts the common features of the two-period data by sharing weights. The latter sub-network extracts features corresponding input data, respectively, resulting in an increase in the number of trainable parameters and complexity, but also in its flexibility. Similarly, the authors of <ref type="bibr" target="#b158">[159]</ref> designed a triple network consisting of three sub-networks with sharing weights for change detection. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.1.">Siamese Structure</head><p>As shown in Figure <ref type="figure" target="#fig_12">6a</ref>, the Siamese structure generally consists of two sub-networks with the same structure, i.e., feature extractors, which convert the input two-period data into feature maps. Finally, the change map is obtained by using change analysis (i.e., decision maker). The main advantage of this structure is that its two sub-networks are directly trained at the same time to learn the deep features of the input two-period data.</p><p>According to whether the weights of sub-networks are shared, this can be divided into the pure-Siamese structure <ref type="bibr" target="#b21">[22,</ref><ref type="bibr" target="#b67">68,</ref><ref type="bibr">94,</ref><ref type="bibr" target="#b116">117,</ref><ref type="bibr" target="#b154">155,</ref><ref type="bibr" target="#b155">156]</ref> and the pseudo-Siamese structure <ref type="bibr" target="#b78">[79,</ref><ref type="bibr" target="#b108">109,</ref><ref type="bibr" target="#b156">157,</ref><ref type="bibr" target="#b157">158]</ref>. The main difference is that the former sub-network extracts the common features of the two-period data by sharing weights. The latter sub-network extracts features corresponding input data, respectively, resulting in an increase in the number of trainable parameters and complexity, but also in its flexibility. Similarly, the authors of <ref type="bibr" target="#b158">[159]</ref> designed a triple network consisting of three sub-networks with sharing weights for change detection.</p><p>Although this structure enables the feature extractor to directly learn deep features by supervised training with labeled samples, unsupervised training is more challenging. A common solution is to train feature extractors individually in an unsupervised manner <ref type="bibr" target="#b104">[105]</ref><ref type="bibr" target="#b105">[106]</ref><ref type="bibr" target="#b106">[107]</ref><ref type="bibr" target="#b159">160]</ref>. These pre-trained feature extractors provide the latent representation of the original data (i.e., feature maps) for further change detection. To generate change maps, the output feature maps in the two periods can be directly classified by the concatenation of channels <ref type="bibr" target="#b90">[91,</ref><ref type="bibr" target="#b154">155,</ref><ref type="bibr" target="#b160">161]</ref> or can be used to produce difference maps using a certain distance metric <ref type="bibr" target="#b8">[9]</ref>, and then used for further change analysis <ref type="bibr" target="#b161">[162,</ref><ref type="bibr" target="#b162">163]</ref>. To retain multi-scale change information, feature maps at different depths can be concatenated for change detection <ref type="bibr" target="#b163">[164]</ref><ref type="bibr" target="#b164">[165]</ref><ref type="bibr" target="#b165">[166]</ref><ref type="bibr" target="#b166">[167]</ref>, and this works well.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.2.">Transfer Learning-Based Structure</head><p>The transfer learning-based structure is proposed to alleviate the lack of training samples and optimize the training process. Transfer learning uses training in one domain to enable better results in another domain and, specifically, the lower to midlevel features learned in the original domain can be transferred as useful features in the new domain <ref type="bibr" target="#b12">[13]</ref>. The pre-trained AI model, as a feature extractor, is used to generate feature maps for two periods, and the feature extractors of the two periods can be the same, as shown in Figure <ref type="figure" target="#fig_12">6b</ref>. Whether the pre-trained model can correctly extract the deep feature map or latent feature representation of the input data determines the performance of the change detection task. Although this structure enables the feature extractor to directly learn deep features by supervised training with labeled samples, unsupervised training is more challenging. A common solution is to train feature extractors individually in an unsupervised manner <ref type="bibr" target="#b104">[105]</ref><ref type="bibr" target="#b105">[106]</ref><ref type="bibr" target="#b106">[107]</ref><ref type="bibr" target="#b159">160]</ref>. These pre-trained feature extractors provide the latent representation of the original data (i.e., feature maps) for further change detection. To generate change maps, the output feature maps in the two periods can be directly classified by the concatenation of channels <ref type="bibr" target="#b90">[91,</ref><ref type="bibr" target="#b154">155,</ref><ref type="bibr" target="#b160">161]</ref> or can be used to produce difference maps using a certain distance metric <ref type="bibr" target="#b8">[9]</ref>, and then used for further change analysis <ref type="bibr" target="#b161">[162,</ref><ref type="bibr" target="#b162">163]</ref>. To retain multi-scale change information, feature maps at different depths can be concatenated for change detection <ref type="bibr" target="#b163">[164]</ref><ref type="bibr" target="#b164">[165]</ref><ref type="bibr" target="#b165">[166]</ref><ref type="bibr" target="#b166">[167]</ref>, and this works well.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.2.">Transfer Learning-Based Structure</head><p>The transfer learning-based structure is proposed to alleviate the lack of training samples and optimize the training process. Transfer learning uses training in one domain to enable better results in another domain and, specifically, the lower to midlevel features learned in the original domain can be transferred as useful features in the new domain <ref type="bibr" target="#b12">[13]</ref>. The pre-trained AI model, as a feature extractor, is used to generate feature maps for two periods, and the feature extractors of the two periods can be the same, as shown in Figure <ref type="figure" target="#fig_12">6b</ref>. Whether the pre-trained model can correctly extract the deep feature map or latent feature representation of the input data determines the performance of the change detection task.</p><p>The transfer learning-based structure usually has two training phases, namely the deep feature learning phase and the fine-tuning phase. In the deep feature learning phase, the AI model is usually supervised, pre-trained with sufficient labeled samples in other domain data <ref type="bibr" target="#b66">[67,</ref><ref type="bibr" target="#b109">110,</ref><ref type="bibr" target="#b167">168]</ref>. The fine-tuning phase is optional and, in this phase, only a small number of labeled samples are required for fine-tuning <ref type="bibr" target="#b124">[125,</ref><ref type="bibr" target="#b168">[169]</ref><ref type="bibr">[170]</ref><ref type="bibr" target="#b170">[171]</ref><ref type="bibr" target="#b171">[172]</ref> or additional classifier training <ref type="bibr" target="#b89">[90,</ref><ref type="bibr" target="#b139">140]</ref>. Therefore, the change map can be directly obtained by the trained classifier. Without fine-tuning, final change maps can be obtained based on the two-period feature maps using change analysis, such as low rank analysis <ref type="bibr" target="#b172">[173]</ref>, CVA <ref type="bibr" target="#b71">[72]</ref>, clustering <ref type="bibr" target="#b72">[73,</ref><ref type="bibr" target="#b81">82]</ref>, and threshold <ref type="bibr" target="#b118">[119,</ref><ref type="bibr" target="#b173">174]</ref>. This means that no more labeled samples are needed for further training. Moreover, based on the idea of transfer learning, the pre-trained AI model can also be used to generate training samples or masks to achieve the unsupervised scheme <ref type="bibr" target="#b77">[78]</ref>, which is a very practical strategy.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.3.">Post-Classification Structure</head><p>As shown in Figure <ref type="figure" target="#fig_12">6c</ref>, the post-classification structure consists of two classifiers, which can usually be converted into classification tasks and trained in a joint or independent way. It provides a classification map for each period data and the change map with change directions can be obtained by comparing classification maps. Nevertheless, the accuracy of the change detection results of these methods depends on the performance of the classifier.</p><p>A number of studies <ref type="bibr" target="#b42">[43,</ref><ref type="bibr" target="#b43">44,</ref><ref type="bibr" target="#b51">52,</ref><ref type="bibr">56,</ref><ref type="bibr" target="#b60">61,</ref><ref type="bibr" target="#b69">70,</ref><ref type="bibr" target="#b74">75,</ref><ref type="bibr" target="#b75">76,</ref><ref type="bibr" target="#b130">131,</ref><ref type="bibr" target="#b175">175,</ref><ref type="bibr" target="#b176">176]</ref> have proven that AI techniques increase the accuracy of land-cover classification to a notable level and the results can be further used for change detection. By converting the direct geometric or spectral comparison to label changes, the post-classification structure can be regarded as a very general and practical structure, and it provides a type change matrix. Advantageously, it works robustly for data acquired under different acquisition conditions (illumination condition, sensor attitude, season, etc.) or even different sensors <ref type="bibr" target="#b44">[45,</ref><ref type="bibr" target="#b49">50,</ref><ref type="bibr" target="#b61">62,</ref><ref type="bibr" target="#b129">130]</ref>. Supervised training of the AI-based classifier requires a large number of training samples, which can be generated by existing GIS data representing land cover <ref type="bibr" target="#b127">[128]</ref> or thematic maps <ref type="bibr" target="#b177">[177]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Multi-Model Integrated Structure</head><p>Many works have integrated multiple AI models to improve the performance of change detection methods. Considering the large number and complex structure, only a representative structure is summarized, as shown in Figure <ref type="figure" target="#fig_13">7</ref>.</p><p>based on the two-period feature maps using change analysis, such as low rank analysis <ref type="bibr" target="#b172">[173]</ref>, CVA <ref type="bibr" target="#b71">[72]</ref>, clustering <ref type="bibr" target="#b72">[73,</ref><ref type="bibr" target="#b81">82]</ref>, and threshold <ref type="bibr" target="#b118">[119,</ref><ref type="bibr" target="#b173">174]</ref>. This means that no more labeled samples are needed for further training. Moreover, based on the idea of transfer learning, the pre-trained AI model can also be used to generate training samples or masks to achieve the unsupervised scheme <ref type="bibr" target="#b77">[78]</ref>, which is a very practical strategy.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.3.">Post-Classification Structure</head><p>As shown in Figure <ref type="figure" target="#fig_12">6c</ref>, the post-classification structure consists of two classifiers, which can usually be converted into classification tasks and trained in a joint or independent way. It provides a classification map for each period data and the change map with change directions can be obtained by comparing classification maps. Nevertheless, the accuracy of the change detection results of these methods depends on the performance of the classifier.</p><p>A number of studies <ref type="bibr" target="#b42">[43,</ref><ref type="bibr" target="#b43">44,</ref><ref type="bibr" target="#b51">52,</ref><ref type="bibr">56,</ref><ref type="bibr" target="#b60">61,</ref><ref type="bibr" target="#b69">70,</ref><ref type="bibr" target="#b74">75,</ref><ref type="bibr" target="#b75">76,</ref><ref type="bibr" target="#b130">131,</ref><ref type="bibr" target="#b175">175,</ref><ref type="bibr" target="#b176">176]</ref> have proven that AI techniques increase the accuracy of land-cover classification to a notable level and the results can be further used for change detection. By converting the direct geometric or spectral comparison to label changes, the post-classification structure can be regarded as a very general and practical structure, and it provides a type change matrix. Advantageously, it works robustly for data acquired under different acquisition conditions (illumination condition, sensor attitude, season, etc.) or even different sensors <ref type="bibr" target="#b44">[45,</ref><ref type="bibr" target="#b49">50,</ref><ref type="bibr" target="#b61">62,</ref><ref type="bibr" target="#b129">130]</ref>. Supervised training of the AI-based classifier requires a large number of training samples, which can be generated by existing GIS data representing land cover <ref type="bibr" target="#b127">[128]</ref> or thematic maps <ref type="bibr" target="#b177">[177]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Multi-Model Integrated Structure</head><p>Many works have integrated multiple AI models to improve the performance of change detection methods. Considering the large number and complex structure, only a representative structure is summarized, as shown in Figure <ref type="figure" target="#fig_13">7</ref>. The multi-model integrated framework is a hybrid structure, which is similar to the double stream structure, but it contains more types of AI models and can also be trained in multiple stages. Change detection is a spatiotemporal analysis and can be achieved by acquiring the spatial-spectral features through an AI-based feature extractor as a spectral-spatial module, and then modeling temporal dependency through an AI-based classifier as a temporal module <ref type="bibr" target="#b13">[14,</ref><ref type="bibr" target="#b37">38,</ref><ref type="bibr" target="#b52">53,</ref><ref type="bibr" target="#b73">74]</ref>. Moreover, this hybrid structure is skillfully used for unsupervised change detection <ref type="bibr" target="#b99">[100]</ref> and object-level change detection <ref type="bibr" target="#b86">[87]</ref>. This makes the whole change detection process more complicated while improving performance. The multi-model integrated framework is a hybrid structure, which is similar to the double stream structure, but it contains more types of AI models and can also be trained in multiple stages. Change detection is a spatiotemporal analysis and can be achieved by acquiring the spatial-spectral features through an AI-based feature extractor as a spectral-spatial module, and then modeling temporal dependency through an AI-based classifier as a temporal module <ref type="bibr" target="#b13">[14,</ref><ref type="bibr" target="#b37">38,</ref><ref type="bibr" target="#b52">53,</ref><ref type="bibr" target="#b73">74]</ref>. Moreover, this hybrid structure is skillfully used for unsupervised change detection <ref type="bibr" target="#b99">[100]</ref> and object-level change detection <ref type="bibr" target="#b86">[87]</ref>. This makes the whole change detection process more complicated while improving performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.">Unsupervised Schemes in Change Detection Frameworks</head><p>AI-based change detection frameworks usually include feature extractors or classifiers, which require supervised and unsupervised training. Since obtaining a large number of labeled samples for supervised training is usually time-consuming and labor-intensive, many efforts have been made to achieve AI-based change detection in an unsupervised or semi-supervised manner. As introduced in Section 4.2.2, transfer learning can reduce or even eliminate the need for training samples, but these are not pure unsupervised schemes, as samples from other domains are required. In addition, the most commonly used unsupervised scheme is to use the change analysis method and sample selection strategy to select absolute changed or/and unchanged as training samples for AI models. Its flow chart is shown in Figure <ref type="figure" target="#fig_16">8a</ref>.</p><p>AI-based change detection frameworks usually include feature extractors or classifiers, which require supervised and unsupervised training. Since obtaining a large number of labeled samples for supervised training is usually time-consuming and labor-intensive, many efforts have been made to achieve AI-based change detection in an unsupervised or semi-supervised manner. As introduced in Section 4.2.2, transfer learning can reduce or even eliminate the need for training samples, but these are not pure unsupervised schemes, as samples from other domains are required. In addition, the most commonly used unsupervised scheme is to use the change analysis method and sample selection strategy to select absolute changed or/and unchanged as training samples for AI models. Its flow chart is shown in Figure <ref type="figure" target="#fig_16">8a</ref>. It can be seen that there are two change detection stages in this scheme. The first stage, i.e., preclassification, is usually simple but worth studying, and most of them are unsupervised methods, which can be implemented with difference analysis and clustering <ref type="bibr" target="#b100">[101]</ref>, such as K-means <ref type="bibr" target="#b161">[162]</ref>, fuzzy c-means (FCM) <ref type="bibr" target="#b89">[90,</ref><ref type="bibr" target="#b98">99,</ref><ref type="bibr" target="#b99">100,</ref><ref type="bibr" target="#b110">111,</ref><ref type="bibr" target="#b150">151,</ref><ref type="bibr" target="#b159">160,</ref><ref type="bibr" target="#b164">165,</ref><ref type="bibr" target="#b178">[178]</ref><ref type="bibr" target="#b179">[179]</ref><ref type="bibr" target="#b180">[180]</ref><ref type="bibr" target="#b181">[181]</ref>, spatial FCM <ref type="bibr" target="#b101">[102,</ref><ref type="bibr" target="#b153">154]</ref>, or hierarchical FCM <ref type="bibr" target="#b20">[21,</ref><ref type="bibr" target="#b112">113]</ref>. This stage in some works are implemented by threshold analysis <ref type="bibr" target="#b17">[18,</ref><ref type="bibr" target="#b38">39]</ref>, saliency analysis <ref type="bibr" target="#b77">[78]</ref>, or well-designed rules <ref type="bibr" target="#b37">[38,</ref><ref type="bibr" target="#b82">83,</ref><ref type="bibr">84,</ref><ref type="bibr" target="#b123">124,</ref><ref type="bibr" target="#b147">148,</ref><ref type="bibr" target="#b182">182,</ref><ref type="bibr" target="#b183">183]</ref>. After obtaining high-confidence changed or/and unchanged samples, the AI model can be trained in a supervised manner for change detection in the second stage. Moreover, another commonly used unsupervised scheme is based on the latent change map, as shown in Figure <ref type="figure" target="#fig_16">8b</ref>. In addition to the pre-trained model obtained by transfer learning, it can be generated by an unsupervised AI model (e.g., AEs), and the final change map is then generated by using a clustering algorithm <ref type="bibr" target="#b22">[23,</ref><ref type="bibr" target="#b78">79,</ref><ref type="bibr" target="#b97">98,</ref><ref type="bibr" target="#b106">107,</ref><ref type="bibr" target="#b156">157,</ref><ref type="bibr" target="#b162">163,</ref><ref type="bibr" target="#b184">184]</ref>.</p><p>Although unsupervised change detection does not require labeled training samples, sometimes the lack of prior knowledge makes it unsuitable for change detection involving semantic information. Weakly and semi-supervised schemes use inaccurate or insufficient labeled samples as a priori knowledge to solve this problem, which can be implemented with label aggregation <ref type="bibr" target="#b96">[97]</ref>, iterative learning <ref type="bibr" target="#b57">[58,</ref><ref type="bibr" target="#b185">185]</ref>, deep generative models <ref type="bibr" target="#b186">[186]</ref> (see Section 5.6 for a more detailed review), sample generation strategies <ref type="bibr" target="#b155">[156]</ref>, or novel cost functions <ref type="bibr" target="#b35">[36,</ref><ref type="bibr" target="#b187">187]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Mainstream Networks in AI</head><p>Section 4 summarizes general AI-based change detection frameworks, but the detailed introduction of their feature extractors and classifiers are not provided. In this section, the various network structures in AI used for change detection are specifically analyzed. At present, they mainly include autoencoders (AEs), deep belief networks (DBNs), CNNs, recurrent neural networks (RNNs), pulse couple neural networks (PCNNs), and generative adversarial networks (GANs), as can be seen in Figure <ref type="figure" target="#fig_17">9</ref>. In addition, other networks or methods in AI used for change detection have also been summarized briefly. It can be seen that there are two change detection stages in this scheme. The first stage, i.e., preclassification, is usually simple but worth studying, and most of them are unsupervised methods, which can be implemented with difference analysis and clustering <ref type="bibr" target="#b100">[101]</ref>, such as K-means <ref type="bibr" target="#b161">[162]</ref>, fuzzy c-means (FCM) <ref type="bibr" target="#b89">[90,</ref><ref type="bibr" target="#b98">99,</ref><ref type="bibr" target="#b99">100,</ref><ref type="bibr" target="#b110">111,</ref><ref type="bibr" target="#b150">151,</ref><ref type="bibr" target="#b159">160,</ref><ref type="bibr" target="#b164">165,</ref><ref type="bibr" target="#b178">[178]</ref><ref type="bibr" target="#b179">[179]</ref><ref type="bibr" target="#b180">[180]</ref><ref type="bibr" target="#b181">[181]</ref>, spatial FCM <ref type="bibr" target="#b101">[102,</ref><ref type="bibr" target="#b153">154]</ref>, or hierarchical FCM <ref type="bibr" target="#b20">[21,</ref><ref type="bibr" target="#b112">113]</ref>. This stage in some works are implemented by threshold analysis <ref type="bibr" target="#b17">[18,</ref><ref type="bibr" target="#b38">39]</ref>, saliency analysis <ref type="bibr" target="#b77">[78]</ref>, or well-designed rules <ref type="bibr" target="#b37">[38,</ref><ref type="bibr" target="#b82">83,</ref><ref type="bibr">84,</ref><ref type="bibr" target="#b123">124,</ref><ref type="bibr" target="#b147">148,</ref><ref type="bibr" target="#b182">182,</ref><ref type="bibr" target="#b183">183]</ref>. After obtaining high-confidence changed or/and unchanged samples, the AI model can be trained in a supervised manner for change detection in the second stage. Moreover, another commonly used unsupervised scheme is based on the latent change map, as shown in Figure <ref type="figure" target="#fig_16">8b</ref>. In addition to the pre-trained model obtained by transfer learning, it can be generated by an unsupervised AI model (e.g., AEs), and the final change map is then generated by using a clustering algorithm <ref type="bibr" target="#b22">[23,</ref><ref type="bibr" target="#b78">79,</ref><ref type="bibr" target="#b97">98,</ref><ref type="bibr" target="#b106">107,</ref><ref type="bibr" target="#b156">157,</ref><ref type="bibr" target="#b162">163,</ref><ref type="bibr" target="#b184">184]</ref>.</p><p>Although unsupervised change detection does not require labeled training samples, sometimes the lack of prior knowledge makes it unsuitable for change detection involving semantic information. Weakly and semi-supervised schemes use inaccurate or insufficient labeled samples as a priori knowledge to solve this problem, which can be implemented with label aggregation <ref type="bibr" target="#b96">[97]</ref>, iterative learning <ref type="bibr" target="#b57">[58,</ref><ref type="bibr" target="#b185">185]</ref>, deep generative models <ref type="bibr" target="#b186">[186]</ref> (see Section 5.6 for a more detailed review), sample generation strategies <ref type="bibr" target="#b155">[156]</ref>, or novel cost functions <ref type="bibr" target="#b35">[36,</ref><ref type="bibr" target="#b187">187]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Mainstream Networks in AI</head><p>Section 4 summarizes general AI-based change detection frameworks, but the detailed introduction of their feature extractors and classifiers are not provided. In this section, the various network structures in AI used for change detection are specifically analyzed. At present, they mainly include autoencoders (AEs), deep belief networks (DBNs), CNNs, recurrent neural networks (RNNs), pulse couple neural networks (PCNNs), and generative adversarial networks (GANs), as can be seen in Figure <ref type="figure" target="#fig_17">9</ref>. In addition, other networks or methods in AI used for change detection have also been summarized briefly.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.">Autoencoder</head><p>The basic structure of the AE is shown in Figure <ref type="figure" target="#fig_18">9a</ref>. It mainly includes two parts: an encoder and a decoder. The encoder encodes the input vector x to get latent features h(x), which can be formulated as: h(x) = f(Wx + b). The decoder, which can be formulated as: x = f(W h(x) + b ), reconstructs the learned latent features to output a vector x that should be as close as possible to the original x. W and b are trainable parameters and can be obtained through unsupervised schemes. Intuitively, an AE can be used for feature dimensionality reduction, similar to PCA, but its performance is better due to the strong feature learning capabilities of the neural network. Thus, it is widely used in change detection tasks as the feature extractor. The commonly used AE models are stacked AEs <ref type="bibr" target="#b96">[97,</ref><ref type="bibr" target="#b97">98,</ref><ref type="bibr">104]</ref>, stacked denoising AEs <ref type="bibr">[16,</ref><ref type="bibr" target="#b100">101,</ref><ref type="bibr" target="#b105">106,</ref><ref type="bibr" target="#b120">[121]</ref><ref type="bibr">[122]</ref><ref type="bibr" target="#b122">[123]</ref><ref type="bibr" target="#b150">151,</ref><ref type="bibr" target="#b159">160,</ref><ref type="bibr" target="#b188">188]</ref>, stacked fisher AEs <ref type="bibr" target="#b189">[189]</ref>, sparse AEs <ref type="bibr" target="#b79">[80]</ref>, denoising AEs <ref type="bibr" target="#b101">[102]</ref>, fuzzy AEs <ref type="bibr" target="#b104">[105]</ref>, and contractive AEs <ref type="bibr" target="#b98">[99,</ref><ref type="bibr" target="#b102">103]</ref>. These AEs preserve spatial information by expanding pixel neighborhoods into vectors, while convolutional AEs are implemented directly through convolution kernels <ref type="bibr">[170,</ref><ref type="bibr" target="#b190">190]</ref>. According to its characteristics, AEs can be used to implement change detection in an unsupervised manner and perform well. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.">Autoencoder</head><p>The basic structure of the AE is shown in Figure <ref type="figure" target="#fig_18">9a</ref>. It mainly includes two parts: an encoder and a decoder. The encoder encodes the input vector 𝒙 to get latent features 𝒉(𝒙) , which can be formulated as: 𝒉(𝒙) = 𝒇(𝑾𝒙 + 𝒃). The decoder, which can be formulated as: 𝒙 ̃= 𝒇(𝑾 ′ 𝒉(𝒙) + 𝒃 ′ ), reconstructs the learned latent features to output a vector 𝒙 ̃ that should be as close as possible to the original 𝒙. 𝑾 and 𝒃 are trainable parameters and can be obtained through unsupervised schemes. Intuitively, an AE can be used for feature dimensionality reduction, similar to PCA, but its performance is better due to the strong feature learning capabilities of the neural network. Thus, it is widely used in change detection tasks as the feature extractor. The commonly used AE models are stacked AEs <ref type="bibr" target="#b96">[97,</ref><ref type="bibr" target="#b97">98,</ref><ref type="bibr">104]</ref>, stacked denoising AEs <ref type="bibr">[16,</ref><ref type="bibr" target="#b100">101,</ref><ref type="bibr" target="#b105">106,</ref><ref type="bibr" target="#b120">[121]</ref><ref type="bibr">[122]</ref><ref type="bibr" target="#b122">[123]</ref><ref type="bibr" target="#b150">151,</ref><ref type="bibr" target="#b159">160,</ref><ref type="bibr" target="#b188">188]</ref>, stacked fisher AEs <ref type="bibr" target="#b189">[189]</ref>, sparse AEs <ref type="bibr" target="#b79">[80]</ref>, denoising AEs <ref type="bibr" target="#b101">[102]</ref>, fuzzy AEs <ref type="bibr" target="#b104">[105]</ref>, and contractive AEs <ref type="bibr" target="#b98">[99,</ref><ref type="bibr" target="#b102">103]</ref>. These AEs preserve spatial information by expanding pixel neighborhoods into vectors, while convolutional AEs are implemented directly through convolution kernels <ref type="bibr">[170,</ref><ref type="bibr" target="#b190">190]</ref>. According to its characteristics, AEs can be used to implement change detection in an unsupervised manner and perform well.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.">Deep Belief Network</head><p>A DBN is a generative graphical model and learns to probabilistically reconstruct its inputs. It </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.">Deep Belief Network</head><p>A DBN is a generative graphical model and learns to probabilistically reconstruct its inputs. It can be formed by stacking multiple simple and unsupervised networks such as restricted Boltzmann machines (RBMs) or AEs. As shown in Figure <ref type="figure" target="#fig_18">9b</ref>, a DBN consists of multiple layers of hidden units, with connections between the layers. However, its units within the same layer are not connected to each other and each hidden layer serves as the visible layer for the next. As a feature extractor, it can be trained greedily, i.e., one layer at a time, and appears in many unsupervised change detection methods <ref type="bibr" target="#b22">[23,</ref><ref type="bibr" target="#b36">37,</ref><ref type="bibr" target="#b156">157,</ref><ref type="bibr" target="#b183">183]</ref>. On the other hand, the deep Boltzmann machine (DBM), as a graph similar to DBN but undirected, can also achieve such a function <ref type="bibr" target="#b182">[182]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3.">Convolutional Neural Network</head><p>Deep features extracted by deep neural networks can be divided into two categories according to whether spatial relationships are considered. One type uses one-dimensional data as input (e.g., DBNs and AEs). Their input is a column vector converted from the input image patch that loses the intact spatial information. The other uses two-dimensional data as input, considering the spatial relationship and its typical representative is CNN. As shown in Figure <ref type="figure" target="#fig_18">9c</ref>, it detects features hierarchically through local connections and shared weight and is closer to the human visual perception mechanism. For those unfamiliar with CNNs, an excellent book by Goodfellow et al. can be found in <ref type="bibr" target="#b191">[191]</ref>.</p><p>Due to its strong ability to automatically learn deep features, the CNN has achieved a satisfactory performance in various image-processing tasks. Many classic CNNs and their improvements are used as classifiers or feature extractors for change detection, such as VGGNet <ref type="bibr" target="#b77">[78,</ref><ref type="bibr" target="#b85">86,</ref><ref type="bibr" target="#b118">119,</ref><ref type="bibr" target="#b139">140,</ref><ref type="bibr" target="#b163">164,</ref><ref type="bibr" target="#b167">168]</ref>, CaffeNet <ref type="bibr" target="#b173">[174]</ref>, SegNet <ref type="bibr" target="#b192">[192]</ref>, UNet <ref type="bibr" target="#b168">[169,</ref><ref type="bibr" target="#b193">193]</ref>, InceptionNet <ref type="bibr" target="#b66">[67]</ref>, and ResNet <ref type="bibr" target="#b194">[194,</ref><ref type="bibr" target="#b195">195]</ref>. Nevertheless, most of the network structures in the literature are newly designed, and a CNN, generally, consists of an input layer and a series of convolutional layers, pooling layers, activation functions, and fully connected layers. However, to achieve special functions, the CNNs integrate some special structures for change detection. For instance, the region CNN (R-CNN), primitively designed for object detection in CV, contains a region-proposals structure to predict the regions of the changed objects <ref type="bibr" target="#b86">[87,</ref><ref type="bibr" target="#b196">196,</ref><ref type="bibr" target="#b197">197]</ref>. The PCANet, with its convolution filter banks chosen from PCA filters, is able to reduce the influence of speckle noise and has been used in SAR image change detection <ref type="bibr" target="#b178">[178,</ref><ref type="bibr" target="#b180">180]</ref>. More recent work proposes a kernel PCA convolution to extract representative spatial-spectral features from RS images in an unsupervised manner <ref type="bibr" target="#b162">[163]</ref>.</p><p>In conclusion, the use of CNNs enables the change detection method to reach the state of the art, but there is no systematic way yet to design and/or train the network, which is a long-standing problem in the RS community.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.4.">Recurrent Neural Network</head><p>Obviously, the input for change detection includes two or more periods of data, so the change detection task can be converted into a process of obtaining change information directly from the multi-period sequence data. RNN, as a memory network whose input is sequence data, is very suitable for this situation. As shown in Figure <ref type="figure" target="#fig_18">9d</ref>, its core part is a directed graph that can be unfold to a chain, with units (i.e., RNN cells) linked in sequence. An RNN cell has two inputs: one is the current time input x t , which is used to update the state in real time, and the other is the state of the hidden layer h t-1 at the previous time, which is used to remember the state. The network at different times shares the same set of parameters F .</p><p>A long short-term memory (LSTM) network, a special RNN that alleviates the problem of gradient disappearance and gradient explosion during long sequence training, has been employed as a temporal module (see Section 4.3) in change detection tasks <ref type="bibr" target="#b13">[14,</ref><ref type="bibr" target="#b37">38,</ref><ref type="bibr" target="#b52">53,</ref><ref type="bibr">64,</ref><ref type="bibr" target="#b73">74]</ref>. In <ref type="bibr" target="#b50">[51]</ref>, the authors used an improved LSTM network to acquire and record the change information of multi-temporal RS data. Advantageously, the trained model could be transferred to other data domains; that is, it has a good generalization ability. Further, in <ref type="bibr" target="#b51">[52]</ref>, based on the same idea, i.e., knowledge transfer, the authors propose an RNN-based framework to detect annual urban dynamics of four cities. These methods can help to address the problem of temporal spectral variance and insufficient samples in the long-term detection of urban change.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.5.">Pulse Couple Neural Network</head><p>The PCNN is a bionic neural network inspired by the visual cortex of mammals. Unlike traditional neural networks, it does not require a learning and training stage to extract effective information from very complex backgrounds, which means that it is unsupervised and can easily be used as a feature extractor. As shown in Figure <ref type="figure" target="#fig_18">9e</ref>, it mainly consists of three parts: a received field, a modulation field, and a pulse generator. A PCNN receives a two-dimensional input image, and each neuron corresponds to one pixel in the image. The value of each pixel serves as an external stimulus for each neuron, and its connected neighboring neurons provide local stimuli. The external and local stimuli are combined in a modulation field and a pulse generator to produce a pulsed output. As the number of iterations increases, the PCNN generates a pulse sequence that can be used for image segmentation and feature extraction <ref type="bibr" target="#b198">[198]</ref> and, similarly, for change detection <ref type="bibr" target="#b53">[54,</ref><ref type="bibr" target="#b65">66,</ref><ref type="bibr">71,</ref><ref type="bibr">85,</ref><ref type="bibr" target="#b123">124,</ref><ref type="bibr" target="#b199">[199]</ref><ref type="bibr" target="#b200">[200]</ref><ref type="bibr" target="#b201">[201]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.6.">Generative Adversarial Networks</head><p>GANs are algorithmic architectures that use two neural networks, namely, a generator and a discriminator, to contest with each other in a game to obtain the best generation and discrimination model <ref type="bibr" target="#b202">[202]</ref>, as shown in Figure <ref type="figure" target="#fig_18">9f</ref>. The generator learns to generate plausible data as negative training examples for the discriminator, while the discriminator learns to distinguish these fake data from real data. Therefore, with a small amount of labeled data (i.e., real data), a well-performed discrimination model can be trained for change detection <ref type="bibr" target="#b16">[17]</ref>. Different from using random Gaussian noises to generate fake data <ref type="bibr" target="#b82">[83]</ref>, the authors of <ref type="bibr" target="#b203">[203]</ref> used a CNN-based generator to produce fake difference maps based on the joint distribution of two-period data, which can help to weaken the impact of the bad pixels on the network. In <ref type="bibr" target="#b204">[204]</ref>, the authors designed a W-Net as a generator to decrease the network parameters and make them easy to train, and used many manually annotated samples to improve the reliability of the results. More recent work <ref type="bibr" target="#b126">[127]</ref> proposed a conditional GAN to achieve change detection of heterogeneous data, where a translation network and an approximation network were used to transform the heterogeneous data into same feature space, and the change map could then be obtained by direct comparison. Similarly, a coupling translation network was employed in <ref type="bibr" target="#b125">[126]</ref>. For mapping landslides, the authors of <ref type="bibr" target="#b92">[93]</ref> used a mapping generator to transform the preand post-landslide images into the same domain, and a Siamese network was then employed to detect landslide changes.</p><p>To obtain a well-functioning GAN, the methods need a well-designed loss function and a good training strategy; otherwise, the model results may be unsatisfactory due to the freedom of the neural network. In addition, real data are needed to ensure the reliability of the network. These are challenges that exist in many practical applications.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.7.">Other Artificial Neural Networks and AI Methods</head><p>There are many types of artificial neural networks in AI and the mainstream network structures used for change detection are described above. In addition, other networks, such as Hopfield networks [47, <ref type="bibr" target="#b47">48,</ref><ref type="bibr" target="#b64">65,</ref><ref type="bibr" target="#b205">[205]</ref><ref type="bibr" target="#b206">[206]</ref><ref type="bibr" target="#b207">[207]</ref>, back propagation networks <ref type="bibr" target="#b41">[42,</ref><ref type="bibr" target="#b148">149,</ref><ref type="bibr" target="#b208">208,</ref><ref type="bibr" target="#b209">209]</ref>, multilayer perceptrons (MLPs) <ref type="bibr" target="#b69">[70,</ref><ref type="bibr" target="#b210">[210]</ref><ref type="bibr" target="#b211">[211]</ref><ref type="bibr">[212]</ref><ref type="bibr" target="#b213">[213]</ref><ref type="bibr" target="#b214">[214]</ref>, extreme learning machines <ref type="bibr" target="#b215">[215]</ref>, and self-organizing map (SOM) networks <ref type="bibr" target="#b54">[55,</ref><ref type="bibr" target="#b216">[216]</ref><ref type="bibr" target="#b217">[217]</ref><ref type="bibr" target="#b218">[218]</ref><ref type="bibr" target="#b219">[219]</ref><ref type="bibr" target="#b220">[220]</ref><ref type="bibr" target="#b221">[221]</ref>, do not require a large number of training samples to learn high-level abstract features as deep neural networks do, but due to their shallow network structure, low sample size requirements, and easy training process, they are also widely used in change detection tasks and can achieve satisfactory results. Since they can be regarded as traditional machine learning techniques, we will not make more detailed comments here due to space limitations and existing reviews <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b222">222,</ref><ref type="bibr" target="#b223">223]</ref>.</p><p>In addition to the neural network in AI, there are other AI techniques used for implementing change detection. Recently, dictionary learning has been employed, and it focuses on learning internal feature representations from datasets <ref type="bibr" target="#b140">[141,</ref><ref type="bibr" target="#b176">176,</ref><ref type="bibr" target="#b224">224,</ref><ref type="bibr" target="#b225">225]</ref>, just like AEs. The cellular automata (CA), a spatially and temporally discrete model inspired by cellular behavior, can help to model future changes in LULC <ref type="bibr" target="#b226">[226]</ref> and predict urban spatial expansion <ref type="bibr" target="#b227">[227]</ref>. The development of these AI techniques has significantly promoted research on change detection, which helps to develop more automatic, intelligent and accurate methods to meet the needs of various applications.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Applications</head><p>In practical applications, according to the change information, the final change maps can be grouped into four types, namely, binary maps, one-class maps, from-to maps, and instance maps, as can been see in Figure <ref type="figure" target="#fig_19">10</ref>.</p><p>automatic, intelligent and accurate methods to meet the needs of various applications.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Applications</head><p>In practical applications, according to the change information, the final change maps can be grouped into four types, namely, binary maps, one-class maps, from-to maps, and instance maps, as can been see in Figure <ref type="figure" target="#fig_19">10</ref>. A binary map uses 1 and 0 to indicate change and no change. It contains any changes and cannot provide an additional type of information for the changed ground object. A one-class map provides single-type change information, which indicates the appearance and disappearance of a specific type of ground object. For example, the result of building change detection is a one-class map indicating the addition and removal of buildings, which can be used for urban management <ref type="bibr" target="#b85">[86]</ref>. A from-to map provides change transfer information, indicating that the ground object is changed from one type to another, and these change types are determined by the classifier <ref type="bibr" target="#b127">[128]</ref>. A change instance map provides boundaries for each change instance, which can be the result of object-based change detection <ref type="bibr" target="#b88">[89]</ref>. These change detection maps can be generated by trained AI models and used in various applications. To demonstrate the future potential application demands and development possibilities of AI-based change detection techniques, the current attempts and work in various application fields are summarized in this section. The development of AI-based change detection techniques has greatly facilitated many applications and has improved their automation and intelligence. Most AI-based change detection generates binary maps, and these studies only focus on the algorithm itself, without a specific application field. Therefore, it can be considered that they are generally suitable for LULC change detection. In this section, we focus on the techniques that are associated with specific applications, and they can be broadly divided into four categories: A binary map uses 1 and 0 to indicate change and no change. It contains any changes and cannot provide an additional type of information for the changed ground object. A one-class map provides single-type change information, which indicates the appearance and disappearance of a specific type of ground object. For example, the result of building change detection is a one-class map indicating the addition and removal of buildings, which can be used for urban management <ref type="bibr" target="#b85">[86]</ref>. A from-to map provides change transfer information, indicating that the ground object is changed from one type to another, and these change types are determined by the classifier <ref type="bibr" target="#b127">[128]</ref>. A change instance map provides boundaries for each change instance, which can be the result of object-based change detection <ref type="bibr" target="#b88">[89]</ref>. These change detection maps can be generated by trained AI models and used in various applications. To demonstrate the future potential application demands and development possibilities of AI-based change detection techniques, the current attempts and work in various application fields are summarized in this section. The development of AI-based change detection techniques has greatly facilitated many applications and has improved their automation and intelligence. Most AI-based change detection generates binary maps, and these studies only focus on the algorithm itself, without a specific application field. Therefore, it can be considered that they are generally suitable for LULC change detection. In this section, we focus on the techniques that are associated with specific applications, and they can be broadly divided into four categories:</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>•</head><p>Urban contexts: urban expansion, public space management, and building change detection; • Resources and environment: human-driven environmental changes, hydro-environmental changes, sea ice, surface water, and forest monitoring; • Natural disasters: landslide mapping and damage assessment; • Astronomy: planetary surfaces.</p><p>We provide an overview of the various change detection techniques in the literature for the different application categories. The works and data types associated with these applications are listed in Table <ref type="table" target="#tab_3">2</ref>. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Applications Data Types</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Urban contexts</head><p>Urban expansion a. Satellite images <ref type="bibr" target="#b51">[52,</ref><ref type="bibr" target="#b228">228]</ref> b. SAR images <ref type="bibr" target="#b229">[229]</ref> Public space management Street view images <ref type="bibr" target="#b116">[117]</ref> Building change detection a. Aerial images <ref type="bibr" target="#b85">[86,</ref><ref type="bibr" target="#b88">89,</ref><ref type="bibr" target="#b89">90]</ref> b. Satellite images <ref type="bibr">[85,</ref><ref type="bibr" target="#b192">192]</ref> c. Satellite/Aerial images <ref type="bibr" target="#b87">[88,</ref><ref type="bibr">94]</ref> d. Airborne laser scanning data and aerial images <ref type="bibr" target="#b90">[91]</ref> e. SAR images <ref type="bibr" target="#b111">[112]</ref> f. Satellite images and GIS map <ref type="bibr" target="#b177">[177]</ref> Resources &amp; environment Human-driven environmental changes Satellite images <ref type="bibr" target="#b68">[69]</ref> Hydro-environmental changes Satellite images <ref type="bibr">[56]</ref> Sea ice SAR images <ref type="bibr" target="#b170">[171]</ref> Surface water Satellite images <ref type="bibr" target="#b230">[230,</ref><ref type="bibr" target="#b231">231]</ref> monitoring Satellite images <ref type="bibr" target="#b44">[45,</ref><ref type="bibr" target="#b62">63,</ref><ref type="bibr" target="#b149">150,</ref><ref type="bibr" target="#b196">196,</ref><ref type="bibr" target="#b232">232]</ref> Natural disasters Landslide mapping a. Aerial images <ref type="bibr" target="#b19">[20,</ref><ref type="bibr" target="#b92">93]</ref> b. Satellite images <ref type="bibr" target="#b128">[129,</ref><ref type="bibr" target="#b214">214,</ref><ref type="bibr" target="#b233">233]</ref> Damage assessment a. Satellite images (caused by tsunami <ref type="bibr" target="#b190">[190,</ref><ref type="bibr" target="#b234">234]</ref>, particular incident <ref type="bibr" target="#b155">[156]</ref>, flood <ref type="bibr" target="#b235">[235]</ref>, or earthquake <ref type="bibr">[</ref> Urbanization is a significant factor causing land surface change. As a result of population growth, the expansion of urbanization plays an important role in transforming natural land cover into urban facilities for people. In <ref type="bibr" target="#b51">[52]</ref>, the authors proposed a new framework based on transfer learning and an RNN for urban area extraction and change detection. Its overall accuracy of single-year urban maps is approximately 96% among the four target cities (Beijing, New York, Melbourne, and Munich). Using a genetic-algorithm-evolved ANN <ref type="bibr" target="#b228">[228]</ref> or a CNN <ref type="bibr" target="#b229">[229]</ref>, urban changes were obtained by taking the difference in the predicted urban distribution maps. For public space management, change detection based on street view images is a good way to identify the encroachment of public spaces. In <ref type="bibr" target="#b116">[117]</ref>, a CNN-using Siamese structure and transfer learning achieved 98.3% pixel accuracy in the VL-CMU-CD dataset. In addition, many studies focus on building changes. Due to the small scale of buildings, change detection is usually carried out based on high-or very-high-spatial-resolution RS data, such as aerial photos <ref type="bibr" target="#b85">[86,</ref><ref type="bibr" target="#b89">90]</ref> and satellite images from Quickbird <ref type="bibr" target="#b192">[192]</ref> or <ref type="bibr">Worldview 2 [85]</ref>. However, due to differences in the experimental data, it is difficult to evaluate which one has the best performance. Although some methods are based on the WHU building dataset <ref type="bibr" target="#b87">[88]</ref>, unfortunately, their experimental data are different with regard to area, which makes it difficult to compare them directly. In <ref type="bibr" target="#b88">[89]</ref>, the authors proposed two CNN models, a mask R-CNN for object-based instance segmentation and a multi-scale CNN for pixel-based semantic segmentation, and their intersection of union (IoU) accuracy of buildings was more than 0.83. In [94], the authors proposed a pyramid feature-based attention-guided Siamese network to detect building changes and the IoU of change map exceeded 0.97. Sometimes, the problem of constant cloud coverage prevents optical RS images from being fully utilized, and SAR data represent a good alternative <ref type="bibr" target="#b111">[112]</ref>. On the other hand, to obtain more building information to promote change detection, airborne laser scanning data <ref type="bibr" target="#b90">[91]</ref> and building thematic data <ref type="bibr" target="#b177">[177]</ref> can provide 3-D information and a priori knowledge of buildings, respectively, and proved to be effective.</p><p>Land cover changes typically reflect changes in climatology and hydrology. Thus, AI-based change detection techniques can provide effective methods to monitor changes in resources and the environment. For example, forest monitoring is usually achieved by detecting changes using multi-period Landsat satellite images <ref type="bibr" target="#b44">[45,</ref><ref type="bibr" target="#b62">63,</ref><ref type="bibr" target="#b149">150,</ref><ref type="bibr" target="#b196">196,</ref><ref type="bibr" target="#b232">232]</ref>. In addition to the Landsat data, the authors of [56] used an ANN to investigate LULC changes and the effect on outlet runoff by detecting LULC change locations. Surface water is essential for humans; using an ANN <ref type="bibr" target="#b230">[230]</ref> or CNN <ref type="bibr" target="#b231">[231]</ref>, its changes can be detected effectively. Obtaining sea ice changes is crucial for navigation safety and climate research in the polar regions. The authors of <ref type="bibr" target="#b170">[171]</ref> employed a transfer learning-based framework and a CNN model to detect sea ice changes from two-period SAR images, and obtained results with kappa coefficient exceeding 94%.</p><p>Natural disaster is a powerful agent that changes the appearance of the landscape. Therefore, change detection techniques based on RS data are significant measures to monitor natural disasters. For example, more automatic and accurate landslide mapping can be achieved through CNN-based change detection methods <ref type="bibr" target="#b19">[20,</ref><ref type="bibr" target="#b92">93,</ref><ref type="bibr" target="#b128">129,</ref><ref type="bibr" target="#b233">233]</ref>. Damage assessment is an important application field of change detection. After a natural disaster, AI-based change techniques can help to identify damaged areas using pre-event and post-event data. Existing research mainly includes tsunamis <ref type="bibr" target="#b91">[92,</ref><ref type="bibr" target="#b118">119,</ref><ref type="bibr" target="#b190">190,</ref><ref type="bibr" target="#b234">234,</ref><ref type="bibr" target="#b236">236]</ref>, floods <ref type="bibr" target="#b235">[235]</ref>, fires [104], and earthquakes <ref type="bibr" target="#b18">[19,</ref><ref type="bibr" target="#b109">110]</ref>.</p><p>Many change detection techniques focus on applications that are closely related to people's daily lives, but in [170], a new AI-based approach is proposed for planetary surface change detection, employing a transfer learning-based framework and convolutional AE models. The experiments showed that the proposed method was superior to a difference image-based method.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.">Challenges and Opportunities for AI-Based Change Detection</head><p>By combining general AI-based change detection frameworks and the network structure summarized in Sections 4 and 5, change detection for various applications can be implemented. The design of the AI model usually needs to consider the multi-period input data type, training set size, and desired change map. Specifically, a mapping transformation-based structure or pseudo-siamese structure should be considered first based on heterogeneous data. To obtain from-to change maps, post-classification structure is the best choice. If training samples are insufficient, a transfer learning-based structure can help alleviate this problem, and the use of AEs and GANs can also reduce the dependence on ground truth. Change detection based on long-term sequence data is usually implemented using the multi-model integrated structure and an RNN model. A CNN has strong feature extraction capability, and is the best choice when there are sufficient training samples.</p><p>Various applications of AI-based change detection have demonstrated that AI techniques have achieved great success in the field of change detection in RS community. However, there are many challenges in the processes, and they relate to the following:</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>•</head><p>With the development of various platforms and sensors, they bring significant challenges such as high-dimensional datasets (the high spatial resolution and hyperspectral features), complex data structures (nonlinear and overlapping distributions), and the nonlinear optimization problem (high computational complexity). The complexity of multi-source data greatly contributes to the difficulty of learning robust and discriminative representations from training data with AI techniques. This can be considered a challenge of heterogeneous big data processing; Some researchers have explored solutions to these problems and proposed useful strategies. We will discuss them separately and give our views.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.1.">Heterogeneous Big Data Processing</head><p>Heterogeneity is one of the main characteristics of big data and heterogeneous data, causing problems in the generation and analytics of change detection results <ref type="bibr" target="#b237">[237]</ref>. From the data source perspective, RS technology can provide various data types for change detection, such as SAR, GIS data, high-resolution satellite images, and various time and space measured data. These data with high variability of data types and formats are difficult to use due to missing values, high data redundancy, and untruthfulness. Moreover, the generalization ability of existing AI methods needs to be improved in RS data processing, especially in heterogeneous big data processing <ref type="bibr" target="#b87">[88]</ref>. Therefore, in our opinion, the following aspects need further study:</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>•</head><p>Although some AI-based change detection methods based on heterogeneous data have achieved satisfactory results, as summarized in Section 3.1.4, the types of sensor and data size of these studies are relatively limited. Moreover, they mainly consider change detection between different source data rather than finding the fusion of data in the same period. The full use of multi-source data at the same period (e.g., optical RS images and DEM) and data fusion theories (i.e., mutual compensation of various types of data), combined with AI techniques, would help improve the accuracy of change detection sufficiently; • Since current change detection methods mainly depends on the detection of 2D information, with the development of 3D reconstruction techniques, using 3D data to detect changes in buildings, etc., is also a direction of future development <ref type="bibr" target="#b5">[6]</ref>. Among such techniques, 3D reconstruction based on oblique images or laser point cloud data and 3D information integration based on aerial imagery and ground-level street view imagery (i.e., air-ground integration) are the hot topics of research. There are still no effective AI techniques that implement 3D change detection;</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>•</head><p>The processing of RS big data requires a large amount of computing resources, limiting the implementation of the AI model. For example, the processing of large-format data usually needs to be processed in blocks, which easily leads to edge problems. The large amount of data means that large trainable parameters in the AI model are required, resulting in a difficult training process and consuming a high amount of computing resources. Therefore, it is necessary to balance the amount of data and the number of trainable parameters. They pose challenges to the design of AI-based change detection approaches.</p><p>In short, heterogeneous big data should be considered when designing AI models for change detection so that it can be practically used for RS big data processing, which is worth pursuing.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.2.">Unsupervised AI</head><p>Although domain knowledge can be used to help design representations in traditional machine learning methods, the quest for AI is motivating the design of more powerful unsupervised representation-learning algorithms <ref type="bibr" target="#b238">[238]</ref>. This is because unsupervised AI possesses the capacity of learning hierarchy features directly from the data itself, and can be used to make data-driven decisions. The research on unsupervised AI can be considered in the following aspects:</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>•</head><p>Due to the lack of labeled samples to train efficient AI models in the past few years, many researchers have devoted great efforts to these problems and have consistently produced impressive results. New unsupervised AI techniques are constantly emerging, including GAN, transfer learning, and AEs, as summarized in Section 4.4. Although these techniques alleviate the lack of samples to a certain extent, there is still room for improvement; • Change detection is generally regarded as a low-likelihood problem (i.e., the unchanged in the change map is much larger than the changed), with the uncertainty of the change location and direction. The current unsupervised AI techniques do not easily solve this problem due to the lack of prior knowledge. Excluding supervised AI, weakly-and semi-supervised AI techniques are feasible solutions, but further research is needed to improve performance. Nevertheless, pure unsupervised AI technique for change detection should be the ultimate goal; • One of the reasons for studying unsupervised AI techniques is the lack of training samples, i.e., prior knowledge. Considering that the Internet has entered the Web 2.0 era (emphasizing user-generated content, ease of use, participatory culture and interoperability for end users), using crowd-source data as a priori knowledge is a good alternative solution. For example, OpenStreetMap <ref type="bibr" target="#b31">[32]</ref>, a free wiki world map, can provide a large amount of annotation data labeled by volunteers for the training of AI models. Although the label precision of some crowd-sourced data is not high, the AI model can also be trained in a weakly supervised manner to achieve change detection.</p><p>On the other hand, given the current trends in CV, unsupervised AI techniques will remain a hot research field and even more popular in change detection as well.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.3.">Reliability of AI</head><p>Although many change detection frameworks using AI present the model structure, their trainable parameters are opaque, like black boxes, which make it difficult to determine why they do what they do or how they work <ref type="bibr" target="#b239">[239]</ref>. The reliability of AI aims to develop techniques to help improve the reliability and interpretability of the change detection methods. Therefore, it is necessary to develop robust AI and interpretable AI for change detection. The relevant theoretical literature can be found in <ref type="bibr" target="#b240">[240,</ref><ref type="bibr" target="#b241">241]</ref>. We only discuss strategies that can be used to improve the reliability of change detection results from the following aspects:</p><p>• Strategy 1: Reduce errors caused by data sources, such as using preprocessing (e.g., spectral and radiometric correction) to reduce the uncertainty of data caused by geometric errors and spectral differences, or fusing multiple data to improve the reliability of the original data, thereby improving the reliability of the change detection results. To date, there have been some studies considering the impact of registration <ref type="bibr" target="#b242">[242]</ref> and algorithm fusion <ref type="bibr" target="#b243">[243]</ref>; • Strategy 2: Improve the interpretability of AI models through a sub-modular model structure, which can help to understand the operation principle of the entire AI model by understanding the function of each sub-module. For example, the region-proposals component in R-CNN can be clearly understood as a generator to predict the regions of the objects; • Strategy 3: Improve the robustness of AI models by integration of multiple algorithms and results. Ensemble learning is a good solution <ref type="bibr" target="#b14">[15,</ref><ref type="bibr" target="#b244">244]</ref>, which can improve the accuracy of the final result by using the results of multiple models; • Strategy 4: Improve the practicality of the AI model results by integrating post-processing algorithms, such as the Markov random field <ref type="bibr" target="#b245">[245]</ref>, the conditional random field <ref type="bibr" target="#b246">[246]</ref>, and level set evolution <ref type="bibr" target="#b247">[247]</ref>, which can help remove noise points and provide accurate boundaries. This is critical for some cartographic applications; • Strategy 5: Improve the fineness of change maps through refined detection units. According to the detection unit of change detection, it can be divided into scene level, patch or super-pixel level, pixel level and sub-pixel level from coarse to fine. From the aspect of reliability, sub-pixel level is the best choice because it alleviates the problem of mixed pixels in RS images. However, this easily leads to high computational complexity. Therefore, using different detection units according to different land cover types is the best solution, which requires a well-designed AI model; • Strategy 6: Improve the representation of change maps by detecting changes in each instance. As we introduced in Section 6, the change maps can be grouped into binary maps, one-class maps, from-to maps, and instance maps. The instance change map is more practical but still lacks research. It can provide change information for each instance, and is more reflective of real-world changes. Moreover, it can avoid the limitation of the binary map without semantic information and the restriction of the from-to map by the classification system, thereby improving the reliability of the final result.</p><p>When using AI techniques for change detection, factors that affect the reliability of data preprocessing, model training, change feature extraction, and accuracy assessment should be considered. This aims toward the most reasonable AI framework to improve the reliability of change detection results.</p><p>In this section, a summary of the challenges and opportunities for AI-based change detection techniques has been delineated and we have put forward our prospects. The development of AI-based change detection techniques depends on the future endeavor on overcoming these challenges; the efforts and innovations of researchers would push forward further successes of the techniques.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8.">Conclusions</head><p>This review presents the latest methods, applications, and challenges of the AI-based change detection techniques. For beginners, the implementation process of AI-based change detection is introduced. Considering that the validity of training data is one of the major challenges, the commonly used data sources and existing datasets used for change detection were fully surveyed. Although the current public datasets have increased significantly, openly labeled datasets for change detection are still scarce and deficient, which requires the joint efforts of the RS community. The systematic analysis of the general network frameworks and commonly used networks in AI adopted for change detection shows that great progress has been made in the combination of AI for change detection, but there are still many challenges in change detection with heterogeneous big data processing, unsupervised AI, and the reliability of AI. This means that further research needs to be pushed forward. This review offers a clearer organization and will help researchers understand this field.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 .</head><label>1</label><figDesc>Figure 1. General schematic diagram of change detection.</figDesc><graphic coords="2,87.40,170.51,420.09,183.40" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 1 .</head><label>1</label><figDesc>Figure 1. General schematic diagram of change detection.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head></head><label></label><figDesc>• aAlgebra-based methods: the change map is obtained by performing algebraic operation or transformation on multi-temporal data, such as image differencing, image regression, image ratioing, and change vector analysis (CVA); • Transformation: data reduction methods, such as principle component analysis (PCA), Tasseled Cap (KT), multivariate alteration detection (MAD), Gramm-Schmidt (GS), and Chi-Square, are used to suppress correlated information and highlight variance in multi-temporal data; • Classification-based methods: changes are identified by comparing multiple classification maps (i.e., post-classification comparison), or using a trained classifier to directly classify data from multiple periods (i.e., multidate classification or direct classification); • Advanced models: advanced models, such as the Li-Strahler reflectance model, the spectral mixture model, and the biophysical parameter method, are used to convert the spectral reflectance values of multi-period data into physically based parameters or fractions to perform change analysis, and this is more intuitive and has physical meaning, but it is complicated and time-consuming; • Others: hybrid approaches and others, such as knowledge-based, spatial-statistics-based, and integrated GIS and RS methods, are used.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 2 .</head><label>2</label><figDesc>Figure 2. Implementation process of AI-based change detection (black arrows indicate workflow and red arrow indicates an example).</figDesc><graphic coords="5,86.65,89.65,420.09,151.72" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 2 .</head><label>2</label><figDesc>Figure 2. Implementation process of AI-based change detection (black arrows indicate workflow and red arrow indicates an example).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 3 .</head><label>3</label><figDesc>Figure 3. Examples of different data sources for change detection: (a) Optical RS image (obtained by Quickbird); (b) SAR image (obtained by the Advanced Land Observing Satellite (ALOS) Phased Array type L-band Synthetic Aperture Radar (PALSAR)); (c) digital elevation model (DEM); (d)</figDesc><graphic coords="6,231.79,234.73,132.54,120.23" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 3 .</head><label>3</label><figDesc>Figure 3. Examples of different data sources for change detection: (a) Optical RS image (obtained by Quickbird); (b) SAR image (obtained by the Advanced Land Observing Satellite (ALOS) Phased Array type L-band Synthetic Aperture Radar (PALSAR)); (c) digital elevation model (DEM); (d) geographic information system (GIS) data (from OpenStreetMap [32]); (e) Point cloud data (from International Society for Photogrammetry and Remote Sensing (ISPRS) benchmarks [33]); (f) Street view image (from Cityscapes datasets [34]).</figDesc><graphic coords="6,84.39,234.73,132.54,120.23" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head></head><label></label><figDesc>Remote Sens. 2020, 12, x FOR PEER REVIEW 7 of 36</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Figure 4 .</head><label>4</label><figDesc>Figure 4. Examples of hyperspectral, multispectral, and panchromatic images, where the hyperspectral image is from Indian Pines; multispectral and panchromatic images are from Quickbird.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>Figure 4 .</head><label>4</label><figDesc>Figure 4. Examples of hyperspectral, multispectral, and panchromatic images, where the hyperspectral image is from Indian Pines; multispectral and panchromatic images are from Quickbird.</figDesc><graphic coords="7,87.77,278.93,420.09,260.97" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head>Figure 5 .</head><label>5</label><figDesc>Figure 5. Schematic diagram of single-stream framework structures of AI-based change detection: (a) the direct classification structure; (b) the mapping transformation-based structure.</figDesc><graphic coords="10,282.77,455.39,218.90,67.25" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_11"><head>Figure 6 .</head><label>6</label><figDesc>Figure 6. Schematic diagram of double-stream framework structure of AI-based change detection: (a) the Siamese structure; (b) the transfer-learning-based structure; (c) the post-classification structure.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_12"><head>Figure 6 .</head><label>6</label><figDesc>Figure 6. Schematic diagram of double-stream framework structure of AI-based change detection: (a) the Siamese structure; (b) the transfer-learning-based structure; (c) the post-classification structure.</figDesc><graphic coords="12,162.32,213.22,273.92,80.25" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_13"><head>Figure 7 .</head><label>7</label><figDesc>Figure 7. Schematic diagram of multi-model integrated structure of AI-based change detection.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_14"><head>Figure 7 .</head><label>7</label><figDesc>Figure 7. Schematic diagram of multi-model integrated structure of AI-based change detection.</figDesc><graphic coords="13,137.68,363.40,319.85,127.70" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_15"><head>Figure 8 .</head><label>8</label><figDesc>Figure 8. Flow chart of the most commonly used unsupervised schemes.</figDesc><graphic coords="14,86.25,127.49,200.69,88.75" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_16"><head>Figure 8 .</head><label>8</label><figDesc>Figure 8. Flow chart of the most commonly used unsupervised schemes, (a) use the change analysis method and sample selection strategy, (b) base on the latent change map.</figDesc><graphic coords="14,304.86,127.44,205.06,88.85" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_17"><head>Figure 9 .</head><label>9</label><figDesc>Figure 9. Schematic diagram of general network architectures in AI used for change detection: (a) autoencoder (AE); (b) deep belief network (DBN); (c) convolutional neural network (CNN); (d) recurrent neural network (RNN); (e) pulse couple neural network (PCNN); (f) generative adversarial networks (GANs).</figDesc><graphic coords="15,89.87,414.72,204.95,88.19" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_18"><head>Figure 9 .</head><label>9</label><figDesc>Figure 9. Schematic diagram of general network architectures in AI used for change detection: (a) autoencoder (AE); (b) deep belief network (DBN); (c) convolutional neural network (CNN); (d) recurrent neural network (RNN); (e) pulse couple neural network (PCNN); (f) generative adversarial networks (GANs).</figDesc><graphic coords="15,315.69,424.91,199.80,68.09" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_19"><head>Figure 10 .</head><label>10</label><figDesc>Figure 10. Schematic diagram of four change maps. To meet different application requirements, four different change maps can be obtained by detecting the change of the two-period images, i.e., (a) binary maps, (b) one-class maps, (c) from-to maps, and (d) instance maps.</figDesc><graphic coords="18,101.52,90.92,392.67,310.35" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_20"><head>Figure 10 .</head><label>10</label><figDesc>Figure 10. Schematic diagram of four change maps. To meet different application requirements, four different change maps can be obtained by detecting the change of the two-period images, i.e., (a) binary maps, (b) one-class maps, (c) from-to maps, and (d) instance maps.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>By deploying a trained AI model, change maps can be generated more intelligently and automatically for practical applications. Moreover, this can help validate the generalization ability and robustness of the model, which is an important aspect of evaluating the practicality of the AI-based change detection technique.</figDesc><table /><note><p><p><p><p><p><p><p><p><p><p><p>To alleviate the problem of lack of training data, data augmentation, which is widely used, is a good strategy, such as horizontal or vertical flip, rotation, change in scale, crop, translation, or adding noise, which can significantly increase the diversity of data available for training models, without actually collecting new data; 3.</p>Model training: After the training set is generated, it can usually be divided into two datasets according to the number of samples or the geographic area: a training set for AI model training and a test set for accuracy evaluation during the training process</p><ref type="bibr" target="#b19">[20]</ref></p>. The training and testing processes are performed alternately and iteratively. During the training process, the model is optimized according to a learning criterion, which can be a loss function in deep learning (e.g., softmax loss</p><ref type="bibr" target="#b20">[21]</ref></p>, contrastive loss</p><ref type="bibr" target="#b21">[22]</ref></p>, Euclidean loss</p><ref type="bibr" target="#b22">[23]</ref></p>, or cross-entropy loss [24]). By monitoring the training process and test accuracy, the convergence state of the AI model can be obtained, which can help in adjusting its hyperparameters (such as the learning rate), and also in judging whether the model performance has reached the best (i.e., termination) condition; 4.</p>Model serving:</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 1 .</head><label>1</label><figDesc>A list of open datasets for change detection.</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 1 .</head><label>1</label><figDesc>Cont.    </figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 2 .</head><label>2</label><figDesc>Summary of main applications of AI-based change detection techniques.</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>•</head><label></label><figDesc>The supervised AI methods require massive training samples, which are usually obtained by time-consuming and labor-intensive processes such as human interpretation of RS products and field surveys. It is a big challenge to achieve a robust model of AI-based methods with insufficient training samples. Unsupervised AI techniques need to be developed; • There are various efficient and accurate AI models and frameworks, as we review in Sections 4 and 5. At present, researchers constantly propose novel AI-based change detection approaches endlessly. Still, it is also a great challenge to choose an efficient one and ensure its accuracy for different applications. The reliability of AI needs to be considered in practical applications.</figDesc><table /></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgments:</head><p>The authors sincerely appreciate that academic editors and reviewers give their help comments and constructive suggestions.</p></div>
			</div>


			<div type="funding">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Funding: This research was funded by the Ministry of Science and Technology of the People's Republic of China, Grant No. 2017YFB0503604.</p></div>
			</div>

			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Author Contributions: All authors contributed in a substantial way to the manuscript. W.S. and M.Z. conceived the review. M.Z. wrote the manuscript. M.Z. and R.Z. contributed to the discussion of the review. M.Z., R.Z., S.C. and Z.Z. made contribution to the review of related literature. All authors discussed the basic structure of the manuscript. W.S. designed the overall structure of the review, reviewed the manuscript and supervised the study for all the stages. All authors have read and agreed to the published version of the manuscript.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Conflicts of Interest:</head><p>The authors declare no conflict of interest.</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Digital change detection techniques using remotely sensed data</title>
		<author>
			<persName><forename type="first">A</forename><surname>Singh</surname></persName>
		</author>
		<idno type="DOI">10.1080/01431168908903939</idno>
	</analytic>
	<monogr>
		<title level="j">Int. J. Remote Sens</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="page" from="989" to="1003" />
			<date type="published" when="1989">1989</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">A Review of Change Detection in Multitemporal Hyperspectral Images: Current Techniques, Applications, and Challenges</title>
		<author>
			<persName><forename type="first">S</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Marinelli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Bruzzone</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Bovolo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Geosci. Remote Sens. Mag</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page" from="140" to="158" />
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">A critical synthesis of remotely sensed optical image change detection techniques</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">P</forename><surname>Tewkesbury</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Comber</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><forename type="middle">J</forename><surname>Tate</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Lamb</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">F</forename><surname>Fisher</surname></persName>
		</author>
		<idno type="DOI">10.1016/j.rse.2015.01.006</idno>
	</analytic>
	<monogr>
		<title level="j">Remote Sens. Environ</title>
		<imprint>
			<biblScope unit="volume">160</biblScope>
			<biblScope unit="page" from="1" to="14" />
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Change detection techniques</title>
		<author>
			<persName><forename type="first">D</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Mausel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Brondizio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Morán</surname></persName>
		</author>
		<idno type="DOI">10.1080/0143116031000139863</idno>
	</analytic>
	<monogr>
		<title level="j">Int. J. Remote Sens</title>
		<imprint>
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="page" from="2365" to="2401" />
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Object-based change detection</title>
		<author>
			<persName><forename type="first">G</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">J</forename><surname>Hay</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">M T</forename><surname>De Carvalho</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Wulder</surname></persName>
		</author>
		<idno type="DOI">10.1080/01431161.2011.648285</idno>
	</analytic>
	<monogr>
		<title level="j">Int. J. Remote Sens</title>
		<imprint>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page" from="4434" to="4457" />
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">3D change detection-Approaches and applications</title>
		<author>
			<persName><forename type="first">R</forename><surname>Qin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Reinartz</surname></persName>
		</author>
		<idno type="DOI">10.1016/j.isprsjprs.2016.09.013</idno>
	</analytic>
	<monogr>
		<title level="j">ISPRS J. Photogramm. Remote Sens</title>
		<imprint>
			<biblScope unit="volume">122</biblScope>
			<biblScope unit="page" from="41" to="56" />
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Change detection from remotely sensed images: From pixel-based to object-based approaches</title>
		<author>
			<persName><forename type="first">M</forename><surname>Hussain</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">M</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Stanley</surname></persName>
		</author>
		<idno type="DOI">10.1016/j.isprsjprs.2013.03.006</idno>
	</analytic>
	<monogr>
		<title level="j">ISPRS J. Photogramm. Remote Sens</title>
		<imprint>
			<biblScope unit="volume">80</biblScope>
			<biblScope unit="page" from="91" to="106" />
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Siri in my hand: Who&apos;s the fairest in the land? On the interpretations, illustrations, and implications of artificial intelligence</title>
		<author>
			<persName><forename type="first">A</forename><surname>Kaplan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Haenlein</surname></persName>
		</author>
		<idno type="DOI">10.1016/j.bushor.2018.08.004</idno>
	</analytic>
	<monogr>
		<title level="j">Bus. Horiz</title>
		<imprint>
			<biblScope unit="volume">62</biblScope>
			<biblScope unit="page" from="15" to="25" />
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">The Spectral-Spatial Joint Learning for Change Detection in Multispectral Imagery</title>
		<author>
			<persName><forename type="first">W</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Lu</surname></persName>
		</author>
		<idno type="DOI">10.3390/rs11030240</idno>
	</analytic>
	<monogr>
		<title level="j">Remote Sens</title>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="page">240</biblScope>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">Dual Learning-Based Siamese Framework for Change Detection Using Bi-Temporal VHR Optical Remote Sensing Images</title>
		<author>
			<persName><forename type="first">B</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Kou</surname></persName>
		</author>
		<idno type="DOI">10.3390/rs11111292</idno>
		<imprint>
			<date type="published" when="1292">2019. 1292</date>
			<biblScope unit="volume">11</biblScope>
		</imprint>
	</monogr>
	<note>Remote Sens</note>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Computational intelligence in optical remote sensing image processing</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y.-S</forename><surname>Ong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
		<idno type="DOI">10.1016/j.asoc.2017.11.045</idno>
	</analytic>
	<monogr>
		<title level="j">Appl. Soft Comput</title>
		<imprint>
			<biblScope unit="volume">64</biblScope>
			<biblScope unit="page" from="75" to="93" />
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Deep learning in remote sensing applications: A meta-analysis and review</title>
		<author>
			<persName><forename type="first">L</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Ye</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Yin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">A</forename><surname>Johnson</surname></persName>
		</author>
		<idno type="DOI">10.1016/j.isprsjprs.2019.04.015</idno>
	</analytic>
	<monogr>
		<title level="j">ISPRS J. Photogramm. Remote. Sens</title>
		<imprint>
			<biblScope unit="volume">152</biblScope>
			<biblScope unit="page" from="166" to="177" />
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Comprehensive survey of deep learning in remote sensing: Theories, tools, and challenges for the community</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">E</forename><surname>Ball</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">T</forename><surname>Anderson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">S</forename><surname>Chan</surname></persName>
		</author>
		<idno type="DOI">10.1117/1.JRS.11.042609</idno>
	</analytic>
	<monogr>
		<title level="j">J. Appl. Remote Sens</title>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="issue">1</biblScope>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Change Detection in Multisource VHR Images via Deep Siamese Convolutional Multiple-Layers Recurrent Neural Network</title>
		<author>
			<persName><forename type="first">H</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<idno type="DOI">10.1109/TGRS.2019.2956756</idno>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Geosci. Remote Sens</title>
		<imprint>
			<biblScope unit="volume">58</biblScope>
			<biblScope unit="page" from="2848" to="2864" />
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Object-Based Change Detection in Urban Areas from High Spatial Resolution Images Based on Multiple Features and Ensemble Learning</title>
		<author>
			<persName><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Xia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Remote Sens</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="page">276</biblScope>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Change detection based on deep feature representation and mapping transformation for multi-spatial-resolution remote sensing images</title>
		<author>
			<persName><forename type="first">P</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Li</surname></persName>
		</author>
		<idno type="DOI">10.1016/j.isprsjprs.2016.02.013</idno>
	</analytic>
	<monogr>
		<title level="j">ISPRS J. Photogramm. Remote Sens</title>
		<imprint>
			<biblScope unit="volume">116</biblScope>
			<biblScope unit="page" from="24" to="41" />
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Incorporating Metric Learning and Adversarial Network for Seasonal Invariant Change Detection</title>
		<author>
			<persName><forename type="first">W</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Mou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Bo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">J</forename><surname>Emery</surname></persName>
		</author>
		<idno type="DOI">10.1109/TGRS.2019.2953879</idno>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Geosci. Remote Sens</title>
		<imprint>
			<biblScope unit="volume">58</biblScope>
			<biblScope unit="page" from="2720" to="2731" />
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Deep learning to classify difference image for image change detection</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">J</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">G</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">C</forename><surname>Jiao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2014 International Joint Conference on Neural Networks</title>
		<meeting>the 2014 International Joint Conference on Neural Networks<address><addrLine>Beijing, China; New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2014-07-11">6-11 July 2014. 2014</date>
			<biblScope unit="page" from="397" to="403" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">A Comparative Study of Texture and Convolutional Neural Network Features for Detecting Collapsed Buildings After Earthquakes Using Pre-and Post-Event Satellite Imagery</title>
		<author>
			<persName><forename type="first">M</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">F</forename><surname>Buchroithner</surname></persName>
		</author>
		<idno type="DOI">10.3390/rs11101202</idno>
		<imprint/>
	</monogr>
	<note>Remote Sens. 2019, 11, 1202. [CrossRef</note>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Landslide Inventory Mapping From Bitemporal Images Using Deep Convolutional Neural Networks</title>
		<author>
			<persName><forename type="first">T</forename><surname>Lei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Lv</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">K</forename><surname>Nandi</surname></persName>
		</author>
		<idno type="DOI">10.1109/LGRS.2018.2889307</idno>
	</analytic>
	<monogr>
		<title level="j">IEEE Geosci. Remote Sens. Lett</title>
		<imprint>
			<biblScope unit="volume">16</biblScope>
			<biblScope unit="page" from="1" to="5" />
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Saliency-Guided Deep Neural Networks for SAR Image Change Detection</title>
		<author>
			<persName><forename type="first">J</forename><surname>Geng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Wang</surname></persName>
		</author>
		<idno type="DOI">10.1109/TGRS.2019.2913095</idno>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Geosci. Remote Sens</title>
		<imprint>
			<biblScope unit="volume">57</biblScope>
			<biblScope unit="page" from="7365" to="7377" />
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Change Detection Based on Deep Siamese Convolutional Network for Optical Aerial Images</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Qiu</surname></persName>
		</author>
		<idno type="DOI">10.1109/LGRS.2017.2738149</idno>
	</analytic>
	<monogr>
		<title level="j">IEEE Geosci. Remote Sens. Lett</title>
		<imprint>
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="page" from="1845" to="1849" />
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">A new difference image creation method based on deep neural networks for change detection in remote-sensing images</title>
		<author>
			<persName><forename type="first">G</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H.-C</forename><surname>Xavier</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Southworth</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Int. J. Remote Sens</title>
		<imprint>
			<biblScope unit="volume">38</biblScope>
			<biblScope unit="page" from="7161" to="7175" />
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Change Detection in Synthetic Aperture Radar Images Based on Deep Neural Networks</title>
		<author>
			<persName><forename type="first">M</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Miao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Jiao</surname></persName>
		</author>
		<idno type="DOI">10.1109/TNNLS.2015.2435783</idno>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Neural Netw. Learn. Syst</title>
		<imprint>
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="page" from="125" to="138" />
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title/>
		<author>
			<persName><surname>Tensorflow</surname></persName>
		</author>
		<ptr target="https://www.tensorflow.org/" />
		<imprint>
			<date type="published" when="2020-05">May 2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title/>
		<author>
			<persName><surname>Keras</surname></persName>
		</author>
		<ptr target="https://keras.io/" />
		<imprint>
			<date type="published" when="2020-05">May 2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<title/>
		<author>
			<persName><surname>Pytorch</surname></persName>
		</author>
		<ptr target="https://pytorch.org/" />
		<imprint>
			<date type="published" when="2020-05">May 2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<title/>
		<author>
			<persName><surname>Caffe</surname></persName>
		</author>
		<ptr target="https://caffe.berkeleyvision.org/" />
		<imprint>
			<date type="published" when="2020-05">May 2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Coupling ground-level panoramas and aerial imagery for change detection</title>
		<author>
			<persName><forename type="first">N</forename><surname>Ghouaiel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Lefevre</surname></persName>
		</author>
		<idno type="DOI">10.1080/10095020.2016.1244998</idno>
	</analytic>
	<monogr>
		<title level="j">Geospat. Inf. Sci</title>
		<imprint>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="page" from="222" to="232" />
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<title level="m" type="main">Bridging the Domain Gap for Ground-to-Aerial Image Matching</title>
		<author>
			<persName><forename type="first">K</forename><surname>Regmi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Shah</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1904.11045</idno>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Building instance classification using street view images</title>
		<author>
			<persName><forename type="first">J</forename><surname>Kang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Körner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Taubenbock</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><forename type="middle">X</forename><surname>Zhu</surname></persName>
		</author>
		<idno type="DOI">10.1016/j.isprsjprs.2018.02.006</idno>
	</analytic>
	<monogr>
		<title level="j">ISPRS J. Photogramm. Remote. Sens</title>
		<imprint>
			<biblScope unit="volume">145</biblScope>
			<biblScope unit="page" from="44" to="59" />
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
		<title/>
		<author>
			<persName><surname>Openstreetmap</surname></persName>
		</author>
		<ptr target="http://www.openstreetmap.org/" />
		<imprint>
			<date type="published" when="2020-05-04">4 May 2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
		<title/>
		<author>
			<persName><surname>Benchmarks</surname></persName>
		</author>
		<ptr target="http://www2.isprs.org/commissions/comm3/wg4/3d-semantic-labeling.html" />
		<imprint>
			<date type="published" when="2020-05">May 2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">The cityscapes dataset for semantic urban scene understanding</title>
		<author>
			<persName><forename type="first">M</forename><surname>Cordts</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Omran</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Ramos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Rehfeld</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Enzweiler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Benenson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">U</forename><surname>Franke</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Roth</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Schiele</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition<address><addrLine>Seattle, WA, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2016-06">June 2016</date>
			<biblScope unit="page" from="3213" to="3223" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">220 band aviris hyperspectral image data set: June 12, 1992 indian pine test site 3</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">F</forename><surname>Baumgardner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">L</forename><surname>Biehl</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">A</forename><surname>Landgrebe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Purdue Univ. Res. Repos</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
	<note>R7RX991C</note>
</biblStruct>

<biblStruct xml:id="b35">
	<monogr>
		<title level="m" type="main">Unsupervised Deep Noise Modeling for Hyperspectral Image Change Detection</title>
		<author>
			<persName><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Wang</surname></persName>
		</author>
		<idno type="DOI">10.3390/rs11030258</idno>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="volume">11</biblScope>
		</imprint>
	</monogr>
	<note>Remote Sens</note>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Hyperspectral remote sensing image change detection based on tensor and deep learning</title>
		<author>
			<persName><forename type="first">F</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Feng</surname></persName>
		</author>
		<idno type="DOI">10.1016/j.jvcir.2018.11.004</idno>
	</analytic>
	<monogr>
		<title level="j">J. Vis. Commun. Image Represent</title>
		<imprint>
			<biblScope unit="volume">58</biblScope>
			<biblScope unit="page" from="233" to="244" />
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<monogr>
		<title level="m" type="main">Change Detection in Hyperspectral Images Using Recurrent 3D Fully Convolutional Networks</title>
		<author>
			<persName><forename type="first">A</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Kim</surname></persName>
		</author>
		<idno type="DOI">10.3390/rs10111827</idno>
		<imprint>
			<date type="published" when="1827">2018. 1827</date>
			<biblScope unit="volume">10</biblScope>
		</imprint>
	</monogr>
	<note>Remote Sens</note>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">GETNET: A General End-to-End 2-D CNN Framework for Hyperspectral Image Change Detection</title>
		<author>
			<persName><forename type="first">Q</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
		<idno type="DOI">10.1109/TGRS.2018.2849692</idno>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Geosci. Remote Sens</title>
		<imprint>
			<biblScope unit="volume">57</biblScope>
			<biblScope unit="page" from="3" to="13" />
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<monogr>
		<title level="m" type="main">High Spatial Resolution Remote Sensing: Data, Analysis, and Applications</title>
		<author>
			<persName><forename type="first">Y</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Weng</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018">2018</date>
			<publisher>CRC Press</publisher>
			<pubPlace>Boca Raton, FL, USA</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">A robust multi-kernel change detection framework for detecting leaf beetle defoliation using Landsat 7 ETM+ data</title>
		<author>
			<persName><forename type="first">A</forename><surname>Anees</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Aryal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">M</forename><surname>O'reilly</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Gale</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Wardlaw</surname></persName>
		</author>
		<idno type="DOI">10.1016/j.isprsjprs.2016.10.011</idno>
	</analytic>
	<monogr>
		<title level="j">ISPRS J. Photogramm. Remote Sens</title>
		<imprint>
			<biblScope unit="volume">122</biblScope>
			<biblScope unit="page" from="167" to="178" />
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Remotely sensed change detection based on artificial neural networks</title>
		<author>
			<persName><forename type="first">X</forename><forename type="middle">L</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Khorram</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Photogramm. Eng. Remote Sens</title>
		<imprint>
			<biblScope unit="volume">65</biblScope>
			<biblScope unit="page" from="1187" to="1194" />
			<date type="published" when="1999">1999</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">An RBF neural network approach for detecting land-cover transitions</title>
		<author>
			<persName><forename type="first">L</forename><surname>Bruzzone</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Cossu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Image and Signal Processing for Remote Sensing Vii; Serpico, S.B</title>
		<meeting><address><addrLine>Bellingham, WA, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Spie-Int Soc Optical Engineering</publisher>
			<date type="published" when="2002">2002</date>
			<biblScope unit="volume">4541</biblScope>
			<biblScope unit="page" from="223" to="231" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<monogr>
		<title level="m" type="main">Change Detection Using Adaptive Fuzzy Neural Networks. Remote Sens. Environ</title>
		<author>
			<persName><forename type="first">A</forename><surname>Abuelgasim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Ross</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Gopal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Woodcock</surname></persName>
		</author>
		<idno type="DOI">10.1016/S0034-4257(99)00039-5</idno>
		<imprint>
			<date type="published" when="1999">1999</date>
			<biblScope unit="volume">70</biblScope>
			<biblScope unit="page" from="208" to="223" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Comparison of pixel -based and artificial neural networks classification methods for detecting forest cover changes in Malaysia</title>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">R</forename><surname>Deilmai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">D</forename><surname>Kanniah</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">W</forename><surname>Rasib</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Ariffin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 8th International Symposium of the Digital Earth</title>
		<meeting>the 8th International Symposium of the Digital Earth<address><addrLine>Kuching, Malaysia</addrLine></address></meeting>
		<imprint>
			<publisher>Bristol, UK</publisher>
			<date type="published" when="2013-08">August 2013. 2014</date>
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="page" from="26" to="29" />
		</imprint>
		<respStmt>
			<orgName>Univ Teknologi Malaysia</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">A neural network-based technique for change detection of linear features and its application to a Mediterranean region</title>
		<author>
			<persName><forename type="first">I</forename><surname>Feldberg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><forename type="middle">S</forename><surname>Netanyahu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Shoshany</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Geoscience and Remote Sensing Symposium</title>
		<meeting>the IEEE International Geoscience and Remote Sensing Symposium<address><addrLine>Toronto, ON, Canada</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2002-06">June 2002</date>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="1195" to="1197" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Integration of Gibbs Markov Random Field and Hopfield-Type Neural Networks for Unsupervised Change Detection in Remotely Sensed Multitemporal Images</title>
		<author>
			<persName><forename type="first">A</forename><surname>Ghosh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">N</forename><surname>Subudhi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Bruzzone</surname></persName>
		</author>
		<idno type="DOI">10.1109/TIP.2013.2259833</idno>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Image Process</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="page" from="3087" to="3096" />
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">A Context-Sensitive Technique for Unsupervised Change Detection Based on Hopfield-Type Neural Networks</title>
		<author>
			<persName><forename type="first">S</forename><surname>Ghosh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Bruzzone</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Patra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Bovolo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Ghosh</surname></persName>
		</author>
		<idno type="DOI">10.1109/TGRS.2006.888861</idno>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Geosci. Remote Sens</title>
		<imprint>
			<biblScope unit="volume">45</biblScope>
			<biblScope unit="page" from="778" to="789" />
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">An unsupervised context-sensitive change detection technique based on modified self-organizing feature map neural network</title>
		<author>
			<persName><forename type="first">S</forename><surname>Ghosh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Patra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Ghosh</surname></persName>
		</author>
		<idno type="DOI">10.1016/j.ijar.2008.01.008</idno>
	</analytic>
	<monogr>
		<title level="j">Int. J. Approx. Reason</title>
		<imprint>
			<biblScope unit="volume">50</biblScope>
			<biblScope unit="page" from="37" to="50" />
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<monogr>
		<title level="m" type="main">Object-wise joint-classification change detection for remote sensing images based on entropy query-by fuzzy ARTMAP. GISci. Remote Sens</title>
		<author>
			<persName><forename type="first">M</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhou</surname></persName>
		</author>
		<idno type="DOI">10.1080/15481603.2018.1430100</idno>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="volume">55</biblScope>
			<biblScope unit="page" from="265" to="284" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<monogr>
		<title level="m" type="main">Learning a Transferable Change Rule from a Recurrent Neural Network for Land Cover Change Detection</title>
		<author>
			<persName><forename type="first">H</forename><surname>Lyu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Mou</surname></persName>
		</author>
		<idno type="DOI">10.3390/rs8060506</idno>
		<imprint/>
	</monogr>
	<note>Remote Sens. 2016, 8, 506. [CrossRef</note>
</biblStruct>

<biblStruct xml:id="b51">
	<monogr>
		<title level="m" type="main">Long-Term Annual Mapping of Four Cities on Different Continents by Applying a Deep Information Learning Method to Landsat Data</title>
		<author>
			<persName><forename type="first">H</forename><surname>Lyu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Mou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">S</forename><surname>Wright</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><forename type="middle">X</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Yu</surname></persName>
		</author>
		<idno type="DOI">10.3390/rs10030471</idno>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="page">471</biblScope>
		</imprint>
	</monogr>
	<note>Remote Sens</note>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">A recurrent convolutional neural network for land cover change detection in multispectral images</title>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">C</forename><surname>Mou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><forename type="middle">X</forename><surname>Zhu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Igarss 2018 IEEE International Geoscience and Remote Sensing Symposium</title>
		<meeting>the Igarss 2018 IEEE International Geoscience and Remote Sensing Symposium<address><addrLine>Valencia, Spain; New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2018-07">July 2018. 2018</date>
			<biblScope unit="page" from="4363" to="4366" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">A new multispectral pixel change detection approach using pulse-coupled neural networks for change vector analysis</title>
		<author>
			<persName><forename type="first">V</forename><forename type="middle">E</forename><surname>Neagoe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">D</forename><surname>Ciotec</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">V</forename><surname>Carata</surname></persName>
		</author>
		<idno type="DOI">10.1109/igarss.2016.7729875</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2016 IEEE International Geoscience and Remote Sensing Symposium</title>
		<meeting>the 2016 IEEE International Geoscience and Remote Sensing Symposium<address><addrLine>Beijing, China; New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2016-07-15">10-15 July 2016. 2016</date>
			<biblScope unit="page" from="3386" to="3389" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">A modular neural network model for change detection in earth observation imagery</title>
		<author>
			<persName><forename type="first">V</forename><forename type="middle">E</forename><surname>Neagoe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">M</forename><surname>Stoica</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">I</forename><surname>Ciurea</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2013 IEEE International Geoscience and Remote Sensing Symposium</title>
		<meeting>the 2013 IEEE International Geoscience and Remote Sensing Symposium<address><addrLine>Melbourne, Australia; New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2013-07">July 2013. 2013</date>
			<biblScope unit="page" from="3321" to="3324" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<analytic>
		<title level="a" type="main">An inverse method for watershed change detection using hybrid conceptual and artificial intelligence approaches</title>
		<author>
			<persName><forename type="first">V</forename><surname>Nourani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Roushangar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Andalib</surname></persName>
		</author>
		<idno type="DOI">10.1016/j.jhydrol.2018.05.018</idno>
	</analytic>
	<monogr>
		<title level="j">J. Hydrol</title>
		<imprint>
			<biblScope unit="volume">562</biblScope>
			<biblScope unit="page" from="371" to="384" />
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<monogr>
		<title level="m" type="main">Unsupervised Change Detection in Remote-Sensing Images Using Modified Self-Organizing Feature Map Neural Network</title>
		<author>
			<persName><forename type="first">S</forename><surname>Patra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Ghosh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Ghosh</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2007">2007</date>
			<publisher>IEEE Computer Soc</publisher>
			<biblScope unit="page">716</biblScope>
			<pubPlace>Los Alamitos, CA, USA</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<analytic>
		<title level="a" type="main">A novel approach for change detection of remotely sensed images using semi-supervised multiple classifier system</title>
		<author>
			<persName><forename type="first">M</forename><surname>Roy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Ghosh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Ghosh</surname></persName>
		</author>
		<idno type="DOI">10.1016/j.ins.2014.01.037</idno>
	</analytic>
	<monogr>
		<title level="j">Inf. Sci</title>
		<imprint>
			<biblScope unit="volume">269</biblScope>
			<biblScope unit="page" from="35" to="47" />
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b58">
	<analytic>
		<title level="a" type="main">A Neural Approach Under Active Learning Mode for Change Detection in Remotely Sensed Images</title>
		<author>
			<persName><forename type="first">M</forename><surname>Roy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Ghosh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Ghosh</surname></persName>
		</author>
		<idno type="DOI">10.1109/JSTARS.2013.2293175</idno>
	</analytic>
	<monogr>
		<title level="j">IEEE J. Sel. Top. Appl. Earth Obs. Remote Sens</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page" from="1200" to="1206" />
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b59">
	<analytic>
		<title level="a" type="main">A new fuzzy measurement approach for automatic change detection using remotely sensed images</title>
		<author>
			<persName><forename type="first">V</forename><surname>Sadeghi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><forename type="middle">F</forename><surname>Ahmadi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Ebadi</surname></persName>
		</author>
		<idno type="DOI">10.1016/j.measurement.2018.05.097</idno>
	</analytic>
	<monogr>
		<title level="j">Measurement</title>
		<imprint>
			<biblScope unit="volume">127</biblScope>
			<biblScope unit="page" from="1" to="14" />
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b60">
	<monogr>
		<title level="m" type="main">Comparing ARTMAP Neural Network with the Maximum-Likelihood Classifier for Detecting Urban Change. Photogramm. Eng. Remote Sens</title>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">C</forename><surname>Seto</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
		<idno type="DOI">10.14358/PERS.69.9.981</idno>
		<imprint>
			<date type="published" when="2003">2003</date>
			<biblScope unit="volume">69</biblScope>
			<biblScope unit="page" from="981" to="990" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b61">
	<analytic>
		<title level="a" type="main">Detection of land use changes in NorthEastern Iran by Landsat satellite data</title>
		<author>
			<persName><forename type="first">S</forename><surname>Varamesh</surname></persName>
		</author>
		<idno type="DOI">10.15666/aeer/1503_14431454</idno>
	</analytic>
	<monogr>
		<title level="j">Appl. Ecol. Environ. Res</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="page" from="1443" to="1454" />
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b62">
	<analytic>
		<title level="a" type="main">Monitoring large areas for forest change using Landsat: Generalization across space, time and Landsat sensors</title>
		<author>
			<persName><forename type="first">C</forename><surname>Woodcock</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Macomber</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Pax-Lenney</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">B</forename><surname>Cohen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Remote Sens. Environ</title>
		<imprint>
			<biblScope unit="volume">78</biblScope>
			<biblScope unit="page" from="194" to="203" />
			<date type="published" when="2001">2001</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b63">
	<analytic>
		<title level="a" type="main">Learning Spectral-Spatial-Temporal Features via a Recurrent Convolutional Neural Network for Change Detection in Multispectral Imagery</title>
		<author>
			<persName><forename type="first">L</forename><surname>Mou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Bruzzone</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><forename type="middle">X</forename><surname>Zhu</surname></persName>
		</author>
		<idno type="DOI">10.1109/TGRS.2018.2863224</idno>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Geosci. Remote Sens</title>
		<imprint>
			<biblScope unit="volume">57</biblScope>
			<biblScope unit="page" from="924" to="935" />
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b64">
	<analytic>
		<title level="a" type="main">A spatial-temporal Hopfield neural network approach for super-resolution land cover mapping with multi-temporal different resolution remotely sensed images</title>
		<author>
			<persName><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Ling</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<idno type="DOI">10.1016/j.isprsjprs.2014.03.013</idno>
	</analytic>
	<monogr>
		<title level="j">ISPRS J. Photogramm. Remote Sens</title>
		<imprint>
			<biblScope unit="volume">93</biblScope>
			<biblScope unit="page" from="76" to="87" />
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b65">
	<analytic>
		<title level="a" type="main">Sentinel-1 and Sentinel-2 data fusion for urban change detection</title>
		<author>
			<persName><forename type="first">A</forename><surname>Benedetti</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Picchiani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Del Frate</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2018 IEEE International Geoscience and Remote Sensing Symposium</title>
		<meeting>the 2018 IEEE International Geoscience and Remote Sensing Symposium<address><addrLine>Valencia, Spain; New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2018-07">July 2018. 2018</date>
			<biblScope unit="page" from="1962" to="1965" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b66">
	<analytic>
		<title level="a" type="main">Sentinel-2 change detection based on deep features</title>
		<author>
			<persName><forename type="first">A</forename><surname>Pomente</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Picchiani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Del Frate</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2018 IEEE International Geoscience and Remote Sensing Symposium</title>
		<meeting>the 2018 IEEE International Geoscience and Remote Sensing Symposium<address><addrLine>Valencia, Spain; New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2018-07">July 2018. 2018</date>
			<biblScope unit="page" from="6859" to="6862" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b67">
	<analytic>
		<title level="a" type="main">Optical remote sensing change detection through deep siamese network</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">E A</forename><surname>Arabi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">S</forename><surname>Karoui</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Djerriri</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2018 IEEE International Geoscience and Remote Sensing Symposium</title>
		<meeting>the 2018 IEEE International Geoscience and Remote Sensing Symposium<address><addrLine>Valencia, Spain; New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2018-07">July 2018. 2018</date>
			<biblScope unit="page" from="5041" to="5044" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b68">
	<analytic>
		<title level="a" type="main">Comprehensive analysis of regional human-driven environmental change with multitemporal remote sensing images using observed object-specified dynamic Bayesian network</title>
		<author>
			<persName><forename type="first">H</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Hua</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<idno type="DOI">10.1117/1.JRS.10.016021</idno>
	</analytic>
	<monogr>
		<title level="j">J. Appl. Remote Sens</title>
		<imprint>
			<date type="published" when="2016">2016, 10, 16021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b69">
	<analytic>
		<title level="a" type="main">An Innovative Neural-Net Method to Detect Temporal Changes in High-Resolution Optical Satellite Imagery</title>
		<author>
			<persName><forename type="first">F</forename><surname>Pacifici</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Del Frate</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Solimini</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Emery</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Geosci. Remote Sens</title>
		<imprint>
			<biblScope unit="volume">45</biblScope>
			<biblScope unit="page" from="2940" to="2952" />
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b70">
	<analytic>
		<title level="a" type="main">Automatic Change Detection in Very High Resolution Images with Pulse-Coupled Neural Networks</title>
		<author>
			<persName><forename type="first">F</forename><surname>Pacifici</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Del Frate</surname></persName>
		</author>
		<idno type="DOI">10.1109/LGRS.2009.2021780</idno>
	</analytic>
	<monogr>
		<title level="j">IEEE Geosci. Remote. Sens. Lett</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page" from="58" to="62" />
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b71">
	<analytic>
		<title level="a" type="main">Unsupervised Deep Change Vector Analysis for Multiple-Change Detection in VHR Images</title>
		<author>
			<persName><forename type="first">S</forename><surname>Saha</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Bovolo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Bruzzone</surname></persName>
		</author>
		<idno type="DOI">10.1109/TGRS.2018.2886643</idno>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Geosci. Remote Sens</title>
		<imprint>
			<biblScope unit="volume">57</biblScope>
			<biblScope unit="page" from="3677" to="3693" />
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b72">
	<analytic>
		<title level="a" type="main">High-resolution optical remote sensing imagery change detection through deep transfer learning</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">E A</forename><surname>Larabi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Chaib</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Bakhti</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Hasni</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">A</forename><surname>Bouhlala</surname></persName>
		</author>
		<idno type="DOI">10.1117/1.JRS.13.046512</idno>
	</analytic>
	<monogr>
		<title level="j">J. Appl. Remote Sens</title>
		<imprint>
			<biblScope unit="volume">13</biblScope>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b73">
	<analytic>
		<title level="a" type="main">Remote Sensing Image Change Detection Based on Information Transmission and Attention Mechanism</title>
		<author>
			<persName><forename type="first">R</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
		<idno type="DOI">10.1109/ACCESS.2019.2947286</idno>
	</analytic>
	<monogr>
		<title level="j">IEEE Access</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page" from="156349" to="156359" />
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b74">
	<analytic>
		<title level="a" type="main">Change detection of land use and land cover in an urban region with SPOT-5 images and partial Lanczos extreme learning machine</title>
		<author>
			<persName><forename type="first">M</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N.-B</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L.-C</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Xu</surname></persName>
		</author>
		<idno type="DOI">10.1117/1.3518096</idno>
	</analytic>
	<monogr>
		<title level="j">J. Appl</title>
		<imprint/>
	</monogr>
	<note>Remote Sens. 2010, 4, 43551. [CrossRef</note>
</biblStruct>

<biblStruct xml:id="b75">
	<analytic>
		<title level="a" type="main">Neural Network Combination by Fuzzy Integral for Robust Change Detection in Remotely Sensed Imagery</title>
		<author>
			<persName><forename type="first">H</forename><surname>Nemmour</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Chibani</surname></persName>
		</author>
		<idno type="DOI">10.1155/ASP.2005.2187</idno>
	</analytic>
	<monogr>
		<title level="j">EURASIP J. Adv. Signal Process</title>
		<imprint>
			<biblScope unit="page" from="2187" to="2195" />
			<date type="published" when="2005">2005. 2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b76">
	<analytic>
		<title level="a" type="main">Fuzzy neural network architecture for change detection in remotely sensed imagery</title>
		<author>
			<persName><forename type="first">H</forename><surname>Nemmour</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Chibani</surname></persName>
		</author>
		<idno type="DOI">10.1080/01431160500275648</idno>
	</analytic>
	<monogr>
		<title level="j">Int. J. Remote Sens</title>
		<imprint>
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="page" from="705" to="717" />
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b77">
	<analytic>
		<title level="a" type="main">Unsupervised change detection method based on saliency analysis and convolutional neural network</title>
		<author>
			<persName><forename type="first">D</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Guan</surname></persName>
		</author>
		<idno type="DOI">10.1117/1.JRS.13.024512</idno>
	</analytic>
	<monogr>
		<title level="j">J. Appl. Remote Sens</title>
		<imprint>
			<biblScope unit="volume">13</biblScope>
			<date type="published" when="2019">2019. 024512</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b78">
	<analytic>
		<title level="a" type="main">Unsupervised Difference Representation Learning for Detecting Multiple Types of Changes in Multitemporal Remote Sensing Images</title>
		<author>
			<persName><forename type="first">P</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Ban</surname></persName>
		</author>
		<idno type="DOI">10.1109/TGRS.2018.2872509</idno>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Geosci. Remote Sens</title>
		<imprint>
			<biblScope unit="volume">57</biblScope>
			<biblScope unit="page" from="2277" to="2289" />
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b79">
	<analytic>
		<title level="a" type="main">A Novel Joint Change Detection Approach Based on Weight-Clustering Sparse Autoencoders</title>
		<author>
			<persName><forename type="first">J</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Han</surname></persName>
		</author>
		<idno type="DOI">10.1109/JSTARS.2019.2892951</idno>
	</analytic>
	<monogr>
		<title level="j">IEEE J. Sel. Top. Appl. Earth Obs. Remote Sens</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="page" from="685" to="699" />
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b80">
	<analytic>
		<title level="a" type="main">Comparison of four machine learning methods for object-oriented change detection in high-resolution satellite imagery</title>
		<author>
			<persName><forename type="first">T</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Mippr 2017: Remote Sensing Image Processing, Geographic Information Systems, and Other Applications</title>
		<editor>
			<persName><forename type="first">N</forename><surname>Sang</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">J</forename><surname>Ma</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Z</forename><surname>Chen</surname></persName>
		</editor>
		<meeting><address><addrLine>Bellingham, WA, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Spie-Int Soc Optical Engineering</publisher>
			<date type="published" when="2018">2018</date>
			<biblScope unit="volume">10611</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b81">
	<analytic>
		<title level="a" type="main">Unsupervised multiple-change detection in VHR optical images using deep features</title>
		<author>
			<persName><forename type="first">S</forename><surname>Saha</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Bovolo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Bruzzone</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2018 IEEE International Geoscience and Remote Sensing Symposium</title>
		<meeting>the 2018 IEEE International Geoscience and Remote Sensing Symposium<address><addrLine>Valencia, Spain; New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2018-07">July 2018. 2018</date>
			<biblScope unit="page" from="1902" to="1905" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b82">
	<analytic>
		<title level="a" type="main">A Generative Discriminatory Classified Network for Change Detection in Multispectral Imagery</title>
		<author>
			<persName><forename type="first">M</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Zhan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Niu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE J. Sel. Top. Appl. Earth Obs. Remote Sens</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="page" from="321" to="333" />
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b83">
	<analytic>
		<title level="a" type="main">Superpixel-Based Difference Representation Learning for Change Detection in Multispectral Remote Sensing Images</title>
		<author>
			<persName><forename type="first">M</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Zhan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Miao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Geosci. Remote. Sens</title>
		<imprint>
			<biblScope unit="volume">55</biblScope>
			<biblScope unit="page" from="2658" to="2673" />
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b84">
	<analytic>
		<title level="a" type="main">Automatic building change image quality assessment in high resolution remote sensing based on deep learning</title>
		<author>
			<persName><forename type="first">F</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Feng</surname></persName>
		</author>
		<idno type="DOI">10.1016/j.jvcir.2019.102585</idno>
	</analytic>
	<monogr>
		<title level="j">J. Vis. Commun. Image Represent</title>
		<imprint>
			<biblScope unit="volume">63</biblScope>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b85">
	<analytic>
		<title level="a" type="main">Building change detection via a combination of CNNs using only RGB aerial imageries</title>
		<author>
			<persName><forename type="first">K</forename><surname>Nemoto</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Imaizumi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Hikosaka</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Hamaguchi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Sato</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Fujita</surname></persName>
		</author>
		<idno type="DOI">10.1117/12.2277912</idno>
	</analytic>
	<monogr>
		<title level="j">Remote Sens. Technol. Appl. Urban Environ</title>
		<imprint>
			<date type="published" when="2017">2017. 10431</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b86">
	<analytic>
		<title level="a" type="main">Aerial image change detection using dual regions of interest networks</title>
		<author>
			<persName><forename type="first">P</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Leng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Bu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Li</surname></persName>
		</author>
		<idno type="DOI">10.1016/j.neucom.2019.04.029</idno>
	</analytic>
	<monogr>
		<title level="j">Neurocomputing</title>
		<imprint>
			<biblScope unit="volume">349</biblScope>
			<biblScope unit="page" from="190" to="201" />
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b87">
	<analytic>
		<title level="a" type="main">Fully Convolutional Networks for Multisource Building Extraction from an Open Aerial and Satellite Imagery Data Set</title>
		<author>
			<persName><forename type="first">S</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Lu</surname></persName>
		</author>
		<idno type="DOI">10.1109/TGRS.2018.2858817</idno>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Geosci. Remote Sens</title>
		<imprint>
			<biblScope unit="volume">57</biblScope>
			<biblScope unit="page" from="574" to="586" />
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b88">
	<analytic>
		<title level="a" type="main">Building Instance Change Detection from Large-Scale Aerial Images using Convolutional Neural Networks and Simulated Samples</title>
		<author>
			<persName><forename type="first">S</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<idno type="DOI">10.3390/rs11111343</idno>
	</analytic>
	<monogr>
		<title level="j">Remote Sens</title>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<date type="published" when="1343">2019. 1343</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b89">
	<analytic>
		<title level="a" type="main">A deep learning approach to detecting changes in buildings from aerial images</title>
		<author>
			<persName><forename type="first">B</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G.-Z</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q.-H</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Symposium on Neural Networks</title>
		<meeting>the International Symposium on Neural Networks<address><addrLine>Moscow, Russia</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2019-07-12">10-12 July 2019</date>
			<biblScope unit="page" from="414" to="421" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b90">
	<analytic>
		<title level="a" type="main">Detecting Building Changes between Airborne Laser Scanning and Photogrammetric Data</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Vosselman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Gerke</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Persello</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Tuia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">Y</forename><surname>Yang</surname></persName>
		</author>
		<idno type="DOI">10.3390/rs11202417</idno>
	</analytic>
	<monogr>
		<title level="j">Remote Sens</title>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<date type="published" when="2019">2019. 2417</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b91">
	<analytic>
		<title level="a" type="main">Damage detection from aerial images via convolutional neural networks</title>
		<author>
			<persName><forename type="first">A</forename><surname>Fujita</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Sakurada</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Imaizumi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Ito</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Hikosaka</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Nakamura</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2017 Fifteenth IAPR International Conference on Machine Vision Applications (MVA)</title>
		<meeting>the 2017 Fifteenth IAPR International Conference on Machine Vision Applications (MVA)<address><addrLine>Nagoya Univ, Nagoya, Japan</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2017-05">May 2017</date>
			<biblScope unit="page" from="5" to="8" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b92">
	<analytic>
		<title level="a" type="main">GAN-Based Siamese Framework for Landslide Inventory Mapping Using Bi-Temporal Optical Remote Sensing Images</title>
		<author>
			<persName><forename type="first">B</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Kou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Geosci. Remote Sens. Lett</title>
		<imprint>
			<biblScope unit="volume">2020</biblScope>
			<biblScope unit="page" from="1" to="5" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b93">
	<monogr>
		<title level="m" type="main">Pyramid Feature-Based Attention-Guided Siamese Network for Remote Sensing Orthoimagery Building Change Detection</title>
		<author>
			<persName><forename type="first">H</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><surname>Pga-Siamnet</surname></persName>
		</author>
		<idno type="DOI">10.3390/rs12030484</idno>
		<imprint/>
	</monogr>
	<note>Remote Sens. 2020, 12, 484. [CrossRef</note>
</biblStruct>

<biblStruct xml:id="b94">
	<analytic>
		<title level="a" type="main">Fusion Network for Change Detection of High-Resolution Panchromatic Imagery</title>
		<author>
			<persName><forename type="first">W</forename><surname>Wiratama</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Sim</surname></persName>
		</author>
		<idno type="DOI">10.3390/app9071441</idno>
	</analytic>
	<monogr>
		<title level="j">Appl. Sci</title>
		<imprint>
			<date type="published" when="1441">2019, 9, 1441</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b95">
	<analytic>
		<title level="a" type="main">An introduction to synthetic aperture radar (SAR)</title>
		<author>
			<persName><forename type="first">Y</forename><forename type="middle">K</forename><surname>Chan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><forename type="middle">C</forename><surname>Koo</surname></persName>
		</author>
		<idno type="DOI">10.2528/PIERB07110101</idno>
	</analytic>
	<monogr>
		<title level="j">Prog. Electromagn. Res. B</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="27" to="60" />
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b96">
	<analytic>
		<title level="a" type="main">A novel change detection framework based on deep learning for the analysis of multi-temporal polarimetric SAR images</title>
		<author>
			<persName><forename type="first">S</forename><surname>De</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Pirrone</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Bovolo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Bruzzone</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Bhattacharya</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2017 IEEE International Geoscience and Remote Sensing Symposium</title>
		<meeting>the 2017 IEEE International Geoscience and Remote Sensing Symposium<address><addrLine>Fort Worth, TX, USA; New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2017-07">July 2017. 2017</date>
			<biblScope unit="page" from="5193" to="5196" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b97">
	<analytic>
		<title level="a" type="main">Fast unsupervised deep fusion network for change detection of multitemporal SAR images</title>
		<author>
			<persName><forename type="first">H</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Jiao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Hou</surname></persName>
		</author>
		<idno type="DOI">10.1016/j.neucom.2018.11.077</idno>
	</analytic>
	<monogr>
		<title level="j">Neurocomputing</title>
		<imprint>
			<biblScope unit="volume">332</biblScope>
			<biblScope unit="page" from="56" to="70" />
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b98">
	<analytic>
		<title level="a" type="main">Change Detection of SAR Images Based on Supervised Contractive Autoencoders and Fuzzy Clustering</title>
		<author>
			<persName><forename type="first">J</forename><surname>Geng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">C</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><forename type="middle">R</forename><surname>Ma</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Workshop on Remote Sensing with Intelligent Processing</title>
		<editor>
			<persName><forename type="first">Hai</forename><surname>Shang</surname></persName>
		</editor>
		<meeting>the International Workshop on Remote Sensing with Intelligent Processing<address><addrLine>China; New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2017-05">May 2017. 2017</date>
			<biblScope unit="page" from="18" to="21" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b99">
	<analytic>
		<title level="a" type="main">Feature learning and change feature classification based on deep learning for ternary change detection in SAR images</title>
		<author>
			<persName><forename type="first">M</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Zhang</surname></persName>
		</author>
		<idno type="DOI">10.1016/j.isprsjprs.2017.05.001</idno>
	</analytic>
	<monogr>
		<title level="j">ISPRS J. Photogramm. Remote Sens</title>
		<imprint>
			<biblScope unit="volume">129</biblScope>
			<biblScope unit="page" from="212" to="225" />
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b100">
	<analytic>
		<title level="a" type="main">Multiscale Superpixel Segmentation with Deep Features for Change Detection</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Lei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Lei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<idno type="DOI">10.1109/ACCESS.2019.2902613</idno>
	</analytic>
	<monogr>
		<title level="j">IEEE Access</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page" from="36600" to="36616" />
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b101">
	<analytic>
		<title level="a" type="main">Spatial fuzzy clustering and deep auto-encoder for unsupervised change detection in synthetic aperture radar images</title>
		<author>
			<persName><forename type="first">Y</forename><forename type="middle">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">H</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">C</forename><surname>Jiao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2018 IEEE International Geoscience and Remote Sensing Symposium</title>
		<meeting>the 2018 IEEE International Geoscience and Remote Sensing Symposium<address><addrLine>Valencia, Spain; New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2018-07">July 2018. 2018</date>
			<biblScope unit="page" from="4479" to="4482" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b102">
	<analytic>
		<title level="a" type="main">Deep Learning and Superpixel Feature Extraction Based on Contractive Autoencoder for Change Detection in SAR Images</title>
		<author>
			<persName><forename type="first">N</forename><surname>Lv</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Qiu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">K</forename><surname>Sangaiah</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Ind. Inform</title>
		<imprint>
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="page" from="5530" to="5538" />
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b103">
	<analytic>
		<title level="a" type="main">Temporal Change Detection in SAR Images Using Log Cumulants and Stacked Autoencoder</title>
		<author>
			<persName><forename type="first">P</forename><surname>Planinšič</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Gleich</surname></persName>
		</author>
		<idno type="DOI">10.1109/LGRS.2017.2786344</idno>
	</analytic>
	<monogr>
		<title level="j">IEEE Geosci. Remote Sens. Lett</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="page" from="297" to="301" />
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b104">
	<analytic>
		<title level="a" type="main">Fuzzy autoencoder for multiple change detection in remote sensing images</title>
		<author>
			<persName><forename type="first">L</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Cao</surname></persName>
		</author>
		<idno type="DOI">10.1117/1.JRS.12.035014</idno>
	</analytic>
	<monogr>
		<title level="j">J. Appl. Remote Sens</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="page">35014</biblScope>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b105">
	<analytic>
		<title level="a" type="main">Detecting multiple changes from multi-temporal images by using stacked denosing autoencoder based change vector analysis</title>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">Z</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">G</forename><surname>Gong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2016 International Joint Conference on Neural Networks</title>
		<meeting>the 2016 International Joint Conference on Neural Networks<address><addrLine>Vancouver, Canada; New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2016-07">July 2016. 2016</date>
			<biblScope unit="page" from="1269" to="1276" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b106">
	<monogr>
		<title level="m" type="main">Differentially Deep Subspace Representation for Unsupervised Change Detection of SAR Images</title>
		<author>
			<persName><forename type="first">B</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<idno type="DOI">10.3390/rs11232740</idno>
		<imprint>
			<date type="published" when="2019">2019. 2740</date>
			<biblScope unit="volume">11</biblScope>
		</imprint>
	</monogr>
	<note>Remote Sens</note>
</biblStruct>

<biblStruct xml:id="b107">
	<analytic>
		<title level="a" type="main">Local Descriptor Learning for Change Detection in Synthetic Aperture Radar Images via Convolutional Neural Networks</title>
		<author>
			<persName><forename type="first">H</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Jiao</surname></persName>
		</author>
		<idno type="DOI">10.1109/ACCESS.2018.2889326</idno>
	</analytic>
	<monogr>
		<title level="j">IEEE Access</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page" from="15389" to="15403" />
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b108">
	<analytic>
		<title level="a" type="main">Change detection in multitemporal synthetic aperture radar images using dual-channel convolutional neural network</title>
		<author>
			<persName><forename type="first">T</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Shen</surname></persName>
		</author>
		<idno type="DOI">10.1117/1.JRS.11.042615</idno>
	</analytic>
	<monogr>
		<title level="j">J. Appl. Remote Sens</title>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="issue">1</biblScope>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b109">
	<analytic>
		<title level="a" type="main">Destroyed-buildings detection from VHR SAR images using deep features</title>
		<author>
			<persName><forename type="first">S</forename><surname>Saha</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Bovolo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Bruzzone</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In Image and Signal Processing for Remote Sensing aXxiv</title>
		<editor>
			<persName><forename type="first">L</forename><surname>Bruzzone</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">F</forename><surname>Bovolo</surname></persName>
		</editor>
		<imprint>
			<biblScope unit="volume">10789</biblScope>
			<date type="published" when="2018">2018</date>
			<publisher>Spie-Int Soc Optical Engineering</publisher>
			<pubPlace>Bellingham, WA, USA</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b110">
	<analytic>
		<title level="a" type="main">A Deep Learning Method for Change Detection in Synthetic Aperture Radar Images</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Jiao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Shang</surname></persName>
		</author>
		<idno type="DOI">10.1109/TGRS.2019.2901945</idno>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Geosci. Remote Sens</title>
		<imprint>
			<biblScope unit="volume">57</biblScope>
			<biblScope unit="page" from="5751" to="5763" />
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b111">
	<analytic>
		<title level="a" type="main">Newly Built Construction Detection in SAR Images Using Deep Learning</title>
		<author>
			<persName><forename type="first">R</forename><surname>Jaturapitpornchai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Matsuoka</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Kanemoto</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Kuzuoka</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Ito</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Nakamura</surname></persName>
		</author>
		<idno type="DOI">10.3390/rs11121444</idno>
	</analytic>
	<monogr>
		<title level="j">Remote Sens</title>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<date type="published" when="1444">2019. 1444</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b112">
	<analytic>
		<title level="a" type="main">An Unsupervised SAR Change Detection Method Based on Stochastic Subspace Ensemble Learning</title>
		<author>
			<persName><forename type="first">B</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Wu</surname></persName>
		</author>
		<idno type="DOI">10.3390/rs11111314</idno>
	</analytic>
	<monogr>
		<title level="j">Remote Sens</title>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<date type="published" when="1314">2019. 1314</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b113">
	<analytic>
		<title level="a" type="main">Local Restricted Convolutional Neural Network for Change Detection in Polarimetric SAR Images</title>
		<author>
			<persName><forename type="first">F</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Jiao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Hou</surname></persName>
		</author>
		<idno type="DOI">10.1109/TNNLS.2018.2847309</idno>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Neural Netw. Learn. Syst</title>
		<imprint>
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="page" from="818" to="833" />
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b114">
	<monogr>
		<title level="m" type="main">Learning to measure change: Fully convolutional Siamese metric networks for scene change detection</title>
		<author>
			<persName><forename type="first">E</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1810.09111</idno>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b115">
	<analytic>
		<title level="a" type="main">Change detection tool based on GSV to help training</title>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">G</forename><surname>Huélamo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">F</forename><surname>Alcantarilla</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">M</forename><surname>Bergasa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>López-Guillén</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Workshop of Physical Agents</title>
		<meeting>the Workshop of Physical Agents<address><addrLine>Madrid, Spain</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2018-11">November 2018</date>
			<biblScope unit="page" from="115" to="131" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b116">
	<analytic>
		<title level="a" type="main">ChangeNet: A deep learning architecture for visual change detection</title>
		<author>
			<persName><forename type="first">A</forename><surname>Varghese</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Gubbi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Ramaswamy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Balamuralidhar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV)</title>
		<meeting>the European Conference on Computer Vision (ECCV)<address><addrLine>Munich, Germany</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2018-09">September 2018</date>
			<biblScope unit="page" from="129" to="145" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b117">
	<monogr>
		<title level="m" type="main">Dense optical flow based change detection network robust to difference of camera viewpoints</title>
		<author>
			<persName><forename type="first">K</forename><surname>Sakurada</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Kawaguchi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Nakamura</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1712.02941</idno>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b118">
	<analytic>
		<title level="a" type="main">Change detection from a street image pair using CNN features and superpixel segmentation</title>
		<author>
			<persName><forename type="first">K</forename><surname>Sakurada</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Okatani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the British Machine Vision Conference (BMVC)</title>
		<meeting>the British Machine Vision Conference (BMVC)<address><addrLine>Swansea, UK</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2015-09">September 2015</date>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="page" from="61" to="61" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b119">
	<analytic>
		<title level="a" type="main">Mask-CDNet: A mask based pixel change detection network</title>
		<author>
			<persName><forename type="first">S</forename><surname>Bu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Leng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Li</surname></persName>
		</author>
		<idno type="DOI">10.1016/j.neucom.2019.10.022</idno>
	</analytic>
	<monogr>
		<title level="j">Neurocomputing</title>
		<imprint>
			<biblScope unit="volume">378</biblScope>
			<biblScope unit="page" from="166" to="178" />
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b120">
	<analytic>
		<title level="a" type="main">A Deep Convolutional Coupling Network for Change Detection Based on Heterogeneous Optical and Radar Images</title>
		<author>
			<persName><forename type="first">J</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Qin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Neural Netw. Learn. Syst</title>
		<imprint>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="page" from="545" to="559" />
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b121">
	<analytic>
		<title level="a" type="main">Log-Based Transformation Feature Learning for Change Detection in Heterogeneous Images</title>
		<author>
			<persName><forename type="first">T</forename><surname>Zhan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Li</surname></persName>
		</author>
		<idno type="DOI">10.1109/LGRS.2018.2843385</idno>
	</analytic>
	<monogr>
		<title level="j">IEEE Geosci. Remote Sens. Lett</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="page" from="1352" to="1356" />
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b122">
	<analytic>
		<title level="a" type="main">Iterative feature mapping network for detecting multiple changes in multi-source remote sensing images</title>
		<author>
			<persName><forename type="first">T</forename><surname>Zhan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Zhang</surname></persName>
		</author>
		<idno type="DOI">10.1016/j.isprsjprs.2018.09.002</idno>
	</analytic>
	<monogr>
		<title level="j">ISPRS J. Photogramm. Remote Sens</title>
		<imprint>
			<biblScope unit="volume">146</biblScope>
			<biblScope unit="page" from="38" to="51" />
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b123">
	<monogr>
		<title level="m" type="main">Change Detection in Remote Sensing Images Based on Image Mapping and a Deep Capsule Network</title>
		<author>
			<persName><forename type="first">W</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X.-R</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Jiao</surname></persName>
		</author>
		<idno type="DOI">10.3390/rs11060626</idno>
		<imprint/>
	</monogr>
	<note>Remote Sens. 2019, 11, 626. [CrossRef</note>
</biblStruct>

<biblStruct xml:id="b124">
	<analytic>
		<title level="a" type="main">Transferred Deep Learning-Based Change Detection in Remote Sensing Images</title>
		<author>
			<persName><forename type="first">M</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Jiao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Hou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Yang</surname></persName>
		</author>
		<idno type="DOI">10.1109/TGRS.2019.2909781</idno>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Geosci. Remote Sens</title>
		<imprint>
			<biblScope unit="volume">57</biblScope>
			<biblScope unit="page" from="6960" to="6973" />
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b125">
	<analytic>
		<title level="a" type="main">A coupling translation network for change detection in heterogeneous images</title>
		<author>
			<persName><forename type="first">M</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Niu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Zhan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Zhang</surname></persName>
		</author>
		<idno type="DOI">10.1080/01431161.2018.1547934</idno>
	</analytic>
	<monogr>
		<title level="j">Int. J. Remote Sens</title>
		<imprint>
			<biblScope unit="volume">40</biblScope>
			<biblScope unit="page" from="3647" to="3672" />
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b126">
	<analytic>
		<title level="a" type="main">A Conditional Adversarial Network for Change Detection in Heterogeneous Images</title>
		<author>
			<persName><forename type="first">X</forename><surname>Niu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Zhan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
		<idno type="DOI">10.1109/LGRS.2018.2868704</idno>
	</analytic>
	<monogr>
		<title level="j">IEEE Geosci. Remote Sens. Lett</title>
		<imprint>
			<biblScope unit="volume">16</biblScope>
			<biblScope unit="page" from="45" to="49" />
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b127">
	<analytic>
		<title level="a" type="main">Detecting Large-Scale Urban Land Cover Changes from Very High Resolution Remote Sensing Images Using CNN-Based Classification</title>
		<author>
			<persName><forename type="first">C</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Lu</surname></persName>
		</author>
		<idno type="DOI">10.3390/ijgi8040189</idno>
	</analytic>
	<monogr>
		<title level="j">ISPRS Int. J. Geo Inf</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page">189</biblScope>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b128">
	<analytic>
		<title level="a" type="main">Automated Landslides Detection for Mountain Cities Using Multi-Temporal Remote Sensing Imagery</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Ouyang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Ma</surname></persName>
		</author>
		<idno type="DOI">10.3390/s18030821</idno>
	</analytic>
	<monogr>
		<title level="j">Sensors</title>
		<imprint>
			<biblScope unit="volume">18</biblScope>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b129">
	<analytic>
		<title level="a" type="main">A method of detecting land use change of remote sensing images based on texture features and DEM</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">M</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">T</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">C</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">L</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Conference on Intelligent Earth Observing and Applications</title>
		<editor>
			<persName><forename type="first">G</forename><surname>Zhou</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">C</forename><surname>Kang</surname></persName>
		</editor>
		<meeting>the International Conference on Intelligent Earth Observing and Applications<address><addrLine>Guilin, China; Bellingham, WA, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Spie-Int Soc Optical Engineering</publisher>
			<date type="published" when="2015-10">October 2015. 2015</date>
			<biblScope unit="volume">9808</biblScope>
			<biblScope unit="page" from="23" to="24" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b130">
	<analytic>
		<title level="a" type="main">CNN-based generation of high-accuracy urban distribution maps utilising SAR satellite imagery for short-term change monitoring</title>
		<author>
			<persName><forename type="first">S</forename><surname>Iino</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Ito</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Doi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Imaizumi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Hikosaka</surname></persName>
		</author>
		<idno type="DOI">10.1080/19479832.2018.1491897</idno>
	</analytic>
	<monogr>
		<title level="j">Int. J. Image Data Fusion</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="page" from="302" to="318" />
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b131">
	<analytic>
		<title level="a" type="main">Changedetection. net: A new change detection benchmark dataset</title>
		<author>
			<persName><forename type="first">N</forename><surname>Goyette</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P.-M</forename><surname>Jodoin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Porikli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Konrad</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Ishwar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2012 IEEE Computer Society Conference on Computer Vision and Pattern Recognition Workshops</title>
		<meeting>the 2012 IEEE Computer Society Conference on Computer Vision and Pattern Recognition Workshops<address><addrLine>Providence, RI, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2012-06">June 2012</date>
			<biblScope unit="page" from="1" to="8" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b132">
	<analytic>
		<title level="a" type="main">CDnet 2014: An Expanded Change Detection Benchmark Dataset</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P.-M</forename><surname>Jodoin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Porikli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Konrad</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Benezeth</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Ishwar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2014 IEEE Conference on Computer Vision and Pattern Recognition Workshops</title>
		<meeting>the 2014 IEEE Conference on Computer Vision and Pattern Recognition Workshops<address><addrLine>Columbus, OH, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2014-06">June 2014</date>
			<biblScope unit="page" from="393" to="400" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b133">
	<analytic>
		<title level="a" type="main">A Novel Video Dataset for Change Detection Benchmarking</title>
		<author>
			<persName><forename type="first">N</forename><surname>Goyette</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P.-M</forename><surname>Jodoin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Porikli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Konrad</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Ishwar</surname></persName>
		</author>
		<idno type="DOI">10.1109/TIP.2014.2346013</idno>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Image Process</title>
		<imprint>
			<biblScope unit="volume">23</biblScope>
			<biblScope unit="page" from="4663" to="4679" />
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b134">
	<monogr>
		<title level="m" type="main">Hyperspectral Change Detection Dataset</title>
		<ptr target="https://citius.usc.es/investigacion/datasets/hyperspectral-change-detection-dataset" />
		<imprint>
			<date type="published" when="2020-05">May 2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b135">
	<analytic>
		<title level="a" type="main">Multitask learning for large-scale semantic change detection</title>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">C</forename><surname>Daudt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Le Saux</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Boulch</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Gousseau</surname></persName>
		</author>
		<idno type="DOI">10.1016/j.cviu.2019.07.003</idno>
	</analytic>
	<monogr>
		<title level="j">Comput. Vis. Image Underst</title>
		<imprint>
			<biblScope unit="volume">187</biblScope>
			<date type="published" when="2019">2019. 102783</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b136">
	<analytic>
		<title level="a" type="main">Change Detection in Optical Aerial Images by a Multilayer Conditional Mixed Markov Model</title>
		<author>
			<persName><forename type="first">C</forename><surname>Benedek</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Sziranyi</surname></persName>
		</author>
		<idno type="DOI">10.1109/TGRS.2009.2022633</idno>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Geosci. Remote Sens</title>
		<imprint>
			<biblScope unit="volume">47</biblScope>
			<biblScope unit="page" from="3416" to="3430" />
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b137">
	<analytic>
		<title level="a" type="main">A Mixed Markov model for change detection in aerial photos with large time differences</title>
		<author>
			<persName><forename type="first">C</forename><surname>Benedek</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Sziranyi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2008 19th International Conference on Pattern Recognition</title>
		<meeting>the 2008 19th International Conference on Pattern Recognition<address><addrLine>Tampa, FL, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2008-12-11">8-11 December 2008</date>
			<biblScope unit="page" from="1" to="4" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b138">
	<analytic>
		<title level="a" type="main">Urban change detection for multispectral earth observation using convolutional neural networks</title>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">C</forename><surname>Daudt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Le Saux</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Boulch</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Gousseau</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IGARSS 2018 IEEE International Geoscience and Remote Sensing Symposium</title>
		<meeting>the IGARSS 2018 IEEE International Geoscience and Remote Sensing Symposium<address><addrLine>Valencia, Spain</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2018-07">July 2018</date>
			<biblScope unit="page" from="2115" to="2118" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b139">
	<analytic>
		<title level="a" type="main">A Feature Difference Convolutional Neural Network-Based Change Detection Method</title>
		<author>
			<persName><forename type="first">M</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Shi</surname></persName>
		</author>
		<idno type="DOI">10.1109/tgrs.2020.2981051</idno>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Geosci. Remote Sens</title>
		<imprint>
			<biblScope unit="page" from="1" to="15" />
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b140">
	<analytic>
		<title level="a" type="main">A scene change detection framework for multi-temporal very high resolution remote sensing images</title>
		<author>
			<persName><forename type="first">C</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
		<idno type="DOI">10.1016/j.sigpro.2015.09.020</idno>
	</analytic>
	<monogr>
		<title level="m">Signal Process</title>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="volume">124</biblScope>
			<biblScope unit="page" from="184" to="197" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b141">
	<analytic>
		<title level="a" type="main">Creating xBD: A dataset for assessing building damage from satellite imagery</title>
		<author>
			<persName><forename type="first">R</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Goodman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Patel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Hosfelt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Sajeev</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Heim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Doshi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Lucas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Choset</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Gaston</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition Workshops</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition Workshops<address><addrLine>Long Beach, CA, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2019-06">June 2019</date>
			<biblScope unit="page" from="10" to="17" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b142">
	<analytic>
		<title level="a" type="main">Constrained optical flow for aerial image change detection</title>
		<author>
			<persName><forename type="first">N</forename><surname>Bourdis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Marraud</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Sahbi</surname></persName>
		</author>
		<idno type="DOI">10.1109/igarss.2011.6050150</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2011 IEEE International Geoscience and Remote Sensing Symposium</title>
		<meeting>the 2011 IEEE International Geoscience and Remote Sensing Symposium<address><addrLine>Vancouver, BC, Canada</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2011-07">July 2011</date>
			<biblScope unit="page" from="4176" to="4179" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b143">
	<analytic>
		<title level="a" type="main">Change detection in remote sensing images using conditional adversarial networks</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">A</forename><surname>Lebedev</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><forename type="middle">V</forename><surname>Vizilter</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><forename type="middle">V</forename><surname>Vygolov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><forename type="middle">A</forename><surname>Knyaz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">Y</forename><surname>Rubis</surname></persName>
		</author>
		<idno type="DOI">10.5194/isprs-archives-XLII-2-565-2018</idno>
	</analytic>
	<monogr>
		<title level="j">ISPRS Int. Arch. Photogramm. Remote Sens. Spat. Inf. Sci</title>
		<imprint>
			<biblScope unit="page" from="565" to="571" />
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b144">
	<analytic>
		<title level="a" type="main">Street-view change detection with deconvolutional networks</title>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">F</forename><surname>Alcantarilla</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Stent</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Ros</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Arroyo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Gherardi</surname></persName>
		</author>
		<idno type="DOI">10.1007/s10514-018-9734-5</idno>
	</analytic>
	<monogr>
		<title level="j">Auton. Robot</title>
		<imprint>
			<biblScope unit="volume">42</biblScope>
			<biblScope unit="page" from="1301" to="1322" />
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b145">
	<analytic>
		<title level="a" type="main">Detecting changes in 3D structure of a scene from multi-view images captured by a vehicle-mounted camera</title>
		<author>
			<persName><forename type="first">K</forename><surname>Sakurada</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Okatani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Deguchi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition<address><addrLine>Portland, OR, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2013-06">June 2013</date>
			<biblScope unit="page" from="137" to="144" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b146">
	<analytic>
		<title level="a" type="main">Learning to compare image patches via convolutional neural networks</title>
		<author>
			<persName><forename type="first">S</forename><surname>Zagoruyko</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Komodakis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2015 IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the 2015 IEEE Conference on Computer Vision and Pattern Recognition<address><addrLine>Boston, MA, USA; New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2015-06">June 2015. 2015</date>
			<biblScope unit="page" from="4353" to="4361" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b147">
	<analytic>
		<title level="a" type="main">Difference representation learning using stacked restricted Boltzmann machines for change detection in SAR images</title>
		<author>
			<persName><forename type="first">J</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Jiao</surname></persName>
		</author>
		<idno type="DOI">10.1007/s00500-014-1460-0</idno>
	</analytic>
	<monogr>
		<title level="j">Soft Comput</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="page" from="4645" to="4657" />
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b148">
	<analytic>
		<title level="a" type="main">Improving change detection methods of SAR images using fractals</title>
		<author>
			<persName><forename type="first">H</forename><surname>Aghababaee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Amini</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Tzeng</surname></persName>
		</author>
		<idno type="DOI">10.1016/j.scient.2012.11.006</idno>
	</analytic>
	<monogr>
		<title level="j">Sci. Iran</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="page" from="15" to="22" />
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b149">
	<analytic>
		<title level="a" type="main">Remote sensing of forest change using artificial neural networks</title>
		<author>
			<persName><forename type="first">S</forename><surname>Gopal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Woodcock</surname></persName>
		</author>
		<idno type="DOI">10.1109/36.485117</idno>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Geosci. Remote Sens</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="page" from="398" to="404" />
			<date type="published" when="1996">1996</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b150">
	<analytic>
		<title level="a" type="main">Combining iterative slow feature analysis and deep feature learning for change detection in high-resolution remote sensing images</title>
		<author>
			<persName><forename type="first">J</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Appl. Remote Sens</title>
		<imprint>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="page">24506</biblScope>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b151">
	<analytic>
		<title level="a" type="main">A k-nearest neighbor approach to improve change detection from remote sensing: Application to optical aerial images</title>
		<author>
			<persName><forename type="first">A</forename><surname>Touazi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Bouchaffra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Abraham</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">M</forename><surname>Alimi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Haqiq</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">O</forename><surname>Barbosa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Benamar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2015 15th International Conference on Intelligent Systems Design and Applications</title>
		<editor>
			<persName><forename type="first">A</forename><surname>Berqia</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">M</forename><surname>Benhalima</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">A</forename><forename type="middle">M</forename><surname>Muda</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">K</forename><surname>Ma</surname></persName>
		</editor>
		<meeting>the 2015 15th International Conference on Intelligent Systems Design and Applications<address><addrLine>Marrakech, Morocco; New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2015-12-16">14-16 December 2015. 2015</date>
			<biblScope unit="page" from="98" to="103" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b152">
	<analytic>
		<title level="a" type="main">Change Detection from Synthetic Aperture Radar Images Based on Channel Weighting-Based Deep Cascade Network</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Wang</surname></persName>
		</author>
		<idno type="DOI">10.1109/JSTARS.2019.2953128</idno>
	</analytic>
	<monogr>
		<title level="j">IEEE J. Sel. Top. Appl. Earth Obs. Remote Sens</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="page" from="4517" to="4529" />
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b153">
	<analytic>
		<title level="a" type="main">Change Detection in SAR Images Based on Deep Learning</title>
		<author>
			<persName><forename type="first">H</forename><surname>Keshk</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X.-C</forename><surname>Yin</surname></persName>
		</author>
		<idno type="DOI">10.1007/s42405-019-00222-0</idno>
	</analytic>
	<monogr>
		<title level="j">Int. J. Aeronaut. Space Sci</title>
		<imprint>
			<biblScope unit="page" from="1" to="11" />
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b154">
	<analytic>
		<title level="a" type="main">Triplet-Based Semantic Relation Learning for Aerial Remote Sensing Image Change Detection</title>
		<author>
			<persName><forename type="first">M</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Sun</surname></persName>
		</author>
		<idno type="DOI">10.1109/LGRS.2018.2869608</idno>
	</analytic>
	<monogr>
		<title level="j">IEEE Geosci. Remote Sens. Lett</title>
		<imprint>
			<biblScope unit="volume">16</biblScope>
			<biblScope unit="page" from="266" to="270" />
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b155">
	<analytic>
		<title level="a" type="main">Change detection from unlabeled remote sensing images using siamese ANN</title>
		<author>
			<persName><forename type="first">R</forename><surname>Hedjam</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Abdesselam</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Melgani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IGARSS 2019-2019 IEEE International Geoscience and Remote Sensing Symposium</title>
		<meeting>the IGARSS 2019-2019 IEEE International Geoscience and Remote Sensing Symposium<address><addrLine>Yokohama, Japan</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2019-08-02">28 July-2 August 2019</date>
			<biblScope unit="page" from="1530" to="1533" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b156">
	<analytic>
		<title level="a" type="main">Change detection of remote sensing image based on deep neural networks</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Chu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Hayat</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2016 2nd International Conference on Artificial Intelligence and Industrial Engineering</title>
		<editor>
			<persName><forename type="first">R</forename><forename type="middle">E</forename><surname>Sehiemy</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">M</forename><forename type="middle">B I</forename><surname>Reaz</surname></persName>
		</editor>
		<meeting>the 2016 2nd International Conference on Artificial Intelligence and Industrial Engineering<address><addrLine>Beijing, China; Paris, France</addrLine></address></meeting>
		<imprint>
			<publisher>Atlantis Press</publisher>
			<date type="published" when="2016-11-11">20-11 November 2016. 2016</date>
			<biblScope unit="volume">133</biblScope>
			<biblScope unit="page" from="262" to="267" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b157">
	<analytic>
		<title level="a" type="main">Dual-Dense Convolution Network for Change Detection of High-Resolution Panchromatic Imagery</title>
		<author>
			<persName><forename type="first">W</forename><surname>Wiratama</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S.-E</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Sim</surname></persName>
		</author>
		<idno type="DOI">10.3390/app8101785</idno>
	</analytic>
	<monogr>
		<title level="j">Appl. Sci</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<date type="published" when="1785">2018. 1785</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b158">
	<analytic>
		<title level="a" type="main">Change Detection by Training a Triplet Network for Motion Feature Extraction</title>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">P</forename><surname>Nguyen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">C</forename><surname>Pham</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">V</forename><surname>Ha</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">W</forename><surname>-U.; Jeon</surname></persName>
		</author>
		<idno type="DOI">10.1109/TCSVT.2018.2795657</idno>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Circuits Syst. Video Technol</title>
		<imprint>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="page" from="433" to="446" />
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b159">
	<analytic>
		<title level="a" type="main">Deep learning and mapping based ternary change detection for information unbalanced images</title>
		<author>
			<persName><forename type="first">L</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Yang</surname></persName>
		</author>
		<idno type="DOI">10.1016/j.patcog.2017.01.002</idno>
	</analytic>
	<monogr>
		<title level="j">Pattern Recognit</title>
		<imprint>
			<biblScope unit="volume">66</biblScope>
			<biblScope unit="page" from="213" to="228" />
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b160">
	<analytic>
		<title level="a" type="main">AggregationNet: Identifying multiple changes based on convolutional neural network in bitemporal optical remote sensing images</title>
		<author>
			<persName><forename type="first">Q</forename><surname>Ye</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Huo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Wan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Fang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Pacific-Asia Conference on Knowledge Discovery and Data Mining</title>
		<meeting>the Pacific-Asia Conference on Knowledge Discovery and Data Mining<address><addrLine>Macau, China</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2019-04-17">14-17 April 2019</date>
			<biblScope unit="page" from="375" to="386" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b161">
	<analytic>
		<title level="a" type="main">Unsupervised Deep Slow Feature Analysis for Change Detection in Multi-Temporal Remote Sensing Images</title>
		<author>
			<persName><forename type="first">B</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Ru</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
		<idno type="DOI">10.1109/TGRS.2019.2930682</idno>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Geosci. Remote Sens</title>
		<imprint>
			<biblScope unit="volume">57</biblScope>
			<biblScope unit="page" from="9976" to="9992" />
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b162">
	<monogr>
		<title level="m" type="main">Unsupervised change detection in multi-temporal VHR images based on deep kernel PCA convolutional mapping network</title>
		<author>
			<persName><forename type="first">C</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Do</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1912.08628</idno>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b163">
	<analytic>
		<title level="a" type="main">Siamese network with multi-level features for patch-based change detection in satellite imagery</title>
		<author>
			<persName><forename type="first">F</forename><surname>Rahman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Vasu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Van Cor</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Kerekes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Savakis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2018 IEEE Global Conference on Signal and Information Processing</title>
		<meeting>the 2018 IEEE Global Conference on Signal and Information Processing<address><addrLine>Anaheim, CA, USA; New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2018-11">November 2018. 2018</date>
			<biblScope unit="page" from="958" to="962" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b164">
	<analytic>
		<title level="a" type="main">Deep siamese multi-scale convolutional network for change detection in multi-temporal VHR images</title>
		<author>
			<persName><forename type="first">H</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2019 10th International Workshop on the Analysis of Multitemporal Remote Sensing Images (MultiTemp)</title>
		<meeting>the 2019 10th International Workshop on the Analysis of Multitemporal Remote Sensing Images (MultiTemp)<address><addrLine>Shanghai, China</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2019-08">August 2019</date>
			<biblScope unit="page" from="1" to="4" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b165">
	<monogr>
		<title level="m" type="main">A Deep Siamese Network with Hybrid Convolutional Feature Extraction Module for Change Detection Based on Multi-sensor Remote Sensing Images</title>
		<author>
			<persName><forename type="first">M</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Chen</surname></persName>
		</author>
		<idno type="DOI">10.3390/rs12020205</idno>
		<imprint/>
	</monogr>
	<note>Remote Sens. 2020, 12, 205. [CrossRef</note>
</biblStruct>

<biblStruct xml:id="b166">
	<analytic>
		<title level="a" type="main">Change detection in high resolution satellite images using an ensemble of convolutional neural networks</title>
		<author>
			<persName><forename type="first">K</forename><surname>Lim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C.-S</forename><surname>Kim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2018 Asia-Pacific Signal and Information Processing Association Annual Summit and Conference (APSIPA ASC)</title>
		<meeting>the 2018 Asia-Pacific Signal and Information Processing Association Annual Summit and Conference (APSIPA ASC)<address><addrLine>Honolulu, HI, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2018-11">November 2018</date>
			<biblScope unit="page" from="509" to="515" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b167">
	<analytic>
		<title level="a" type="main">Zoom out CNNs Features for Optical Remote Sensing Change Detection</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">M</forename><surname>El Amin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2017 2nd International Conference on Image, Vision and Computing (ICIVC)</title>
		<meeting>the 2017 2nd International Conference on Image, Vision and Computing (ICIVC)<address><addrLine>Chengdu, China</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2017-06">June 2017</date>
			<biblScope unit="page" from="812" to="817" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b168">
	<analytic>
		<title level="a" type="main">Convolutional Neural Network-Based Transfer Learning for Optical Aerial Images Change Detection</title>
		<author>
			<persName><forename type="first">J</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Diao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Han</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Geosci. Remote Sens. Lett</title>
		<imprint>
			<biblScope unit="volume">17</biblScope>
			<biblScope unit="page" from="127" to="131" />
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b169">
	<analytic>
		<title level="a" type="main">Toward Generalized Change Detection on Planetary Surfaces with Convolutional Autoencoders and Transfer Learning</title>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">R</forename><surname>Kerner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">L</forename><surname>Wagstaff</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">D</forename><surname>Bue</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">C</forename><surname>Gray</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">F</forename><surname>Bell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Ben Amor</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">F B</forename><surname>Iii</surname></persName>
		</author>
		<idno type="DOI">10.1109/JSTARS.2019.2936771</idno>
	</analytic>
	<monogr>
		<title level="j">IEEE J. Sel. Top. Appl. Earth Obs. Remote Sens</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="page" from="3900" to="3918" />
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b170">
	<analytic>
		<title level="a" type="main">Transferred Deep Learning for Sea Ice Change Detection from Synthetic-Aperture Radar Images</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Wang</surname></persName>
		</author>
		<idno type="DOI">10.1109/LGRS.2019.2906279</idno>
	</analytic>
	<monogr>
		<title level="j">IEEE Geosci. Remote Sens. Lett</title>
		<imprint>
			<biblScope unit="volume">16</biblScope>
			<biblScope unit="page" from="1655" to="1659" />
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b171">
	<analytic>
		<title level="a" type="main">Scene change detection via deep convolution canonical correlation analysis neural network</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Ru</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Luo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IGARSS 2019-2019 IEEE International Geoscience and Remote Sensing Symposium</title>
		<meeting>the IGARSS 2019-2019 IEEE International Geoscience and Remote Sensing Symposium<address><addrLine>Yokohama, Japan</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2019-08-02">28 July-2 August 2019</date>
			<biblScope unit="page" from="198" to="201" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b172">
	<analytic>
		<title level="a" type="main">Change Detection Based on Deep Features and Low Rank</title>
		<author>
			<persName><forename type="first">B</forename><surname>Hou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Liu</surname></persName>
		</author>
		<idno type="DOI">10.1109/LGRS.2017.2766840</idno>
	</analytic>
	<monogr>
		<title level="j">IEEE Geosci. Remote Sens. Lett</title>
		<imprint>
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="page" from="2418" to="2422" />
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b173">
	<analytic>
		<title level="a" type="main">Convolutional neural network features based change detection in satellite images</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">M</forename><surname>El Amin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Froceedings of the First International Workshop on Pattern Recognition</title>
		<meeting><address><addrLine>Tokyo, Japan</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2016-05">May 2016</date>
			<biblScope unit="page" from="11" to="13" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b174">
	<monogr>
		<title/>
		<author>
			<persName><forename type="first">X</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Capi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Ishii</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="volume">0011</biblScope>
			<pubPlace>Bellingham, WA, USA</pubPlace>
		</imprint>
		<respStmt>
			<orgName>Spie-Int Soc Optical Engineering</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct xml:id="b175">
	<analytic>
		<title level="a" type="main">Land-Use Change Detection with Convolutional Neural Network Methods</title>
		<author>
			<persName><forename type="first">C</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Dragićević</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Li</surname></persName>
		</author>
		<idno type="DOI">10.3390/environments6020025</idno>
	</analytic>
	<monogr>
		<title level="j">Environments</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b176">
	<analytic>
		<title level="a" type="main">Kernel Slow Feature Analysis for Scene Change Detection</title>
		<author>
			<persName><forename type="first">C</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Du</surname></persName>
		</author>
		<idno type="DOI">10.1109/TGRS.2016.2642125</idno>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Geosci. Remote Sens</title>
		<imprint>
			<biblScope unit="volume">55</biblScope>
			<biblScope unit="page" from="2367" to="2384" />
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b177">
	<monogr>
		<title level="m" type="main">Post-Disaster Building Database Updating Using Automated Deep Learning: An Integration of Pre-Disaster OpenStreetMap and Multi-Temporal Satellite Data</title>
		<author>
			<persName><forename type="first">S</forename><surname>Ghaffarian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Kerle</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Pasolli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">J</forename><surname>Arsanjani</surname></persName>
		</author>
		<idno type="DOI">10.3390/rs11202427</idno>
		<imprint/>
	</monogr>
	<note>Remote Sens. 2019, 11, 2427. [CrossRef</note>
</biblStruct>

<biblStruct xml:id="b178">
	<analytic>
		<title level="a" type="main">Automatic Change Detection in Synthetic Aperture Radar Images Based on PCANet</title>
		<author>
			<persName><forename type="first">F</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Xu</surname></persName>
		</author>
		<idno type="DOI">10.1109/LGRS.2016.2611001</idno>
	</analytic>
	<monogr>
		<title level="j">IEEE Geosci. Remote Sens. Lett</title>
		<imprint>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="page" from="1" to="5" />
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b179">
	<monogr>
		<title level="m" type="main">Change Detection in SAR Images Based on Deep Semi-NMF and SVD Networks</title>
		<author>
			<persName><forename type="first">F</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Zhong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Jian</surname></persName>
		</author>
		<idno type="DOI">10.3390/rs9050435</idno>
		<imprint/>
	</monogr>
	<note>Remote Sens. 2017, 9, 435. [CrossRef</note>
</biblStruct>

<biblStruct xml:id="b180">
	<analytic>
		<title level="a" type="main">Image Change Detection Using PCANet Guided by Saliency Detection</title>
		<author>
			<persName><forename type="first">M</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Lia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>An</surname></persName>
		</author>
		<author>
			<persName><surname>Sar</surname></persName>
		</author>
		<idno type="DOI">10.1109/LGRS.2018.2876616</idno>
	</analytic>
	<monogr>
		<title level="j">IEEE Geosci. Remote Sens. Lett</title>
		<imprint>
			<biblScope unit="volume">16</biblScope>
			<biblScope unit="page" from="402" to="406" />
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b181">
	<analytic>
		<title level="a" type="main">Change detection by deep neural networks for synthetic aperture radar images</title>
		<author>
			<persName><forename type="first">F</forename><surname>Liao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Koshelev</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Milton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Lu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2017 International Conference on Computing, Networking and Communications (ICNC)</title>
		<meeting>the 2017 International Conference on Computing, Networking and Communications (ICNC)<address><addrLine>Santa Clara, CA, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2017-01">January 2017</date>
			<biblScope unit="page" from="947" to="951" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b182">
	<analytic>
		<title level="a" type="main">Three-class change detection in synthetic aperture radar images based on deep belief network</title>
		<author>
			<persName><forename type="first">Q</forename><forename type="middle">N</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">G</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Zhan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Bio-Inspired Computing-Theories and Applications</title>
		<editor>
			<persName><forename type="first">M</forename><surname>Gong</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">L</forename><surname>Pan</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">T</forename><surname>Song</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">K</forename><surname>Tang</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">X</forename><surname>Zhang</surname></persName>
		</editor>
		<meeting><address><addrLine>Germany</addrLine></address></meeting>
		<imprint>
			<publisher>Springer: Berlin/Heidelberg</publisher>
			<date type="published" when="2015">2015. 2015</date>
			<biblScope unit="volume">562</biblScope>
			<biblScope unit="page" from="696" to="705" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b183">
	<analytic>
		<title level="a" type="main">Change detection in SAR images using deep belief network: A new training approach based on morphological images</title>
		<author>
			<persName><forename type="first">F</forename><surname>Samadi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Akbarizadeh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Kaabi</surname></persName>
		</author>
		<idno type="DOI">10.1049/iet-ipr.2018.6248</idno>
	</analytic>
	<monogr>
		<title level="j">IET Image Process</title>
		<imprint>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="page" from="2255" to="2264" />
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b184">
	<analytic>
		<title level="a" type="main">Discriminative Feature Learning for Unsupervised Change Detection in Heterogeneous Images Based on a Coupled Neural Network</title>
		<author>
			<persName><forename type="first">W</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Liu</surname></persName>
		</author>
		<idno type="DOI">10.1109/TGRS.2017.2739800</idno>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Geosci. Remote Sens</title>
		<imprint>
			<biblScope unit="volume">55</biblScope>
			<biblScope unit="page" from="7066" to="7080" />
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b185">
	<monogr>
		<title level="m" type="main">Guided anisotropic diffusion and iterative learning for weakly supervised change detection</title>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">C</forename><surname>Daudt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">L</forename><surname>Saux</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Boulch</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Gousseau</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1904.08208</idno>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b186">
	<analytic>
		<title level="a" type="main">Semi-supervised deep generative models for change detection in very high resolution imagery</title>
		<author>
			<persName><forename type="first">C</forename><surname>Connors</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">R</forename><surname>Vatsavai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2017 IEEE International Geoscience and Remote Sensing Symposium</title>
		<meeting>the 2017 IEEE International Geoscience and Remote Sensing Symposium<address><addrLine>Fort Worth, TX, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2017-07">July 2017</date>
			<biblScope unit="page" from="1063" to="1066" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b187">
	<analytic>
		<title level="a" type="main">Deep nonsmooth nonnegative matrix factorization network with semi-supervised learning for SAR image change detection</title>
		<author>
			<persName><forename type="first">H.-C</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">J</forename><surname>Emery</surname></persName>
		</author>
		<idno type="DOI">10.1016/j.isprsjprs.2019.12.002</idno>
	</analytic>
	<monogr>
		<title level="j">ISPRS J. Photogramm. Remote Sens</title>
		<imprint>
			<biblScope unit="volume">160</biblScope>
			<biblScope unit="page" from="167" to="179" />
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b188">
	<monogr>
		<title level="m" type="main">Land Cover Change Detection from High-Resolution Remote Sensing Imagery Using Multitemporal Deep Feature Collaborative Learning and a Semi-supervised Chan-Vese Model</title>
		<author>
			<persName><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Lv</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Peng</surname></persName>
		</author>
		<idno type="DOI">10.3390/rs11232787</idno>
		<imprint/>
	</monogr>
	<note>Remote Sens. 2019, 11, 2787. [CrossRef</note>
</biblStruct>

<biblStruct xml:id="b189">
	<analytic>
		<title level="a" type="main">Stacked Fisher autoencoder for SAR change detection</title>
		<author>
			<persName><forename type="first">G</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Jiao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
		<idno type="DOI">10.1016/j.patcog.2019.106971</idno>
	</analytic>
	<monogr>
		<title level="j">Pattern Recognit</title>
		<imprint>
			<biblScope unit="volume">96</biblScope>
			<date type="published" when="2019">2019. 106971</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b190">
	<monogr>
		<title level="m" type="main">Automatic Post-Disaster Damage Mapping Using Deep-Learning Techniques for Change Detection: Case Study of the Tohoku Tsunami</title>
		<author>
			<persName><forename type="first">J</forename><surname>Sublime</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Kalinicheva</surname></persName>
		</author>
		<idno type="DOI">10.3390/rs11091123</idno>
		<imprint>
			<date type="published" when="1123">2019. 1123</date>
			<biblScope unit="volume">11</biblScope>
		</imprint>
	</monogr>
	<note>Remote Sens</note>
</biblStruct>

<biblStruct xml:id="b191">
	<monogr>
		<title/>
		<author>
			<persName><forename type="first">I</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">Deep</forename><surname>Courville</surname></persName>
		</author>
		<author>
			<persName><surname>Learning</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016">2016</date>
			<publisher>MIT Press</publisher>
			<pubPlace>Cambridge, MA, USA</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b192">
	<analytic>
		<title level="a" type="main">Change Detection Based on the Combination of Improved SegNet Neural Network and Morphology</title>
		<author>
			<persName><forename type="first">B</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Zhu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2018 IEEE 3rd International Conference on Image, Vision and Computing (ICIVC), Chong Qing</title>
		<meeting>the 2018 IEEE 3rd International Conference on Image, Vision and Computing (ICIVC), Chong Qing<address><addrLine>China</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2018-06">June 2018</date>
			<biblScope unit="page" from="55" to="59" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b193">
	<analytic>
		<title level="a" type="main">Guan End-to-End Change Detection for High Resolution Satellite Images Using Improved UNet++</title>
		<author>
			<persName><forename type="first">D</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Guan</surname></persName>
		</author>
		<idno type="DOI">10.3390/rs11111382</idno>
	</analytic>
	<monogr>
		<title level="j">Remote Sens</title>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<date type="published" when="1382">2019. 1382</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b194">
	<analytic>
		<title level="a" type="main">Automatic Semantic Segmentation with DeepLab Dilated Learning Network for Change Detection in Remote Sensing Images</title>
		<author>
			<persName><forename type="first">N</forename><surname>Venugopal</surname></persName>
		</author>
		<idno type="DOI">10.1007/s11063-019-10174-x</idno>
	</analytic>
	<monogr>
		<title level="j">Neural Process. Lett</title>
		<imprint>
			<biblScope unit="volume">2020</biblScope>
			<biblScope unit="page" from="1" to="23" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b195">
	<analytic>
		<title level="a" type="main">Sample Selection Based Change Detection with Dilated Network Learning in Remote Sensing Images</title>
		<author>
			<persName><forename type="first">N</forename><surname>Venugopal</surname></persName>
		</author>
		<idno type="DOI">10.1007/s11220-019-0252-0</idno>
	</analytic>
	<monogr>
		<title level="j">Sens. Imaging: Int. J</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="page">31</biblScope>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b196">
	<analytic>
		<title level="a" type="main">Forest Change Detection in Incomplete Satellite Images with Deep Neural Networks</title>
		<author>
			<persName><forename type="first">S</forename><surname>Khan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Porikli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Bennamoun</surname></persName>
		</author>
		<idno type="DOI">10.1109/TGRS.2017.2707528</idno>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Geosci. Remote Sens</title>
		<imprint>
			<biblScope unit="volume">55</biblScope>
			<biblScope unit="page" from="5407" to="5423" />
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b197">
	<analytic>
		<title level="a" type="main">Change detection based on Faster R-CNN for high-resolution remote sensing images</title>
		<author>
			<persName><forename type="first">Q</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Zhu</surname></persName>
		</author>
		<idno type="DOI">10.1080/2150704X.2018.1492172</idno>
	</analytic>
	<monogr>
		<title level="j">Remote Sens. Lett</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="page" from="923" to="932" />
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b198">
	<analytic>
		<title level="a" type="main">A review of pulse coupled neural network</title>
		<author>
			<persName><forename type="first">N</forename><surname>Dewan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Kashyap</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">S</forename><surname>Kushwaha</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Iioab J</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="page" from="61" to="65" />
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b199">
	<analytic>
		<title level="a" type="main">Image Change Detection Method Based on Pulse-Coupled Neural Network</title>
		<author>
			<persName><forename type="first">R</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Qin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><forename type="middle">K</forename><surname>Kasabov</surname></persName>
		</author>
		<author>
			<persName><surname>Sar</surname></persName>
		</author>
		<idno type="DOI">10.1007/s12524-015-0507-8</idno>
	</analytic>
	<monogr>
		<title level="j">J. Indian Soc. Remote Sens</title>
		<imprint>
			<biblScope unit="volume">44</biblScope>
			<biblScope unit="page" from="443" to="450" />
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b200">
	<analytic>
		<title level="a" type="main">Toward Fully Automatic Detection of Changes in Suburban Areas from VHR SAR Images by Combining Multiple Neural-Network Models</title>
		<author>
			<persName><forename type="first">C</forename><surname>Pratola</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Del Frate</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Schiavon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Solimini</surname></persName>
		</author>
		<idno type="DOI">10.1109/TGRS.2012.2236846</idno>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Geosci. Remote Sens</title>
		<imprint>
			<biblScope unit="volume">51</biblScope>
			<biblScope unit="page" from="2055" to="2066" />
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b201">
	<analytic>
		<title level="a" type="main">Change Detection Based on Pulse-Coupled Neural Networks and the NMI Feature for High Spatial Resolution Remote Sensing Imagery</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
		<idno type="DOI">10.1109/LGRS.2014.2349937</idno>
	</analytic>
	<monogr>
		<title level="j">IEEE Geosci. Remote Sens. Lett</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="page" from="537" to="541" />
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b202">
	<analytic>
		<title level="a" type="main">Generative adversarial nets</title>
		<author>
			<persName><forename type="first">I</forename><forename type="middle">J</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Pouget-Abadie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Mirza</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Warde-Farley</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Ozair</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Courville</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems 27; Ghahramani</title>
		<editor>
			<persName><forename type="first">M</forename><surname>Welling</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">C</forename><surname>Cortes</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">N</forename><forename type="middle">D</forename><surname>Lawrence</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">K</forename><forename type="middle">Q</forename><surname>Weinberger</surname></persName>
		</editor>
		<meeting><address><addrLine>Jolla, CA, USA</addrLine></address></meeting>
		<imprint>
			<publisher>La</publisher>
			<date type="published" when="2014">2014</date>
			<biblScope unit="volume">27</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b203">
	<analytic>
		<title level="a" type="main">Generative Adversarial Networks for Change Detection in Multispectral Imagery</title>
		<author>
			<persName><forename type="first">M</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Niu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Li</surname></persName>
		</author>
		<idno type="DOI">10.1109/LGRS.2017.2762694</idno>
	</analytic>
	<monogr>
		<title level="j">IEEE Geosci. Remote. Sens. Lett</title>
		<imprint>
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="page" from="2310" to="2314" />
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b204">
	<analytic>
		<title level="a" type="main">From W-Net to CDGAN: Bitemporal Change Detection via Deep Learning Techniques</title>
		<author>
			<persName><forename type="first">B</forename><surname>Hou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<idno type="DOI">10.1109/TGRS.2019.2948659</idno>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Geosci. Remote Sens</title>
		<imprint>
			<biblScope unit="volume">58</biblScope>
			<biblScope unit="page" from="1790" to="1802" />
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b205">
	<analytic>
		<title level="a" type="main">Land Cover Change Detection at Subpixel Resolution with a Hopfield Neural Network</title>
		<author>
			<persName><forename type="first">Q</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Atkinson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Li</surname></persName>
		</author>
		<idno type="DOI">10.1109/JSTARS.2014.2355832</idno>
	</analytic>
	<monogr>
		<title level="j">IEEE J. Sel. Top. Appl. Earth Obs. Remote Sens</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page" from="1" to="14" />
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b206">
	<monogr>
		<title level="m" type="main">Unsupervised Change Detection in High Spatial Resolution Optical Imagery Based on Modified Hopfield Neural Network</title>
		<author>
			<persName><forename type="first">K</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Huo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Lu</surname></persName>
		</author>
		<idno type="DOI">10.1109/icnc.2008.456</idno>
		<imprint>
			<date type="published" when="2008">2008</date>
			<publisher>IEEE</publisher>
			<biblScope unit="page" from="281" to="285" />
			<pubPlace>New York, NY, USA</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b207">
	<analytic>
		<title level="a" type="main">Spatial constraint hopfield-type neural networks for detecting changes in remotely sensed multitemporal images</title>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">N</forename><surname>Subudhi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Ghosh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Ghosh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2013 20th IEEE International Conference on Image Processing</title>
		<meeting>the 2013 20th IEEE International Conference on Image Processing<address><addrLine>Melbourne, VIC, Australia</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2013-09">September 2013</date>
			<biblScope unit="page" from="3815" to="3819" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b208">
	<monogr>
		<title level="m" type="main">Supervised Sub-Pixel Mapping for Change Detection from Remotely Sensed Images with Different Resolutions</title>
		<author>
			<persName><forename type="first">K</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
		<idno type="DOI">10.3390/rs9030284</idno>
		<imprint/>
	</monogr>
	<note>Remote Sens. 2017, 9, 284. [CrossRef</note>
</biblStruct>

<biblStruct xml:id="b209">
	<analytic>
		<title level="a" type="main">An enhanced back propagation method for change analysis of remote sensing images with adaptive preprocessing</title>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">P</forename><surname>Dalmiya</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Santhi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Sathyabama</surname></persName>
		</author>
		<idno type="DOI">10.1080/22797254.2019.1692637</idno>
	</analytic>
	<monogr>
		<title level="j">Eur. J. Remote Sens</title>
		<imprint>
			<biblScope unit="page" from="1" to="12" />
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b210">
	<analytic>
		<title level="a" type="main">A composed supervised/unsupervised approach to improve change detection from remote sensing</title>
		<author>
			<persName><forename type="first">L</forename><surname>Castellana</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>D'addabbo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Pasquariello</surname></persName>
		</author>
		<idno type="DOI">10.1016/j.patrec.2006.08.010</idno>
	</analytic>
	<monogr>
		<title level="j">Pattern Recognit. Lett</title>
		<imprint>
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="page" from="405" to="413" />
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b211">
	<analytic>
		<title level="a" type="main">Monitoring Urban Land Cover in Rome, Italy, and Its Changes by Single-Polarization Multitemporal SAR Images</title>
		<author>
			<persName><forename type="first">F</forename><surname>Del Frate</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Pacifici</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Solimini</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE J. Sel. Top. Appl. Earth Obs. Remote Sens</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="87" to="97" />
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b212">
	<analytic>
		<title level="a" type="main">Land use/cover change modelling in a mediterranean rural landscape using multi-layer perceptron and markov chain (mlp-mc)</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">E</forename><surname>Mirici</surname></persName>
		</author>
		<idno type="DOI">10.15666/aeer/1601_467486</idno>
	</analytic>
	<monogr>
		<title level="j">Appl. Ecol. Environ. Res</title>
		<imprint>
			<biblScope unit="volume">16</biblScope>
			<biblScope unit="page" from="467" to="486" />
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b213">
	<analytic>
		<title level="a" type="main">Change detection of remote sensing images with semi-supervised multilayer perceptron</title>
		<author>
			<persName><forename type="first">S</forename><surname>Patra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Ghosh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Ghosh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Fundam. Inform</title>
		<imprint>
			<biblScope unit="volume">84</biblScope>
			<biblScope unit="page" from="429" to="442" />
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b214">
	<analytic>
		<title level="a" type="main">Remote sensed data for automatic detection of land-use changes due to human activity in support to landslide studies</title>
		<author>
			<persName><forename type="first">C</forename><surname>Tarantino</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Blonda</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Pasquariello</surname></persName>
		</author>
		<idno type="DOI">10.1007/s11069-006-9041-x</idno>
	</analytic>
	<monogr>
		<title level="j">Nat. Hazards</title>
		<imprint>
			<biblScope unit="volume">41</biblScope>
			<biblScope unit="page" from="245" to="267" />
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b215">
	<analytic>
		<title level="a" type="main">Time-series processing of large scale remote sensing data with extreme learning machine</title>
		<author>
			<persName><forename type="first">J</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Wu</surname></persName>
		</author>
		<idno type="DOI">10.1016/j.neucom.2013.02.051</idno>
	</analytic>
	<monogr>
		<title level="j">Neurocomputing</title>
		<imprint>
			<biblScope unit="volume">128</biblScope>
			<biblScope unit="page" from="199" to="206" />
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b216">
	<analytic>
		<title level="a" type="main">A Novel Remote Sensing Image Change Detection Algorithm Based on Self-Organizing Feature Map Neural Network Model</title>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">H</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><forename type="middle">H</forename><surname>Cheng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2016 International Conference on Communication and Electronics Systems (ICCES)</title>
		<meeting>the 2016 International Conference on Communication and Electronics Systems (ICCES)<address><addrLine>Coimbatore, India; New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2016-10">October 2016. 2016</date>
			<biblScope unit="page" from="1033" to="1038" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b217">
	<analytic>
		<title level="a" type="main">SOMDNCD: Image Change Detection Based on Self-Organizing Maps and Deep Neural Networks</title>
		<author>
			<persName><forename type="first">R</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Ni</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Lin</surname></persName>
		</author>
		<idno type="DOI">10.1109/ACCESS.2018.2849110</idno>
	</analytic>
	<monogr>
		<title level="j">IEEE Access</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page" from="35915" to="35925" />
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b218">
	<analytic>
		<title level="a" type="main">Urban Change Detection Based on Self-Organizing Feature Map Neural Network</title>
		<author>
			<persName><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><forename type="middle">W</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">W</forename><surname>Ma</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2004 IEEE International Geoscience and Remote Sensing Symposium</title>
		<meeting>the 2004 IEEE International Geoscience and Remote Sensing Symposium<address><addrLine>Anchorage, AK, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2004-09">September 2004</date>
			<biblScope unit="page" from="3428" to="3431" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b219">
	<analytic>
		<title level="a" type="main">Semi-supervised change detection using modified self-organizing feature map neural network</title>
		<author>
			<persName><forename type="first">S</forename><surname>Ghosh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Roy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Ghosh</surname></persName>
		</author>
		<idno type="DOI">10.1016/j.asoc.2013.09.010</idno>
	</analytic>
	<monogr>
		<title level="j">Appl. Soft Comput</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="page" from="1" to="20" />
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b220">
	<analytic>
		<title level="a" type="main">Unsupervised Change Detection in Remote-Sensing Images using One-dimensional Self-Organizing Feature Map Neural Network</title>
		<author>
			<persName><forename type="first">S</forename><surname>Patra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Ghosh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Ghosh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 9th International Conference on Information Technology (ICIT&apos;06)</title>
		<meeting>the 9th International Conference on Information Technology (ICIT&apos;06)<address><addrLine>Bhubaneswar, India</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2006-12-21">18-21 December 2006</date>
			<biblScope unit="page" from="141" to="142" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b221">
	<analytic>
		<title level="a" type="main">A novel image change detection method based on enhanced growing self-organization feature map</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
		<idno type="DOI">10.1117/12.713024</idno>
	</analytic>
	<monogr>
		<title level="j">Geoinformatics Remote Sens. Data Inf</title>
		<imprint>
			<biblScope unit="volume">6419</biblScope>
			<date type="published" when="2006">2006. 641915</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b222">
	<analytic>
		<title level="a" type="main">Review ArticleDigital change detection methods in ecosystem monitoring: A review</title>
		<author>
			<persName><forename type="first">P</forename><surname>Coppin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Jonckheere</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Nackaerts</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Muys</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Lambin</surname></persName>
		</author>
		<idno type="DOI">10.1080/0143116031000101675</idno>
	</analytic>
	<monogr>
		<title level="j">Int. J. Remote Sens</title>
		<imprint>
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="page" from="1565" to="1596" />
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b223">
	<analytic>
		<title level="a" type="main">Monitoring Land-Cover Changes: A Machine-Learning Perspective</title>
		<author>
			<persName><forename type="first">A</forename><surname>Karpatne</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">R</forename><surname>Vatsavai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Shekhar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Kumar</surname></persName>
		</author>
		<idno type="DOI">10.1109/MGRS.2016.2528038</idno>
	</analytic>
	<monogr>
		<title level="j">IEEE Geosci. Remote Sens. Mag</title>
		<imprint>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page" from="8" to="21" />
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b224">
	<monogr>
		<title level="m" type="main">Change Detection under Global Viewpoint Uncertainty</title>
		<author>
			<persName><forename type="first">M</forename><surname>Tomoya</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Kanji</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1703.00552</idno>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b225">
	<analytic>
		<title level="a" type="main">Unsupervised Change Detection Based on a Unified Framework for Weighted Collaborative Representation with RDDL and Fuzzy Clustering</title>
		<author>
			<persName><forename type="first">G</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H.-C</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W.-Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">J</forename><surname>Emery</surname></persName>
		</author>
		<idno type="DOI">10.1109/TGRS.2019.2923643</idno>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Geosci. Remote Sens</title>
		<imprint>
			<biblScope unit="volume">57</biblScope>
			<biblScope unit="page" from="8890" to="8903" />
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b226">
	<analytic>
		<title level="a" type="main">Modelling land use/cover change in Lake Mogan and surroundings using CA-Markov Chain Analysis</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Durmusoglu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Tanriover</surname></persName>
		</author>
		<idno type="DOI">10.22438/jeb/38/5(SI)/GM-15</idno>
	</analytic>
	<monogr>
		<title level="j">J. Environ. Boil</title>
		<imprint>
			<biblScope unit="volume">38</biblScope>
			<biblScope unit="page" from="981" to="989" />
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b227">
	<analytic>
		<title level="a" type="main">Temporal and spatial change detecting (1998-2003) and predicting of land use and land cover in Core corridor of Pearl River Delta (China) by using TM and ETM+ images</title>
		<author>
			<persName><forename type="first">F</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><surname>Wang</surname></persName>
		</author>
		<idno type="DOI">10.1007/s10661-007-9734-y</idno>
	</analytic>
	<monogr>
		<title level="j">Environ. Monit. Assess</title>
		<imprint>
			<biblScope unit="volume">137</biblScope>
			<biblScope unit="page" from="127" to="147" />
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b228">
	<analytic>
		<title level="a" type="main">Detection of urban sprawl using a genetic algorithm-evolved artificial neural network classification in remote sensing: A case study in Jiading and Putuo districts of Shanghai, China</title>
		<author>
			<persName><forename type="first">X</forename><surname>Tong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Liu</surname></persName>
		</author>
		<idno type="DOI">10.1080/01431160903475290</idno>
	</analytic>
	<monogr>
		<title level="j">Int. J. Remote Sens</title>
		<imprint>
			<biblScope unit="volume">31</biblScope>
			<biblScope unit="page" from="1485" to="1504" />
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b229">
	<analytic>
		<title level="a" type="main">Generating high-accuracy urban distribution map for short-term change monitoring based on convolutional neural network by utilizing SAR imagery</title>
		<author>
			<persName><forename type="first">S</forename><surname>Iino</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Ito</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Doi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Imaizumi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Hikosaka</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Earth Resources and Environmental Remote Sensing/GIS Applications VIII</title>
		<editor>
			<persName><forename type="first">U</forename><surname>Michel</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">K</forename><surname>Schulz</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">K</forename><forename type="middle">G</forename><surname>Nikolakopoulos</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">D</forename><surname>Civco</surname></persName>
		</editor>
		<meeting><address><addrLine>Bellingham, WA, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Spie-Int Soc Optical Engineering</publisher>
			<date type="published" when="2017">2017</date>
			<biblScope unit="volume">10428</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b230">
	<analytic>
		<title level="a" type="main">A new approach for surface water change detection: Integration of pixel level image fusion and image classification techniques</title>
		<author>
			<persName><forename type="first">K</forename><surname>Rokni</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Ahmad</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Solaimani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Hazini</surname></persName>
		</author>
		<idno type="DOI">10.1016/j.jag.2014.08.014</idno>
	</analytic>
	<monogr>
		<title level="j">Int. J. Appl. Earth Obs. Geoinformation</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="page" from="226" to="234" />
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b231">
	<analytic>
		<title level="a" type="main">Change Detection of Surface Water in Remote Sensing Images Based on Fully Convolutional Network</title>
		<author>
			<persName><forename type="first">A</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Kim</surname></persName>
		</author>
		<idno type="DOI">10.2112/SI91-086.1</idno>
	</analytic>
	<monogr>
		<title level="j">J. Coast. Res</title>
		<imprint>
			<biblScope unit="volume">91</biblScope>
			<biblScope unit="page" from="426" to="430" />
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b232">
	<monogr>
		<title level="m" type="main">Assessing Global Forest Land-Use Change by Object-Based Image Analysis</title>
		<author>
			<persName><forename type="first">E</forename><surname>Lindquist</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>D'annunzio</surname></persName>
		</author>
		<idno type="DOI">10.3390/rs8080678</idno>
		<imprint/>
	</monogr>
	<note>Remote Sens. 2016, 8, 678. [CrossRef</note>
</biblStruct>

<biblStruct xml:id="b233">
	<analytic>
		<title level="a" type="main">Automatic Recognition of Landslide Based on CNN and Texture Change Detection</title>
		<author>
			<persName><forename type="first">A</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Dai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2016 31st Youth Academic Annual Conference of Chinese-Association-of-Automation (YAC)</title>
		<meeting>the 2016 31st Youth Academic Annual Conference of Chinese-Association-of-Automation (YAC)<address><addrLine>Wuhan, China</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2016-11">November 2016</date>
			<biblScope unit="page" from="444" to="448" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b234">
	<analytic>
		<title level="a" type="main">Detection of tsunami-induced changes using generalized improved fuzzy radial basis function neural network</title>
		<author>
			<persName><forename type="first">A</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">K</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">J</forename><surname>Nigam</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Pal</surname></persName>
		</author>
		<idno type="DOI">10.1007/s11069-015-1595-z</idno>
	</analytic>
	<monogr>
		<title level="j">Nat. Hazards</title>
		<imprint>
			<biblScope unit="volume">77</biblScope>
			<biblScope unit="page" from="367" to="381" />
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b235">
	<monogr>
		<title level="m" type="main">Patch Similarity Convolutional Neural Network for Urban Flood Extent Mapping Using Bi-Temporal Satellite Multispectral Imagery</title>
		<author>
			<persName><forename type="first">B</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Meng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Wang</surname></persName>
		</author>
		<idno type="DOI">10.3390/rs11212492</idno>
		<imprint/>
	</monogr>
	<note>Remote Sens. 2019, 11, 2492. [CrossRef</note>
</biblStruct>

<biblStruct xml:id="b236">
	<analytic>
		<title level="a" type="main">Temporal city modeling using street level imagery</title>
		<author>
			<persName><forename type="first">K</forename><surname>Sakurada</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Tetsuka</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Okatani</surname></persName>
		</author>
		<idno type="DOI">10.1016/j.cviu.2017.01.012</idno>
	</analytic>
	<monogr>
		<title level="j">Comput. Vis. Image Underst</title>
		<imprint>
			<biblScope unit="volume">157</biblScope>
			<biblScope unit="page" from="55" to="71" />
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b237">
	<analytic>
		<title level="a" type="main">Heterogeneous Data and Big Data Analytics</title>
		<author>
			<persName><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<idno type="DOI">10.12691/acis-3-1-3</idno>
	</analytic>
	<monogr>
		<title level="j">Autom. Control. Inf. Sci</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page" from="8" to="15" />
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b238">
	<monogr>
		<title level="m" type="main">Unsupervised Feature Learning and Deep Learning: A Review and New Perspectives</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">C</forename><surname>Courville</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Vincent</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1206.5538</idno>
		<imprint>
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b239">
	<analytic>
		<title level="a" type="main">A Survey of Methods for Explaining Black Box Models</title>
		<author>
			<persName><forename type="first">R</forename><surname>Guidotti</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Monreale</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Ruggieri</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Turini</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Giannotti</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Pedreschi</surname></persName>
		</author>
		<idno type="DOI">10.1145/3236009</idno>
	</analytic>
	<monogr>
		<title level="j">ACM Comput. Surv</title>
		<imprint>
			<biblScope unit="volume">51</biblScope>
			<biblScope unit="page" from="1" to="42" />
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b240">
	<analytic>
		<title level="a" type="main">Steps Toward Robust Artificial Intelligence</title>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">G</forename><surname>Dietterich</surname></persName>
		</author>
		<idno type="DOI">10.1609/aimag.v38i3.2756</idno>
	</analytic>
	<monogr>
		<title level="j">AI Mag</title>
		<imprint>
			<biblScope unit="volume">38</biblScope>
			<biblScope unit="page" from="3" to="24" />
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b241">
	<monogr>
		<title level="m" type="main">Explanation in human-AI systems: A literature meta-review, synopsis of key ideas and publications, and bibliography for explainable AI</title>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">T</forename><surname>Mueller</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">R</forename><surname>Hoffman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Clancey</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Emrey</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Klein</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1902.01876</idno>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b242">
	<analytic>
		<title level="a" type="main">Analysis of spatial distribution pattern of change-detection error caused by misregistration</title>
		<author>
			<persName><forename type="first">W</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Hao</surname></persName>
		</author>
		<idno type="DOI">10.1080/01431161.2013.810353</idno>
	</analytic>
	<monogr>
		<title level="j">Int. J. Remote Sens</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="page" from="6883" to="6897" />
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b243">
	<monogr>
		<title level="m" type="main">A Reliability-Based Multi-Algorithm Fusion Technique in Detecting Changes in Land Cover</title>
		<author>
			<persName><forename type="first">P</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M.-S</forename><surname>Wong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Chen</surname></persName>
		</author>
		<idno type="DOI">10.3390/rs5031134</idno>
		<imprint>
			<date type="published" when="2013">2013</date>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page" from="1134" to="1151" />
		</imprint>
	</monogr>
	<note>Remote Sens</note>
</biblStruct>

<biblStruct xml:id="b244">
	<analytic>
		<title level="a" type="main">Detection of land-cover transitions by combining multidate classifiers</title>
		<author>
			<persName><forename type="first">L</forename><surname>Bruzzone</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Cossu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Vernazza</surname></persName>
		</author>
		<idno type="DOI">10.1016/j.patrec.2004.06.002</idno>
	</analytic>
	<monogr>
		<title level="j">Pattern Recognit. Lett</title>
		<imprint>
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="page" from="1491" to="1500" />
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b245">
	<analytic>
		<title level="a" type="main">Advanced Markov random field model based on local uncertainty for unsupervised change detection</title>
		<author>
			<persName><forename type="first">P</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Miao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Cai</surname></persName>
		</author>
		<idno type="DOI">10.1080/2150704X.2015.1054045</idno>
	</analytic>
	<monogr>
		<title level="j">Remote Sens. Lett</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page" from="667" to="676" />
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b246">
	<analytic>
		<title level="a" type="main">Semantic Image Segmentation with Deep Convolutional Nets, Atrous Convolution, and Fully Connected CRFs</title>
		<author>
			<persName><forename type="first">L.-C</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Papandreou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Kokkinos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Murphy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">L</forename><surname>Yuille</surname></persName>
		</author>
		<author>
			<persName><surname>Deeplab</surname></persName>
		</author>
		<idno type="DOI">10.1109/TPAMI.2017.2699184</idno>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Pattern Anal. Mach. Intell</title>
		<imprint>
			<biblScope unit="volume">40</biblScope>
			<biblScope unit="page" from="834" to="848" />
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b247">
	<analytic>
		<title level="a" type="main">Object instance annotation with deep extreme level set evolution</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Acuna</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Ling</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Kar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Fidler</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition<address><addrLine>Long Beach, CA, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2019-06-20">16-20 June 2019</date>
			<biblScope unit="page" from="7500" to="7508" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
