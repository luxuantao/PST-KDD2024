<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Audio-Oriented Multimodal Machine Comprehension via Dynamic Inter-and Intra-modality Attention</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Zhiqi</forename><surname>Huang</surname></persName>
							<email>zhiqihuang@pku.edu.cn</email>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">ADSPLAB</orgName>
								<orgName type="department" key="dep2">School of ECE</orgName>
								<orgName type="institution">Peking University</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
							<affiliation key="aff3">
								<orgName type="department">Equal Contribution</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Fenglin</forename><surname>Liu</surname></persName>
							<email>fenglinliu98@pku.edu.cn</email>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">ADSPLAB</orgName>
								<orgName type="department" key="dep2">School of ECE</orgName>
								<orgName type="institution">Peking University</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
							<affiliation key="aff3">
								<orgName type="department">Equal Contribution</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Xian</forename><surname>Wu</surname></persName>
							<affiliation key="aff1">
								<address>
									<region>Tencent</region>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Shen</forename><surname>Ge</surname></persName>
							<email>shenge@tencent.com</email>
							<affiliation key="aff1">
								<address>
									<region>Tencent</region>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Helin</forename><surname>Wang</surname></persName>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">ADSPLAB</orgName>
								<orgName type="department" key="dep2">School of ECE</orgName>
								<orgName type="institution">Peking University</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Wei</forename><surname>Fan</surname></persName>
							<affiliation key="aff1">
								<address>
									<region>Tencent</region>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Yuexian</forename><surname>Zou</surname></persName>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">ADSPLAB</orgName>
								<orgName type="department" key="dep2">School of ECE</orgName>
								<orgName type="institution">Peking University</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
							<affiliation key="aff2">
								<orgName type="department">Peng Cheng Laboratory</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Audio-Oriented Multimodal Machine Comprehension via Dynamic Inter-and Intra-modality Attention</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.2" ident="GROBID" when="2022-12-25T13:26+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>While Machine Comprehension (MC) has attracted extensive research interests in recent years, existing approaches mainly belong to the category of Machine Reading Comprehension task which mines textual inputs (paragraphs and questions) to predict the answers (choices or text spans). However, there are a lot of MC tasks that accept audio input in addition to the textual input, e.g. English listening comprehension test. In this paper, we target the problem of Audio-Oriented Multimodal Machine Comprehension, and its goal is to answer questions based on the given audio and textual information. To solve this problem, we propose a Dynamic Inter-and Intra-modality Attention (DIIA) model to effectively fuse the two modalities (audio and textual). DIIA can work as an independent component and thus be easily integrated into existing MC models. Moreover, we further develop a Multimodal Knowledge Distillation (MKD) module to enable our multimodal MC model to accurately predict the answers based only on either the text or the audio. As a result, the proposed approach can handle various tasks including: Audio-Oriented Multimodal Machine Comprehension, Machine Reading Comprehension and Machine Listening Comprehension, in a single model, making fair comparisons possible between our model and the existing unimodal MC models. Experimental results and analysis prove the effectiveness of the proposed approaches. First, the proposed DIIA boosts the baseline models by up to 21.08% in terms of accuracy; Second, under the unimodal scenarios, the MKD module allows our multimodal MC model to significantly outperform the unimodal models by up to 18.87%, which are trained and tested with only audio or textual data. 1</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Introduction</head><p>Recently, there is a surge of research interests in Machine Comprehension (MC), which aims to teach the machine to answer questions after giving comprehension materials <ref type="bibr" target="#b19">(Nguyen et al. 2016;</ref><ref type="bibr" target="#b23">Rajpurkar et al. 2016;</ref><ref type="bibr" target="#b13">Lai et al. 2017)</ref>. As shown in Figure <ref type="figure" target="#fig_0">1</ref>, conventional MC system accepts unimodal textual inputs, and predict the corresponding answers to the given multiple-choice questions. By adopting various deep learning techniques, many models have been developed for the MC problem and are proven to be effective <ref type="bibr" target="#b17">(Liu et al. 2019c;</ref><ref type="bibr" target="#b21">Qiu et al. 2019</ref>).</p><p>However, conventional MC only focus on accepting single modal textual inputs, while in real life many multimodal (audio and textual modalities) scenarios exist, such as playing music with lyrics, and taking a listening examination in TOEFL, etc.. Moreover, multimodal inputs often convey more information than single modality inputs, and it is easy to make wrong judgments under single modal scenarios. For example, people could express opposite intentions by using different tones to say the sentence "that's interesting". If the emphasis is put on the word "interesting!", he/she may really be interested and wants to know more; on the other hand, if the whole sentence is expressed intermittently, e.g., "that's... um... interesting.", he/she may not be interested at all. Another example in English exams like TOEFL, i.e., "He hasn't seen his parent four years!" and "He hasn't seen his parent for years!", may only be distinguished by their different sound emphasis, showing a clear trap in the audio. As a result, different tones of voice could lead to different meanings, even when the textual sentences are nearly identical. Inspired by these real-world applications and observations, we propose the novel problem of Audio-Oriented Multimodal Machine Comprehension. As shown in Figure <ref type="figure" target="#fig_0">1</ref>, the novel problem requires the system to consider both the audio and textual modality inputs in selecting the correct answers. Compared to the conventional setting of unimodal based MC, our new problem poses two fundamental challenges. First, in the learning of multimodal tasks, due to the great disparities between the textual and the audio domains, plus the distinct features of the modalities, one core challenge is to effectively bridge the gap between textual and the audio domains and learn an effective fusion of multimodality features <ref type="bibr" target="#b18">(Lu et al. 2016;</ref><ref type="bibr" target="#b5">Gao et al. 2019;</ref><ref type="bibr" target="#b13">Liu et al. 2019a</ref><ref type="bibr" target="#b17">Liu et al. , 2020b))</ref>. Second, due to the abundance of unimodal (textual or audio) real-world application scenarios, e.g., the conventional machine reading comprehension (textual) <ref type="bibr" target="#b13">(Lai et al. 2017</ref>) and the end-to-end spoken language understanding (audio) <ref type="bibr" target="#b25">(Serdyuk et al. 2018)</ref>, it is necessary to enable the multimodal MC model to work in the unimodal scenarios. In other words, we should empower the proposed multimodal MC model with the capability to accurately predict the answers based only on the textual or only on audio input. To tackle the first challenge, we propose a novel Dynamic Inter-and Intra-modality Attention (DIIA) model to better capture the high-level interactions between audio and textual features, resulting in an efficient multimodality feature fusion to answer the questions accurately. As shown in Figure <ref type="figure" target="#fig_1">2</ref>, the DIIA integrates the self-attention and coattention to learn the inter-and intra-relationships between audio and textual modalities in an effective manner <ref type="bibr" target="#b13">(Liu et al. 2019a</ref><ref type="bibr">(Liu et al. ,b, 2020a))</ref>. The core intuition behind our motivation is that, each textual word should obtain information not only from its associated audio information but also from related words/phrases to infer the answer to the question, and so do audio. To tackle the second challenge, based on our proposed DIIA model, we further develop a Multimodal Knowledge Distillation (MKD) module, which transfers representative knowledge from multimodal to either textual or audio modality. In implementation, as shown in Figure <ref type="figure" target="#fig_1">2</ref>, the learned multimodal representations from our pretrained multimodal MC model, i.e., DIIA, are used to guide the learning of unimodal representations. As a result, the MKD module associates the multimodal knowledge hidden behind the fused multimodal features to facilitate the understanding of unimodal information. The design of the proposed MKD allows our approach to be applied to scenarios where only single modal data is available. In other words, our approach can accurately answer the questions based only on input audio or input text, so that our approach can be used for fair comparisons with existing textual based MC models.</p><p>Moreover, to better handle this problem, we also collect two audio-oriented multimodal machine comprehension datasets, i.e., L-TOEFL and CET, from English listening tests, which contain questions and answers in the form of text, as well as the comprehension passages in both textual and audio modalities. The extensive experiments and analysis on the proposed L-TOEFL and CET datasets validate our arguments and prove the effectiveness of our approach.</p><p>Overall, our main contributions are as follows:</p><p>• We propose the audio-oriented multimodal machine comprehension task, which requires the system to understand both input audio and textual information together, rather than only use textual information in previous works. We also assemble two audio-oriented multimodal machine comprehension datasets (L-TOEFL and CET) for the task. The experiments also show that the proposed MKD module enables the multimodal MC model to be applied in the unimodal scenarios and outperform the conventional unimodal models significantly.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Related Work</head><p>The related works are introduced from two aspects: 1) Multi-Choices Machine Comprehension and 2) Machine Comprehension of Spoken Content.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Multi-Choices Machine Comprehension</head><p>There are four types of Machine Comprehension (MC) <ref type="bibr" target="#b1">(Chen 2018)</ref>, including cloze-style <ref type="bibr" target="#b8">(Hermann et al. 2015)</ref>, multi-choices <ref type="bibr" target="#b13">(Lai et al. 2017)</ref>, span extraction <ref type="bibr" target="#b23">(Rajpurkar et al. 2016;</ref><ref type="bibr" target="#b22">Rajpurkar, Jia, and Liang 2018)</ref>, and generative format <ref type="bibr" target="#b11">(Kociský et al. 2018)</ref>. In this paper, we focus on multi-choices machine comprehension: the goal is to find the only correct answer in the multiple (usually 4) choices based on the given inputs, i.e., passage, question, and multiple choices.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Machine Comprehension of Spoken Content</head><p>Tseng <ref type="bibr">et al. (2016)</ref> proposed to deal with the MC task of spoken content by first employ an ASR model <ref type="bibr" target="#b32">(Yu and Li 2017)</ref> to recognize speech into text, then, an MC model is designed to process the ASR transcriptions for selecting the correct answer out of 4 choices. Although such method take the spoken content into consideration, the system is still a text-based MC system. <ref type="bibr" target="#b2">Chuang et al. (2020)</ref>; <ref type="bibr" target="#b12">Kuo, Luo, and Chen (2020)</ref> studied the end-to-end spoken question answering problem by introducing a pre-trained modality fusion model learned from audio and text, however, in the inference phase, they need input with both modalities while ours can predict with only audio or only text input. And there are also some related work studied different tasks <ref type="bibr" target="#b31">(Yang et al. 2003)</ref> or auxiliary tool <ref type="bibr">(Zhang et al. 2018)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Approach</head><p>In this section, we first formulate the conventional machine comprehension (MC) and the proposed multimodal machine comprehension problems; then we describe the proposed approach in detail. Specifically, to better capture the high-level interactions between audio and textual features, and generate efficient multimodality feature fusion to accurately answer questions, we propose the novel Dynamic Inter-and Intra-modality Attention (DIIA) model. Based on our DIIA model, we further propose the Multimodal Knowledge Distillation (MKD) to enable our multimodal MC model to accurately predict the answers based only on the text or audio.</p><p>Figure <ref type="figure" target="#fig_1">2</ref> illustrates the proposed approach.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Problem Formulation</head><p>In this section, we formulate the problems of conventional multi-choices machine comprehension and the proposed multi-choices multimodal machine comprehension.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Problem Formulation of Conventional Multi-Choices</head><p>Machine Comprehension Task Taking a passage P I as input, the goal of conventional multi-choices machine comprehension is to predict the correct choice C ans based on the given questions Q and candidate choices C candidate . Some well-performing frameworks <ref type="bibr" target="#b29">(Wang et al. 2018;</ref><ref type="bibr" target="#b4">Dhingra et al. 2017;</ref><ref type="bibr" target="#b3">Devlin et al. 2019</ref>) normally include a text encoder and an answer predictor, which can be formulated as:</p><p>Text Encoder :</p><formula xml:id="formula_0">P I → P Answer Predictor : P, Q, C candidate → C ans</formula><p>The text encoder aims to generate the textual features P of the input passage P I . In implementation, given an input sequence with length N , the text features are usually generated by 300-dimensional word embeddings with GloVe vectors <ref type="bibr" target="#b20">(Pennington, Socher, and Manning 2014)</ref>, and represented as P ∈ R N ×d (d = 300). The answer predictor, e.g., co-matching <ref type="bibr" target="#b29">(Wang et al. 2018)</ref>, is used to predict the correct choice C ans from the given questions Q, the candidate choices C candidate and P. Given the ground truth choice, we can simply train the framework by minimizing training loss, e.g., cross-entropy loss.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Problem Formulation of Multimodal Multi-Choices Machine Comprehension Task</head><p>The main difference between the audio-oriented multimodal machine comprehension and conventional machine comprehension is the different available information that can be used to predict the correct answer. Specifically, for audio-oriented multimodal machine comprehension task, it requires the system to further consider the audio information A I when selecting the correct answer, which can be formulated as:</p><p>Text Encoder : P I → P Audio Encoder : A I → A Answer Predictor : P, A, Q, C candidate → C ans where A denotes the extracted audio features from the input audio A I . In implementation, we adopt VGGish (Hershey et al. 2017) pre-trained on AudioSet <ref type="bibr" target="#b6">(Gemmeke et al. 2017)</ref> to extract the audio features, represented as A ∈ R M ×d .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Dynamic Inter-and Intra-modality Attention</head><p>Our Audio-Oriented Multimodal Machine Comprehension task requires the MC system to understand both input audio and textual information, thus learning fine-grained joint representations of audio and text are of paramount importance. In other words, it is vital to learn the alignments and relationships between audio and textual modalities. To this end, as shown in Figure <ref type="figure" target="#fig_1">2</ref>, we propose the Dynamic Interand Intra-modality Attention (DIIA) to effectively fuse the audio and textual features before predicting answers. In particular, inspired by the success of Multi-Head Attention (MHA) <ref type="bibr" target="#b28">(Vaswani et al. 2017)</ref>, we refer to the MHA mechanism and propose the DIIA model, which consists of an Inter-modality Attention module and an Intra-modality Attention module, to learn the inter-and intra-relationships of audio and textual modalities in an effective manner.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Multi-Head Attention</head><p>In order to extract the relationship between the intra-modality and inter-modality of audio features and textual features, we adopt the Multi-Head Attention (MHA) <ref type="bibr" target="#b28">(Vaswani et al. 2017)</ref>, which compute the association weights between different features. The attention mechanism allows probabilistic many-to-many relations instead of monotonic relations, as in <ref type="bibr" target="#b30">Xu et al. (2015)</ref>; <ref type="bibr" target="#b13">Liu et al. (2019a)</ref>. The following MHA consists of n parallel heads and each head is represented as scaled dot-product attention.</p><formula xml:id="formula_1">Att(Q,K,V) = softmax QW Q (KW K ) T √ d k VW V</formula><p>where Q ∈ R l×d , K ∈ R k×d and V ∈ R k×d represent respectively the query matrix, the key matrix and the value matrix; The l and k denote the length of the query and key/value, respectively; W Q , W K , W V ∈ R d×d k are the learnable parameters of linear transformations and d k = d/n is the scaling factor, where n is the number of heads. Following the multi-head attention is a fully-connected Feed-Forward Network (FFN), which is defined as follows:</p><formula xml:id="formula_2">FFN(x) = ReLU(xW f + b f )W ff + b ff</formula><p>where W f and W ff denote matrices for linear transformation; b f and b ff represent the bias terms. Each sub-layer, i.e., MHA and FFN, is followed by an operation sequence<ref type="foot" target="#foot_0">2</ref> of dropout <ref type="bibr" target="#b26">(Srivastava et al. 2014)</ref>, shortcut connection <ref type="bibr" target="#b7">(He et al. 2016)</ref>, and layer normalization <ref type="bibr" target="#b0">(Ba, Kiros, and Hinton 2016)</ref>.</p><p>We take advantage of the MHA to implement the idea of learning the inter-and intra-relationships of audio and textual modalities. Inter-modality Attention 1) To represent textual features P with high quality, we need to find the most relevant audio descriptions A to identify the direct relations between audio and text. 2) Similarly, we need to find the most relevant textual features P to summarize the properties of the audio features A.</p><p>According to the attention theorem <ref type="bibr" target="#b28">(Vaswani et al. 2017</ref>), taking the first situation as example, the textual features P ∈ R N ×d serve as query, and the audio features A ∈ R M ×d server as key and value. Consequently, the result A inter ∈ R M ×d turns out to be a set of attended audio features for textual features:</p><formula xml:id="formula_3">A inter = FFN(MHA(A, P, P))</formula><p>Similarly, the P inter ∈ R N ×d can be computed as follow:</p><formula xml:id="formula_4">P inter = FFN(MHA(P, A, A))</formula><p>Now we can assume that the relation between the audio and textual features are built and represent the two updated features as A inter ∈ R M ×d and P inter ∈ R N ×d .</p><p>Intra-modality Attention After the inter-modality attention module, the cross-modal relations between audio and text have been modeled. However, information from different modalities may have varying predictive power and noise.</p><p>We argue that modeling relationships in a single modality can make up for this deficiency. The intra-modality attention module explores how the knowledge learned from two modalities can be fused in an appropriate way to help the training of the multimodal MC model. We adapt the following formula to learn the intra-relationships of audio and textual features:</p><formula xml:id="formula_5">A intra = FFN(MHA( Âinter , Âinter , A inter )) P intra = FFN(MHA( Pinter , Pinter , P inter ))</formula><p>It is worth noticing that to better promote the learning of intra-relationships <ref type="bibr" target="#b9">(Hu et al. 2020;</ref><ref type="bibr" target="#b15">Liu et al. 2018)</ref>, we further design a conditional gate operation G to update the queries and keys. The process is defined as follows:</p><formula xml:id="formula_6">Âinter = (1 + G P ) A inter Pinter = (1 + G A ) P inter</formula><p>where represents the element-wise multiplication. The conditional gate operation G P and G A are defined as:</p><formula xml:id="formula_7">G A = σ (Avg pool(A)W A ) G P = σ (Avg pool(P)W P )</formula><p>where the σ and Avg pool denote the sigmoid function and average pooling, respectively.</p><p>Through the formula, in the audio domain, the intramodality attention learns salient audio groupings and integrates naturally related audio information. In the textual domain, it learns text collocations and has the ability to consider associations and collocations of sentences in the passage during answer predicting. The learned intrarelationships of audio and textual features are super beneficial for multimodal machine comprehension task.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Multimodal Knowledge Distillation</head><p>The intuition of enabling a model to accurately predict the answer based only on either the text or the audio is that conventionally the model is only allowed to accept a single modality as input. However, due to the fact that the proposed DIIA have learned the fine-grained multimodal representations, inspired by the knowledge distillation technique <ref type="bibr" target="#b24">(Romero et al. 2015)</ref>, we further introduce the Multimodal Knowledge Distillation Module (MKD) to distill the representative knowledge from multimodal learnt by DIIA to either textual modal or audio modal to enhance the input features with single modality only. In particular, the MKD consists of two Multi-Layer Perceptrons (MLPs). We represent the output of the MLPs as A inter , A intra for audio distillation block and P inter , P intra for passage distillation block, which are defined as:</p><formula xml:id="formula_8">A inter = MLP(A); A intra = MLP(A inter ) P inter = MLP(P); P intra = MLP(P inter )</formula><p>Then we apply the following formulas to distill the knowledge from the output of DIIA to the MKD, which can be computed with mean squared error (MSE) loss and be represented as:</p><formula xml:id="formula_9">L MKD A = MSE(A inter , A inter ) + MSE(A intra , A intra )</formula><p>L MKD P = MSE(P inter , P inter ) + MSE(P intra , P intra )</p><p>In this way, with the help of MKD, we can use only one modal data input in the answer prediction process, while implicitly use the interaction information the two modalities to enhance the unimodal representations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Implementation</head><p>From the above process, the proposed DIIA model focuses on learning the relationships between audio and textual features to obtain the multimodal representations, and the MKD module can distill the multimodal knowledge learned from the DIIA. In this section, we describe the training process detail of our approach by introducing three applied problems, i.e., Multimodal Machine Comprehension, Unimodal Machine Reading Comprehension and Unimodal Machine Listening Comprehension.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Multimodal Machine Comprehension As shown in Fig-</head><p>ure <ref type="figure" target="#fig_1">2</ref>, the outputs of the predictors of multimodal machine comprehension problem are y A and y P for the audio and textual features, respectively. Given the ground truth y label , we adopt the Cross-Entropy (CE) loss function to optimize our multimodal MC model, including the DIIA. The optimization is defined as: pred (y A , y label ) = CE(y A , y label ) pred (y P , y label ) = CE(y P , y label ) Specifically, for better optimization, we further distill the knowledge loss on the logits to narrow the distance between the audio logits and the textual logits through MSE loss:</p><p>pred (y A , y P ) = MSE(y A , y P ) Overall, the final objective loss function is computed as: L 1 = pred (y A , y label ) + pred (y P , y label ) + pred (y A , y P ) At the testing stage, the input P and A are sent to the DIIA to obtain the P intra and A intra , then sent to the predictor to obtain the y A and y P . Finally, we add the two predicted logits as the final predicted logits to predict the answer.</p><p>Unimodal Machine Reading Comprehension For practical use, we further propose to enable our multimodal MC model to accurately predict the answers based only on the text, which means that we only use unimodal textual information in inference. Because the unimodal scenario requires no modal interaction, we remove the DIIA, instead, we introduce an MKD module to transfer the representative multimodal knowledge learned from our DIIA to the textual modality (see Figure <ref type="figure" target="#fig_1">2</ref>). Specifically, at the training stage, we first directly adopt the pre-trained multimodal MC model. Next, we freeze the parameters of the text encoder, DIIA and predictor, and use the proposed L MKD P to train the MKD. At the testing stage, we obtain the input passage features of the predictor Pinter through the MKD module, and then output the predicted logits y P .</p><p>In this way, our model can work on unimodal input in practice with multimodal information being introduced during the distillation training process. So that our model can be compared fairly with conventional MC models.</p><p>Unimodal Machine Listening Comprehension Similar to the Unimodal Machine Reading Comprehension scenario, we train the MKD using L MKD A with the pre-trained multimodal MC model froze. And generate the predicted logits y A with the trained MKD at the test stage.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Experiments</head><p>We describe the collected datasets and the training details, followed by the evaluation of the proposed approach.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Datasets</head><p>In this paper, to address the audio-oriented multimodal MC problem, we propose the L-TOEFL and CET datasets, where L-TOEFL is collected from the TOEFL Educational Testing Service (TOEFL ETS)<ref type="foot" target="#foot_1">3</ref> , which is an English ability test designed to measure the ability to listen for basic comprehension, pragmatic understanding and synthesizing information, and CET is collected from the College English Test (CET)<ref type="foot" target="#foot_2">4</ref> , a national English as a foreign language test in the People's Republic of China. Designed by educational experts, L-TOEFL and CET datasets aim to test non-native English speakers using various types of complicated questions, and the passages are divided into types of conversation and lecture. Specifically, L-TOEFL and CET are collected from a total of 106 official examinations, with each </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Experimental Settings</head><p>The encoder takes the audio features and text features as input. In implementation, we adopt the VGGish (Hershey et al. 2017) pre-trained on AudioSet <ref type="bibr" target="#b6">(Gemmeke et al. 2017</ref>) to extract the audio features denoted as A ∈ R M ×300 . First, we resample the raw audio file to the rate assumed by VGGish, then generate a 128-dimensional embedding of each Au-dioSet segment. After that, we employ a linear transformation to map the dimension from 128 to 300. Thus, the feature dimension of each audio is f rame × 300 where the f rame range from 109 to 469. Given an input sequence with length N , the text features are initialized by 300-dimensional word embeddings with GloVe vectors <ref type="bibr" target="#b20">(Pennington, Socher, and Manning 2014)</ref> denoted as W ∈ R N ×300 . Considering the actual length of the datasets, we set the maximum length M and N as 384 empirically. We adopt the Adam optimizer for optimizing the parameters, with a mini-batch size of 12 and initial learning rate of 0.001. After training 100 epochs, we select the model which works the best on the dev set, and then evaluate it on the test set in terms of accuracy (%).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Experimental Results</head><p>In this section, we will present our evaluation of the unimodal MC models, i.e., conventional Machine Reading Comprehension model, Machine Listening Comprehension model, as well as the models with our proposed MKD module, and the multimodal MC model with our proposed DIIA model. We conduct experiments on three representative baseline MC systems, i.e., GA Reader <ref type="bibr" target="#b4">(Dhingra et al. 2017)</ref>, Co-Matching <ref type="bibr" target="#b29">(Wang et al. 2018)</ref>, and DCMN <ref type="bibr" target="#b33">(Zhang et al. 2019)</ref>. As can be seen in Table <ref type="table" target="#tab_1">1</ref>, under the unimodal setting, the models using text features outperform the models using audio features, which is due to the text is cleaner and easier to be processed by machines. Besides, our proposed MKD help the model distill multimodal knowledge in the unimodal scenarios, and can consequently achieve better performance than the conventional MC models, which verifies the effectiveness of our approach. From the last four rows of Table <ref type="table" target="#tab_1">1</ref>, we find that the fusion of multimodal features can significantly improve the performance of the model, which proves our motivation and effectiveness in proposing the audio-oriented MC task. In addition, compared to the shallow-fusion setting that simply add audio and textual features together, both the proposed Inter-modality Attention and Intra-modality Attention can achieve better performance by learning the inter-and intra-relationships between the two modalities, respectively, so as to obtain better multimodal representations. We also implement two unimodal models (audio and text) via averaging with Co-Matching as the predictor. The ensemble model acquires an accuracy of 44.64 and 52.74 on the L-TOEFL/CET datasets, while the proposed model achieves a 52.03 and 57.83 accuracy, therefore the proposed model consistently outperforms the ensemble of unimodal models. Furthermore, we can see that the model equipped with the DIIA achieves the best performance among different model settings. Please note that our approach is not a replacement of existing approaches, but can be easily integrated into them to boost the performance, such as <ref type="bibr" target="#b36">Zhang et al. (2020)</ref>; <ref type="bibr" target="#b38">Zhu, Zhao, and Li (2020)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Analysis</head><p>In this section, we analyze the effectiveness of the proposed DIIA and visualize the attention weights to show the advantage of the proposed approach in an intuitive manner. We also analyze the effectiveness of different passage types.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Effect of the DIIA</head><p>To explore the effectiveness of the multimodal features and the correlations between audio and textual features learned by DIIA, we visualize the attention weights in the intermodality attention module. As shown in Figure <ref type="figure" target="#fig_2">3</ref>, we find that the unimodal MC system predicts the wrong answer choice 'C' while the multimodal MC system predicts the correct answer choice 'A'. To explore the reason for this difference, we visualize the attention weights in the intermodality attention module, finding that by introducing the audio features and the DIIA model, the passage can put more attention on the key information including transitional, time and noun words, while the audio can extract more useful information, e.g., the tone information. The visualization of the attention weights on audio and textual information also verify our hypothesis and demonstrate the effectiveness of our approach. Since our model can capture the alignments and relationships between audio and textual modalities, the distribution of attention weights between audio and textual features is similar, which indicates that the textual features are properly enriched by the aligned audio features.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Types of Passage</head><p>L-TOEFL is composed of different types of passages, namely conversation and lecture, and they also correspond to different types of audio. Such difference may be exhibited in audio length, the number of characters or the overall audio emotions, etc. To better understand the improvement brought by audio information, we further explore the impacts of the passage and audio types on the MC system. Specifically, we conduct experiments for conversation and lecture passages on different model settings, i.e., the conventional unimodal model, the "w/ MKD" model, and the proposed multimodal model. We use the co-matching model <ref type="bibr" target="#b29">(Wang et al. 2018)</ref> as the predictor. Table <ref type="table" target="#tab_2">2</ref> shows that our approach brings greater improvements in passages of conversation type than passages of lecture type. We attribute this to the fact that the human mood and tone change more evidently in human conversation and are relatively smoother in lecture, which means the conversation audio can provide more useful information to the model and the multimodal MC system can predict the answer more precisely.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Conclusions</head><p>In this paper, we introduce the Audio-Oriented Multimodal Machine Comprehension task. To achieve this goal, we collect two datasets named L-TOEFL and CET, which consist of 1,324 and 876 audio-passage-question-choices pairs, respectively, and propose a Dynamic Inter-and Intra-modality Attention (DIIA) model, which consists of an inter-modality attention model and an intra-modality attention model. The DIIA model can learn the inter-and intra-relationships between the audio and the textual modalities. DIIA can work as an independent component and thus can be easily integrated into existing machine comprehension models. Furthermore, considering the abundance of unimodal (textual or audio) real-world application scenarios, we further develop a Multimodal Knowledge Distillation (MKD) module to enable our multimodal MC model to accurately predict the answers based only on either the text or the audio. The experimental results and the analysis demonstrates that audio input can improve the accuracy of machine comprehension models that solely relies on textual input. Specifically, the multimodal MC models achieve better results than the unimodal MC models, and the proposed DIIA model could fuse the audio and textual modalities effectively, thus boosts the baseline models by up to 21.08% in terms of accuracy; Furthermore, the MKD module allows our multimodal MC model (pretrain with both audio and textual input, predict with either audio or textual input) to outperform the unimodal models (train and predict with only audio or textual input) by up to 18.87% in terms of accuracy.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: Comparison between the conventional unimodal machine comprehension and the proposed audio-oriented multimodal machine comprehension. The red colored text indicate the ground-truth answer.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>Figure2: The architecture of our proposed approach. The DIIA model consists of the inter-modality attention module and the intra-modality attention module, aiming to capture the correlation and build the relationship between audio and textual modalities. The blue dashed lines represent the MSE loss between y A , y P (the training label) and y label (the ground truth label). The red dashed lines represent the hidden states based distillation. The predictor is adapted from the existing machine comprehension models, such as Co-Matching<ref type="bibr" target="#b29">(Wang et al. 2018)</ref>.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: Visualization of the attention weights on the audio and passage in the inter-modality module of our proposed Dynamic Inter-and Intra-modality Attention. Each token's importance score is calculated by summing up the attention weight between this token and the tokens of other input modality. Darker color means higher weights. The red and green color denote the correct and wrong answer, respectively.</figDesc><graphic url="image-7.png" coords="7,54.00,41.04,503.99,121.59" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 1 :</head><label>1</label><figDesc><ref type="bibr" target="#b4">Dhingra et al. 2017)</ref> <ref type="bibr" target="#b29">(Wang et al. 2018)</ref> <ref type="bibr" target="#b33">(Zhang et al. 2019)</ref> <ref type="bibr" target="#b4">(Dhingra et al. 2017)</ref> <ref type="bibr" target="#b29">(Wang et al. 2018)</ref> <ref type="bibr" target="#b33">(Zhang et al. 2019)</ref> Performance (Accuracy (%)) on the proposed L-TOEFL and CET datasets. Multimodal and Unimodal represent the input modalities we use for the models, i.e., audio and text, audio only, or text only. Conventional means the conventional models that only employ the predictor and the feature encoder. "w/ MKD" means that we employ our proposed MKD module as well as our training method in the unimodal setting. Shallow fusion means directly add the two unimodal representations for prediction and bypass the DIIA module. official examination contains 1 to 6 passage(s) with corresponding audio, each passage contains 1 to 6 question(s), and each question is accompanied with 4 choices. Thus, our datasets consist of 4-attributes pair: {audio, passage, question, 4 answer choices with the correct one}. After deleting some improper pairs, such as multiple answers (more than one correct answer), etc., we get a total of 2,200 such A-P-Q-C pairs. We randomly divide the L-TOEFL and CET datasets into 1000/162/162 and 657/110/109 examples as for train/dev/test data partitioning, respectively, following ratios of 0.75/0.125/0.125. The volume of collected English listening test data set is larger than the one used in<ref type="bibr" target="#b27">Tseng et al. (2016)</ref> (963 examples).</figDesc><table><row><cell></cell><cell></cell><cell>Datasets</cell><cell></cell><cell></cell><cell></cell><cell cols="2">L-TOEFL</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>CET</cell><cell></cell></row><row><cell cols="2">Modalities</cell><cell cols="2">Methods (Data Partitions Dev</cell><cell cols="2">GAReader Test</cell><cell cols="2">Co-Matching Dev Test</cell><cell>Dev</cell><cell>DCMN Test</cell><cell>Dev</cell><cell cols="2">GAReader Test</cell><cell cols="2">Co-Matching Dev Test</cell><cell>Dev</cell><cell>DCMN Test</cell></row><row><cell>Unimodal</cell><cell>Audio Text</cell><cell>Conventional w/ MKD Conventional w/ MKD</cell><cell cols="2">31.29 43.76 36.35 44.69</cell><cell>30.63 43.31 36.19 43.60</cell><cell>34.55 48.34 38.26 49.98</cell><cell>33.01 48.09 38.75 49.20</cell><cell>43.42 62.40 46.85 64.33</cell><cell>41.61 60.51 46.77 62.68</cell><cell cols="2">34.11 47.11 41.73 49.04</cell><cell>33.82 46.44 41.22 47.98</cell><cell>39.96 52.66 47.73 55.31</cell><cell>39.44 52.50 47.90 55.00</cell><cell>52.86 65.39 60.07 67.63</cell><cell>51.61 63.99 58.68 66.18</cell></row><row><cell></cell><cell></cell><cell cols="3">Shallow-fusion 46.28</cell><cell>45.19</cell><cell>50.68</cell><cell>50.75</cell><cell>63.22</cell><cell>62.69</cell><cell cols="2">51.33</cell><cell>48.62</cell><cell>57.81</cell><cell>56.11</cell><cell>66.52</cell><cell>64.39</cell></row><row><cell cols="2">Multimodal</cell><cell>w/ Inter-w/ Intra-</cell><cell cols="2">47.15 45.98</cell><cell>46.11 45.60</cell><cell>52.40 51.93</cell><cell>51.34 51.14</cell><cell>64.01 63.79</cell><cell>63.77 62.92</cell><cell cols="2">53.13 52.05</cell><cell>50.74 50.22</cell><cell>58.03 57.85</cell><cell>56.50 56.49</cell><cell>68.94 67.35</cell><cell>67.80 66.01</cell></row><row><cell></cell><cell></cell><cell>w/ DIIA</cell><cell cols="2">48.36</cell><cell>47.78</cell><cell>53.22</cell><cell>52.03</cell><cell>65.94</cell><cell>63.68</cell><cell cols="2">54.20</cell><cell>51.12</cell><cell>59.01</cell><cell>57.83</cell><cell>69.13</cell><cell>68.09</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2 :</head><label>2</label><figDesc>Comparison between different type of passage and audio in the L-TOEFL dataset. Con. and Lec. stand for the Conversation passages and Lecture passages, respectively. ∆ denotes the improvement over the model under the conventional setting. The models are explored with the Co-Matching<ref type="bibr" target="#b29">(Wang et al. 2018)</ref> as the predictor.</figDesc><table><row><cell>Modalities</cell><cell>Methods</cell><cell>Con.</cell><cell>∆</cell><cell>Lec.</cell><cell>∆</cell></row><row><cell>Unimodal</cell><cell cols="2">Conventional 37.94</cell><cell>-</cell><cell>37.72</cell><cell>-</cell></row><row><cell>(Text)</cell><cell>w/ MKD</cell><cell>47.63</cell><cell>+9.69</cell><cell>46.37</cell><cell>+8.65</cell></row><row><cell>Multimodal</cell><cell>w/ DIIA</cell><cell cols="4">52.63 +14.69 52.17 +14.45</cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_0">For conciseness, the operation sequence in this paper is omitted.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3" xml:id="foot_1">https://www.ets.org/toefl/</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4" xml:id="foot_2">http://cet.neea.edu.cn/</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgments</head><p>Special acknowledgments are given to AOTO-PKUSZ Joint Research Center for Artificial Intelligence on Scene Cognition Technology Innovation for its support. We thank all the anonymous reviewers for their constructive comments and suggestions.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">J</forename><surname>Ba</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Kiros</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1607.06450</idno>
		<title level="m">Layer Normalization</title>
				<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">Neural Reading Comprehension and Beyond</title>
		<author>
			<persName><forename type="first">D</forename><surname>Chen</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
		<respStmt>
			<orgName>Stanford University</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">PhD thesis</note>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">Speech-BERT: An Audio-and-Text Jointly Learned Language Model for End-to-End Spoken Question Answering</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Chuang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Lee</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note>In IN-TERSPEECH</note>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding</title>
		<author>
			<persName><forename type="first">J</forename><surname>Devlin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Toutanova</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NAACL-HLT</title>
				<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Gated-Attention Readers for Text Comprehension</title>
		<author>
			<persName><forename type="first">B</forename><surname>Dhingra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">W</forename><surname>Cohen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Salakhutdinov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL</title>
				<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Dynamic Fusion With Intra-and Inter-Modality Attention Flow for Visual Question Answering</title>
		<author>
			<persName><forename type="first">P</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>You</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">C H</forename><surname>Hoi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
				<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Audio Set: An ontology and human-labeled dataset for audio events</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">F</forename><surname>Gemmeke</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">P W</forename><surname>Ellis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Freedman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Jansen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Lawrence</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">C</forename><surname>Moore</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Plakal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Ritter</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICASSP</title>
				<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Deep Residual Learning for Image Recognition</title>
		<author>
			<persName><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
				<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Teaching Machines to Read and Comprehend</title>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">M</forename><surname>Hermann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Kociský</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Grefenstette</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Espeholt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Kay</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Suleyman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Blunsom</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Hershey</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Chaudhuri</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">P W</forename><surname>Ellis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">F</forename><surname>Gemmeke</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Jansen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">C</forename><surname>Moore</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Plakal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Platt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">A</forename><surname>Saurous</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Seybold</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Slaney</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">J</forename><surname>Weiss</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">W</forename><surname>Wilson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
				<imprint>
			<date type="published" when="2015">2015. 2017</date>
		</imprint>
	</monogr>
	<note>CNN architectures for large-scale audio classification. In ICASSP</note>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Squeeze-and-Excitation Networks</title>
		<author>
			<persName><forename type="first">J</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Albanie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Wu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Pattern Anal. Mach. Intell</title>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Federated Learning for Spoken Language Understanding</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Zou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">COLING</title>
				<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<author>
			<persName><forename type="first">T</forename><surname>Kociský</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Schwarz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Blunsom</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Dyer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">M</forename><surname>Hermann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Melis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Grefenstette</surname></persName>
		</author>
		<title level="m">The Narra-tiveQA Reading Comprehension Challenge</title>
				<imprint>
			<publisher>TACL</publisher>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">An Audio-Enriched BERT-Based Framework for Spoken Multiple-Choice Question Answering</title>
		<author>
			<persName><forename type="first">C</forename><surname>Kuo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">INTERSPEECH</title>
				<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">Aligning Visual Regions and Textual Concepts for Semantic-Grounded Image Representations</title>
		<author>
			<persName><forename type="first">G</forename><surname>Lai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Hovy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Sun</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1704.04683</idno>
		<imprint>
			<date type="published" when="2017">2017. 2019a</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
	<note>RACE: Large-scale ReAding Comprehension Dataset From Examinations. In NeurIPS</note>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Exploring and Distilling Cross-Modal Information for Image Captioning</title>
		<author>
			<persName><forename type="first">F</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Lei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IJCAI</title>
				<imprint>
			<date type="published" when="2019">2019b</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">simNet: Stepwise Image-Topic Merging Network for Generating Detailed and Comprehensive Image Captions</title>
		<author>
			<persName><forename type="first">F</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP</title>
				<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Federated Learning for Vision-and-Language Grounding Problems</title>
		<author>
			<persName><forename type="first">F</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Ge</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Zou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
				<imprint>
			<date type="published" when="2020">2020a</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Bridging the Gap between Vision and Language Domains for Improved Image Captioning</title>
		<author>
			<persName><forename type="first">F</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Ge</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Zou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Zhang</surname></persName>
		</author>
		<idno>CoRR abs/1907.01118</idno>
	</analytic>
	<monogr>
		<title level="m">Neural Machine Reading Comprehension: Methods and Trends</title>
				<imprint>
			<date type="published" when="2019">2020b. 2019c</date>
		</imprint>
	</monogr>
	<note>ACM Multimedia</note>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Hierarchical Question-Image Co-Attention for Visual Question Answering</title>
		<author>
			<persName><forename type="first">J</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Batra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Parikh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NeurIPS</title>
				<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">MS MARCO: A Human Generated MAchine Reading COmprehension Dataset</title>
		<author>
			<persName><forename type="first">T</forename><surname>Nguyen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Rosenberg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Tiwary</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Majumder</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Deng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
				<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Glove: Global Vectors for Word Representation</title>
		<author>
			<persName><forename type="first">J</forename><surname>Pennington</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP</title>
				<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">A Survey on Neural Machine Reading Comprehension</title>
		<author>
			<persName><forename type="first">B</forename><surname>Qiu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Sun</surname></persName>
		</author>
		<idno>CoRR abs/1906.03824</idno>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Know What You Don&apos;t Know: Unanswerable Questions for SQuAD</title>
		<author>
			<persName><forename type="first">P</forename><surname>Rajpurkar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Liang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL</title>
				<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">SQuAD: 100, 000+ Questions for Machine Comprehension of Text</title>
		<author>
			<persName><forename type="first">P</forename><surname>Rajpurkar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Lopyrev</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Liang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP</title>
				<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">FitNets: Hints for Thin Deep Nets</title>
		<author>
			<persName><forename type="first">A</forename><surname>Romero</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Ballas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">E</forename><surname>Kahou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Chassang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Gatta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
				<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Towards End-to-end Spoken Language Understanding</title>
		<author>
			<persName><forename type="first">D</forename><surname>Serdyuk</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Fuegen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICASSP</title>
				<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Dropout: a simple way to prevent neural networks from overfitting</title>
		<author>
			<persName><forename type="first">N</forename><surname>Srivastava</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Salakhutdinov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<title level="m" type="main">Towards machine comprehension of spoken content: Initial TOEFL listening comprehension test by machine</title>
		<author>
			<persName><forename type="first">B.-H</forename><surname>Tseng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S.-S</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H.-Y</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L.-S</forename><surname>Lee</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
	<note>In IN-TERSPEECH</note>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Attention is All you Need</title>
		<author>
			<persName><forename type="first">A</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">N</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Polosukhin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
				<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">A Co-Matching Model for Multi-choice Reading Comprehension</title>
		<author>
			<persName><forename type="first">S</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Chang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL</title>
				<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Show, Attend and Tell: Neural Image Caption Generation with Visual Attention</title>
		<author>
			<persName><forename type="first">K</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Ba</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Kiros</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Courville</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Salakhudinov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Zemel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
				<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">VideoQA: question answering on news video</title>
		<author>
			<persName><forename type="first">H</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Chaisorn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Neo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Chua</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACM Multimedia</title>
				<imprint>
			<date type="published" when="2003">2003</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Recent progresses in deep learning based acoustic models</title>
		<author>
			<persName><forename type="first">D</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE CAA J. Autom. Sinica</title>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
		<title level="m" type="main">Dual Co-Matching Network for Multi-choice Reading Comprehension</title>
		<author>
			<persName><forename type="first">S</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Zhou</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note>CoRR abs/1901.09381</note>
</biblStruct>

<biblStruct xml:id="b34">
	<monogr>
		<title/>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Kozareva</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">J</forename><surname>Smola</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Song</surname></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Variational Reasoning for Question Answering With Knowledge Graph</title>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
				<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">SG-Net: Syntax-Guided Machine Reading Comprehension</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Duan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
				<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">PIN: A Novel Parallel Interactive Network for Spoken Language Understanding</title>
		<author>
			<persName><forename type="first">P</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Zou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICPR</title>
				<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<monogr>
		<title level="m" type="main">Dual Multi-head Coattention for Multi-choice Reading Comprehension</title>
		<author>
			<persName><forename type="first">P</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
		<idno>CoRR abs/2001.09415</idno>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
