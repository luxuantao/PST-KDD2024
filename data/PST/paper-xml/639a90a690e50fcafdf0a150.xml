<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">TOWARDS SMOOTH VIDEO COMPOSITION</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date type="published" when="2022-12-14">14 Dec 2022</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Qihang</forename><surname>Zhang</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">The Chinese University of Hong Kong</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Ceyuan</forename><surname>Yang</surname></persName>
							<affiliation key="aff1">
								<orgName type="laboratory">Shanghai AI Laboratory</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Yujun</forename><surname>Shen</surname></persName>
							<affiliation key="aff2">
								<orgName type="department">Ant Group</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Yinghao</forename><surname>Xu</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">The Chinese University of Hong Kong</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Bolei</forename><surname>Zhou</surname></persName>
							<affiliation key="aff3">
								<orgName type="institution">University of California</orgName>
								<address>
									<settlement>Los Angeles</settlement>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">TOWARDS SMOOTH VIDEO COMPOSITION</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2022-12-14">14 Dec 2022</date>
						</imprint>
					</monogr>
					<idno type="arXiv">arXiv:2212.07413v1[cs.CV]</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-01-03T08:58+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Video generation requires synthesizing consistent and persistent frames with dynamic content over time. This work investigates modeling the temporal relations for composing video with arbitrary length, from a few frames to even infinite, using generative adversarial networks (GANs). First, towards composing adjacent frames, we show that the alias-free operation for single image generation, together with adequately pre-learned knowledge, brings a smooth frame transition without compromising the per-frame quality. Second, by incorporating the temporal shift module (TSM), originally designed for video understanding, into the discriminator, we manage to advance the generator in synthesizing more consistent dynamics. Third, we develop a novel B-Spline based motion representation to ensure temporal smoothness to achieve infinite-length video generation. It can go beyond the frame number used in training. A low-rank temporal modulation is also proposed to alleviate repeating contents for long video generation. We evaluate our approach on various datasets and show substantial improvements over video generation baselines. Code and models will be publicly available at https://genforce.github.io/StyleSV.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">INTRODUCTION</head><p>Synthesizing images using generative adversarial network (GAN) <ref type="bibr" target="#b12">(Goodfellow et al., 2014;</ref><ref type="bibr" target="#b35">Radford et al., 2016;</ref><ref type="bibr" target="#b20">Karras et al., 2019;</ref><ref type="bibr">2020b;</ref><ref type="bibr">2021;</ref><ref type="bibr">2018)</ref> usually requires composing diverse visual concepts and their spatial arrangement. Recent advances in GANs have enabled many appealing applications such as customized editing <ref type="bibr" target="#b11">(Goetschalckx et al., 2019;</ref><ref type="bibr" target="#b41">Shen et al., 2020;</ref><ref type="bibr" target="#b17">Jahanian et al., 2020;</ref><ref type="bibr" target="#b61">Yang et al., 2021)</ref> and animation <ref type="bibr" target="#b34">(Qiu et al., 2022;</ref><ref type="bibr" target="#b0">Alaluf et al., 2022)</ref>. However, employing GANs for video generation remains challenging given the additional requirement of modeling the temporal dynamics.</p><p>In fact, a video is not simply a stack of images. Instead, the content in video frames should have a smooth transition over time, and the video may last arbitrarily long. Thus, compared to image synthesis, the crux of video synthesis lies in modeling the temporal relations across frames. We categorize the temporal relations into three folds regarding the time scale. First, when looking at a transient dynamic, we focus more on the subtle change between neighbor frames and expect decent local motions, such as facial muscle movement and cloud drifting. When the duration becomes longer, more content may vary across frames. Under such a case, it is vital to learn a consistent global motion representation. For example, in a first-view driving video, trees and buildings on the sides of the road should move backward when the car is driving forward. Finally, in videos that last longer, new objects have to be generated over time. Therefore it requires the movements of the newly added objects are consistent and coherent.</p><p>This work aims at smooth video composition by modeling multi-scale temporal relations with GANs. First, we confirm that, same as in image synthesis, the texture sticking problem (i.e., some visual concepts are bound to their coordinates) also exists in video generation, interrupting the smooth flow of content across frames. To tackle this issue, we deploy the alias-free technique <ref type="bibr" target="#b23">(Karras et al., 2021)</ref> from single image generation and preserve the frame quality via appropriate pretraining. Then, to make the generator produce realistic motion dynamics, we incorporate a temporal shift module (TSM) <ref type="bibr" target="#b29">(Lin et al., 2019)</ref> in the discriminator as an inductive bias. Thus the discriminator captures more information from the temporal dimension for real/fake classification, providing better guidance to the generator. Furthermore, we observe that the motion representation proposed in previous work <ref type="bibr" target="#b44">(Skorokhodov et al., 2022)</ref> suffers from undesired content jittering (see Sec. 2.4 for details) for long video generation. We identify its cause as the first-order discontinuity when interpolating motion embeddings. To mitigate this issue, we design a novel B-Spline based motion representation that can transit smoothly over time. A low-rank strategy is further proposed to alleviate the issue that the content across frames may repeat cyclically. We evaluate the improved models on various video generation benchmarks, including YouTube driving dataset <ref type="bibr" target="#b64">(Zhang et al., 2022)</ref>, SkyTimelapse <ref type="bibr" target="#b58">(Xiong et al., 2018)</ref>, Taichi-HD <ref type="bibr" target="#b43">(Siarohin et al., 2019b)</ref> and observe consistent and substantial improvements over existing alternatives. Given its simplicity and efficiency, our approach brings a simple yet strong baseline for video generation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">METHOD</head><p>We introduce several improvements made on the prior art StyleGAN-V <ref type="bibr" target="#b44">(Skorokhodov et al., 2022)</ref> to set a new baseline for video generation. We first introduce the default configuration (Config-A) of the StyleGAN-V in Sec. 2.1. We then make a comprehensive overhaul of it. Concretely, in Sec. 2.2 we confirm that the alias-free technique (Config-B) in single image generation, together with adequately pre-learned knowledge (Config-C), results in a smooth transition of two adjacent frames. Sec. 2.3 shows that when the temporal information is explicitly modeled into the discriminator through the temporal shift module <ref type="bibr" target="#b29">(Lin et al., 2019)</ref> (Config-D), the generator produces significantly better dynamic content across frames. Although prior arts could already generate arbitrarily long videos, the cyclic jittering is observed as a video continues. Thus We propose a B-Spline based motion representation (Config-E) to ensure the continuity, which together with a low-rank temporal modulation (Config-F) produces much more realistic long videos in Sec. 2.4.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">PRELIMINARY</head><p>StyleGAN-V <ref type="bibr" target="#b44">(Skorokhodov et al., 2022)</ref> introduces continuous motion representations and a holistic discriminator for video generation. Specifically, continuous frames I t could be obtained by feeding continuous t into a generator G(?):</p><formula xml:id="formula_0">I t = G(u, v t ),<label>(1)</label></formula><p>where u and v t denote the content code and continuous motion representation. Concretely, content code is sampled from a standard distribution while the motion representation v t consists of two embeddings: time positional embedding v pe t and interpolated motion embedding v me t . To obtain the time positional embedding v pe t , we first randomly sample N A codes as a set of discrete time anchors A i , i ? [0, ? ? ? , N A -1] that share equal interval (256 frames in practice). Convolutional operation with 1D kernel is then applied on anchor A i for temporal modeling, producing the corresponding features a i with timestamp t i . For an arbitrary continuous t, its corresponding interval is first found with the nearest left and right anchor feature a l and a l+1 , at time t l and t l+1 , so that </p><formula xml:id="formula_1">t l ? t &lt; t l+1 , l ? [0, ? ? ? , N A -2]. For time positional embedding v pe t ,</formula><formula xml:id="formula_2">: ? = M ? (a l ), ? = M ? (a l ), ? = M ? (a l ), v pe t =&lt; ? ? sin(? ? t + ?), ? ? cos(? ? t + ?) &gt;,<label>(2)</label></formula><p>where M ? , M ? , M ? are learnable MLPs. The interpolated motion embedding v me t could be obtained through linear interpolation between anchor feature a l and a l+1 based on t:</p><formula xml:id="formula_3">v me t = (t -t l )a l + (t l+1 -t)a l+1 t l+1 -t l .<label>(3)</label></formula><p>The final motion representation v t is thus the sum of these two terms:</p><formula xml:id="formula_4">v t = v pe t + v me t .<label>(4)</label></formula><p>Additionally, a sparse sampling strategy is proposed for efficient training. Namely, N t discrete time steps t i and real video frames I i are sampled respectively. By feeding N t discrete time steps and one content code into generator, multiple synthesis I i = G(u, v ti ) are obtained. In order to distinguish real video frames from synthesized ones, a discriminator D(?) extracts feature y i for the individual frame, and then a fusion function ? would perform the temporal aggregation over all frames:</p><formula xml:id="formula_5">y = ?(y i ) = ?(D(I i )), i = 0, 1, ? ? ? , N t -1.<label>(5)</label></formula><p>The final discriminative logit l is computed by a MLP M conditioned on time steps:</p><formula xml:id="formula_6">l = M (y, t 0 , ? ? ? , t Nt-1 ).<label>(6)</label></formula><p>In the following sections, we would make a comprehensive overhaul of the default config, leading to a new simple yet strong approach for video generation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">ALIAS-FREE OPERATIONS AND PRE-LEARNED KNOWLEDGE</head><p>Generated frames within a short temporal window usually share similar visual concepts while the micro motion gradually occurs. Therefore, the smooth transition between frames tends to make the synthesis much more realistic. However, by examining the synthesized frames of StyleGAN-V, we find that texture sticking <ref type="bibr" target="#b23">(Karras et al., 2021)</ref> exists. We track the pixels at certain coordinates as the video continues and the brush effect in Fig. <ref type="figure" target="#fig_0">2a</ref> indicates that these pixels actually move little. Thus, the texture sticks to fixed coordinates.</p><p>Alias-free operations (Config-B). To mitigate the texture sticking issue, we follow the Style-GAN3 <ref type="bibr" target="#b23">(Karras et al., 2021)</ref> to adopt the alias-free technique in the generator, which reduces the dependency on absolute pixel coordinates. As shown in Fig. <ref type="figure" target="#fig_0">2b</ref>, brush effect disappears i.e., texture sticking is significantly alleviated by the alias-free technique.</p><p>However, as reported in Tab. 1, direct adoption of StyleGAN3 brings lower performance in terms of the FID and FVD scores. One possible reason is that the training receipt is directly inherited We can observe the brush effect from the baseline <ref type="bibr" target="#b44">(Skorokhodov et al., 2022)</ref>, where the pixel values are strongly anchored to the coordinates. Instead, our approach achieves smoother frame transition between neighbor frames.</p><p>from the original StyleGAN-V without any modification. Despite its impairment on FID and FVD to some extent, we still use StyleGAN3 for the sake of its alias-free benefit.</p><p>Pre-learned knowledge (Config-C). Simply replacing the generator backbone with StyleGAN3 deteriorates the synthesis. To fix it, we propose to make the best of pre-learned knowledge of image generation network. As discussed before, the key challenge for video generation is to model temporal relations between frames. However, it is entangled with the target of modeling image distribution. We confirm that pre-training at the image level and then fine-tuning at the video level can decouple the two targets of modeling individual frames and cross-frame dynamics. As discovered in <ref type="bibr" target="#b20">(Karras et al., 2019;</ref><ref type="bibr" target="#b61">Yang et al., 2021</ref>), GAN's deep layers mainly control details, for example, coloring, detailed refinements, and etc. We hypothesize that these parts' knowledge can be pre-learned and re-used in the video generation learning process.</p><p>To be specific, we set N t as 1 in Eq. ( <ref type="formula" target="#formula_5">5</ref>), and train the whole network from scratch as the image pretraining stage. Then deep layers' weights are loaded in the video fine-tuning stage. With pre-trained deep layers' weights, now the optimization process can focus on early layers to learn to model sequential changes in structures that show realistic movements. As shown in Tab. 1, after adopting Config-C, the generated quality gets improved compared to Config-B. Now, with the alias-free technique, we achieve smooth frame transition without compromising the per-frame quality.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">EXPLICIT TEMPORAL REASONING IN DISCRIMINATOR</head><p>In the adversarial learning of GAN, it is essential to have a strong discriminator, assuring the sufficient training of the generator. In the video generation task, the discriminator thus has to model the temporal relations of multiple frames to distinguish unnatural movements from real ones. However, in previous work only a simple concatenation operation ? is used: y = ? i y i , where y i denotes a single frame's feature extracted by the discriminator and y denotes the feature after temporal fusion.</p><p>Temporal modeling (Config-D). We incorporate an explicit temporal modeling approach i.e., temporal shift module (TSM) that shows exceptional performance for video classification. Concretely, at each convolution layer before temporal fusion, we have features from multiple frames:</p><formula xml:id="formula_7">y n i ? R H?W ?C , i = 0, ? ? ? , N t -1,</formula><p>where n is the layer index, H and W are the feature map resolution and C is the feature channel number. Then, TSM performs channel-wise swapping between adjacent frames:</p><formula xml:id="formula_8">y n i (x, y) ? ? 3 (y n i-1 (x, y)[: C 8 ], y n i (x, y)[ C 8 : 7C 8 ], y n i+1 (x, y)[ 7C 8 :]),<label>(7)</label></formula><p>where ? 3 (?) is a concatenation operation, and (x, y) are arbitrary coordinates. In this way, one quarter of the channels of a single frame after temporal fusion contain information from adjacent frames. Follow-up convolution kernels in deeper layers can perform efficient temporal reasoning based on this mixed representation. In our ablation study, while FID becomes slightly worse </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.4">TOWARDS INFINITE VIDEO GENERATION</head><p>The metrics FID, FVD 16 and FVD 128 used before merely measure the synthesis quality of the relatively short videos (e.g., 128 frames usually cover around 6 seconds), we further investigate whether current configuration is able to produce arbitrarily long videos. Note that, by saying arbitrarily long, we mean infinite length along the time axis, not infinite content. Ideally, we could easily generate an infinite number of frames by continuously sampling time t. The generator is supposed to produce the corresponding synthesis. However, as the video continues, a conspicuous jittering effect occurs periodically every 10 seconds. For example, Fig. <ref type="figure" target="#fig_1">3</ref> shows the crossroad moves forward at the beginning, and then suddenly goes backward.</p><p>Discontinuity of motion embeddings. As introduced in Sec. 2.1, motion embedding v t contains time positional embedding v pe t and interpolated motion embedding v me t . In particular, v pe t could be obtained by a learned wave function on the left nearest anchor feature a l while the motion embedding v me t derives from the linear interpolation between the left and right anchor feature a l and a l+1 . This interpolation would result in the first-order discontinuity for both learned wave function and linear interpolation when getting through multiple anchors, as shown in Fig. <ref type="figure" target="#fig_3">4a</ref>. Moreover, when the T-SNE (Van der <ref type="bibr" target="#b49">Maaten &amp; Hinton, 2008)</ref> is applied to the motion embedding given a long period, we can see in Fig. <ref type="figure" target="#fig_5">5a</ref> that such discontinuity causes the drastic change and jittering in the generated content.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B-Spline based motion representations (Config-E).</head><p>With an assumption of the discontinuity of motion embeddings, we design a basic spline (B-Spline) based motion representation that could guarantee first-order numerical smoothness. To be specific, B-Spline of order n is a piece-wise polynomial function of degree n -1 in a given variable. It is widely used in computer-aided design and computer graphics for curve fitting, controlling a smooth curve with calculated weights w i on several control points. In particular, the number of control points is determined by the order of the B-Spline. With a pre-defined knot sequence {t i }, a B-Spline with any order can be defined by means of the Cox-de Boor recursion formula <ref type="bibr">(de Boor, 1971</ref>). The first-order B-Spline is then defined by:</p><formula xml:id="formula_9">B i,1 (t) := 1 if t i ? t &lt; t i+1 , 0 otherwise. (<label>8</label></formula><formula xml:id="formula_10">)</formula><p>Here i is the interval index. B-spline of higher orders is further defined recursively:</p><formula xml:id="formula_11">B i,k+1 (t) := ? i,k (t)B i,k (t) + [1 -? i+1,k (t)] B i+1,k (t), (<label>9</label></formula><formula xml:id="formula_12">)</formula><p>where k is the order and  B-spline is able to smooth the interpolation between various anchor features, and hence improve the first-order numerical continuity.</p><formula xml:id="formula_13">? i,k (t) := t-ti t i+k -ti t i+k = t i , 0 otherwise. (<label>10</label></formula><formula xml:id="formula_14">)</formula><p>We treat each anchor feature a i as the control point and use B i,k (t) as its corresponding weight given time t. Hence, B-Spline based anchor feature is defined as:</p><formula xml:id="formula_15">?(t) = i B i,k (t)a i .<label>(11)</label></formula><p>We further use ?(t) instead of discretized a i to calculate time positional embedding v pe t and interpolated motion embedding v me t as defined in Eq. ( <ref type="formula" target="#formula_2">2</ref>) and Eq. (3). Fig. <ref type="figure" target="#fig_3">4b</ref> suggests the continuity property of B-Spline. Meanwhile, through T-SNE again, the B-Spline based motion representations become much smoother in Fig. <ref type="figure" target="#fig_5">5b</ref>. As shown in Fig. <ref type="figure" target="#fig_1">3</ref>, the crossroad gradually approaches the observer without any sudden jittering.</p><p>Low-rank temporal modulation (Config-F). After adopting B-Spline based motion embedding, the jittering effect gets erased. However, we find that similar content periodically appears as video continues(see Fig. <ref type="figure" target="#fig_6">A6a</ref>). This result implies that motion embedding poorly represents visual concepts. We thus hypothesize that the new motion embedding might be endowed with a stronger capacity to represent various visual concepts. Inspired by recent works <ref type="bibr" target="#b2">(Bau et al., 2020;</ref><ref type="bibr">Wang et al., 2022a)</ref>, we suppress the representation capacity through the low-rank strategy.</p><p>Original StyleGAN-based architecture incorporates styles into convolutional kernels via modulation trick <ref type="bibr">(Karras et al., 2020a)</ref>. When generating videos, a similar technique is also applied with small modifications. Content embedding u together with the motion embedding v t would be fed into an affine layer M , generating the style code to modulate the original weight W :</p><formula xml:id="formula_16">W = W ? M (u ? v t ),<label>(12)</label></formula><p>where ? stands for the concatenation operation. That is, motion and content embedding could equally contribute to the final style to change the visual concepts of frames. In order to suppress the representational capacity of motion embeddings, we first separate the motion and content codes and then replace the kernel regarding the motion code with a low-rank one:</p><formula xml:id="formula_17">W = W co ? M co (u) + W mo ? M mo (v t ), where W mo = U V,<label>(13)</label></formula><p>where U, V is tall matrix that guarantees W mo is a low-rank matrix. With such a low-rank strategy, the capacity of motion embedding is suppressed. The repeating issue is alleviated in Fig. <ref type="figure" target="#fig_6">A6b</ref>.</p><p>To summarize, Config-D is the best choice for generating short video (less than 10 seconds), while Config-F is better for long video generation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">EXPERIMENTS</head><p>We evaluate our method on several datasets and compare it against prior arts.</p><p>Evaluation Metrics. Akin to the literature, Frechet Inception Distance (FID) <ref type="bibr" target="#b14">(Heusel et al., 2017)</ref> and Frechet Video Distance (FVD) <ref type="bibr" target="#b47">(Unterthiner et al., 2018)</ref> serve as the quantitative metrics to  Each dot refers to a motion code v t , which represents a single frame's motion. A set of progressively darkening dots construct one trajectory that represents a synthesized frame sequence. We can tell that there usually exist undesired twists in the motion trajectory of StyleGAN-V <ref type="bibr" target="#b44">(Skorokhodov et al., 2022)</ref>, indicating the discontinuity of motion embedding. On the contrary, our approach helps alleviate this issue, resulting in smoother transition throughout the entire video.</p><p>Table <ref type="table">2</ref>: Quantitative comparison between our approach and existing video generation methods on SkyTimelapse <ref type="bibr" target="#b58">(Xiong et al., 2018)</ref>. For all metrics, the smaller number the better. evaluate the synthesis for image and video respectively. In terms of FVD, two temporal spans (consecutive 16 and 128 frames) are chosen to measure the synthesized videos.</p><p>Benchmarks. In Sec. 2, we have already conducted comprehensive ablation studies on SkyTimelapse <ref type="bibr" target="#b58">(Xiong et al., 2018)</ref>, Taichi-HD <ref type="bibr" target="#b43">(Siarohin et al., 2019b)</ref>, and YouTube Driving dataset <ref type="bibr" target="#b64">(Zhang et al., 2022)</ref>. To be specific, SkyTimelapse contains over two thousand videos which in average last twenty seconds. Various types of scenes (daytime, nightfall, and aurora) are included. Similar to StyleGAN-V <ref type="bibr" target="#b44">(Skorokhodov et al., 2022)</ref>, we resize the videos of SkyTimelapse to 256 ? 256 resolution. Taichi-HD contains over three thousand videos recording a person performing Tai-chi.</p><p>We also resize the videos to 256 ? 256 resolution. YouTube Driving dataset <ref type="bibr" target="#b64">(Zhang et al., 2022)</ref> consists of 134 first-view driving videos with a total length of over 120 hours, covering up to 68 cities, showing various conditions, from different weather, different regions, to diverse scene types. The driving scenes have tight geometrical constraints, with most of the objects (vehicles, buildings) following the rigid body constraint. We resize the videos to 180 ? 320 resolution. A part of videos with countryside views in 320 ? 640 resolution is also chosen to benchmark high resolution video generation. For training, we resize them to 256 ? 256 and 512 ? 512 resolution respectively.</p><p>Additionally, we compare our approach against previous methods including MoCoGAN <ref type="bibr" target="#b46">(Tulyakov et al., 2018)</ref> and its StyleGAN2 based variant, MoCoGAN-HD <ref type="bibr" target="#b45">(Tian et al., 2021)</ref>, VideoGPT <ref type="bibr" target="#b59">(Yan et al., 2021)</ref>, DIGAN <ref type="bibr" target="#b63">(Yu et al., 2022)</ref>, TATS <ref type="bibr" target="#b10">(Ge et al., 2022)</ref>, LongVideoGAN <ref type="bibr" target="#b4">(Brooks et al., 2022)</ref>, and StyleGAN-V <ref type="bibr" target="#b44">(Skorokhodov et al., 2022)</ref>.</p><p>Training. We follow the training scheme of StyleGAN-V and train models on a server with 8 A100 GPUs. In terms of various methods and datasets, we grid search the R 1 regularization weight, whose details are available in Appendix. In particular, performances of MoCoGAN and its StyleGAN2 based variant, MoCoGAN-HD, VideoGPT, and DIGAN are directly borrowed from StyleGAN-V paper.</p><p>Table <ref type="table">3</ref>: Quantitative comparison between our approach and existing video generation methods on Taichi-HD <ref type="bibr" target="#b43">(Siarohin et al., 2019b)</ref>. For all metrics, a smaller number is better. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">MAIN RESULTS</head><p>As shown in Tab. 2, our method significantly outperforms existing baselines on SkyTimelapse in terms of FVD 16 and FVD 128 . On Taichi-HD, our method achieves consistent improvement in terms of FID, FVD 16 , and FVD 128 as reported in Tab. 3. We also compare our method with StyleGAN-V in challenging YouTube Driving dataset. at both 256 ? 256 and 512 ? 512 resolution. For 256 ? 256 resolution in Tab. 4, our method achieves better performance in both FID, FVD 16 , and FVD 128 . For 512 ? 512 resolution, StyleGAN-V has a comparable FID score but much worse FVD score. For both two resolutions on YouTube driving, we yield a strong performance of less than 50% FVD score than the prior art. As shown in Fig. <ref type="figure">1</ref>, our method achieves an appealing visual effect, with smooth frame transition.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">STRENGTHS OF THE IMPROVED MODEL</head><p>Good translation equivariance. We adopt alias-free technique from image generation for a smooth frame transition. This design also allows for geometric manipulation of the video using a human-specified translation matrix. We showcase several examples in Fig. <ref type="figure" target="#fig_6">6</ref> by feeding different translation matrix to the video generator. It is clear that videos can be successfully translated, and translation equivariance is well maintained over time. This equivariance property improves human controllability over the video generation process.</p><p>Table <ref type="table">5</ref>: FVD evaluated on the frames around anchors. temporal smoothness over the anchor features. To quantify the smoothness of generated videos, we sample 16 frames across the anchor (eight frames before and behind the anchor) and calculate the FID 16 metric (which we name "FID 16 -anchor") in YouTube Driving dataset. As reported in Tab. 5, Config-D suffers from severe performance drop when approaching anchors. While Config-F with B-Spline based motion representation maintains the performance, indicating that our method can generate long videos with smooth transitions without performance degrading.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">RELATED WORK</head><p>Unconditional Video generation. Most of the video generation frameworks are built upon GANs <ref type="bibr" target="#b12">(Goodfellow et al., 2014;</ref><ref type="bibr" target="#b3">Brock et al., 2019;</ref><ref type="bibr" target="#b20">Karras et al., 2019;</ref><ref type="bibr">2020b;</ref><ref type="bibr">2021;</ref><ref type="bibr">2018;</ref><ref type="bibr" target="#b35">Radford et al., 2016)</ref>, owing to their flexibility as well as strong performance in image generation. Prior arts <ref type="bibr" target="#b45">(Tian et al., 2021;</ref><ref type="bibr" target="#b39">Saito et al., 2017;</ref><ref type="bibr" target="#b46">Tulyakov et al., 2018;</ref><ref type="bibr" target="#b9">Fox et al., 2021;</ref><ref type="bibr" target="#b44">Skorokhodov et al., 2022;</ref><ref type="bibr" target="#b63">Yu et al., 2022;</ref><ref type="bibr" target="#b50">Vondrick et al., 2016;</ref><ref type="bibr" target="#b40">Saito et al., 2020;</ref><ref type="bibr" target="#b6">Clark et al., 2019;</ref><ref type="bibr">Wang et al., 2022b)</ref> adopt image generators to synthesize frames from a time-coherent sequence of latent codes generated by recurrent networks. To explicitly disentangle the motion from the content, MoCoGAN <ref type="bibr" target="#b46">(Tulyakov et al., 2018)</ref> and TGAN <ref type="bibr" target="#b39">(Saito et al., 2017)</ref> employ a motion code and a content code as input noise for the generator, serving as a common strategy for the later works. StyleGAN-V <ref type="bibr" target="#b44">(Skorokhodov et al., 2022)</ref>, which we use as a strong baseline, and DIGAN <ref type="bibr" target="#b63">(Yu et al., 2022)</ref> both use implicit neural-based representations for continuous video synthesis. <ref type="bibr" target="#b4">Brooks et al. (2022)</ref> leverages a multiresolution training strategy to prioritize the time axis and ensure long-term consistency. Another line of works <ref type="bibr" target="#b59">(Yan et al., 2021;</ref><ref type="bibr" target="#b18">Kalchbrenner et al., 2017;</ref><ref type="bibr" target="#b55">Weissenborn et al., 2019;</ref><ref type="bibr" target="#b36">Rakhimov et al., 2020;</ref><ref type="bibr" target="#b51">Walker et al., 2021)</ref> use autoregressive models to achieve video generation. <ref type="bibr" target="#b18">Kalchbrenner et al. (2017)</ref> use a PixelCNN <ref type="bibr" target="#b48">(Van den Oord et al., 2016)</ref> decoder to synthesize the next frame pixel by pixel in an autoregressive manner. VideoGPT <ref type="bibr" target="#b59">(Yan et al., 2021)</ref> built upon VQ-VAE adopts an autoregressive model to decompose videos into a sequence of tokens. TATS-base <ref type="bibr" target="#b10">(Ge et al., 2022)</ref> similarly leverages 3D-VQGAN to decompose videos into tokens and further uses transformers to model the relationship between frame tokens. Additionally, some works using diffusion-based models also present promising results on video generation. Video diffusion <ref type="bibr" target="#b15">(Ho et al., 2022</ref>) models entire videos using a 3D video architecture while <ref type="bibr" target="#b62">Yang et al. (2022)</ref> uses an image diffusion model to synthesize frames with a recurrent network.</p><p>Video generation with guided conditions. A close line of research is video prediction, aiming to generate full videos from the observation of previous frames <ref type="bibr" target="#b1">(Babaeizadeh et al., 2017;</ref><ref type="bibr" target="#b26">Kumar et al., 2019;</ref><ref type="bibr" target="#b27">Lee et al., 2018;</ref><ref type="bibr" target="#b31">Luc et al., 2020;</ref><ref type="bibr" target="#b32">Nash et al., 2022;</ref><ref type="bibr" target="#b8">Finn et al., 2016)</ref> or a given conditions <ref type="bibr" target="#b24">(Kim et al., 2020;</ref><ref type="bibr">2021;</ref><ref type="bibr" target="#b13">Ha &amp; Schmidhuber, 2018)</ref>. These works typically employ the reconstruction losses to make the future frames predictable for the model. Video translation <ref type="bibr" target="#b33">(Pan et al., 2019;</ref><ref type="bibr" target="#b60">Yang et al., 2018;</ref><ref type="bibr">Wang et al., 2018;</ref><ref type="bibr" target="#b5">Chan et al., 2019;</ref><ref type="bibr">Siarohin et al., 2019a;</ref><ref type="bibr" target="#b37">Ren et al., 2020;</ref><ref type="bibr" target="#b43">Siarohin et al., 2019b)</ref> is also a paradigm for video synthesis, which translates the given segmentation masks and keypoints into videos. Some <ref type="bibr" target="#b28">(Li et al., 2022;</ref><ref type="bibr" target="#b30">Liu et al., 2021;</ref><ref type="bibr" target="#b38">Ren &amp; Wang, 2022)</ref>  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">CONCLUSION AND DISCUSSION</head><p>In this work, we set up a simple yet effective video generation baseline. By introducing the alias-free operations together with pre-learned knowledge, per-frame quality is improved. Explicit temporal modeling in discriminator enhances the dynamic synthesis. In addition, a new B-Spline based motion representation is proposed to enable smooth infinite frame generation. Experimental results show substantial improvements over the previous methods on different datasets.</p><p>Discussion. Our approach has several limitations. The current framework follows prior arts using two latent codes to represent content and motion respectively. However, in our experiments, it is suggested that they usually entangle with each other, especially for the driving scenes where it is difficult to separate the content and motion. Moreover, the frame quality is still far from ideal. The object shape is not consistent across consecutive frames, producing apparent artifacts. This could be further improved by introducing structural priors. Last, although we could generate infinite APPENDIX This appendix is organized as follows. appendix A and appendix B present the implementation and dataset details respectively, followed by appendix C and appendix D showing user study result and more visual results.</p><p>A IMPLEMENTATION DETAILS.</p><p>Our method is developed based on the official implementation of StyleGAN-V <ref type="bibr" target="#b44">(Skorokhodov et al., 2022)</ref>. We adopt hyper-parameters, the optimizer, the loss function, and the training script to ensure a fair comparison. Notably, the total number of seen images for sufficient data training is 25 million regardless of datasets. We evaluate once after training on every 2.5 million images, and report the result with highest FVD 16 score.</p><p>Due to statistics variance of different datasets, we search training regularization term, i.e. R 1 value, for each method and dataset. Empirically, we find that a smaller R 1 value (e.g., 0.25) works well for pretraining stage (Config-C). While a larger R 1 value (e.g., 4) better suits to video generation learning.</p><p>B DATASET DETAILS.</p><p>In this section, we introduce the datasets we use, briefly analyze their characteristics and challenges brought for video generation, and show sampled real video frames.</p><p>SkyTimelapse. SkyTimelapse <ref type="bibr" target="#b58">(Xiong et al., 2018)</ref>  Countryside. We select a part of videos with countryside scenes from YouTube Driving dataset and resize them to 512 ? 512 resolution for training. We use this dataset to benchmark the performance of generating high-resolution videos. Real samples are presented in Fig. <ref type="figure" target="#fig_11">A3</ref>.</p><p>Taichi-HD. Taichi-HD <ref type="bibr" target="#b43">(Siarohin et al., 2019b)</ref>  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C USER STUDY</head><p>To further prove our method's efficacy, we conduct user study over baseline (Config-A), Config-D, and Config-F on YouTube Driving dataset and Taichi-HD dataset. We evaluate from three perspectives:</p><p>Single Image Quality. We provide user with a collection of images randomly sampled from generated videos.</p><p>Short Video Quality. We provide user with a collection of generated videos with consecutive 128 frames (up to about 5 seconds).</p><p>Long Video Quality. We provide user with a collection of generated videos with consecutive 500 frames (up to 20 seconds). Texture Sticking phenomenon. Texture sticking exists in videos generated by prior methods. Our method leverages the alias-free technique in the generator to overcome this issue. Fig. <ref type="figure" target="#fig_3">A4</ref> shows more comparison between our method and StyleGAN-V <ref type="bibr" target="#b44">(Skorokhodov et al., 2022)</ref>.</p><p>Jittering phenomenon. Motion embedding designed in previous method is not first-order continuous, leading to jittering phenomenon when generating long videos. We develop B-Spline based motion embedding which guarantees smoothness and alleviates the problem. We show examples comparing video sequences generated by our method with StyleGAN-V <ref type="bibr" target="#b44">(Skorokhodov et al., 2022)</ref> in Fig. <ref type="figure" target="#fig_5">A5</ref>. Our videos are smooth, without abnormal movements. In videos generated by StyleGAN-V <ref type="bibr" target="#b44">(Skorokhodov et al., 2022)</ref>, street lights and zebra crossings may suddenly change their moving directions.</p><p>Repeating contents. With B-Spline based motion embedding, Config-E can compose smooth long videos. However, similar contents appear periodically as shown in Fig. <ref type="figure" target="#fig_6">A6a</ref>. B-Spline based motion embedding is endowed with so strong capacity that it represents content and motion concepts simultaneously, rather than motion concepts only. We suppress the representation capacity through the low-rank strategy. As shown in Fig. <ref type="figure" target="#fig_6">A6b</ref>, Config-F with low rank time modulation can generate progressively forward contents without repeating contents.   </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>StyleGAN-V</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Ours time expansion time expansion</head><p>Figure <ref type="figure" target="#fig_3">A4</ref>: More examples on texture sticking phenomenon. We concatenate slices in each frame of the generated video (marked by red and green). We observe brush effect when texture sticks at specific location. Instead, our approach achieves more smooth frame transition between neighbor frames.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 2 :</head><label>2</label><figDesc>Figure2: Visualization of the texture sticking effect. On the top are a few samples from a continuous sequence of frames, while the bottom visualizes the varying content at a fixed position. We can observe the brush effect from the baseline<ref type="bibr" target="#b44">(Skorokhodov et al., 2022)</ref>, where the pixel values are strongly anchored to the coordinates. Instead, our approach achieves smoother frame transition between neighbor frames.</figDesc><graphic url="image-29.png" coords="4,310.02,136.06,193.80,58.85" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 3 :</head><label>3</label><figDesc>Figure3: Visualization of the content jittering effect. As the video continues, we can observe the unstable crossroad in the baseline model<ref type="bibr" target="#b44">(Skorokhodov et al., 2022)</ref>, caused by discontinuous motion representation. Instead, our approach alleviates it through the proposed B-Spline motion representation.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 4 :</head><label>4</label><figDesc>Figure 4: Comparison between linear interpolation and our B-Spline controlled interpolation.B-spline is able to smooth the interpolation between various anchor features, and hence improve the first-order numerical continuity.</figDesc><graphic url="image-48.png" coords="6,321.06,106.09,112.30,80.31" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 5 :</head><label>5</label><figDesc>Figure 5: Visualization of motion embeddings using T-SNE (Van der Maaten &amp; Hinton, 2008).Each dot refers to a motion code v t , which represents a single frame's motion. A set of progressively darkening dots construct one trajectory that represents a synthesized frame sequence. We can tell that there usually exist undesired twists in the motion trajectory of StyleGAN-V<ref type="bibr" target="#b44">(Skorokhodov et al., 2022)</ref>, indicating the discontinuity of motion embedding. On the contrary, our approach helps alleviate this issue, resulting in smoother transition throughout the entire video.</figDesc><graphic url="image-50.png" coords="7,401.15,149.13,58.02,56.63" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 6 :</head><label>6</label><figDesc>Figure 6: Application of video 2D translation, where we are able to control the shift of the frame content, such as moving the setting sun left and moving the grassland down.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head></head><label></label><figDesc>focus on synthesizing videos of indoor and outdoor scenes for given camera trajectories, which need to model the 3D scene explicitly. Besides, Ho et al. (2022); Hong et al. (2022); Wu et al. (2021a;b) explore text-to-video generation with diffusion-based models and transformers.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head></head><label></label><figDesc>As shown in Tab. A1, in terms of single image quality, users prefer Config-D over other settings in both YouTube Driving and Taichi-HD dataset. The same trend occurs in short video quality assessment. With pre-learned knowledge and a stronger discriminator with temporal shift module (TSM), Config-D reaps a significantly higher number of votes. However, for long video generation, Config-F becomes the best choice for both YouTube Driving and Taichi-HD dataset. Unnatural jittering and discontinuous motion can drastically affect the users' video viewing experience. With B-Spline based motion embedding and low rank time modulation, Config-F is undoubtedly the best option for long video generation. D MORE VISUAL RESULTS.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>Figure A1 :</head><label>A1</label><figDesc>Figure A1: Sampled video frames from SkyTimelapse (Xiong et al., 2018).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head>Figure A2 :</head><label>A2</label><figDesc>Figure A2: Sampled video frames from YouTube Driving dataset (Zhang et al., 2022).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_11"><head>Figure A3 :</head><label>A3</label><figDesc>Figure A3: Sampled video frames from Countryside dataset (Zhang et al., 2022).</figDesc><graphic url="image-139.png" coords="17,108.00,454.32,96.61,54.31" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 :</head><label>1</label><figDesc>Evaluation of various configurations. Config-A is the baseline model StyleGAN-V<ref type="bibr" target="#b44">(Skorokhodov et al., 2022)</ref>. Config-B and Config-C target fixing the texture sticking issue yet maintain the per-frame quality (Sec. 2.2), which is primarily evaluated by FID. Config-D aims to help the generator to produce more reasonable dynamics (Sec. 2.3), which is primarily evaluated by FVD 16 and FVD 128 . Config-E and Config-F alleviate the discontinuity when interpolating motion embeddings (Sec. 2.4). For all metrics, a lower number is better.</figDesc><table><row><cell>Configuration</cell><cell cols="2">SkyTimelapse</cell><cell cols="2">YouTube Driving</cell><cell>Taichi-HD</cell><cell></cell></row><row><cell></cell><cell cols="6">FID FVD16 FVD128 FID FVD16 FVD128 FID FVD16 FVD128</cell></row><row><cell>A StyleGAN-V</cell><cell>40.8 73.9</cell><cell>248.3</cell><cell>28.3 449.8</cell><cell>460.6</cell><cell>33.8 152.0</cell><cell>267.3</cell></row><row><cell>B + Alias free</cell><cell>54.0 118.8</cell><cell>221.4</cell><cell>56.4 729.8</cell><cell>886.0</cell><cell>31.4 171.7</cell><cell>522.9</cell></row><row><cell cols="2">C + Image pretrain 52.2 73.5</cell><cell>230.3</cell><cell>15.6 272.8</cell><cell>447.5</cell><cell>20.8 104.3</cell><cell>314.2</cell></row><row><cell>D + TSM</cell><cell>49.9 49.0</cell><cell>135.9</cell><cell>19.2 207.2</cell><cell>221.5</cell><cell>26.0 84.6</cell><cell>176.2</cell></row><row><cell>E + B-Spline</cell><cell>63.5 64.8</cell><cell>185.3</cell><cell>21.8 281.6</cell><cell>375.2</cell><cell>25.5 82.6</cell><cell>169.7</cell></row><row><cell>F + Low-rank</cell><cell>60.4 61.9</cell><cell>229.1</cell><cell>23.0 260.4</cell><cell>278.9</cell><cell>24.8 98.3</cell><cell>186.8</cell></row><row><cell cols="7">a wave function is derived with learned frequency ?, phase ? and amplitude ? from the left anchor</cell></row><row><cell>feature a l</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 4 :</head><label>4</label><figDesc>Quantitative comparison between our approach and StyleGAN-V<ref type="bibr" target="#b44">(Skorokhodov et al., 2022)</ref> on the self-collected YouTube Driving dataset. For all metrics, a smaller number is better.</figDesc><table><row><cell>Method</cell><cell></cell><cell>FID</cell><cell>FVD16</cell><cell>FVD128</cell></row><row><cell cols="2">MoCoGAN-HD (Tian et al., 2021)</cell><cell>-</cell><cell>144.7</cell><cell>-</cell></row><row><cell cols="2">DIGAN (Yu et al., 2022)</cell><cell>-</cell><cell>128.1</cell><cell>-</cell></row><row><cell cols="2">TATS-base (Ge et al., 2022)</cell><cell>-</cell><cell>94.6</cell><cell>-</cell></row><row><cell cols="2">StyleGAN-V (Skorokhodov et al., 2022)</cell><cell>33.8</cell><cell>152.0</cell><cell>267.3</cell></row><row><cell>Ours</cell><cell></cell><cell>26.0</cell><cell>84.6</cell><cell>176.2</cell></row><row><cell>Resolution</cell><cell>Method</cell><cell>FID</cell><cell>FVD16</cell><cell>FVD128</cell></row><row><cell>256</cell><cell>StyleGAN-V Ours</cell><cell>28.3 19.2</cell><cell>449.8 207.2</cell><cell>460.6 221.5</cell></row><row><cell>512</cell><cell>StyleGAN-V Ours</cell><cell>14.6 14.5</cell><cell>262.4 116.0</cell><cell>285.4 139.2</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head></head><label></label><figDesc>is a time-lapse dataset collected from the Internet showing dynamic sky scenes, such as the cloudy sky with moving clouds, and the starry sky with moving stars. It contains various conditions, for instance, different weather conditions (daytime, nightfall, dawn, starry night and aurora), different kinds of foreground object (cloudy sky, starry sky, and sunny sky), and different motion patterns of the sky. It also contains some other objects like trees, mountains, and buildings which further improve the visual diversity. However, since clouds are fluids, there are very few constraints on movement. Minor disturbances and deformations can make a generated video look realistic.The whole dataset contains over two thousand videos which in average last twenty seconds. We resize the videos to 256 ? 256 following prior works. We sample several video clips in Fig.A1.YouTube Driving. YouTube Driving Dataset<ref type="bibr" target="#b64">(Zhang et al., 2022)</ref> is crawled from YouTube which contains a massive amount of real-world driving frames with various conditions, from different weather, different regions, to diverse scene types. To be specific, 134 videos with a total length of over 120 hours are collected, covering up to 68 cities. Fig.A2shows sampled frames. The videos are resized to 256 ? 256 resolution for training.The driving scenes have tighter geometrical constraints, with most of the objects (vehicles, buildings) following the rigid body constraint. The 3D structure of the whole scene should be maintained without any deformation. Besides, different types of objects show different motion patterns. Roads and buildings recede at a reasonable speed opposite to that of ego car, while other cars show lane changes, stops, accelerations.</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table A1 :</head><label>A1</label><figDesc>User study result. We report the percentage of favorite users for the quality of a set of single images, a set of short videos, and a set of long videos under models from three different settings.</figDesc><table><row><cell></cell><cell></cell><cell>YouTube Driving</cell><cell></cell><cell></cell><cell>Taichi-HD</cell><cell></cell></row><row><cell></cell><cell cols="6">image(%) short video(%) long video(%) image(%) short video(%) long video(%)</cell></row><row><cell>Config-A</cell><cell>4</cell><cell>6</cell><cell>8</cell><cell>0</cell><cell>2</cell><cell>4</cell></row><row><cell>Config-D</cell><cell>62</cell><cell>60</cell><cell>12</cell><cell>52</cell><cell>66</cell><cell>36</cell></row><row><cell>Config-F</cell><cell>34</cell><cell>34</cell><cell>80</cell><cell>48</cell><cell>32</cell><cell>60</cell></row></table><note><p>is a collection of YouTube videos recording a single person performing Tai-chi. It contains 3049 training videos in total, with various foreground objects (people with different styles of clothing) and diverse background. It requires realistic human motion generation, which contains non-rigid motion modeling, which poses a significant challenge for video generation.</p></note></figure>
		</body>
		<back>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>videos by gradually changing content code, direct control of novel content over time is lacking. <ref type="bibr" target="#b10">Ge et al. (2022)</ref> uses explicit planning in the latent space to achieve the controllability of the synthesis, especially for infinite video generation. Similar ideas could be adopted in our framework to improve the quality of long videos.  <ref type="bibr" target="#b44">(Skorokhodov et al., 2022)</ref>. Obviously, as highlighted in red bounding boxes, the street lights and crossroad move unsteadily (e.g., gradually fading out first and suddenly fading in). On the contrary, our approach enables much more reasonable and continuous dynamics of certain objects (see green bounding boxes). </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>StyleGAN-V Ours</head></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<author>
			<persName><forename type="first">Yuval</forename><surname>Alaluf</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Or</forename><surname>Patashnik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zongze</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Asif</forename><surname>Zamir</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Eli</forename><surname>Shechtman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dani</forename><surname>Lischinski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Daniel</forename><surname>Cohen-Or</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2201.13433</idno>
		<title level="m">Third time&apos;s the charm? image and video editing with stylegan3</title>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<author>
			<persName><forename type="first">Mohammad</forename><surname>Babaeizadeh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chelsea</forename><surname>Finn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dumitru</forename><surname>Erhan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Roy</forename><forename type="middle">H</forename><surname>Campbell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sergey</forename><surname>Levine</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1710.11252</idno>
		<title level="m">Stochastic variational video prediction</title>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Rewriting a deep generative model</title>
		<author>
			<persName><forename type="first">David</forename><surname>Bau</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Steven</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tongzhou</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jun-Yan</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Antonio</forename><surname>Torralba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European conference on computer vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="351" to="369" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Large scale GAN training for high fidelity natural image synthesis</title>
		<author>
			<persName><forename type="first">Andrew</forename><surname>Brock</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jeff</forename><surname>Donahue</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Karen</forename><surname>Simonyan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Int. Conf. Learn. Represent</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<author>
			<persName><forename type="first">Tim</forename><surname>Brooks</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Janne</forename><surname>Hellsten</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Miika</forename><surname>Aittala</surname></persName>
		</author>
		<author>
			<persName><surname>Ting-Chun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Timo</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jaakko</forename><surname>Aila</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ming-Yu</forename><surname>Lehtinen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alexei</forename><forename type="middle">A</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tero</forename><surname>Efros</surname></persName>
		</author>
		<author>
			<persName><surname>Karras</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2206.03429</idno>
		<title level="m">Generating long videos of dynamic scenes</title>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Everybody dance now</title>
		<author>
			<persName><forename type="first">Caroline</forename><surname>Chan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shiry</forename><surname>Ginosar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tinghui</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alexei</forename><forename type="middle">A</forename><surname>Efros</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Int. Conf. Comput. Vis</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">Adversarial video generation on complex datasets</title>
		<author>
			<persName><forename type="first">Aidan</forename><surname>Clark</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jeff</forename><surname>Donahue</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Karen</forename><surname>Simonyan</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1907.06571</idno>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">Subroutine package for calculating with b-splines</title>
		<imprint>
			<date type="published" when="1971">1971</date>
			<publisher>Carl de Boor</publisher>
			<pubPlace>Los Alamos, NM (United States</pubPlace>
		</imprint>
		<respStmt>
			<orgName>Los Alamos National Lab.(LANL)</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">Technical report</note>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Unsupervised learning for physical interaction through video prediction</title>
		<author>
			<persName><forename type="first">Chelsea</forename><surname>Finn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ian</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sergey</forename><surname>Levine</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Adv. Neural Inform. Process. Syst</title>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<author>
			<persName><forename type="first">Gereon</forename><surname>Fox</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ayush</forename><surname>Tewari</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mohamed</forename><surname>Elgharib</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christian</forename><surname>Theobalt</surname></persName>
		</author>
		<author>
			<persName><surname>Stylevideogan</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2107.07224</idno>
		<title level="m">A temporal generative model using a pretrained stylegan</title>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<author>
			<persName><forename type="first">Songwei</forename><surname>Ge</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Thomas</forename><surname>Hayes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Harry</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xi</forename><surname>Yin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Guan</forename><surname>Pang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><surname>Jacobs</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jia-Bin</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Devi</forename><surname>Parikh</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2204.03638</idno>
		<title level="m">Long video generation with time-agnostic vqgan and time-sensitive transformer</title>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Ganalyze: Toward visual definitions of cognitive image properties</title>
		<author>
			<persName><forename type="first">Lore</forename><surname>Goetschalckx</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alex</forename><surname>Andonian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aude</forename><surname>Oliva</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Phillip</forename><surname>Isola</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Int. Conf. Comput. Vis</title>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="5744" to="5753" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Generative adversarial nets</title>
		<author>
			<persName><forename type="first">Ian</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jean</forename><surname>Pouget-Abadie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mehdi</forename><surname>Mirza</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bing</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><surname>Warde-Farley</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sherjil</forename><surname>Ozair</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aaron</forename><surname>Courville</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Adv. Neural Inform. Process. Syst</title>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Recurrent world models facilitate policy evolution</title>
		<author>
			<persName><forename type="first">David</forename><surname>Ha</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J?rgen</forename><surname>Schmidhuber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Adv. Neural Inform. Process. Syst</title>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Gans trained by a two time-scale update rule converge to a local nash equilibrium</title>
		<author>
			<persName><forename type="first">Martin</forename><surname>Heusel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hubert</forename><surname>Ramsauer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Thomas</forename><surname>Unterthiner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bernhard</forename><surname>Nessler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sepp</forename><surname>Hochreiter</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="volume">30</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<author>
			<persName><forename type="first">Jonathan</forename><surname>Ho</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tim</forename><surname>Salimans</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alexey</forename><surname>Gritsenko</surname></persName>
		</author>
		<author>
			<persName><forename type="first">William</forename><surname>Chan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mohammad</forename><surname>Norouzi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><forename type="middle">J</forename><surname>Fleet</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2204.03458</idno>
		<title level="m">Video diffusion models</title>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">Cogvideo: Large-scale pretraining for text-to-video generation via transformers</title>
		<author>
			<persName><forename type="first">Wenyi</forename><surname>Hong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ming</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wendi</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xinghan</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jie</forename><surname>Tang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2205.15868</idno>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">On the&quot; steerability&quot; of generative adversarial networks</title>
		<author>
			<persName><forename type="first">Ali</forename><surname>Jahanian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lucy</forename><surname>Chai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Phillip</forename><surname>Isola</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Int. Conf. Learn. Represent</title>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Ivo Danihelka, Oriol Vinyals, Alex Graves, and Koray Kavukcuoglu. Video pixel networks</title>
		<author>
			<persName><forename type="first">Nal</forename><surname>Kalchbrenner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A?ron</forename><surname>Oord</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Karen</forename><surname>Simonyan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Int. Conf. Mach. Learn</title>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Progressive growing of gans for improved quality, stability, and variation</title>
		<author>
			<persName><forename type="first">Tero</forename><surname>Karras</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Timo</forename><surname>Aila</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Samuli</forename><surname>Laine</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jaakko</forename><surname>Lehtinen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Int. Conf. Learn. Represent</title>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">A style-based generator architecture for generative adversarial networks</title>
		<author>
			<persName><forename type="first">Tero</forename><surname>Karras</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Samuli</forename><surname>Laine</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Timo</forename><surname>Aila</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conf. Comput. Vis. Pattern Recog</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Analyzing and improving the image quality of stylegan</title>
		<author>
			<persName><forename type="first">Tero</forename><surname>Karras</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Samuli</forename><surname>Laine</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Miika</forename><surname>Aittala</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Janne</forename><surname>Hellsten</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jaakko</forename><surname>Lehtinen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Timo</forename><surname>Aila</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF conference on computer vision and pattern recognition</title>
		<meeting>the IEEE/CVF conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="8110" to="8119" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Analyzing and improving the image quality of stylegan</title>
		<author>
			<persName><forename type="first">Tero</forename><surname>Karras</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Samuli</forename><surname>Laine</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Miika</forename><surname>Aittala</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Janne</forename><surname>Hellsten</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jaakko</forename><surname>Lehtinen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Timo</forename><surname>Aila</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conf. Comput. Vis. Pattern Recog</title>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="8110" to="8119" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Alias-free generative adversarial networks</title>
		<author>
			<persName><forename type="first">Tero</forename><surname>Karras</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Miika</forename><surname>Aittala</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Samuli</forename><surname>Laine</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Erik</forename><surname>H?rk?nen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Janne</forename><surname>Hellsten</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jaakko</forename><surname>Lehtinen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Timo</forename><surname>Aila</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">NIPS</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="page" from="852" to="863" />
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Learning to Simulate Dynamic Environments with GameGAN</title>
		<author>
			<persName><forename type="first">Seung Wook</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuhao</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jonah</forename><surname>Philion</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Antonio</forename><surname>Torralba</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sanja</forename><surname>Fidler</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conf. Comput. Vis. Pattern Recog</title>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">DriveGAN: Towards a Controllable High-Quality Neural Simulation</title>
		<author>
			<persName><forename type="first">Seung Wook</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jonah</forename><surname>Philion</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Antonio</forename><surname>Torralba</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sanja</forename><surname>Fidler</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conf. Comput. Vis. Pattern Recog</title>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<title level="m" type="main">Videoflow: A conditional flow-based model for stochastic video generation</title>
		<author>
			<persName><forename type="first">Manoj</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mohammad</forename><surname>Babaeizadeh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dumitru</forename><surname>Erhan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chelsea</forename><surname>Finn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sergey</forename><surname>Levine</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Laurent</forename><surname>Dinh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Durk</forename><surname>Kingma</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1903.01434</idno>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<author>
			<persName><forename type="first">Alex X</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Richard</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Frederik</forename><surname>Ebert</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pieter</forename><surname>Abbeel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chelsea</forename><surname>Finn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sergey</forename><surname>Levine</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1804.01523</idno>
		<title level="m">Stochastic adversarial video prediction</title>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<title level="m" type="main">Infinitenature-zero: Learning perpetual view generation of natural scenes from single images</title>
		<author>
			<persName><forename type="first">Zhengqi</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qianqian</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Noah</forename><surname>Snavely</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Angjoo</forename><surname>Kanazawa</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2207.11148</idno>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Tsm: Temporal shift module for efficient video understanding</title>
		<author>
			<persName><forename type="first">Ji</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chuang</forename><surname>Gan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Song</forename><surname>Han</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Int. Conf. Comput. Vis</title>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="7083" to="7093" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Infinite nature: Perpetual view generation of natural scenes from a single image</title>
		<author>
			<persName><forename type="first">Andrew</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Richard</forename><surname>Tucker</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Varun</forename><surname>Jampani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ameesh</forename><surname>Makadia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Noah</forename><surname>Snavely</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Angjoo</forename><surname>Kanazawa</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conf. Comput. Vis. Pattern Recog</title>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
		<author>
			<persName><forename type="first">Pauline</forename><surname>Luc</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aidan</forename><surname>Clark</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sander</forename><surname>Dieleman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Diego</forename><surname>De Las</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yotam</forename><surname>Casas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Albin</forename><surname>Doron</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Karen</forename><surname>Cassirer</surname></persName>
		</author>
		<author>
			<persName><surname>Simonyan</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2003.04035</idno>
		<title level="m">Transformation-based adversarial video prediction on large-scale data</title>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
		<title level="m" type="main">Transframer: Arbitrary frame prediction with generative models</title>
		<author>
			<persName><forename type="first">Charlie</forename><surname>Nash</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jo?o</forename><surname>Carreira</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jacob</forename><surname>Walker</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Iain</forename><surname>Barr</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrew</forename><surname>Jaegle</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mateusz</forename><surname>Malinowski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Peter</forename><surname>Battaglia</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2203.09494</idno>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Video generation from single semantic label map</title>
		<author>
			<persName><forename type="first">Junting</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chengyu</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xu</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jing</forename><surname>Shao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lu</forename><surname>Sheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Junjie</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaogang</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conf. Comput. Vis. Pattern Recog</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<monogr>
		<title level="m" type="main">Stylefacev: Face video generation via decomposing and recomposing pretrained stylegan3</title>
		<author>
			<persName><forename type="first">Haonan</forename><surname>Qiu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuming</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hang</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wayne</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ziwei</forename><surname>Liu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2208.07862</idno>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Unsupervised representation learning with deep convolutional generative adversarial networks</title>
		<author>
			<persName><forename type="first">Alec</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Luke</forename><surname>Metz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Soumith</forename><surname>Chintala</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Int. Conf. Learn. Represent</title>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<monogr>
		<author>
			<persName><forename type="first">Ruslan</forename><surname>Rakhimov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Denis</forename><surname>Volkhonskiy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alexey</forename><surname>Artemov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Denis</forename><surname>Zorin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Evgeny</forename><surname>Burnaev</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2006.10704</idno>
		<title level="m">Latent video transformer</title>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Human motion transfer from poses in the wild</title>
		<author>
			<persName><forename type="first">Jian</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Menglei</forename><surname>Chai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sergey</forename><surname>Tulyakov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chen</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaohui</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jianchao</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Eur. Conf. Comput. Vis</title>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Look outside the room: Synthesizing a consistent long-term 3d scene video from a single image</title>
		<author>
			<persName><forename type="first">Xuanchi</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaolong</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conf. Comput. Vis. Pattern Recog</title>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Temporal generative adversarial nets with singular value clipping</title>
		<author>
			<persName><forename type="first">Masaki</forename><surname>Saito</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Eiichi</forename><surname>Matsumoto</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shunta</forename><surname>Saito</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Int. Conf. Comput. Vis</title>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Train sparsely, generate densely: Memory-efficient unsupervised training of high-resolution temporal gan</title>
		<author>
			<persName><forename type="first">Masaki</forename><surname>Saito</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shunta</forename><surname>Saito</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Masanori</forename><surname>Koyama</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sosuke</forename><surname>Kobayashi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Int. J. Comput. Vis</title>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Interpreting the disentangled face representation learned by gans</title>
		<author>
			<persName><forename type="first">Yujun</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ceyuan</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaoou</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bolei</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><surname>Interfacegan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Pattern Anal. Mach. Intell</title>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Animating arbitrary objects via deep motion transfer</title>
		<author>
			<persName><forename type="first">Aliaksandr</forename><surname>Siarohin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">St?phane</forename><surname>Lathuili?re</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sergey</forename><surname>Tulyakov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Elisa</forename><surname>Ricci</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nicu</forename><surname>Sebe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conf. Comput. Vis. Pattern Recog</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">First order motion model for image animation</title>
		<author>
			<persName><forename type="first">Aliaksandr</forename><surname>Siarohin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">St?phane</forename><surname>Lathuili?re</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sergey</forename><surname>Tulyakov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Elisa</forename><surname>Ricci</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nicu</forename><surname>Sebe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Adv. Neural Inform. Process. Syst</title>
		<imprint>
			<date type="published" when="2019">2019b</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Stylegan-v: A continuous video generator with the price, image quality and perks of stylegan2</title>
		<author>
			<persName><forename type="first">Ivan</forename><surname>Skorokhodov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sergey</forename><surname>Tulyakov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mohamed</forename><surname>Elhoseiny</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conf. Comput. Vis. Pattern Recog</title>
		<imprint>
			<date type="published" when="2022">2022</date>
			<biblScope unit="page" from="3626" to="3636" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">A good image generator is what you need for high-resolution video synthesis</title>
		<author>
			<persName><forename type="first">Yu</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jian</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Menglei</forename><surname>Chai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kyle</forename><surname>Olszewski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xi</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dimitris</forename><forename type="middle">N</forename><surname>Metaxas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sergey</forename><surname>Tulyakov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Int. Conf. Learn. Represent</title>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Mocogan: Decomposing motion and content for video generation</title>
		<author>
			<persName><forename type="first">Sergey</forename><surname>Tulyakov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ming-Yu</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaodong</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jan</forename><surname>Kautz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conf. Comput. Vis. Pattern Recog</title>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<monogr>
		<title level="m" type="main">Towards accurate generative models of video: A new metric &amp; challenges</title>
		<author>
			<persName><forename type="first">Thomas</forename><surname>Unterthiner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Karol</forename><surname>Sjoerd Van Steenkiste</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Raphael</forename><surname>Kurach</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Marcin</forename><surname>Marinier</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sylvain</forename><surname>Michalski</surname></persName>
		</author>
		<author>
			<persName><surname>Gelly</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1812.01717</idno>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Lasse Espeholt, Oriol Vinyals, Alex Graves, et al. Conditional image generation with pixelcnn decoders</title>
		<author>
			<persName><forename type="first">Aaron</forename><surname>Van Den Oord</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nal</forename><surname>Kalchbrenner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Adv. Neural Inform. Process. Syst</title>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Visualizing data using t-sne</title>
		<author>
			<persName><forename type="first">Laurens</forename><surname>Van Der Maaten</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Geoffrey</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of machine learning research</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">11</biblScope>
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Generating videos with scene dynamics</title>
		<author>
			<persName><forename type="first">Carl</forename><surname>Vondrick</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hamed</forename><surname>Pirsiavash</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Antonio</forename><surname>Torralba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Adv. Neural Inform. Process. Syst</title>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<monogr>
		<author>
			<persName><forename type="first">Jacob</forename><surname>Walker</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ali</forename><surname>Razavi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A?ron</forename><surname>Van Den Oord</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2103.01950</idno>
		<title level="m">Predicting video with vqvae</title>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">Rewriting geometric rules of a gan</title>
		<author>
			<persName><forename type="first">Sheng-Yu</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><surname>Bau</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jun-Yan</forename><surname>Zhu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Transactions on Graphics (TOG)</title>
		<imprint>
			<biblScope unit="volume">41</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="1" to="16" />
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<monogr>
		<title/>
		<author>
			<persName><surname>Ting-Chun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ming-Yu</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jun-Yan</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Guilin</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrew</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jan</forename><surname>Tao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bryan</forename><surname>Kautz</surname></persName>
		</author>
		<author>
			<persName><surname>Catanzaro</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1808.06601</idno>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="report_type">Video-to-video synthesis. arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b54">
	<monogr>
		<title level="m" type="main">Latent image animator: Learning to animate images via latent space navigation</title>
		<author>
			<persName><forename type="first">Yaohui</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Di</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Francois</forename><surname>Bremond</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Antitza</forename><surname>Dantcheva</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2203.09043</idno>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b55">
	<monogr>
		<author>
			<persName><forename type="first">Dirk</forename><surname>Weissenborn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Oscar</forename><surname>T?ckstr?m</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jakob</forename><surname>Uszkoreit</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1906.02634</idno>
		<title level="m">Scaling autoregressive video models</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b56">
	<monogr>
		<author>
			<persName><forename type="first">Chenfei</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lun</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qianxi</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Binyang</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lei</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fan</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Guillermo</forename><surname>Sapiro</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nan</forename><surname>Duan</surname></persName>
		</author>
		<author>
			<persName><surname>Godiva</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2104.14806</idno>
		<title level="m">Generating open-domain videos from natural descriptions</title>
		<imprint>
			<biblScope unit="page">2021</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b57">
	<monogr>
		<title level="m" type="main">N\&quot; uwa: Visual synthesis pre-training for neural visual world creation</title>
		<author>
			<persName><forename type="first">Chenfei</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jian</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lei</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fan</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuejian</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Daxin</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nan</forename><surname>Duan</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2111.12417</idno>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b58">
	<analytic>
		<title level="a" type="main">Learning to generate time-lapse videos using multi-stage dynamic generative adversarial networks</title>
		<author>
			<persName><forename type="first">Wei</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wenhan</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lin</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wei</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiebo</forename><surname>Luo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="2364" to="2373" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b59">
	<monogr>
		<title level="m" type="main">Videogpt: Video generation using vq-vae and transformers</title>
		<author>
			<persName><forename type="first">Wilson</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yunzhi</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pieter</forename><surname>Abbeel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aravind</forename><surname>Srinivas</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2104.10157</idno>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b60">
	<analytic>
		<title level="a" type="main">Pose guided human video generation</title>
		<author>
			<persName><forename type="first">Ceyuan</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhe</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xinge</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chen</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jianping</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dahua</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Eur. Conf. Comput. Vis</title>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b61">
	<analytic>
		<title level="a" type="main">Semantic hierarchy emerges in deep generative representations for scene synthesis</title>
		<author>
			<persName><forename type="first">Ceyuan</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yujun</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bolei</forename><surname>Zhou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Int. J. Comput. Vis</title>
		<imprint>
			<biblScope unit="volume">129</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="1451" to="1466" />
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b62">
	<monogr>
		<title level="m" type="main">Diffusion probabilistic modeling for video generation</title>
		<author>
			<persName><forename type="first">Ruihan</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Prakhar</forename><surname>Srivastava</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stephan</forename><surname>Mandt</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2203.09481</idno>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b63">
	<monogr>
		<title level="m" type="main">Generating videos with dynamics-aware implicit generative adversarial networks</title>
		<author>
			<persName><forename type="first">Sihyun</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jihoon</forename><surname>Tack</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sangwoo</forename><surname>Mo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hyunsu</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Junho</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jung-Woo</forename><surname>Ha</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jinwoo</forename><surname>Shin</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2202.10571</idno>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b64">
	<analytic>
		<title level="a" type="main">Learning to drive by watching youtube videos: Action-conditioned contrastive policy pretraining</title>
		<author>
			<persName><forename type="first">Qihang</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhenghao</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bolei</forename><surname>Zhou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2022">2022</date>
			<biblScope unit="page" from="111" to="128" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
