<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Limago: an FPGA-based Open-source 100 GbE TCP/IP Stack</title>
				<funder ref="#_qtTZN4H">
					<orgName type="full">European Regional Development Fund</orgName>
				</funder>
				<funder>
					<orgName type="full">Spanish Ministry of Economy and Competitiveness</orgName>
				</funder>
				<funder ref="#_JH372fE">
					<orgName type="full">European Commission</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Mario</forename><surname>Ruiz</surname></persName>
							<email>mario.ruiz@uam.es</email>
							<affiliation key="aff0">
								<orgName type="department">High Performance Computing and Networking Research Group</orgName>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="department">Escuela Polit?cnica Superior</orgName>
								<orgName type="institution">Universidad Aut?noma de Madrid</orgName>
								<address>
									<country key="ES">Spain</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">David</forename><surname>Sidler</surname></persName>
							<email>dasidler@inf.ethz.ch</email>
							<affiliation key="aff2">
								<orgName type="department">Department of Computer Science</orgName>
								<orgName type="laboratory">Systems Group</orgName>
								<orgName type="institution">ETH Z?rich</orgName>
								<address>
									<country key="CH">Switzerland</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Gustavo</forename><surname>Sutter</surname></persName>
							<email>gustavo.sutter@uam.es</email>
							<affiliation key="aff0">
								<orgName type="department">High Performance Computing and Networking Research Group</orgName>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="department">Escuela Polit?cnica Superior</orgName>
								<orgName type="institution">Universidad Aut?noma de Madrid</orgName>
								<address>
									<country key="ES">Spain</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Gustavo</forename><surname>Alonso</surname></persName>
							<email>alonso@inf.ethz.ch</email>
							<affiliation key="aff2">
								<orgName type="department">Department of Computer Science</orgName>
								<orgName type="laboratory">Systems Group</orgName>
								<orgName type="institution">ETH Z?rich</orgName>
								<address>
									<country key="CH">Switzerland</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Sergio</forename><surname>L?pez-Buedo</surname></persName>
							<email>sergio.lopez-buedo@uam.es</email>
							<affiliation key="aff0">
								<orgName type="department">High Performance Computing and Networking Research Group</orgName>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="department">Escuela Polit?cnica Superior</orgName>
								<orgName type="institution">Universidad Aut?noma de Madrid</orgName>
								<address>
									<country key="ES">Spain</country>
								</address>
							</affiliation>
							<affiliation key="aff3">
								<orgName type="department">NAUDIT HPCN</orgName>
								<address>
									<country key="ES">Spain</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Limago: an FPGA-based Open-source 100 GbE TCP/IP Stack</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="DOI">10.1109/FPL.2019.00053</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-01-03T09:01+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>The realization that the network is becoming an important bottleneck in computing clusters and in the cloud has led in the past years to an increase scrutiny of how networking functionality is deployed. From TCP Offload Engines (TOEs) to Software Defined Networking (SDN), including Smart NICs and In-Network Data Processing, a wide range of approaches are currently being explored to increase the efficiency of networks and tailor its functionality to the actual needs of the application at hand. To address the need for an open and customizable networking stack, in this paper we introduce Limago, an FPGAbased open-source implementation of a TCP/IP stack operating at 100 Gbit/s. To our knowledge, Limago provides the first complete description of an FPGA-based TCP/IP stack at these speeds, thereby illustrating the bottlenecks that must be addressed, proposing several innovative designs to reach the necessary throughput, and showing how to incorporate advanced protocol features into the design. As an example, Limago supports the TCP Window Scale option, addressing the Long Fat Pipe issue. Limago not only enables 100 Gbit/s Ethernet links in an open source package, but also paves the way to programmable and fully customizable NICs based on FPGAs.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>I. INTRODUCTION</head><p>The growing amount of data and the complexity of the workloads that characterize modern distributed computing have turned the network into a potential bottleneck <ref type="bibr" target="#b0">[1]</ref>. Besides, in cloud environments, the network also limits the number of virtualized/containerized applications that can be deployed on a single server: The more CPU cycles needed to deal with an increasingly complex networking stack, which needs to provide not only TCP/IP packet processing but additional functionality such as Network Function Virtualization (NFV) or Remote Direct Memory Access (RDMA), the less CPU cycles that are available to applications. In addition, the trend towards specialization seen in cloud computing opens up the possibility of tailored network designs through Smart Network Interface Cards (NICs), which push application-level processing to the network <ref type="bibr" target="#b1">[2]</ref>. As a result, we are witnessing a flurry of activity around programmable networks based on a variety of designs and architectures.</p><p>A concrete example of these developments is provided by Microsoft Catapult <ref type="bibr" target="#b2">[3]</ref>, a deployment of FPGAs in the cloud that has evolved through several generations <ref type="bibr" target="#b3">[4]</ref>, <ref type="bibr" target="#b4">[5]</ref>. The current version inserts an FPGA on the data path between the top of rack (ToR) switch and the server machine. Hence, all network traffic in and out of the host goes through the FPGA.</p><p>The FPGA is then used to augment the network functionality with system and application-level features. For instance, it can be used as a customizable smartNIC to offload network virtualization functionality <ref type="bibr" target="#b4">[5]</ref>, application-level functionality such as RDMA packet processing to support key-value stores <ref type="bibr" target="#b5">[6]</ref>, or for distributed machine learning algorithms <ref type="bibr" target="#b6">[7]</ref>. Catapult is, by far, not the only possible design. In IBM's cloudFPGA <ref type="bibr" target="#b7">[8]</ref>, the FPGA is deployed as a network-attached accelerator. Similarly, Caribou <ref type="bibr" target="#b8">[9]</ref> deploys FPGAs as storage nodes that extend the TCP/IP stack with distributed consensus functionality (a network function) <ref type="bibr" target="#b9">[10]</ref> as well as scans and string processing (application-level functionality) <ref type="bibr" target="#b8">[9]</ref>.</p><p>Promising as they are, for FPGA-based designs a challenge remains: Scalability with increasing network bandwidth. To address this challenge, in this paper we introduce Limago, an open-source 100 Gbit/s TCP/IP network stack on an FPGA. Limago explores the changes needed to upgrade an existing open-source TCP/IP stack from 10 Gbit/s <ref type="bibr" target="#b10">[11]</ref> to 100 Gbit/s, but maintaining the same high-productivity design methodology, based on Vivado-HLS, that was utilized in the previous design. In doing so, Limago illustrates how to tackle the problem of FPGA-based packet processing at such rates. From the existing design, Limago inherits the scalability in terms of the number of connections as well as the control flow and congestion avoidance functionality. Limago not only transforms and adapts these existing features to increase the supported bandwidth from 10 Gbit/s to 100 Gbit/s, but also contributes novel features widening its applicability. The changes are non-trivial extensions of the existing stack. For instance, the data path had to be widened by 8x and the operating frequency doubled to reach the target bandwidth, several low-level architectural changes and balanced pipeline stages were necessary to meet timing, and accessory modules were redesigned. Limago also incorporates functionality such as the TCP Window Scale option, an extension to the basic TCP/IP protocol, addressing the Long Fat Pipe issue.</p><p>Limago serves as a platform for further research in programmable networking and as a design guideline on how to tackle high network bandwidths with FPGA-based systems.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>II. CHALLENGES AT 100 GBIT/S</head><p>Limago uses the 322 MHz clock provided by the integrated 100G CMAC, and a 512-bit AXI4-Stream interface. With respect to the 10 Gbit/s version, that is an 8x increase in the width of the datapath and more than a 2x increase in the operating frequency. Moreover, the smallest packet (64-Byte) just fits into a single transaction and, for such short packets, the processing rate must be 148.8 million packets per second. The greater data rate implies novel designs for several components often taken for granted. For instance, existing SmartCAM designs, used for flow identification, do not operate at such frequency and a new solution is thus needed. Similarly, certain optimizations are optional at lower rates, but a must at such bandwidth. For instance, the Long Fat Pipe issue might not be observable at 10 Gbit/s but must be addressed to reach 100 Gbit/s. This requires additional circuitry to support and negotiate the TCP Window Scale option.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. TCP/IP Checksum</head><p>Checksum computations are widely used when processing TCP/IP packets. Limago uses an efficient implementation, leveraging 7 to 3 Carry Save Adder (CSA) circuits <ref type="bibr" target="#b11">[12]</ref> to calculate the checksum within one clock cycle. The module was written in HDL to achieve a low latency in this recurrent circuit. Actually, this is one of the few modules of Limago written in HDL; the vast majority of blocks are written in Vivado-HLS. But in this case, an efficient and low latency implementation was needed, impossible to achieve with the Vivado-HLS version being used (2018.2). The circuit is described in detail in <ref type="bibr" target="#b12">[13]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. CuckooCAM</head><p>The 10 Gbit/s version of the stack is based on the smart-CAM <ref type="bibr" target="#b13">[14]</ref> module provided by Xilinx. It used a four-tuple consisting of IP source and destination addresses plus TCP source and destination ports as a key. We replaced this module with our own implementation, CuckooCAM, based on cuckoo hashing and requiring one clock cycle for lookup and deletion. In CuckooCAM, insertion time depends on the load factor and occupancy can exceed 90% due to a secondary memory structure known as a stash. It is clocked at 322 MHz, providing more than 300 million lookups per second. The width of the key and value are configurable; therefore, we have reduced the size of the key to a three-tuple by removing Limago's own IP address, which does not change during operation. The reduction of the key from 96-bit to 64-bit results in a significant reduction in BRAM usage for this module (22%).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. DRAM Memory Access</head><p>To support a large number of connections, the TOE uses external memory for its receive and send buffers. In particular, this is necessary for the send buffer, where the payload has to be stored until it is acknowledged. For 100 Gbit/s, the resulting requirements in terms of memory bandwidth are close to the peak bandwidth provided by DDR4. Additionally, the offsets into the receive and send buffer are determined by the TCP sequence number. This can result in unaligned memory accesses affecting the memory bandwidth further. Therefore, we verified the viability of storing the buffers in external DDR4-2400 through several microbenchmarks, varying the memory-alignment as well as the access size. We observed a peak bandwidth of 125 Gbit/s with 64-Byte aligned words and approximately a 6% performance loss when transfers were not aligned, thereby ensuring the design achieves enough memory bandwidth for all cases. Since the buffers in external memory are organized as a circular buffer, additional logic is required to handle the wrap-around when the "end" of the buffer is reached. Particularly, a single data transfer is split into two transfers (one before the wrap-around and one after), requiring data re-alignment. The HLS code for this module was redesigned carefully to guide the synthesis tool to the most efficient implementation involving a 64 to 1 multiplexer.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D. TCP Window Scale Option</head><p>Links with a large bandwidth ? delay product suffer from the Long Fat Pipe issue: those links where the bandwidth ? delay product is larger than the buffer size <ref type="bibr" target="#b14">[15]</ref>. The Window Scale option is used to allocate any fix-size buffer in the range of 2 16 to 2 30 bytes, thereby leading to a better usage of links.</p><p>Currently, Window Scale is the only supported TCP option in Limago. Due to the lack of a standard TCP option layout, the parsing of options is done sequentially, one clock cycle each. Fortunately, the Window Scale option is only negotiated during the initial three-way handshake, i.e., options are only parsed once in the lifetime of a connection. The Window Scale is set to the minimum value advertised by both endpoints. Support for the Window Scale option has to be enabled at synthesis.</p><p>Finally, the maximum number of connections depends on the external DRAM capacity and the Window Scale factor, as shown by Equation <ref type="formula">1</ref>. DRAM b is the log 2 (DRAM Size) and W S b is the log 2 (W indowScale). As an example, with 4 GB of DRAM and a Window Scale of 128, 2 32-7-16 = 2 9 = 512 concurrent connections can be supported.</p><formula xml:id="formula_0">#conn = 2 DRAM b -W S b -16</formula><p>(1)</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>III. RELATED WORK</head><p>The benefits of TCP/IP offloading are well-known <ref type="bibr" target="#b15">[16]</ref>- <ref type="bibr" target="#b17">[18]</ref>: reduced CPU utilization and bypassing of the Operating System. In a TOE, packet processing is moved to the NIC, whereas the control decision remains in the host. Nowadays, most NICs offer some degree of offloading. In this section, we focus on FPGA implementations of TCP/IP.</p><p>LDA technologies <ref type="bibr" target="#b18">[19]</ref> offers an FPGA-based TOE. Their solution includes independent transmitter and receiver modules. For sixteen connections, 44 BRAMs and 2,704 LUTs are necessary. Published results for this TOE using Solarflare NICs are based on 10 Gbit/s connections. Chevin Technology <ref type="bibr" target="#b19">[20]</ref> offers a 10/25 Gbit/s TCP/IP core, which can work both as client or server. It supports 1 to 256 simultaneous connections. The Tx and Rx buffers can be configured from 1 KiB to 1 GiB, implying Window Scale support. For sixteen connections, 5 BRAMs and 12,000 LUTs (plus the external buffer) are necessary. Enyx <ref type="bibr" target="#b20">[21]</ref> offers an RTL TOE solution with up to 4,000 connections, but not further details about resource utilization are provided. They also have announced a 25 Gbit/s implementation <ref type="bibr" target="#b21">[22]</ref>. Dini <ref type="bibr" target="#b22">[23]</ref> offers a 10 Gbit/s solution where the FPGA is used as a NIC. The buffer size is configurable from 4 KiB to 64 KiB and it supports up to 128 connections per instantiated IP-Core and out-of-order packet delivery. Algo-Logic <ref type="bibr" target="#b23">[24]</ref> supports full duplex rates up to 20 Gbit/s per instance, claiming more than 200 Gbit/s can be achieved with multiple instances. The design targets low latency applications such as high-frequency trading.</p><p>The authors in <ref type="bibr" target="#b24">[25]</ref> presented a comparison of three 10 Gbit/s alternatives: a pure software TCP/IP stack, a software TOE with kernel-bypassing and a hardware TOE (Fraunhofer HHI 10 GbE TCP/IP) with kernel-bypassing, concluding that the hardware solution has less latency and a more deterministic behaviour. The work in <ref type="bibr" target="#b25">[26]</ref> presents a complete TOE implementation supporting jumbo frames and configurable Maximum Segment Size (MSS) and timestamp. Only one connection is supported with a 90 ns latency for a 100-Byte packet. Their solution is compared against a commercial, one achieving better latency. The paper in <ref type="bibr" target="#b26">[27]</ref> introduces a TCP implementation using XFSMs, which is claimed to be "code-once-port-everywhere". The implementation is tested over three different architectures, software, FPGA, and NS3 emulator, reaching similar results. Probably, the closest work to Limago is <ref type="bibr" target="#b27">[28]</ref>, an asymmetrical standalone TCP/IP implementation oriented to video-on-demand, which supports 20,480 connections working as a client and 2,048 connections working as a server. It also can send up to 40 Gbit/s but only receive up to 4 Gbit/s. The starting point for Limago is a 10 Gbit/s TOE written by Sidler et al. in C++ using Vivado-HLS <ref type="bibr" target="#b10">[11]</ref>, <ref type="bibr" target="#b28">[29]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>IV. LIMAGO ARCHITECTURE</head><p>Figure <ref type="figure" target="#fig_0">1</ref> shows Limago's main components. We use AXI4-Stream to interface with the application logic as well as with the network modules. Since CMAC exposes an LBUS interface, we added an adapter module that converts between AXI4-Stream and LBUS. Rx and Tx checksum and CuckooCAM are respectively presented in sections II-A and II-B.</p><p>Inbound Packet Handler: parses Ethernet and IPv4 headers of every incoming packet. If the packet matches the filter, the signal TDEST will carry a different identifier for each kind of packet. Then an AXI4-Stream Switch forwards the packet to the appropriated module. If the packet does not belong to one of the previous categories, it is dropped.</p><p>ARP module: when an ARP request arrives and the IP address matches, it generates an ARP reply packet. Its main function is to associate IP addresses with MAC (physical) addresses, which is done using a 256-element table. The ARP module also receives MAC address requests from the Outbound Packet Handler. If the entry is not present in the table, an ARP request packet will be generated, and a miss will be reported. Additionally, an ARP request is generated at startup to notify other network devices in the same LAN.</p><p>ICMP module: provides responses to echo request packets, a.k.a., ping. The module is useful to verify connectivity and gives a fair estimation of the Round-Trip delay Time (RTT).</p><p>Memory Interface: is composed of a Data Mover and Memory Interface Generator (MIG), both Xilinx IP-Cores. The MIG exposes a 512-bit AXI4 memory mapped interface and communicates with the off-chip DDR4 memory. The Data Mover is in charge of merging data and commands, which are produced in a streaming fashion, to an AXI4 interface.</p><p>Outbound Packet Handler: gathers packets coming from ARP, ICMP and TOE modules. If needed, a MAC address lookup, consisting of the IP destination address, is issued to the ARP module. If the lookup is a hit, the Ethernet header is constructed using the returned MAC address, prepended to the packet, and transmitted. Otherwise, the packet is dropped and an ARP request is generated instead. Moreover, the packet size is evaluated and padded to 60-Byte if needed.</p><p>DMA subsystem: we use the DMA for PCI Express (PCIe) Subsystem Xilinx IP-Core for providing users access to memory mapped registers within the logic. Limago uses the Xilinx's drivers both for debugging and communication. The necessary customization is built on top of them.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>V. TOE ARCHITECTURE</head><p>This section describes the overall architecture of the TOE (Figure <ref type="figure">2</ref>). It is divided into three parts, the incoming data path (Rx Engine), the outgoing data path (Tx Engine), and the state-keeping data structures <ref type="bibr" target="#b10">[11]</ref>, <ref type="bibr" target="#b28">[29]</ref>. The dash boxes are optional modules that can be enabled at synthesis.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Rx Engine</head><p>Incoming packets are processed by the Rx Engine. To verify the checksum, first the TCP pseudo header is constructed If the lookup was a miss but the packet has the SYN flag set, the three-tuple is inserted with a new sessionID and a SYN-ACK event is generated. The FSM uses the sessionID to retrieve the sequence and acknowledgment number from the two SAR Tables and, if necessary, updates them. Finally, if the packet contains a payload, a notification is sent to the application while the payload is written to the Rx Buffer -in the case that is enabled. The FSM in the Rx Engine enforces a strict order of the packets and does not support out-of-order processing.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Data Structures</head><p>Session Lookup: provides the means to interface with the CukooCAM, using the three-tuple to obtain the sessionID. The sessionID is used to index every data structure to access the state of the corresponding connection. In case of a SYN or a SYN-ACK packet, if the three-tuple has not been inserted yet, it will be inserted using a new sessionID identifying the new connection. Additionally, the Session Lookup module contains a table that maps the sessionID to the three-tuple. This mapping is used by the Tx Engine to generate the IPv4 and TCP headers of outgoing packets.</p><p>Port Table : keeps track of the state of each port, which can be CLOSE, LISTEN or ACTIVE. The standard port range for static and ephemeral ports are used. If an incoming packet targets a port in CLOSE state, it is discarded and a RST packet is generated as a response.</p><p>State Table : stores the current state of each connection as specified by RFC793 <ref type="bibr" target="#b29">[30]</ref>. The State Table can be updated by the Rx Engine when incoming packets are processed and by the Tx App If when the application opens a connection. Consistency is guaranteed by using atomic operations.</p><p>Timers: this module supports all time-based event triggering as required by the protocol, three timer modules are implemented: Re-transmission, Probe and Time-Wait Timer. It follows the same approach of the original version, which provides linear scaling of on-chip memory.</p><p>Event Engine: gathers events from the Rx Engine, the Timers, and the Tx App If. Consequently, events are merged and forwarded to the Tx Engine that processes them to generate the corresponding outgoing packets. Each event will trigger the generation of a new TCP packet.</p><p>Buffering and Window Management: Since TCP is a stream-based protocol, it requires buffering on the receiving and transmitting side. On the receiving side, data is buffered in case the application is not able to immediately consume it. On the sending side, buffering is required for re-transmission in case of packet loss. Thus, when supporting multiple connections, the amount of memory that is needed increases linearly with the number of connections. For more than ten concurrent connections, the routing of on-chip memory becomes very complex and using DRAM to store the payloads becomes therefore mandatory. For every connection the memory buffer is logically implemented as a circular buffer which is stored in a fixed and pre-allocated segment within the off-chip memory. Stored in the Tx and Rx SAR Tables there are pointers, e.g., ack'ed, transmitted. which represent the state of the TCP window of each connection at a given time. The information stored in these tables is mandatory to handle the segmentation and reassembly (SAR) of packets. Moreover, to support the Window Scale TCP option, the Tx SAR Table <ref type="table">stores</ref> the Window Scale negotiated when the connection is established. This value defines the size and boundaries of the buffer.</p><p>Statistics: this module gathers events for inbound and outbound packets. The values can be read through an AXI4-Lite interface using the DMA subsystem. This element is optional and can be removed at synthesis.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Tx Engine</head><p>Each event triggers the generation of a new packet; the packet generation is done by the Tx Engine. The source of new packets can be the user application by either initiating a data transmission or by opening a new connection, which triggers a SYN packet. The Rx Engine generates events that generate ACK packets, including SYN-ACK. The Timers module triggers timeout-related events, such as re-transmission, probe packets and FIN packets for teardown. Like the Rx Engine, the Tx Engine has a FSM to handle each possible Contrary to Rx Engine, since each event carries the sessionID and event type, the sessionID is known when the event arrives. Consequently, the data-structures are queried immediately getting the necessary metadata to generate the packet. The Destination IP address and TCP ports are queried from the Session Lookup. Once the metadata is retrieved, the TCP pseudo header can be built. If the packet has payload, it is fetched from the external memory or directly from the application. Prepending the TCP pseudo header with the payload, the Tx Checksum computes the TCP checksum. Later, the IP header is prepended to the TCP packet. Finally, the packet is forwarded to the Outbound Packet Handler.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>VI. EVALUATION</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Setup</head><p>The evaluation of Limago covers both functionality and performance. In terms of functionality, first the ARP and ICMP modules were tested using Linux-GNU's arping and ping programs. Then, to test the TOE, we implemented an echo server transmitting the received payload back to the sender. Such a design allows to test both the Rx and Tx Engines. The same echo server is used to verify the correct functionality of the internal elements as well as verifying connectivity.</p><p>For the performance evaluation, we use iPerf <ref type="bibr" target="#b30">[31]</ref> version 2. We implemented iPerf in hardware, using Vivado-HLS, supporting both client and server modes. As a client, the application actively opens a connection and sends data at the highest possible rate to the server. As a server, the application waits for a SYN packet to establish a new connection. Once the connection is established, the client starts transmitting data and the application on the FPGA consumes the incoming payload while the TOE acknowledges the received packets. We have also built a user program on top of the Xilinx DMA driver to interact with the iPerf application deployed on the FPGA.</p><p>Limago was tested using two different configurations (Fig. <ref type="figure" target="#fig_1">3</ref>). Scheme 1 corresponds to a standard implementation, each TOE communicates with the corresponding CMAC, and a 100G cable connects both CMACs. In this case the maximum throughput is limited by the Ethernet connection. Scheme 2 removes the Ethernet CMAC and connects the TOE using a 512-bit AXI4-Stream interface clocked at 322 MHz. The idea behind this configuration is to verify the maximum throughput. In the second configuration, we also have tested replacing </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Bandwidth</head><p>To test the bandwidth of Limago we measured the throughput under two configurations (Fig. <ref type="figure" target="#fig_2">4</ref>) one to test the throughput over a network and another to test the maximum processing rate of Limago when not limited by the network. In this experiment, TOE 0 transmits data to TOE 1 , i.e, only the memory attached to TOE 0 is involved. The throughput reported measures the complete Ethernet frame, i.e., including the Ethernet, IP and TCP headers as well as the payload. In this experiment, the application transmitted segments ranging between 1024-Byte to 4096-Byte in steps of 64-Byte, using only one connection, each experiment lasted five minutes. For scheme 1, using external DRAM and transmitting packets over the 100 Gbit/s Ethernet link, the throughput is bound by the network. Scheme 2, using DRAM, Limago transmits more than 100 Gbit/s for all cases. However, beyond 2048-Byte segment size, the DRAM bandwidth limits the throughput. Scheme 2(b), using on-chip URAM, looks like a logarithmic function where the throughput increases with an increasing segment size. These experiments show that Limago is able to surpass 100 Gbit/s when it is not bound by network.</p><p>We also have carried out experiments with multiple connections at the same time. For those experiments we have used two servers and a Huawei cloudEngine 8800 switch. The specifications of the severs are as follows: both servers run on a 4.14.7-gentooHPC OS and use a Mellanox MT27800 ConnectX-5 100 Gbit/s NIC; server A has an Intel Xeon CPU E5-2630 v4 at 2.20 GHz and 128 GB of RAM memory, whereas, server B has an Intel Xeon Gold 6126 CPU at 2.60 GHz and 192 GB of RAM memory. All offloading capabilities have been enabled in both machines, using ethtool. We use iPerf to test the performance, this time the servers work as a client, which means they send the data. Three different scenarios have been evaluated, each server individually and both servers combined. For both servers combined, each one contributes with half of the connections. The number of concurrent connections range from two to thirty in steps of two, each test lasted five minutes and was repeated five times, we used the maximum packet size which is 1460-Byte. Figure <ref type="figure" target="#fig_3">5</ref> shows the results -which are measured at the application level -the mean and standard deviation are plotted, as well as the theoretical maximum. In general, the performance increases with a higher number of concurrent connections, until it is stable. With regard to both servers sending data simultaneously, a better performance is not observed, from this we notice that the switch could be the bottleneck. Further experiments are necessary to confirm this.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Resource Usage and Code Complexity</head><p>Limago has been implemented using Vivado and Vivado HLS 2018.2. The prototype uses a VCU118 board with a Virtex Ultrascale+ FPGA. Table I list the BRAM usage of the TOE for a wide variety of number of connections at a specific Window Scale. The LUTs and Flip-Flop cost is omitted to due to small difference between the different scenarios -36 K to 41 K. As explained earlier and confirmed by the actual BRAM usage, the data structures in our implementation scale linearly with the number of supported connections.</p><p>The resource usage of Limago for 10,000 connections and no Window Scale is listed in Table <ref type="table" target="#tab_2">II</ref>. The overall LUT usage is at 10% whereby 3.1% is used by the TOE. The TOE is also 1.5% of the available Flip-Flops which is around 20% of the total usage. But at the same time 90% of the logic resources are still available and can be used to deploy an application on the FPGA. BRAM capacity is a scarcer resource, the TOE uses almost 12% of them, overall around 80% of BRAM and 100% URAM capacity is still available for further use. The table also shows the resource summary for the 10 G starting point implementation, the resources of the TOE increased by a factor of 1.2 to 2.1. The overall logic resources increased by a factor of two and the BRAM usage by 20%. Particularly noteworthy, the tenfold bandwidth increase, at worse, only requires twice as much resources.</p><p>Limago has ten core modules, seven of them are written in HLS. Apart from the checksum, the other two HDL modules are straightforward, however determinism is needed. We used cloc <ref type="bibr" target="#b31">[32]</ref> to count the lines of code (no headers), the HLS part is 7,456 lines; whereas the HDL is 1,482 lines.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>VII. CONCLUSIONS</head><p>Limago is an open-source <ref type="bibr" target="#b32">[33]</ref> 100 Gbit/s TCP/IP stack that can be implemented on FPGA to enable research and development in programmable NICs and in-network computing. Starting from a pre-existing stack operating at 10 Gbit/s, Limago provides a tenfold increase in bandwidth at the cost of a mere 20% increase in BRAM usage, without jeopardizing the ability to support multiple connections of the original design, and maintaining the same design methodology based on Vivado-HLS. The current prototype has been implemented and successfully tested on Xilinx VCU118 and Alveo U200 boards. Future work includes further optimizations of the stack to, e.g., enable reordering of out-of-order packets and additional TCP features taking advantage of the increasing availability of High Bandwidth Memory in FPGAs. This feature will improve the throughput when packet loss occurs as well as support application level processing <ref type="bibr" target="#b33">[34]</ref>, <ref type="bibr" target="#b34">[35]</ref>.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 1 :</head><label>1</label><figDesc>Fig. 1: General Architecture Overview.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 3 :</head><label>3</label><figDesc>Fig. 3: TOE Interconnection Schemes</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 4 :</head><label>4</label><figDesc>Scheme 1 (Link Bounded) Scheme 2 (DRAM Bounded) Scheme 2(b) (Design Bounded)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Fig. 5 :</head><label>5</label><figDesc>Fig. 5: Concurrent Connections, mean and standard deviation.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>TABLE I :</head><label>I</label><figDesc>No. of BRAM18 for different TOE configurations.</figDesc><table><row><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">Window Scale</cell><cell></cell><cell></cell><cell></cell></row><row><cell>#conn</cell><cell>0</cell><cell>1</cell><cell>2</cell><cell>3</cell><cell>4</cell><cell>5</cell><cell>6</cell><cell>7</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">Number of BRAM18</cell><cell></cell><cell></cell><cell></cell></row><row><cell>1</cell><cell>198</cell><cell>221</cell><cell>221</cell><cell>222</cell><cell>222</cell><cell>222</cell><cell>222</cell><cell>222</cell></row><row><cell>128</cell><cell>202</cell><cell>225</cell><cell>225</cell><cell>226</cell><cell>226</cell><cell>226</cell><cell>226</cell><cell>226</cell></row><row><cell>512</cell><cell>202</cell><cell>225</cell><cell>225</cell><cell>226</cell><cell>226</cell><cell>226</cell><cell>226</cell><cell>226</cell></row><row><cell>1,024</cell><cell>204</cell><cell>227</cell><cell>227</cell><cell>233</cell><cell>233</cell><cell>233</cell><cell>233</cell><cell>233</cell></row><row><cell>2,048</cell><cell>228</cell><cell>251</cell><cell>251</cell><cell>257</cell><cell>257</cell><cell>257</cell><cell>257</cell><cell>257</cell></row><row><cell>4,096</cell><cell>276</cell><cell>299</cell><cell>299</cell><cell>305</cell><cell>305</cell><cell>305</cell><cell>305</cell><cell>311</cell></row><row><cell>8,192</cell><cell>371</cell><cell>397</cell><cell>397</cell><cell>403</cell><cell>403</cell><cell>409</cell><cell>409</cell><cell>414</cell></row><row><cell>10,000</cell><cell>495</cell><cell>514</cell><cell>514</cell><cell>520</cell><cell>526</cell><cell>532</cell><cell>538</cell><cell>544</cell></row><row><cell>16,384</cell><cell>566</cell><cell>602</cell><cell>613</cell><cell>619</cell><cell>625</cell><cell>631</cell><cell>637</cell><cell>643</cell></row><row><cell>32,768</cell><cell>974</cell><cell cols="3">1,023 1,035 1,047</cell><cell cols="4">1,059 1,071 1,083 1,095</cell></row><row><cell>65,536</cell><cell cols="2">1,774 1,843</cell><cell cols="6">1,867 1,891 1,915 1,947 1,963 1,995</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>TABLE II :</head><label>II</label><figDesc>Full design resource usage on VCU118</figDesc><table><row><cell>Element</cell><cell cols="2">LUT</cell><cell>FF</cell><cell></cell><cell cols="2">BRAM</cell></row><row><cell></cell><cell></cell><cell cols="2">100 G</cell><cell></cell><cell></cell><cell></cell></row><row><cell>Memory</cell><cell>17,423</cell><cell>1.5%</cell><cell>25,995</cell><cell>1.1%</cell><cell>41.5</cell><cell>1.9%</cell></row><row><cell>CMAC</cell><cell>14,614</cell><cell>1.2%</cell><cell>39,550</cell><cell>1.7%</cell><cell>26.5</cell><cell>1.2%</cell></row><row><cell>ARP</cell><cell>1,260</cell><cell>0.1%</cell><cell>3,193</cell><cell>0.1%</cell><cell>1.5</cell><cell>0.1%</cell></row><row><cell>ICMP</cell><cell>2,056</cell><cell>0.2%</cell><cell>5,561</cell><cell>0.2%</cell><cell>0</cell><cell>0.0%</cell></row><row><cell>Inbound</cell><cell>1,816</cell><cell>0.2%</cell><cell>6,293</cell><cell>0.3%</cell><cell>8.5</cell><cell>0.4%</cell></row><row><cell>Outbound</cell><cell>2,680</cell><cell>0.2%</cell><cell>9,324</cell><cell>0.4%</cell><cell>34</cell><cell>1.6%</cell></row><row><cell>CuckooCAM</cell><cell>2,095</cell><cell>0.2%</cell><cell>1,392</cell><cell>0.1%</cell><cell>36</cell><cell>1.7%</cell></row><row><cell>TOE</cell><cell>36,469</cell><cell>3.1%</cell><cell>36,229</cell><cell>1.5%</cell><cell>247.5</cell><cell>11.5%</cell></row><row><cell>Total</cell><cell>119,844</cell><cell>10.1%</cell><cell>178,339</cell><cell>7.5%</cell><cell>441.5</cell><cell>20.4%</cell></row><row><cell></cell><cell></cell><cell></cell><cell>10 G</cell><cell></cell><cell></cell><cell></cell></row><row><cell>TOE</cell><cell>15,415</cell><cell>1.3%</cell><cell>16,616</cell><cell>0.7%</cell><cell>186.5</cell><cell>8.7%</cell></row><row><cell>SmartCAM</cell><cell>2,201</cell><cell>0.2%</cell><cell>1,772</cell><cell>0.1%</cell><cell>57.5</cell><cell>2.7%</cell></row><row><cell>Total</cell><cell>77,393</cell><cell>6.6%</cell><cell>85,306</cell><cell>3.6%</cell><cell>369</cell><cell>17.1%</cell></row></table></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div><p>ACKNOWLEDGMENT This work was supported in part by the <rs type="funder">Spanish Ministry of Economy and Competitiveness</rs> and the <rs type="funder">European Regional Development Fund</rs> under project <rs type="projectName">TR ?FICA</rs> (<rs type="grantNumber">MINECO/FEDER TEC2015-69417-C2-1-R</rs>) and by the <rs type="funder">European Commission</rs> under project <rs type="projectName">METRO-HAUL</rs> (grant agreement No. <rs type="grantNumber">761727</rs>). Part of the work on Limago of <rs type="person">M. Ruiz</rs> and <rs type="person">G. Sutter</rs> was done at ETH Z?rich as part of a visit funded by <rs type="institution">Univ</rs>. <rs type="person">Aut?noma de Madrid</rs> and <rs type="person">Jos? Castillejo program</rs>, respectively.</p></div>
			</div>
			<listOrg type="funding">
				<org type="funded-project" xml:id="_qtTZN4H">
					<idno type="grant-number">MINECO/FEDER TEC2015-69417-C2-1-R</idno>
					<orgName type="project" subtype="full">TR ?FICA</orgName>
				</org>
				<org type="funded-project" xml:id="_JH372fE">
					<idno type="grant-number">761727</idno>
					<orgName type="project" subtype="full">METRO-HAUL</orgName>
				</org>
			</listOrg>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">The Impact of Capacity Growth in National Telecommunications Networks</title>
		<author>
			<persName><forename type="first">A</forename><surname>Lord</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Soppera</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Jacquet</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Philosophical Transactions of the Royal Society A: Mathematical, Physical and Engineering Sciences</title>
		<imprint>
			<biblScope unit="volume">374</biblScope>
			<biblScope unit="page">20140431</biblScope>
			<date type="published" when="2016">2062. 2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">The Case For In-Network Computing On Demand</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Tokusashi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">T</forename><surname>Dang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Pedone</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Soul?</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Zilberman</surname></persName>
		</author>
		<idno type="DOI">10.1145/3302424.3303979</idno>
		<ptr target="http://doi.acm.org/10.1145/3302424.3303979" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Fourteenth EuroSys Conference 2019, ser. EuroSys &apos;19</title>
		<meeting>the Fourteenth EuroSys Conference 2019, ser. EuroSys &apos;19<address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2019">2019</date>
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="page" from="1" to="21" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">A Cloud-Scale Acceleration Architecture</title>
		<author>
			<persName><forename type="first">A</forename><surname>Caulfield</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Chung</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Putnam</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Angepat</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Fowers</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Haselman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Heil</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Humphrey</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Kaur</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J.-Y</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Lo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Massengill</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Ovtcharov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Papamichael</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Woods</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Lanka</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Chiou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Burger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The 49th Annual IEEE/ACM International Symposium on Microarchitecture</title>
		<imprint>
			<date type="published" when="2016-10">October 2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title/>
		<author>
			<persName><forename type="first">A</forename><surname>Putnam</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Caulfield</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Chung</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Chiou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Constantinides</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Demme</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Esmaeilzadeh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Fowers</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Gray</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Haselman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Hauck</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Heil</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Hormati</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J.-Y</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Lanka</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Peterson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Smith</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Thong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">Y</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Burger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Larus</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">P</forename><surname>Gopal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Pope</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
	<note>Reconfigurable Fabric for Accelerating Large-Scale Datacenter Services,&quot; in ISCA&apos;14</note>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Azure Accelerated Networking: SmartNICs in the Public Cloud</title>
		<author>
			<persName><forename type="first">D</forename><surname>Firestone</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Putnam</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Mundkur</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Chiou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NSDI&apos;18)</title>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">KV-Direct: High-Performance In-Memory Key-Value Store with Programmable NIC</title>
		<author>
			<persName><forename type="first">B</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Ruan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Putnam</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
	<note>in SOSP &apos;17</note>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Serving DNNs in Real Time at Datacenter Scale with Project Brainwave</title>
		<author>
			<persName><forename type="first">E</forename><surname>Chung</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Fowers</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Ovtcharov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Papamichael</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Caulfield</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Massengill</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Ghandi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Lo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Reinhardt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Alkalay</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Angepat</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Chiou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Forin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Burger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Woods</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Weisz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Haselman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Micro</title>
		<imprint>
			<biblScope unit="volume">38</biblScope>
			<biblScope unit="page" from="8" to="20" />
			<date type="published" when="2018-03">March 2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Enabling FPGAs in Hyperscale Data Centers</title>
		<author>
			<persName><forename type="first">J</forename><surname>Weerasinghe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Abel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Hagleitner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Herkersdorf</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2015 IEEE 12th Intl Conf on Ubiquitous Intelligence and Computing and 2015 IEEE 12th Intl Conf on Autonomic and Trusted Computing and 2015 IEEE 15th Intl Conf on Scalable Computing and Communications and Its Associated Workshops (UIC-ATC-ScalCom)</title>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Caribou: Intelligent Distributed Storage</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Istv?n</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Sidler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Alonso</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">PVLDB</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="issue">11</biblScope>
			<date type="published" when="2017-08">Aug. 2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">Consensus in a Box: Inexpensive Coordination in Hardware</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Istv?n</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Sidler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Alonso</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Vukolic</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
	<note>in NSDI&apos;16</note>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Scalable 10Gbps TCP/IP Stack Architecture for Reconfigurable Hardware</title>
		<author>
			<persName><forename type="first">D</forename><surname>Sidler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Alonso</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Blott</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Karras</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Vissers</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Carley</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2015 IEEE 23rd Annual International Symposium on Field-Programmable Custom Computing Machines</title>
		<imprint>
			<date type="published" when="2015-05">May 2015</date>
			<biblScope unit="page" from="36" to="43" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<author>
			<persName><forename type="first">J.-P</forename><surname>Deschamps</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">J</forename><surname>Bioul</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">D</forename><surname>Sutter</surname></persName>
		</author>
		<title level="m">Synthesis of Arithmetic Circuits: FPGA, ASIC and Embedded Systems</title>
		<imprint>
			<publisher>John Wiley &amp; Sons</publisher>
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">FPGA-based TCP/IP Checksum Offloading Engine for 100 Gbps Networks</title>
		<author>
			<persName><forename type="first">G</forename><surname>Sutter</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Ruiz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Lopez-Buedo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Alonso</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2018 International Conference on ReConFigurable Computing and FPGAs (ReConFig)</title>
		<imprint>
			<date type="published" when="2018-12">Dec 2018</date>
			<biblScope unit="page" from="1" to="6" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Exact Match Binary CAM Search IP for SDNet</title>
		<author>
			<persName><forename type="first">Xilinx</forename><surname>Inc</surname></persName>
		</author>
		<ptr target="https://www.xilinx.com/support/documentation/ipdocumentation/cam/pg189-cam.pdf" />
	</analytic>
	<monogr>
		<title level="j">Xilinx Inc., Tech. Rep</title>
		<imprint>
			<date type="published" when="2017">11 2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">RFC7323: TCP Extensions for High Performance</title>
		<author>
			<persName><forename type="first">D</forename><surname>Borman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Braden</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Jacobson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Scheffenegger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Tech. Rep</title>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">TCP Offload to the Rescue</title>
		<author>
			<persName><forename type="first">A</forename><surname>Currid</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Queue</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="58" to="65" />
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">TCP Offload Engines</title>
		<author>
			<persName><forename type="first">S</forename><surname>Senapathi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Hernandez</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Network AND Communications magazine pp</title>
		<imprint>
			<date type="published" when="2004">2004</date>
			<biblScope unit="page" from="103" to="107" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Performance Characterization of a 10-Gigabit Ethernet TOE</title>
		<author>
			<persName><forename type="first">W.-C</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Balaji</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Baron</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">N</forename><surname>Bhuyan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">K</forename><surname>Panda</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">High Performance Interconnects, 2005. Proceedings. 13th Symposium on</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2005">2005</date>
			<biblScope unit="page" from="58" to="63" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">LDA Lightspeed TCP</title>
		<author>
			<persName><surname>Lda Technologies</surname></persName>
		</author>
		<ptr target="http://www.ldatech.com/lda-lightspeed-tcp/" />
		<imprint>
			<biblScope unit="page" from="2018" to="2025" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">Chevin Technology&apos;s TCP/IP</title>
		<author>
			<persName><forename type="first">Chevin</forename><surname>Technologies</surname></persName>
		</author>
		<ptr target="https://chevintechnology.com/ethernet-ip-2/ct1008-xgtcp/" />
		<imprint>
			<biblScope unit="page" from="2019" to="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">10G TCP/IP Full-Hardware Stack IP Core Offload Engine for Xilinx FPGA</title>
		<author>
			<persName><surname>Enyx</surname></persName>
		</author>
		<ptr target="http://www.enyx.com/nxtcp-xilinx/" />
		<imprint>
			<biblScope unit="page" from="2018" to="2025" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<author>
			<persName><surname>Enyx</surname></persName>
		</author>
		<ptr target="http://www.enyx.com/2016/11/enyx-premieres-25g-tcp-udp-offload-engines-wxilinx-virtex-utlrascale-16nm-fpga-bittwares-xupp3r-pcie-board/" />
		<title level="m">Enyx Premieres 25G TCP and UDP Offload Engines with Xilinx Virtex UltraScale+ 16nm FPGA on BittWares XUPP3R PCIe Board</title>
		<imprint>
			<biblScope unit="page" from="2018" to="2025" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<author>
			<persName><forename type="first">Dini</forename><surname>Group</surname></persName>
		</author>
		<ptr target="https://www.dinigroup.com/web/TOE128.php" />
		<title level="m">TCP Offload Engine IP -128 Sessions (TOE128)</title>
		<imprint>
			<biblScope unit="page" from="2018" to="2025" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<author>
			<persName><surname>Algo-Logic</surname></persName>
		</author>
		<ptr target="http://algo-logic.com/tcp" />
		<title level="m">10G TCP Endpoint</title>
		<imprint>
			<biblScope unit="page" from="2018" to="2025" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">A 10 GbE TCP/IP Hardware Stack as Part of a Protocol Acceleration Platform</title>
		<author>
			<persName><forename type="first">U</forename><surname>Langenbach</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Berthe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Traskov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Weide</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Hofmann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Gregorius</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Consumer Electronics? Berlin (ICCE-Berlin)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2013">2013. 2013. 2013</date>
			<biblScope unit="page" from="381" to="384" />
		</imprint>
	</monogr>
	<note>ICCEBerlin</note>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Hardware TCP Offload Engine based on 10-Gbps Ethernet for low-latency Network Communication</title>
		<author>
			<persName><forename type="first">L</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Kang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Yin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Field-Programmable Technology (FPT), 2016 International Conference on</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="269" to="272" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">A Fully Portable TCP Implementation Using XFSMs</title>
		<author>
			<persName><forename type="first">G</forename><surname>Bianchi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Welzl</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Tulumello</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Belocchi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Faltelli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Pontarelli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the ACM SIGCOMM 2018 Conference on Posters and Demos</title>
		<meeting>the ACM SIGCOMM 2018 Conference on Posters and Demos</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="99" to="101" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">40Gbps Multi-Connection TCP/IP Offload Engine</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q.-S</forename><surname>Hu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Wireless Communications and Signal Processing (WCSP), 2011 International Conference on</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2011">2011</date>
			<biblScope unit="page" from="1" to="5" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Low-latency TCP/IP Stack for Data Center Applications</title>
		<author>
			<persName><forename type="first">D</forename><surname>Sidler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Istv?n</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Alonso</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Field Programmable Logic and Applications (FPL), 2016 26th International Conference</title>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="1" to="4" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<title level="m" type="main">Transmission Control Protocol RFC 793</title>
		<author>
			<persName><forename type="first">J</forename><surname>Postel</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1981">1981</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
		<author>
			<persName><forename type="first">A</forename><surname>Tirumala</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Qin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Dugan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Ferguson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Gibbs</surname></persName>
		</author>
		<ptr target="http://iperf.sourceforge.net" />
		<title level="m">iPerf: the TCP/UDP Bandwidth Measurement Tool (2005)</title>
		<imprint>
			<date type="published" when="2005">2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
		<ptr target="https://github.com/AlDanial/cloc/" />
		<title level="m">Count Lines of Code (cloc)</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
		<title level="m" type="main">100G-fpga-network-stack-core</title>
		<ptr target="https://github.com/hpcn-uam/100G-fpga-network-stack-core/" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Ibex -An Intelligent Storage Engine with Support for Advanced SQL Offloading</title>
		<author>
			<persName><forename type="first">L</forename><surname>Woods</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Istv?n</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Alonso</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the VLDB Endowment</title>
		<meeting>the VLDB Endowment</meeting>
		<imprint>
			<date type="published" when="2014">2014</date>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page" from="963" to="974" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Complex Event Detection at Wire Speed with FPGAs</title>
		<author>
			<persName><forename type="first">L</forename><surname>Woods</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Teubner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Alonso</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the VLDB Endowment</title>
		<meeting>the VLDB Endowment</meeting>
		<imprint>
			<date type="published" when="2010">2010</date>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page" from="660" to="669" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
