<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Diverse Sequential Subset Selection for Supervised Video Summarization</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Boqing</forename><surname>Gong</surname></persName>
							<email>boqinggo@usc.edu</email>
						</author>
						<author>
							<persName><forename type="first">Wei-Lun</forename><surname>Chao</surname></persName>
							<email>weilunc@usc.edu</email>
						</author>
						<author>
							<persName><forename type="first">Kristen</forename><surname>Grauman</surname></persName>
							<email>grauman@cs.utexas.edu</email>
						</author>
						<author>
							<persName><forename type="first">Fei</forename><surname>Sha</surname></persName>
							<email>feisha@usc.edu</email>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science</orgName>
								<orgName type="institution">University of Southern California</orgName>
								<address>
									<postCode>90089</postCode>
									<settlement>Los Angeles</settlement>
									<region>CA</region>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff1">
								<orgName type="department">Department of Computer Science</orgName>
								<orgName type="institution">University of Southern California</orgName>
								<address>
									<postCode>90089</postCode>
									<settlement>Los Angeles</settlement>
									<region>CA</region>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff2">
								<orgName type="department">Department of Computer Science</orgName>
								<orgName type="institution">University of Texas at Austin Austin</orgName>
								<address>
									<postCode>78701</postCode>
									<region>TX</region>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff3">
								<orgName type="department">Department of Computer Science</orgName>
								<orgName type="institution">University of Southern California</orgName>
								<address>
									<postCode>90089</postCode>
									<settlement>Los Angeles</settlement>
									<region>CA</region>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Diverse Sequential Subset Selection for Supervised Video Summarization</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">439FA3080BE367302BD2C47A51902499</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.3" ident="GROBID" when="2023-07-28T10:59+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Video summarization is a challenging problem with great application potential. Whereas prior approaches, largely unsupervised in nature, focus on sampling useful frames and assembling them as summaries, we consider video summarization as a supervised subset selection problem. Our idea is to teach the system to learn from human-created summaries how to select informative and diverse subsets, so as to best meet evaluation metrics derived from human-perceived quality. To this end, we propose the sequential determinantal point process (seqDPP), a probabilistic model for diverse sequential subset selection. Our novel seqDPP heeds the inherent sequential structures in video data, thus overcoming the deficiency of the standard DPP, which treats video frames as randomly permutable items. Meanwhile, seqDPP retains the power of modeling diverse subsets, essential for summarization. Our extensive results of summarizing videos from 3 datasets demonstrate the superior performance of our method, compared to not only existing unsupervised methods but also naive applications of the standard DPP model.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>It is an impressive yet alarming fact that there is far more video being captured-by consumers, scientists, defense analysts, and others-than can ever be watched or browsed efficiently. For example, 144,000 hours of video are uploaded to YouTube daily; lifeloggers with wearable cameras amass Gigabytes of video daily; 422,000 CCTV cameras perched around London survey happenings in the city 24/7. With this explosion of video data comes an ever-pressing need to develop automatic video summarization algorithms. By taking a long video as input and producing a short video (or keyframe sequence) as output, video summarization has great potential to reign in raw video and make it substantially more browseable and searchable.</p><p>Video summarization methods often pose the problem in terms of subset selection: among all the frames (subshots) in the video, which key frames (subshots) should be kept in the output summary? There is a rich literature in computer vision and multimedia developing a variety of ways to answer this question <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b1">2,</ref><ref type="bibr" target="#b2">3,</ref><ref type="bibr" target="#b3">4,</ref><ref type="bibr" target="#b4">5,</ref><ref type="bibr" target="#b5">6,</ref><ref type="bibr" target="#b6">7,</ref><ref type="bibr" target="#b7">8,</ref><ref type="bibr" target="#b8">9,</ref><ref type="bibr" target="#b9">10]</ref>. Existing techniques explore a plethora of properties that a good summary should capture, designing criteria that the algorithm should prioritize when deciding which subset of frames (or subshots) to select. These include representativeness (the frames should depict the main contents of the videos) <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b1">2,</ref><ref type="bibr" target="#b9">10]</ref>, diversity (they should not be redundant) <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b10">11]</ref>, interestingness (they should have salient motion/appearance <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b2">3,</ref><ref type="bibr" target="#b5">6]</ref> or trackable objects <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b11">12,</ref><ref type="bibr" target="#b6">7]</ref>), or importance (they should contain important objects that drive the visual narrative) <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b8">9]</ref>. Despite valuable progress in developing the desirable properties of a summary, prior approaches are impeded by their unsupervised nature. Typically the selection algorithm favors extracting content that satisfies criteria like the above (diversity, importance, etc.), and performs some sort of frame clustering to discover events. Unfortunately, this often requires some hand-crafting to combine the criteria effectively. After all, the success of a summary ultimately depends on human perception. Furthermore, due to the large number of possible subsets that could be selected, it is difficult to directly optimize the criteria jointly on the selected frames as a subset; instead, sampling methods that identify independently useful frames (or subshots) are common.</p><p>To address these limitations, we propose to consider video summarization as a supervised subset selection problem. The main idea is to use examples of human-created summaries-together with their original source videos-to teach the system how to select informative subsets. In doing so, we can escape the hand-crafting often necessary for summarization, and instead directly optimize the (learned) factors that best meet evaluation metrics derived from human-perceived quality. Furthermore, rather than independently select "high scoring" frames, we aim to capture the interlocked dependencies between a given frame and all others that could be chosen.</p><p>To this end, we propose the sequential determinantal point process (seqDPP), a new probabilistic model for sequential and diverse subset selection. The determinantal point process (DPP) has recently emerged as a powerful method for selecting a diverse subset from a "ground set" of items <ref type="bibr" target="#b12">[13]</ref>, with applications including document summarization <ref type="bibr" target="#b13">[14]</ref> and information retrieval <ref type="bibr" target="#b14">[15]</ref>. However, existing DPP techniques have a fatal modeling flaw if applied to video (or documents) for summarization: they fail to capture their inherent sequential nature. That is, a standard DPP treats the inputs as bags of randomly permutable items agnostic to any temporal structure. Our novel seqDPP overcomes this deficiency, making it possible to faithfully represent the temporal dependencies in video data. At the same time, it lets us pose summarization as a supervised learning problem.</p><p>While learning how to summarize from examples sounds appealing, why should it be possibleparticularly if the input videos are expected to vary substantially in their subject matter? <ref type="foot" target="#foot_0">1</ref> Unlike more familiar supervised visual recognition tasks, where test data can be reasonably expected to look like the training instances, a supervised approach to video summarization must be able to learn generic properties that transcend the specific content of the training set. For example, the learner can recover a "meta-cue" for representativeness, if the input features record profiles of the similarity between a frame and its increasingly distant neighbor frames. Similarly, category-independent cues about an object's placement in the frame, the camera person's active manipulation of viewpoint/zoom, etc., could play a role. In any such case, we can expect the learning algorithm to focus on those meta-cues that are shared by the human-selected frames in the training set, even though the subject matter of the videos may differ.</p><p>In short, our main contributions are: a novel learning model (seqDPP) for selecting diverse subsets from a sequence, its application to video summarization (the model is applicable to other sequential data as well), an extensive empirical study with three benchmark datasets, and a successful firststep/proof-of-concept towards using human-created video summaries for learning to select subsets.</p><p>The rest of the paper is organized as follows. In section 2, we review DPP and its application to document summarization. In section 3, we describe our seqDPP method, followed by a discussion of related work in section 4. We report results in section 5, then conclude in section 6.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Determinantal point process (DPP)</head><p>The DPP was first used to characterize the Pauli exclusion principle, which states that two identical particles cannot occupy the same quantum state simultaneously <ref type="bibr" target="#b15">[16]</ref>. The notion of exclusion has made DPP an appealing tool for modeling diversity in application such as document summarization <ref type="bibr" target="#b13">[14,</ref><ref type="bibr" target="#b12">13]</ref>, or image search and ranking <ref type="bibr" target="#b16">[17]</ref>. In what follows, we give a brief account on DPP and how to apply it to document summarization where the goal is to generate a summary by selecting several sentences from a long document <ref type="bibr" target="#b17">[18,</ref><ref type="bibr" target="#b18">19]</ref>. The selected sentences should be not only diverse (i.e., different) to reduce the redundancy in the summary, but also representative of the document.</p><p>Background Let Y = {1, 2, • • • , N} be a ground set of N items (eg., sentences). In its simplest form, a DPP defines a discrete probability distribution over all the 2 N subsets of Y. Let Y denote the random variable of selecting a subset. Y is then distributed according to</p><formula xml:id="formula_0">P (Y = y) = det(L y ) det(L + I)<label>(1)</label></formula><p>for y ⊆ Y. The kernel L ∈ S N×N + is the DPP's parameter and is constrained to be positive semidefinite. I is the identity matrix. L y is the principal minor (sub-matrix) with rows and columns selected according to the indices in y. The determinant function det(•) gives rise to the interesting property of pairwise repulsion. To see that, consider selecting a subset of two items i and j. We have</p><formula xml:id="formula_1">P (Y = {i, j}) ∝ L ii L jj -L 2 ij .<label>(2)</label></formula><p>If the items i and j are the same, then P (Y = {i, j}) = 0 (because</p><formula xml:id="formula_2">L ij = L ii = L jj ).</formula><p>Namely, identical items should not appear together in the same set. A more general case also holds: if i and j are similar to each other, then the probability of observing i and j in a subset together is going to be less than that of observing either one of them alone (see the excellent tutorial <ref type="bibr" target="#b12">[13]</ref> for details).</p><p>The most diverse subset of Y is thus the one that attains the highest probability</p><formula xml:id="formula_3">y * = arg max y P (Y = y) = arg max y det(L y ),<label>(3)</label></formula><p>where y * results from MAP inference. This is a NP-hard combinatorial optimization problem. However, there are several approaches to obtaining approximate solutions <ref type="bibr" target="#b12">[13,</ref><ref type="bibr" target="#b19">20]</ref>.</p><p>Learning DPPs for document summarization Suppose we model selecting a subset of sentences as a DPP over all sentences in a document. We are given a set of training samples in the form of documents (i.e., ground sets) and the ground-truth summaries. How can we discover the underlying parameter L so as to use it for generating summaries for new documents? Note that the new documents will likely have sentences that have not been seen before in the training samples. Thus, the kernel matrix L needs to be reparameterized in order to generalize to unseen documents. <ref type="bibr" target="#b13">[14]</ref> proposed a special reparameterization called quality/diversity decomposition:</p><formula xml:id="formula_4">L ij = q i φ T i φ j q j , q i = exp 1 2 θ T x i ,<label>(4)</label></formula><p>where φ i is the normalized TF-IDF vector of the sentence i so that φ T i φ j computes the cosine angle between two sentences. The "quality" feature vector x i encodes the contextual information about i and its representativeness of other items. In document summarization, x i are the sentence lengths, positions of the sentences in the texts, and other meta cues. The parameter θ is then optimized with maximum likelihood estimation (MLE) such that the target subsets have the highest probabilities</p><formula xml:id="formula_5">θ * = arg max θ n log P (Y = y * n ; L n (θ)),<label>(5)</label></formula><p>where L n is the L matrix formulated using sentences in the n-th ground set, and y * n is the corresponding ground-truth summary.</p><p>Despite its success in document summarization <ref type="bibr" target="#b13">[14]</ref>, a direct application of DPP to video summarization is problematic. The DPP model is agnostic about the order of the items. For video (and to a large degree, text data), it does not respect the inherent sequential structures. The second limitation is that the quality-diversity decomposition, while cleverly leading to a convex optimization, limits the power of modeling complex dependencies among items. Specifically, only the quality factor q i is optimized on the training data. We develop new approaches to overcoming those limitations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Approach</head><p>In what follows, we describe our approach for video summarization. Our approach contains three components: <ref type="bibr" target="#b0">(1)</ref> a preparatory yet crucial step that generates ground-truth summaries from multiple human-created ones (section 3.1); ( <ref type="formula" target="#formula_1">2</ref>) a new probabilistic model-the sequential determinantal point process (seqDPP)-that models the process of sequentially selecting diverse subsets (section 3.2);</p><p>(3) a novel way of re-parameterizing seqDPP that enables learning more flexible and powerful representations for subset selection from standard visual and contextual features (section 3.3).</p><p>Figure <ref type="figure">1</ref>: The agreement among human-created summaries is high, as is the agreement between the oracle summary generated by our algorithm (cf. section 3.1) and human annotations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Generating ground-truth summaries</head><p>The first challenge we need to address is what to provide to our learning algorithm as ground-truth summaries. In many video datasets, each video is annotated (manually summarized) by multiple human users. While the users were often well instructed on the annotation task, discrepancies are expected due to many uncontrollable individual factors such as whether the person was attentive, idiosyncratic viewing preferences, etc. There are some studies on how to evaluate automatically generated summaries in the presence of multiple human-created annotations <ref type="bibr" target="#b20">[21,</ref><ref type="bibr" target="#b21">22,</ref><ref type="bibr" target="#b22">23]</ref>. However, for learning, our goal is to generate one single ground-truth or "oracle" summary per video.</p><p>Our main idea is to synthesize the oracle summary that maximally agrees with all annotators. Our hypothesis is that despite the discrepancies, those summaries nonetheless share the common traits of reflecting the subject matters in the video. These commonalities, to be discovered by our synthesis algorithm, will provide strong enough signals for our learning algorithm to be successful.</p><p>To begin with, we first describe a few metrics in quantifying the agreement in the simplest setting where there are only two summaries. These metrics will also be used later in our empirical studies to evaluate various summarization methods. Using those metrics, we then analyze the consistency of human-created summaries in two video datasets to validate our hypothesis. Finally, we present our algorithm for synthesizing one single oracle summary per video.</p><p>Evaluation metrics Given two video summaries A and B, we measure how much they are in agreement by first matching their frames, as they might be of different lengths. Following <ref type="bibr" target="#b23">[24]</ref>, we compute the pairwise distances between all frames across the two summaries. Two frames are then "matched" if their visual difference is below some threshold; a frame is constrained to appear in the matched pairs at most once. After the matching, we compute the following metrics (commonly known as Precision, Recall and F-score):</p><formula xml:id="formula_6">P AB = #matched frames #frames in A , R AB = #matched frames #frames in B , F AB = P AB • R AB 0.5(P AB + R AB )</formula><p>.</p><p>All of them lie between 0 and 1, and higher values indicate better agreement between A and B. Note that these metrics are not symmetric -if we swap A and B, the results will be different.</p><p>Our idea of examining the consistency among all summaries is to treat each summary in turn as if it were the gold-standard (and assign it as B) while treating the other summaries as A's. We report our analysis of existing video datasets next.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Consistency in existing video databases</head><p>We analyze video summaries in two video datasets: 50 videos from the Open Video Project (OVP) <ref type="bibr" target="#b24">[25]</ref> and another 50 videos from Youtube <ref type="bibr" target="#b23">[24]</ref>. Details about these two video datasets are in section 5. We briefly point out that the two datasets have very different subject matters and composition styles. Each of the 100 videos has 5 annotated summaries.</p><p>For each video, we compute the pairwise evaluation metrics in precision, recall, and F-score by forming total 20 pairs of summaries from two different annotators. We then average them per video. We plot how these averaged metrics distribute in Fig. <ref type="figure">1</ref>. The plots show the number of videos (out of 100) whose averaged metrics exceed certain thresholds, marked on the horizontal axes. For example, more than 80% videos have an averaged F-score greater than 0.6, and 60% more than 0.7. Note that there are many videos (≈20) with averaged F-scores greater than 0.8, indicating that on average, human-created summaries have a high degree of agreement. Note that the mean values of the averaged metrics per video are also high.</p><p>Greedy algorithm for synthesizing an oracle summary Encouraged by our findings, we develop a greedy algorithm for synthesizing one oracle summary per video, from multiple human-created ones. This algorithm is adapted from a similar one for document summarization <ref type="bibr" target="#b13">[14]</ref>. Specifically, for each video, we initialize the oracle summary with the empty set y * = ∅. Iteratively, we then add to y * one frame i at a time from the video sequence</p><formula xml:id="formula_7">y * ← y * ∪ arg max i u F y * ∪i,yu .<label>(6)</label></formula><p>In words, the frame i is selected to maximally increase the F-score between the new oracle summary and the human-created summaries y u . To avoid adding all frames in the video sequence, we stop the greedy process as soon as there is no frame that can increase the F-score.</p><p>We measure the quality of the synthesized oracle summaries by computing their mean agreement with the human annotations. The results are shown in Fig. <ref type="figure">1</ref> too. The quality is high: more than 90% of the oracle summaries agree well with other summaries, with an F-score greater than 0.6. In what follows, we will treat the oracle summaries as ground-truth to inform our learning algorithms.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Sequential determinantal point processes (seqDPP)</head><p>The determinantal point process, as described in section 2, is a powerful tool for modeling diverse subset selection. However, video frames are more than items in a set. In particular, in DPP, the ground set is a bag -items are randomly permutable such that the most diverse subset remains unchanged. Translating this into video summarization, this modeling property essentially suggests that we could randomly shuffle video frames and expect to get the same summary!</p><p>To address this serious deficiency, we propose sequential DPP, a new probabilistic model to introduce strong dependency structures between items. As a motivating example, consider a video portraying the sequence of someone leaving home for school, coming back to home for lunch, leaving for market and coming back for dinner. If only visual appearance cues are available, a vanilla DPP model will likely select only one frame from the home scene and repel other frames occurring at the home. Our model, on the other hand, will recognize that the temporal span implies those frames are still diverse despite their visual similarity. Thus, our modeling intuition is that diversity should be a weaker prior for temporally distant frames but ought to act more strongly for closely neighboring frames. We now explain how our seqDPP method implements this intuition.</p><p>Model definition Given a ground set (a long video sequence) Y, we partition it into T disjoint yet consecutive short segments T t=1 Y t = Y. At time t, we introduce a subset selection variable Y t . We impose a DPP over two neighboring segments where the ground set is U t = Y t ∪ y t-1 , ie., the union between the video segments and the selected subset in the immediate past. Let Ω t denote the L-matrix defined over the ground set U t . The conditional distribution of Y t is thus given by,</p><formula xml:id="formula_8">P (Y t = y t |Y t-1 = y t-1 ) = det Ω yt-1∪yt det(Ω t + I t ) . (<label>7</label></formula><formula xml:id="formula_9">)</formula><p>As before, the subscript y t-1 ∪ y t selects the corresponding rows and columns from Ω t . I t is a diagonal matrix, the same size as U t . However, the elements corresponding to y t-1 are zeros and the elements corresponding to Y t are 1 (see <ref type="bibr" target="#b12">[13]</ref> for details). Readers who are familiar with DPP might identify the conditional distribution is also a DPP, restricted to the ground set Y t .</p><p>The conditional probability is defined in such a way that at time t, the subset selected should be diverse among Y t as well as be diverse from previously selected y t-1 . However, beyond those two priors, the subset is not constrained by subsets selected in the distant past. Fig. <ref type="figure" target="#fig_0">2</ref> illustrates the idea in graphical model notation. In particular, the joint distribution of all subsets is factorized</p><formula xml:id="formula_10">P (Y 1 = y 1 , Y 2 = y 2 , • • • , Y T = y T ) = P (Y 1 = y 1 ) t=2 P (Y t = y t |Y t-1 = y t-1 ). (<label>8</label></formula><formula xml:id="formula_11">)</formula><p>Inference and learning The MAP inference for the seqDPP model eq. ( <ref type="formula" target="#formula_10">8</ref>) is as hard as the standard DPP model. Thus, we propose to use the following online inference, analogous to Bayesian belief updates (for Kalman filtering): Note that, at each step, the ground set could be quite small; thus an exhaustive search for the most diverse subset is plausible. The parameter learning is similar to the standard DPP model. We describe the details in the supplementary material.</p><formula xml:id="formula_12">y * 1 = arg max y∈Y1 P (Y 1 = y) y * 2 = arg max y∈Y2 P (Y 2 = y|Y 1 = y * 1 ) • • • y * t = arg max y∈Yt P (Y t = y|Y t-1 = y * t-1 ) • • • • • • Y1 Y2 Y3 • • • Yt • • • YT Y1 Y2 Y3 Yt YT</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Learning representations for diverse subset selection</head><p>As described previously, the kernel L of DPP hinges on the reparameterization with features of the items that can generalize across different ground sets. The quality-diversity decomposition in eq. ( <ref type="formula" target="#formula_4">4</ref>), while elegantly leading to convex optimization, is severely limited in its power in modeling complex items and dependencies among them. In particular, learning the subset selection rests solely on learning the quality factor, as the diversity component remains handcrafted and fixed.</p><p>We overcome this deficiency with more flexible and powerful representations. Concretely, let f i stand for the feature representation for item (frame) i, including both low-level visual cues and meta-cues such as contextual information. We reparameterize the L matrix with f i in two ways.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Linear embeddings</head><p>The simplest way is to linearly transform the original features</p><formula xml:id="formula_13">L ij = f T i W T W f j ,<label>(9)</label></formula><p>where W is the transformation matrix.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Nonlinear hidden representation</head><p>We use a one-hidden-layer neural network to infer a hidden representation for f i</p><formula xml:id="formula_14">L ij = z T i W T W z j where z i = tanh(U f i ),<label>(10)</label></formula><p>where tanh(•) stands for the hyperbolic transfer function.</p><p>To learn the parameters W or U and W , we use maximum likelihood estimation (cf. eq. ( <ref type="formula" target="#formula_5">5</ref>)), with gradient-descent to optimize. Details are given in the supplementary material.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Related work</head><p>Space does not permit a thorough survey of video summarization methods. Broadly speaking, existing approaches develop a variety of selection criteria to prioritize frames for the output summary, often combined with temporal segmentation. Prior work often aims to retain diverse and representative frames <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b0">1,</ref><ref type="bibr" target="#b9">10,</ref><ref type="bibr" target="#b3">4,</ref><ref type="bibr" target="#b10">11]</ref>, and/or defines novel metrics for object and event saliency <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b1">2,</ref><ref type="bibr" target="#b5">6,</ref><ref type="bibr" target="#b7">8]</ref>.</p><p>When the camera is known to be stationary, background subtraction and object tracking are valuable cues (e.g., <ref type="bibr" target="#b4">[5]</ref>). Recent developments tackle summarization for dynamic cameras that are worn or handheld <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b7">8,</ref><ref type="bibr" target="#b8">9]</ref> or design online algorithms to process streaming data <ref type="bibr" target="#b6">[7]</ref>.</p><p>Whereas existing methods are largely unsupervised, our idea to explicitly learn subset selection from human-given summaries is novel. Some prior work includes supervised learning components that are applied during selection (e.g., to generate learned region saliency metrics <ref type="bibr" target="#b7">[8]</ref> or train classifiers for canonical viewpoints <ref type="bibr" target="#b9">[10]</ref>), but they do not train/learn the subset selection procedure itself. Our idea is also distinct from "interactive" methods, which assume a human is in the loop to give supervision/feedback on each individual test video <ref type="bibr" target="#b25">[26,</ref><ref type="bibr" target="#b26">27,</ref><ref type="bibr" target="#b11">12]</ref>.</p><p>Our focus on the determinantal point process as the building block is largely inspired by its appealing property in modeling diversity in subset selection, as well as its success in search and ranking <ref type="bibr" target="#b16">[17]</ref>, document summarization <ref type="bibr" target="#b13">[14]</ref>, news headline displaying <ref type="bibr" target="#b27">[28]</ref>, and pose estimation <ref type="bibr" target="#b28">[29]</ref>. Applying DPP to video summarization, however, is novel to the best of our knowledge.</p><p>Our seqDPP is closest in spirit to the recently proposed Markov DPP <ref type="bibr" target="#b27">[28]</ref>. While both models enjoy the Markov property by defining conditional probabilities depending only on the immediate past, </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Experiments</head><p>We validate our approach of sequential determinantal point processes (seqDPP) for video summarization on several datasets, and obtain superior performance to competing methods.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Setup</head><p>Data We benchmark various methods on 3 video datasets: the Open Video Project (OVP), the Youtube dataset <ref type="bibr" target="#b23">[24]</ref>, and the Kodak consumer video dataset <ref type="bibr" target="#b31">[32]</ref>. They have 50, 39<ref type="foot" target="#foot_1">2</ref> , and 18 videos, respectively. The first two have 5 human-created summaries per video and the last has one humancreated summary per video. Thus, for the first two datasets, we follow the algorithm described in section 3.1 to create an oracle summary per video. We follow the same procedure as in <ref type="bibr" target="#b23">[24]</ref> to preprocess the video frames. We uniformly sample one frame per second and then apply two stages of pruning to remove uninformative frames. Details are in the supplementary material.</p><p>Features Each frame is encoded with an 2-normalized 8192-dimensional Fisher vector φ i <ref type="bibr" target="#b32">[33]</ref>, computed from SIFT features <ref type="bibr" target="#b33">[34]</ref>. The Fisher vector represents well the visual appearance of the video frame, and is hence used to compute the pairwise correlations of the frames in the qualitydiversity decomposition (cf. eq. ( <ref type="formula" target="#formula_4">4</ref>)). We derive the quality features x i by measuring the representativeness of the frame. Specifically, we place a contextual window centered around the frame of interest, and then compute its mean correlation (using the SIFT Fisher vector) to the other frames in the window. By varying the size of the windows from 5 to 15, we obtain 12-dimensional contextual features. We also add features computed from the frame saliency map <ref type="bibr" target="#b34">[35]</ref>. To apply our method for learning representations (cf. section 3.3), however, we do not make a distinction between the two types, and instead compose a feature vector f i by concatenating x i and φ i . The dimension of our linear transformed features W f i is 10, 40 and 100 for OVP, Youtube, and Kodak, respectively. For the neural network, we use 50 hidden units and 50 output units.</p><p>Other details For each dataset, we randomly choose 80% of the videos for training and use the remaining 20% for testing. We run 100 rounds of experiments and report the average performance, which is evaluated by the aforementioned F-score, Precision, and Recall (cf. section 3.1). For evaluation, we follow the standard procedure: for each video, we treat each human-created summary as golden-standard and assess the quality of the summary output by our algorithm. We then average over all human annotators to obtain the evaluation metrics for that video.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Results</head><p>We contrast our approach to several state-of-the-art methods for video summarization-which include several leading unsupervised methods-as well as the vanilla DPP model that has been successfully used for document summarization but does not model sequential structures. We compare the methods in greater detail on the OVP dataset. Table <ref type="table" target="#tab_0">1</ref> shows the results. Figure <ref type="figure">3</ref>: Exemplar video summaries by our seqDPP LINEAR vs. VSUMM summary <ref type="bibr" target="#b23">[24]</ref>.</p><p>Unsupervised or supervised? The four unsupervised methods are DT <ref type="bibr" target="#b29">[30]</ref>, STIMO <ref type="bibr" target="#b30">[31]</ref>, VSUMM 1 <ref type="bibr" target="#b23">[24]</ref>, and VSUMM 2 with a postprocessing step to VSUMM 1 to improve the precision of the results. We implement VSUMM ourselves using features described in the orignal paper and tune its parameters to have the best test performance. All 4 methods use clustering-like procedures to identify key frames as video summaries. Results of DT and STIMO are taken from their original papers. They generally underperform VSUMM.</p><p>What is interesting is that the vanilla DPP does not outperform the unsupervised methods, despite its success in other tasks. On the other end, our supervised method seqDPP, when coupled with the linear or neural network representation learning, performs significantly better than all other methods. We believe the improvement can be attributed to two factors working in concert: (1) modeling sequential structures of the video data, and (2) more flexible and powerful representation learning. This is evidenced by the rather poor performance of seqDPP with the quality/diversity (Q/D) decomposition, where the representation of the items is severely limited such that modeling temporal structures alone is simply insufficient.</p><p>Linear or nonlinear? Table <ref type="table" target="#tab_1">2</ref> concentrates on comparing the effectiveness of these two types of representation learning. The performances of VSUMM are provided for reference only. We see that learning representations with neural networks generally outperforms the linear representations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Qualitative results</head><p>We present exemplar video summaries by different methods in Fig. <ref type="figure">3</ref>. The challenging Youtube video illustrates the advantage of sequential diverse subset selection. The visual variance in the beginning of the video is far greater (due to close-shots of people) than that at the end (zooming out). Thus the clustering-based VSUMM method is prone to select key frames from the first half of the video, collapsing the latter part. In contrast, our seqDPP copes with time-varying diversity very well. The Kodak video demonstrates again our method's ability in attaining high recall when users only make diverse selections locally but not globally. VSUMM fails to acknowledge temporally distant frames can be diverse despite their visual similarities.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Conclusion</head><p>Our novel learning model seqDPP is a successful first step towards using human-created summaries for learning to select subsets for the challenging video summarization problem. We just scratched the surface of this fruit-bearing direction. We plan to investigate how to learn more powerful representations from low-level visual cues.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: Our sequential DPP for modeling sequential video data, drawn as a Bayesian network</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 :</head><label>1</label><figDesc>Performance of various video summarization methods on OVP. Ours and its variants perform the best.</figDesc><table><row><cell></cell><cell cols="3">Unsupervised methods</cell><cell></cell><cell cols="2">Supervised subset selection</cell><cell></cell></row><row><cell>DT</cell><cell>STIMO</cell><cell>VSUMM1</cell><cell>VSUMM2</cell><cell>DPP + Q/D</cell><cell></cell><cell>Ours (seqDPP+)</cell><cell></cell></row><row><cell>[30]</cell><cell>[31]</cell><cell>[24]</cell><cell>[24]</cell><cell>[14]</cell><cell>Q/D</cell><cell>LINEAR</cell><cell>N.NETS</cell></row><row><cell>F 57.6</cell><cell>63.4</cell><cell>70.3</cell><cell>68.2</cell><cell>70.8±0.3</cell><cell cols="3">68.5±0.3 75.5±0.4 77.7±0.4</cell></row><row><cell>P 67.7</cell><cell>60.3</cell><cell>70.6</cell><cell>73.1</cell><cell>71.5±0.4</cell><cell cols="3">66.9±0.4 77.5±0.5 75.0±0.5</cell></row><row><cell>R 53.2</cell><cell>72.2</cell><cell>75.8</cell><cell>69.1</cell><cell>74.5±0.3</cell><cell cols="3">75.8±0.5 78.4±0.5 87.2±0.3</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 :</head><label>2</label><figDesc>Performance of our method with different representation learning Markov DPP's ground set is still the whole video sequence, whereas seqDPP can select diverse sets from the present time. Thus, one potential drawback of applying Markov DPP is to select video frames out of temporal order, thus failing to model the sequential nature of the data faithfully.</figDesc><table><row><cell></cell><cell></cell><cell cols="2">VSUMM2 [24]</cell><cell></cell><cell>seqDPP+LINEAR</cell><cell></cell><cell></cell><cell>seqDPP+N. NETS</cell></row><row><cell></cell><cell>F</cell><cell>P</cell><cell>R</cell><cell>F</cell><cell>P</cell><cell>R</cell><cell>F</cell><cell>P</cell><cell>R</cell></row><row><cell cols="10">Youtube 55.7 59.7 58.7 57.8±0.5 54.2±0.7 69.8±0.5 60.3±0.5 59.4±0.6 64.9±0.5</cell></row><row><cell>Kodak</cell><cell cols="9">68.9 75.7 80.6 75.3±0.7 77.8±1.0 80.4±0.9 78.9±0.5 81.9±0.8 81.1±0.9</cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0"><p>After all, not all videos on YouTube are about cats.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_1"><p>In total there are 50 Youtube videos. We keep</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="39" xml:id="foot_2"><p>of them after excluding the cartoon videos.</p></note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Acknowledgments B. G., W. C. and F. S. are partially supported by DARPA D11-AP00278, NSF IIS-1065243, and ARO #W911NF-12-1-0241. K. G. is supported by ONR YIP Award N00014-12-1-0754 and gifts from Intel and Google. B. G. and W. C. also acknowledge supports from USC Viterbi Doctoral Fellowship and USC Annenberg Graduate Fellowship. We are grateful to Jiebo Luo for providing the Kodak dataset <ref type="bibr" target="#b31">[32]</ref>.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Event driven summarization for web videos</title>
		<author>
			<persName><forename type="first">R</forename><surname>Hong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Ngo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Chua</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In ACM SIGMM Workshop on Social Media</title>
		<imprint>
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Automatic video summarization by graph modeling</title>
		<author>
			<persName><forename type="first">C.-W</forename><surname>Ngo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y.-F</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H.-J</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2003">2003</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">A user attention model for video summarization</title>
		<author>
			<persName><forename type="first">Yu-Fei</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lie</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hong-Jiang</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mingjing</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACM MM</title>
		<imprint>
			<date type="published" when="2002">2002</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Optimization algorithms for the selection of key frame sequences of variable length</title>
		<author>
			<persName><forename type="first">Tiecheng</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">John</forename><forename type="middle">R</forename><surname>Kender</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2002">2002</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Webcam synopsis: Peeking around the world</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Pritch</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Rav-Acha</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Gutman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Peleg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Space-time video montage</title>
		<author>
			<persName><forename type="first">H</forename><surname>Kang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Matsushita</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Online content-aware video condensation</title>
		<author>
			<persName><forename type="first">Shikun</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhen</forename><surname>Lei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dong</forename><surname>Yi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stan</forename><forename type="middle">Z</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Discovering important people and objects for egocentric video summarization</title>
		<author>
			<persName><forename type="first">Jae</forename><surname>Yong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Joydeep</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kristen</forename><surname>Ghosh</surname></persName>
		</author>
		<author>
			<persName><surname>Grauman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Story-driven summarization for egocentric video</title>
		<author>
			<persName><forename type="first">Zheng</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kristen</forename><surname>Grauman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Large-scale video summarization using web-image priors</title>
		<author>
			<persName><forename type="first">A</forename><surname>Khosla</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Hamid</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C-J.</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Sundaresan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">An integrated system for contentbased video retrieval and browsing</title>
		<author>
			<persName><forename type="first">Hong-Jiang</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jianhua</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Di</forename><surname>Zhong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stephen</forename><forename type="middle">W</forename><surname>Smoliar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pattern Recognition</title>
		<imprint>
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="643" to="658" />
			<date type="published" when="1997">1997</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">A hierarchical visual model for video object summarization</title>
		<author>
			<persName><forename type="first">D</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gang</forename><surname>Hua</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tsuhan</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">PAMI</title>
		<imprint>
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page" from="2178" to="2190" />
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Determinantal point processes for machine learning</title>
		<author>
			<persName><forename type="first">Alex</forename><surname>Kulesza</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ben</forename><surname>Taskar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Foundations and Trends R in Machine Learning</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="issue">2-3</biblScope>
			<biblScope unit="page" from="123" to="286" />
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Learning determinantal point processes</title>
		<author>
			<persName><forename type="first">Alex</forename><surname>Kulesza</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ben</forename><surname>Taskar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">UAI</title>
		<imprint>
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Discovering diverse and salient threads in document collections</title>
		<author>
			<persName><forename type="first">Jennifer</forename><surname>Gillenwater</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alex</forename><surname>Kulesza</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ben</forename><surname>Taskar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP/CNLL</title>
		<imprint>
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">The coincidence approach to stochastic point processes</title>
		<author>
			<persName><forename type="first">Odile</forename><surname>Macchi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in Applied Probability</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="83" to="122" />
			<date type="published" when="1975">1975</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">k-dpps: Fixed-size determinantal point processes</title>
		<author>
			<persName><forename type="first">Alex</forename><surname>Kulesza</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ben</forename><surname>Taskar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Overview of duc 2005</title>
		<author>
			<persName><forename type="first">Trang</forename><surname>Hoa</surname></persName>
		</author>
		<author>
			<persName><surname>Dang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Document Understanding Conf</title>
		<imprint>
			<date type="published" when="2005">2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Multi-document summarization via budgeted maximization of submodular functions</title>
		<author>
			<persName><forename type="first">Hui</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jeff</forename><surname>Bilmes</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NAACL/HLT</title>
		<imprint>
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Near-optimal map inference for determinantal point processes</title>
		<author>
			<persName><forename type="first">Jennifer</forename><surname>Gillenwater</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alex</forename><surname>Kulesza</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ben</forename><surname>Taskar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Automatic evaluation of video summaries</title>
		<author>
			<persName><forename type="first">Víctor</forename><surname>Valdés</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>José</surname></persName>
		</author>
		<author>
			<persName><surname>Martínez</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Trans. on multimedia computing, communications, and applications</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page">25</biblScope>
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Automatic evaluation method for rushes summary content</title>
		<author>
			<persName><forename type="first">Emilie</forename><surname>Dumont</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bernard</forename><surname>Mérialdo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICME</title>
		<imprint>
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Vert: automatic evaluation of video summaries</title>
		<author>
			<persName><forename type="first">Yingbo</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bernard</forename><surname>Merialdo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACM MM</title>
		<imprint>
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Vsumm: A mechanism designed to produce static video summaries and a novel evaluation method</title>
		<author>
			<persName><forename type="first">Sandra</forename><surname>Eliza Fontes De Avila</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ana</forename><surname>Paula Brandão</surname></persName>
		</author>
		<author>
			<persName><surname>Lopes</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pattern Recognition Letters</title>
		<imprint>
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="56" to="68" />
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<ptr target="http://www.open-video.org/" />
		<title level="m">Open video project</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Im(s)2: Interactive movie summarization system</title>
		<author>
			<persName><forename type="first">M</forename><surname>Ellouze</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Boujemaa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Alimi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J VCIR</title>
		<imprint>
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="283" to="294" />
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Schematic storyboarding for video visualization and editing</title>
		<author>
			<persName><forename type="first">Brian</forename><surname>Dan B Goldman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Steven</forename><forename type="middle">M</forename><surname>Curless</surname></persName>
		</author>
		<author>
			<persName><surname>Seitz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SIGGRAPH</title>
		<imprint>
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Markov determinantal point processes</title>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">H</forename><surname>Affandi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Kulesza</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">B</forename><surname>Fox</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">UAI</title>
		<imprint>
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Structured determinantal point processes</title>
		<author>
			<persName><forename type="first">A</forename><surname>Kulesza</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Taskar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Keyframe-based video summarization using delaunay clustering</title>
		<author>
			<persName><forename type="first">Padmavathi</forename><surname>Mundur</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yong</forename><surname>Rao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yelena</forename><surname>Yesha</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Int&apos;l J. on Digital Libraries</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="219" to="232" />
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Stimo: Still and moving video storyboard for the web scenario</title>
		<author>
			<persName><forename type="first">Marco</forename><surname>Furini</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Filippo</forename><surname>Geraci</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Manuela</forename><surname>Montangero</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Marco</forename><surname>Pellegrini</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Multimedia Tools and Applications</title>
		<imprint>
			<biblScope unit="volume">46</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="47" to="69" />
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Towards extracting semantically meaningful key frames from personal video clips: from humans to computers</title>
		<author>
			<persName><forename type="first">Jiebo</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christophe</forename><surname>Papin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kathleen</forename><surname>Costello</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. on Circuits and Systems for Video Technology</title>
		<imprint>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="289" to="301" />
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Fisher kernels on visual vocabularies for image categorization</title>
		<author>
			<persName><forename type="first">Florent</forename><surname>Perronnin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christopher</forename><surname>Dance</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Distinctive image features from scale-invariant keypoints</title>
		<author>
			<persName><surname>David G Lowe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IJCV</title>
		<imprint>
			<biblScope unit="volume">60</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="91" to="110" />
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Segmenting salient objects from images and videos</title>
		<author>
			<persName><forename type="first">Esa</forename><surname>Rahtu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Juho</forename><surname>Kannala</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mikko</forename><surname>Salo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Janne</forename><surname>Heikkil</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
