<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">TEXT2EVENT: Controllable Sequence-to-Structure Generation for End-to-end Event Extraction</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Yaojie</forename><surname>Lu</surname></persName>
							<email>yaojie2017@iscas.ac.cn</email>
							<affiliation key="aff0">
								<orgName type="laboratory">Chinese Information Processing Laboratory</orgName>
							</affiliation>
							<affiliation key="aff2">
								<orgName type="institution">University of Chinese Academy of Sciences</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Hongyu</forename><surname>Lin</surname></persName>
							<email>hongyu@iscas.ac.cn</email>
							<affiliation key="aff0">
								<orgName type="laboratory">Chinese Information Processing Laboratory</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Jin</forename><surname>Xu</surname></persName>
							<email>jinxxu@tencent.com</email>
							<affiliation key="aff3">
								<orgName type="department">Data Quality Team</orgName>
								<address>
									<addrLine>WeChat</addrLine>
									<region>Tencent Inc</region>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Xianpei</forename><surname>Han</surname></persName>
							<email>xianpei@iscas.ac.cn</email>
							<affiliation key="aff0">
								<orgName type="laboratory">Chinese Information Processing Laboratory</orgName>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="department">State Key Laboratory of Computer Science Institute of Software</orgName>
								<orgName type="institution">Chinese Academy of Sciences</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Jialong</forename><surname>Tang</surname></persName>
							<email>jialong2019@iscas.ac.cn</email>
							<affiliation key="aff0">
								<orgName type="laboratory">Chinese Information Processing Laboratory</orgName>
							</affiliation>
							<affiliation key="aff2">
								<orgName type="institution">University of Chinese Academy of Sciences</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Annan</forename><surname>Li</surname></persName>
							<affiliation key="aff0">
								<orgName type="laboratory">Chinese Information Processing Laboratory</orgName>
							</affiliation>
							<affiliation key="aff2">
								<orgName type="institution">University of Chinese Academy of Sciences</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Le</forename><surname>Sun</surname></persName>
							<email>sunle@iscas.ac.cn</email>
							<affiliation key="aff0">
								<orgName type="laboratory">Chinese Information Processing Laboratory</orgName>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="department">State Key Laboratory of Computer Science Institute of Software</orgName>
								<orgName type="institution">Chinese Academy of Sciences</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Meng</forename><surname>Liao</surname></persName>
							<affiliation key="aff3">
								<orgName type="department">Data Quality Team</orgName>
								<address>
									<addrLine>WeChat</addrLine>
									<region>Tencent Inc</region>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Shaoyi</forename><surname>Chen</surname></persName>
							<email>shaoyichen@tencent.com</email>
							<affiliation key="aff3">
								<orgName type="department">Data Quality Team</orgName>
								<address>
									<addrLine>WeChat</addrLine>
									<region>Tencent Inc</region>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">TEXT2EVENT: Controllable Sequence-to-Structure Generation for End-to-end Event Extraction</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.2" ident="GROBID" when="2022-12-25T14:07+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Event extraction is challenging due to the complex structure of event records and the semantic gap between text and event. Traditional methods usually extract event records by decomposing the complex structure prediction task into multiple subtasks. In this paper, we propose TEXT2EVENT, a sequence-tostructure generation paradigm that can directly extract events from the text in an end-to-end manner. Specifically, we design a sequenceto-structure network for unified event extraction, a constrained decoding algorithm for event knowledge injection during inference, and a curriculum learning algorithm for efficient model learning. Experimental results show that, by uniformly modeling all tasks in a single model and universally predicting different labels, our method can achieve competitive performance using only record-level annotations in both supervised learning and transfer learning settings.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Event extraction is an essential task for natural language understanding, aiming to transform the text into event records <ref type="bibr" target="#b7">(Doddington et al., 2004;</ref><ref type="bibr" target="#b0">Ahn, 2006)</ref>. For example, in Figure <ref type="figure" target="#fig_0">1</ref>, mapping "The man returned to Los Angeles from Mexico following his capture Tuesday by bounty hunters." into two event records {Type: Transport, Trigger: returned, Arg1 Role: Artifact, Arg1: The man, Arg2 Role: Destination, Arg2: Los Angeles, ... } and {Type: Arrest-Jail, Trigger: capture, Arg1 Role: Person, Arg1: The man, Arg2 Role: Agent, Arg2: bounty hunters, ... }.</p><p>Event extraction is challenging due to the complex structure of event records and the semantic gap between text and event. First, an event record contains event type, trigger, and arguments, which form a table-like structure. And different event types have different structures. For example, in Figure <ref type="figure" target="#fig_0">1</ref>, Transport and Arrest-Jail have entirely different structures. Second, an event can be expressed using very different utterances, such as diversified trigger words and heterogeneous syntactic structures. For example, both "the dismission of the man" and "the man departed his job" express the same event record {Type: End-Position, Arg1 Role: PERSON, Arg1: the man}.</p><p>Currently, most event extraction methods employ the decomposition strategy <ref type="bibr" target="#b5">(Chen et al., 2015;</ref><ref type="bibr" target="#b33">Nguyen and Nguyen, 2019;</ref><ref type="bibr" target="#b43">Wadden et al., 2019;</ref><ref type="bibr" target="#b54">Zhang et al., 2019b;</ref><ref type="bibr" target="#b9">Du and Cardie, 2020;</ref><ref type="bibr" target="#b18">Li et al., 2020;</ref><ref type="bibr">Paolini et al., 2021)</ref>, i.e., decomposing the prediction of complex event structures into multiple separated subtasks (mostly including entity recognition, trigger detection, argument classification), and then compose the components of different subtasks for predicting the whole event structure (e.g., pipeline modeling, joint modeling or joint inference). The main drawbacks of these decomposition-based methods are: (1) They need massive and fine-grained annotations for different subtasks, often resulting in the data inefficiency problem. For example, they need different finegrained annotations for Transport trigger detection, for Person entity recognition, for Transport.Artifact argument classification, etc. (2) It is very challenging to design the optimal composition architecture of different subtasks manually. For instance, the pipeline models often lead to error propagation. And the joint models need to heuristically predefine the information sharing and decision dependence between trigger detection, argument classification, and entity recognition, often resulting in suboptimal and inflexible architectures.</p><p>In this paper, we propose a sequence-tostructure generation paradigm for event extraction -TEXT2EVENT, which can directly extract events from the text in an end-to-end manner. Specifically, instead of decomposing event structure prediction into different subtasks and predicting labels, we uniformly model the whole event extraction process in a neural network-based sequence-tostructure architecture, and all triggers, arguments, and their labels are universally generated as natural language words. For example, we generate a subsequence "Attack fire" for trigger extraction, where both "Attack" and "fire" are treated as natural language words. Compared with previous methods, our method is more data-efficient: it can be learned using only coarse parallel text-record annotations, i.e., pairs of sentence, event records , rather than fine-grained token-level annotations. Besides, the uniform architecture makes it easy to model, learn and exploit the interactions between different underlying predictions, and the knowledge can be seamlessly shared and transferred between different components. Furthermore, we design two algorithms for effective sequence-to-structure event extraction. First, we propose a constrained decoding algorithm, which can guide the generation process using event schemas. In this way, the event knowledge can be injected and exploited during inference on-thefly. Second, we design a curriculum learning algorithm, which starts with current pre-trained lan-guage models (PLMs), then trains them on simple event substructure generation tasks such as trigger generation and independent argument generation, finally trains the model on the full event structure generation task.</p><p>We conducted experiments 1 on ACE and ERE datasets, and the results verified the effectiveness of TEXT2EVENT in both supervised learning and transfer learning settings. In summary, the contributions are as follows:</p><p>1. We propose a new paradigm for event extraction --sequence-to-structure generation, which can directly extract events from the text in an end-to-end manner. By uniformly modeling all tasks in a single model and universally predicting different labels, our method is effective, data-efficient, and easy to implement.</p><p>2. We design an effective sequence-to-structure architecture, which is enhanced with a constrained decoding algorithm for event knowledge injection during inference and a curriculum learning algorithm for efficient model learning.</p><p>3. Many information extraction tasks can be formulated as structure prediction tasks. Our sequence-to-structure method can motivate the learning of other information extraction models.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">TEXT2EVENT: End-to-end Event Extraction as Controllable Generation</head><p>Given the token sequence x = x 1 , ..., x |x| of the input text, TEXT2EVENT directly generate the event structures E = e 1 , ..., e |E| via an encoderdecoder architecture. For example, in Figure <ref type="figure" target="#fig_0">1</ref>, TEXT2EVENT take the raw text as input and output two event records including {Type: Transport, Trigger: returned, Arg1 Role: Artifact, Arg1: The man, ...} and {Type: Arrest-Jail, Trigger: capture, ..., Arg2 Role: Agent, Arg2: bounty hunters, ...}.</p><p>For end-to-end event extraction, TEXT2EVENT first encodes input text, then generates the linearized structure using the constrained decoding algorithm. In the following, we first introduce how to reformulate event extraction as structure generation via structure linearization, then describe the sequence-to-structure model and the constrained decoding algorithm.  For example, "Transport-returned" is a label-span relation edge, which head is "Transport" and tail is "returned".</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Event Extraction as Structure Generation</head><p>This section describes how to linearize event structure so that events can be generated in an end-toend manner. Specifically, the linearized event representations should: (1) be able to express multiple event records in a text as one expression;</p><p>(2) be easy to reversibly converted to event records in a deterministic way; (3) be similar to the token sequence of general text generation tasks so that text generation models can be leveraged and transferred easily.</p><p>Concretely, the process of converting from record format to linearized format is shown in Figure 2. We first convert event records (Figure <ref type="figure" target="#fig_1">2a</ref>) into a labeled tree (Figure <ref type="figure" target="#fig_1">2b</ref>) by: 1) first labeling the root of the tree with the type of event (Root -Transport, Root -Arrest-Jail), 2) then connecting multiple event argument role types with event types (Transport -Artifact, Transport -Origin, etc.), and 3) finally linking the text spans from the raw text to the corresponding nodes as leaves (Transportreturned, Transport -Origin -Mexico, Transport -Artifact -The man, etc.). Given the converted event tree, we linearize it into a token sequence (Figure <ref type="figure" target="#fig_1">2c</ref>) via depth-first traversal <ref type="bibr" target="#b42">(Vinyals et al., 2015)</ref>, where "(" and ")" are structure indicators used to represent the semantic structure of linear expressions. The traversal order of the same depth is the order in which the text spans appear in the text, e.g., first "return" then "capture" in Figure <ref type="figure" target="#fig_1">2b</ref>. Noted that each linearized form has a virtual root -Root. For a sentence that contains multiple event records, each event links to Root directly. For a sentence that doesn't express any event, its tree format will be linearized as "()".</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Sequence-to-Structure Network</head><p>Based on the above linearization strategy, TEXT2EVENT generates the event structure via a transformer-based encoder-decoder architecture <ref type="bibr" target="#b41">(Vaswani et al., 2017)</ref>. Given the token sequence x = x 1 , ..., x |x| as input, TEXT2EVENT outputs the linearized event representation y = y 1 , ..., y |y| . To this end, TEXT2EVENT first computes the hidden vector representation H = h 1 , ..., h |x| of the input via a multi-layer transformer encoder:</p><formula xml:id="formula_0">H = Encoder(x 1 , ..., x |x| )<label>(1)</label></formula><p>where each layer of Encoder(•) is a transformer block with the multi-head attention mechanism.</p><p>After the input token sequence is encoded, the decoder predicts the output structure token-bytoken with the sequential input tokens' hidden vectors. At the step i of generation, the self-attention decoder predicts the i-th token y i in the linearized form and decoder state h d i as:</p><formula xml:id="formula_1">y i , h d i = Decoder([H; h d 1 , ..., h d i−1 ], y i−1 ) (2)</formula><p>where each layer of Decoder(•) is a transformer block that contains self-attention with decoder state h d i and cross-attention with encoder state H.</p><p>The generated output structured sequence starts from the start token " bos " and ends with the end token " eos ". The conditional probability of the whole output sequence p(y|x) is progressively combined by the probability of each step p(y i |y &lt;i , x):</p><formula xml:id="formula_2">p(y|x) = |y| i p(y i |y &lt;i , x)<label>(3)</label></formula><p>where y &lt;i = y 1 ...y i−1 , and p(y i |y &lt;i , x) is the probability over the target vocabulary V normalized by softmax(•) . Because all tokens in linearized event representations are also natural language words, we adopt the pre-trained language model T5 <ref type="bibr" target="#b35">(Raffel et al., 2020)</ref> as our transformer-based encoder-decoder architecture. In this way, the general text generation knowledge can be directly reused.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Constrained Decoding</head><p>Given the hidden sequence H, the sequence-tostructure network needs to generate the linearized event representations token-by-token. One straightforward solution is to use a greedy decoding algorithm, which selects the token with the highest predicted probability p(y i |y &lt;i , x) at each decoding step i. Unfortunately, this greedy decoding algorithm cannot guarantee the generation of valid event structures. In other words, it could end up with invalid event types, mismatch of argumenttype, and incomplete structure. Furthermore, the greedy decoding algorithm ignores the useful event schema knowledge, which can be used to guide the decoding effectively. For example, we can constrain the model to only generate event type tokens in the type position.</p><p>To exploit the event schema knowledge, we propose to employ a trie-based constrained decoding algorithm <ref type="bibr" target="#b4">(Chen et al., 2020a;</ref><ref type="bibr" target="#b3">Cao et al., 2021)</ref> for event generation. During constrained decoding, the event schema knowledge is injected as the prompt of the decoder and ensures the generation of valid event structures.</p><p>Concretely, unlike the greedy decoding algorithm that selects the token from the whole target vocabulary V at each step, our trie-based constrained decoding method dynamically chooses and prunes a candidate vocabulary V based on the current generated state. A complete linearized form decoding process can be represented by executing a trie tree search, as shown in Figure <ref type="figure">3a</ref>. Specifically, each generation step of TEXT2EVENT has three kinds of candidate vocabulary V :</p><p>• Event schema: label names of event types T and argument roles R;</p><p>• Mention strings: event trigger word and argument mention S, which is the text span in the raw input;</p><p>• Structure indicator: "(" and ")" which are used to combine event schemas and mention strings.</p><p>The decoding starts from the root " bos " and ends at the terminator " eos ". At the generation Figure <ref type="figure">3</ref>: The prefix tree (trie) of the constrained decoding algorithm for controllable structure generation.</p><p>T and R indicate the label name of event type and argument role. S indicates the text span in the raw text, which is the event trigger or argument mention of the extracted event.</p><p>step i, the candidate vocabulary V is the children nodes of the last generated node. For instance, at the generation step with the generated string " bos (", the candidate vocabulary V is {"(", ")"} in Figure <ref type="figure">3a</ref>. When generating the event type name T , argument role name R and text span S, the decoding process can be considered as executing search on a subtree of the trie tree. For example, in Figure <ref type="figure">3b</ref>, the candidate vocabulary V for "( Transfer" is {"Ownership", "Money"}. Finally, the decoder's output will be transformed to event records and used as final extraction results.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Learning</head><p>This section describes how to learn the TEXT2EVENT neural network in an end-toend manner. Our method can be learned using only the coarse parallel text-record annotations, i.e., pairs of sentence, event records , with no need for fine-grained token-level annotation used in traditional methods. Given a training dataset D = {(x 1 , y 1 ), ...(x |D| , y |D| )} where each instance is a sentence, event records pair, the learning objective is the negative log-likelihood function as:</p><formula xml:id="formula_3">L = − (x,y)∈D log p(y|x, θ)<label>(4)</label></formula><p>where θ is model parameters.</p><p>Unfortunately, unlike general text-to-text generation models, the learning of sequence-to-structure generation models is more challenging: 1) There is an output gap between the event generation model and the text-to-text generation model. Compared with natural word sequences, the linearized event structure contains many non-semantic indicators such as "(" and ")", and they don't follow the syntax constraints of natural language sentences. 2) The non-semantic indicators "(" and ")" appear very frequently but contain little semantic information, which will mislead the learning process.</p><p>To address the above challenges, we employ a curriculum learning <ref type="bibr" target="#b2">(Bengio et al., 2009;</ref><ref type="bibr" target="#b46">Xu et al., 2020)</ref> strategy. Specifically, we first train PLMs using simple event substructure generation tasks so that they would not overfit in non-semantic indicators; then we train the model on the full event structure generation task.</p><p>Substructure Learning. Because event representations often have complex structures and their token sequences are different from natural language word sequences, it is challenging to train them with the full sequence generation task directly. Therefore, we first train TEXT2EVENT on simple event substructures.</p><p>Specifically, we learn our model by starting from generating only "(label, span)" substructures, including "(type, trigger words)" and "(role, argument words)" substructures. For example, we will extract substructure tasks in Figure <ref type="figure" target="#fig_1">2c</ref> in this stage as: (Transport returned) (Artifact The man) (Arrest-Jail capture), etc. We construct a sentence, substructures pair for each extracted substructures, then train our model using the loss in equation 4.</p><p>Full Structure Learning. After the substructure learning stage, we further train our model for the full structure generation task using the loss in equation 4. We found the curriculum learning strategy uses data annotation more efficiently and makes the learning process more smooth.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experiments</head><p>This section evaluates the proposed TEXT2EVENT model by conducting experiments in both supervised learning and transfer learning settings. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Experimental Settings</head><p>Datasets. We conducted experiments on the event extraction benchmark -ACE2005 <ref type="bibr" target="#b44">(Walker et al., 2006)</ref>, which has 599 English annotated documents and 33 event types. We used the same split and preprocessing step as the previous work <ref type="bibr" target="#b54">(Zhang et al., 2019b;</ref><ref type="bibr" target="#b43">Wadden et al., 2019;</ref><ref type="bibr" target="#b9">Du and Cardie, 2020)</ref>, and we denote it as ACE05-EN.</p><p>In addition to ACE05-EN, we also conducted experiments on two other benchmarks: ACE05-EN + and ERE-EN, using the same split and preprocessing step in the previous work <ref type="bibr" target="#b23">(Lin et al., 2020)</ref>. Compared to ACE05-EN, ACE05-EN + and ERE-EN further consider pronoun roles and multi-token event triggers. ERE-EN contains 38 event categories and 458 documents.</p><p>Statistics of all datasets are shown in Table <ref type="table" target="#tab_1">1</ref>.</p><p>For evaluation, we used the same criteria in previous work <ref type="bibr" target="#b54">(Zhang et al., 2019b;</ref><ref type="bibr" target="#b43">Wadden et al., 2019;</ref><ref type="bibr" target="#b23">Lin et al., 2020)</ref>. Since TEXT2EVENT is a text generation model, we reconstructed the offset of predicted trigger mentions by finding the matched utterance in the input sequence one by one. For argument mentions, we found the nearest matched utterance to the predicted trigger mention as the predicted offset.</p><p>Baselines. Currently, event extraction supervision can be conducted at two different levels: 1) Token-level annotation, which labels each token in a sentence with event labels, e.g., "The/O dismission/B-End-Position of/O .."; 2) Parallel textrecord annotation, which only gives sentence, event pairs but without expensive token-level annotations, e.g., The dismission of ..., {Type: End-Position, Trigger: dismission, ...} . Furthermore, some previous works also leverage golden entity annotation for model training, which marks all entity mentions with their golden types, to facilitate event extraction. Introducing more supervision knowledge will benefit the event extraction but is more label-intensive. The proposed Text2Event only uses parallel text-record annotation, which makes it more practical in a real-world application.</p><p>To verify TEXT2EVENT, we compare our method with the following groups of baselines:</p><p>1. Baselines using token annotation: TANL is the SOTA sequence generation-based method that models event extraction as a trigger-argument pipeline manner <ref type="bibr">(Paolini et al., 2021)</ref>; Multi-task TANL extends TANL by transferring structure knowledge from other tasks; EEQA <ref type="bibr" target="#b9">(Du and Cardie, 2020)</ref> and MQAEE <ref type="bibr" target="#b18">(Li et al., 2020)</ref> are QA-based models which use machine reading comprehension model for trigger detection and argument extraction.</p><p>2. Baselines using both token annotation and entity annotation: Joint3EE is a joint entity, trigger, argument extraction model based on the shared hidrepresentations (Nguyen and Nguyen, 2019); DYGIE++ is a BERT-based model which captures both within-sentence and cross-sentence context <ref type="bibr" target="#b43">(Wadden et al., 2019)</ref>; GAIL is an inverse reinforcement learning-based joint entity and event extraction model <ref type="bibr" target="#b54">(Zhang et al., 2019b)</ref>; OneIE is an end-to-end IE system which employs global feature and beam search to extract globally optimal event structures <ref type="bibr" target="#b23">(Lin et al., 2020)</ref>.</p><p>Implementations. We optimized our model using label smoothing <ref type="bibr" target="#b40">(Szegedy et al., 2016;</ref><ref type="bibr" target="#b31">Müller et al., 2019)</ref> and AdamW <ref type="bibr" target="#b26">(Loshchilov and Hutter, 2019)</ref> with learning rate=5e-5 for T5-large, 1e-4 for T5-base. For curriculum learning, the epoch of substructure learning is 5, and full structure learning is 30. We conducted each experiment on a single NVIDIA GeForce RTX 3090 24GB. Due to GPU memory limitation, we used different batch sizes for different models: 8 for T5-large and 16 for T5-base; and truncated the max length of raw text to 256 and linearized form to 128 during training. We added the task name as the prefix for the T5 default setup.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Results in Supervised Learning Setting</head><p>Table <ref type="table" target="#tab_2">2</ref> presents the performance of all baselines and TEXT2EVENT on ACE05-EN. And Table <ref type="table" target="#tab_3">3</ref> shows the performance of SOTA and TEXT2EVENT on ACE05-EN + and ERE-EN. We can see that:</p><p>1) By uniformly modeling all tasks in a single model and predicting labels universally, TEXT2EVENT can achieve competitive performance with weaker supervision and simpler architecture. Our method, only using the weak parallel text-record annotations, surpasses most of the baselines using token and entity annotations and achieves competitive performance with SOTA. Furthermore, using the simple encoder-decoder architecture, TEXT2EVENT outperforms most of the counterparts with complicated architectures.</p><p>2) By directly generating event structure from the text, TEXT2EVENT can significantly outperform sequence generation-based methods. Our method improves Arg-C F1 by 4.6% and 2.7% over the SOTA generation baseline and its extended multitask TANL. Compared with sequence generation, structure generation can be effectively guided using event schema knowledge during inference, and there is no need to generate irrelevant information.</p><p>3) By uniformly modeling and sharing information between different tasks and labels, the sequence-to-structure framework can achieve robust performance. From Table <ref type="table" target="#tab_2">2</ref> and Table <ref type="table" target="#tab_3">3</ref>, we can see that the performance of OneIE decreases on the harder dataset ACE05-EN + , which has more pronoun roles and multi-token triggers. By contrast, the performance of TEXT2EVENT remains nearly the same on ACE05-EN. We believe this may be because the proposed sequence-to-structure model is a universal model that doesn't specialize in labels and can better share information between different labels.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Results in Transfer Learning Setting</head><p>TEXT2EVENT is a universal model, therefore can facilitate the knowledge transfer between different labels. To verify the transfer ability of TEXT2EVENT, we conducted experiments in the transfer learning setting, and the results are shown in Table <ref type="table" target="#tab_4">4</ref>. Specifically, we first randomly split the sentences which length larger than 8 in ACE05-EN + into two equal-sized subsets src and tgt: src only retains the annotations of the top 10 frequent event types, and tgt only retains the annotations of the remaining 23 event types. For both src and tgt, we use 80% of the dataset for model training and 20% for evaluation. For transfer learning, We first pre-trained an event extraction model on the src dataset, then fine-tuned the pre-trained model for extracting the new event types in tgt. From Table <ref type="table" target="#tab_4">4</ref>, we can see that:</p><p>1) Data-efficient TEXT2EVENT can make bet- ter use of supervision signals.</p><p>Even training on tgt from scratch, the proposed method also outperforms strong baselines. We believe that this may because baselines using token and entity annotation require massive fine-grained data for model learning. Different from baselines, TEXT2EVENT uniformly models all subtasks, thus the knowledge can be seamlessly transferred, which is more dataefficient.</p><p>2) TEXT2EVENT can effectively transfer knowl- transfer learning. Note that the information of entity annotation is shared across src and tgt. As a result, OneIE can leverage such information to better argument prediction even with worse trigger prediction. However, even without using entity annotation, the proposed method can still achieve a similar improvement in the transfer learning setting. This is because the labels are provided universally in TEXT2EVENT, so the parameters are not labelspecific.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Detailed Analysis</head><p>This section analyzes the effects of event schema knowledge, constrained decoding, and curriculum learning algorithm in TEXT2EVENT. We designed four ablated variants based on T5-base:</p><p>• "TEXT2EVENT" is the base model that is directly trained with the full structure learning.</p><p>• "+ CL" indicates training TEXT2EVENT with the proposed curriculum learning algorithm.</p><p>• "w/o CD" discards the constrained decoding during inference and generates event structures as an unconstrained generation model.</p><p>• "w/o ES" replaces the names of event types and roles with meaningless symbols, which is used to verify the effect of event schema knowledge.</p><p>Table <ref type="table">5</ref> shows the results on the development set of ACE05-EN using different training data sizes. We can see that: 1) Constrained decoding can effectively guide the generation with event schemas, especially in low-resource settings. Comparing to "w/o CD", constrained decoding improves the performance of TEXT2EVENT, especially in lowresource scenarios, e.g., using 1%, 5% training set. 2) Curriculum learning is useful for model learning. Substructure learning improves 4.7% Trig-C F1 and 5.8% Arg-C F1 on average. 3) It is crucial to encode and generate event labels as words, rather than meaningless symbols. Because by encoding labels as natural language words, our method can effectively transfer knowledge from pre-trained language models.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Related Work</head><p>Our work is a synthesis of two research directions: event extraction and structure prediction via neural generation model.</p><p>Event extraction has received widespread attention in recent years, and mainstream methods usually use different strategies to obtain a complete event structure. These methods can be divided into: 1) pipeline classification <ref type="bibr" target="#b0">(Ahn, 2006;</ref><ref type="bibr" target="#b16">Ji and Grishman, 2008;</ref><ref type="bibr" target="#b21">Liao and Grishman, 2010;</ref><ref type="bibr" target="#b11">Hong et al., 2011</ref><ref type="bibr" target="#b12">Hong et al., , 2018;;</ref><ref type="bibr" target="#b15">Huang and Riloff, 2012;</ref><ref type="bibr" target="#b5">Chen et al., 2015;</ref><ref type="bibr" target="#b36">Sha et al., 2016;</ref><ref type="bibr" target="#b22">Lin et al., 2018;</ref><ref type="bibr" target="#b48">Yang et al., 2019;</ref><ref type="bibr" target="#b45">Wang et al., 2019;</ref><ref type="bibr" target="#b28">Ma et al., 2020;</ref><ref type="bibr" target="#b55">Zhang et al., 2020c)</ref>, 2) multi-task joint models <ref type="bibr" target="#b29">(McClosky et al., 2011;</ref><ref type="bibr" target="#b20">Li et al., 2013</ref><ref type="bibr" target="#b19">Li et al., , 2014;;</ref><ref type="bibr" target="#b47">Yang and Mitchell, 2016;</ref><ref type="bibr" target="#b32">Nguyen et al., 2016;</ref><ref type="bibr" target="#b25">Liu et al., 2018;</ref><ref type="bibr" target="#b52">Zhang et al., 2019a;</ref><ref type="bibr" target="#b56">Zheng et al., 2019)</ref>, 3) semantic structure grounding <ref type="bibr" target="#b13">(Huang et al., 2016</ref><ref type="bibr" target="#b14">(Huang et al., , 2018;;</ref><ref type="bibr" target="#b51">Zhang et al., 2020a)</ref>, and 4) question-answering <ref type="bibr" target="#b6">(Chen et al., 2020b;</ref><ref type="bibr" target="#b9">Du and Cardie, 2020;</ref><ref type="bibr" target="#b18">Li et al., 2020;</ref><ref type="bibr" target="#b24">Liu et al., 2020)</ref>.</p><p>Compared with previous methods, we model all subtasks of event extraction in a uniform sequenceto-structure framework, which leads to better decision interactions and information sharing. The neural encoder-decoder generation architecture <ref type="bibr" target="#b39">(Sutskever et al., 2014;</ref><ref type="bibr" target="#b1">Bahdanau et al., 2015)</ref> has shown its strong structure prediction ability and has been widely used in many NLP tasks, such as machine translation <ref type="bibr" target="#b17">(Kalchbrenner and Blunsom, 2013)</ref>, semantic parsing <ref type="bibr" target="#b8">(Dong and Lapata, 2016;</ref><ref type="bibr" target="#b37">Song et al., 2020)</ref>, entity extraction <ref type="bibr" target="#b38">(Straková et al., 2019)</ref>, relation extraction <ref type="bibr" target="#b50">(Zeng et al., 2018;</ref><ref type="bibr" target="#b53">Zhang et al., 2020b)</ref>, and aspect term extraction <ref type="bibr" target="#b27">(Ma et al., 2019)</ref>. Like TEXT2EVENT in this paper, TANL <ref type="bibr">(Paolini et al., 2021)</ref> and GRIT <ref type="bibr" target="#b10">(Du et al., 2021)</ref> also employ neural generation models for event extraction, but they focus on sequence generation, rather than structure generation. Different from previous works that extract text span via labeling <ref type="bibr" target="#b38">(Straková et al., 2019)</ref> or copy/pointer mechanism <ref type="bibr" target="#b50">(Zeng et al., 2018;</ref><ref type="bibr" target="#b10">Du et al., 2021)</ref>, TEXT2EVENT directly generate event schemas and text spans to form event records via constrained decoding <ref type="bibr" target="#b3">(Cao et al., 2021;</ref><ref type="bibr" target="#b4">Chen et al., 2020a)</ref>, which allows TEXT2EVENT to handle various event types and transfer to new types easily.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Conclusions</head><p>In this paper, we propose TEXT2EVENT, a sequence-to-structure generation paradigm for event extraction. TEXT2EVENT directly learns from parallel text-record annotation and uniformly models all subtasks of event extraction in a sequence-to-structure framework. Concretely, we propose an effective sequence-to-structure network for event extraction, which is further enhanced by a constrained decoding algorithm for event knowledge injection during inference and a curriculum learning algorithm for efficient model learning. Experimental results in supervised learning and transfer learning settings show that TEXT2EVENT can achieve competitive performance with the previous SOTA using only coarse text-record annotation.</p><p>For future work, we plan to adapt our method to other information extraction tasks, such as N-ary relation extraction.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: The framework of TEXT2EVENT. Here, TEXT2EVENT takes raw text as input and generates a Transport event and an Arrest-Jail event.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>Figure2: Examples of three event representations. The red solid line indicates the event-role relation; the blue dotted line indicates the label-span relation where the head is a label and the tail is a text span. For example, "Transport-returned" is a label-span relation edge, which head is "Transport" and tail is "returned".</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head></head><label></label><figDesc>The trie of event type T .</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>The man returned to Los Angeles from Mexico following his capture Tuesday by bounty hunters.</figDesc><table><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>Root</cell><cell></cell><cell></cell><cell></cell><cell>((Transport returned</cell></row><row><cell></cell><cell></cell><cell></cell><cell cols="2">Transport</cell><cell cols="3">Arrest-Jail</cell><cell>(Artifact The man)</cell></row><row><cell>Event Type Trigger Artifact Destination Origin</cell><cell>Transport returned The man Los Angeles Mexico</cell><cell>Event Type Arrest-Jail Trigger capture Person The man Time Tuesday Agent bounty hunters</cell><cell>returned Artifact The man</cell><cell>Origin Destination Mexico Los Angeles</cell><cell>capture Person The man</cell><cell>Time Tuesday</cell><cell>…</cell><cell>(Destination Los Angeles) (Origin Mexico)) (Arrest-Jail capture (Person The man) (Time Tuesday) (Agent bounty hunters))</cell></row><row><cell></cell><cell cols="2">(a) Record format.</cell><cell></cell><cell cols="2">(b) Tree format.</cell><cell></cell><cell></cell><cell>(c) Linearized format.</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 1 :</head><label>1</label><figDesc>Dataset statistics.</figDesc><table><row><cell>Dataset</cell><cell cols="4">Split #Sents #Events #Roles</cell></row><row><cell></cell><cell cols="2">Train 17,172</cell><cell>4,202</cell><cell>4,859</cell></row><row><cell>ACE05-EN</cell><cell>Dev</cell><cell>923</cell><cell>450</cell><cell>605</cell></row><row><cell></cell><cell>Test</cell><cell>832</cell><cell>403</cell><cell>576</cell></row><row><cell></cell><cell cols="2">Train 19,216</cell><cell>4,419</cell><cell>6,607</cell></row><row><cell>ACE05-EN +</cell><cell>Dev</cell><cell>901</cell><cell>468</cell><cell>759</cell></row><row><cell></cell><cell>Test</cell><cell>676</cell><cell>424</cell><cell>689</cell></row><row><cell></cell><cell cols="2">Train 14,736</cell><cell>6,208</cell><cell>8,924</cell></row><row><cell>ERE-EN</cell><cell>Dev</cell><cell>1,209</cell><cell>525</cell><cell>730</cell></row><row><cell></cell><cell>Test</cell><cell>1,163</cell><cell>551</cell><cell>822</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2 :</head><label>2</label><figDesc>Experiment results on ACE05-EN. Trig-C indicates trigger identification and classification. Arg-C indicates argument identification and classification. PLM represents the pre-trained language models used by each model.</figDesc><table><row><cell></cell><cell cols="2">Models</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>Trig-C</cell><cell>Arg-C</cell><cell>PLM</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>P</cell><cell>R</cell><cell>F1</cell><cell>P</cell><cell>R</cell><cell>F1</cell></row><row><cell></cell><cell></cell><cell cols="7">Models using Token Annotation + Entity Annotation</cell></row><row><cell cols="9">Joint3EE (Nguyen and Nguyen, 2019) 68.0 71.8 69.8 52.1 52.1 52.1</cell><cell>-</cell></row><row><cell cols="6">DYGIE++ (Wadden et al., 2019)</cell><cell>-</cell><cell>-</cell><cell>69.7</cell><cell>-</cell><cell>-</cell><cell>48.8</cell><cell>BERT-large</cell></row><row><cell></cell><cell cols="8">GAIL (Zhang et al., 2019b) 74.8 69.4 72.0 61.6 45.7 52.4</cell><cell>ELMo</cell></row><row><cell cols="6">OneIE w/o Global (Lin et al., 2020)</cell><cell>-</cell><cell>-</cell><cell>73.5</cell><cell>-</cell><cell>-</cell><cell>53.9</cell><cell>BERT-large</cell></row><row><cell></cell><cell cols="5">OneIE (Lin et al., 2020)</cell><cell>-</cell><cell>-</cell><cell>74.7</cell><cell>-</cell><cell>-</cell><cell>56.8</cell><cell>BERT-large</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="5">Models using Token Annotation</cell></row><row><cell cols="9">EEQA (Du and Cardie, 2020) 71.1 73.7 72.4 56.8 50.2 53.3 2×BERT-base</cell></row><row><cell></cell><cell cols="5">MQAEE (Li et al., 2020)</cell><cell>-</cell><cell>-</cell><cell>71.7</cell><cell>-</cell><cell>-</cell><cell>53.4 3×BERT-large</cell></row><row><cell></cell><cell></cell><cell cols="7">Generation-based Baselines using Token Annotation</cell></row><row><cell></cell><cell cols="5">TANL (Paolini et al., 2021)</cell><cell>-</cell><cell>-</cell><cell>68.4</cell><cell>-</cell><cell>-</cell><cell>47.6</cell><cell>T5-base</cell></row><row><cell cols="6">Multi-Task TANL (Paolini et al., 2021)</cell><cell>-</cell><cell>-</cell><cell>68.5</cell><cell>-</cell><cell>-</cell><cell>48.5</cell><cell>T5-base</cell></row><row><cell></cell><cell></cell><cell></cell><cell cols="6">Our Model using Parallel Text-Record Annotation</cell></row><row><cell></cell><cell></cell><cell cols="7">TEXT2EVENT 67.5 71.2 69.2 46.7 53.4 49.8</cell><cell>T5-base</cell></row><row><cell></cell><cell></cell><cell cols="7">TEXT2EVENT 69.6 74.4 71.9 52.5 55.2 53.8</cell><cell>T5-large</cell></row><row><cell></cell><cell></cell><cell>Trig-C</cell><cell></cell><cell cols="2">Arg-C</cell><cell></cell><cell></cell></row><row><cell>Datasets</cell><cell>P</cell><cell>R</cell><cell>F1</cell><cell>P</cell><cell>R</cell><cell>F1</cell><cell></cell></row><row><cell cols="5">SOTA (Token + Entity Annotation)</cell><cell></cell><cell></cell><cell></cell></row><row><cell>ACE05-EN +</cell><cell>-</cell><cell>-</cell><cell>72.8</cell><cell>-</cell><cell>-</cell><cell>54.8</cell><cell></cell></row><row><cell>ERE-EN  *</cell><cell cols="6">56.9 58.7 57.8 51.9 47.8 49.8</cell><cell></cell></row><row><cell cols="7">TEXT2EVENT (Parallel Text-Record Annotation)</cell><cell></cell></row><row><cell cols="7">ACE05-EN + 71.2 72.5 71.8 54.0 54.8 54.4</cell><cell></cell></row><row><cell>ERE-EN</cell><cell cols="6">59.2 59.6 59.4 49.4 47.2 48.3</cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 3 :</head><label>3</label><figDesc>Experiment results on ACE05-EN + and ERE-EN. SOTA indicates the state-of-the-art system -OneIE. * The result of SOTA for ERE-EN is reproduced by the official release code because of the slightly different dataset statistic result on ERE-EN.</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 4 :</head><label>4</label><figDesc>Experiment results on the tgt subset of ACE05-EN + in the transfer learning setting.</figDesc><table><row><cell>Settings</cell><cell>Trig-C</cell><cell></cell><cell></cell><cell>Arg-C</cell><cell></cell></row><row><cell>P</cell><cell>R</cell><cell>F1</cell><cell>P</cell><cell>R</cell><cell>F1</cell></row><row><cell cols="5">OneIE (Token + Entity Annotation)</cell><cell></cell></row><row><cell cols="6">Non-transfer 78.1 62.3 69.3 50.9 37.9 43.5</cell></row><row><cell cols="6">Transfer 78.9 61.7 69.2 57.1 40.0 47.0</cell></row><row><cell>Gain</cell><cell></cell><cell>-0.1</cell><cell></cell><cell></cell><cell>+3.5</cell></row><row><cell cols="4">EEQA (Token Annotation)</cell><cell></cell><cell></cell></row><row><cell cols="6">Non-transfer 69.9 67.3 68.6 36.5 37.4 36.9</cell></row><row><cell cols="6">Transfer 79.5 61.7 69.5 33.9 41.2 37.2</cell></row><row><cell>Gain</cell><cell></cell><cell>+0.9</cell><cell></cell><cell></cell><cell>+0.3</cell></row><row><cell cols="6">TEXT2EVENT (Parallel Text-Record Annotation)</cell></row><row><cell cols="6">Non-transfer 79.4 61.1 69.0 58.4 40.9 48.0</cell></row><row><cell cols="6">Transfer 82.1 65.3 72.7 58.8 45.4 51.2</cell></row><row><cell>Gain</cell><cell></cell><cell>+3.7</cell><cell></cell><cell></cell><cell>+3.2</cell></row><row><cell cols="6">edge between different labels. Compared with the</cell></row><row><cell cols="6">non-transfer setting, which is directly trained on tgt</cell></row><row><cell cols="6">training set, the transfer setting of TEXT2EVENT</cell></row><row><cell cols="6">can achieve significant F1 improvements of 3.7 and</cell></row><row><cell cols="6">3.2 on Trig-C and Arg-C, respectively. By contrast,</cell></row><row><cell cols="6">the other two baselines cannot obtain significant</cell></row><row><cell cols="6">F1 improvements of both Trig-C and Arg-C via</cell></row></table></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgments</head><p>We sincerely thank the reviewers for their insightful comments and valuable suggestions. This work is supported by the National Natural Science Foundation of China under Grants no. U1936207 and 61772505, Beijing Academy of Artificial Intelligence (BAAI2019QN0502), and in part by the Youth Innovation Promotion Association CAS(2018141).</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">The stages of event extraction</title>
		<author>
			<persName><forename type="first">David</forename><surname>Ahn</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Workshop on Annotating and Reasoning about Time and Events</title>
				<meeting>the Workshop on Annotating and Reasoning about Time and Events<address><addrLine>Sydney</addrLine></address></meeting>
		<imprint>
			<publisher>Australia. Association for Computational Linguistics</publisher>
			<date type="published" when="2006">2006</date>
			<biblScope unit="page" from="1" to="8" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Neural machine translation by jointly learning to align and translate</title>
		<author>
			<persName><forename type="first">Dzmitry</forename><surname>Bahdanau</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kyunghyun</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The Third International Conference on Learning Representations</title>
				<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Curriculum learning</title>
		<author>
			<persName><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jérôme</forename><surname>Louradour</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ronan</forename><surname>Collobert</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jason</forename><surname>Weston</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 26th International Conference on Machine Learning</title>
				<meeting>the 26th International Conference on Machine Learning</meeting>
		<imprint>
			<publisher>Montreal. Omnipress</publisher>
			<date type="published" when="2009">2009</date>
			<biblScope unit="page" from="41" to="48" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Autoregressive entity retrieval</title>
		<author>
			<persName><forename type="first">Nicola</forename><surname>De Cao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gautier</forename><surname>Izacard</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sebastian</forename><surname>Riedel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fabio</forename><surname>Petroni</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
				<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Parallel sentence mining by constrained decoding</title>
		<author>
			<persName><forename type="first">Pinzhen</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nikolay</forename><surname>Bogoychev</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kenneth</forename><surname>Heafield</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Faheem</forename><surname>Kirefu</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2020.acl-main.152</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics</title>
				<meeting>the 58th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<publisher>Online. Association for Computational Linguistics</publisher>
			<date type="published" when="2020">2020a</date>
			<biblScope unit="page" from="1672" to="1678" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Event extraction via dynamic multipooling convolutional neural networks</title>
		<author>
			<persName><forename type="first">Yubo</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Liheng</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Daojian</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jun</forename><surname>Zhao</surname></persName>
		</author>
		<idno type="DOI">10.3115/v1/P15-1017</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing</title>
		<title level="s">Long Papers</title>
		<meeting>the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing<address><addrLine>Beijing, China</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="167" to="176" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Reading the manual: Event extraction as definition comprehension</title>
		<author>
			<persName><forename type="first">Yunmo</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tongfei</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Seth</forename><surname>Ebner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aaron</forename><surname>Steven White</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Benjamin</forename><surname>Van Durme</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2020.spnlp-1.9</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Fourth Workshop on Structured Prediction for NLP</title>
				<meeting>the Fourth Workshop on Structured Prediction for NLP</meeting>
		<imprint>
			<publisher>Online. Association for Computational Linguistics</publisher>
			<date type="published" when="2020">2020b</date>
			<biblScope unit="page" from="74" to="83" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">The automatic content extraction (ACE) program -tasks, data, and evaluation</title>
		<author>
			<persName><forename type="first">George</forename><surname>Doddington</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alexis</forename><surname>Mitchell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mark</forename><surname>Przybocki</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lance</forename><surname>Ramshaw</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stephanie</forename><surname>Strassel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ralph</forename><surname>Weischedel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Fourth International Conference on Language Resources and Evaluation (LREC&apos;04)</title>
				<meeting>the Fourth International Conference on Language Resources and Evaluation (LREC&apos;04)<address><addrLine>Lisbon, Portugal</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
	<note>European Language Resources Association (ELRA</note>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Language to logical form with neural attention</title>
		<author>
			<persName><forename type="first">Li</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mirella</forename><surname>Lapata</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/P16-1004</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics</title>
		<title level="s">Long Papers</title>
		<meeting>the 54th Annual Meeting of the Association for Computational Linguistics<address><addrLine>Berlin, Germany</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2016">2016</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="33" to="43" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Event extraction by answering (almost) natural questions</title>
		<author>
			<persName><forename type="first">Xinya</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Claire</forename><surname>Cardie</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2020.emnlp-main.49</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)</title>
				<meeting>the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)</meeting>
		<imprint>
			<publisher>Online. Association for Computational Linguistics</publisher>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="671" to="683" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">GRIT: Generative role-filler transformers for document-level event entity extraction</title>
		<author>
			<persName><forename type="first">Xinya</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alexander</forename><surname>Rush</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Claire</forename><surname>Cardie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 16th Conference of the European Chapter of the Association for Computational Linguistics: Main Volume</title>
				<meeting>the 16th Conference of the European Chapter of the Association for Computational Linguistics: Main Volume</meeting>
		<imprint>
			<publisher>Online. Association for Computational Linguistics</publisher>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="634" to="644" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Using cross-entity inference to improve event extraction</title>
		<author>
			<persName><forename type="first">Yu</forename><surname>Hong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jianfeng</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bin</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jianmin</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Guodong</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qiaoming</forename><surname>Zhu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies</title>
				<meeting>the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies<address><addrLine>Portland, Oregon, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2011">2011</date>
			<biblScope unit="page" from="1127" to="1136" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Self-regulation: Employing a generative adversarial network to improve event detection</title>
		<author>
			<persName><forename type="first">Yu</forename><surname>Hong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wenxuan</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jingli</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Guodong</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qiaoming</forename><surname>Zhu</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/P18-1048</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics</title>
		<title level="s">Long Papers</title>
		<meeting>the 56th Annual Meeting of the Association for Computational Linguistics<address><addrLine>Melbourne, Australia</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2018">2018</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="515" to="526" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Liberal event extraction and event schema induction</title>
		<author>
			<persName><forename type="first">Lifu</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Taylor</forename><surname>Cassidy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaocheng</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Heng</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Clare</forename><forename type="middle">R</forename><surname>Voss</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiawei</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Avirup</forename><surname>Sil</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/P16-1025</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics</title>
		<title level="s">Long Papers</title>
		<meeting>the 54th Annual Meeting of the Association for Computational Linguistics<address><addrLine>Berlin, Germany</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2016">2016</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="258" to="268" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Zero-shot transfer learning for event extraction</title>
		<author>
			<persName><forename type="first">Lifu</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Heng</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kyunghyun</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ido</forename><surname>Dagan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sebastian</forename><surname>Riedel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Clare</forename><surname>Voss</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/P18-1201</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics</title>
				<meeting>the 56th Annual Meeting of the Association for Computational Linguistics<address><addrLine>Melbourne, Australia</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2018">2018</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="2160" to="2170" />
		</imprint>
	</monogr>
	<note>Long Papers)</note>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Modeling textual cohesion for event extraction</title>
		<author>
			<persName><forename type="first">Ruihong</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ellen</forename><surname>Riloff</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Twenty-Sixth AAAI Conference on Artificial Intelligence, AAAI&apos;12</title>
				<meeting>the Twenty-Sixth AAAI Conference on Artificial Intelligence, AAAI&apos;12</meeting>
		<imprint>
			<publisher>AAAI Press</publisher>
			<date type="published" when="2012">2012</date>
			<biblScope unit="page" from="1664" to="1670" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Refining event extraction through cross-document inference</title>
		<author>
			<persName><forename type="first">Heng</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ralph</forename><surname>Grishman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ACL-08: HLT</title>
				<meeting>ACL-08: HLT</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2008">2008</date>
			<biblScope unit="page" from="254" to="262" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Recurrent continuous translation models</title>
		<author>
			<persName><forename type="first">Nal</forename><surname>Kalchbrenner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Phil</forename><surname>Blunsom</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing</title>
				<meeting>the 2013 Conference on Empirical Methods in Natural Language Processing<address><addrLine>Seattle, Washington, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2013">2013</date>
			<biblScope unit="page" from="1700" to="1709" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Event extraction as multi-turn question answering</title>
		<author>
			<persName><forename type="first">Fayuan</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Weihua</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuguang</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Quan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lu</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yajuan</forename><surname>Lyu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yong</forename><surname>Zhu</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2020.findings-emnlp.73</idno>
	</analytic>
	<monogr>
		<title level="m">Findings of the Association for Computational Linguistics: EMNLP 2020</title>
				<imprint>
			<publisher>Online. Association for Computational Linguistics</publisher>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="829" to="838" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Constructing information networks using one single model</title>
		<author>
			<persName><forename type="first">Qi</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Heng</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yu</forename><surname>Hong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sujian</forename><surname>Li</surname></persName>
		</author>
		<idno type="DOI">10.3115/v1/D14-1198</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP)</title>
				<meeting>the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP)<address><addrLine>Doha, Qatar</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="1846" to="1851" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Joint event extraction via structured prediction with global features</title>
		<author>
			<persName><forename type="first">Qi</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ji</forename><surname>Heng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Liang</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics</title>
		<title level="s">Long Papers</title>
		<meeting>the 51st Annual Meeting of the Association for Computational Linguistics<address><addrLine>Sofia, Bulgaria</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2013">2013</date>
			<biblScope unit="page" from="73" to="82" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Using document level cross-event inference to improve event extraction</title>
		<author>
			<persName><forename type="first">Shasha</forename><surname>Liao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ralph</forename><surname>Grishman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics</title>
				<meeting>the 48th Annual Meeting of the Association for Computational Linguistics<address><addrLine>Uppsala, Sweden</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2010">2010</date>
			<biblScope unit="page" from="789" to="797" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Nugget proposal networks for Chinese event detection</title>
		<author>
			<persName><forename type="first">Hongyu</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yaojie</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xianpei</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Le</forename><surname>Sun</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/P18-1145</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics</title>
		<title level="s">Long Papers</title>
		<meeting>the 56th Annual Meeting of the Association for Computational Linguistics<address><addrLine>Melbourne</addrLine></address></meeting>
		<imprint>
			<publisher>Australia. Association for Computational Linguistics</publisher>
			<date type="published" when="2018">2018</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="1565" to="1574" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">A joint neural model for information extraction with global features</title>
		<author>
			<persName><forename type="first">Ying</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Heng</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fei</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lingfei</forename><surname>Wu</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2020.acl-main.713</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics</title>
				<meeting>the 58th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<publisher>Online. Association for Computational Linguistics</publisher>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="7999" to="8009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Event extraction as machine reading comprehension</title>
		<author>
			<persName><forename type="first">Jian</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yubo</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wei</forename><surname>Bi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaojiang</forename><surname>Liu</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2020.emnlp-main.128</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)</title>
				<meeting>the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)</meeting>
		<imprint>
			<publisher>Online. Association for Computational Linguistics</publisher>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="1641" to="1651" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Jointly multiple events extraction via attentionbased graph information aggregation</title>
		<author>
			<persName><forename type="first">Xiao</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhunchen</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Heyan</forename><surname>Huang</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/D18-1156</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing</title>
				<meeting>the 2018 Conference on Empirical Methods in Natural Language Processing<address><addrLine>Brussels, Belgium</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="1247" to="1256" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Decoupled weight decay regularization</title>
		<author>
			<persName><forename type="first">Ilya</forename><surname>Loshchilov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Frank</forename><surname>Hutter</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Seventh International Conference on Learning Representations</title>
				<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Exploring sequence-tosequence learning in aspect term extraction</title>
		<author>
			<persName><forename type="first">Dehong</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sujian</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fangzhao</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xing</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Houfeng</forename><surname>Wang</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/P19-1344</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics</title>
				<meeting>the 57th Annual Meeting of the Association for Computational Linguistics<address><addrLine>Florence, Italy</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="3538" to="3547" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Resourceenhanced neural model for event argument extraction</title>
		<author>
			<persName><forename type="first">Jie</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shuai</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rishita</forename><surname>Anubhai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Miguel</forename><surname>Ballesteros</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yaser</forename><surname>Al-Onaizan</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2020.findings-emnlp.318</idno>
	</analytic>
	<monogr>
		<title level="m">Findings of the Association for Computational Linguistics: EMNLP 2020</title>
				<imprint>
			<publisher>Online. Association for Computational Linguistics</publisher>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="3554" to="3559" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Event extraction as dependency parsing</title>
		<author>
			<persName><forename type="first">David</forename><surname>Mcclosky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mihai</forename><surname>Surdeanu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christopher</forename><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies</title>
				<meeting>the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies</meeting>
		<imprint>
			<date type="published" when="2011">2011</date>
			<biblScope unit="page" from="1626" to="1635" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
		<title/>
		<author>
			<persName><forename type="first">Oregon</forename><surname>Portland</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Usa</forename></persName>
		</author>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">When does label smoothing help?</title>
		<author>
			<persName><forename type="first">Rafael</forename><surname>Müller</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Simon</forename><surname>Kornblith</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Geoffrey</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
				<imprint>
			<publisher>Curran Associates, Inc</publisher>
			<date type="published" when="2019">2019</date>
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="page" from="4694" to="4703" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Joint event extraction via recurrent neural networks</title>
		<author>
			<persName><forename type="first">Thien</forename><surname>Huu Nguyen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kyunghyun</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ralph</forename><surname>Grishman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2016 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</title>
				<meeting>the 2016 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies<address><addrLine>San Diego, California</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="300" to="309" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">One for all: Neural joint modeling of entities and events</title>
		<author>
			<persName><forename type="first">Minh</forename><surname>Trung</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Thien</forename><surname>Nguyen</surname></persName>
		</author>
		<author>
			<persName><surname>Huu Nguyen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The Thirty-Third AAAI Conference on Artificial Intelligence, AAAI &apos;2019. Association for the Advancement of Artificial Intelligence</title>
				<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Cicero Nogueira dos Santos, Bing Xiang, and Stefano Soatto. 2021. Structured prediction as translation between augmented natural languages</title>
		<author>
			<persName><forename type="first">Giovanni</forename><surname>Paolini</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ben</forename><surname>Athiwaratkun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jason</forename><surname>Krone</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jie</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alessandro</forename><surname>Achille</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rishita</forename><surname>Anubhai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The Ninth International Conference on Learning Representations</title>
				<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Exploring the limits of transfer learning with a unified text-totext transformer</title>
		<author>
			<persName><forename type="first">Colin</forename><surname>Raffel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Noam</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Adam</forename><surname>Roberts</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Katherine</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sharan</forename><surname>Narang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><surname>Matena</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yanqi</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wei</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Peter</forename><forename type="middle">J</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="issue">140</biblScope>
			<biblScope unit="page" from="1" to="67" />
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">RBPB: Regularization-based pattern balancing method for event extraction</title>
		<author>
			<persName><forename type="first">Lei</forename><surname>Sha</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jing</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chin-Yew</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sujian</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Baobao</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhifang</forename><surname>Sui</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/P16-1116</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics</title>
		<title level="s">Long Papers</title>
		<meeting>the 54th Annual Meeting of the Association for Computational Linguistics<address><addrLine>Berlin, Germany</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2016">2016</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="1224" to="1234" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Structural information preserving for graph-to-text generation</title>
		<author>
			<persName><forename type="first">Linfeng</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ante</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jinsong</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yue</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kun</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yubin</forename><surname>Ge</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dong</forename><surname>Yu</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2020.acl-main.712</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics</title>
				<meeting>the 58th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<publisher>Online. Association for Computational Linguistics</publisher>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="7987" to="7998" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Neural architectures for nested NER through linearization</title>
		<author>
			<persName><forename type="first">Jana</forename><surname>Straková</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Milan</forename><surname>Straka</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/P19-1527</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics</title>
				<meeting>the 57th Annual Meeting of the Association for Computational Linguistics<address><addrLine>Florence, Italy</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2019">Jan Hajic. 2019</date>
			<biblScope unit="page" from="5326" to="5331" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Sequence to sequence learning with neural networks</title>
		<author>
			<persName><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Oriol</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Quoc V</forename><surname>Le</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
				<imprint>
			<publisher>Curran Associates, Inc</publisher>
			<date type="published" when="2014">2014</date>
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="page" from="3104" to="3112" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Rethinking the inception architecture for computer vision</title>
		<author>
			<persName><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Vanhoucke</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Ioffe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Shlens</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Wojna</surname></persName>
		</author>
		<idno type="DOI">10.1109/CVPR.2016.308</idno>
	</analytic>
	<monogr>
		<title level="m">2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
				<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="2818" to="2826" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Attention is all you need</title>
		<author>
			<persName><forename type="first">Ashish</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Noam</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Niki</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jakob</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Llion</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aidan</forename><forename type="middle">N</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ł</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Illia</forename><surname>Polosukhin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
				<imprint>
			<publisher>Curran Associates, Inc</publisher>
			<date type="published" when="2017">2017</date>
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="page" from="5998" to="6008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Grammar as a foreign language</title>
		<author>
			<persName><forename type="first">Oriol</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Terry</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Slav</forename><surname>Koo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ilya</forename><surname>Petrov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Geoffrey</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
				<imprint>
			<publisher>Curran Associates, Inc</publisher>
			<date type="published" when="2015">2015</date>
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="page" from="2773" to="2781" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Entity, relation, and event extraction with contextualized span representations</title>
		<author>
			<persName><forename type="first">David</forename><surname>Wadden</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ulme</forename><surname>Wennberg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yi</forename><surname>Luan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hannaneh</forename><surname>Hajishirzi</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/D19-1585</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)</title>
				<meeting>the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)<address><addrLine>Hong Kong, China</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="5784" to="5789" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<monogr>
		<title/>
		<author>
			<persName><forename type="first">Christopher</forename><surname>Walker</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stephanie</forename><surname>Strassel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Julie</forename><surname>Medero</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kazuaki</forename><surname>Maeda</surname></persName>
		</author>
		<idno type="DOI">10.35111/mwxc-vh88</idno>
		<imprint>
			<date type="published" when="2005">2006. 2005</date>
		</imprint>
	</monogr>
	<note>multilingual training corpus</note>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">HMEAE: Hierarchical modular event argument extraction</title>
		<author>
			<persName><forename type="first">Xiaozhi</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ziqi</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xu</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhiyuan</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Juanzi</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Peng</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Maosong</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jie</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiang</forename><surname>Ren</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/D19-1584</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)</title>
				<meeting>the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)<address><addrLine>Hong Kong, China</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="5777" to="5783" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Curriculum learning for natural language understanding</title>
		<author>
			<persName><forename type="first">Benfeng</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Licheng</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhendong</forename><surname>Mao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Quan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hongtao</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yongdong</forename><surname>Zhang</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2020.acl-main.542</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics</title>
				<meeting>the 58th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<publisher>Online. Association for Computational Linguistics</publisher>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="6095" to="6104" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Joint extraction of events and entities within a document context</title>
		<author>
			<persName><forename type="first">Bishan</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tom</forename><forename type="middle">M</forename><surname>Mitchell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2016 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</title>
				<meeting>the 2016 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies<address><addrLine>San Diego, California</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="289" to="299" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<monogr>
		<title level="m" type="main">Exploring pre-trained language models for event extraction and generation</title>
		<author>
			<persName><forename type="first">Sen</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dawei</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Linbo</forename><surname>Qiao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhigang</forename><surname>Kan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dongsheng</forename><surname>Li</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/P19-1522</idno>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<monogr>
		<title level="m">Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics</title>
				<meeting>the 57th Annual Meeting of the Association for Computational Linguistics<address><addrLine>Florence, Italy</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<biblScope unit="page" from="5284" to="5294" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Extracting relational facts by an end-to-end neural model with copy mechanism</title>
		<author>
			<persName><forename type="first">Xiangrong</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Daojian</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shizhu</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jun</forename><surname>Zhao</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/P18-1047</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics</title>
		<title level="s">Long Papers</title>
		<meeting>the 56th Annual Meeting of the Association for Computational Linguistics<address><addrLine>Melbourne</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2018">2018</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="506" to="514" />
		</imprint>
	</monogr>
	<note>Australia</note>
</biblStruct>

<biblStruct xml:id="b51">
	<monogr>
		<title level="m" type="main">Unsupervised label-aware event trigger and argument classification</title>
		<author>
			<persName><forename type="first">Hongming</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Haoyu</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dan</forename><surname>Roth</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020">2020a</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">Extracting entities and events as a single task using a transition-based neural model</title>
		<author>
			<persName><forename type="first">Junchi</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yanxia</forename><surname>Qin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yue</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mengchi</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Donghong</forename><surname>Ji</surname></persName>
		</author>
		<idno type="DOI">10.24963/ijcai.2019/753</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Twenty-Eighth International Joint Conference on Artificial Intelligence, IJCAI-19</title>
				<meeting>the Twenty-Eighth International Joint Conference on Artificial Intelligence, IJCAI-19</meeting>
		<imprint>
			<date type="published" when="2019">2019a</date>
			<biblScope unit="page" from="5422" to="5428" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">Minimize exposure bias of Seq2Seq models in joint entity and relation extraction</title>
		<author>
			<persName><forename type="first">Qianying</forename><surname>Ranran Haoran Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aysa</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Heng</forename><surname>Xuemo Fan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Daojian</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fei</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Daisuke</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sadao</forename><surname>Kawahara</surname></persName>
		</author>
		<author>
			<persName><surname>Kurohashi</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2020.findings-emnlp.23</idno>
	</analytic>
	<monogr>
		<title level="m">Findings of the Association for Computational Linguistics: EMNLP 2020</title>
				<imprint>
			<publisher>Online. Association for Computational Linguistics</publisher>
			<date type="published" when="2020">2020b</date>
			<biblScope unit="page" from="236" to="246" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">Joint entity and event extraction with generative adversarial imitation learning</title>
		<author>
			<persName><forename type="first">Tongtao</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ji</forename><surname>Heng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Avirup</forename><surname>Sil</surname></persName>
		</author>
		<idno type="DOI">10.1162/dint_a_00014</idno>
	</analytic>
	<monogr>
		<title level="j">Data Intelligence</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="99" to="120" />
			<date type="published" when="2019">2019b</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<analytic>
		<title level="a" type="main">A two-step approach for implicit event argument detection</title>
		<author>
			<persName><forename type="first">Zhisong</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiang</forename><surname>Kong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhengzhong</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xuezhe</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Eduard</forename><surname>Hovy</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2020.acl-main.667</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics</title>
				<meeting>the 58th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<publisher>Online. Association for Computational Linguistics</publisher>
			<date type="published" when="2020">2020c</date>
			<biblScope unit="page" from="7479" to="7485" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<analytic>
		<title level="a" type="main">Doc2EDAG: An end-to-end document-level framework for Chinese financial event extraction</title>
		<author>
			<persName><forename type="first">Shun</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wei</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wei</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiang</forename><surname>Bian</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/D19-1032</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)</title>
				<meeting>the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)<address><addrLine>Hong Kong, China</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="337" to="346" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
