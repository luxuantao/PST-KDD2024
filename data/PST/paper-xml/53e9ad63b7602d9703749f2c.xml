<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Learning CPG-based biped locomotion with a policy gradient method</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date type="published" when="2006-08-01">1 August 2006</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Takamitsu</forename><surname>Matsubara</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Nara Institute of Science and Technology</orgName>
								<address>
									<postCode>8916-5, 630-0101</postCode>
									<settlement>Takayama-cho, Ikoma</settlement>
									<region>Nara</region>
									<country key="JP">Japan</country>
								</address>
							</affiliation>
							<affiliation key="aff2">
								<orgName type="laboratory">ATR</orgName>
								<orgName type="institution">CNS</orgName>
								<address>
									<addrLine>2-2-2, Seika-cho, Soraku-gun</addrLine>
									<postCode>619-0288</postCode>
									<settlement>Hikaridai, Kyoto</settlement>
									<country key="JP">Japan</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Jun</forename><surname>Morimoto</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution" key="instit1">ICORP</orgName>
								<orgName type="institution" key="instit2">JST</orgName>
								<address>
									<addrLine>2-2-2, Seika-cho, Soraku-gun</addrLine>
									<postCode>619-0288</postCode>
									<settlement>Hikaridai, Kyoto</settlement>
									<country key="JP">Japan</country>
								</address>
							</affiliation>
							<affiliation key="aff2">
								<orgName type="laboratory">ATR</orgName>
								<orgName type="institution">CNS</orgName>
								<address>
									<addrLine>2-2-2, Seika-cho, Soraku-gun</addrLine>
									<postCode>619-0288</postCode>
									<settlement>Hikaridai, Kyoto</settlement>
									<country key="JP">Japan</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Jun</forename><surname>Nakanishi</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution" key="instit1">ICORP</orgName>
								<orgName type="institution" key="instit2">JST</orgName>
								<address>
									<addrLine>2-2-2, Seika-cho, Soraku-gun</addrLine>
									<postCode>619-0288</postCode>
									<settlement>Hikaridai, Kyoto</settlement>
									<country key="JP">Japan</country>
								</address>
							</affiliation>
							<affiliation key="aff2">
								<orgName type="laboratory">ATR</orgName>
								<orgName type="institution">CNS</orgName>
								<address>
									<addrLine>2-2-2, Seika-cho, Soraku-gun</addrLine>
									<postCode>619-0288</postCode>
									<settlement>Hikaridai, Kyoto</settlement>
									<country key="JP">Japan</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Masa-Aki</forename><surname>Sato</surname></persName>
							<affiliation key="aff2">
								<orgName type="laboratory">ATR</orgName>
								<orgName type="institution">CNS</orgName>
								<address>
									<addrLine>2-2-2, Seika-cho, Soraku-gun</addrLine>
									<postCode>619-0288</postCode>
									<settlement>Hikaridai, Kyoto</settlement>
									<country key="JP">Japan</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Kenji</forename><surname>Doya</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Nara Institute of Science and Technology</orgName>
								<address>
									<postCode>8916-5, 630-0101</postCode>
									<settlement>Takayama-cho, Ikoma</settlement>
									<region>Nara</region>
									<country key="JP">Japan</country>
								</address>
							</affiliation>
							<affiliation key="aff2">
								<orgName type="laboratory">ATR</orgName>
								<orgName type="institution">CNS</orgName>
								<address>
									<addrLine>2-2-2, Seika-cho, Soraku-gun</addrLine>
									<postCode>619-0288</postCode>
									<settlement>Hikaridai, Kyoto</settlement>
									<country key="JP">Japan</country>
								</address>
							</affiliation>
							<affiliation key="aff3">
								<orgName type="department">Initial Research Project</orgName>
								<orgName type="institution" key="instit1">Neural Computation Unit</orgName>
								<orgName type="institution" key="instit2">Okinawa Institute of Science and Technology</orgName>
								<address>
									<addrLine>12-22</addrLine>
									<postCode>904-2234</postCode>
									<settlement>Suzaki, Gushikawa</settlement>
									<region>Okinawa</region>
									<country key="JP">Japan</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff4">
								<orgName type="institution">Nara Institute of Science and Technology</orgName>
								<address>
									<postCode>8916-5, 630-0101</postCode>
									<settlement>Takayama-cho, Ikoma</settlement>
									<region>Nara</region>
									<country key="JP">Japan</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Learning CPG-based biped locomotion with a policy gradient method</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2006-08-01">1 August 2006</date>
						</imprint>
					</monogr>
					<idno type="MD5">3D2A4146F564E7279386B9B1ED2D3F54</idno>
					<idno type="DOI">10.1016/j.robot.2006.05.012</idno>
					<note type="submission">Received 15 October 2005; received in revised form 12 May 2006; accepted 24 May 2006</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.3" ident="GROBID" when="2023-07-28T14:08+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Reinforcement learning</term>
					<term>Policy gradient</term>
					<term>Biped locomotion</term>
					<term>Central pattern generator</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>In this paper, we propose a learning framework for CPG-based biped locomotion with a policy gradient method. We demonstrate that appropriate sensory feedback to adjust the rhythm of the CPG (Central Pattern Generator) can be learned using the proposed method within a few hundred trials in simulations. We investigate linear stability of a periodic orbit of the acquired walking pattern considering its approximated return map. Furthermore, we apply the controllers acquired in numerical simulations to our physical 5-link biped robot in order to empirically evaluate the robustness of walking in the real environment. Experimental results demonstrate that the robot was able to successfully walk using the acquired controllers even in the cases of an environmental change by placing a seesaw-like metal sheet on the ground and a parametric change of the robot dynamics with an additional weight on a shank, which was not modeled in the numerical simulations.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Recently, there has been a growing interest in biologically inspired control approaches for biped locomotion using neural oscillators (e.g., <ref type="bibr" target="#b0">[1]</ref>) as a central pattern generator (CPG). Notably, Taga <ref type="bibr" target="#b1">[2]</ref> demonstrated the effectiveness of this approach for biped locomotion to achieve the desired walking behavior in unpredicted environments in numerical simulations. Following this pioneering work, several attempts have been made to explore neural oscillator based controllers for legged locomotion <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b3">4]</ref>. Neural oscillators have desirable properties such as entrainment through interaction with the environment. However, in order to achieve the desired behavior of the oscillators, much effort is required in manually tuning their parameters. Our goal in this study is to develop an efficient learning framework for CPG-based locomotion of biped robots.</p><p>As parameter optimization methods for CPG-based locomotion controllers, a genetic algorithm <ref type="bibr" target="#b4">[5]</ref> and reinforcement learning <ref type="bibr" target="#b5">[6]</ref> were applied to determine the open parameters of the CPG considering high dimensional state space. However, these methods often require a large number of iterations to obtain the solution, and typically suffer from high computational costs with an increase of dimensionality of the state space. These undesirable features make it infeasible to directly apply these methods to real robots in real-time implementation.</p><p>In this paper, we focus on learning appropriate sensory feedback to the CPG in order to achieve the desired walking behavior. The importance of sensory feedback to CPG in order to achieve adaptation to the environment is pointed out in <ref type="bibr" target="#b1">[2]</ref>. We propose a learning framework for a CPG-based biped locomotion controller using a policy gradient reinforcement learning method for a 5-link biped robot (Fig. <ref type="figure">1</ref>). The policy gradient method is a technique for maximizing an accumulated reward with respect to the parameters of a stochastic policy by trial-and-error in an unknown environment <ref type="bibr" target="#b6">[7]</ref><ref type="bibr" target="#b7">[8]</ref><ref type="bibr" target="#b8">[9]</ref><ref type="bibr" target="#b9">[10]</ref><ref type="bibr" target="#b10">[11]</ref>. However, the policy gradient method also suffers from high computational costs with an increase of dimensionality of the state space when the use of function approximator with a large number of parameters is desirable to represent a nonlinear controller. Thus, in order to reduce the dimensionality of the Fig. <ref type="figure">1</ref>. 5-link biped robot (left) and its model (right). x-z plane is defined as "sagittal plane". state space used for learning, we only use partial physical states of the robot in our proposed learning system, i.e., we do not use internal states of the CPG and the rest of the states of the robot for learning. As a result, we conceive of the proposed learning framework as a partially observable Markov decision process (POMDP). A general reinforcement learning approach, for example, Q-learning, finds a deterministic optimal policy that maximizes the values of all the states simultaneously assuming that the environment has the Markov property <ref type="bibr" target="#b11">[12]</ref>. However, as discussed in <ref type="bibr" target="#b12">[13]</ref>, stochastic policies often show better performance than deterministic policies in POMDPs. Moreover, the effectiveness of the policy gradient method in POMDPs has been empirically demonstrated for a 4-legged robot <ref type="bibr" target="#b13">[14]</ref> and a passive-dynamics based biped robot <ref type="bibr" target="#b14">[15]</ref>. Thus, we choose to use a policy gradient method for our learning system among other possible reinforcement learning methods.</p><p>This paper is organized as follows. In Section 2, we introduce a central pattern generator which is used for generation of walking behavior in this study. In Section 3, we describe a policy gradient reinforcement learning method for a CPG-based biped locomotion controller. In Section 4, we present the proposed control architecture for our 5-link robot to achieve biped walking behavior. In Section 5, first, we demonstrate the effectiveness of the proposed learning framework by numerical simulations. Then, we investigate linear stability of a periodic orbit of the acquired walking pattern considering its approximated return map. In Section 6, we present experimental results suggesting that successful biped walking with our physical 5-link robot can be achieved using the controller learned in numerical simulations. Moreover, we analyze convergence properties to steady-state walking with variations in initial conditions based on a return map, and experimentally demonstrate the robustness of the acquired walking against environmental changes and parametric changes in the robot dynamics. In Section 7, we summarize this paper and discuss the approaches of policy search in motor learning and control. Finally, we address open problems and future work.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Central pattern generator</head><p>The CPG-based controller is composed of a neural oscillator model and a sensory feedback controller which maps the states of the robot to the input to the neural oscillator model. In Section 2.1, we present the neural oscillator model. Then, in Section 2.2, we introduce sensory feedback to the neural oscillator model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.">Neural oscillator model</head><p>We use a neural oscillator model proposed by Matsuoka <ref type="bibr" target="#b0">[1]</ref>. The oscillator dynamics of i-th unit are:</p><formula xml:id="formula_0">τ CPG żi (t) = -z i (t) - n j=1 w i j q j (t) -βp i (t) + z 0 + v i (t), (1) τ CPG ṗi (t) = -p i (t) + q i (t),<label>(2)</label></formula><formula xml:id="formula_1">q i (t) = max(0, z i (t)), (<label>3</label></formula><formula xml:id="formula_2">)</formula><p>where n is the number of neurons, z i (t) and p i (t) are internal states of a CPG. τ CPG and τ CPG are time constants for the internal states. w i j is a inhibitory synaptic weight from the j-th neuron to the i-th neuron. z 0 is a bias. v i (t) is a feedback signal which will be defined in (4) below.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.">Sensory feedback</head><p>The feedback signal to the neural oscillator model v i (t) in Eq. ( <ref type="formula">1</ref>) is given by</p><formula xml:id="formula_3">v i (t) = v max i g(a i (t)),<label>(4)</label></formula><p>where g(a i ) = 2 π arctan π 2 a i , and v max i is the maximum value of the feedback signal. The output of the feedback controller a = (a 1 , . . . , a m ) T is sampled from a stochastic policy:</p><formula xml:id="formula_4">π(x, a; W µ , w σ ) = 1 ( √ 2π ) m |D(w σ )| 1 2 exp - 1 2 (a -µ(x; W µ )) T D -1 × (w σ )(a -µ(x; W µ )) ,<label>(5)</label></formula><p>where x are partial states of the robot. W µ is the m×k parameter matrix, w σ is the m-dimensional parameter vector of the policy, where m is the number of outputs, and k is the number of parameters. µ(x; W µ ) is the mean vector of the policy. The covariance matrix D is defined as D(w σ ) = S T (w σ )S(w σ ). We can equivalently represent a by</p><formula xml:id="formula_5">a(t) = µ(x(t); W µ ) + S(w σ )n(t),<label>(6)</label></formula><p>where n(t) ∈ R m is a noise vector and n i (t) is sampled from the normal distribution with the mean of 0 and the variance of 1. Note that the matrix S(w σ ) must be chosen such that D(w σ ) is positive definite.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Learning sensory feedback to CPG with a policy gradient method</head><p>We describe the use of a policy gradient method in order to acquire a policy of the sensory feedback controller to the neural oscillator model. In Section 3.1, we first define the value function and temporal difference error (TD error) in continuous time and space <ref type="bibr" target="#b15">[16]</ref>, which is used in the policy gradient method. In Section 3.2, we describe the learning method to improve the policy of the sensory feedback controller.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Learning the value function</head><p>Consider a continuous-time system which represents both the robot and the CPG dynamics, dx all (t) dt = f (x all (t), a(t)), <ref type="bibr" target="#b6">(7)</ref> where x all ∈ X ⊂ R l consists of the state of the robot and the CPG, and a ∈ A ⊂ R m is the output of the feedback controller, that is, input to the CPG. We denote the immediate reward as r (t) = r (x all (t), a(t)).</p><p>(</p><formula xml:id="formula_6">)<label>8</label></formula><p>The value function of the state x all (t) based on a policy π(x all , a) is defined as</p><formula xml:id="formula_7">V π (x all (t)) = E ∞ t e -s-t τ r (x all (s), a(s))ds π , (<label>9</label></formula><formula xml:id="formula_8">)</formula><p>where τ is a time constant for discounting future rewards. The consistency condition for the value function is given by the time derivative of (9) as</p><formula xml:id="formula_9">dV π (x all (t)) dt = 1 τ V π (x all (t)) -r (t).<label>(10)</label></formula><p>We denote a current estimate of the value function as V (x all (t)) = V (x all (t); w c ), where w c is the parameter of the function approximator. If the current estimate of the value function V is perfect, it should satisfy the consistency condition <ref type="bibr" target="#b9">(10)</ref>. If this condition is not satisfied, the prediction should be adjusted to decrease the inconsistency</p><formula xml:id="formula_10">δ(t) = r (t) - 1 τ V (t) + V (t).<label>(11)</label></formula><p>This is the continuous-time counterpart of the TD error <ref type="bibr" target="#b15">[16]</ref>.</p><p>Because we consider a learning framework in POMDPs, i.e., we observe only the partial state x of the state x all , the TD error does not usually converge to zero. However, Kimura et al. <ref type="bibr" target="#b7">[8]</ref> suggested that the approximated value function can be useful to reduce the variance of the gradient estimation in (13) even if the consistency condition in <ref type="bibr" target="#b9">(10)</ref> is not satisfied. The update laws for the parameter vector of the value function w c and the eligibility trace vector e c for w c are defined respectively as</p><formula xml:id="formula_11">ẇc (t) = αδ(t)e c (t), ėc (t) = - 1 κ c e c (t) + ∂ V w c ∂w c , (<label>12</label></formula><formula xml:id="formula_12">)</formula><p>where α is the learning rate and κ c is the time constant of the eligibility trace.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Learning a policy of the sensory feedback controller</head><p>In <ref type="bibr" target="#b7">[8]</ref>, Kimura et al. presented that by using TD error δ(t) and eligibility trace vector e a (t), it is possible to obtain an estimate of the gradient of the expected actual return V t with respect to the parameter vector w a in the limit of where</p><formula xml:id="formula_13">κ a = τ as ∂ ∂w a E {V t | π w a } = E{δ(t)e a (t)},<label>(13)</label></formula><formula xml:id="formula_14">V t = ∞ t e -s-t τ r (s)ds, (<label>14</label></formula><formula xml:id="formula_15">)</formula><p>w a is the parameter vector of the policy π w a = π(x, a; w a ), and e a (t) is the eligibility trace vector for the parameter vector w a . The parameter vector w a is represented as</p><formula xml:id="formula_16">w a = (w µ 1 T , . . . , w µ m T , w σ T ) T ,</formula><p>where w µ j is the j-th column vector of the parameter matrix W µ . The update laws for the parameter vector of the policy w a and the eligibility trace vector e a (t) can be derived respectively as</p><formula xml:id="formula_17">ẇa (t) = βδ(t)e a (t), ėa (t) = - 1 κ a e a (t) + ∂ ln π w a ∂w a (<label>15</label></formula><formula xml:id="formula_18">)</formula><p>where β is the learning rate and κ a is the time constant of the eligibility trace. In the case of κ a = τ ≈ ∞, the actual return used as a criteria for the policy improvement in this algorithm is similar to the average reward criteria widely used in other policy gradient methods <ref type="bibr" target="#b8">[9]</ref><ref type="bibr" target="#b9">[10]</ref><ref type="bibr" target="#b10">[11]</ref>. (See Section 7.3 for more details.)</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Control architecture for 5-link robot</head><p>In this paper, we use a planar 5-link biped robot (Fig. <ref type="figure">1</ref>) developed in <ref type="bibr" target="#b16">[17]</ref>. The experimental setting is depicted in Fig. <ref type="figure" target="#fig_0">2</ref>. The height of the robot is 0.4 m and the total mass is about 2 kg. The length of each link of the leg is 0.2 m. The masses of the body, thigh and shank are 1.0 kg, 0.43 kg and 0.05 kg, respectively. The motion of the robot is constrained within the sagittal plane which is defined as shown in Fig. <ref type="figure">1</ref> (right) by a tether boom. The hip joints are directly actuated by direct drive motors, and the knee joints are driven by motors through a wire transmission mechanism with a reduction ratio of 2.0. These transmission mechanisms with low reduction ratio provide high back drivability at the joints. Foot contact with the ground is detected by foot switches. The robot is an underactuated system having rounded soles with no ankles. Thus, it is challenging to design a controller to achieve biped locomotion with this robot since no actuation can be applied between the stance leg and the ground unlike many of the existing biped robots which have flat feet with ankle joint actuation. In the following, we denote the left hip and knee angles by θ l hip and θ l knee , respectively. Similar definitions are also applied to the joint angles of the right leg. Fig. <ref type="figure" target="#fig_1">3</ref> illustrates our control architecture for the biped robot, which consists of the CPG-based controller for the hip joints and the state-machine controller for the knee joints. Section 4.1 presents a CPG-based controller which generates periodic walking patterns. Section 4.2 presents a state-machine controller which makes foot clearance at appropriate timing according to the state of the hip joint and the foot contact information with the ground.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">CPG-based controller for the hip joints</head><p>In the proposed control architecture, the hip joints of the robot are driven by the CPG-based controller described in Section 2. The hip joint controller is composed of four neurons (i = 1, . . . , 4) in Eqs. ( <ref type="formula">1</ref>)-( <ref type="formula" target="#formula_1">3</ref>): i = 1: extensor neuron for left hip, i = 2: flexor neuron for left hip, i = 3: extensor neuron for right hip, i = 4: flexor neuron for right hip. For the sensory feedback (5), we consider the states of the hip joints x = (θ l hip + θ p , θl hip + θp , θ r hip + θ p , θr hip + θp ) T as the input states. The target joint angle for the hip joint is determined by the oscillator output q i :</p><formula xml:id="formula_19">θl hip = -q 1 + q 2 , θr hip = -q 3 + q 4 . (<label>16</label></formula><formula xml:id="formula_20">)</formula><p>The torque output u at each hip joint is given by a PD controller:</p><formula xml:id="formula_21">u hip = K hi p p ( θhip -θ hip ) -K hi p d θhip ,<label>(17)</label></formula><p>where K hi p p is a position gain and K hi p d is a velocity gain.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">State-machine controller for the knee joints</head><p>We design a state-machine controller for the knee joints as depicted in Fig. <ref type="figure" target="#fig_2">4</ref>. The state-machine controller changes the pre-designed target joint angles for the knee joints according to transition conditions defined by the hip joint angles and the foot contact information with the ground. The torque command to each knee joint is given by a PD controller: where K knee p is a position gain and K knee d is a velocity gain. We define four target joint angles, θ 1 , . . . , θ 4 , for the state-machine controller (Fig. <ref type="figure" target="#fig_2">4</ref>). We use the hip joint angles and the foot contact information to define the transition conditions of the state-machine controller. The transition conditions defined by the hip joint angles are given by θ l hip -θ r hip &lt; b or θ r hip -θ l hip &lt; b, where b is a threshold of the transition conditions.</p><formula xml:id="formula_22">u knee = K knee p ( θknee -θ knee ) -K knee d θknee ,<label>(18)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Numerical simulations</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.">Simulation setup</head><p>In Section 5.1.1, we present function approximators for the value function and policy of sensory feedback to CPG. In Section 5.1.2, we describe reward function designed to achieve biped walking though learning. In Section 5.1.3, we present the parameter settings in the controller used for the simulations. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.1.">Function approximator for the value function and the policy</head><p>We use a normalized Gaussian Network (NGnet) <ref type="bibr" target="#b15">[16]</ref> to model the value function and the mean of the policy. This function approximator was used in the authors' previous studies of reinforcement learning in continuous time and space, and shown to be effective in the examples of a swing-up task of an inverted pendulum and a dynamic stand-up behavior of a real robot <ref type="bibr" target="#b15">[16,</ref><ref type="bibr" target="#b17">18]</ref>. By this way, it is possible to achieve smooth control compared to the tile-coding approach often used in discrete reinforcement learning <ref type="bibr" target="#b11">[12]</ref>. In addition, practical feasibility of this function approximator was demonstrated for real-time implementation of the control policy on a hardware robot to achieve the desired behavior <ref type="bibr" target="#b17">[18]</ref>. The variance of the policy is modeled by a sigmoidal function <ref type="bibr" target="#b7">[8]</ref>. The value function is approximated with the NGnet:</p><formula xml:id="formula_23">V (x; w c ) = w cT b(x)<label>(19)</label></formula><p>where b</p><formula xml:id="formula_24">(x) = (b 1 (x), b 2 (x), . . . , b K (x)) T , b k (x) = φ k (x) K l=1 φ l (x) and φ k (x) = e -s T k (x-c k ) 2 . (<label>20</label></formula><formula xml:id="formula_25">)</formula><p>K is the number of the basis functions, and w c is the parameter vector of value function. The vectors c k and s k define the center and the size of the k-th basis function, respectively. The mean µ and the covariance matrix D of the policy are represented with the NGnet and the sigmoidal function, respectively:</p><formula xml:id="formula_26">µ(x; W µ ) = W µT b(x), D(w σ ) = S T (w σ )S(w σ ),<label>(21)</label></formula><p>where S(w σ ) = diag(σ 1 , σ 2 , σ 3 , σ 4 ),</p><formula xml:id="formula_27">σ i = 1 1 + exp(-w σ i )</formula><p>and</p><formula xml:id="formula_28">w σ = (w σ 1 , w σ 2 , w σ 3 , w σ 4 ) T . (<label>22</label></formula><formula xml:id="formula_29">)</formula><p>We locate basis functions φ k (x) on a grid with an even interval in each dimension of the input space</p><formula xml:id="formula_30">(-π 3 ≤ θ l hip + θ p ≤ π 3 , -3.0π ≤ θl hip + θp ≤ 3.0π, -π 3 ≤ θ r hip + θ p ≤ π 3 , -3.0π ≤ θr hip + θp ≤ 3.0π</formula><p>). We used 9216 (=12 × 8 × 12 × 8) basis functions to approximate the value function and the mean of the policy respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.2.">Rewards</head><p>We used the following simple reward function:</p><formula xml:id="formula_31">r = k ν max(0, ν), (<label>23</label></formula><formula xml:id="formula_32">)</formula><p>where the reward is designed to encourage forward progress of the robot by giving a reward proportional to the forward velocity of walking ν. In this study, the parameter for the reward is chosen as k ν = 0.05. The robot also receives a punishment (negative reward) r = -1 for 0.5 s if it falls over. = 0.07 N m s/rad, respectively. These CPG parameters were roughly tuned to achieve some desirable natural frequency and amplitude through numerical simulations. However, note that as seen in Fig. <ref type="figure" target="#fig_6">6</ref>, the robot cannot walk only with the CPG, i.e., appropriate learned sensory feedback is necessary for successful walking. Moreover, we will demonstrate that choice of CPG parameters does not significantly affect the performance of learning in our proposed framework (see Section 5. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.">Simulation results</head><p>In the following simulations, the initial posture of the robot is determined as θ l hip = 5.5 • , θ r hip = -5.5 • , θ p = 0.0 • , θ l knee = 20.5 • , θ r knee = 0.0 • (see the definition of each angle in Fig. <ref type="figure" target="#fig_1">3</ref>) and the initial velocity of the robot is randomly sampled from an uniform distribution between 0.05 m/s and 0.20 m/s. In these simulations, we define that a learning episode is successful when the biped robot does not fall over for 10 successive trials. We applied the policy gradient method with these settings to the biped robot. Fig. <ref type="figure" target="#fig_3">5</ref> (solid line) shows an accumulated reward at each trial with the policy gradient method. An appropriate feedback controller of the CPG-based controller was acquired in 181 trials (averaged over 50 experiments). Fig. <ref type="figure" target="#fig_6">6(a)</ref> shows the initial walking pattern before learning, where the robot falls over after a few steps. Fig. <ref type="figure" target="#fig_6">6</ref>(b) shows an acquired walking pattern at the 1000-th trial with the learned sensory feedback of the CPG-based controller.</p><p>As a comparison, we also implemented a value-functionbased reinforcement learning method proposed in <ref type="bibr" target="#b15">[16]</ref>. The result is also presented in Fig. <ref type="figure" target="#fig_3">5</ref> (dash-dot line). Although the value-function-based RL could also acquire appropriate biped walking controllers, it required a larger number of trials compared with the policy gradient method (1064 trials was required with the value-function-based RL on average). Moreover, we observed that the learning was unstable with higher learning rate in updating of policy parameters with the value-function-based RL. The result is consistent to a consideration that value-function-based reinforcement learning methods are not suitable for POMDPs as pointed out in <ref type="bibr" target="#b12">[13]</ref>. This point will be discussed in Section 7.3 in more detail.</p><p>We observed a large phase difference between the target and actual trajectories in the hip joints while the knee joint trajectories achieved good tracking of the target. This is due to the choice of low PD gains for the hip joints. Despite this large phase difference between the target and actual hip joint trajectories, the robot could achieve successful walking. This suggests that our method does not necessarily require very accurate tracking with a high gain servo which is typically used in model-based trajectory planning approaches <ref type="bibr" target="#b18">[19]</ref><ref type="bibr" target="#b19">[20]</ref><ref type="bibr" target="#b20">[21]</ref>.</p><p>In order to investigate sensitivity of learning against the changes in the CPG parameters, we varied the CPG parameters which characterize the frequency (τ CPG , τ CPG ) and amplitude (z 0 ) from the values chosen above by ±25%, respectively. In all cases, we could acquire successful walking within 1000 trials. We did not observe significant differences in the resultant walking with the learned feedback controller even with these varied CPG parameters. This suggests that careful tuning of CPG parameters is not a prerequisite in our learning framework.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3.">Linear stability of a periodic orbit in learned biped walking</head><p>In this section, we analyze linear stability of a periodic orbit in learned biped walking around a fixed point using a return map <ref type="bibr" target="#b21">[22]</ref>. The return map is defined as a mapping of the states of the robot and CPG from the 4th step to the 6th step when the right hip is in swinging phase and the angle is 0.2 rad. The return map is an 18 dimensional mapping which consists of the states of the robot and CPG except for the walking distance of the robot. Initial velocity of the robot was randomly sampled from an uniform distribution between 0.05 m/s and 0.15 m/s, introducing perturbations in each dimension.</p><p>We analyzed the linearized return map which was approximated using 1500 sampled data, and confirmed all eigenvalues were inside of the unit circle. The result implies that the periodic biped walking is locally stable around the fixed point.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Hardware experiments</head><p>In this section, we implement the proposed control architecture on the physical biped robot depicted in Fig. <ref type="figure">1</ref>. We use the same parameters for the CPG and state-machine and also the same PD gains as used in the numerical simulations presented in Section 5.1. In the state-machine controller, a lowpass filter, with the time constant of 0.03 s, is used to avoid discontinuous change in the target angles of the knee joint, which is practically undesirable. To initiate locomotion in the experiments, we first suspend the robot with the legs swinging in the air, and then place the robot on the ground manually. Thus, the initial condition of each run was not consistent. Occasionally, the robot could not start walking or fell over after a couple of steps when the timing was not appropriate.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.1.">Walking performance of the learned controller in the real environment</head><p>We implemented ten feedback controllers acquired in the numerical simulations, and then we confirmed that seven controllers out of ten successfully achieved biped locomotion with the physical robot. Fig. <ref type="figure">7</ref> shows the walking pattern without a learned feedback controller, and Fig. <ref type="figure" target="#fig_7">8</ref> shows snapshots of a walking pattern using one of the feedback controllers. Fig. <ref type="figure" target="#fig_10">11</ref> presents trajectories of a successful walking pattern at each joint in the right foot.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.2.">Convergence property from various initial conditions</head><p>The robot could achieve biped walking even though the initial conditions in these experiments were not consistent. In order to investigate the convergence property to steady-state walking with variations in initial conditions, we analyze the linear stability of a periodic orbit in learned biped walking Fig. <ref type="figure">7</ref>. Initial walking pattern without a feedback controller. The robot could not walk.   around a fixed point using its return map <ref type="bibr" target="#b21">[22]</ref>. We consider a one dimensional return map with respect to the successive step length d defined as a distance between the right and left foot when the right leg touches down with the ground. In Fig. <ref type="figure" target="#fig_11">12</ref>, we plot the return map obtained in the experiments. The absolute value of the slope of the return map is 0.82, which is less than 1. The result implies that walking with the physical robot converges to steady-state walking even if the initial conditions are not consistent.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.3.">Robustness of the learned controllers</head><p>We experimentally investigate the robustness of the learned controller against environmental changes and parametric changes in the robot dynamics. As an example of an environmental change, we placed a seesaw-like metal sheet with a slight change in the slope on the ground (Fig. <ref type="figure" target="#fig_8">9</ref>). As an example of a parametric change in the robot dynamics, we added a weight (150 g) on the right shank (Fig. <ref type="figure" target="#fig_9">10</ref>), which is about 38% increase of right leg mass. Figs. 9 and 10 suggest the robustness of the learned walking against environmental changes and parametric changes in the robot dynamics, respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.">Discussion</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.1.">Summary</head><p>In this paper, we presented a learning framework for a CPGbased biped walking controller with a policy gradient method. Numerical simulations demonstrated that an appropriate sensory feedback controller to the CPG could be acquired with the proposed learning architecture to achieve the desired walking behavior. We showed that the acquired walking was a locally stable periodic orbit based on a linearized return map around a fixed point. We also implemented the learned controller on the physical 5-link biped robot. We analyzed the convergence property of the learned walking to steadystate walking with variations in initial conditions based on a return map, and experimentally demonstrated the robustness of the learned controller against environmental changes and parametric changes in robot dynamics such as placing a seesawlike metal sheet on the ground and adding a weight. As a immediate next step, we address improvement of the acquired controller by additional learning with the physical robot.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.2.">Issues in motor skill learning with reinforcement learning</head><p>In this study, our general interest is in the acquisition of motor skills or dynamic behavior of complex robotic systems such as humanoid robots. This paper has focused on the development of a learning framework for a simple biped robot to achieve the desired walking behavior. Among learning motor skill problems, in particular, learning biped locomotion is a challenging task which involves dynamic interaction with an environment and it is desirable that the controller be robust enough to deal with uncertainties of the environment and unexpected disturbances.</p><p>Model-based approaches for motion generation of biped robots have been successfully demonstrated to be effective <ref type="bibr" target="#b18">[19]</ref><ref type="bibr" target="#b19">[20]</ref><ref type="bibr" target="#b20">[21]</ref>. However, they typically require precise modeling of the dynamics of the robot and the structure of the environment. Thus, we employed our proposed CPG-based control framework with a policy gradient method which does not require such precise models to achieve robust locomotion in an unstructured environment. Our empirical results demonstrated the effectiveness of the proposed approach. However, in general, there are difficulties in the application of the reinforcement learning framework to motor skill learning with robotic systems. First, in motor control, it is desirable to use smooth continuous actions, i.e., the output of the policy should be smooth and continuously computed from the current state which is typically measured by sensors in the real robotic systems. Previously, in many applications of reinforcement learning, discretization techniques have been widely used <ref type="bibr" target="#b11">[12]</ref>. However, as pointed out in <ref type="bibr" target="#b15">[16]</ref>, coarse discretization may result in poor performance, and fine discretization would require a large number of states and iteration steps. Thus, in order to deal with continuous state and action, we find it useful to use function approximators. Moreover, the use of algorithms derived in continuous time is also suitable for such dynamical systems <ref type="bibr" target="#b15">[16]</ref>. Second, when considering hardware implementation of the policy for robot control, calculation of motor commands needs to be done in real-time. Thus, computationally efficient representation of the policy should be considered. To our knowledge, there have been few successful applications of a reinforcement learning framework to motor skill learning with physical robotic systems <ref type="bibr" target="#b13">[14,</ref><ref type="bibr" target="#b17">18]</ref> in which the dimensionality of the systems is still relatively small. In this research, we used a CPG-based controller to achieve robust biped walking with a rather high dimensional system. The use of a CPG-based controller also makes learning of such a complex motor skill much simpler by introducing a periodic rhythm. However, still other alternative approaches and algorithms can be considered. In the following section, we discuss several possible policy search approaches which might be applicable to the learning problem in this paper.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.3.">Comparison to alternative policy search approaches</head><p>In this paper, we adopted a policy gradient method proposed in <ref type="bibr" target="#b7">[8]</ref> as a method for policy search in a CPG-based locomotion controller. This section discusses possible alternative policy search approaches, for example, genetic algorithms <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b22">23]</ref>, value-function-based reinforcement learning methods <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b15">16]</ref>, and other policy gradient algorithms such as GPOMDP <ref type="bibr" target="#b9">[10]</ref> and IState-GPOMDP <ref type="bibr" target="#b23">[24]</ref>.</p><p>Genetic algorithms (GAs) are an optimization method inspired by evolution processes. This method is known to be effective for complex search problems (typically discrete problems) in a large space, and also was applied to policy search in biped locomotion <ref type="bibr" target="#b4">[5]</ref> and locomotion of a snakelike robot <ref type="bibr" target="#b22">[23]</ref>. However, the optimization process does not use the gradient information which is useful to determine how to improve the policy in a systematic manner. Also, there are a number of open design parameters, for example, the number of individuals and the probability of mutation, which need to be determined somewhat in a heuristic manner. Moreover, there is a problem of policy coding-it is not clear how to represent a policy in an appropriate way for the given problem.</p><p>Value function based reinforcement learning (RL) methods have been successfully applied to many policy search problems <ref type="bibr" target="#b15">[16,</ref><ref type="bibr" target="#b17">18,</ref><ref type="bibr" target="#b24">25,</ref><ref type="bibr" target="#b25">26]</ref>. However, value function based RL assumes MDPs (Markov decision process) in which all the states are observable, and it is not suitable for POMDPs as pointed out in <ref type="bibr" target="#b12">[13]</ref>. In fact, we performed additional numerical simulations to test a value function based RL for the locomotion task in the same simulation settings which is conceived of as a POMDP (see the result in Fig. <ref type="figure" target="#fig_3">5</ref>). However, the value function based RL<ref type="foot" target="#foot_0">1</ref> needed a larger number of trials to acquire an appropriate feedback controller compared with the policy gradient method <ref type="bibr" target="#b7">[8]</ref> in this POMDP environment. There is still a possibility to consider full state observation including all the robot states and the internal states of the CPG to make the learning problem of biped locomotion be an MDP. However, due to the significant increase of dimensionality of the state space, it is computationally too expensive for real-time implementation in a hardware system in the current settings. These observations above indicate that policy search algorithms which are capable of handling the POMDP situation would be preferable.</p><p>Policy gradient methods are policy search techniques which are suitable for POMDPs <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b7">8,</ref><ref type="bibr" target="#b9">10]</ref>. In this paper, we chose to use the policy gradient algorithm proposed in <ref type="bibr" target="#b7">[8]</ref> as a policy search method, which has been empirically shown to be effective as a learning method for physical legged robotic systems <ref type="bibr" target="#b13">[14,</ref><ref type="bibr" target="#b14">15]</ref> in POMDPs. We would like to mention that this algorithm is essentially equivalent to another policy gradient algorithm, GPOMDP, developed by Baxter <ref type="bibr" target="#b9">[10]</ref>. Although objective functions used in Kimura's algorithm (expected actual return) and Baxter's GPOMDP algorithm (average reward) are different, <ref type="bibr" target="#b9">[10]</ref> shows that the gradients of the expected actual return is proportional to the gradient of the average reward. Both Kimura's formulation and Baxter's formulation obtain a gradient of the average reward with respect to the policy parameters if the probability distribution of all the state and action is known, i.e., the environment is completely known. However, in practice, we need to estimate the environment's dynamics from sampled data when there is no prior knowledge of the environment. In such a case, Kimura's algorithm which uses an approximated value function as the reward baseline (introduced in <ref type="bibr" target="#b6">[7]</ref>) is empirically shown to be advantageous in reducing the variance of the estimation of the policy gradient <ref type="bibr" target="#b7">[8]</ref>.</p><p>Finally, we would like to mention the internal-state policy gradient algorithm for POMDP (IState-GPOMDP) which has internal states with memory as an extension of the GPOMDP algorithm <ref type="bibr" target="#b23">[24]</ref>. Conceptually, this framework has a similarity to the structure of our learning system with the CPG in that it contains internal states. Thus, there might be a potential possibility to optimize the parameters of the learning system including the mapping from the oscillator output to the torque at hip joints, which was implemented by a pre-designed PD controller in <ref type="bibr" target="#b16">(17)</ref>. However, learning additional parameters would be computationally too expensive due to the complex representation of the entire policy, and therefore would not be suitable for real-time implementation in a hardware system in the current settings.</p><p>Although policy gradient methods are generally considered to be suitable for POMDPs, these methods find a local optimal solution only within the parameter space of the state-dependent policy designed in advance. In this study, we manually selected the partial states (only hip joint states) for the policy from all the states including the robot and CPG. Because of this simplification, the real-time implementation was achieved in the current settings. On the other hand, this simplification might reduce the performance of the resultant policy acquired though learning. One of the key factors for successful learning in this study was the choice of those partial states selected by our intuition, which are likely to be dominant states in our proposed CPG-based biped locomotion controller. If a different simplification is introduced for this learning task, for example if CPG's internal states are only used for the learning, acquired controllers may not be good enough to achieve biped walking.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.4.">Open problems and future work</head><p>With the recent progress in the theoretical studies of policy gradient methods and development of advanced algorithms, it is possible to improve the policy for a given reward using a policy gradient method towards a local optimal policy. One's hope is that the desired task or behavior can be achieved if the reward is chosen appropriately. In this paper, we have successively achieved learning CPG-based biped walking with a robot which is considered as a high dimensional system for learning using a policy gradient method by using simple reward in <ref type="bibr" target="#b22">(23)</ref>. However, it is not clear how to choose a reward which best describes the desired task. This remains still an open problem not only in the policy gradient method but also in the reinforcement learning framework in general. The direction of our future work is towards development of an efficient learning algorithm for integration of dynamic behaviors, e.g., combination of walking, balancing and reactive motions against unexpected disturbances, in highly complex systems such as full body humanoid robots addressing the problem above.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 2 .</head><label>2</label><figDesc>Fig. 2. Experimental setting.</figDesc><graphic coords="3,316.25,66.76,241.58,122.45" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 3 .</head><label>3</label><figDesc>Fig. 3. Proposed control architecture for 5-link biped robot.</figDesc><graphic coords="4,148.72,66.40,287.65,181.36" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 4 .</head><label>4</label><figDesc>Fig. 4. State transition 1 ∼ 4 in the state-machine controller for knee joints.</figDesc><graphic coords="4,306.72,277.42,240.85,210.97" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Fig. 5 .</head><label>5</label><figDesc>Fig. 5. Accumulated reward at each trial: averaged by 50 experiments and smoothed out by taking a 10-moving average. Error bar is the standard deviation. Solid line: policy gradient method. Dash-dot line: value-functionbased RL.</figDesc><graphic coords="5,47.93,66.76,240.24,170.37" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>5. 1 . 3 .</head><label>13</label><figDesc>Parameters for the controllers Parameters of the neural oscillators used in (1)-(3) are τ CPG = 0.041, τ CPG = 0.36, β = 2.5, z 0 = 0.4, w 12 = w 21 = w 34 = w 43 = 2.0, w 13 = w 31 = w 24 = w 42 = 1.0. Initial values of the internal states are given by z 1 (0) = 0.05, z 2 (0) = 0.05. We select the learning parameters as τ = 1.0, α = 50, β µ = 20, β σ = 10, κ c = 0.1, κ µ = 1.0, κ σ = 1.0. PD gains for hip joints are set to K hi p p = 4.0 N m/rad and K hi p d</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head></head><label></label><figDesc>2 below). Parameters of the state-machine are θ 1 = 32 • , θ 2 = 16 • , θ 3 = 15 • , θ 4 = 7.5 • and b = 8.6 • . PD gains for knee joints are chosen as K knee p = 8.0 N m/rad and K knee d = 0.09 N m s/rad, respectively.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Fig. 6 .</head><label>6</label><figDesc>Fig. 6. Acquired biped walking pattern: (a) before learning, (b) after learning. The arrow indicates the direction of walking.</figDesc><graphic coords="6,37.33,66.76,241.80,119.32" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Fig. 8 .</head><label>8</label><figDesc>Fig. 8. Successful walking pattern with a learned feedback controller in numerical simulation with 1000 trials.</figDesc><graphic coords="7,97.29,66.76,410.52,85.32" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Fig. 9 .</head><label>9</label><figDesc>Fig. 9. Example of an environmental change. Walking pattern on a seesaw-like metal sheet.</figDesc><graphic coords="7,97.29,184.69,410.52,85.32" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>Fig. 10 .</head><label>10</label><figDesc>Fig. 10. Example of a parametric change of the robot dynamics. Walking pattern with an additional weight of 150 g on the right shank.</figDesc><graphic coords="7,323.54,418.00,226.80,232.57" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head>Fig. 11 .</head><label>11</label><figDesc>Fig. 11. Joint angles and sensory feedback signals of successful walking with the physical robot using a controller acquired in numerical simulations. The top and second plots are joint trajectories of the right hip and knee, respectively. The third and the bottom plots show sensory feedback signals corresponding to the extensor and flexor neurons for the right hip joint, respectively.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_11"><head>Fig. 12 .</head><label>12</label><figDesc>Fig. 12. The linearized return map of acquired walking with the physical robot. d is a step length when the right leg touches down with the ground. The thick line is the return map from d n to d n+1 , and the thin line represents the identity map.</figDesc><graphic coords="8,63.64,66.76,189.20,177.11" type="bitmap" /></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0"><p>We used the value function based RL in continuous time and state proposed in<ref type="bibr" target="#b15">[16]</ref> and also used it for the real robot control in<ref type="bibr" target="#b17">[18]</ref>.</p></note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Sustained oscillations generated by mutually inhibiting neurons with adaptation</title>
		<author>
			<persName><forename type="first">K</forename><surname>Matsuoka</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Biological Cybernetics</title>
		<imprint>
			<biblScope unit="volume">52</biblScope>
			<biblScope unit="page" from="367" to="376" />
			<date type="published" when="1985">1985</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Self-organized control of bipedal locomotion by neural oscillators in unpredictable environment</title>
		<author>
			<persName><forename type="first">G</forename><surname>Taga</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Yamaguchi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Shimizu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Biological Cybernetics</title>
		<imprint>
			<biblScope unit="volume">65</biblScope>
			<biblScope unit="page" from="147" to="159" />
			<date type="published" when="1991">1991</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Adaptive dynamic walking of a quadruped robot on irregular terrain based on biological concepts</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Fukuoka</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Kimura</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Cohen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The International Journal of Robotics Research</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="issue">3-4</biblScope>
			<biblScope unit="page" from="187" to="202" />
			<date type="published" when="2003">2003</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">An empirical exploration of a neural oscillator for biped locomotion control</title>
		<author>
			<persName><forename type="first">G</forename><surname>Endo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Morimoto</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Nakanishi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Cheng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Robotics and Automation</title>
		<imprint>
			<date type="published" when="2004">2004</date>
			<biblScope unit="page" from="3036" to="3042" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Computer simulation of the ontogeny of biped walking</title>
		<author>
			<persName><forename type="first">K</forename><surname>Hase</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Yamazaki</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Anthropological Science</title>
		<imprint>
			<biblScope unit="volume">106</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="327" to="347" />
			<date type="published" when="1998">1998</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Reinforcement learning for biped locomotion</title>
		<author>
			<persName><forename type="first">M</forename><surname>Sato</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Nakamura</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Ishii</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Artificial Neural Networks</title>
		<imprint>
			<date type="published" when="2002">2002</date>
			<biblScope unit="page" from="777" to="782" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Simple statistical gradient-following algorithms for connectionist reinforcement learning</title>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">J</forename><surname>Williams</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Machine Learning</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page" from="229" to="256" />
			<date type="published" when="1992">1992</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">An analysis of actor/critic algorithms using eligibility traces: Reinforcement learning with imperfect value function</title>
		<author>
			<persName><forename type="first">H</forename><surname>Kimura</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Kobayashi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Internal Conference on Machine Learning</title>
		<imprint>
			<biblScope unit="page" from="278" to="286" />
			<date type="published" when="1998">1998</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Policy gradient methods for reinforcement learning with function approximation</title>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">S</forename><surname>Sutton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Mcallester</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Mansour</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in Neural Information Processing Systems</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="page" from="1057" to="1063" />
			<date type="published" when="2000">2000</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Infinite-horizon policy-gradient estimation</title>
		<author>
			<persName><forename type="first">J</forename><surname>Baxter</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">L</forename><surname>Bartlett</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Artificial Intelligence Research</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="page" from="319" to="350" />
			<date type="published" when="2001">2001</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">On actor-critic algorithms</title>
		<author>
			<persName><forename type="first">V</forename><surname>Konda</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Tsitsiklis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Society for Industrial and Applied Mathematics</title>
		<imprint>
			<biblScope unit="volume">42</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="1143" to="1166" />
			<date type="published" when="2003">2003</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">S</forename><surname>Sutton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">G</forename><surname>Barto</surname></persName>
		</author>
		<title level="m">Reinforcement Learning: An Introduction</title>
		<imprint>
			<publisher>MIT Press</publisher>
			<date type="published" when="1998">1998</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Learning without state-estimation in partially observable markovian decision processes</title>
		<author>
			<persName><forename type="first">S</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Jaakkola</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Jordan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Machine Learning: Proceedings of the Eleventh International Conference</title>
		<imprint>
			<date type="published" when="1994">1994</date>
			<biblScope unit="page" from="284" to="292" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Reinforcement learning of walking behavior for a four-legged robot</title>
		<author>
			<persName><forename type="first">H</forename><surname>Kimura</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Yamashita</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Kobayashi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Decision and Control</title>
		<meeting>the IEEE Conference on Decision and Control</meeting>
		<imprint>
			<date type="published" when="2001">2001</date>
			<biblScope unit="page" from="411" to="416" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<author>
			<persName><forename type="first">R</forename><surname>Tedrake</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">W</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">S</forename><surname>Seung</surname></persName>
		</author>
		<title level="m">Proceedings of the IEEE International Conference on Intelligent Robots and Systems</title>
		<meeting>the IEEE International Conference on Intelligent Robots and Systems</meeting>
		<imprint>
			<date type="published" when="2004">2004</date>
			<biblScope unit="page" from="2849" to="2854" />
		</imprint>
	</monogr>
	<note>Stochastic policy gradient reinforcement learning on a simple 3D biped</note>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Reinforcement learning in continuous time and space</title>
		<author>
			<persName><forename type="first">K</forename><surname>Doya</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural Computation</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="page" from="219" to="245" />
			<date type="published" when="2000">2000</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Minimax differential dynamic programming: Application to a biped walking robot</title>
		<author>
			<persName><forename type="first">J</forename><surname>Morimoto</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Zeglin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Atkeson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/RSJ International Conference on Intelligent Robots and Systems</title>
		<meeting>the IEEE/RSJ International Conference on Intelligent Robots and Systems</meeting>
		<imprint>
			<date type="published" when="2003">2003</date>
			<biblScope unit="page" from="1927" to="1932" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Acquisition of stand-up behavior by a real robot using hierarchical reinforcement learning</title>
		<author>
			<persName><forename type="first">J</forename><surname>Morimoto</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Doya</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Robotics and Autonomous Systems</title>
		<imprint>
			<biblScope unit="volume">36</biblScope>
			<biblScope unit="page" from="37" to="51" />
			<date type="published" when="2001">2001</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">Autobalancer: An online dynamic balance compensation scheme for humanoid robots</title>
		<author>
			<persName><forename type="first">S</forename><surname>Kagami</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Kanehiro</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Tamiya</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Inaba</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Inoue</surname></persName>
		</author>
		<editor>B.R. Donald, K. Lynch, D. Rus</editor>
		<imprint>
			<date type="published" when="2001">2001</date>
			<publisher>A K Peters, Ltd</publisher>
			<biblScope unit="page" from="329" to="340" />
		</imprint>
	</monogr>
	<note>Algorithmic and Computational Robotics: New Directions</note>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">A fast dynamically equilibrated walking trajectory generation method of humanoid robot</title>
		<author>
			<persName><forename type="first">S</forename><surname>Kagami</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Kitagawa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Nishiwaki</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Sugihara</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Inaba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Autonomous Robots</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="page" from="71" to="82" />
			<date type="published" when="2002">2002</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">The development of Honda humanoid robot</title>
		<author>
			<persName><forename type="first">K</forename><surname>Hirai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Hirose</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Haikawa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Takenaka</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Robotics and Automation</title>
		<imprint>
			<date type="published" when="1998">1998</date>
			<biblScope unit="page" from="1321" to="1326" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">H</forename><surname>Strogatz</surname></persName>
		</author>
		<title level="m">Nonlinear Dynamics and Chaos</title>
		<imprint>
			<publisher>Westview press</publisher>
			<date type="published" when="1994">1994</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Policy learning by ga using importance sampling</title>
		<author>
			<persName><forename type="first">C</forename><surname>Tuchiya</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Kimura</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Kobayashi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The 8th Conference on Intelligent Autonomous Systems</title>
		<imprint>
			<date type="published" when="2004">2004</date>
			<biblScope unit="page" from="281" to="290" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">Scalable internal-state policy-gradient methods for pomdps</title>
		<author>
			<persName><forename type="first">D</forename><surname>Aberdeen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Baxter</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2002">2002</date>
			<publisher>ICML</publisher>
			<biblScope unit="page" from="3" to="10" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Td-gammon, A self teaching backgammon program, achieves master legel play</title>
		<author>
			<persName><forename type="first">G</forename><surname>Tesauro</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural Computation</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page" from="215" to="219" />
			<date type="published" when="1994">1994</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Reward functions for accelerated learning</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">J</forename><surname>Mataric</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Machine Learning: Proceedings of the Eleventh International Conference</title>
		<imprint>
			<date type="published" when="1994">1994</date>
			<biblScope unit="page" from="181" to="189" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">He is currently a researcher at ATR Computational Neuroscience Laboratories and with the Computational Brain Project, ICORP, Japan Science and Technology Agency. His research interests include motor learning and control in robotic systems. He received the IEEE ICRA 2002 Best Paper Award. Masa-aki Sato is a department head of Computational Brain Imaging group at ATR Computational Neuroscience Laboratories. He received his Ph.D. in 1980 from Osaka University, studying high energy physics. His current research interests include Bayesian estimation, noninvasive brain imaging, neural networks and learning of dynamical systems</title>
	</analytic>
	<monogr>
		<title level="m">He was a Research Associate at the Department of Micro System Engineering, Nagoya University, from 2000 to 2001, and was a presidential postdoctoral fellow at the Computer Science Department</title>
		<meeting><address><addrLine>Osaka, Japan; Nara, Japan; Nara, Japan; Osaka, Japan; Nara, Japan; Pittsburgh, PA; Nagoya, Japan; Ann Arbor, USA, from; Los Angeles, USA; Kyoto, Japan</addrLine></address></meeting>
		<imprint>
			<date type="published" when="1995">2002. 2004. 1996. 2001. 1999. 2001. 2001. 2002. 1995 and 1997. 1995 to 1996. 2001 to 2002. 2002. 2004</date>
			<biblScope unit="page" from="12" to="22" />
		</imprint>
		<respStmt>
			<orgName>Osaka Prefecture University ; Nara Institute of Science and Technology ; Department of Information Science at Nara Institute of Science and Technology ; Carnegie Mellon University ; University of Southern California ; University of California San Diego ; Okinawa Institute of Science and Technology</orgName>
		</respStmt>
	</monogr>
	<note>in 1991, and Salk Institute in 1993. He joined ATR in 1994 and is currently the head of Computational Neurobiology Department, ATR Computational Neuroscience Laboratories. he was appointed as a principal investigator of Initial Research Project. He is interested in understanding the functions of basal ganglia and neuromodulators based on the theory of reinforcement learning. Contact: doya@oist.jp</note>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<title/>
		<author>
			<persName><surname>Suzaki</surname></persName>
		</author>
		<imprint>
			<biblScope unit="page" from="904" to="2234" />
			<pubPlace>Uruma, Okinawa; Japan</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
