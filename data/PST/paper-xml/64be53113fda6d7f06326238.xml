<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">QASA: Advanced Question Answering on Scientific Articles</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Yoonjoo</forename><surname>Lee</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Kyungjae</forename><surname>Lee</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">LG AI Research</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Sunghyun</forename><surname>Park</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">LG AI Research</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Dasol</forename><surname>Hwang</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">LG AI Research</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Jaehyeon</forename><surname>Kim</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">LG AI Research</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Hong-In</forename><surname>Lee</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution">Yonsei University</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Moontae</forename><surname>Lee</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">LG AI Research</orgName>
							</affiliation>
							<affiliation key="aff2">
								<orgName type="institution">University of Illinois Chicago</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">QASA: Advanced Question Answering on Scientific Articles</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-01-03T08:39+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Reasoning is the crux of intellectual thinking. While question answering (QA) tasks are prolific with various computational models and benchmark datasets, they mostly tackle factoid or shallow QA without asking deeper understanding. Dual process theory asserts that human reasoning consists of associative thinking to collect relevant pieces of knowledge and logical reasoning to consciously conclude grounding on evidential rationale. Based on our intensive think-aloud study that revealed the three types of questions: surface, testing, and deep questions, we first propose the QASA benchmark that consists of 1798 novel question answering pairs that require fullstack reasoning on scientific articles in AI and ML fields. Then we propose the QASA approach that tackles the full-stack reasoning with large language models via associative selection, evidential rationale-generation, and systematic composition. Our experimental results show that QASA's fullstack inference outperforms the state-of-the-art INSTRUCTGPT by a big margin. We also find that rationale-generation is critical for the performance gain, claiming how we should rethink advanced question answering. The dataset is available at https://github.com/lgresearch/QASA.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Reasoning differentiates human intellectual capabilities from low-level intelligence. Dual process models theorize that cognitive reasoning is a two-stage process where the first stage performs associative thinking and the second stage performs logical reasoning <ref type="bibr" target="#b39">(Wason &amp; Evans, 1974;</ref><ref type="bibr" target="#b36">Tsujii &amp; Watanabe, 2009;</ref><ref type="bibr" target="#b7">Evans, 2012)</ref>. Within the con-Figure <ref type="figure">1</ref>. An example of QASA. A question that the reader/author asks about the paper while reading the paper. To formulate the answer, one classifies whether the paragraph contains evidence to answer the question. Evidential rationales are written for each evidential paragraph and are systematically composed into a comprehensive answer.</p><p>text of Question Answering (QA), the first stage extracts associative pieces of knowledge based on lexical matching and other cognitive heuristics, inductively expanding potential evidences. Then the second stage consciously finds evidential rationales, deductively converging to the answer via systematic compositions of the evidences. This process uniquely characterizes advanced human reasoning, posing a non-trivial challenge to machine learning QA systems.</p><p>Reading Comprehension (RC) is one type of reasoning task that can formulate various questions and answers. SQuAD <ref type="bibr" target="#b27">(Rajpurkar et al., 2016)</ref>, NewsQA <ref type="bibr" target="#b35">(Trischler et al., 2017)</ref>, DROP <ref type="bibr" target="#b6">(Dua et al., 2019)</ref>, and Natural Questions <ref type="bibr">(Kwiatkowski et al., 2019a)</ref> have been proposed. While com-peting on their model performance significantly improves machine answering capabilities, these datasets consist of factoid QAs mostly in the form of "what", "when", "where", or "who". Thus extracting short spans from the relevant context can easily provide correct answers, but the trained models can barely answer "how" and "why" questions.</p><p>Recent work on open-domain QA <ref type="bibr" target="#b15">(Karpukhin et al., 2020;</ref><ref type="bibr" target="#b11">Guu et al., 2020;</ref><ref type="bibr" target="#b21">Liu et al., 2021;</ref><ref type="bibr" target="#b13">Izacard &amp; Grave, 2021;</ref><ref type="bibr" target="#b14">Izacard et al., 2022)</ref> exploits the Retrieve-then-read approaches, where the system first retrieves relevant documents from a large corpus then reads out concrete answers. These approaches target shallow questions that are often inferable relying only on the first stage rather than jointly using the both stages. Some reasoning tasks like bAbI and its permuted version <ref type="bibr" target="#b41">(Weston et al., 2015;</ref><ref type="bibr" target="#b26">Rajendran et al., 2018)</ref> require logically correct spatial reasoning. However, the artificial nature of their QAs minimally leverages the second stage as their reasoning tasks do require neither rich retrieval of associative information from the first stage nor systematic composition of the final answers <ref type="bibr" target="#b19">(Lee et al., 2016)</ref>.</p><p>Our think-aloud study reveals that reading scientific articles not only raises surface questions but also induces testing and deep questions that require full-stack reasoning. In addition, carefully answering surface questions turns out to involve both first and second stage reasoning, requiring significantly more elaborated efforts compared to what previous datasets and models implicitly assumed. To answer for such naturally advanced questions, we propose the Question Answering on Scientific Articles (QASA), a novel QA benchmark and an approach that realize the full-stack cognitive reasoning from the first to the second stages. Our QASA benchmark differs from existing ones on the following aspects:</p><p>? Based on our think-aloud study, we design a schema for advanced questions as surface, testing, and deep questions, then collecting balanced QA pairs form the authors of research papers as well as from expert readers.</p><p>? We guide readers and authors to ask questions while reading the whole paper rather than gathering only extractive questions from paper abstracts.</p><p>? Readers and authors are asked to propose their multifaceted long-form answers to the collected questions, then composing a comprehensive final passage than simply summarizing evidential rationales with added fluency.</p><p>Our QASA benchmark contains 1798 QA pairs on AI/ML papers where the questions are asked by regular readers of AI/ML papers and answered by AI/ML experts. Each paper has 15.1 questions on average, up to a maximum of 29 questions for a single paper. We collect 39.4% of deep reasoning level questions based on our own question schema. And, </p><formula xml:id="formula_0">? ? ? ELI5 ? ? ? ASQA ? ? ? AQuaMuSe ? ? ? QASA (ours) ? ? ?</formula><p>Table <ref type="table">1</ref>. Comparison of existing datasets and our QASA.</p><p>maximum 9 evidential rationales are leveraged to compose the final answer.</p><p>Our QASA approach models the full-stack reasoning process via state-of-the-art large language models. We decompose the process into three subtasks: associative selection (to extract relevant information from paragraphs), evidential rationale-generation (to grasp only evidential rationale from each extracted paragraph), and systematic composition (to stitch evidential rationales into a comprehensive answer without redundancy). Modeling each subtask by pretrained large language model with existing datasets, we demonstrate that our best test-bed outperforms the state-of-the-art InstructGPT (OpenAI's text-davinci-003) by 5.11 Rouge-1 points. We further verify that directly generating an answer from selected paragraphs causes performance drop, opening a crucial insight for tackling advanced question answering.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Work</head><p>The relevant research consists of three categories: QA for academic research papers, long-form QA, and query-based multi-document summarization. Table <ref type="table">1</ref> highlights our method against existing approaches in each groups.</p><p>QA for Academic Research Papers Several datasets have been proposed for QA on academic research papers including emrQA <ref type="bibr" target="#b23">(Pampari et al., 2018)</ref>, BioRead <ref type="bibr" target="#b24">(Pappas et al., 2018)</ref>, and BioMRC <ref type="bibr" target="#b31">(Stavropoulos et al., 2020)</ref>. They automatically construct their QA examples by extracting entities and relations as well as structure knowledge resources. Thus these datasets would unlikely reflect real-world scenarios where users have more advanced and open-ended questions <ref type="bibr">(Kwiatkowski et al., 2019b)</ref>. Closest to our work, QASPER <ref type="bibr" target="#b5">(Dasigi et al., 2021)</ref>, consists of 5K QA on NLP domain papers. However, most examples in QASPER represent shallow questions focused on completing concepts because the annotators produced the questions after reading only the title and abstract of a provided paper. Additionally, in QASPER, more than 70% of answerable questions consist of short-form answers, such as yes/no and small extractive span. In contrast, we ask our annotators to read further into main sections, demanding various types of questions based on our studied schema. As a result, the questions in our QASA cannot be simply answered with extracting spans form selected evidence paragraph. It truly urges full-stack reasoning.</p><p>Open-domain Long-form QA ELI5 <ref type="bibr" target="#b8">(Fan et al., 2019)</ref> collected open-domain questions with paragraph-level answers collected from Reddit forum and extracted the relevant sentences from web documents, which are provided as supporting evidence. However, only 65% of the questions have sufficient supporting evidence, while all the answers of our dataset have evidence paragraph through associative selection. <ref type="bibr" target="#b32">Stelmakh et al. (2022)</ref> have claimed that factoid questions in the ELI5 dataset are mostly ambiguous, and thus can be decomposed into sub-questions. ASQA <ref type="bibr" target="#b32">(Stelmakh et al., 2022)</ref> requires to answer all the sub-questions over multiple passages. Our QASA differs in two ways:</p><p>(1) our dataset includes evidential rationales, compositional element of long-form answer, (2) ours is made through a systematic composition that considers implicit relations between multiple rationales, rather than simply synthesizing the sub-answers.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Query-focused Multi-Document Summarization (qMDS)</head><p>For qMDS, some datasets in various domains have been proposed, such as QMSum for meeting transcripts <ref type="bibr" target="#b44">(Zhong et al., 2021)</ref>, Squality for science fiction <ref type="bibr">(Wang et al., 2022a)</ref>, and AQuaMuSe for wikipedia <ref type="bibr" target="#b16">(Kulkarni et al., 2020)</ref>. The goal of these tasks is to find an answer over multiple documents, which is similar with ours. However, qMDS datasets such as AQuaMuSe and QMSum have the limitation of using noisy and insufficient contexts as multi-documents, since they used automatically-generated passages extracted by lexical matching. To address the issue of insufficiency of dedicated training data, the previous work <ref type="bibr" target="#b1">(Baumel et al., 2018)</ref> adopts transfer learning techniques. In comparison to qMDS, our task provides human-annotated evidences aligned with a particular paragraph and answer summaries composed of multi-evidences. Additionally, qMDS focuses on summarizing text without redundancy, while we aim to generate rich long-form answers including multiple rationales.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Proposed Task</head><p>In this section, we propose a new task for question answering over scientific articles. The core idea of our proposed task is to answer the questions based on multiple evidence snippets that are spread over a long research paper. Specifically, we denote a question as q, an answer as a, and paragraphs in the paper as P = {p 1 , ...p N }. A one-step approach to process N paragraphs would be adopting lengthscalable transformer such as LongFormer <ref type="bibr" target="#b2">(Beltagy et al., 2020)</ref>, which enables to encode the multiple snippets at once. In contrast, our advanced questions triggered from research papers requires to connect between rationales for deep reasoning. Hence, we design this problem as multi-step subtasks: (1) associative selection, (2) evidential rationalegeneration, and (3) systematic composition. Figure <ref type="figure" target="#fig_0">2</ref> shows the overview of our approach.</p><p>Associative Selection While research papers have multiple paragraphs (e.g., 20-60 paragraphs), the first step is to extract associative knowledge from the paragraphs, corresponding to a question. Specifically, given question q and paragraphs P = {p 1 , ...p N }, we aim to select evidential paragraphs P = {p 1 , ...p k } that contains an answer or rationales to question q, where k ? N . While the previous work <ref type="bibr" target="#b28">(Rajpurkar et al., 2018)</ref> aims to classify answerability whether a given passage contains answer a to question q, the answer in our task is composed of multiple rationales including a main answer. Our associative selection task can be viewed as the super-task of answerability (i.e., answerable is evidential, but not the reverse).</p><p>Evidential Rationale-Generation In this step, we generate an evidential rationale on each selected paragraph, which could be part of a final long-form answer in the next step.</p><p>Based on the prior work about discourse structure of answers to complex questions <ref type="bibr" target="#b42">(Xu et al., 2022)</ref>, the evidential rationale can be the (1) main answer (i.e., main content of the answer which directly addresses the question), (2) elaboration (i.e., sentences which elaborate on the main answer), and (3) auxiliary information (i.e., background knowledge that could be helpful to the user). Specifically, we denote the evidential rationale that is inferred from (q,p i ) as e i . That is, from the selected P = {p 1 , ...p k }, we obtain a list of evidential rationales {e 1 , ..., e k }.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Systematic Composition</head><p>To provide concise and readable information to users, the goal of this last step is to systematically compose all the evidential rationales {e 1 , ..., e k } into a final comprehensive answer a.</p><p>Assuming that the answer is composed of multi-rationales, we aim to preserve all the rich rationales in the final answer, except duplicated texts. Specifically, we aggregate the list of texts {e 1 , ..., e k } into a single context, and then compose final answer a from the context. The answer a grounded on a given paper could be viewed as comprehensive explanations about question q.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Building the QASA Dataset</head><p>Prior to data collection, we conducted a preliminary study for identifying what kinds of questions are raised when reading papers. Based on our findings, we design a schema to collect diverse and balanced questions with different levels of reasoning. As the source of the QASA, we gather a set of open-access AI/ML papers. To collect advanced questions that require reasoning over evidential rationales, we recruited AI/ML practitioners or researchers who regularly read research papers and conducted two separate sessions from the perspective of both readers and authors.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Preliminary Study</head><p>In the aim of identifying what kinds of questions readers ask while reading, we conduct a think-aloud study (N = 10), a standard approach in human-computer interaction (HCI) for capturing human's intent during a task. Our analysis of 127 questions revealed that 67% of questions required a two-stage process, and the types of reasoning needed in the second stage varied even among these questions. Referring to the prior literature on question taxonomy in the education domain, there are distinct types of questions ranging from surface questions to deeper questions that require more reasoning and interpretation to answer <ref type="bibr" target="#b10">(Graesser et al., 1992;</ref><ref type="bibr" target="#b9">Graesser &amp; Person, 1994)</ref>. To gather diverse and balanced types of questions, we design a schema for paper questions by adapting the prior literature in education to a paper reading context and interpreting data collected from our think-aloud study. This schema includes not only questions requiring second stage reasoning, but also a spectrum of reasoning types needed to answer them. The definitions of each question type are shown below and detailed explanations with examples can be found in Appendix B.</p><p>? Surface questions aim to verify and understand basic concepts in the content. The answer content is directly related to the words in the question and immediate context. This type includes verification, distinctive, concept completion questions.</p><p>? Testing questions are focused on meaning-making and forming alignment with readers' prior knowledge. These questions aim to find similar examples (example), quantify variables (quantification), and find meaning and make comparisons across concepts (comparison).</p><p>? Deep questions ask about the connections among the concepts in the content and elicit advanced reasoning in logical, causal, or goal-oriented systems. This type includes causal antecedent, causal consequence, goal orientation, instrumental/procedural, rationale, expectation questions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Papers</head><p>To collect papers, we adopt S2ORC <ref type="bibr" target="#b22">(Lo et al., 2020)</ref>, a collection of machine-readable full text for open-access papers, and the arXiv<ref type="foot" target="#foot_0">1</ref> paper collection. We only use papers within the CS.AI domain in the arXiv dataset and apply two filtering criteria to the papers in the S2ORC collection: (1) published after 2015 and (2) has more than 100 citations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Data Collection</head><p>With the aim of collecting various advanced questions (surface to deep), we conduct two types of sessions, reader sessions where we collected QAs from general readers and author sessions where authors annotated questions about their own papers. We perform author sessions since authors are the optimal annotators who can make challenging and insightful questions that could be asked by experts, like reviewers-granting greater diversity to the questions in our benchmark. For the reader session, to make the data collection process similar to a real context, we decouple the questioning and answering phase following the collection process of QASPER <ref type="bibr" target="#b5">(Dasigi et al., 2021)</ref>. For both tasks, we recruited graduate students studying AI/ML and freelancers practicing AI/ML through professional networks and Upwork.<ref type="foot" target="#foot_1">2</ref> For the answering task, we qualify annotators through the exams related to our task and experience in the domain. Details on the data collection procedure and workers' information are given in Appendix A.</p><p>Questions To ensure that our questions are realistic, we allow annotators to choose papers that they wanted to read. Additionally to replicate differing reading styles, we asked annotators to follow one of two scenarios: read all the sections in the paper (i.e., deep reading) or read only certain sections (i.e., skim reading). To collect diverse types of questions, we provide them with the question schema and asked them to make a balanced number of questions for each type. In the same vein, we also recommend annotators to make at least one question per subsection that they read. When annotating questions, annotators were instructed to write the trigger sentences that raised the question but that did not contain the answer. While they were not used in this work, trigger sentences could be used in future research for question generation from long-form text and to complement the ambiguity of questions that occur in long-form text.</p><p>Answers To collect answers, we ask answerers to choose papers from the papers that the questioners worked on. We guide answerers to compose their answers into a comprehensive passage based on the their own-generated evidential rationales from the selected paragraphs. To let them follow our guideline more easily, we provide the annotators with the answering interface when answering the questions. They were shown the question, the full paper, the name of the section that triggered this question, and ten paragraphs that are the most relevant to the question. We provide top ten paragraphs by following that existing IR research <ref type="bibr" target="#b3">(Carterette et al., 2010)</ref> adopted a pooling method, where top ranked documents are selected to create the pool of documents that need to be judged when creating evaluation dataset. Our top-10 relevant paragraphs were chosen with an off-theshelf embedding model. <ref type="foot" target="#foot_2">3</ref> When answering each question, annotators were asked to do the following subtasks.</p><p>First, they are asked to look through the ten relevant paragraphs and, for each, make a binary decision as to whether the paragraph is evidence paragraph. If there is no relevant paragraph chosen to have evidence, annotators could freely choose other paragraphs from the paper as having evidence in addition to the ten paragraphs we provided. Second, for each paragraph that was chosen, annotators are instructed to write evidential rationale from that paragraph. Evidential rationale could be the (1) main answer to the question, (2) elaboration, or (3) auxiliary information <ref type="bibr" target="#b42">(Xu et al., 2022)</ref>. Third, they write a final comprehensive answer by composing the multiple evidential rationales that they generated for each evidence paragraph. When the answer cannot be fully answered even after composing multiple evidential rationales, annotators are instructed to answer as much as possible with the available information and then specify which part of the question cannot be answered. When a question is completely unanswerable, we ask annotators to indicate that the author do not provide an explanation for the missing information and to specify what information is missing. Finally, they annotate whether writing the final answer requires to compose multiple evidential rationales (True) or not (False)-i.e., no complex reasoning is needed and they only simplify text from the paper without adding redundancies.</p><p>All annotators who ask questions and write answers conducted a practice session to familiarize themselves with the annotation guidelines. Annotations from the practice sessions were reviewed by two authors, and discrepancies between the annotators and the guidelines were discussed. Additional practice sessions were conducted for annotators with substantial discrepancies. If annotators were judged as not having sufficient background knowledge or understanding of the task even after these sessions, we did not let them participate in the tasks.</p><p>Authors We recruited paper authors to annotate QAs for their own papers to cover deeper questions that existing datasets rarely cover. We instructed authors to make only testing and deep types of questions and to annotate trigger sentences that might cause readers to become curious about that question. The rest of the annotation process is similar to the readers' sessions. We recruited 17 authors whose domains are distributed in CV, NLP, GNN, generative models, and music information retrieval.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.">QASA Analysis</head><p>Representative examples from QASA is in Table <ref type="table" target="#tab_8">7</ref> (in Appendix E).</p><p>Question types In terms of question types, two domain experts manually evaluated 100 randomly sampled questions. 89% of the annotated question types were aligned with domain experts' annotations. <ref type="foot" target="#foot_3">4</ref> To describe the diversity of our dataset, we analyze the distribution of the types of the questions in QASA. Among the three types, 39.4% of the questions are deep questions, 30.0% are testing, and 30.7% are surface-level. Among the deep questions, instrumental sub-type (12%) accounts for most of the deep questions, and comparison (11%) and concept completion (17%) are the most annotated questions for the testing and surface questions, respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Distribution of evidential rationale</head><p>We also analyze to identify the number of evidential rationales that are needed to answer the questions depending on their types. Among all the questions, 12% of questions are annotated as having no evidential rationales, which means that they are unanswerable questions. Out of the answerable questions, the average number of evidential rationales is 1.67. Surface questions need the most evidential rationales (1.73) while testing questions and deep questions need 1.66 and 1.63, respectively, which implies that our surface-level questions like "Do the authors claim that bigger datasets would improve the performance and expressiveness of reading comprehension models?" also need systematic reasoning to answer.</p><p>Composition, Correctness, Groundedness On average, 49.6% of answers require annotators to compose evidential rationales, while the rest (50.4% of answers) only need simplifying redundant rationales. To analyze which question type requires the most reasoning to answer, we analyze the ratio of compositionality depending on the question type. Deep questions need composing the most (44.6%) in comparison with testing (29.0%) and shallow (26.4%) questions. To estimate the correctness of the answer annotations and groundedness of the answer annotation, domain experts manually analyzed 100 randomly sampled questions.</p><p>We find that 90% of the answers are correct and 87% are grounded well on the paper.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">QASA Approach</head><p>In this section, we propose a QA approach for QASA over research papers. Our task requires to answer questions based on multiple passages whose supporting evidences are spread over a whole paper. As above-mentioned, our tasks consist of (1) associative selection, (2) evidential rationalegeneration, and (3) systematic composition. As shown in Figure <ref type="figure" target="#fig_0">2</ref>, we train LM models with multi-task instructions, following recent works <ref type="bibr" target="#b4">(Chung et al., 2022;</ref><ref type="bibr" target="#b40">Wei et al., 2021;</ref><ref type="bibr" target="#b0">Aribandi et al., 2021;</ref><ref type="bibr" target="#b29">Sanh et al., 2021)</ref>.</p><p>5.1. Multi-step QA system based on LM Pre-processing via Retrieval Before the first step of associative selection, we consider pre-processing step using a retrieval model to narrow the search space, from a whole paper to top-N related paragraphs (we set N =10). This enables the efficient selection step, while compromising the recall of evidential paragraphs. Specifically, we used the off-the-shelf model provided by OpenAI, and leave the question of improving retrieval for future work. Through the retrieval, we encode all paragraphs in the given paper and a target question into dense vectors, and extract top-N nearest neighbor paragraphs by using cosine similarity.</p><p>Finetuning Large Language Model with Multitask Instructions We finetune large language models (LLMs) on a mixture of our subtasks through instruction tuning <ref type="bibr" target="#b40">(Wei et al., 2021)</ref>. As in previous work <ref type="bibr" target="#b40">(Wei et al., 2021;</ref><ref type="bibr" target="#b0">Aribandi et al., 2021)</ref>, it is known that instruction tuning makes LMs generalizable on unseen tasks. As shown in Figure <ref type="figure" target="#fig_0">2</ref>, a single LM takes task input with instruction that indicates each subtask. The output of the previous step is sequentially passed to the next step. However, in the selection task, if the model does not select any paragraph as evidence, it also cannot generate rationales or answers. Instead of solving this problem, we used top-3 paragraphs if none were selected, which is left to future work. For task-specific prompts, we used manually-written instructions for each subtask (See Appendix D). As state-of-the-art LLMs, we consider the following models:</p><p>? T5 <ref type="bibr" target="#b25">(Raffel et al., 2020)</ref> (Version 1.1, LM-Adapted): it is pretrained on Common Crawl <ref type="bibr" target="#b25">(Raffel et al., 2020)</ref> using Transformer with encoder-decoder architecture.</p><p>? T0 <ref type="bibr" target="#b29">(Sanh et al., 2021)</ref>: starting from T5, it is further trained on 8 downstream tasks.</p><p>? FLAN-T5 <ref type="bibr" target="#b4">(Chung et al., 2022)</ref>: similar with T0, it is further trained with scaling up multi-tasks (1k+) including reasoning tasks.</p><p>? GALACTICA <ref type="bibr" target="#b33">(Taylor et al., 2022)</ref>: it is pretrained on a large collection of scientific papers, with the decode-only architecture like GPT.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.">Training Data</head><p>No training resources have been proposed that support our full-stack QA, and we therefore exploit public and synthetic data for the purpose of each subtask.  For associative selection, we adopt answerability labelswhether the pair of question and knowledge is answerable or not. In case of ASQA <ref type="bibr" target="#b32">(Stelmakh et al., 2022)</ref>, we treat the pair of (q and p + ) as a positive example, and (question q, randomly-sampled p -) as negative. For QASPER <ref type="bibr" target="#b5">(Dasigi et al., 2021)</ref>, we leverage pairs of (question q, gold paragraph p + that contains an answer). The limitation of adopting these datasets is that they aim to capture the presence of an answer, while we target that of evidential rationales, which may affect recall of rationales.</p><p>The rationale-generation task requires to generate evidential rationale e from (question q, paragraph p). Unfortunately, to the best of our knowledge, there is no data to support this task. As an alternative source, we used the triplets of (q,p,a) in QASPER <ref type="bibr" target="#b5">(Dasigi et al., 2021)</ref>, where gold knowledge p always contains information about answer a to q. We treat answer a as evidential rationale, since the QA labels in QASPER do not require to reason over multiple passages, which is expected to learn the ability of extracting question-focused evidences from a given paragraph.</p><p>Lastly, for systematic composition, we adopt long-form QA data with multiple evidences. We select ELI5 and the subset of ASQA, which provide selected evidence passages from the pool of passages for answer generation. That is, this task is to generate answer a inferred from the context (question q, multi-evidences {e 1 , ...e n }), which requires to consolidate and summarize scattered information.</p><p>For synthetic data, recent works distill training data from InstructGPT <ref type="bibr">(Wang et al., 2022b;</ref><ref type="bibr" target="#b12">Honovich et al., 2022)</ref>. Inspired, we distil training examples for QASA from large language models by prompting instruction and an in-context example. We use OpenAI's InstructGPT (text-davinci-003) with the temperature set to 0.1, which is the state-of-the-art model on many NLP tasks. Specifically, we first extract AI and ML papers from arXiv, and generate questions over each paragraph sampled from the papers. Then, given the questions, we test InstructGPT following instructions in our subtasks, as shown in Appendix D. While LLMs have a general problem of factual inconsistency, known as hallucination, we found that InstructGPT performs well on the rationale generation task (See Table <ref type="table" target="#tab_4">3</ref>). Although there is no public data to support rationale generation, we can alleviate the insufficiency through evidential rationales obtained from InstructGPT, which would boost our full-stack QA.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Experiment</head><p>In this section, we evaluate our QASA approach on the proposed benchmark. In our experiment, we apply stateof-the-art LMs as two variants: Pretrained and Finetuned versions. In Sec 6.1 and 6.2, we automatically evaluate models on three subtasks: (1) associative selection, (2) rationale-generation, (3) answer composition, and their fullstack QA task. To complement automatic evaluations in our generation task, we conduct human evaluation in Sec 6.3 and an error analysis in Sec 6.4.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.1.">Experimental Setting</head><p>Evaluation of Subtasks and Full-stack QA For subtask evaluation, we provide oracle (or gold) contexts, in order to evaluate each subtask independently. In the associative selection task, we consider both positive paragraphs labeled by humans and negative paragraphs among top-10 retrieved results as candidate pool. For rationale-generation, we generate evidential rationale conditioned only on each of gold positive paragraphs. Similarly, for answer composition, we provide a list of gold evidential rationales as contexts. In contrast, for the full-stack QA, we consider the results of previous task as input to the next task sequentially, which could propagate the errors of the previous steps. Meanwhile, we conduct an ablation experiment, to directly generate final answers from selected paragraphs without rationalegeneration ("w/o Rationale Gen" in Table <ref type="table" target="#tab_5">4</ref>).</p><p>Metric For associative selection, we measured the precision (P), recall (R), and F1 score. For rationale-generation and answer composition tasks, we used a standard text generation metric -ROUGE scores <ref type="bibr" target="#b20">(Lin, 2004)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.2.">Main Results</head><p>Table <ref type="table" target="#tab_4">3</ref> shows the automatic evaluation results of several QA systems on three subtasks and full-stack QA task.</p><p>Which pretrained LM is best? Among the pretrained LMs, INSTRUCTGPT (175B) outperformed others. Especially in the rationale-generation task, it shows the best performance among all models. Among T5-based LMs, the number of downstream tasks used during training had a significant impact on the performances in full-stack QA, showing T5 &lt; T0 &lt; FLAN-T5.</p><p>Which finetuned LM is best? When comparing finetuned T0, T5, and FLAN-T5, these models show little difference in performances on three subtasks. However, FLAN-T5 outperformed all other LMs on the full-stack QA, even the state-of-the-art model, INSTRUCTGPT (175B). Based on this observation, we suggest the finetuned FLAN-T5 could serve as a good test-bed for QASA.</p><p>The effect of training resources we curated For an ablation study, we trained individual Flan-T5 on each one of four datasets (QASPER, ASQA, ELI5, Augmented Data from GPT (or GPT AUG)). Through the comparison, we can observe negative transfer across datasets, e.g., FLAN-T5 trained on ASQA-ONLY shows the best results in the answer composition task, outperforming FLAN-T5 trained on combined data. Meanwhile, training of GPT AUG improved significantly the performances in the rationalegeneration task, which is essential for our full-stack QA, as other resources do not contain rationales.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Method</head><p>Associative Selection Rationale Generation Answer Composition  Does our task indeed need rationale-generation? For our full-stack QA, while we first generate rationales and then compose them into a final answer, we can directly generate an answer from selected paragraphs, skipping the step of rationale generation. However, as shown in Table <ref type="table" target="#tab_5">4</ref>, FLAN-T5 "w/o Rationale Gen" showed poor performance, compared to our three-step approach, which means the rationale generation step is crucial for the full-stack QA.</p><formula xml:id="formula_1">(P) (R) (F1) (R-1) (R-2) (R-L) (R-1) (R-2) (R-</formula><p>The failure of Galactica Although GALACTICA was pretrained on a large-scale collection of research papers, it performed worse on overall tasks compared to other models. The low performance of GALACTICA was consistently observed in <ref type="bibr" target="#b30">Singhal et al. (2022)</ref>, compared to PubMedGPT of 2.7B. We empirically found that GALACTICA often answered either "yes" or "no", and terminated the generation, in which case the Rouge score is almost zero.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.3.">Human Evaluation</head><p>Although automated metrics can measure crucial aspects of our task, they are not guaranteed to closely approximate the judgment of humans, whose satisfaction is an overarching goal of a QA system. Therefore, we performed human evaluations based on the dimensions that should be satisfied in this task.</p><p>We conducted a pairwise evaluation scheme where evaluators compare two answers to the same question, inspired by <ref type="bibr" target="#b32">Stelmakh et al. (2022)</ref>. We provided two responses to each human evaluator, one from ours and the other from In-structGPT. The human evaluators could read the rationales and the generated responses side-by-side. Then, the evaluators were asked to choose the better answer in terms of four criteria: Groundedness, Completeness, Specificity, and Fluency, following prior work <ref type="bibr" target="#b32">(Stelmakh et al., 2022;</ref><ref type="bibr" target="#b34">Thoppilan et al., 2022)</ref>. For each data point, we assigned three evaluators to collect three trials of such pairwise judgments.</p><p>The scoring system awards one point for a win and half a point for a tie in pairwise comparisons. The annotations were collected on 100 QA pairs by 9 experts.</p><p>The results of this human evaluation in Figure <ref type="figure" target="#fig_1">3</ref> show that the answers from our full-stack QA tend to be more complete and grounded than those from InstructGPT, which is consistent with the results from the automatic evaluation. In contrast, the InstructGPT's answers tend to be more fluent and specific, regardless of the reliability of its generated text. We also added some qualitative examples to show how the answers generated by our approach differ to those by InstructGPT in Appendix F.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.4.">Error Analysis</head><p>To gain a deeper understanding of the model's errors, we sample 50 test examples with Rouge-L scores below 10 (i.e., bottom 25%). We exclude instances that are unanswerable based on the given paper. We then classify errors into five categories, ranging from E1 to E5.</p><p>E1 refers to cases where the model incorrectly classified the question as unanswerable. E2 is the generation of irrelevant content. E3 is cases where the model provides implicit evidence but fails to generate an explicit answer. E4 refers to cases where the generation is not factually grounded on the source document. Lastly, E5 refers to cases with low completeness, where the generation only covers a partial answer (i.e., a sub-question). Additionally, a low Rouge score does not necessarily indicate a wrong generation. We identify two correct scenarios for this (C1 and C2). C1 refers to cases where the human labels are incorrect. C2 is cases where both the generation and human label are correct, but the lexical overlap between the two texts is low due to the diversity of expressions.</p><p>Table <ref type="table" target="#tab_7">5</ref> shows error analysis results. 36% of InstructGPT's answers and 34% of ours belong to C1 and C2: cases with low ROUGE score, but correct. 48% of InstructGPT's answers are cases of refusal to answer (e.g., "I cannot find any specific information..."), although the context contains relevant evidences. We conjecture that InstructGPT has been trained to avoid answering in uncertain cases for safety. In contrast, our system did not generate such refusal responses, since there is no such example in our training data. 44% of our system's answers are irrelevant to a given question,</p><p>although the text is grounded on evidence.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.">Limitation</head><p>While we proposed a new benchmark for QA task on scientific articles, evaluation is becoming difficult, especially on recently emerging language models (InstructGPT as well as ChatGPT, Bard). Such language models aim not only for accurate responses, but also for longer responses through structured writing. Hence, evaluation metrics using string matching (such as ROUGE) may not represent the overall quality of generated results. The concurrent work showed that none of automatic metrics reliably matches human judgments of overall answer quality <ref type="bibr" target="#b43">(Xu et al., 2023)</ref>. Future work for our QA task could look deeper into adopting multifaceted evaluations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8.">Conclusion</head><p>Conventional information search requires a series of nontrivial efforts from retrieving and reranking relevant information to manually reading and restructuring the selected information. Due to growing volumes of scientific papers and professional articles, the traditional process is no longer feasible, urging an innovation in knowledge processing and reasoning. Generative QA would be a promising alternative, but it lacks appropriate benchmark and principled methodologies that are focused on human intellectual capabilities: full-stack reasoning.</p><p>In this paper, we propose the QASA: a novel benchmark dataset and a computational approach. Our QASA benchmark guides expert readers and paper authors to generate various types of questions and answers from surface to testing and deep levels. Our QASA approach decomposes the full-stack reasoning process into three reasoning subtasks: associative selection, evidential rationale-generation, and systematic composition. By modeling each subtask by pretrained LM, we show that FLAN-T5 finetuned on public and synthetic data could serve as the best test-bed for our QASA, proposing a new horizon of full-stack cognitive reasoning on scientific articles such as research papers and manuscripts.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Dataset collection details</head><p>Question writers and answer writers were paid US $28 and $63 (respectively) per paper on average and we have 26 question-makers and 28 answer-makers in terms of reader sessions and N authors in author sessions. We did not specify the number of questions per paper to allow annotators to create meaningful questions rather than be forced to add unnecessary questions. However, we recommend making around 15 questions per paper in order to guarantee dataset size.</p><p>Workers (i.e., question/answer writers and authors) provided basic information about their expertise in AI/ML and question writers were asked to provide how familiar they already were with the paper for which they asked questions. The field of workers was in the order of CV, NLP, and Applied ML, and there were also workers from theoretical ML, GNN, RL, MLOps, music IR, and Human-centered AI. Most question writers (84.6%) had some experience in AI/ML, with 31.8% having more than four years of experience. Similarly, the majority of answer writers (88%) had experience in AI/ML, and 36.4% of them had over four years of experience. 50% of the authors have over four years of relevant experience and 66.7% of the authors have submitted three or more papers from their domains. 89% of the papers were seen by the question writers for the first time.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Question level taxonomy</head><p>For each question level, we provided the types of questions that are in that level and examples for each of these question types.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.1. Surface questions</head><p>Surface-level questions aim to verify, compare, and understand basic concepts in the content. The answer content is directly related to the words in the question and immediate context.  datasets would improve the performance and expressiveness of reading comprehension models?" ["Mentions that previous approaches in the literature that attempt to use synthetic dataset approaches (which can quickly be used to generated datasets of large sizes) have shown promise for reading comprehension tasks", "Authors mention that creating a large-scale labelled dataset as one of their contributions.", "Indicates that attention based models and LSTM models are effective at reading comprehension tasks.", "Explains that they (the authors) intend to provide a dataset that could be used to benchmark performance on reading comprehension tasks" ] "Based on the information in this paper alone, it is unclear if a bigger dataset would improve the performance of reading comprehension models. While authors explain that a key contribution they make is the creation of a real-world, massive labelled reading comprehension dataset, it is unclear if such a dataset is essential to improve the performance of reading comprehension models -the authors pitch their dataset-building approach also as a way of evaluating performance of these models, which is different from the dataset itself leading to better performance." True Testing question "What does "interaction between the pixels to the text embedding through the diffusion process" mean?" ["During the diffusion process, we predict the noise of an image given a noisy image and text embedding using U shaped network. and this process yield our final image at the last step. and the interaction between the two modality occurs during the noise prediction. And the interaction between the visual and textual features are fused using Crossattention layers that produce spatial attention maps for each textual token.", "Authors consider using internal cross-attention in their work, and the cross-attention maps are high-dimensional tensors that bind pixels and tokens extracted from the prompt text."] "To answer this question we need to recall the diffusion process, which is in order to predict the noise of an image we have two inputs 1-noisy image and 2-text embedding, and the interaction between the two inputs are fused using Cross-attention layers that produce spatial attention maps for each textual token. and that is what is meant by the interaction between pixels to text embedding."</p><p>False Deep question "When defining the reading comprehension task, the authors explain that they wish to estimate p(a|c, q). What would a model trained on this task do if the context 'c' itself had factually incorrect information?" ["This paragraph explains that the authors wish to estimate the conditional probability of an answer (a) being relevant to a question (q) given some context (c). Why is this a necessary step in the process?" ["Explains the difference between an original (unprocessed) data point and anonymized sample through an example. This paragraph points out that the non-anonymous version of the query could potentially be answered by an agent (either human or an ML model) even without reading the paragraph/context, while that would not be possible postanonymization. This change ensures that the metric being measured is reading comprehension only and not anything else.", "Explains that they replace all entities with an abstract entity marker."] "Since the authors are attempting to build a reading comprehension model, not anonymizing the entities before using the dataset might lead to a situation where models use external information, or statistics on the distribution/frequency of words themselves to guess answers. These steps are needed to ensure that models use the context to answer the questions."' False Two examples of public BERT-style English corpora are BookCorpus and OpenWebText. BookCorpus is a largescale corpus of 11,038 books written in English, while OpenWebText is a collection of over 8 million webpages. Both corpora are used to train and evaluate natural language processing models, such as BERT.</p><p>['We consider five English-language corpora of varying sizes and domains(...) We use the following text corpora:?BookCorpus <ref type="bibr">(Zhu et al., 2015)</ref> plus English Wikipedia. This is the original data used to train BERT. (16GB).?CC-News, which we collected from the English portion of the Com-monCrawl News dataset <ref type="bibr">(Nagel et al." 2016)</ref> the papers they are annotating. However, we did not collect personal identifiable information without the annotators' explicit consent, except for payment purposes. Additionally, the information was not included in the dataset we proposed.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 2 .</head><label>2</label><figDesc>Figure 2. An overview of QASA approach. The language model works depending on task-specific instructions.</figDesc><graphic url="image-2.png" coords="4,77.51,67.06,439.36,212.05" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 3 .</head><label>3</label><figDesc>Figure 3. The results of human evaluation, comparing Ours to InstructGPT on four dimensions across 100 samples.</figDesc><graphic url="image-3.png" coords="9,64.27,67.06,213.84,139.84" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Verification?</head><label></label><figDesc>Is this true? Did an event occur? ? Examples -Did the authors have an experiment with training the state-of-the-art QA model with QuAC dataset? -They claim that LSTM can synthesize unseen compositions. Is this true? Disjunctive ? Is X or Y the case? ? Examples -For metrics involving co-occurrence C, were they measured with the original C or the rectified C? Concept completion ? Who? What? When? Where? ? Examples -What are the metrics used to measure the audio quality in the model comparison experiment? -Who were recruited as annotators of the entities and relations of concepts of lecture transcripts? B.2. Testing questions Testing questions are focused on meaning-making and alignment with readers' prior knowledge. The questions are marked by qualifying parameters of the components and generating initial interdependencies between the concepts. These questions aim to find similar examples, quantify the variables, find meaning and make comparisons across concepts. Example ? What is an example label or instance of the category? ? Examples -What are the examples of the style of websites?</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0"><head></head><label></label><figDesc></figDesc><graphic url="image-1.png" coords="1,307.44,185.33,242.97,250.50" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2</head><label>2</label><figDesc></figDesc><table><row><cell>shows a</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2 .</head><label>2</label><figDesc>Training Resources for our QA system.</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 3 .</head><label>3</label><figDesc>The results of baseline systems on three subtasks in QASA, measured by Precision, Recall, F1 score, and ROUGE scores. The best results in each column are bold-faced, and 2nd best results are underlined.</figDesc><table><row><cell>Method</cell><cell cols="3">Full-stack QA</cell></row><row><cell></cell><cell cols="3">(R-1) (R-2) (R-L)</cell></row><row><cell cols="4">Pretrained LMs (Accessible Checkpoints or API)</cell></row><row><cell>GALACTICA (6.7B)</cell><cell cols="3">15.56 3.65 11.44</cell></row><row><cell>T5 (3B)</cell><cell>9.83</cell><cell>0.58</cell><cell>8.01</cell></row><row><cell>T0 (3B)</cell><cell cols="3">15.60 4.28 12.15</cell></row><row><cell>FLAN-T5 (3B)</cell><cell cols="3">22.48 9.52 18.45</cell></row><row><cell cols="4">INSTRUCTGPT (175B) 27.11 11.90 19.75</cell></row><row><cell cols="2">Finetuned LMs (on Collected Data)</cell><cell></cell><cell></cell></row><row><cell>GALACTICA (6.7B)</cell><cell cols="3">20.93 6.16 15.01</cell></row><row><cell>T5 (3B)</cell><cell cols="3">26.66 11.45 20.73</cell></row><row><cell>T0 (3B)</cell><cell cols="3">29.75 13.13 22.75</cell></row><row><cell>FLAN-T5 (3B)</cell><cell cols="3">32.22 14.62 24.53</cell></row><row><cell>w/o Rationale Gen</cell><cell cols="3">27.73 11.31 19.32</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 4 .</head><label>4</label><figDesc>The results of full-stack QA systems on QASA.</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 5 .</head><label>5</label><figDesc>Error Analysis of InstructGPT and Ours.</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>Table 7 .</head><label>7</label><figDesc>Examples from QASA.</figDesc><table><row><cell>Composition</cell><cell></cell><cell></cell></row><row><cell>Answer</cell><cell></cell><cell></cell></row><row><cell>Evidential rationales</cell><cell></cell><cell></cell></row><row><cell>Question</cell><cell>"Do the authors claim that bigger</cell><cell></cell></row><row><cell>Type</cell><cell>Surface</cell><cell>question</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_10"><head></head><label></label><figDesc>", "Explains that the proposed dataset's objective is to test if a model is able to read and comprehend a document -not test a model's global knowledge of whether a statement is true or false.", "Mentions that the task they are building the dataset for is a reading comprehension task."] "The authors are training a reading comprehension model. Therefore, if the context "c" has incorrect information, the model is likely to answer based on the factually incorrect information itself. The authors clearly explain that the task their model is being built for and evaluated on is of identifying answers from a given text (i.e. comprehension) and not knowledge of global correctness."</figDesc><table><row><cell>True</cell><cell></cell><cell></cell><cell></cell></row><row><cell>"What weaknesses would a dataset</cell><cell>that without entity replacement or</cell><cell>anonymization have when training</cell><cell>a reading comprehension model?</cell></row><row><cell></cell><cell></cell><cell></cell><cell>Deep</cell><cell>question</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_11"><head>Table 8 .</head><label>8</label><figDesc>The qualitative examples of answers, evidential paragraphs, and rationales from Ours and InstructGPT</figDesc><table><row><cell>Question</cell><cell cols="2">Answer-</cell><cell>Answer-InstructGPT</cell><cell>Evidential Paragraphs</cell><cell>Rationales</cell></row><row><cell></cell><cell>Ours</cell><cell></cell><cell></cell><cell></cell></row><row><cell>Give two</cell><cell cols="2">BookCorpus</cell><cell></cell><cell></cell></row><row><cell>examples of</cell><cell cols="2">and English</cell><cell></cell><cell></cell></row><row><cell>public BERT-</cell><cell cols="2">Wikipedia</cell><cell></cell><cell></cell></row><row><cell>style english</cell><cell>are</cell><cell>two</cell><cell></cell><cell></cell></row><row><cell>corpora.</cell><cell>public</cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell cols="2">BERT-style</cell><cell></cell><cell></cell></row><row><cell></cell><cell>English</cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell cols="2">corpora.</cell><cell></cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_12"><head></head><label></label><figDesc>. (...)] 'A random sample of the tokens in the input sequence is selected and replaced with the special token [MASK]. (...) BERT uniformly selects 15% of the input tokens for possible replacement. Of the selected tokens, 80% are replaced with [MASK], 10% are left unchanged, and 10% are replaced by a randomly selected vocabulary token.']</figDesc><table><row><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">['CC-News,</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">BookCorpus</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">and Wikipedia</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>are</cell><cell>BERT-</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">style english</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">corpora.']</cell></row><row><cell>How many</cell><cell cols="2">BERT train-</cell><cell>In BERT training, 15% of tokens are</cell><cell cols="2">['Of the se-</cell></row><row><cell>tokens are</cell><cell>ing</cell><cell>uses</cell><cell>changed to [MASK], resulting in a ratio</cell><cell cols="2">lected tokens</cell></row><row><cell>changed to</cell><cell cols="2">[MASK]</cell><cell>of 1:6.4 (15% of 512 tokens is approxi-</cell><cell cols="2">15%, 80% are</cell></row><row><cell>[MASK] in</cell><cell cols="2">to replace</cell><cell>mately 80).</cell><cell cols="2">replaced with</cell></row><row><cell>BERT training?</cell><cell cols="2">80% of the</cell><cell></cell><cell cols="2">[MASK] dur-</cell></row><row><cell>Give a ratio.</cell><cell>tokens.</cell><cell></cell><cell></cell><cell cols="2">ing training.']</cell></row></table><note><p>[</p></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0"><p>https://arxiv.org/help/bulk_data</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_1"><p>https://upwork.com/</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3" xml:id="foot_2"><p>https://api.openai.com/v1/embeddings</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4" xml:id="foot_3"><p>Two domain-experts independently judged these and achieved Cohen's ? scores of 0.91.</p></note>
		</body>
		<back>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Quantification</head><p>? What is the value of a quantitative variable? How much? How many? ? Examples -How was the ratio of toxic words in the total vocabulary? -According to the statement that the validation set is 15% of tottal dataset, how many data points are in the validation set?</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Definition</head><p>? What does X mean?</p><p>? Examples -What does "non-factoid" mean? -The result showed most dialogs in the QuAC dataset cover three to six of the chunks, but what does "chunk" mean?</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Comparison</head><p>? How is X similar to Y? How is X different from Y?</p><p>? Examples -What points in DDPM are novel compared to LDM? -Likelihood-based methods do not suffer from the model-data mismatch issue. What are the benefits of using spectral methods instead of using standard probabilistic inference?</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.3. Deep questions</head><p>The questions ask connections among the concepts in the content and elicit advanced reasoning in logical, causal, or goal-oriented system.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Causal antecedent</head><p>? What state or event causally led to an event or state?</p><p>? Examples -Why would end-users want to stylize or customize websites? -Why do approaches that train transformation modules face difficulties in accessing prior knowledge with new concepts?</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Causal consequence</head><p>? What are the consequences of an event or state? What if X occurred? What if X did not occur?</p><p>? Examples -The Low-rank Anchor Word algorithm (LAW) involves computing the QR decomposition of Y = QR. What is the additional cost incurred by this step? -The author used only 3-5 images of a user-provided concept to learn to represent it through new "words" in the embedding space, would results improve with more images? Why or why not? -While fine-tuning, the proposed method begins by unfreezing only the last layer and beginning training on that unfrozen layer only. Is this method likely to work for generative (encoder-only) models, or is this something that would work only in decoder-encoder models?</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Goal orientation</head><p>? What are the motives or goals behind an agent's action? Why did an agent do some action?</p><p>? Examples -Why was a large language model used in classifying the relation between concepts? -What are the different metrics are used in experiment 1 and 2?</p><p>Instrumental/procedural</p><p>? What plan or instrument allows an author to accomplish a goal? How did an author or author's artifact do some action?</p><p>? Examples -How did the authors handled the issues with turker's different cultural backgrounds? -How does the proposed method address the issue of catastrophic forgetting?</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Rationale</head><p>? How does the author show X (claim)? How does the result infer X (claim)? Why is it possible to say X (claim)?</p><p>? Examples -How do they show that single word embeddings capture unique and varied concepts? -How is "increased contextuality" observed in the data?</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Expectation</head><p>? Why did some expected events not occur?</p><p>? Examples -Why the patterns over increasing x-axis values are not always consistent?</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Training Details</head><p>All of our experiments were conducted using 16 A100 GPUs. To simplify all experiments, we fixed the initial learning rate to 1e-5. We trained all models until 5 epochs and selected the best checkpoint with average R-2 scores of answer compositionon validation set.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D. Instructions</head><p>Our tasks consist of (1) associative selection, (2) evidential rationale generation, and (3) systematic composition. As shown in Table <ref type="table">6</ref>, we composed instructional templates for each task.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E. Examples</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>F. Qualitative Examples of Answers G. Ethics Statement</head><p>We present a new dataset that uses papers authored by other researchers. In compliance with copyright regulations, we have limited paper selection to papers available on arXiv that has been released under a Creative Commons Attribution license.</p><p>Prior to conducting the annotation process, we obtained consent from the annotators once explaining the purpose the task.</p><p>We collected information about annotators' background in AI/ML, their number of publications, and their familiarity with</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Ext5: Towards extreme multi-task scaling for transfer learning</title>
		<author>
			<persName><forename type="first">V</forename><surname>Aribandi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Tay</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Schuster</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Rao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">S</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">V</forename><surname>Mehta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Zhuang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><forename type="middle">Q</forename><surname>Tran</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Bahri</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Ni</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">Query focused abstractive summarization: Incorporating query relevance, multi-document coverage, and summary length constraints into seq2seq models</title>
		<author>
			<persName><forename type="first">T</forename><surname>Baumel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Eyal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Elhadad</surname></persName>
		</author>
		<ptr target="https://arxiv.org/abs/1801.07704" />
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">The long-document transformer</title>
		<author>
			<persName><forename type="first">I</forename><surname>Beltagy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">E</forename><surname>Peters</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Cohan</surname></persName>
		</author>
		<author>
			<persName><surname>Longformer</surname></persName>
		</author>
		<ptr target="https://arxiv.org/abs/2004.05150" />
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Low cost evaluation in information retrieval</title>
		<author>
			<persName><forename type="first">B</forename><surname>Carterette</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Kanoulas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Yilmaz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 33rd international ACM SIGIR conference on Research and development in information retrieval</title>
		<meeting>the 33rd international ACM SIGIR conference on Research and development in information retrieval</meeting>
		<imprint>
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">Scaling instruction-finetuned language models</title>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">W</forename><surname>Chung</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Hou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Longpre</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Zoph</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Tay</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Fedus</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Dehghani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Brahma</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2210.11416</idno>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">A dataset of information-seeking questions and answers anchored in research papers</title>
		<author>
			<persName><forename type="first">P</forename><surname>Dasigi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Lo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Beltagy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Cohan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><forename type="middle">A</forename><surname>Smith</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Gardner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">North American Chapter</title>
		<imprint>
			<publisher>the Association for Computational Linguistics</publisher>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Drop: A reading comprehension benchmark requiring discrete reasoning over paragraphs</title>
		<author>
			<persName><forename type="first">D</forename><surname>Dua</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Dasigi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Stanovsky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Gardner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</title>
		<title level="s">Long and Short Papers</title>
		<meeting>the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="2368" to="2378" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Questions and challenges for the new psychology of reasoning</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">S B</forename><surname>Evans</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Thinking &amp; Reasoning</title>
		<imprint>
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="5" to="31" />
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Long form question answering</title>
		<author>
			<persName><forename type="first">A</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Jernite</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Perez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Grangier</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Weston</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Auli</surname></persName>
		</author>
		<author>
			<persName><surname>Eli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 57th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page" from="3558" to="3567" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Question asking during tutoring</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">C</forename><surname>Graesser</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><forename type="middle">K</forename><surname>Person</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">American educational research journal</title>
		<imprint>
			<biblScope unit="volume">31</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="104" to="137" />
			<date type="published" when="1994">1994</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">Mechanisms that generate questions. Questions and information systems</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">C</forename><surname>Graesser</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Person</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Huber</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1992">1992</date>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="167" to="187" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Retrieval augmented language model pre-training</title>
		<author>
			<persName><forename type="first">K</forename><surname>Guu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Tung</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Pasupat</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Chang</surname></persName>
		</author>
		<idno>PMLR</idno>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="3929" to="3938" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">Unnatural instructions: Tuning language models with (almost) no human labor</title>
		<author>
			<persName><forename type="first">O</forename><surname>Honovich</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Scialom</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Levy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Schick</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2212.09689</idno>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Leveraging passage retrieval with generative models for open domain question answering</title>
		<author>
			<persName><forename type="first">G</forename><surname>Izacard</surname></persName>
		</author>
		<author>
			<persName><forename type="first">?</forename><surname>Grave</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 16th Conference of the European Chapter</title>
		<meeting>the 16th Conference of the European Chapter</meeting>
		<imprint>
			<publisher>Main Volume</publisher>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="874" to="880" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">Few-shot learning with retrieval augmented language models</title>
		<author>
			<persName><forename type="first">G</forename><surname>Izacard</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Lewis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Lomeli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Hosseini</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Petroni</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Schick</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Dwivedi-Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Joulin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Riedel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Grave</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2208.03299</idno>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Dense passage retrieval for open-domain question answering</title>
		<author>
			<persName><forename type="first">V</forename><surname>Karpukhin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Oguz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Min</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Lewis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Edunov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W.-T</forename><surname>Yih</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2020 Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="6769" to="6781" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">Automatically generating datasets for querybased multi-document summarization</title>
		<author>
			<persName><forename type="first">S</forename><surname>Kulkarni</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Chammas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Sha</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Ie</surname></persName>
		</author>
		<author>
			<persName><surname>Aquamuse</surname></persName>
		</author>
		<ptr target="https://arxiv.org/abs/2010.12694" />
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Natural questions: A benchmark for question answering research</title>
		<author>
			<persName><forename type="first">T</forename><surname>Kwiatkowski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Palomaki</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Redfield</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Collins</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Parikh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Alberti</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Epstein</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Polosukhin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Devlin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Transactions of the Association for Computational Linguistics</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page" from="452" to="466" />
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Natural questions: A benchmark for question answering research</title>
		<author>
			<persName><forename type="first">T</forename><surname>Kwiatkowski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Palomaki</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Redfield</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Collins</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">P</forename><surname>Parikh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Alberti</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Epstein</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Polosukhin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Devlin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Toutanova</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Kelcey</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M.-W</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">M</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><forename type="middle">V</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Petrov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Transactions of the Association for Computational Linguistics</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page" from="453" to="466" />
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">Reasoning in vector space: An exploratory study of question answering</title>
		<author>
			<persName><forename type="first">M</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W.-T</forename><surname>Yih</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Smolensky</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Rouge: A package for automatic evaluation of summaries</title>
		<author>
			<persName><forename type="first">C.-Y</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Text summarization branches out</title>
		<imprint>
			<date type="published" when="2004">2004</date>
			<biblScope unit="page" from="74" to="81" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Dense hierarchical retrieval for open-domain question answering</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Hashimoto</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Yavuz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">Y</forename><surname>Philip</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Findings of the Association for Computational Linguistics: EMNLP 2021</title>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="188" to="200" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">S2ORC: The semantic scholar open research corpus</title>
		<author>
			<persName><forename type="first">K</forename><surname>Lo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">L</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Neumann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Kinney</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Weld</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2020.acl-main.447</idno>
		<ptr target="https://www.aclweb.org/anthology/2020.acl-main.447" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 58th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2020-07">July 2020</date>
			<biblScope unit="page" from="4969" to="4983" />
		</imprint>
	</monogr>
	<note type="report_type">Online</note>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">emrqa: A large corpus for question answering on electronic medical records</title>
		<author>
			<persName><forename type="first">A</forename><surname>Pampari</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Raghavan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">J</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Peng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conference on Empirical Methods in Natural Language Processing</title>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Bioread: A new dataset for biomedical reading comprehension</title>
		<author>
			<persName><forename type="first">D</forename><surname>Pappas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Androutsopoulos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Papageorgiou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Language Resources and Evaluation</title>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Exploring the limits of transfer learning with a unified text-to-text transformer</title>
		<author>
			<persName><forename type="first">C</forename><surname>Raffel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Roberts</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Narang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Matena</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">J</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Mach. Learn. Res</title>
		<imprint>
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="issue">140</biblScope>
			<biblScope unit="page" from="1" to="67" />
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Learning end-to-end goal-oriented dialog with multiple answers</title>
		<author>
			<persName><forename type="first">J</forename><surname>Rajendran</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Ganhotra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Polymenakos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2018 Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Squad: 100,000+ questions for machine comprehension of text</title>
		<author>
			<persName><forename type="first">P</forename><surname>Rajpurkar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Lopyrev</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Liang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2016 Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="2383" to="2392" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Know what you don&apos;t know: Unanswerable questions for squad</title>
		<author>
			<persName><forename type="first">P</forename><surname>Rajpurkar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Liang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 56th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<publisher>Short Papers</publisher>
			<date type="published" when="2018">2018</date>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="784" to="789" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Multitask prompted training enables zero-shot task generalization</title>
		<author>
			<persName><forename type="first">V</forename><surname>Sanh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Webson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Raffel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Bach</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Sutawika</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Alyafeai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Chaffin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Stiegler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Raja</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Dey</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
		<author>
			<persName><forename type="first">K</forename><surname>Singhal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Azizi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Tu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">S</forename><surname>Mahdavi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">W</forename><surname>Chung</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Scales</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Tanwani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Cole-Lewis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Pfohl</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2212.13138</idno>
		<title level="m">Large language models encode clinical knowledge</title>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
		<title level="m" type="main">Biomrc: A dataset for biomedical machine reading comprehension</title>
		<author>
			<persName><forename type="first">P</forename><surname>Stavropoulos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Pappas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Androutsopoulos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">T</forename><surname>Mc-Donald</surname></persName>
		</author>
		<idno>ArXiv, abs/2005.06376</idno>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
		<author>
			<persName><forename type="first">I</forename><surname>Stelmakh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Luan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Dhingra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M.-W</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName><surname>Asqa</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2204.06092</idno>
		<title level="m">Factoid questions meet long-form answers</title>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
		<author>
			<persName><forename type="first">R</forename><surname>Taylor</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Kardas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Cucurull</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Scialom</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Hartshorn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Saravia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Poulton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Kerkez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Stojnic</surname></persName>
		</author>
		<author>
			<persName><surname>Galactica</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2211.09085</idno>
		<title level="m">A large language model for science</title>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b34">
	<monogr>
		<author>
			<persName><forename type="first">R</forename><surname>Thoppilan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">D</forename><surname>Freitas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Hall</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Kulshreshtha</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H.-T</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Bos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Baker</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">S</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Ghafouri</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Menegali</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Krikun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Lepikhin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Qin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Roberts</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Bosma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C.-C</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Krivokon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Rusch</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Pickett</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Srinivasan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Man</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Meier-Hellstern</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">R</forename><surname>Morris</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Doshi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">D</forename><surname>Santos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Duke</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Soraker</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Zevenbergen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Prabhakaran</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Diaz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Hutchinson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Olson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Molina</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Hoffman-John</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Aroyo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Rajakumar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Butryna</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Lamm</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Kuzmina</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Fenton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Cohen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Bernstein</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Kurzweil</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Aguera-Arcas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Croak</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Chi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName><surname>Lamda</surname></persName>
		</author>
		<title level="m">Language models for dialog applications</title>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Newsqa: A machine comprehension dataset</title>
		<author>
			<persName><forename type="first">A</forename><surname>Trischler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Harris</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Sordoni</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Bachman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Suleman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2nd Workshop on Representation Learning for NLP</title>
		<meeting>the 2nd Workshop on Representation Learning for NLP</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="191" to="200" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Neural correlates of dual-task effect on belief-bias syllogistic reasoning: a near-infrared spectroscopy study</title>
		<author>
			<persName><forename type="first">T</forename><surname>Tsujii</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Watanabe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Brain research</title>
		<imprint>
			<biblScope unit="volume">1287</biblScope>
			<biblScope unit="page" from="118" to="125" />
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<monogr>
		<title level="m" type="main">Squality: Building a long-document summarization dataset the hard way</title>
		<author>
			<persName><forename type="first">A</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">Y</forename><surname>Pang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Phang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">R</forename><surname>Bowman</surname></persName>
		</author>
		<ptr target="https://arxiv.org/abs/2205.11465" />
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<monogr>
		<title level="m" type="main">Self-instruct: Aligning language model with self generated instructions</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Kordi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Mishra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><forename type="middle">A</forename><surname>Smith</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Khashabi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Hajishirzi</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2212.10560</idno>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Dual processes in reasoning?</title>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">C</forename><surname>Wason</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">S B</forename><surname>Evans</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Cognition</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="141" to="154" />
			<date type="published" when="1974">1974</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<monogr>
		<author>
			<persName><forename type="first">J</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Bosma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><forename type="middle">Y</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Guu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">W</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Lester</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">M</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><forename type="middle">V</forename><surname>Le</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2109.01652</idno>
		<title level="m">Finetuned language models are zero-shot learners</title>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b41">
	<monogr>
		<title level="m" type="main">Towards aicomplete question answering: A set of prerequisite toy tasks</title>
		<author>
			<persName><forename type="first">J</forename><surname>Weston</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Bordes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Chopra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">M</forename><surname>Rush</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Van Merri?nboer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Joulin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Mikolov</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1502.05698</idno>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">How do we answer complex questions: Discourse structure of long-form answers</title>
		<author>
			<persName><forename type="first">F</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Choi</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2022.acl-long.249</idno>
		<ptr target="https://aclanthology.org/2022.acl-long.249" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 60th Annual Meeting of the Association for Computational Linguistics<address><addrLine>Dublin, Ireland</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2022-05">May 2022</date>
			<biblScope unit="page" from="3556" to="3572" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<monogr>
		<title level="m" type="main">A critical evaluation of evaluations for long-form question answering</title>
		<author>
			<persName><forename type="first">F</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Iyyer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Choi</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2305.18201</idno>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">QMSum: A new benchmark for querybased multi-domain meeting summarization</title>
		<author>
			<persName><forename type="first">M</forename><surname>Zhong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Yin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Zaidi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Mutuma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Jha</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">H</forename><surname>Awadallah</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Celikyilmaz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Qiu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Radev</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2021.naacl-main.472</idno>
		<ptr target="https://aclanthology.org/2021.naacl-main.472" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</title>
		<meeting>the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</meeting>
		<imprint>
			<date type="published" when="2021-06">June 2021</date>
			<biblScope unit="page" from="5905" to="5921" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
