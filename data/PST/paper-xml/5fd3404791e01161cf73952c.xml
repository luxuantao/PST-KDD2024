<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Bipartite Graph Embedding via Mutual Information Maximization</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date type="published" when="2020-12-10">10 Dec 2020</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Jiangxia</forename><surname>Cao</surname></persName>
							<email>caojiangxia@iie.ac.cn</email>
						</author>
						<author>
							<persName><forename type="first">Xixun</forename><surname>Lin</surname></persName>
							<email>linxixun@iie.ac.cn</email>
						</author>
						<author>
							<persName><forename type="first">Shu</forename><surname>Guo</surname></persName>
							<email>guoshu@cert.org.cn</email>
						</author>
						<author>
							<persName><forename type="first">Luchen</forename><surname>Liu</surname></persName>
							<email>liuluchen@iie.ac.cn</email>
						</author>
						<author>
							<persName><forename type="first">Tingwen</forename><surname>Liu</surname></persName>
							<email>liutingwen@iie.ac.cn</email>
						</author>
						<author>
							<persName><forename type="first">Bin</forename><surname>Wang</surname></persName>
							<email>wangbin11@xiaomi.com</email>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">Institute of Information Engineering</orgName>
								<orgName type="department" key="dep2">School of Cyber Security</orgName>
								<orgName type="institution" key="instit1">Chinese Academy of Sciences</orgName>
								<orgName type="institution" key="instit2">University of Chinese Academy of Sciences</orgName>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff1">
								<orgName type="department">National Computer Network Emergency Response Technical Team/Coordination Center</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff2">
								<orgName type="department" key="dep1">Institute of Information Engineering</orgName>
								<orgName type="department" key="dep2">School of Cyber Security</orgName>
								<orgName type="institution" key="instit1">Chinese Academy of Sciences</orgName>
								<orgName type="institution" key="instit2">University of Chinese Academy of Sciences</orgName>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff3">
								<orgName type="department">Xiaomi AI Lab</orgName>
								<orgName type="institution">Xiaomi Inc</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Bipartite Graph Embedding via Mutual Information Maximization</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2020-12-10">10 Dec 2020</date>
						</imprint>
					</monogr>
					<idno type="DOI">10.1145/XXXXXX.XXXXXX</idno>
					<idno type="arXiv">arXiv:2012.05442v1[cs.SI]</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.2" ident="GROBID" when="2022-12-25T13:12+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Bipartite Graph Embedding</term>
					<term>Global Properties</term>
					<term>Mutual Information Maximization</term>
					<term>Recommender System</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Bipartite graph embedding has recently attracted much attention due to the fact that bipartite graphs are widely used in various application domains. Most previous methods, which adopt random walk-based or reconstruction-based objectives, are typically effective to learn local graph structures. However, the global properties of bipartite graph, including community structures of homogeneous nodes and long-range dependencies of heterogeneous nodes, are not well preserved. In this paper, we propose a bipartite graph embedding called BiGI to capture such global properties by introducing a novel local-global infomax objective. Specifically, BiGI first generates a global representation which is composed of two prototype representations. BiGI then encodes sampled edges as local representations via the proposed subgraph-level attention mechanism. Through maximizing the mutual information between local and global representations, BiGI enables nodes in bipartite graph to be globally relevant. Our model is evaluated on various benchmark datasets for the tasks of top-K recommendation and link prediction. Extensive experiments demonstrate that BiGI achieves consistent and significant improvements over state-of-the-art baselines. Detailed analyses verify the high effectiveness of modeling the global properties of bipartite graph.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>CCS CONCEPTS</head><p>• Information systems → Data mining; • Computing methodologies → Neural networks.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">INTRODUCTION</head><p>Bipartite graph is a general structure to model the relationship between two node types. It has been widely adopted in many realworld applications, arranging from recommender system <ref type="bibr" target="#b34">[34]</ref>, drug discovery <ref type="bibr" target="#b39">[39]</ref> to information retrieval <ref type="bibr" target="#b43">[43]</ref>. For instance, in recommender systems, user and item represent two node types. The interactions between users and items are formed as a bipartite graph, where observed edges record previous purchasing behaviours of users. Furthermore, different from heterogeneous graphs, bipartite graph has its own structural characteristics, e.g., there are no direct links between nodes of the same type.</p><p>Learning meaningful node representations for bipartite graphs is a long-standing challenge. Recently, a significant amount of progresses have been made toward the graph embedding paradigm <ref type="bibr" target="#b2">[2,</ref><ref type="bibr" target="#b5">5,</ref><ref type="bibr" target="#b14">14]</ref>. Although they work pretty well in the settings of homogeneous and heterogeneous graphs, most of them are not tailored for modeling bipartite graphs. As a result, they are suboptimal to learn bipartite graph embedding <ref type="bibr" target="#b7">[7,</ref><ref type="bibr" target="#b8">8]</ref>. To remedy such a problem, several studies have been specifically proposed for modeling bipartite graphs. They can be roughly divided into two branches: random walk-based and reconstruction-based methods. The former <ref type="bibr" target="#b7">[7,</ref><ref type="bibr" target="#b8">8,</ref><ref type="bibr" target="#b44">44]</ref> relies on designing the heuristics of random walks to generate different node sequences. Afterwards, they learn node representations via predicting context nodes within a sliding window <ref type="bibr" target="#b45">[45]</ref>. The reconstruction-based works <ref type="bibr" target="#b15">[15,</ref><ref type="bibr" target="#b32">32,</ref><ref type="bibr" target="#b34">34,</ref><ref type="bibr" target="#b37">37,</ref><ref type="bibr" target="#b40">40,</ref><ref type="bibr" target="#b42">42]</ref> are closely related with collaborative filtering <ref type="bibr" target="#b28">[28]</ref>. They attempt to reconstruct the adjacency matrix by learning different encoders. In particular, some works <ref type="bibr" target="#b34">[34,</ref><ref type="bibr" target="#b37">37,</ref><ref type="bibr" target="#b40">40,</ref><ref type="bibr" target="#b42">42]</ref> train graph neural networks (GNNs) <ref type="bibr" target="#b9">[9,</ref><ref type="bibr" target="#b20">20,</ref><ref type="bibr" target="#b22">22,</ref><ref type="bibr" target="#b38">38]</ref> to learn node representations via aggregating features of neighborhood nodes recursively. The orange shaded area represents a underlying community structure where three movies may share similar genres. The blue dotted lines denote the long-range dependency between "Lily" and "Ice Age". However, these global properties are hard to be well learned from local graph structures.</p><p>Above methods achieve promising results to some extent, but they mainly focus on learning local graph structures with the assumption that nodes within the sliding window or neighborhoods are closely relevant <ref type="bibr" target="#b7">[7,</ref><ref type="bibr" target="#b37">37,</ref><ref type="bibr" target="#b41">41]</ref>. We argue that they lack the capability of better modeling the global properties of bipartite graph including community structures of homogeneous nodes and longrange dependencies of heterogeneous nodes. A concrete example is shown in Figure <ref type="figure" target="#fig_0">1</ref>. In the user-movie bipartite graph, the movies "Lion King", "Ice Age" and "Toy Story" can be regarded as belonging to the same group since they have similar genres, but the community structure of these three homogeneous nodes is not well preserved by previous methods, due to the fact that "Lion King" is unreachable to "Ice Age" and "Toy Story". In addition, because "Lily" and "Ice Age" are distant from each other, the long-range dependency between these two heterogeneous nodes is also hard to be revealed from the local graph structures of them, even "Lily" is likely to be interested with "Ice Age".</p><p>To recognize the global properties of bipartite graph, we propose a novel Bipartite Graph embedding called BiGI via mutual Information maximization. Specifically, BiGI first introduces a global representation which is composed of two prototype representations, and each prototype representation is generated by aggregating the corresponding homogeneous nodes. BiGI then encodes sampled edges as local representations via the proposed subgraph-level attention mechanism. On top of that, we develop a novel local-global infomax objective to maximize the mutual information (MI) between local and global representations. In this way, our infomax objective can preserve community structures of homogeneous nodes via maximizing the MI between each node and its homogeneous prototype. Simultaneously, long-range dependencies of heterogeneous nodes are also captured by maximizing the MI between each node and its heterogeneous prototype. The main contributions of our work are as follows,</p><p>• We propose a novel bipartite graph embedding called BiGI to capture the global properties of bipartite graph including community structures of homogeneous nodes and longrange dependencies of heterogeneous nodes.</p><p>• A novel local-global infomax objective is developed via integrating the information of two node types into local and global representations. The global representation is composed of two prototype representations, and the local representation is further armed with an h-hop enclosing subgraph to preserve the rich interaction information of sampled edge. • Our model is evaluated on multiple benchmark datasets for the tasks of top-K recommendation and link prediction. Experimental results demonstrate that our method yields consistent and significant improvements over state-of-the-art baselines<ref type="foot" target="#foot_0">1</ref> .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">RELATED WORK 2.1 Bipartite Graph Embedding</head><p>Homogeneous and heterogeneous graph embeddings are usually used for modeling bipartite graphs. The pioneering homogeneous graph methods include DeepWalk <ref type="bibr" target="#b27">[27]</ref>, LINE <ref type="bibr" target="#b33">[33]</ref>, Node2vec <ref type="bibr" target="#b11">[11]</ref> and VGAE <ref type="bibr" target="#b19">[19]</ref>. Some representative heterogeneous graph methods are Metapath2vec <ref type="bibr" target="#b6">[6]</ref> and DMGI <ref type="bibr" target="#b41">[41]</ref>. But they are not tailored for bipartite graphs, and the structural characteristics of bipartite graph are hard to be preserved by them. IGE <ref type="bibr" target="#b44">[44]</ref>, PinSage <ref type="bibr" target="#b40">[40]</ref>, BiNE <ref type="bibr" target="#b7">[7]</ref> and FOBE <ref type="bibr" target="#b32">[32]</ref> are specially designed for bipartite graphs. However, as mentioned in the introduction, they mainly focus on how to model local graph structures in the latent space.</p><p>Matrix completion <ref type="bibr" target="#b34">[34,</ref><ref type="bibr" target="#b42">42]</ref> and collaborative filtering <ref type="bibr" target="#b15">[15,</ref><ref type="bibr" target="#b37">37]</ref> are also connected with modeling bipartite graphs closely. They propose various DNNs to solve recommendation tasks. For example, GC-MC <ref type="bibr" target="#b34">[34]</ref> uses one relation-aware graph convolution layer to learn node embeddings, thus only the direct links in user-item bipartite graphs are exploited. NGCF <ref type="bibr" target="#b37">[37]</ref> incorporates collaborative signals into the embedding process by aggregating features of neighborhood nodes. However, it still overlooks the importance of modeling the global properties of bipartite graph.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Mutual Information Maximization</head><p>Maximizing the MI between inputs and corresponding latent embeddings provides a desirable paradigm for the unsupervised learning <ref type="bibr" target="#b35">[35]</ref>. However, estimating MI is generally intractable in highdimensional continuous settings <ref type="bibr" target="#b25">[25]</ref>. MINE <ref type="bibr" target="#b1">[1]</ref> derives a lower bound of MI and works by training a discriminator to distinguish samples coming from the joint distribution of two random variables or the product of their marginals. DIM <ref type="bibr" target="#b16">[16]</ref> introduces the structural information into input patches and adopts different infomax objectives.</p><p>DGI <ref type="bibr" target="#b36">[36]</ref> is the first work that applies the infomax objective to homogeneous graphs. It provides a new approach for the task of unsupervised node classification. Based on DIM, InfoGraph <ref type="bibr" target="#b31">[31]</ref> tries to learn unsupervised graph representations via maximizing the MI between the graph-level representation and the representations of substructures. DMGI <ref type="bibr" target="#b41">[41]</ref> extends DGI into heterogeneous graphs. It splits the original graph into multiple homogeneous ones and adopts the infomax objective used in DGI for modeling split graphs. So DMGI still puts more emphasis on learning the correlation of homogeneous nodes. GMI <ref type="bibr" target="#b26">[26]</ref> proposes a new approach  The yellow dotted lines (Eq.( <ref type="formula" target="#formula_0">1</ref>) and Eq.( <ref type="formula" target="#formula_2">3</ref>)) and the green dotted lines (Eq.(2) and Eq.( <ref type="formula" target="#formula_3">4</ref>)) demonstrate how to derive node embeddings 𝒖 𝑘 𝑖 and 𝒗 𝑘 𝑛 .</p><p>to measure the MI between input homogeneous graphs and node embeddings directly. Compared with them, we combine two types of node information for generating local and global representations and develop a novel infomax objective that is more suitable for bipartite graphs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">BACKGROUND</head><p>We begin by providing the background of our work </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">PROPOSED MODEL</head><p>A novel bipartite graph embedding termed as BiGI is proposed from the perspective of mutual information maximization. We first describe a basic bipartite graph encoder to generate the initial node representations. Taking these node representations as the inputs of our framework, we then demonstrate how to construct the global representation, local representations and the local-global infomax objective. The detailed model analysis is provided in the end.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Bipartite Graph Encoder</head><p>In this section, we introduce a basic bipartite graph encoder following the principle of GNN to learn the initial node representations. The proposed encoder is well compatible with our infomax objective. Compared with other GNN encoders <ref type="bibr" target="#b37">[37,</ref><ref type="bibr" target="#b40">40]</ref> for bipartite graphs, it achieves promising performances empirically. Different from homogeneous graphs, each node in bipartite graph is not the same type as its adjacent nodes. Therefore, directly updating the node embedding via aggregating features of its one-hop neighbors is ill-posed. To alleviate such an issue, our encoder attempts to learn each node embedding from its two-hop neighbors in each layer. As shown in Figure <ref type="figure" target="#fig_1">2</ref>, both of the learning processes of 𝒖 𝑘−1 𝑖 and 𝒗 𝑘−1 𝑛 have two operations in the 𝑘-th layer. Taking 𝒖 𝑘−1 𝑖 for example ((a) and (b) in Figure <ref type="figure" target="#fig_1">2</ref>), we first generate temporary neighborhood representations, e.g., 𝒗 𝑘 𝑗 via a mean operation (MEAN) with a non-linear transformation:</p><formula xml:id="formula_0">𝒗 𝑘 𝑗 = 𝛿 𝑊 𝑘 𝑣 • MEAN {𝒖 𝑘−1 𝑖 : 𝑢 𝑖 ∈ N (𝑣 𝑗 )} ,<label>(1)</label></formula><p>where 𝛿 denotes the LeakyReLU activation function, 𝑊 𝑘 𝑣 is a weight matrix and N (𝑣 𝑗 ) denotes one-hop neighbors of 𝑣 𝑗 . In contrast with common graph convolutional operators <ref type="bibr" target="#b9">[9,</ref><ref type="bibr" target="#b13">13,</ref><ref type="bibr" target="#b20">20]</ref>, we only aggregate neighborhood features, and the own feature 𝒗 𝑘−1 𝑗 is not involved in Eq.( <ref type="formula" target="#formula_0">1</ref>). Hence, 𝒗 𝑘 𝑗 can be approximately regarded as a 𝑢-type node embedding. Afterwards, we use homogeneous graph convolution to obtain 𝒖 𝑘 𝑖 :</p><formula xml:id="formula_1">𝒖 𝑘 𝑖 = 𝛿 𝑊 𝑘 𝑢 • MEAN { 𝒗 𝑘 𝑗 : 𝑣 𝑗 ∈ N (𝑢 𝑖 )} , 𝒖 𝑘 𝑖 = 𝑊 𝑘 𝑢 • 𝒖 𝑘 𝑖 𝒖 𝑘−1 𝑖 ,<label>(2)</label></formula><p>where 𝑊 </p><formula xml:id="formula_2">𝒖 𝑘 𝑚 = 𝛿 𝑊 𝑘 𝑢 • MEAN {𝒗 𝑘−1 𝑛 : 𝑣 𝑛 ∈ N (𝑢 𝑚 )} .<label>(3)</label></formula><p>The final node embedding 𝒗 𝑘 𝑛 is defined as:</p><formula xml:id="formula_3">𝒗 𝑘 𝑛 = 𝛿 𝑊 𝑘 𝑣 • MEAN { 𝒖 𝑘 𝑚 : 𝑢 𝑚 ∈ N (𝑣 𝑛 )} , 𝒗 𝑘 𝑛 = 𝑊 𝑘 𝑣 • 𝒗 𝑘 𝑛 𝒗 𝑘−1 𝑛 .<label>(4)</label></formula><p>𝑊 𝑘 𝑢 , 𝑊 𝑘 𝑣 and 𝑊 𝑘 𝑣 in Eq.( <ref type="formula" target="#formula_2">3</ref>) and Eq.( <ref type="formula" target="#formula_3">4</ref>) are also weight matrices. Dropout <ref type="bibr" target="#b30">[30]</ref> is applied to each layer of our encoder to regularize model parameters.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Local-Global Infomax</head><p>Building upon the generated node representations, in this section, we first present the calculations of global and local representations. A novel local-global infomax objective is then developed to capture the global properties of bipartite graph.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.1">Global Representation.</head><p>The global representation is a holistic representation of bipartite graph, which is generated via a simple composition function (COM) that combines two prototype representations. Specifically, for each node type, we introduce a prototype representation to aggregate all homogeneous node information. Our insight is similar to the classic few-shot learning <ref type="bibr" target="#b29">[29]</ref> which would generate a prototype representation of each class. There are many choices to induce the prototype representation. In our work, we also adopt the mean operation which averages the information of all homogeneous nodes to obtain the corresponding prototype representation. The concrete procedures can be formulated as follows,</p><formula xml:id="formula_4">𝒑 𝑢 = MEAN {𝒖 𝑖 : 𝑢 𝑖 ∈ 𝑈 } , 𝒑 𝑣 = MEAN {𝒗 𝑖 : 𝑣 𝑖 ∈ 𝑉 } , 𝒈 = COM 𝒑 𝑢 , 𝒑 𝑣 = 𝜎 (𝒑 𝑢 ) 𝜎 (𝒑 𝑣 ) ,<label>(5)</label></formula><p>where 𝒖 𝑖 and 𝒗 𝑖 denote the outputs of our encoder. 𝒈 is the global representation composed of two prototype representations 𝒑 𝑢 and 𝒑 𝑣 . For efficiency, we select the simple concatenation operation with the sigmoid activation function 𝜎 as our composition function.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.2">Local</head><p>Representation. Each input of local representation is a bipartite edge i.e., (𝑢, 𝑣), and we further arm it with an h-hop enclosing subgraph <ref type="bibr" target="#b42">[42]</ref> to describe the surrounding environment of (𝑢, 𝑣). The concrete definition of h-hop enclosing subgraph is given below. For a specific edge (𝑢, 𝑣) ∈ 𝐸 with the corresponding h-hop enclosing subgraph 𝐺 ℎ (𝑢,𝑣) (The subscripts of 𝑢 and 𝑣 are omitted for simplicity), we use an attention mechanism (ATT) to calculate the local representation. Given node 𝑢 and node 𝑣 𝑖 ∈ 𝐺 ℎ (𝑢), the attention weight 𝛼 𝑢,𝑖 can be expressed as:</p><formula xml:id="formula_5">𝛼 𝑢,𝑖 = exp 𝑊 𝑎 • 𝒗 𝑖 𝑇 • 𝑊 ′ 𝑎 • 𝒖 𝑣 𝑗 ∈𝐺 ℎ (𝑢) exp 𝑊 𝑎 • 𝒗 𝑗 𝑇 • 𝑊 ′ 𝑎 • 𝒖 ,<label>(6)</label></formula><p>where 𝑇 denotes the transpose operation,𝑊 𝑎 and𝑊 ′ 𝑎 are two shared trainable matrices. The similar calculation procedure for node 𝑣 and node 𝑢 𝑖 ∈ 𝐺 ℎ (𝑣) can be defined as:</p><formula xml:id="formula_6">𝛼 𝑣,𝑖 = exp 𝑊 ′ 𝑎 • 𝒖 𝑖 𝑇 • 𝑊 𝑎 • 𝒗 𝑢 𝑗 ∈𝐺 ℎ (𝑣) exp 𝑊 ′ 𝑎 • 𝒖 𝑗 𝑇 • 𝑊 𝑎 • 𝒗 . (<label>7</label></formula><formula xml:id="formula_7">)</formula><p>The final representation of local input 𝒈 ℎ (𝑢,𝑣) is formulated as:</p><formula xml:id="formula_8">𝒈 ℎ (𝑢,𝑣) = 𝜎 ∑︁ 𝑣 𝑖 ∈𝐺 ℎ (𝑢) 𝛼 𝑢,𝑖 𝒗 𝑖 + 𝒖 𝜎 ∑︁ 𝑢 𝑖 ∈𝐺 ℎ (𝑣) 𝛼 𝑣,𝑖 𝒖 𝑖 + 𝒗 . (<label>8</label></formula><formula xml:id="formula_9">)</formula><p>The local attentive representation also combines different local environments together via the same composition function used in Eq. <ref type="bibr" target="#b5">(5)</ref>. It not only highlights the central role of (𝑢, 𝑣), but also adaptively assigns different importance factors to neighboring nodes by the subgraph-level attention mechanism. </p><formula xml:id="formula_10">𝑆 𝑖,𝑗 = Bernoulli(𝛽), 𝐺 =(𝑈 , 𝑉 , 𝐸) = C(𝐺, 𝛽) = 𝐴 ⊕ 𝑆,<label>(9)</label></formula><p>where 𝛽 is the corruption rate, ⊕ denotes the XOR (exclusive-OR) operation, 𝐺 is the corrupted graph and 𝐸 is the corresponding set of corrupted edges. The concrete loss function is defined as:</p><formula xml:id="formula_11">L 𝑚 = − 1 |𝐸| + | 𝐸| |𝐸 | ∑︁ 𝑖=1 E 𝐺 logD (𝒈 ℎ (𝑢,𝑣) 𝑖 , 𝒈) + | 𝐸 | ∑︁ 𝑖=1 E 𝐺 log 1 − D ( 𝒈 ℎ (𝑢,𝑣) 𝑖 , 𝒈) .<label>(10)</label></formula><p>Here, D is a discriminator to score local-global representations via a bilinear mapping function:</p><formula xml:id="formula_12">D (𝒈 ℎ (𝑢,𝑣) 𝑖 , 𝒈) = 𝜎 (𝒈 ℎ (𝑢,𝑣) 𝑖 ) 𝑇 𝑊 𝑏 𝒈 ,<label>(11)</label></formula><p>where 𝑊 𝑏 is a weight matrix. The binary cross-entropy loss in Eq.( <ref type="formula" target="#formula_11">10</ref>) is an effective MI estimator. It can maximize the MI between 𝒈 ℎ (𝑢,𝑣) 𝑖 and 𝒈, based on Jensen-Shannon divergence between the joint distribution and the product of marginals. Because it follows a standard minmax game originated from the generative adversarial network (GAN) <ref type="bibr" target="#b10">[10]</ref>, and the "GAN" distance and Jensen-Shannon divergence are closely related <ref type="bibr" target="#b24">[24]</ref>.</p><p>From Eq.( <ref type="formula" target="#formula_4">5</ref>), we can observe that the information of two node types is integrated into the global representation via the generated prototype representations, and these two prototypes are not entangled together. Through Eq.( <ref type="formula" target="#formula_11">10</ref>) and Eq. <ref type="bibr" target="#b11">(11)</ref>, each node has access to the homogeneous prototype and to the heterogeneous prototype simultaneously, which enables our model to break the limit of local graph topology. Therefore, the global properties can be naturally captured even the correlated nodes in bipartite graph are distant from each other.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Model Training</head><p>The total loss function L contains two terms:</p><formula xml:id="formula_13">L = 𝜆L 𝑚 + (1 − 𝜆)L 𝑟 , (<label>12</label></formula><formula xml:id="formula_14">)</formula><p>where 𝜆 is the harmonic factor. L 𝑟 is a margin-based ranking loss over observed edges for our encoder, which is formulated as follows,</p><formula xml:id="formula_15">L 𝑟 = ∑︁ (𝑢,𝑣) ∈𝐸 ∑︁ (𝑢 ′ ,𝑣 ′ ) ∈𝐸 ′ (𝑢,𝑣) 𝛾 + 𝜙 [𝒖 ′ |𝒗 ′ ] − 𝜙 [𝒖|𝒗] + , (<label>13</label></formula><formula xml:id="formula_16">)</formula><p>where 𝜙 is a ranking function parameterized by a two-layer multilayer perceptron (MLP), [𝑥] + denotes the positive part of 𝑥 and 𝛾 is a margin. 𝐸 ′ (𝑢,𝑣) is the set of negative node pairs, which can be defined as:</p><formula xml:id="formula_17">𝐸 ′ (𝑢,𝑣) = (𝑢 ′ , 𝑣)|𝑢 ′ ∈ 𝑈 ∪ (𝑢, 𝑣 ′ )|𝑣 ′ ∈ 𝑉 .<label>(14)</label></formula><p>The negative sampling used in Eq.( <ref type="formula" target="#formula_17">14</ref>) is similar to <ref type="bibr" target="#b12">[12,</ref><ref type="bibr" target="#b21">21]</ref>: 𝐸 ′ (𝑢,𝑣) is composed of real interactions with either the head or tail replaced by a random node from the same node set. BiGI is an end-to-end model which is optimized by Adam <ref type="bibr" target="#b17">[17]</ref>. The whole architecture of BiGI is shown in Figure <ref type="figure">3</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Model Analysis</head><p>4.4.1 Time Complexity. The main operations of BiGI are learning the initial node representations and calculating the total loss. To avoid parameter overhead, we use the shared encoder to learn node representations of 𝐺 and 𝐺. The computational complexity of BiGI is approximated as 𝑂 (𝑘 (|𝐸| + | 𝐸|)𝑑 2 ), where 𝑘 is the number of layers in our encoder and 𝑑 is the embedding size. In addition, we also provide an experimental comparison to verify that our model can be deployed to large-scale bipartite graphs in Section 5.3.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.2">Relation with DGI.</head><p>Our model is closely related to DGI, since they use a local-global infomax objective on graphs. However, there are important design differences between them. 1) BiGI focuses on modeling bipartite graphs, which integrates the information of two node types into local and global representations. By contrast, DGI is designed for homogeneous node embeddings. 2) DGI tries to maximize the MI between node-level and graph-level representations, while we actually maximize the MI between subgraph-level and graph-level representations. The subgraph-level representations are capable of effectively preserving rich interactions of sampled edges.</p><p>3) The choice of encoders is different. Considering the structural characteristics of bipartite graphs, we design a novel basic encoder to learn initial node representations. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">EXPERIMENTS 5.1 Datasets</head><p>Four benchmark datasets, i.e., DBLP 2 , MovieLens-100K (ML-100K) 3 , MovieLens-10M (ML-10M) 4 and Wikipedia 5 are used in experiments. DBLP, ML-100K and ML-10M are adopted for top-K recommendation. Wikipedia is used for link prediction. We convert their user-item interaction matrices into the implicit data. The concrete statistics of them are listed in Table <ref type="table" target="#tab_3">1</ref>. From it, we can observe that ML-10M is much larger than other datasets, since it is used to test whether our model can be deployed to large-scale bipartite graphs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.1">Data Preprocessing.</head><p>As used in BiNE <ref type="bibr" target="#b7">[7]</ref>, we select 60% edges for training and remaining edges for test in both of DBLP and ML-10M. We use the same division in IGMC <ref type="bibr" target="#b42">[42]</ref> for ML-100K. Following experimental settings in the previous work <ref type="bibr" target="#b8">[8]</ref>, we split Wikipedia into two datasets, i.e., Wiki (5:5) and Wiki (4:6). The training/test ratios of these two datasets are 5:5 and 4:6, respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Experimental Setting</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.1">Evaluation Metrics.</head><p>In top-K recommendation, for each user, we first filter out some items that the user has already interacted with in training process. Then, we rank remaining items and evaluate ranking results with the following evaluation metrics: 𝐹 1 score, 𝑁 𝐷𝐶𝐺 (Normalized Discounted Cumulative Gain), 𝑀𝐴𝑃 (Mean Average Precision) and 𝑀𝑅𝑅 (Mean Reciprocal Rank). All of these metrics are widely used in recommendation tasks. Two common metrics are used to evaluate the results of link prediction: 𝐴𝑈 𝐶-𝑅𝑂𝐶 (area under the ROC curve) and 𝐴𝑈𝐶-𝑃𝑅 (area under the Precison-Recall curve).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.2">Compared</head><p>Baselines. We compare our model with the following strong baselines which can be divided into:</p><p>• Homogeneous graph embedding: DeepWalk <ref type="bibr" target="#b27">[27]</ref>, LINE <ref type="bibr" target="#b33">[33]</ref>, Node2vec <ref type="bibr" target="#b11">[11]</ref> and VGAE <ref type="bibr" target="#b19">[19]</ref>. DeepWalk and Node2vec are typically random-walk based. LINE learns a joint probability distribution of connected nodes, and LINE (2nd) is exploited here due to its expressive performances. Based on variational auto-encoder <ref type="bibr" target="#b18">[18]</ref>, VGAE adopts the graph convolutional network (GCN) <ref type="bibr" target="#b20">[20]</ref> as the basic encoder to learn graph-structured data.  Model F1@10 NDCG@3 NDCG@5 NDCG@10 MAP@3 MAP@5 MAP@10 MRR@3 MRR@5 MRR@10 DeepWalk  Model F1@10 NDCG@3 NDCG@5 NDCG@10 MAP@3 MAP@5 MAP@10 MRR@3 MRR@5 MRR@10 DeepWalk 14.20 7.17  Model F1@10 NDCG@3 NDCG@5 NDCG@10 MAP@3 MAP@5 MAP@10 MRR@3 MRR@5 MRR@10 DeepWalk 7.25 3.12 4.39  learn node embeddings. DMGI <ref type="bibr" target="#b41">[41]</ref> also follows the principle of MI maximization, and it uses the same infomax objective in DGI <ref type="bibr" target="#b36">[36]</ref>. • Bipartite graph embedding: PinSage <ref type="bibr" target="#b40">[40]</ref> and BiNE <ref type="bibr" target="#b7">[7]</ref>.</p><p>PinSage integrates random walk into GNN architectures for high-scalable performances. BiNE jointly optimizes explicit and implicit relations in a unified framework. • Matrix completion: GC-MC <ref type="bibr" target="#b34">[34]</ref> and IGMC <ref type="bibr" target="#b42">[42]</ref>. GC-MC introduces a relation-aware graph auto-encoder to learn embeddings of users and items. These representations are then used to reconstruct the rating links through a bilinear decoder. IGMC proposes a novel GNN based on local subgraphs for the task of inductive matrix completion. • Collaborative filtering: NeuMF <ref type="bibr" target="#b15">[15]</ref> and NGCF <ref type="bibr" target="#b37">[37]</ref>. NeuMF uses MLP to learn the nonlinear interactions between user and item embeddings. NGCF considers the high-order connectivity via the proposed embedding propagation layer.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.3">Implementation Details.</head><p>PinSage is implemented by ourselves.</p><p>Except from it, we use official implementations of other methods.</p><p>To make a fair comparison, the side information of nodes is not exploited in all experiments. The embedding size 𝑑 is fixed as 128, the learning rate is 0.001 and all models are iterated with 100 epochs for convergence. For making a good trade-off between effectiveness and efficiency, we use 1-hop enclosing subgraphs as suggested by IGMC <ref type="bibr" target="#b42">[42]</ref>. The depth of our encoder 𝑘 (the number of stacked layers) is 2. The margin 𝛾 used in Eq.( <ref type="formula" target="#formula_15">13</ref>) is 0.3. the corruption rate 𝛽 is selected from {1e-6, 1e-5, 1e-4, 1e-3, 1e-2, 1e-1}, and the harmonic factor 𝜆 is selected from 0.1 to 0.9 with step length 0.2.</p><p>To verify whether the results of our model are statistically significant, we perform paired t-test for each dataset. In addition, the results of BiNE on ML-10M are not provided. Although we use the official implementation of BiNE<ref type="foot" target="#foot_1">6</ref> for large-scale bipartite graphs, the concrete results of it are hard to be well reproduced (The generation of node sequences has not finished within 72 hours).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3">Top-K Recommendation</head><p>Table <ref type="table" target="#tab_5">2</ref>, Table <ref type="table" target="#tab_7">3</ref> and Table <ref type="table" target="#tab_8">4</ref> demonstrate the performances of compared methods on DBLP, ML-100K and ML-10M. The best performance is in boldface and the second is underlined. From them, we have the following observations. 1) Our method consistently yields the best performances on these datasets for all metrics. It demonstrates the high effectiveness of learning the global properties of bipartite graph. 2) Modeling the structural characteristics of bipartite graph is very important. Homogeneous and heterogeneous graph embeddings ignore such characteristics, and they are inferior to BiGI and to other bipartite graph embeddings. 3) It should be noticed that DMGI also maximizes MI between local and global representations, but the performance of it is not satisfying. Therefore, designing a suitable infomax objective for bipartite graphs plays a central role in our work.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.4">Link Prediction</head><p>For the task of link prediction, given a node pair (𝑢 𝑖 , 𝑣 𝑗 ), we feed the corresponding embeddings 𝒖 𝑖 and 𝒗 𝑗 into a logistic regression classifier which is trained on the observed edges of bipartite graph. Table <ref type="table" target="#tab_10">5</ref> shows the performances of all models, and our method achieves higher predictive results on both datasets. It demonstrates that the global properties of bipartite graph are beneficial to learn node representations. In particular, capturing long-range dependencies of heterogeneous nodes is helpful to down-stream tasks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.5">Discussions of Model Variants</head><p>We investigate the effects of different local representations and the extensibility of proposed infomax objective. The results of these model variants and an ablation study of the proposed encoder are provided in Table <ref type="table">6</ref>. The experiments are conducted on DBLP. BiGI (node) uses each node embedding as the local representation. BiGI (pair) simply concatenates the representations of node pair (𝑢, 𝑣) as the local representation. BiGI (w/o att) calculates the representation of subgraph via the mean operation instead of the attention mechanism. BiGI (VGAE), BiGI (NGCF) and BiGI (PinSage) adopt VGAE, NGCF and PinSage as their encoders, respectively. All of them keep the same infomax objective with BiGI.</p><p>From the results in Table <ref type="table">6</ref>, we can draw the following conclusions. 1) The proposed encoder achieves competitive performance. Furthermore, in contrast with it, the improvements of BiGI are also significant. 2) Through the comparison of different local representations, we find that constructing a suitable local representation is crucial to BiGI. Introducing the subgraph-level attention mechanism into the calculation of local representation is a sensible choice. 3) By contrast with VGAE, NGCF and PinSage, the improvements of BiGI (VGAE), BiGI (NGCF) and BiGI (PinSage) are satisfying. It indicates that the proposed infomax objective can be seamlessly incorporated into other encoders to capture the global properties of bipartite graph.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.6">Parameter Sensitivity</head><p>We investigate the parameter sensitivity of our model on DBLP with respect to two hyper-parameters: the corruption rate 𝛽 in Eq.( <ref type="formula" target="#formula_10">9</ref>) and the harmonic factor 𝜆 in Eq. <ref type="bibr" target="#b12">(12)</ref>. As shown in Figure <ref type="figure" target="#fig_5">4</ref>, when 𝛽=1e-5 and 𝜆 = 0.3, our model achieves the best result. Therefore, choosing relative small values of 𝛽 and 𝜆 is a reasonable way. Moreover, our model is robust to the changes of 𝛽 and 𝜆. Even in the worst settings of 𝛽 and 𝜆, BiGI is still better than other baselines shown in Table <ref type="table" target="#tab_5">2</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.7">Analysis of the Global Properties</head><p>In this section, to validate that our method is better to capture the global properties of bipartite graph, we conduct two detailed comparisons between BiGI and other strong baselines. In the first experiment, we provide two clustering analyses of users and items which are conducted on ML-100K. We first save all representations of users and items and then cluster them via the well-known K-Means algorithm. The clustering metric Calinski-Harabasz Index (CHI) <ref type="bibr" target="#b3">[3]</ref> is used here. CHI measures the ratio between the withincluster dispersion and the between-cluster dispersion. It is also commonly used to evaluate the task of community detection <ref type="bibr" target="#b4">[4,</ref><ref type="bibr" target="#b23">23]</ref>. As shown in Figure <ref type="figure" target="#fig_6">5</ref>, compared with other graph embeddings, BiGI achieves the best clustering results with the varying number of clusters. It demonstrates that BiGI can better capture community structures of users and items simultaneously. Another comparison is used to test whether the long-range dependencies of heterogeneous nodes can be learned by our model. The scores are predicted by BiGI and several baselines for fifteen node pairs {(𝑢 𝑖 , 𝑣 𝑗 )} which are randomly picked from the test data of DBLP. These node pairs can be actually divided into three groups in terms of the distance between 𝑢 𝑖 and 𝑣 𝑗 , i.e., 3, 5 and 7. From Figure <ref type="figure" target="#fig_7">6</ref>, we have the following conclusions. 1) When the distance of target node pair is relative short, e.g. 3, all baselines and BiGI are capable of learning the latent interaction of node pair. 2) With the increase of distance, the observable relation between 𝑢 𝑖 and 𝑣 𝑗 is gradually weakened. Compared with state-of-the-art baselines, BiGI still maintains promising results. It demonstrates that BiGI can learn the long-range dependency of 𝑢 𝑖 and 𝑣 𝑗 even though they are distant from each other.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">CONCLUSION</head><p>In this paper, we propose a novel bipartite graph embedding named as BiGI. We first introduce a novel bipartite graph encoder to learn initial node representations. Two prototype representations are then generated via aggregating different homogeneous node information, which are further used to construct the global representation. Furthermore, we incorporate the structure prior into local representations via the designed subgraph-level attention mechanism. Through maximizing the MI between local and global representations, BiGI can recognize the global properties of bipartite graph effectively. Extensive experiments demonstrate that BiGI consistently outperforms state-of-the-art baselines on various datasets for different tasks.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: An example of user-movie bipartite graph. The orange shaded area represents a underlying community structure where three movies may share similar genres. The blue dotted lines denote the long-range dependency between "Lily" and "Ice Age". However, these global properties are hard to be well learned from local graph structures.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: A simple illustration of the proposed encoder. In 𝑘-th layer, (a) and (b) show the learning process of 𝒖 𝑘−1 𝑖 . (c) and (d) show the learning process of 𝒗 𝑘−1 𝑛 in a similar way. The yellow dotted lines (Eq.(1) and Eq.(3)) and the green dotted lines (Eq.(2) and Eq.(4)) demonstrate how to derive node embeddings 𝒖 𝑘 𝑖 and 𝒗 𝑘 𝑛 .</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>𝑘𝑢</head><label></label><figDesc>and 𝑊 𝑘 𝑢 are two weight matrices and [•|•] is a concatenation operation. The similar procedures are also employed to update 𝒗 𝑘−1 𝑛 . Sub-figure (c) illustrates the neighborhood aggregation of 𝒖 𝑘 𝑚 :</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Definition 4 . 1 .</head><label>41</label><figDesc>(H-hop Enclosing Subgraph) Given a bipartite graph 𝐺 = (𝑈 , 𝑉 , 𝐸), two nodes 𝑢 ∈ 𝑈 and 𝑣 ∈ 𝑉 , the h-hop enclosing subgraph for (𝑢, 𝑣) is the subgraph 𝐺 ℎ (𝑢,𝑣) induced from 𝐺 by the union of two node sets, i.e., 𝐺 ℎ (𝑢) ∪𝐺 ℎ (𝑣). Here, 𝐺 ℎ (𝑢) = {𝑣 𝑖 |𝑑𝑖𝑠 (𝑣 𝑖 , 𝑢) ≤ ℎ}, 𝐺 ℎ (𝑣) = {𝑢 𝑖 |𝑑𝑖𝑠 (𝑢 𝑖 , 𝑣) ≤ ℎ} and 𝑑𝑖𝑠 is a distance function. Due to the particular structure of bipartite graph, ℎ is set as an odd number strictly.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>9</head><label>9</label><figDesc></figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 4 :</head><label>4</label><figDesc>Figure 4: Results of parameter sensitivity.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 5 :</head><label>5</label><figDesc>Figure 5: Results of clustering analysis. BiGI achieves the best clustering results (A higher score is preferred).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure 6 :</head><label>6</label><figDesc>Figure 6: Visualization of prediction scores. BiGI achieves the better results compared with other baselines.</figDesc><graphic url="image-11.png" coords="8,348.90,99.75,171.43,110.23" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head></head><label></label><figDesc>. Let 𝐺 = (𝑈 , 𝑉 , 𝐸) be a bipartite graph, where 𝑈 and 𝑉 are two disjoint node sets, and 𝐸 ⊆ 𝑈 × 𝑉 denotes the edge set. It is obvious that 𝐺 has two node types. The nodes that fall into the same node set are homogeneous, and the nodes belonging to different node sets are heterogeneous. 𝐴 ∈ {0, 1} |𝑈 |× |𝑉 | is a binary adjacency matrix, where each element 𝐴 𝑖,𝑗 describes whether node 𝑢 𝑖 ∈ 𝑈 has interacted with node 𝑣 𝑗 ∈ 𝑉 . Given a bipartite graph 𝐺 = (𝑈 , 𝑉 , 𝐸) with the adjacency matrix 𝐴, the goal of bipartite graph embedding is to map each node in 𝐺 to a 𝑑-dimensional vector. To keep notations simple, we use 𝒖 𝑖 and 𝒗 𝑗 to represent the embedding vectors of 𝑢 𝑖 and 𝑣 𝑗 , respectively.</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 1 :</head><label>1</label><figDesc>Statistics of datasets.</figDesc><table><row><cell>Datasets</cell><cell>|𝑈 |</cell><cell>|𝑉 |</cell><cell>|𝐸|</cell><cell>Density</cell></row><row><cell>DBLP</cell><cell>6,001</cell><cell>1,308</cell><cell>29,256</cell><cell>0.4%</cell></row><row><cell>ML-100K</cell><cell>943</cell><cell>1,682</cell><cell>100,000</cell><cell>6.3%</cell></row><row><cell>ML-10M</cell><cell cols="3">69,878 10,677 10,000,054</cell><cell>1.3%</cell></row><row><cell cols="3">Wikipedia 15,000 3,214</cell><cell>64,095</cell><cell>0.1%</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 2 :</head><label>2</label><figDesc>Performance (%) comparison of top-K recommendation on DBLP.</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 3 :</head><label>3</label><figDesc>Performance (%) comparison of top-K recommendation on ML-100K.</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>Table 4 :</head><label>4</label><figDesc>Performance (%) comparison of top-K recommendation on ML-10M.</figDesc><table><row><cell></cell><cell></cell><cell></cell><cell>.32</cell><cell>13.13</cell><cell>2.72</cell><cell>3.54</cell><cell>4.92</cell><cell>43.86</cell><cell>46.83</cell><cell>48.75</cell></row><row><cell>LINE</cell><cell>13.71</cell><cell>6.52</cell><cell>8.57</cell><cell>12.37</cell><cell>2.45</cell><cell>3.26</cell><cell>4.67</cell><cell>44.16</cell><cell>44.37</cell><cell>46.30</cell></row><row><cell>Node2vec</cell><cell>14.13</cell><cell>7.69</cell><cell>9.91</cell><cell>13.41</cell><cell>3.07</cell><cell>3.90</cell><cell>5.19</cell><cell>44.80</cell><cell>48.02</cell><cell>49.78</cell></row><row><cell>VGAE</cell><cell>11.38</cell><cell>6.43</cell><cell>8.18</cell><cell>10.93</cell><cell>2.35</cell><cell>2.95</cell><cell>3.94</cell><cell>39.39</cell><cell>42.32</cell><cell>43.68</cell></row><row><cell cols="2">Metapath2vec 14.11</cell><cell>7.88</cell><cell>9.87</cell><cell>13.35</cell><cell>2.85</cell><cell>3.71</cell><cell>5.08</cell><cell>45.49</cell><cell>48.74</cell><cell>49.83</cell></row><row><cell>DMGI</cell><cell>19.58</cell><cell>10.16</cell><cell>13.13</cell><cell>18.31</cell><cell>3.98</cell><cell>5.33</cell><cell>7.82</cell><cell>59.33</cell><cell>61.37</cell><cell>62.71</cell></row><row><cell>PinSage</cell><cell>21.68</cell><cell>10.95</cell><cell>14.51</cell><cell>20.27</cell><cell>4.52</cell><cell>6.18</cell><cell>9.13</cell><cell>62.56</cell><cell>64.77</cell><cell>65.76</cell></row><row><cell>BiNE</cell><cell>14.83</cell><cell>7.69</cell><cell>9.96</cell><cell>13.79</cell><cell>2.87</cell><cell>3.80</cell><cell>5.24</cell><cell>48.14</cell><cell>50.94</cell><cell>52.51</cell></row><row><cell>GC-MC</cell><cell>20.65</cell><cell>10.88</cell><cell>13.87</cell><cell>19.21</cell><cell>4.41</cell><cell>5.84</cell><cell>8.43</cell><cell>60.60</cell><cell>62.21</cell><cell>63.53</cell></row><row><cell>IGMC</cell><cell>18.81</cell><cell>9.21</cell><cell>12.20</cell><cell>17.27</cell><cell>3.50</cell><cell>4.82</cell><cell>7.18</cell><cell>56.89</cell><cell>59.13</cell><cell>60.46</cell></row><row><cell>NeuMF</cell><cell>17.03</cell><cell>8.87</cell><cell>11.38</cell><cell>15.89</cell><cell>3.46</cell><cell>4.54</cell><cell>6.45</cell><cell>54.42</cell><cell>56.39</cell><cell>57.79</cell></row><row><cell>NGCF</cell><cell>21.64</cell><cell>11.03</cell><cell>14.49</cell><cell>20.29</cell><cell>4.49</cell><cell>6.15</cell><cell>9.11</cell><cell>62.56</cell><cell>64.62</cell><cell>65.55</cell></row><row><cell>BiGI</cell><cell>23.36  *</cell><cell>12.50  *</cell><cell>15.92  *</cell><cell>22.14  *</cell><cell>5.41  *</cell><cell>7.15  *</cell><cell>10.50  *</cell><cell>66.01  *</cell><cell>67.70  *</cell><cell>68.78</cell></row></table><note>** indicates that the improvements are statistically significant for p &lt; 0.05 judged by paired t-test.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_10"><head>Table 5 :</head><label>5</label><figDesc>Performance comparison (%) of link prediction.</figDesc><table><row><cell>Model</cell><cell cols="2">Wiki (5:5)</cell><cell cols="2">Wiki (4:6)</cell></row><row><cell></cell><cell cols="4">AUC-ROC AUC-PR AUC-ROC AUC-PR</cell></row><row><cell>DeepWalk</cell><cell>87.19</cell><cell>85.30</cell><cell>81.60</cell><cell>80.29</cell></row><row><cell>LINE</cell><cell>66.69</cell><cell>71.49</cell><cell>64.28</cell><cell>69.89</cell></row><row><cell>Node2vec</cell><cell>89.37</cell><cell>88.12</cell><cell>88.41</cell><cell>87.55</cell></row><row><cell>VGAE</cell><cell>87.81</cell><cell>86.93</cell><cell>86.32</cell><cell>85.74</cell></row><row><cell>Metapath2vec</cell><cell>87.20</cell><cell>84.94</cell><cell>86.75</cell><cell>84.63</cell></row><row><cell>DMGI</cell><cell>93.02</cell><cell>93.11</cell><cell>92.01</cell><cell>92.14</cell></row><row><cell>PinSage</cell><cell>94.27</cell><cell>93.95</cell><cell>92.79</cell><cell>92.56</cell></row><row><cell>BiNE</cell><cell>94.33</cell><cell>93.93</cell><cell>93.15</cell><cell>93.34</cell></row><row><cell>GC-MC</cell><cell>91.90</cell><cell>92.19</cell><cell>91.40</cell><cell>91.74</cell></row><row><cell>IGMC</cell><cell>92.85</cell><cell>93.10</cell><cell>91.90</cell><cell>92.19</cell></row><row><cell>NeuMF</cell><cell>92.62</cell><cell>93.38</cell><cell>91.47</cell><cell>92.63</cell></row><row><cell>NGCF</cell><cell>94.26</cell><cell>94.07</cell><cell>93.06</cell><cell>93.37</cell></row><row><cell>BiGI</cell><cell>94.91  *</cell><cell>94.75  *</cell><cell>94.08  *</cell><cell>94.02  *</cell></row><row><cell cols="5">* indicates that the improvements are statistically significant for p</cell></row><row><cell></cell><cell cols="3">&lt; 0.05 judged by paired t-test.</cell><cell></cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0">The source code is available from https://github.com/caojiangxia/BiGI.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="6" xml:id="foot_1">https://github.com/clhchtcjj/BiNE</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" xml:id="foot_2">Jiangxia Cao * , Xixun Lin * , Shu Guo, Luchen Liu, Tingwen Liu, and Bin Wang</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>ACKNOWLEDGEMENT This research was supported by the National Key Research and Development Program of China (grant No.2016YFB0801003), the Strategic Priority Research Program of Chinese Academy of Sciences (grant No.XDC02040400) and the National Social Science Foundation of China (grant No.19BSH022). Shu Guo and Tingwen Liu are corresponding authors.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Performance (%) comparison of model variants</title>
		<idno>F1@10 NDCG@10 MAP@10 MRR@10</idno>
	</analytic>
	<monogr>
		<title level="j">Table</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Mutual Information Neural Estimation</title>
		<author>
			<persName><forename type="first">Mohamed</forename><surname>Ishmael Belghazi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aristide</forename><surname>Baratin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sai</forename><surname>Rajeswar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sherjil</forename><surname>Ozair</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">Devon</forename><surname>Hjelm</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aaron</forename><forename type="middle">C</forename><surname>Courville</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning (ICML)</title>
				<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">A comprehensive survey of graph embedding: Problems, techniques, and applications</title>
		<author>
			<persName><forename type="first">Hongyun</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vincent</forename><forename type="middle">W</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kevin</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">-Chuan</forename><surname>Chang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Knowledge and Data Engineering</title>
		<imprint>
			<date type="published" when="2018">2018. 2018</date>
		</imprint>
	</monogr>
	<note>TKDE)</note>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">A dendrite method for cluster analysis</title>
		<author>
			<persName><forename type="first">Tadeusz</forename><surname>Caliński</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jerzy</forename><surname>Harabasz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Communications in Statistics-theory and Methods</title>
		<imprint>
			<date type="published" when="1974">1974. 1974</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Community detection in sequence similarity networks based on attribute clustering</title>
		<author>
			<persName><forename type="first">Janamejaya</forename><surname>Chowdhary</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Frank</forename><forename type="middle">E</forename><surname>Löffler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jeremy</forename><forename type="middle">C</forename><surname>Smith</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">PloS One</title>
		<imprint>
			<date type="published" when="2017">2017. 2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">A Survey on Network Embedding</title>
		<author>
			<persName><forename type="first">Peng</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiao</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jian</forename><surname>Pei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wenwu</forename><surname>Zhu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Knowledge and Data Engineering</title>
		<imprint>
			<date type="published" when="2017">2017. 2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">metapath2vec: Scalable representation learning for heterogeneous networks</title>
		<author>
			<persName><forename type="first">Yuxiao</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nitesh</forename><forename type="middle">V</forename><surname>Chawla</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ananthram</forename><surname>Swami</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACM Knowledge Discovery and Data Mining (KDD)</title>
				<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">BiNE: Bipartite Network Embedding</title>
		<author>
			<persName><forename type="first">Ming</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Leihui</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Research on Development in Information Retrieval (SIGIR)</title>
				<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note>Xiangnan He, and Aoying Zhou</note>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Learning Vertex Representations for Bipartite Networks</title>
		<author>
			<persName><forename type="first">Ming</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiangnan</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Leihui</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aoying</forename><surname>Zhou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ArXiv</title>
		<imprint>
			<date type="published" when="2019">2019. 2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Neural message passing for quantum chemistry</title>
		<author>
			<persName><forename type="first">Justin</forename><surname>Gilmer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Patrick</forename><forename type="middle">F</forename><surname>Samuel S Schoenholz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Oriol</forename><surname>Riley</surname></persName>
		</author>
		<author>
			<persName><forename type="first">George</forename><forename type="middle">E</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName><surname>Dahl</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning (ICML)</title>
				<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Generative adversarial nets</title>
		<author>
			<persName><forename type="first">Ian</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jean</forename><surname>Pouget-Abadie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mehdi</forename><surname>Mirza</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bing</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><surname>Warde-Farley</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sherjil</forename><surname>Ozair</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aaron</forename><surname>Courville</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Annual Conference on Neural Information Processing Systems (NeurIPS)</title>
				<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">node2vec: Scalable Feature Learning for Networks</title>
		<author>
			<persName><forename type="first">Aditya</forename><surname>Grover</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jure</forename><surname>Leskovec</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACM Knowledge Discovery and Data Mining (KDD)</title>
				<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Knowledge Graph Embedding Preserving Soft Logical Regularity</title>
		<author>
			<persName><forename type="first">Shu</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lin</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhen</forename><surname>Hui</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lingshuai</forename><surname>Meng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bingnan</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wei</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lihong</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Haibin</forename><surname>Zhai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hong</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACM International Conference on Information and Knowledge Management (CIKM)</title>
				<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Inductive representation learning on large graphs</title>
		<author>
			<persName><forename type="first">William</forename><surname>Hamilton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rex</forename><surname>Ying</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jure</forename><surname>Leskovec</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Annual Conference on Neural Information Processing Systems (NeurIPS)</title>
				<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Representation Learning on Graphs: Methods and Applications</title>
		<author>
			<persName><forename type="first">William</forename><forename type="middle">L</forename><surname>Hamilton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rex</forename><surname>Ying</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jure</forename><surname>Leskovec</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Data Engineering Bulletin</title>
		<imprint>
			<date type="published" when="2017">2017. 2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Neural Collaborative Filtering</title>
		<author>
			<persName><forename type="first">Xiangnan</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lizi</forename><surname>Liao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hanwang</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Liqiang</forename><surname>Nie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xia</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tat-Seng</forename><surname>Chua</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International World Wide Web Conferences</title>
				<imprint>
			<publisher>WWW</publisher>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Learning deep representations by mutual information estimation and maximization</title>
		<author>
			<persName><forename type="first">R</forename></persName>
		</author>
		<author>
			<persName><forename type="first">Devon</forename><surname>Hjelm</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alex</forename><surname>Fedorov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Samuel</forename><surname>Lavoie-Marchildon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Karan</forename><surname>Grewal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Adam</forename><surname>Trischler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations (ICLR)</title>
				<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Adam: A Method for Stochastic Optimization</title>
		<author>
			<persName><forename type="first">P</forename><surname>Diederik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jimmy</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName><surname>Ba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations (ICLR)</title>
				<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Auto-encoding variational bayes</title>
		<author>
			<persName><forename type="first">P</forename><surname>Diederik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Max</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName><surname>Welling</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations (ICLR)</title>
				<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Variational Graph Auto-Encoders</title>
		<author>
			<persName><forename type="first">N</forename><surname>Thomas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Max</forename><surname>Kipf</surname></persName>
		</author>
		<author>
			<persName><surname>Welling</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">NeurIPS Workshop on Bayesian Deep Learning</title>
		<imprint>
			<date type="published" when="2016">2016. 2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Semi-supervised classification with graph convolutional networks</title>
		<author>
			<persName><forename type="first">N</forename><surname>Thomas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Max</forename><surname>Kipf</surname></persName>
		</author>
		<author>
			<persName><surname>Welling</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations (ICLR)</title>
				<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Guiding Entity Alignment via Adversarial Knowledge Embedding</title>
		<author>
			<persName><forename type="first">Xixun</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hong</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jia</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chuan</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bin</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference On Data Mining (ICDM)</title>
				<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Exploratory Adversarial Attacks on Graph Neural Networks</title>
		<author>
			<persName><forename type="first">Xixun</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chuan</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hong</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jia</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Haibo</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yanan</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bin</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference On Data Mining (ICDM)</title>
				<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Evaluation of community detection methods</title>
		<author>
			<persName><forename type="first">Xin</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hui-Min</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhong-Yuan</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Knowledge and Data Engineering</title>
		<imprint>
			<date type="published" when="2019">2019. 2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">f-gan: Training generative neural samplers using variational divergence minimization</title>
		<author>
			<persName><forename type="first">Sebastian</forename><surname>Nowozin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Botond</forename><surname>Cseke</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ryota</forename><surname>Tomioka</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Annual Conference on Neural Information Processing Systems (NeurIPS)</title>
				<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Estimation of Entropy and Mutual Information</title>
		<author>
			<persName><forename type="first">Liam</forename><surname>Paninski</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural Computation</title>
		<imprint>
			<date type="published" when="2002">2002. 2002</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Graph Representation Learning via Graphical Mutual Information Maximization</title>
		<author>
			<persName><forename type="first">Zhen</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wenbing</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Minnan</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qinghua</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yu</forename><surname>Rong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tingyang</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Junzhou</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International World Wide Web Conferences</title>
				<imprint>
			<publisher>WWW</publisher>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">DeepWalk: Online Learning of Social Representations</title>
		<author>
			<persName><forename type="first">Bryan</forename><surname>Perozzi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rami</forename><surname>Al-Rfou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">'</forename></persName>
		</author>
		<author>
			<persName><forename type="first">Steven</forename><surname>Skiena</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACM Knowledge Discovery and Data Mining (KDD)</title>
				<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Item-based collaborative filtering recommendation algorithms</title>
		<author>
			<persName><forename type="first">Badrul</forename><surname>Sarwar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">George</forename><surname>Karypis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Joseph</forename><surname>Konstan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">John</forename><surname>Riedl</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International World Wide Web Conferences</title>
				<imprint>
			<publisher>WWW</publisher>
			<date type="published" when="2001">2001</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Prototypical networks for few-shot learning</title>
		<author>
			<persName><forename type="first">Jake</forename><surname>Snell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kevin</forename><surname>Swersky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Richard</forename><surname>Zemel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Annual Conference on Neural Information Processing Systems (NeurIPS)</title>
				<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Dropout: a simple way to prevent neural networks from overfitting</title>
		<author>
			<persName><forename type="first">Nitish</forename><surname>Srivastava</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Geoffrey</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alex</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ruslan</forename><surname>Salakhutdinov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<date type="published" when="2014">2014. 2014</date>
			<publisher>JMLR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Infograph: Unsupervised and semi-supervised graph-level representation learning via mutual information maximization</title>
		<author>
			<persName><forename type="first">Fan-Yun</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jordan</forename><surname>Hoffmann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jian</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations (ICLR)</title>
				<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<author>
			<persName><forename type="first">Justin</forename><surname>Sybrandt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ilya</forename><surname>Safro</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">FOBE and HOBE: First-and High-Order Bipartite Embeddings</title>
				<imprint>
			<date type="published" when="2019">2019. 2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">LINE: Large-scale Information Network Embedding</title>
		<author>
			<persName><forename type="first">Jian</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Meng</forename><surname>Qu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mingzhe</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ming</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jun</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qiaozhu</forename><surname>Mei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International World Wide Web Conferences</title>
				<imprint>
			<publisher>WWW</publisher>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Graph convolutional matrix completion</title>
		<author>
			<persName><forename type="first">Rianne</forename><surname>Van Den</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Thomas</forename><forename type="middle">N</forename><surname>Berg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Max</forename><surname>Kipf</surname></persName>
		</author>
		<author>
			<persName><surname>Welling</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ArXiv</title>
		<imprint>
			<date type="published" when="2017">2017. 2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<author>
			<persName><forename type="first">Aäron</forename><surname>Van Den Oord</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yazhe</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Oriol</forename><surname>Vinyals</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Representation Learning with Contrastive Predictive Coding</title>
				<imprint>
			<date type="published" when="2018">2018. 2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Deep Graph Infomax</title>
		<author>
			<persName><forename type="first">Petar</forename><surname>Velickovic</surname></persName>
		</author>
		<author>
			<persName><forename type="first">William</forename><surname>Fedus</surname></persName>
		</author>
		<author>
			<persName><forename type="first">William</forename><forename type="middle">L</forename><surname>Hamilton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pietro</forename><surname>Liò</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">Devon</forename><surname>Hjelm</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations (ICLR)</title>
				<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Neural Graph Collaborative Filtering</title>
		<author>
			<persName><forename type="first">Xiang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiangnan</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Meng</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fuli</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tat-Seng</forename><surname>Chua</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Research on Development in Information Retrieval (SIGIR)</title>
				<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">A comprehensive survey on graph neural networks</title>
		<author>
			<persName><forename type="first">Zonghan</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shirui</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fengwen</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Guodong</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chengqi</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Philip</forename><surname>Yu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Neural Networks and Learning Systems</title>
		<imprint>
			<biblScope unit="issue">TNNLS</biblScope>
			<date type="published" when="2020">2020. 2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Drug-target interaction prediction from chemical, genomic and pharmacological data in an integrated framework</title>
		<author>
			<persName><forename type="first">Yoshihiro</forename><surname>Yamanishi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Masaaki</forename><surname>Kotera</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Minoru</forename><surname>Kanehisa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Susumu</forename><surname>Goto</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Bioinformatics</title>
				<imprint>
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Graph convolutional neural networks for web-scale recommender systems</title>
		<author>
			<persName><forename type="first">Rex</forename><surname>Ying</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ruining</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kaifeng</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pong</forename><surname>Eksombatchai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">William</forename><forename type="middle">L</forename><surname>Hamilton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jure</forename><surname>Leskovec</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACM Knowledge Discovery and Data Mining (KDD)</title>
				<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Unsupervised Attributed Multiplex Network Embedding</title>
		<author>
			<persName><forename type="first">Donghyun</forename><surname>Chan Young Park</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiawei</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hwanjo</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName><surname>Yu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI Conference on Artificial Intelligence (AAAI)</title>
				<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Inductive Matrix Completion Based on Graph Neural Networks</title>
		<author>
			<persName><forename type="first">Muhan</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yixin</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations (ICLR)</title>
				<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Neural IR Meets Graph Embedding: A Ranking Model for Product Search</title>
		<author>
			<persName><forename type="first">Yuan</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dong</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yan</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International World Wide Web Conferences</title>
				<imprint>
			<publisher>WWW</publisher>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Learning node embeddings in interaction graphs</title>
		<author>
			<persName><forename type="first">Yao</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yun</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiangnan</forename><surname>Kong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yangyong</forename><surname>Zhu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACM International Conference on Information and Knowledge Management (CIKM)</title>
				<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Arbitrary-order proximity preserved network embedding</title>
		<author>
			<persName><forename type="first">Ziwei</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Peng</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiao</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jian</forename><surname>Pei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xuanrong</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wenwu</forename><surname>Zhu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACM Knowledge Discovery and Data Mining (KDD)</title>
				<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
