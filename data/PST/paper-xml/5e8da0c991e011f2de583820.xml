<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">LET&apos;S AGREE TO DEGREE: COMPARING GRAPH CONVOLUTIONAL NETWORKS IN THE MESSAGE-PASSING FRAMEWORK</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date type="published" when="2020-04-06">6 Apr 2020</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Floris</forename><surname>Geerts</surname></persName>
							<email>floris.geerts@uantwerpen.be</email>
							<affiliation key="aff0">
								<orgName type="institution">University of Antwerp</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Filip</forename><surname>Mazowiecki</surname></persName>
							<email>filipm@mpi-sws.org</email>
							<affiliation key="aff1">
								<orgName type="department">Max Planck Institute for Software Sciences</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Guillermo</forename><forename type="middle">A</forename><surname>Pérez</surname></persName>
							<email>guillermoalberto.perez@uantwerpen.be</email>
							<affiliation key="aff2">
								<orgName type="institution">University of Antwerp</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">LET&apos;S AGREE TO DEGREE: COMPARING GRAPH CONVOLUTIONAL NETWORKS IN THE MESSAGE-PASSING FRAMEWORK</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2020-04-06">6 Apr 2020</date>
						</imprint>
					</monogr>
					<idno type="arXiv">arXiv:2004.02593v1[cs.LG]</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.2" ident="GROBID" when="2022-12-25T14:14+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>In this paper we cast neural networks defined on graphs as message-passing neural networks (MPNNs) in order to study the distinguishing power of different classes of such models. We are interested in whether certain architectures are able to tell vertices apart based on the feature labels given as input with the graph. We consider two variants of MPNNS: anonymous MPNNs whose message functions depend only on the labels of vertices involved; and degree-aware MPNNs in which message functions can additionally use information regarding the degree of vertices. The former class covers a popular formalisms for computing functions on graphs: graph neural networks (GNN). The latter covers the so-called graph convolutional networks (GCNs), a recently introduced variant of GNNs by Kipf and Welling. We obtain lower and upper bounds on the distinguishing power of MPNNs in terms of the distinguishing power of the Weisfeiler-Lehman (WL) algorithm. Our results imply that (i) the distinguishing power of GCNs is bounded by the WL algorithm, but that they are one step ahead; (ii) the WL algorithm cannot be simulated by "plain vanilla" GCNs but the addition of a trade-off parameter between features of the vertex and those of its neighbours (as proposed by Kipf and Welling themselves) resolves this problem.</p><p>when no further refinement is obtained. The distinguishing power of the WL algorithm itself is well understood, see e.g., [</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>A standard approach to learning tasks on graph-structured data, such as vertex classification, edge prediction, and graph classification, consists of the construction of a representation of vertices and graphs that captures their structural information. Graph Neural Networks (GNNs) are currently considered as the state-of-the art approach for learning such representations. Many variants of GNNs exist but they all follow a similar strategy. More specifically, each vertex is initially associated with a feature vector. This is followed by a recursive neighbourhood-aggregation scheme where each vertex aggregates feature vectors of its neighbours, possibly combines this with its own current feature vector, to finally obtain its new feature vector. After a number of iterations, each vertex is then represented by the resulting feature vector.</p><p>The adequacy of GNNs for graph learning tasks is directly related to their so-called distinguishing power. Here, distinguishing power refers to the ability of GNNs to distinguish vertices and graphs in terms of the computed representation. That is, when two vertices are represented by the same feature vector, they are considered the same with regards to any subsequent feature-based task.</p><p>Only recently a formal study of the distinguishing power of some GNN variants has been initiated. In two independent studies <ref type="bibr" target="#b15">[Xu et al., 2019</ref><ref type="bibr" target="#b11">, Morris et al., 2019]</ref> the distinguishing power of GNNs is linked to the distinguishing power of the classical Weisfeiler-Lehman (WL) algorithm. The WL algorithm starts from an initial vertex colouring of the graph. Then, similarly as GNNs, the WL algorithm recursively aggregates the colouring of neighbouring vertices. In each recursive step, a vertex colouring is obtained that refines the previous one. The WL algorithm stops</p><p>The general MPNN framework allows us to explore the impact of degree information on the distinguishing power of MPNNs in general and large classes of GNNs and GCNs in particular. More precisely, in this paper we consider two general classes of MPNNs: anonymous MPNNs that do not use degree information, and degree-aware MPNNs that do use degree information. The former class of MPNNs covers the GNNs studied in <ref type="bibr" target="#b15">[Xu et al., 2019</ref><ref type="bibr" target="#b11">, Morris et al., 2019]</ref>, the latter class covers the GCNs <ref type="bibr" target="#b8">[Kipf and Welling, 2017]</ref>, among others.</p><p>Contributions. For general MPNNs, our main results are the following (see Propositions 5.2 and 6.3):</p><p>(i) The distinguishing power of anonymous MPNNs is bounded by the WL algorithm. This result can be seen as a slight generalisation of the results in <ref type="bibr" target="#b15">[Xu et al., 2019</ref><ref type="bibr" target="#b11">, Morris et al., 2019</ref>]. (ii) The distinguishing power of degree-aware MPNNs is bounded by the WL algorithm, but they may be one step ahead. Intuitively, degree-aware MPNNs may be one step ahead of the WL algorithm because the degree information, which is part of degree-aware MPNNs from the start, is only derived by the WL algorithm after one step. (iii) The WL algorithm can be regarded as an anonymous MPNN (and thus also as a degree-aware MPNN). As a consequence, the distinguishing power of the classes of anonymous and degree-aware MPNNs matches that of the WL algorithm.</p><p>For anonymous MPNNs related to GNNs <ref type="bibr" target="#b15">[Xu et al., 2019</ref><ref type="bibr" target="#b11">, Morris et al., 2019]</ref> and degree-aware MPNNs related to GCNs <ref type="bibr" target="#b8">[Kipf and Welling, 2017]</ref>, our main results are the following (see Theorems 5.5 and 5.7, and Propositions 6.9 and 6.10):</p><p>(iv) On a fixed input graph, the WL algorithm can be simulated, step-by-step, by GNNs that use ReLU or sign as activation function. This result refines the result in <ref type="bibr" target="#b11">[Morris et al., 2019]</ref> in that their simulation using the ReLU function requires two GNN "layers" for each step of the WL algorithm. We only require one layer in each step. In addition, our simulation is achieved by means of a very simple form of GNNs (see Equation ( <ref type="formula">10</ref>) at the end of Section 5), which may be of independent interest. (v) The distinguishing power of GCNs is bounded by the WL algorithm, but they may be one step ahead. This is due to GCNs being degree-aware MPNNs (for which result (ii) applies). This advantage of GCNs over more classical GNNs may explain the success of GCNs in various graph learning tasks. (vi) In contrast, we show that the WL algorithm cannot be simulated by popular GCNs such as those from <ref type="bibr" target="#b8">[Kipf and Welling, 2017]</ref>. This observation is somewhat contradictory to the general belief that GCNs can be seen as a "continuous generalisation" of the WL algorithm. (vii) However, by introducing a learnable trade-off parameter between features of the vertex and those of its neighbours, the simulation of the WL algorithm can be achieved by GCNs. This minor relaxation of GCNs (see Equation ( <ref type="formula" target="#formula_134">18</ref>) at the end of Section 6) was already suggested in <ref type="bibr" target="#b8">[Kipf and Welling, 2017]</ref> based on empirical results. Our simulation result thus provides a theoretical justification of this parameter.</p><p>Structure of the paper. After introducing some notations and concepts in Section 2, we define MPNNs, anonymous and degree-aware MPNNs in Section 3. In Section 4 we formally define how to compare classes of MPNNs with regard to their distinguishing power. We characterise the distinguishing power of anonymous MPNNs in Section 5 and that of degree-aware MPNNs in Section 6. We conclude the paper in Section 7.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Preliminaries</head><p>Let A denote the set of all algebraic numbers; Q, the set of all rational numbers; Z, the set of all integer numbers; N, the set of all natural numbers including zero, i.e., N = {0, 1, 2, . . . }. We write S + to denote the subset of numbers from S which are strictly positive, e.g., N + = N \ {0}. We use {} and { {} } to indicate sets and multisets, respectively.</p><p>Computing with algebraic numbers. Throughout the paper we will perform basic computations, such as addition and multiplication, on numbers. It is well-known that these operations are computable on numbers in N, Z and Q. However, in order to capture numbers used by popular graph neural network architectures, such as roots of integers <ref type="bibr" target="#b8">[Kipf and Welling, 2017]</ref>, we will work with algebraic numbers. An algebraic number is usually represented by a minimal polynomial such that the number is a root of the polynomial and a pair of rational numbers to identify that root. Conveniently, it is known that the operations we will need are indeed computable for algebraic numbers encoded using such a representation (see, e.g., <ref type="bibr" target="#b12">[Ouaknine and Worrell, 2014]</ref>).</p><p>Labelled graphs. Let G = (V, E) be an undirected graph consisting of n ∈ N vertices. Without loss of generality we assume that V = {1, 2, . . . , n}. Given a vertex v ∈ V , we denote by N G (v) its set of neighbours, i.e., N G (v) := {u | {u, v} ∈ E}. Furthermore, the degree of a vertex v, denoted by d v , is the number of vertices in N G (v). With a labelled graph (G, ν ν ν) we mean a graph G = (V, E) whose vertices are labelled using a function ν ν ν : V → Σ for some set Σ of labels. We denote by ν ν ν v the label of v for each v ∈ V .</p><p>Henceforth we fix a labelled graph (G, ν ν ν) with G = (V, E) and denote by A the adjacency matrix (of G). That is, A is a matrix of dimension n × n such that the entry A vw = 1 if {v, w} ∈ E and A vw = 0 otherwise. We denote by D the diagonal matrix such that D vv = d v for each v ∈ V . Throughout the paper we will assume that G does not have isolated vertices, which is equivalent to assuming that D does not have any 0 entries on the diagonal. This assumption will help us to avoid unnecessary technical details in the theoretical analysis. But it is easy to generalise our results by treating isolated nodes separately. We will also assume that there are no self-loops, so the diagonal of A is filled with 0s. For an arbitrary matrix B we denote by B i the i-th row of B. Furthermore, if B is a matrix of dimension n × m, we also represent the rows of B by B v , for v ∈ V .</p><p>We will identify Σ with elements (row vectors) in A s for some s ∈ N + . In this way, a labelling ℓ ℓ ℓ : V → Σ can be regarded as a matrix in A n×s and ℓ ℓ ℓ v corresponds to the v-th row in that matrix. Conversely, a matrix L ∈ A n×s can be regarded as the vertex labelling that labels v with the row vector L v . We use these two interpretations of labellings interchangeably.</p><p>It will be important later on to be able to compare two labellings of G. Given a matrix L ∈ A n×s and a matrix L ′ ∈ A n×s ′ we say that the vertex labelling L ′ is coarser than the vertex labelling L, denoted by</p><formula xml:id="formula_0">L ⊑ L ′ , if for all v, w ∈ V , L v = L w ⇒ L ′ v = L ′ w . The vertex labellings L and L ′ are equivalent, denoted by L ≡ L ′ , if L ⊑ L ′ and L ′ ⊑ L hold. In other words, L ≡ L ′ if and only if for all v, w ∈ V , L v = L w ⇔ L ′ v = L ′ w .</formula><p>Weisfeiler-Lehman labelling. Of particular importance is the labelling obtained by colour refinement, also known as the Weisfeiler-Lehman algorithm (or WL algorithm, for short). The WL algorithm constructs a labelling, in an incremental fashion, based on neighbourhood information and the initial vertex labelling. More specifically, given (G, ν ν ν), the WL algorithm initially sets ℓ ℓ ℓ (0) := ν ν ν. Then, the WL algorithm computes a labelling ℓ ℓ ℓ (t) , for t &gt; 0, as follows:</p><formula xml:id="formula_1">ℓ ℓ ℓ (t) v := HASH ℓ ℓ ℓ (t−1) v , { {ℓ ℓ ℓ (t−1) u | u ∈ N G (v)} } ,</formula><p>where HASH bijectively maps the above pair, consisting of (i) the previous label ℓ ℓ ℓ</p><formula xml:id="formula_2">(t−1) v of v; and (ii) the multiset { {ℓ ℓ ℓ (t−1) u | u ∈ N G (v)} } of</formula><p>labels of the neighbours of v, to a label in Σ which has not been used in previous iterations. When the number of distinct labels in ℓ ℓ ℓ (t) and ℓ ℓ ℓ (t−1) is the same, the WL algorithm terminates. Termination is guaranteed in at most n steps <ref type="bibr" target="#b5">[Immerman and Lander, 1990]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Message Passing Neural Networks</head><p>We start by describing message passing neural networks (MPNNs) for deep learning on graphs, introduced by <ref type="bibr" target="#b3">[Gilmer et al., 2017]</ref>. Roughly speaking, in MPNNs, vertex labels are propagated through a graph according to its connectivity structure. MPNNs are known to model a variety of graph neural network architectures commonly used in practice. We define MPNNs in Section 3.1, provide some examples in Section 3.2, and comment on the choice of formalisation of MPNNs in Section 3.3.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Definition</head><p>Given a labelled graph (G, ν ν ν) and a computable function f : V → A an MPNN computes a vertex labelling ℓ ℓ ℓ : V → A s , for some s ∈ N + . The vertex labelling computed by an MPNN is computed in a finite number of rounds T . After round 0 ≤ t ≤ T the labelling is denoted by ℓ ℓ ℓ (t) . We next detail how ℓ ℓ ℓ (t) is computed.</p><p>Initialisation. We let ℓ ℓ ℓ (0) := ν ν ν.</p><p>Then, for every round t = 1, 2, . . . , T , we define ℓ ℓ ℓ (t) : V → A st , as follows<ref type="foot" target="#foot_0">1</ref> : Message Passing. Each vertex v ∈ V receives messages from its neighbours which are subsequently aggregated.</p><p>Formally, the function MSG (t) receives as input f applied to two vertices v and u, and the corresponding labels of these vertices from the previous iteration ℓ ℓ ℓ</p><formula xml:id="formula_3">(t−1) v</formula><p>and ℓ ℓ ℓ</p><formula xml:id="formula_4">(t−1) u</formula><p>, and outputs a label in A s ′ t . Then, for every vertex v, we aggregate by summing all such labels for every neighbour u.</p><formula xml:id="formula_5">m (t) v := u∈NG(v) MSG (t) ℓ ℓ ℓ (t−1) v , ℓ ℓ ℓ (t−1) u , f (v), f (u) ∈ A s ′ t . Updating. Each vertex v ∈ V further updates m (t)</formula><p>v possibly based on its current label ℓ ℓ ℓ</p><formula xml:id="formula_6">(t−1) v : ℓ ℓ ℓ (t) v := UPD (t) ℓ ℓ ℓ (t−1) v , m (t) v ∈ A st .</formula><p>Here, the message functions MSG (t) and update functions UPD (t) are general (computable) functions. After round T , we define the final labelling ℓ ℓ ℓ : V → A s as ℓ ℓ ℓ v := ℓ ℓ ℓ</p><formula xml:id="formula_7">(T ) v</formula><p>for every v ∈ V . If further aggregation over the entire graph is needed, e.g., for graph classification, an additional readout function READOUT({ {ℓ ℓ ℓ v | v ∈ V } }) can be applied. We omit the readout function here since most of the computation happens during the rounds of an MPNN.</p><p>The role of the function f in this paper is to distinguish between two classes of MPNNs<ref type="foot" target="#foot_1">2</ref> : Those whose message functions only depend on the labels of the vertices involved, in which case we set f to the zero function f (v) = 0, for all v ∈ V , and those whose message functions depend on the labels and on the degrees of the vertices involved, in which case we set f to the degree function f (v) = d v , for all v ∈ V . We will refer to the former class as anonymous MPNNs and to the latter as degree-aware MPNNs. These classes are denoted by M anon and M deg , respectively. We remark that, by definition, M anon ⊆ M deg .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Examples</head><p>We next illustrate anonymous and degree-aware MPNNs by a number of examples. First, we provide two examples of anonymous MPNNs that will play an important role in Section 5.</p><p>Example 3.1 (GNN architectures). We first consider the graph neural network architectures <ref type="bibr" target="#b4">[Hamilton et al., 2017</ref><ref type="bibr" target="#b11">, Morris et al., 2019]</ref> defined by:</p><formula xml:id="formula_8">L (t) := σ L (t−1) W (t) 1 + AL (t−1) W (t) 2 + B (t) ,<label>(1)</label></formula><p>where L (t) is the matrix in A n×st consisting of the n rows ℓ ℓ ℓ</p><formula xml:id="formula_9">(t) v ∈ A st , for v ∈ V , A ∈ A n×n is the adjacency matrix of G, W (t) 1 and W (t)</formula><p>2 are (learnable) weight matrices in A st−1×st , B (t) is a bias matrix in A n×st consisting of n copies of the same row b (t) ∈ A st , and σ is a non-linear activation function. We can regard this architecture as an MPNN.</p><p>Indeed, (1) can be equivalently phrased as the architecture which computes, in round t, for each vertex v ∈ V the label defined by:</p><formula xml:id="formula_10">ℓ ℓ ℓ (t) v := σ   ℓ ℓ ℓ (t−1) v W (t) 1 + u∈NG(v) ℓ ℓ ℓ (t−1) u W (t) 2 + b (t)   ,</formula><p>where we identified the labellings with their images, i.e., a row vector in A st−1 or A st . To phrase this as an MPNN, it suffices to define for each x and y in A st−1 , each v ∈ V and u ∈ N G (v), and each t ≥ 1:</p><formula xml:id="formula_11">MSG (t) x, y, −, −) := yW (t)</formula><p>2 and UPD (t) (x, y) := σ xW</p><formula xml:id="formula_12">(t) 1 + y + b (t) .</formula><p>We write − instead of 0 to emphasise that the message functions use the zero function f (v) = 0, for all v ∈ V , and hence do not depend on f (v) and f (u). In other words, the MPNN constructed is an anonymous MPNN. Without loss of generality we will assume that aMPNNs do not use f (v) and f (u) in the messages. If they do then one can replace them with 0. This way it is easy to see that classes of MPNNs that use different functions f in the messages contain the class of anonymous MPNNs.</p><p>Another example of an anonymous MPNN originates from the Weisfeiler-Lehman algorithm described in the preliminaries.</p><p>Example 3.2 (Weisfeiler-Lehman). We recall that WL computes, in round t ≥ 1, for each vertex v ∈ V the label:</p><formula xml:id="formula_13">ℓ ℓ ℓ (t) v := HASH ℓ ℓ ℓ (t−1) v , { {ℓ ℓ ℓ (t−1) u | u ∈ N G (v)} } .</formula><p>Let us assume that the set Σ of labels is A s for some fixed s ∈ N + . We cast the WL algorithm as an anonymous MPNN by using an injection h : A s → Q. What follows is in fact an adaptation of Lemma 5 from <ref type="bibr" target="#b15">[Xu et al., 2019]</ref> itself based on <ref type="bibr">[Zaheer et al., 2017, Theorem 2]</ref>. We crucially rely on the fact that the set A of algebraic numbers is countable (see e.g., Theorem 2.2. in <ref type="bibr" target="#b6">[Jarvis, 2014]</ref>). As a consequence, also A s is countable.</p><p>Let τ : A s → N + be a computable injective function witnessing the countability of A s . For instance, since elements of A are encoded as a polynomial a 0 + a</p><formula xml:id="formula_14">1 x 1 + a 2 x 2 + • • • + a k x k ∈ Z[x]</formula><p>and a pair n1 /d1, n2 /d2 of rationals, τ can be taken to be the composition of the injection α : A → N + , applied point-wise, and the Cantor tuple function, where</p><formula xml:id="formula_15">α(a 0 + a 1 x 1 + a 2 x 2 + • • • + a k x k , n1 /d1, n2 /d2) → p(1, n 1 )p(2, n 2 )p(3, d 1 )p(4, d 2 ) k i=0 p(i + 5, a i )</formula><p>with π i being the i-th prime number in</p><formula xml:id="formula_16">p(i, z) = π z 2i if z ≥ 0 π −z 2i+1 if z &lt; 0.</formula><p>We next define h : A s → Q + as the mapping x → (n + 1) −τ (x) . Note that h is injective and h(x) can be seen as a number whose (n + 1)-ary representation has a single nonzero digit. We next observe that the multiplicity of every element in S := { {ℓ ℓ ℓ</p><formula xml:id="formula_17">(t−1) u | u ∈ N G (v)} } is</formula><p>bounded by the number of all vertices n -and this for all t ≥ 1. It follows that the function φ mapping any such S to x∈S h(x) is an injection from A s to Q. Therefore, the summands can be recovered by looking at the (n + 1)-ary representation of the sum, and thus the inverse of φ is computable on its image. To conclude, we define the message function</p><formula xml:id="formula_18">MSG (t) (x, y, −, −) := h(y) ∈ A.</formula><p>The update function is defined by UPD (t) (x, y) := HASH(x, φ −1 (y)),</p><p>where y ∈ A since it corresponds to a sum of messages, themselves algebraic numbers. As before, we write − instead of 0 to emphasise that the message functions use the zero function f (v) = 0, for all v ∈ V , and hence do not depend on f (v) and f (u).</p><p>We conclude with an example of a degree-aware MPNN. We study degree-aware MPNNs in Section 6. Example 3.3 (GCNs by <ref type="bibr">Kipf and Welling)</ref>. We consider the GCN architecture by <ref type="bibr" target="#b8">[Kipf and Welling, 2017]</ref>, which in round t ≥ 1 computes</p><formula xml:id="formula_19">L (t) := σ (D + I) −1/2 (A + I)(D + I) −1/2 L (t−1) W (t) ,</formula><p>where we use the same notation as in Example 3.1 but now with a single (learnable) weight matrix W (t) in A st−1×st . This means that, in round t, for each vertex v ∈ V it computes the label:</p><formula xml:id="formula_20">ℓ ℓ ℓ (t) v := σ   1 1 + d v ℓ ℓ ℓ (t−1) v W (t) + u∈NG(v) 1 √ 1 + d v 1 √ 1 + d u ℓ ℓ ℓ (t−1) u W (t)   .<label>(2)</label></formula><p>We can regard this architecture again as an MPNN. Indeed, it suffices to define for each x and y in A st−1 , each v ∈ V and u ∈ N G (v), and each t ≥ 1:</p><formula xml:id="formula_21">MSG (t) (x, y, d v , d u ) := 1 d v 1 1 + d v xW (t) + 1 √ 1 + d v 1 √ 1 + d u yW (t)</formula><p>and</p><formula xml:id="formula_22">UPD (t) (x, y) := σ(y).</formula><p>We remark that the initial factor 1/d v in the message functions is introduced for renormalisation purposes. We indeed observe that the message functions depend only on ℓ ℓ ℓ</p><formula xml:id="formula_23">(t−1) v , ℓ ℓ ℓ (t−1) u</formula><p>, and the degrees d v and d u of the vertices v and u, respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">On the choice of formalism</head><p>The expert reader may have noticed that we use a different formalisation of MPNNs than the one given in the original paper <ref type="bibr" target="#b3">[Gilmer et al., 2017]</ref>. The first difference is that our MPNNs are parameterised by an input computable function f applied to v and u ∈ N G (v). We add this function to avoid a certain ambiguity in the formalisation in <ref type="bibr" target="#b3">[Gilmer et al., 2017]</ref> on what precisely the message functions can depend on. More specifically, only a dependence on ℓ ℓ ℓ <ref type="bibr" target="#b3">[Gilmer et al., 2017]</ref>. In contrast, the examples given in <ref type="bibr" target="#b3">[Gilmer et al., 2017]</ref> use more information, such as the degree of vertices. The use of the function f in the definition of MPNNs makes explicit the information that message functions can use. It is readily verified that every MPNN of <ref type="bibr" target="#b3">[Gilmer et al., 2017]</ref> corresponds to an MPNN in our formalism.</p><formula xml:id="formula_24">(t−1) v and ℓ ℓ ℓ (t−1) u is specified in</formula><p>The second difference is that the MPNNs in <ref type="bibr" target="#b3">[Gilmer et al., 2017]</ref> work on graphs that carry both vertex and edge labels. We ignore edge labellings in this paper but most of our results carry over to that more general setting. Indeed, it suffices to use the extension of the Weisfeiler-Lehman algorithm for edge-labelled graphs as is done for graph neural networks in <ref type="bibr" target="#b6">[Jaume et al., 2019]</ref>.</p><p>We also want to compare our formalisation to the MPNNs from <ref type="bibr" target="#b9">[Loukas, 2019]</ref>. In that paper, the message functions can depend on identifiers of the vertices involved. Such position-aware MPNNs correspond to MPNNs in our setting in which f assigns to each vertex a unique identifier. We remark that <ref type="bibr" target="#b9">[Loukas, 2019]</ref> shows Turing universality of position-aware MPNNs using close connections with the LOCAL model for distributed graph computations of <ref type="bibr" target="#b0">[Angluin, 1980]</ref>. As such, MPNNs from <ref type="bibr" target="#b9">[Loukas, 2019]</ref> can simulate our MPNNs as one could add a few initialisation rounds to compute f (v) and f (u). We also remark that in the MPNNs from <ref type="bibr" target="#b9">[Loukas, 2019]</ref> every vertex can also send itself a message. We provide this functionality by parameterising the update functions with the current label of the vertex itself, just as in <ref type="bibr" target="#b3">[Gilmer et al., 2017]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Comparing the distinguishing power of classes of MPNNs</head><p>The distinguishing power of MPNNs relates to their ability to distinguish vertices based on the labellings that they compute. We are interested in comparing the distinguishing power of classes of MPNNs. In this section we formally define what we mean by such a comparison.</p><p>For a given labelled graph (G, ν ν ν) and MPNN M , we denote by ℓ ℓ ℓ M2 be their corresponding labellings on an input graph (G, ν ν ν) obtained after t rounds of computation for every 0 ≤ t ≤ T . Then M 1 is said to be weaker than M 2 , denoted by</p><formula xml:id="formula_25">M 1 M 2 , if M 1 cannot distinguish more vertices than M 2 in every round of computation. More formally, M 1 M 2 if ℓ ℓ ℓ (t) M2 ⊑ ℓ ℓ ℓ (t)</formula><p>M1 for every t ≥ 0. In this case we also say that M 2 is stronger than M 1 . We lift this notion to classes M 1 and M 2 of MPNNs in a standard way. Definition 4.2. Consider two classes M 1 and M 2 of MPNNs. Then, M 1 is said to be weaker than M 2 , denoted by</p><formula xml:id="formula_26">M 1 M 2 , if for all M 1 ∈ M 1 there exists an M 2 ∈ M 2 which is stronger than M 1 .</formula><p>Finally, we say that M 1 and M 2 are equally strong, denoted by</p><formula xml:id="formula_27">M 1 ≡ M 2 , if both M 1 M 2 and M 2 M 1 hold.</formula><p>We will also need a generalisation of the previous definitions in which we compare labellings computed by MPNNs at different rounds. This is formalised as follows.</p><p>Definition 4.3. Consider two MPNNs M 1 and M 2 with T 1 and T 2 rounds, respectively. Let ℓ ℓ ℓ</p><formula xml:id="formula_28">(t)</formula><p>M1 and ℓ ℓ ℓ</p><formula xml:id="formula_29">(t)</formula><p>M2 be their corresponding labellings on an input graph (G, ν ν ν) obtained after t rounds of computation. Let g : N → N be a monotonic function such that g(T 1 ) = T 2 . We say that M 1 is g-weaker than M 2 , denoted by</p><formula xml:id="formula_30">M 1 g M 2 , if ℓ ℓ ℓ g(t) M2 ⊑ ℓ ℓ ℓ (t) M1 for every 0 ≤ t ≤ T 1 .</formula><p>Only the following special cases of this definition, depending on extra information regarding a function g : N → N, will be relevant in this paper:</p><p>• g(t) = t. This case corresponds to Definition 4.1. If M 1 g M 2 , then we simply say that M 1 is weaker than M 2 , and write M 1 M 2 , as before.</p><p>• g(t) = t + 1. If M 1 g M 2 , then we say that M 1 is weaker than M 2 with one step ahead. We denote this by M 1 +1 M 2 .</p><p>• g(t) = ct for some constant c. If M 1 g M 2 , then we say that M 1 is weaker than M 2 with a linear factor of c. We denote this by M 1 ×c M 2 .</p><p>We lift these definitions to classes of MPNNs, just like in Definition 4.2.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">The distinguishing power of anonymous MPNNs</head><p>In this section we compare classes of anonymous MPNNs in terms of their distinguishing power using Definition 4.2.</p><p>We recall from Section 3 that anonymous MPNNs are MPNNs whose message functions only depend on the previous labels of the vertices involved. The distinguishing power of anonymous MPNNs (or aMPNNs, for short) is well understood. Indeed, as we will shortly see, it follows from two independent works <ref type="bibr" target="#b15">[Xu et al., 2019</ref><ref type="bibr" target="#b11">, Morris et al., 2019]</ref> that the distinguishing power of aMPNNs can be linked to the distinguishing power of the WL algorithm.</p><p>Let (G, ν ν ν) be a labelled graph. We will consider the following classes of aMPNNs. We denote by M WL the class of aMPNNs consisting of an aMPNN M T WL , for each T ∈ N, originating from the WL algorithm (see Example 3.2) being ran for T rounds. In a slight abuse of notation, we will simply write M WL when T is clear from the context. Recall that the class of anonymous MPNNs is denoted M anon . Finally, we introduce two classes of aMPNNs which are of special interest: those arising from the graph neural networks considered in <ref type="bibr" target="#b11">[Morris et al., 2019]</ref>. In Example 3.1 we established that such graph neural networks correspond to aMPNNs. Let us denote by M σ GNN the class of aMPNNs with message and update functions of the form MSG (t) x, y, −, −) := yW  (3)</p><formula xml:id="formula_31">for any x, y ∈ A st−1 , W (t) 1 ∈ A st−1×st , W<label>(t)</label></formula><p>2 ∈ A st−1×st , bias vector b (t) ∈ A st , and non-linear activation function σ.</p><p>The following is our main result for this section. We prove this theorem in the following subsections by providing the relationships that are summarised in Figure <ref type="figure" target="#fig_1">1</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">General anonymous MPNNs</head><p>We presently focus on the relation between the WL algorithm and anonymous MPNNs in general. More specifically, we establish that these are equally strong. We remark that in the proof of Theorem 5.1 we only need that M anon is weaker than M WL , as is indicated in Figure <ref type="figure" target="#fig_1">1</ref>. Proposition 5.2 (Based on <ref type="bibr" target="#b15">[Xu et al., 2019</ref><ref type="bibr" target="#b11">, Morris et al., 2019]</ref>). The classes M anon and M WL are equally strong.</p><p>Proof. First, we prove that M WL is weaker than M anon . It suffices to note that M WL ⊆ M anon .</p><p>It remains to argue that M anon is weaker than M WL . The proof is a trivial adaptation of the proofs of Lemma 2 in <ref type="bibr" target="#b15">[Xu et al., 2019]</ref> and Theorem 5 in <ref type="bibr" target="#b11">[Morris et al., 2019]</ref>. We show, by induction on the number of rounds of computation, that ℓ ℓ ℓ</p><formula xml:id="formula_32">(t) MWL ⊑ ℓ ℓ ℓ (t)</formula><p>M for all M ∈ M anon and every t ≥ 0.</p><p>Clearly, this holds for t = 0 since ℓ ℓ ℓ</p><formula xml:id="formula_33">(0) MWL = ℓ ℓ ℓ<label>(0)</label></formula><p>M := ν ν ν, by definition. We assume next that the induction hypothesis holds up to round t − 1 and consider round t. Let v and w be two vertices such that (ℓ ℓ ℓ</p><formula xml:id="formula_34">(t) MWL ) v = (ℓ ℓ ℓ (t)</formula><p>MWL ) w holds. This implies, by the definition of M WL , that (ℓ ℓ ℓ</p><formula xml:id="formula_35">(t−1) MWL ) v = (ℓ ℓ ℓ (t−1) MWL ) w and { {(ℓ ℓ ℓ (t−1) MWL ) u | u ∈ N G (v)} } = { {(ℓ ℓ ℓ (t−1) MWL ) u | u ∈ N G (w)} }.</formula><p>By the induction hypothesis, this implies that (ℓ ℓ ℓ</p><formula xml:id="formula_36">(t−1) M ) v = (ℓ ℓ ℓ (t−1) M ) w and { {(ℓ ℓ ℓ (t−1) M ) u | u ∈ N G (v)} } = { {(ℓ ℓ ℓ (t−1) M ) u | u ∈ N G (w)} }. As a consequence, there is a bijection between N G (v) and N G (w) such that to every vertex u ∈ N G (v) we can assign a unique vertex u ′ ∈ N G (w) such that (ℓ ℓ ℓ (t−1) M ) u = (ℓ ℓ ℓ (t−1) M ) u ′ . Hence, MSG (t) (ℓ ℓ ℓ (t−1) M ) v , (ℓ ℓ ℓ (t−1) M ) u , −, − = MSG (t) (ℓ ℓ ℓ (t−1) M ) w , (ℓ ℓ ℓ (t−1) M ) u ′ , −, − .</formula><p>Since this mapping between N G (v) and N G (w) is a bijection we also have:</p><formula xml:id="formula_37">m (t) v = u∈NG(v) MSG (t) (ℓ ℓ ℓ (t−1) M ) v , (ℓ ℓ ℓ (t−1) M ) u , −, − = u ′ ∈NG(w) MSG (t) (ℓ ℓ ℓ (t−1) M ) w , (ℓ ℓ ℓ (t−1) M ) u ′ , −, − = m (t) w .</formula><p>We may thus conclude that</p><formula xml:id="formula_38">(ℓ ℓ ℓ (t) M ) v = UPD (t) (ℓ ℓ ℓ (t−1) M ) v , m (t) v = UPD (t) (ℓ ℓ ℓ (t−1) M ) w , m (t) w = (ℓ ℓ ℓ (t) M ) w ,</formula><p>as desired.</p><p>We remark that we cannot use the results in <ref type="bibr" target="#b15">[Xu et al., 2019]</ref> and <ref type="bibr" target="#b11">[Morris et al., 2019]</ref> as a black box because the class M anon is more general than the class considered in those papers. The proofs in <ref type="bibr" target="#b15">[Xu et al., 2019]</ref> and <ref type="bibr" target="#b11">[Morris et al., 2019]</ref> relate to graph neural networks which, in round t ≥ 1, compute for each vertex v a label ℓ ℓ ℓ</p><formula xml:id="formula_39">(t) v , as follows: ℓ ℓ ℓ (t) v := f (t) comb ℓ ℓ ℓ (t−1) v , f (t) aggr { {ℓ ℓ ℓ (t−1) u | u ∈ N G (v)} } ,<label>(4)</label></formula><p>where</p><formula xml:id="formula_40">f (t)</formula><p>comb and f</p><formula xml:id="formula_41">(t)</formula><p>aggr are general (computable) combination and aggregation functions which we assume to assign labels in A st . Furthermore, ℓ ℓ ℓ (0) := ν ν ν, just as before. Every graph neural network of the form (4) is readily cast as an aMPNN. Indeed, it suffices to observe, just as we did in Example 3.2, that the aggregation functions f</p><formula xml:id="formula_42">(t) aggr { {ℓ ℓ ℓ (t−1) u | u ∈ N G (v)} } can be written in the form g (t) u∈NG(v) h (t) (ℓ ℓ ℓ (t−1) u</formula><p>) , based on Lemma 5 from <ref type="bibr" target="#b15">[Xu et al., 2019]</ref>.</p><p>Suppose that ν ν ν : V → A s0 . It now suffices to define for every t ≥ 1, every x and y in A st−1 , every v ∈ V and u ∈ N G (u):</p><formula xml:id="formula_43">MSG (t) (x, y, −, −) := h (t) (y) and UPD (t) (x, y) := f (t) comb x, g (t) (y) .</formula><p>(5) This is clearly an aMPNN which computes the same labelling as (4).</p><p>The aMPNNs that we consider in this paper are slightly more general than those defined by ( <ref type="formula">5</ref>). Indeed, we consider message functions that can also depend on the previous label ℓ ℓ ℓ</p><formula xml:id="formula_44">(t−1) v</formula><p>. In contrast, the message functions in (5) only depend on y, which corresponds to the previous labels ℓ ℓ ℓ</p><formula xml:id="formula_45">(t−1) u of neighbours u ∈ N G (v). Let M −</formula><p>anon denote the class of aMPNNs whose message functions only depend on the previous labels of neighbours. It now suffices to observe (see Example 3.2) that M WL ∈ M − anon to infer, combined with Proposition 5.2, that: Corollary 5.3. The classes M − anon , M anon and M WL are all equally strong.</p><p>We observe, however, that this does not imply that for every aMPNN M in M anon there exists an aMPNN M ′ in M − anon such that ℓ ℓ ℓ</p><formula xml:id="formula_46">(t) M ≡ ℓ ℓ ℓ (t)</formula><p>M ′ for all t ≥ 0. Indeed, the corollary implies that for every M in M anon there exists an aMPNN M ′ in M − anon such that M M ′ , and there exists an M ′′ in M anon , possibly different from M , such that M ′ M ′′ . In fact, such an aMPNN M ′′ , in this case is M WL .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Graph neural network-based anonymous MPNNs</head><p>In this subsection we study the subclasses of aMPNNs arising from graph neural network architectures. For convenience, let us write M GNN := M sign GNN ∪ M ReLU GNN . We start by stating a direct consequence of Proposition 5.2. It follows by observing that M GNN is a subclass of M anon as presented in Example 3.1. Corollary 5.4. The class M GNN is weaker than M anon and is thus also weaker than M WL .</p><p>More challenging is to show that M sign GNN , M ReLU GNN and M WL , and thus also M anon , are equally strong. The following results are known. Theorem 5.5 ( <ref type="bibr" target="#b11">[Morris et al., 2019]</ref>). (i) The classes M sign GNN and M WL are equally strong. (ii) The class M ReLU GNN is weaker than M WL , and M WL is weaker than M ReLU GNN , with a factor of two, i.e., M WL ×2 M ReLU GNN .</p><p>The reason for the factor of two in (ii) in Theorem 5.5 is due to a simulation of the sign activation function by means of a two-fold application of the ReLU function. We next show that this factor of two can be avoided. As a side effect, we obtain a simpler aMPNN M in M GNN , satisfying M WL M , than the one constructed in <ref type="bibr" target="#b11">[Morris et al., 2019]</ref>. The proof strategy is inspired by that of <ref type="bibr" target="#b11">[Morris et al., 2019]</ref>. Crucial in the proof is the notion of row-independence modulo equality, which we define next. Definition 5.6 (Row-independence modulo equality). A labelling ℓ ℓ ℓ : V → A s is row-independent modulo equality if the set of unique labels assigned by ℓ ℓ ℓ is linearly independent.</p><p>In what follows, we always assume that the initial labelling ν ν ν of G is row-independent modulo equality. One can always ensure this by extending the labels. Theorem 5.7. The classes M ReLU GNN and M WL are equally strong.</p><p>Proof. We already know that M ReLU GNN is weaker than M WL (Theorem 5.5 and also Corollary 5.4). It remains to show that M WL is weaker than M ReLU GNN . That is, given an aMPNN M WL , we need to construct an aMPNN M in M ReLU GNN such that ℓ ℓ ℓ</p><formula xml:id="formula_47">(t) M ⊑ ℓ ℓ ℓ (t)</formula><p>MWL , for all t ≥ 0. We observe that since ℓ ℓ ℓ</p><formula xml:id="formula_48">(t) MWL ⊑ ℓ ℓ ℓ (t)</formula><p>M for any M in M ReLU GNN , this is equivalent to constructing an M such that ℓ ℓ ℓ</p><formula xml:id="formula_49">(t) M ≡ ℓ ℓ ℓ (t) MWL .</formula><p>The proof is by induction on the number of computation rounds. The aMPNN M in M ReLU GNN that we will construct will use message and update functions of the form: MSG (t) x, y, −, −) := yW (t) and UPD (t) (x, y) := ReLU pxW (t) + y + b (t)  (6)</p><p>for some value p ∈ A, 0 &lt; p &lt; 1, weight matrix W (t) ∈ A st−1×st , and bias vector b (t) ∈ A st . Note that, in contrast to aMPNNs of the form (3), we only have one weight matrix per round, instead of two, at the cost of introducing an extra parameter p ∈ A. Furthermore, the aMPNN constructed in <ref type="bibr" target="#b11">[Morris et al., 2019]</ref> uses two distinct weight matrices in A (st−1+s0)×(st+s0) (we come back to this at the end of this section) whereas our weight matrices are elements of A st−1×st and thus of smaller dimension.</p><p>The induction hypothesis is that ℓ ℓ ℓ</p><formula xml:id="formula_50">(t) M ≡ ℓ ℓ ℓ (t)</formula><p>MWL and that ℓ ℓ ℓ</p><formula xml:id="formula_51">(t)</formula><p>M is row-independent modulo equality.</p><p>For t = 0, we have that for any M ∈ M ReLU GNN , ℓ ℓ ℓ</p><formula xml:id="formula_52">(0) M = ℓ ℓ ℓ (0) MWL := ν ν ν, by definition. Moreover, ℓ ℓ ℓ (0)</formula><p>M is row-independent modulo equality because ν ν ν is so, by assumption.</p><p>We next assume that up to round t − 1 we have found weight matrices and bias vectors for M such that ℓ ℓ ℓ (t−1) M satisfies the induction hypothesis. We will show that for round t we can find a weight matrix W (t) ∈ A st−1×st and bias vector b (t) ∈ A st such that ℓ ℓ ℓ (t) M also satisfies the hypothesis.</p><p>Let L (t−1) ∈ A n×st−1 denote the matrix consisting of rows (ℓ ℓ ℓ</p><formula xml:id="formula_53">(t−1) M</formula><p>) v , for v ∈ V . Moreover, we denote by uniq(L (t−1) ) a (m × s t−1 )-matrix consisting of the m unique rows in L (t−1) (the order of rows is irrelevant). We denote the rows in uniq(L (t−1) ) by a 1 , . . . , a m ∈ A st−1 . By the induction hypothesis, these rows are linearly independent. Following the same argument as in <ref type="bibr" target="#b11">[Morris et al., 2019]</ref>, this implies that there exists an (s t−1 × m)-matrix U (t)  such that uniq(L (t−1) )U (t) = I. Let us denote by e 1 , . . . , e m ∈ A m the rows of I. In other words, in e i , all entries are zero except for entry i, which holds value 1.</p><p>We consider the following intermediate labelling µ µ µ (t) </p><formula xml:id="formula_54">: V → A m defined by v → (A + pI)L (t−1) U (t) v . (<label>7</label></formula><formula xml:id="formula_55">)</formula><p>We know that for every vertex v, (ℓ ℓ ℓ</p><formula xml:id="formula_56">(t−1) M</formula><p>) v corresponds to a unique row a i in uniq(L (t−1) ). We denote the index of this row by ρ(v). More specifically, (ℓ ℓ ℓ</p><formula xml:id="formula_57">(t−1) M ) v = a ρ(v) . Let N G (v, i) := {u | u ∈ N G (v), ρ(v) = i}. That is, N G (v, i)</formula><p>consists of all neighbours u of v which are labelled as a i by ℓ ℓ ℓ</p><formula xml:id="formula_58">(t−1) M</formula><p>. It is now readily verified that the label µ µ µ</p><formula xml:id="formula_59">(t) v defined in (7) is of the form µ µ µ (t) v = pe ρ(v) + m i=1 |N G (v, i)|e i .<label>(8)</label></formula><p>We clearly have that ℓ ℓ ℓ</p><formula xml:id="formula_60">(t)</formula><p>MWL ⊑ µ µ µ (t) . The converse also holds, as is shown in the following lemma.</p><p>Lemma 5.8. For any two vertices v and w, we have that µ µ µ</p><formula xml:id="formula_61">(t) v = µ µ µ (t) w implies (ℓ ℓ ℓ (t) MWL ) v = (ℓ ℓ ℓ (t) MWL ) w .</formula><p>Proof. We argue by contradiction. Suppose, for the sake of contradiction, that there exist two vertices v, w ∈ V such that µ µ µ (t) v = µ µ µ (t) w and (ℓ ℓ ℓ</p><formula xml:id="formula_62">(t) MWL ) v = (ℓ ℓ ℓ (t) MWL ) w<label>(9)</label></formula><p>hold. We show that this is impossible for any value p satisfying 0 &lt; p &lt; 1. (Recall from (8) that µ µ µ</p><formula xml:id="formula_63">(t)</formula><p>v depends on p.)</p><p>We distinguish between the following two cases. If (ℓ ℓ ℓ</p><formula xml:id="formula_64">(t) MWL ) v = (ℓ ℓ ℓ (t) MWL ) w then either (i) (ℓ ℓ ℓ (t−1) MWL ) v = (ℓ ℓ ℓ (t−1) MWL ) w ; or (ii) (ℓ ℓ ℓ (t−1) MWL ) v = (ℓ ℓ ℓ (t−1)</formula><p>MWL ) w and { {(ℓ ℓ ℓ</p><formula xml:id="formula_65">(t−1) MWL ) u | u ∈ N G (v)} } = { {(ℓ ℓ ℓ (t−1) MWL ) u | u ∈ N G (w)} }.</formula><p>We first consider case (i). Observe that (ℓ ℓ ℓ</p><formula xml:id="formula_66">(t−1) MWL ) v = (ℓ ℓ ℓ (t−1) MWL ) w implies that (ℓ ℓ ℓ (t−1) M ) v = (ℓ ℓ ℓ (t−1) M</formula><p>) w . This follows from the induction hypothesis ℓ ℓ ℓ</p><formula xml:id="formula_67">(t−1) M ≡ ℓ ℓ ℓ (t−1)</formula><p>MWL . It now suffices to observe that µ µ µ</p><formula xml:id="formula_68">(t) v = µ µ µ (t)</formula><p>w implies that the corresponding linear combinations, as described in (8), satisfy:</p><formula xml:id="formula_69">pe ρ(v) + m i=1 |N G (v, i)|e i = pe ρ(w) + m i=1 |N G (w, i)|e i .</formula><p>We can assume, without loss of generality, that (ℓ ℓ ℓ</p><formula xml:id="formula_70">(t−1) M ) v = a 1 and (ℓ ℓ ℓ (t−1) M</formula><p>) w = a 2 . Recall that a 1 and a 2 are two distinct labels. Then, the previous equality implies:</p><formula xml:id="formula_71">(|N G (v, 1)| + p − |N G (w, 1)|) e 1 + (|N G (v, 2)| − |N G (w, 2)| − p) e 2 + m i=3 (|N G (v, i)| − |N G (w, i)|) e i = 0.</formula><p>Since e 1 , . . . , e m are linearly independent, this implies that</p><formula xml:id="formula_72">|N G (v, i)| − |N G (w, i)| = 0 for all i = 3, . . . , m and |N G (v, 1)| + p − |N G (w, 1)| = 0 and |N G (v, 2)| − |N G (w, 2)| − p = 0. Since |N G (v, 1)| − |N G (w, 1)| ∈ Z and 0 &lt; p &lt; 1,</formula><p>this is impossible. We may thus conclude that case (i) cannot occur.</p><p>Suppose next that we are in case (ii). Recall that for case (ii), we have that (ℓ ℓ ℓ</p><formula xml:id="formula_73">(t−1) MWL ) v = (ℓ ℓ ℓ (t−1)</formula><p>MWL ) w and thus also (ℓ ℓ ℓ</p><formula xml:id="formula_74">(t−1) M ) v = (ℓ ℓ ℓ (t−1) M</formula><p>) w . Using the same notation as above, we may assume that (ℓ ℓ ℓ</p><formula xml:id="formula_75">(t−1) M ) v = (ℓ ℓ ℓ (t−1) M</formula><p>) w = a 1 . In case (ii), however, we have that { {(ℓ ℓ ℓ</p><formula xml:id="formula_76">(t−1) MWL ) u | u ∈ N G (v)} } = { {(ℓ ℓ ℓ (t−1) MWL ) u | u ∈ N G (w)} } and thus also { {(ℓ ℓ ℓ (t−1) M ) u | u ∈ N G (v)} } = { {(ℓ ℓ ℓ (t−1) M ) u | u ∈ N G (w)} }.</formula><p>That is, there must exist a label assigned by ℓ ℓ ℓ (t−1) M that does not occur the same number of times in the neighbourhoods of v and w, respectively. Suppose that this label is a 2 . The case when this label is a 1 can be treated similarly. It now suffices to observe that µ µ µ</p><formula xml:id="formula_77">(t) v = µ µ µ (t)</formula><p>w implies that the corresponding linear combinations, as described in (8), satisfy:</p><formula xml:id="formula_78">(|N G (v, 1)| + p) e 1 + |N G (v, 2)|e 2 + m i=3 |N G (v, i)|e i = (|N G (w, 1)| + p) e 1 + |N G (w, 2)|e 2 + m i=3 |N G (w, i)|e i .</formula><p>Using a similar argument as before, based on the linear independence of e 1 , . . . , e m , we can infer that |N G (v, 2)| = |N G (w, 2)|. We note, however, that a 2 appeared a different number of times among the neighbours of v and w. Hence, also case (ii) is ruled out and our assumption ( <ref type="formula" target="#formula_62">9</ref>) is invalid. This implies µ µ µ (t) ⊑ ℓ ℓ ℓ (t) MWL , as desired and thus concludes the proof of the lemma.</p><p>From here, to continue with the proof of Theorem 5.7, we still need to take care of the ReLU activation function. Importantly, its application should ensure row-independence modulo equality and make sure the labelling "refines" ℓ ℓ ℓ (t) MWL . To do so, we again follow closely the proof strategy of <ref type="bibr" target="#b11">[Morris et al., 2019]</ref>. More specifically, we will need an analogue of the following result. In the sequel we denote by J a matrix with all entries having value 1 and whose size will be determined from the context. Lemma 5.9 (Lemma 9 from <ref type="bibr" target="#b11">[Morris et al., 2019]</ref>). Let C ∈ A m×w be a matrix in which all entries are non-negative and all rows are pairwise disjoint. Then there exists a matrix X ∈ A w×m such that sign(CX − J) is a non-singular matrix in A m×m .</p><p>We prove the following for the ReLU function.</p><p>Lemma 5.10. Let C ∈ A m×w be a matrix in which all entries are non-negative, all rows are pairwise disjoint and such that no row consists entirely out of zeroes<ref type="foot" target="#foot_2">3</ref> . Then there exists a matrix X ∈ A w×m and a constant q ∈ A such that ReLU(CX − qJ) is a non-singular matrix in A m×m .</p><p>Proof. Let C be the maximal entry in C and consider the column vector z = (1, C, C 2 , . . . , C w−1 ) T ∈ A w×1 . Then each entry in c = Cz ∈ A m×1 is positive and all entries in c are pairwise distinct. Let P be a permutation matrix in</p><formula xml:id="formula_79">A m×m such that c ′ = Pc is such that c ′ = (c ′ 1 , c ′ 2 , . . . , c ′ m ) T ∈ A m×1 with c ′ 1 &gt; c ′ 2 &gt; • • • &gt; c ′ m &gt; 0. Consider x = 1 c ′ 1 , . . . , 1 c ′ m ∈ A 1×m . Then, for E = c ′ x ∈ A m×m E ij = c ′ i c ′ j and E ij =    1 if i = j &gt; 1 if i &lt; j &lt; 1 if i &gt; j.</formula><p>Let q be the greatest value in E smaller than 1. Consider F = E − qJ. Then,</p><formula xml:id="formula_80">F ij = c ′ i c ′ j − q and F ij =    1 − q if i = j &gt; 0 if i &lt; j ≤ 0 if i &gt; j.</formula><p>As a consequence,</p><formula xml:id="formula_81">ReLU(F) ij =    1 − q if i = j &gt; 0 if i &lt; j 0 if i &gt; j.</formula><p>This is an upper triangular matrix with (nonzero) value 1 − q on its diagonal. It is therefore non-singular.</p><p>We now observe that QReLU(F) = ReLU(QF) for any row permutation Q. Furthermore, non-singularity is preserved under row permutations and QJ = J. Hence, if we define X = zx and use the permutation matrix P, then:</p><formula xml:id="formula_82">PReLU(CX − qJ) = ReLU(PCzx − qPJ) = ReLU(E − qJ) = ReLU(F)</formula><p>, and we have that ReLU(CX − qJ) is non-singular, as desired. This concludes the proof of the lemma.</p><p>We now apply this lemma to the matrix uniq(M (t) ), with M (t) ∈ A n×m consisting of the rows µ µ µ</p><formula xml:id="formula_83">(t) v , for v ∈ V .</formula><p>Inspecting the expression from Equation ( <ref type="formula" target="#formula_59">8</ref>) for µ µ µ (t) v we see that each row in M (t) holds non-negative values and no row consists entirely out of zeroes. Let X (t) and q (t) be the matrix and constant returned by Lemma 5.10 such that ReLU uniq(M (t) )X (t) − q (t) J is an m × m non-singular matrix. We now define</p><formula xml:id="formula_84">ℓ ℓ ℓ (t) M := ReLU M (t) X (t) − q (t) J .</formula><p>From the non-singularity of ReLU uniq(M (t) )X (t) − q (t) J we can immediately infer that ℓ ℓ ℓ</p><formula xml:id="formula_85">(t)</formula><p>M is row-independent modulo equality. It remains to argue that ℓ ℓ ℓ</p><formula xml:id="formula_86">(t) M ≡ ℓ ℓ ℓ (t)</formula><p>MWL . This now follows from the fact that µ µ µ (t) ≡ ℓ ℓ ℓ</p><formula xml:id="formula_87">(t)</formula><p>MWL and each of the m unique labels assigned by µ µ µ (t) uniquely corresponds to a row in uniq(M (t) ), which in turn can be mapped bijectively to a row in ReLU uniq(M (t) )X (t) − q (t) J . We conclude by observing that the desired weight matrices and bias vector at round t for M are now given by W (t) := U (t) X (t) and b (t) := −q (t) 1. This concludes the proof of Theorem 5.7.</p><p>We remark that the previous proof can be used for M sign GNN as well. One just has to use Lemma 5.9 instead of Lemma 5.10. It is interesting to note that the bias vector for the sign activation function in Lemma 5.9 is the same for every t. A similar statement holds for the ReLU function. Indeed, we recall that we apply Lemma 5.10 to uniq(M (t) ). For every t, the entries in this matrix are of the form i + p (which is smaller than i + 1) or i, for i ∈ {1, 2, . . . , n}. Hence, for every t, the maximal entry (denoted by C in the proof of Lemma 5.10) is upper bounded by n + 1. The value q (t) relates to the largest possible ratios, smaller than 1, of elements in the matrix constructed in Lemma 5.10.</p><p>When the lemma is applied to an m × w matrix, this ratio is upper bounded by (n+1) w −1 (n+1) w . Note that, since the lemma is applied to matrices arising from µ µ µ (t) , w will always be at most n. Hence, taking any q (t) := q for (n+1) n −1 (n+1) n &lt; q &lt; 1 suffices. We can take q to be arbitrarily close to 1, but not 1 itself.</p><p>We can thus strengthen Theorem 5.5, as follows. We denote by M GNN − the class of aMPNNs using message and update functions of the form: MSG (t) x, y, −, −) := yW (t) and UPD (t) (x, y) := σ pxW (t) + y − q1 , (10) parameterised with values p, q ∈ A, 0 ≤ p, q ≤ 1 and weight matrices W (t) ∈ A st−1×st , and where σ can be either the sign or ReLU function.</p><p>Corollary 5.11. The class M GNN − is equally strong as M GNN and is equally strong as M WL .</p><p>We remark that the factor two, needed for the ReLU activation function in Theorem 5.5, has been eliminated. Phrased in terms of graph neural networks, an aMPNN in M GNN − is of the form</p><formula xml:id="formula_88">L (t) = σ (A + pI)L (t−1) W (t) − qJ ,<label>(11)</label></formula><p>and, thus, these suffice to implement the WL algorithm. It would be interesting to see how graph neural networks defined by ( <ref type="formula" target="#formula_88">11</ref>), with learnable parameters p and q, perform in practice. In contrast, if one inspects the proof in <ref type="bibr">[Morris et al., 2019, pg. 14, Appendix]</ref>, even for the sign activation function, the graph neural network given to implement the WL algorithm has the more complicated form:</p><formula xml:id="formula_89">L (0) , L (t) := σ L (0) , L (t−1) I 0 0 0 + A L (0) , L (t−1) 0 0 0 W (t) − (0, J) .</formula><p>We thus have obtained a simpler class of aMPNNs, M GNN − , which is equally strong as M WL . We will see in the next section that the parameter p also plays an important role for degree-aware aMPNNs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">The distinguishing power of degree-aware MPNNs</head><p>In this section we compare various classes of degree-aware MPNNs in terms of their distinguishing power. We recall that degree-aware MPNNs (dMPNNs for short) have message functions that depend on the labels and degrees of vertices. To compare these classes we use Definition 4.2 and also Definition 4.3. In the latter definition we will be interested in the function g(n) = n + 1. That is, when comparing classes of dMPNNs we consider the notions of being weaker or stronger with 1 step ahead.</p><p>Table <ref type="table">1</ref>: Various graph neural network formalisms, as reported in e.g., <ref type="bibr" target="#b8">[Kipf and Welling, 2017</ref><ref type="bibr" target="#b13">, Wu et al., 2019a</ref><ref type="bibr" target="#b10">, Meltzer et al., 2019]</ref>, which correspond to degree-aware MPNNs. We implicitly assume the presence of a bias matrix B (t) consisting of copies of the same row b (t) .</p><formula xml:id="formula_90">dGNN 1 : L (t) := σ D −1 AL (t−1) W (t) dGNN 2 : L (t) := σ D −1 /2 AD −1 /2 L (t−1) W (t) dGNN 3 : L (t) := σ (D + I) −1 (A + I)L (t−1) W (t) dGNN 4 : L (t) := σ D + I −1 /2 (A + I) D + I −1 /2 L (t−1) W (t)</formula><p>dGNN 5 :</p><formula xml:id="formula_91">L (t) := σ (D −1 /2 AD −1 /2 + I)L (t−1) W (t) dGNN 6 : L (t) := σ (rI + (1 − r)D) −1 /2 (A + pI)(rI + (1 − r)D) −1 /2 L (t−1) W (t)</formula><p>We will also compare degree-aware MPNNs with anonymous MPNNs. Recall that by Theorem 5.1 all classes of anonymous MPNNs considered in Section 5 are equivalent for ≡. In particular, they are all equivalent to the class M WL . Therefore, instead of comparing a class M of dMPNNs with all classes considered in Section 5 it suffices to compare it with M WL . For example, if M WL g M then the same relationship to M holds for all classes in Section 5 that are equivalent to M WL . Similarly, for when M g M WL holds.</p><p>Quintessential examples of degree-aware MPNNs are the popular graph convolutional networks, as introduced by <ref type="bibr" target="#b8">[Kipf and Welling, 2017]</ref>. These are of the form:</p><formula xml:id="formula_92">L (t) := σ D + I −1/2 (A + I) D + I −1/2 L (t−1) W (t) ,</formula><p>as already described and phrased as dMPNNs in Example 3.3. In fact, many commonly used graph neural networks use degree information. We list a couple of such formalisms in Table <ref type="table">1</ref>. It is easily verified that these can all be cast as dMPNNs along the same lines as Example 3.3. We detail this later in this section.</p><p>We consider the following classes of dMPNNs. First, we recall that M deg is the class of degree-aware MPNNs. Furthermore, for i ∈ {1, 2, . . . , 6}, we define M dGNN i as the class of dMPNNs originating from a GNN of the form dGNN i , from Table <ref type="table">1</ref>, by varying the weight matrices W (t) and, when applicable, the bias B (t) and parameters p, r.</p><p>The following is our main result for this section. Theorem 6.1. For the class of degree-aware MPNNs:</p><formula xml:id="formula_93">1. M WL M deg and M deg M WL ; 2. M deg +1 M WL .</formula><p>For the architectures from Table <ref type="table">1:</ref> 3. M dGNN i M WL for i = 2, 4, 5, 6 and M dGNN i M WL for i = 1, 3;</p><formula xml:id="formula_94">4. M WL M dGNN i for 1 ≤ i ≤ 5 and M WL M dGNN 6 .</formula><p>We prove this theorem in the following subsections by providing the relationships that are summarised in Figure <ref type="figure">2</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.1">General degree-aware MPNNs</head><p>We first focus on the relation between the WL algorithm and dMPNNs in general. More specifically, we start with the first item in Theorem 6.1. As part of the proof we show that M dGNN4 M WL . We can similarly show that M dGNN 2 , M dGNN 5 , M dGNN 6 M WL , hereby also settling the first part of the third item in Theorem 6.1. Proposition 6.2. The class M WL is weaker than M deg ; but the class M deg is not weaker than M WL . Proof. To prove the first part of the claim notice that M anon is weaker than M deg , simply because any aMPNN is a dMPNN. Then the result follows from Theorem 5.1.</p><p>For the second part it suffices to provide a dMPNN M and a labelled graph (G, ν ν ν) such that there exists a round t ≥ 0 for which ℓ ℓ ℓ</p><formula xml:id="formula_95">(t) MWL ⊑ ℓ ℓ ℓ (t)</formula><p>M holds. We construct such an M originating from a GCN <ref type="bibr" target="#b8">[Kipf and Welling, 2017]</ref> defined in Example 3.3. That is, M is a dMPNN in M dGNN 4 . Consider the labelled graph (G, ν ν ν) with vertex labelling ν ν ν v1 = ν ν ν v2 = (1, 0, 0), ν ν ν v3 = ν ν ν v6 = (0, 1, 0) and ν ν ν v4 = ν ν ν v5 = (0, 0, 1), and edges {v 1 , v 3 }, {v 2 , v 3 }, {v 3 , v 4 }, {v 4 , v 5 }, and {v 5 , v 6 }, as depicted in Figure <ref type="figure">3</ref>. </p><formula xml:id="formula_96">M WL M deg M WL M WL M WL M dGNN 1 , M dGNN 3 M dGNN 6 M dGNN 2 , M dGNN 4 , M dGNN 5 M WL Prop.</formula><formula xml:id="formula_97">v 1 v 2 v 3 v 4 v 5 v 6 Figure 3: Graph G.</formula><p>Recall that ℓ ℓ ℓ (0) = ν ν ν and</p><formula xml:id="formula_98">(ℓ ℓ ℓ<label>(1)</label></formula><formula xml:id="formula_99">M ) v := ReLU   1 1 + d v ℓ ℓ ℓ (0) v W (1) + u∈NG(v) 1 √ 1 + d v 1 √ 1 + d u ℓ ℓ ℓ (0) u W (1)   .</formula><p>We next define W (1) :=</p><formula xml:id="formula_100">1 0 0 0 1 0 0 0 1 . It can be verified that ℓ ℓ ℓ (1) M =           1 2 0 0 1 2 0 0 0 1 4 0 0 0 1 3 0 0 1 3 0 1 2 0           +           0 1 2 √ 2 0 0 1 2 √ 2 0 1 √ 2 0 1 2 √ 3 0 1 2 √ 3 1 3 0 1 √ 6 1 3 0 0 1 √ 6           =           1 2 1 2 √ 2 0 1 2 1 2 √ 2 1 √ 2 1 4 1 2 √ 3 0 1 2 √ 3 2 3 0 1 √ 6 2 3 0 1 2 1 √ 6           .</formula><p>We observe that (ℓ ℓ ℓ</p><p>(1)</p><formula xml:id="formula_101">M ) v4 = (ℓ ℓ ℓ<label>(1)</label></formula><p>M ) v5 . We note, however, that (ℓ ℓ ℓ</p><p>(1)</p><formula xml:id="formula_102">MWL ) v4 = HASH (0, 0, 1), { {(0, 0, 1), (0, 1, 0)} } = (ℓ ℓ ℓ (1) MWL ) v5 . Hence, ℓ ℓ ℓ (1) MWL ⊑ ℓ ℓ ℓ (1) M .</formula><p>The rest of this section is devoted to prove the second item in Theorem 6.1. Proposition 6.3. M deg +1 M WL We will need the following lemma that states that anonymous MPNNs can compute the degrees of vertices in the first round of computation. Lemma 6.4. Let (G, ν ν ν) be a labelled graph with ν ν ν : V → A s . There exists an aMPNN M d such that (ℓ ℓ ℓ</p><p>(1)</p><formula xml:id="formula_103">M d ) v = (ν ν ν v , d v ) ∈ A s+1 for every vertex v in V .</formula><p>Proof. We define the aMPNN M d with the following message and update functions. For each x, y ∈ A s , z ∈ A, and vertices v, u ∈ N G (v) we define: MSG (1) (x, y, −, −) := 1 and UPD (1) (x, z) := (x, z) .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Then, m</head><p>(1)</p><formula xml:id="formula_104">v := u∈NG(v) 1 = d v and (ℓ ℓ ℓ (1) M d ) v := UPD (1) (ν ν ν v , d v ) = (ν ν ν v , d v ) ∈ A s+1 , as desired.</formula><p>We are now ready to prove Proposition 6.3.</p><p>Proof of Proposition 6.3. By Theorem 5.1 it suffices to prove that the class M deg is weaker than M anon , with 1 step ahead. Let (G, ν ν ν) be a labelled graph with ν ν ν : V → A s0 . Take an arbitrary dMPNN M 1 such that for every round t ≥ 1 the message function is MSG</p><formula xml:id="formula_105">(t) M1 (x, y, d v , d u ) ∈ A s ′ t and UPD (t)</formula><p>M1 (x, z) is the update function.</p><p>We construct an aMPNN M 2 such that ℓ ℓ ℓ</p><formula xml:id="formula_106">(t+1) M2 ⊑ ℓ ℓ ℓ (t)</formula><p>M1 holds, as follows. We denote the message and update functions of M 2 by MSG (t) M2 and UPD (t) M2 , respectively. We will keep as an invariant (I1) stating that for all v if we have</p><formula xml:id="formula_107">x ′ = (ℓ ℓ ℓ (t−1) M1 ) v ∈ A st−1 then x = (x ′ , d v ) = (ℓ ℓ ℓ (t) M2 ) v ∈ A st−1+1 .</formula><p>For t = 1, we let MSG</p><p>(1)</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>M2 and UPD</head><p>(1) M2 be the functions defined by Lemma 6.4. As a consequence, (ℓ ℓ ℓ</p><p>(1)</p><formula xml:id="formula_108">M2 ) v = (ν ν ν v , d v ) ∈ A s0+1</formula><p>for every vertex v. We clearly have that ℓ ℓ ℓ </p><formula xml:id="formula_109">UPD (t) M2 (x, z) := UPD (t−1) M1 (x ′ , z ′ ), x ∈ A st−1+1 ,</formula><p>where x = (x ′ , x) and by invariant (I1) x = d v . In other words, in each round t ≥ 2, M 2 extracts the degrees from the last entries in the labels and simulates round t − 1 of M 1 . It is readily verified that ℓ ℓ ℓ</p><formula xml:id="formula_110">(t) M2 ⊑ ℓ ℓ ℓ (t−1) M1</formula><p>for every t, as desired and that the invariant (I1) holds.</p><p>In particular it follows from Proposition 6.3 that for the dMPNN M constructed in the proof of Proposition 6.2 it holds that ℓ ℓ ℓ</p><p>(2) MWL ⊑ ℓ ℓ ℓ</p><p>(1) M .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.2">Graph neural network-based degree-aware MPNNs</head><p>We next consider the relation between the WL algorithm and dMPNNs that originate from graph neural networks as those listed in Table <ref type="table">1</ref>. More specifically, we consider the following general graph neural network architecture</p><formula xml:id="formula_111">L (t) := σ L (t−1) W (t) 1 + diag(g)(A + pI)diag(h)L (t−1) W (t) 2 + B (t) ,<label>(12)</label></formula><p>where p ∈ A is parameter satisfying 0 ≤ p ≤ 1, W</p><p>1 and W (t) 2 are learnable weight matrices in A st−1×st , B (t) is a bias matrix consisting of n copies of the same row b (t) , and diag(g) and diag(h) are positive diagonal matrices in A n×n obtained by putting the vectors g and h in A n on their diagonals, respectively. We only consider vectors g and h which are degree-determined. That is, when d v = d w then g v = g w and h v = h w for all vertices v and w. Furthermore, σ is either the sign or ReLU non-linear activation function.</p><p>It is readily verified that all graph neural networks mentioned so far can be seen as special cases of (12). Moreover, graph neural networks of the form (12) can be cast as dMPNNs. We denote the resulting class of dMPNNs by M dGNN . The reason that one obtains dMPNNs is because of the degree-determinacy assumption. More specifically, degreedeterminacy implies that g = (g(d v1 ), g(d v2 ), . . . , g(d vn )) and h = (h(d v1 ), h(d v2 ), . . . , h(d vn ))</p><p>for some functions g : N + → A + and h : N + → A + . Example 6.5. The GCN architecture of <ref type="bibr" target="#b8">[Kipf and Welling, 2017]</ref> corresponds to graph neural networks of the form (12), with</p><formula xml:id="formula_113">W (t) 1 = 0 ∈ A st−1×st , p = 1, b (t) = 0 ∈ A s ,</formula><p>and where g = h are defined by the function</p><formula xml:id="formula_114">g(n) = h(n) = (1 + n) −1/2 .</formula><p>We define the class M dGNN as the class of dMPNNs with message and update functions of the form:</p><formula xml:id="formula_115">MSG (t) x, y, d v , d u ) := 1 d v xW (t) 1 + pg(d v )h(d v )xW (t) 2 + g(d v )h(d u )yW (t) 2<label>(13)</label></formula><p>and</p><formula xml:id="formula_116">UPD (t) (x, y) := σ (y) (14) for any x, y ∈ A st−1 , W (t) 1 ∈ A st−1×st ,W<label>(t)</label></formula><p>2 ∈ A st−1×st , bias vector b (t) ∈ A st , and non-linear activation function σ. We note that this encoding is just a generalisation of the encoding of GCNs as dMPNNs given in Example 3.3.</p><p>We know from Proposition 6.3 that the class M dGNN is weaker than M WL , with 1 step ahead. Indeed, it suffices to note that M dGNN ⊆ M deg . In particular, the classes M dGNN 1 -M dGNN 6 corresponding to the graph neural network architectures from Table 1 are all weaker than M WL , with 1 step ahead. Furthermore, in the proof of Proposition 6.2 we have shown that the condition that M WL is 1 step ahead is necessary for M dGNN 4 , and thus also for M dGNN . We mentioned that one can provide similar examples for M dGNN 2 , M dGNN 5 and M dGNN 6 .</p><p>In contrast, we next show that the two remaining classes, M dGNN 1 and M dGNN 3 , are weaker than M WL (with no step ahead). The reason is that dMPNNs in these classes are equivalent to dMPNNs that only use degree information after aggregation takes places. These in turn are equivalent to anonymous MPNNs. We first show a more general result, related to graph neural networks of the form (12) in which diag(h) = I. In other words, the function h : N + → A underlying h is the constant one function, i.e., h(n) = 1 for all n ∈ N + . Proposition 6.6. The subclass of M dGNN , in which the function h is the constant one function, is weaker than M WL .</p><p>Proof. We show that any MPNN M in this class is an anonymous MPNNs. To see this, it suffices to observe that any dMPNN in M dGNN , and thus also M in particular, is equivalent to a dMPNN with message and update functions defined as follows. For every round t ≥ 1, every x, y ∈ A st−1 , z = (z ′ , z) ∈ A st+1 , and every vertex v and u ∈ N G (v):</p><formula xml:id="formula_117">MSG (t) (x, y, d v , d u ) := h(d u )yW (t) 2 , 1 ∈ A st+1<label>(15)</label></formula><p>and</p><formula xml:id="formula_118">UPD (t) (x, z) := σ xW (t) 1 + g(z)z ′ + pg(z)h(z)xW (t) 2 + b (t) ∈ A st ,<label>(16)</label></formula><p>where z ∈ A will hold the degree information of the vertex under consideration (i.e., d v ) after message passing. That is, we use a similar trick as in Lemma 6.4. Since we consider MPNNs in which h(d u ) = 1, the message function (15) indeed only depends on y. As a consequence, M is equivalent to an anonymous MPNN. From Theorem 5.1 and in particular from M anon M WL , the proposition follows.</p><p>The architectures M dGNN 1 and M dGNN 3 from Table <ref type="table">1</ref> clearly satisfy the assumption in the previous proposition and hence M dGNN 1 , M dGNN 3 M WL .</p><p>We thus have shown the remaining part of the third item in Theorem 6.1. Corollary 6.7. The classes M dGNN 1 and M dGNN 3 are weaker than M WL .</p><p>To conclude, we investigate whether M dGNN and its subclasses M dGNN1 -M dGNN6 are stronger than M WL . For M dGNN this follows from Theorem 5.1, stating in particular that M GNN ≡ M WL , and from the following remark. Remark 6.8. It holds that M GNN M dGNN .</p><p>Indeed, we first note that the class M GNN is not a subclass of M dGNN since these classes differ in the message and update functions used. We observe, however, that M GNN corresponds to the subclass of M dGNN in which the functions g and h are the constant one function, i.e., g(n) = h(n) = 1 for all n ∈ N + , and moreover, p = 0. More precisely, for every MPNN M in M GNN there is an MPNN M ′ in M dGNN such that M ≡ M ′ , from which Remark 6.8 follows.</p><p>So, we know already that M WL M dGNN . However, the aMPNN M in M GNN such that M WL M holds, as constructed for Theorems 5.5 and 5.7, does not comply with the forms of MPNNs corresponding to the graph neural networks given in Table <ref type="table">1</ref>. We next investigate which classes M dGNN i are stronger that M WL .</p><p>We start with some negative results, hereby showing part of the fourth item in Theorem 6.1. Proposition 6.9. None of the classes M dGNN i , for i ∈ {1, 2, . . . , 5}, are stronger than M WL .</p><p>Proof. The proof consists of a number of counterexamples related to the various classes of dMPNNs under consideration. For convenience, we describe the counterexamples in terms of graph neural networks rather than in their dMPNN form.</p><p>We first prove the proposition for classes of dMPNNs related to graph neural networks of the form:</p><formula xml:id="formula_119">L (t) := σ diag(g)Adiag(h)L (t−1) W (t) + B (t) .</formula><p>This includes M dGNN i , for i = 1, 2. Consider the labelled graph (G 1 , ν ν ν) with vertex labelling ν ν ν v1 = (1, 0, 0), ν ν ν v2 = ν ν ν v3 = (0, 1, 0) and ν ν ν v4 = (0, 0, 1), and edges {v 1 , v 2 }, {v 1 , v 3 }, {v 4 , v 2 } and {v 4 , v 3 }, as depicted in Figure <ref type="figure">4</ref>.</p><formula xml:id="formula_120">v 2 v 3 v 4 v 1 Figure 4: Graph G 1 .</formula><p>By definition, L (0) := 1 0 0 0 1 0 0 1 0 0 0 1 . We note that</p><formula xml:id="formula_121">(ℓ ℓ ℓ (1) MWL ) v1 = HASH (1, 0, 0), { {(0, 1, 0), (0, 1, 0)} } = (ℓ ℓ ℓ (1)</formula><p>MWL ) v4 = HASH (0, 0, 1), { {(0, 1, 0), (0, 1, 0)} } .</p><p>We next show that there exist no W (1) , B (1) such that L (1) ⊑ ℓ ℓ ℓ</p><p>(1) MWL . Indeed, since the degree of all vertices is 2 the computation is quite simple</p><formula xml:id="formula_122">L (1) := σ   diag(g)    0 1 1 0 1 0 0 1 1 0 0 1 0 1 1 0    diag(h)L (0) W (1) + B (1)    = σ       0 g(2)h(2) g(2)h(2) 0 g(2)h(2) 0 0 g(2)h(2) g(2)h(2) 0 0 g(2)h(2) 0 g(2)h(2) g(2)h(2) 0       1 0 0 0 1 0 0 1 0 0 0 1    W (1) + B (1)    = σ       0 2g(2)h(2) 0 g(2)h(2) 0 g(2)h(2) g(2)h(2) 0 g(2)h(2) 0 2g(2)h(2) 0    W (1) + B (1)    .</formula><p>Finally, we recall that B (1) consists of n copies of the same row. Hence, independently of the choice of W (1) and B (1) , vertices v 1 and v 4 will be assigned the same label, and thus L (1) ⊑ ℓ ℓ ℓ</p><p>(1)</p><p>MWL . The second class of dMPNNs we consider are those related to graph neural networks of the form:</p><formula xml:id="formula_123">L (t) := σ diag(g)(A + I)diag(h)L (t−1) W (t) + B (t) .</formula><p>This includes M dGNN i , for i = 3, 4. Indeed, consider the labelled graph (G 2 , ν ν ν) with one edge {v 1 , v 2 }, as depicted in Figure <ref type="figure">5</ref>, and vertex labelling ν ν ν v1 = (1, 0) and ν ν ν v2 = (0, 1).</p><formula xml:id="formula_124">v 1 v 2 Figure 5: Graph G 2 .</formula><p>By definition, L (0) := ( 1 0 0 1 ). We also note that</p><formula xml:id="formula_125">(ℓ ℓ ℓ (1) MWL ) v1 = HASH (1, 0), { {(0, 1)} } = (ℓ ℓ ℓ (1) MWL ) v2 = HASH (0, 1), { {(1, 0)} } .</formula><p>We next show that there exist no W (1) , B (1) such that L (1) ⊑ ℓ ℓ ℓ</p><p>(1)</p><p>MWL . Indeed,</p><formula xml:id="formula_126">F (1) := σ diag(g) 0 1 1 0 + 1 0 0 1 diag(h)L (0) W (1) + B (1) = σ g(1) 0 0 g(1) 1 1 1 1 h(1) 0 0 h(1) 1 0 0 1 W (1) + B (1)</formula><p>= σ g(1)h(1) g(1)h(1) g(1)h(1) g(1)g( <ref type="formula" target="#formula_8">1</ref>)</p><formula xml:id="formula_127">W (1) + B (1) .</formula><p>Hence, independently of the choice of W (1) and B (1) , both vertices will be assigned the same label, and thus L (1) ⊑ ℓ ℓ ℓ</p><p>(1)</p><p>MWL . Finally, we deal with the class M dGNN 5 , i.e., dMPNNs related to graph neural networks of the form</p><formula xml:id="formula_128">L (t) := σ (D −1/2 AD −1/2 + I)L (t−1) W (t) + B (t) .</formula><p>We consider the labelled graph (G 3 , ν ν ν) with vertex labelling ν ν ν v1 = ν ν ν w2 = ν ν ν w3 = (1, 0, 0), ν ν ν w1 = ν ν ν v2 = ν ν ν v3 = (0, 1, 0) and ν ν ν v4 = ν ν ν v5 = ν ν ν w4 = ν ν ν w5 = (0, 0, 1) and edges {v 1 , v 2 }, {v 1 , v 3 }, {v 1 , v 4 }, {v 1 , v 5 } and {w 1 , w 2 }, {w 1 , w 3 }, {w 1 , w 4 }, {w 1 , w 5 }, as depicted in Figure <ref type="figure">6</ref>.</p><formula xml:id="formula_129">v 1 v 2 v 3 v 4 v 5 w 1 w 2 w 3 w 4 w 5 Figure 6: Graph G 3 . By definition, L (0) :=      1 0 0 0 1 0 0 1 0 0 0 1 0 0 1 0 1 0 1 0 0 1 0 0 0 0 1 0 0 1     </formula><p>. We also note that (ℓ ℓ ℓ</p><p>(1)</p><p>MWL ) v1 = HASH (1, 0, 0), { {(0, 1, 0), (0, 1, 0), (0, 0, 1), (0, 0, 1)} } =(ℓ ℓ ℓ</p><p>(1) MWL ) w1 = HASH (0, 1, 0), { {(1, 0, 0), (1, 0, 0), (0, 0, 1), (0, 0, 1)} } .</p><p>We next show that there exist no W (1) , B (1) such that L (1) ⊑ ℓ ℓ ℓ</p><p>(1) MWL . Indeed,</p><formula xml:id="formula_130">L (1) := σ                             diag                             1 2 1 1 1 1 1 2 1 1 1 1                                          </formula><p>0 1 1 1 1 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 1 0 0 0 0</p><formula xml:id="formula_131">              diag                             1 2 1 1 1 1 1 2 1 1 1 1                             + I               L (0) W (1) + B (1)               = σ                               0 1 2 1 2</formula><p>1 2 1 2 0 0 0 0 0 1 2 0 0 0 0 0 0 0 0 0 1 2 0 0 0 0 0 0 0 0 0 1 2 0 0 0 0 0 0 0 0 0 1 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 2 1 2 1 2 1 2 0 0 0 0 0 1 2 0 0 0 0 0 0 0 0 0 1 2 0 0 0 0 0 0 0 0 0 1 2 0 0 0 0 0 0 0 0 0 1 2 0 0 0 0</p><formula xml:id="formula_132">                            </formula><p>1 0 0 0 1 0 0 1 0 0 0 1 0 0 1 0 1 0 1 0 0 1 0 0 0 0 1 0 0 1</p><formula xml:id="formula_133">              W (1)                = σ                              </formula><p>In particular, the class M dGNN 4 , corresponding to the popular graph neural networks of <ref type="bibr" target="#b8">[Kipf and Welling, 2017]</ref>, is not stronger than M WL . We also remark, based on the first counterexample in the proof, that the class of aMPNNs, corresponding to simple graph neural networks of the form L (t) := σ(AL (t−1) W (t) +B (t) ), is not stronger than M WL . We know, however, from Corollary 5.11 that the slight extension L (t) = σ (A + pI)L (t−1) W (t) − qJ results in a class of aMPNNs that is stronger than M WL . It will follow from our next result that a similar extension suffices to make the graph neural networks of <ref type="bibr" target="#b8">[Kipf and Welling, 2017]</ref> stronger than M WL .</p><p>We will now argue that the remaining M dGNN 6 class from Table <ref type="table">1</ref> is stronger than M WL , hereby concluding the proof of the fourth item in Theorem 6.1.</p><p>We can now infer that λ λ λ We define m p = max{P 1 ∪ P 2 ∪ P 3 ∪ {0}} and we claim that for all p satisfying m p &lt; p &lt; 1 the lemma holds.</p><p>By definition of P a , αj − i = p and thus i+p j = α for any α ∈ Γ and i, j ∈ {0, 1, . . . , n}. This rules out (a). Similarly, by definition of P b , i−αj α = p and thus i j+p = α for any α ∈ Γ i, j ∈ {0, 1, . . . , n}. This rules out (b). Finally, by definition of P 3 , αj−i 1−α = p and thus i+p j+p = α for any α ∈ Γ i, j ∈ {0, 1, . . . , n}. This rules out (c). We conclude, as our initial assumption cannot be valid for this m p .</p><p>From here, we can again follow the proof of Theorem 5.7 to construct a matrix X (t) such that the labelling ℓ ℓ ℓ (t) M defined by σ(diag(g)(A + pI)diag(h)L (t−1) U (t) X (t) + B (t) ) with B (t) = −J if σ is the sign function, and B (t) = −qJ if σ is the ReLU function, is such that ℓ ℓ ℓ M is row-independent modulo equality. This concludes the proof for dMPNNs arising from graph neural networks of the form (17).</p><p>We already mentioned that the proof of Proposition 6.10 works for any degree-determined g and h. In particular, the class of dMPNNs originating from graph neural networks of the form L (t) := σ D + I −1/2 (A + pI) D + I −1/2 L (t−1) W (t) − qJ ,</p><p>with p, q ∈ A, 0 ≤ p, q ≤ 1, is stronger than M WL . The introduction of the parameter p was already suggested in <ref type="bibr" target="#b8">[Kipf and Welling, 2017]</ref>. The proof of Proposition 6.2 shows that this parameter is necessary to encode the WL algorithm. Our result thus provide a theoretical justification for including this parameter.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">Conclusions</head><p>In this paper we investigate the distinguishing power of two classes of MPNNs, anonymous and degree-aware MPNNs, in order to better understand the presence of degree information in commonly used graph neural network architectures. We show that both classes of MPNNs are equivalent to the WL algorithm, in terms of their distinguishing power, when one ignores the number of computation rounds. Taking the computation rounds into consideration, however, reveals that degree information may boost the distinguishing power.</p><p>Furthermore, we identify classes of MPNNs corresponding to specific linear-algebra-based architectures of graph neural networks. We again distinguish between anonymous graph neural networks <ref type="bibr" target="#b4">[Hamilton et al., 2017</ref><ref type="bibr" target="#b11">, Morris et al., 2019]</ref> and degree-aware graph neural networks <ref type="bibr" target="#b8">[Kipf and</ref><ref type="bibr">Welling, 2017, Meltzer et al., 2019]</ref>. Here, we again make connections to the WL algorithm, identify which architectures of graph neural networks can or cannot simulate the WL algorithm, and describe how a simple modification results in graph neural networks that are as powerful as the WL algorithm.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>M</head><label></label><figDesc>the vertex labelling computed by M after t rounds. We fix the input graph in what follows, so we do not need to include the dependency on the graph in the notation of labellings. Definition 4.1. Consider two MPNNs M 1 and M 2 with the same number of rounds T . Let ℓ ℓ ℓ (t) M1 and ℓ ℓ ℓ (t)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: Summary of relationships amongst major anonymous MPNN classes considered in Section 5.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head></head><label></label><figDesc>and UPD (t) (x, y) := σ xW</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head></head><label></label><figDesc>y + b(t)  </figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head></head><label></label><figDesc>Theorem 5.1. The classes M WL , M ReLU GNN , M sign GNN and M anon are all equally strong.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head></head><label></label><figDesc>invariant (I1) trivially holds. For t ≥ 2, we define the message and update functions of M 2 as follows:MSG (t) M2 (x, y, −, −) := MSG (t−1) M1 (x ′ , y ′ , x,y) where x = (x ′ , x) and y = (y ′ , y) and by invariant (I1) x = d v and y = d u . Notice that the message function remains anonymous as d u and d v are not obtained by setting f (v) = d v and f (u) = d u but instead were computed once by the first message aggregation and encoded in the labels of v and u. The update function is defined as follows:</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>w</head><label></label><figDesc>there are unique positions i 1 and i 2 , respectively, whose corresponding entry contain p. We now consider three cases: the case if and only if i 1 = i 2 ), for some i, j ∈ {0, 1, 2, . . . , n}. To define m p , let Γ := g(dw)g(dv ) g(d v ) = g(d w ) and v, w ∈ V and considerP a := αj − i 0 ≤ αj − i &lt; 1, i, j ∈ {0, 1, 2 . . . , n}, α ∈ Γ P b := i − αj α 0 ≤ i − αj α &lt; 1, i, j ∈ {0, 1, 2, . . . , n}, α ∈ Γ P c := αj − i 1 − α 0 ≤ α(j − i) 1 − α &lt;1, i, j ∈ {0, 1, 2, . . . , n}, α ∈ Γ .</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>Summary of results comparing degree-aware MPNNs in Theorem 6.1. We note that Proposition 6.2 shows only M dGNN 4 M WL , but M dGNN 2 , M dGNN 5 , M dGNN 6 M WL can be easily inferred from it.</figDesc><table><row><cell>6.2</cell><cell>P r o p . 6 . 2</cell><cell>P r o p . 6 . 9 Prop. 6.10</cell><cell>C o r . 6 . 7 Prop. 6.2</cell></row><row><cell></cell><cell>P r o p . 6 . 3 + 1</cell><cell>P r o p . 6 . 9</cell><cell>P r o p . 6 . 2</cell></row><row><cell>Figure 2:</cell><cell></cell><cell></cell><cell></cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0">Note that we allow for labels to have different dimensions st ∈ N + per round t.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_1">In general, one could consider any function f .</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3" xml:id="foot_2">Compared to Lemma 5.9, we additionally require non-zero rows.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" xml:id="foot_3">(1) MWL .</note>
		</body>
		<back>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Proposition 6.10. The class M dGNN 6 is stronger than M WL .</p><p>Proof. We recall that dMPNNs in M dGNN 6 correspond to graph neural network architectures of the form</p><p>where diag(g) = diag(h) = (rI + (1 − r)D) −1 /2 and σ is ReLU or sign. In fact, our proof will work for any degree-determined g and h.</p><p>The argument closely follows the proof of Theorem 5.7. More specifically, we construct a dMPNN M corresponding to ( <ref type="formula">17</ref>) such that ℓ ℓ ℓ</p><p>MWL for all t ≥ 0. The induction hypothesis is that ℓ ℓ ℓ</p><p>MWL and ℓ ℓ ℓ</p><p>M is row-independent modulo equality. This hypothesis is clearly satisfied, by definition, for t = 0.</p><p>For the inductive step we assume that ℓ ℓ ℓ</p><p>MWL and ℓ ℓ ℓ (t−1) M is row-independent modulo equality. Let us define the labelling κ κ κ</p><p>) v for all vertices v. Lemma 6.11. We have that κ κ κ</p><p>MWL and κ κ κ</p><p>Proof. Suppose that there are two vertices v and w such that</p><p>This implies that (ℓ ℓ ℓ</p><p>) w . This is only possible when (ℓ ℓ ℓ</p><p>is row-independent modulo equality. In other words, κ κ κ</p><p>is not row-independent modulo equality then, due to the definition of κ κ κ (t−1) M , this implies that ℓ ℓ ℓ (t−1) M is also not row-independent modulo equality. Lemma 6.11 gives us sufficient conditions to repeat a key part of the argument in the proof of Theorem 5.7. That is, we can find a matrix U (t) such that the labelling µ µ µ (t) </p><p>MWL .</p><p>We will now prove that the labelling λ λ λ (t) defined by λ λ λ</p><p>MWL . We remark that λ λ λ (t) coincides with the labelling:</p><p>Lemma 6.12. The exists a constant m p , only dependent on g and the number n of vertices, such that λ λ λ (t) ⊑ ℓ ℓ ℓ</p><p>MWL , for every m p &lt; p &lt; 1.</p><p>Proof. We will choose m p at the end of the proof. For now suppose that λ λ λ (t) ⊑ ℓ ℓ ℓ (t) MWL . Then there exist two vertices v and w such that</p><p>The latter implies that µ µ µ</p><p>w and thus λ λ λ</p><p>We recall some facts from the proof of Theorem 5.7, and from equations ( <ref type="formula">7</ref>) and (8) in particular. An entry in µ µ µ</p><p>v is either 0 or 1, 2, . . . , n or i + p, for some i ∈ {0, 1, . . . , n}. Furthermore, at least one entry must be distinct from 0. Also, λ λ λ</p><p>w implies that the positions of the non-zero entries in µ µ µ Regarding future work, we point out that, following the work of <ref type="bibr" target="#b11">[Morris et al., 2019]</ref>, we fix the input graph in our analysis. We use this particularly when we prove that certain classes of MPNNs, based on graph neural network architectures, are stronger than the WL algorithm (Theorem 5.7 and Proposition 6.10). We prove this by constructing an MPNN that simulates the WL algorithm on a fixed input graph. As such, the constructed MPNN may not simulate the WL algorithm on another graph. It is natural to ask whether there are graph neural network-based MPNNs that can simulate the WL algorithm on all graphs.</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Local and global properties in networks of processors (extended abstract)</title>
		<author>
			<persName><forename type="first">D</forename><surname>Angluin ; Angluin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 12th Annual ACM Symposium on Theory of Computing, STOC 1980</title>
				<meeting>the 12th Annual ACM Symposium on Theory of Computing, STOC 1980</meeting>
		<imprint>
			<date type="published" when="1980">1980. 1980</date>
			<biblScope unit="page" from="82" to="93" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Graph isomorphism, color refinement, and compactness</title>
		<author>
			<persName><forename type="first">Arvind</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computational Complexity</title>
		<imprint>
			<biblScope unit="volume">26</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="627" to="685" />
			<date type="published" when="2017">2017. 2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">An optimal lower bound on the number of variables for graph identifications</title>
		<author>
			<persName><surname>Cai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Combinatorica</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="389" to="410" />
			<date type="published" when="1992">1992. 1992</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Neural message passing for quantum chemistry</title>
		<author>
			<persName><surname>Gilmer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 34th International Conference on Machine Learning</title>
				<meeting>the 34th International Conference on Machine Learning</meeting>
		<imprint>
			<date type="published" when="2017">2017. 2017. 2017</date>
			<biblScope unit="volume">70</biblScope>
			<biblScope unit="page" from="1263" to="1272" />
		</imprint>
	</monogr>
	<note>of Proceedings of Machine Learning Research</note>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Inductive representation learning on large graphs</title>
		<author>
			<persName><surname>Hamilton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems 30: Annual Conference on Neural Information Processing Systems</title>
				<imprint>
			<date type="published" when="2017">2017. 2017. 2017</date>
			<biblScope unit="page" from="1024" to="1034" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Describing graphs: A first-order approach to graph canonization</title>
		<author>
			<persName><surname>Immerman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Lander ; Immerman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Lander</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Complexity Theory Retrospective: In Honor of Juris Hartmanis on the Occasion of His Sixtieth Birthday</title>
				<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="1990">1990. 1990</date>
			<biblScope unit="page" from="59" to="81" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">edGNN: a simple and powerful GNN for directed labeled graphs</title>
		<author>
			<persName><forename type="first">F</forename><surname>Jarvis ; Jarvis</surname></persName>
		</author>
		<author>
			<persName><surname>Jaume</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Conference on Learning Representations&apos; Workshop on Representation Learning on Graphs and Manifolds</title>
		<title level="s">Springer Undergraduate Mathematics Series</title>
		<meeting>the International Conference on Learning Representations&apos; Workshop on Representation Learning on Graphs and Manifolds</meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2014">2014. 2014. 2019. 2019</date>
		</imprint>
	</monogr>
	<note>Algebraic number theory. RLGM 2019</note>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Graphs identified by logics with counting</title>
		<author>
			<persName><surname>Kiefer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 40th International Symposium on the Mathematical Foundations of Computer Science, MFCS 2015</title>
				<meeting>the 40th International Symposium on the Mathematical Foundations of Computer Science, MFCS 2015</meeting>
		<imprint>
			<date type="published" when="2015">2015. 2015</date>
			<biblScope unit="page" from="319" to="330" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Semi-supervised classification with graph convolutional networks</title>
		<author>
			<persName><surname>Kipf</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">N</forename><surname>Welling ; Kipf</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Welling</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 5th International Conference on Learning Representations</title>
				<meeting>the 5th International Conference on Learning Representations</meeting>
		<imprint>
			<date type="published" when="2017">2017. 2017. 2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">What graph neural networks cannot learn: depth vs width</title>
		<author>
			<persName><forename type="first">A</forename><surname>Loukas ; Loukas</surname></persName>
		</author>
		<idno>ICLR 2019</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 7th International Conference on Learning Representations</title>
				<meeting>the 7th International Conference on Learning Representations</meeting>
		<imprint>
			<date type="published" when="2019">2019. 2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">Pinet: A permutation invariant graph neural network for graph classification</title>
		<author>
			<persName><surname>Meltzer</surname></persName>
		</author>
		<idno>CoRR, abs/1905.03046</idno>
		<imprint>
			<date type="published" when="2019">2019. 2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Weisfeiler and Leman go neural: Higher-order graph neural networks</title>
		<author>
			<persName><surname>Morris</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of The 33rd AAAI Conference on Artificial Intelligence, AAAI 2019</title>
				<meeting>The 33rd AAAI Conference on Artificial Intelligence, AAAI 2019</meeting>
		<imprint>
			<date type="published" when="2019">2019. 2019</date>
			<biblScope unit="page" from="4602" to="4609" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Positivity problems for low-order linear recurrence sequences</title>
		<author>
			<persName><surname>Ouaknine</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Worrell ; Ouaknine</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Worrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 25th Annual ACM-SIAM Symposium on Discrete Algorithms, SODA 2014</title>
				<meeting>the 25th Annual ACM-SIAM Symposium on Discrete Algorithms, SODA 2014</meeting>
		<imprint>
			<date type="published" when="2014">2014. 2014</date>
			<biblScope unit="page" from="366" to="379" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">A survey on the expressive power of graph neural networks</title>
		<author>
			<persName><forename type="first">R</forename><surname>Sato ; Sato</surname></persName>
		</author>
		<author>
			<persName><surname>Wu</surname></persName>
		</author>
		<idno>ArXiv, abs/2003.04078</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 36th International Conference on Machine Learning, ICML 2019</title>
				<meeting>the 36th International Conference on Machine Learning, ICML 2019</meeting>
		<imprint>
			<date type="published" when="2019">2020. 2020. 2019a. 2019a</date>
			<biblScope unit="page" from="6861" to="6871" />
		</imprint>
	</monogr>
	<note>Simplifying graph convolutional networks</note>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">A comprehensive survey on graph neural networks</title>
		<author>
			<persName><surname>Wu</surname></persName>
		</author>
		<idno>CoRR, abs/1901.00596</idno>
		<imprint>
			<date type="published" when="2019">2019b. 2019b</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">How powerful are graph neural networks?</title>
		<author>
			<persName><surname>Xu</surname></persName>
		</author>
		<idno>ICLR 2019</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 7th International Conference on Learning Representations</title>
				<meeting>the 7th International Conference on Learning Representations</meeting>
		<imprint>
			<date type="published" when="2019">2019. 2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Deep sets</title>
		<author>
			<persName><surname>Zaheer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems 30: Annual Conference on Neural Information Processing</title>
				<imprint>
			<date type="published" when="2017">2017. 2017. 2017</date>
			<biblScope unit="page" from="3391" to="3401" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<author>
			<persName><surname>Zhou</surname></persName>
		</author>
		<idno>CoRR, abs/1812.08434</idno>
		<title level="m">Graph neural networks: A review of methods and applications</title>
				<imprint>
			<date type="published" when="2018">2018. 2018</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
