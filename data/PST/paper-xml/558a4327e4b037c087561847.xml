<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Cache-Aware Scratchpad Allocation Algorithm</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Manish</forename><surname>Verma</surname></persName>
							<email>manish.verma@uni-dortmund.de</email>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science</orgName>
								<orgName type="institution">XII University of Dortmund</orgName>
								<address>
									<postCode>44225</postCode>
									<settlement>Dortmund</settlement>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Lars</forename><surname>Wehmeyer</surname></persName>
							<email>lars.wehmeyer@uni-dortmund.de</email>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science</orgName>
								<orgName type="institution">XII University of Dortmund</orgName>
								<address>
									<postCode>44225</postCode>
									<settlement>Dortmund</settlement>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Peter</forename><surname>Marwedel</surname></persName>
							<email>peter.marwedel@uni-dortmund.de</email>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science</orgName>
								<orgName type="institution">XII University of Dortmund</orgName>
								<address>
									<postCode>44225</postCode>
									<settlement>Dortmund</settlement>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Cache-Aware Scratchpad Allocation Algorithm</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">BE005A076F5FBFD8F20CD52406A32E36</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.3" ident="GROBID" when="2023-07-27T05:46+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>In the context of portable embedded systems, reducing energy is one of the prime objectives. Most high-end embedded microprocessors include onchip instruction and data caches, along with a small energy efficient scratchpad. Previous approaches for utilizing scratchpad did not consider caches and hence fail for the au courant architecture. In the presented work, we use the scratchpad for storing instructions and propose a generic Cache Aware Scratchpad Allocation (CASA) algorithm. We report an average reduction of 8-29% in instruction memory energy consumption compared to a previously published technique for benchmarks from the Mediabench suite.</p><p>The scratchpad in the presented architecture is similar to a preloaded loop cache. Comparing the energy consumption of our approach against preloaded loop caches, we report average energy savings of 20-44%.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Contemporary Embedded Systems have to satisfy stringent constraints concerning power, performance and cost. For mobile embedded devices, reduced energy consumption translates to either increased battery life or reduced dimensions, weight and cost of the device or both. Consequently, the overall competitiveness of the product is improved.</p><p>Several researchers <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b10">11]</ref> have identified the memory subsystem as the energy bottleneck of the entire system. Onchip instruction and data caches were introduced to improve the performance of computer systems by exploiting the available locality in the program. Caches allow easy integration and improve both performance and energy of programs without the need for program analysis and optimization. However, caches are not the most energy efficient option available for embedded systems. Scratchpad memories were proposed as an energy efficient alternative to caches. They also require less onchip area and allow tighter bounds on WCET prediction of the system. However unlike caches, scratchpads require explicit support from the compiler. To strike a balance between these two contrasting approaches, most of the high-end embedded microprocessors (e.g. ARM10E <ref type="bibr" target="#b0">[1]</ref>, ColdFire MCF5 <ref type="bibr" target="#b8">[9]</ref>) include both onchip caches and a scratchpad.</p><p>We assume the memory hierarchy as shown in figure 1.(a) and utilize the scratchpad for storing instructions. The decision to store only instructions is motivated by the fact that the instruction memory is accessed on every instruction fetch and the size of programs for mobile embedded devices is smaller compared to their data size. This implies that smaller scratchpad memories allocated with instructions can achieve greater energy savings than with data. In this paper, we model the cache behavior as a conflict graph and allocate objects onto the scratchpad considering their effect on the I-cache. As shown later, the problem of finding the best set of objects to be allocated on the scratchpad can be formulated as a variant of the Maximum Independent Set problem. The problem is solved using an ILP approach. We compare our approach against a published technique <ref type="bibr" target="#b12">[13]</ref> for the aforementioned architecture. Due to the presence of an I-cache in our architecture, the latter fails to produce optimal results and may even lead to the problem of cache thrashing.</p><p>We also compare the energy savings due to our approach for scratchpad to that achieved by preloaded loop caches <ref type="bibr" target="#b11">[12]</ref>, as the utilization of the scratchpad in the current setup (see figure <ref type="figure" target="#fig_0">1</ref>) is similar to a loop cache. Preloaded loop caches are architecturally more complex than scratchpads, but are less flexible as they can be preloaded with only a limited number of loops. We demonstrate that with the aid of a sophisticated allocation algorithm, scratchpad memories can outperform their complex counterparts.</p><p>In the next section, we describe related work and detail the shortcomings of previous approaches. Section 3 describes the information regarding memory objects, cache behavior and the energy model. The proposed algorithm is presented in detail in section 4, followed by the description of the experimental setup. In section 6 we present the results for an ARM7T based system and end the paper with a conclusion and future work. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Work</head><p>Analytical energy models for memories <ref type="bibr" target="#b6">[7]</ref> have been found to be fairly accurate. We use cacti <ref type="bibr" target="#b14">[15]</ref> to determine the energy per access for caches and preloaded loop caches. The energy per access for scratchpad memories was determined using the model presented in <ref type="bibr" target="#b2">[3]</ref>.</p><p>Application code placement techniques <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b13">14]</ref> were developed to improve the CPI (cycles per instruction) by reducing the number of I-cache misses. During the first step, traces were generated by combining the frequently executed basic blocks, followed by the trace placement step. Authors in <ref type="bibr" target="#b9">[10]</ref> placed traces within the function boundary, while <ref type="bibr" target="#b13">[14]</ref> placed them across function boundaries, to reduce the I-cache misses.</p><p>Several researchers <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b10">11]</ref> have utilized scratchpad memories for assigning global/local variables, whereas only Steinke et al. <ref type="bibr" target="#b12">[13]</ref> considered both program and data parts (memory objects) to be allocated onto the scratchpad. They assumed a memory hierarchy composed of only scratchpad and main memory. Profit values were assigned to program and data parts according to their execution and access counts, respectively. They then formulated a knapsack problem to determine the best set of memory objects to be allocated on the scratchpad.</p><p>Although the aforementioned approach is sufficiently accurate for their memory hierarchy, it is fairly imprecise for the current setup. The first imprecision of the approach stems from the assumption that execution (access) counts are sufficient to represent energy consumption by a memory object. This assumption fails in the presence of a cache, where execution (access) counts can be decomposed into cache hits and misses. The energy consumption of a cache miss is significantly larger than that of a cache hit. Consequently, two memory objects can have the same execution (access) counts, yet have substantially different cache misses and hence the energy consumption. The above discussion stresses the need for a fine-grained energy model. The second imprecision is due to the fact that conflict relationship between memory objects is not modeled and hence they are moved instead of copying from main memory to the scratchpad. As a result, the layout of the entire program is changed, which may cause non-conflicting memory ob-jects to conflict with each other and lead to erratic results.</p><p>Various instruction buffers have been proposed to improve the energy consumption of the system. Ross et al. <ref type="bibr" target="#b11">[12]</ref> proposed a Preloaded Loop Cache which can be statically loaded with pre-identified memory objects. Start and end addresses of the memory objects are stored in the controller, which on every instruction fetch determines whether to access the loop cache or the L1 I-cache. Consequently, the loop cache can be preloaded with complex loops as well as functions. However, to keep the energy consumption of the controller low, only a small number of memory objects (typically 2-6) can be preloaded.</p><p>The first disadvantage of the aforementioned approach is due to the architectural feature of the loop cache that allows only a fixed number of memory objects to be preloaded. The problem will get prominent for large programs with several hot spots. The second disadvantage is similar to the one explained above for <ref type="bibr" target="#b12">[13]</ref>, memory objects are greedily selected on the basis of their execution time density (execution time per unit size). In the wake of the discussion we enumerate the following contributions of this paper.</p><p>• It for the first time studies the effect of a scratchpad and an I-cache on the system's energy consumption. • It stresses the need for a sophisticated allocation algorithm by demonstrating the inefficiency of previous algorithms when applied to the present architecture. • It presents a novel scratchpad allocation algorithm which can be easily applied to any memory hierarchy. • It demonstrates that scratchpad together with an allocation algorithm can replace loop caches.</p><p>In the following section, we describe preliminary information required for our algorithm.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Preliminaries</head><p>We start with describing the assumed architecture for the current research work, followed by the description of the memory objects. The interaction of memory objects within the cache is represented using a conflict graph, which forms the basis of the proposed energy model and the algorithm.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Architecture</head><p>For the presented research work we assume a Harvard architecture (see figure <ref type="figure" target="#fig_0">1(a)</ref>) with the scratchpad present at the same horizontal level as the L1 I-cache. The scratchpad is mapped to a region in the processor's address space and acts as an alternative location for fetching instructions. As shown in figure 1(b), the preloaded loop cache setup is similar to using a scratchpad.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Memory Objects</head><p>In the first step of our approach we generate traces and then distribute these traces between offchip main memory and non-cacheable scratchpad memory. A trace is a frequently executed straight-line path, consisting of basic blocks connected by fall-through edges. Our traces are similar to the traces in <ref type="bibr" target="#b13">[14]</ref> except they are smaller than the scratchpad size, as larger traces can not be placed on to the scratchpad as whole. The traces are appended with NOP instructions to align them to cache line boundaries. Consequently, a trace will start and end at a cache line, ensuring a one-to-one relationship between cache misses and corresponding traces. The rational behind using traces is threefold. Firstly, traces improve the performance of both the cache and the processor by enhancing the spatial locality in the program code. Secondly, due to the fact that traces always end with an unconditional jump <ref type="bibr" target="#b13">[14]</ref>, they form an atomic unit of instructions which can be placed anywhere in memory without modifying other traces. Finally, traces are accountable for every cache miss caused by them. In the rest of the paper, unless specified, traces will be referred to as memory objects (MO). In the following subsection, we represent the cache behavior at the granularity of memory objects by a conflict graph.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Cache behavior (conflict graph)</head><p>The cache maps an instruction to a cache line according to the following function:</p><formula xml:id="formula_0">Map(addr) = addr mod CacheSize Associativity * WordsPerLine</formula><p>Similarly, a memory object is mapped to cache line(s) depending upon its start address and size. Two memory objects potentially conflict if they are mapped to at least one common cache line. Conflict cache misses can only be caused by conflicting memory objects and can be represented by a conflict graph. The Conflict Graph G (see figure <ref type="figure" target="#fig_1">2</ref>) is defined as follows:</p><p>Definition: The Conflict Graph G = (X, E) is a directed weighted graph with node set X = {x 1 ,...,x n }. Each vertex x i in G corresponds to a memory object (MO) in the application code. The edge set E contains an edge e i j from node x i to x j if a cache-line belonging to x j is replaced by a cache-line belonging to x i using the cache replacement policy. In other words, e i j ∈ E if there occurs a cache miss of x i due to x j . The weight m i j of the edge e i j is the number of cache misses of x i that occur due to x j . The weight f i of a vertex x i is the total number of instruction fetches within x i .</p><p>Conflict graph as shown in figure . 2 is a directed graph because the conflict relationship determined by any of the cache replacement policies is antisymmetric. The vertices and the edges are marked with the corresponding weights determined by profiling the application. The conflict graph G and the energy values are utilized to compute the energy consumption of a memory object according to the energy model proposed in the following subsection.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4.">Energy Model</head><p>The energy E(x i ) consumed by an MO x i is expressed as:</p><formula xml:id="formula_1">E(x i ) = E SP (x i ) if x i is present on scratchpad E Cache (x i ) otherwise<label>(1)</label></formula><p>where E Cache can be computed as follows:</p><formula xml:id="formula_2">E Cache (x i ) = Hit(x i ) * E Cache hit (2) + Miss(x i ) * E Cache miss</formula><p>where functions Hit(x i ) and Miss(x i ) return the number of hits and misses, respectively, while fetching the instructions of MO x i . E Cache hit is the energy of a hit and E Cache miss is the energy of a miss in the I-cache.</p><formula xml:id="formula_3">Miss(x i ) = ∑ x j ∈N i Miss(x i , x j ) with<label>(3)</label></formula><formula xml:id="formula_4">N i = x j : e i j ∈ E</formula><p>where Miss(x i , x j ) denotes the number of conflict cache misses of MO x i caused due to conflicts with MO x j . The sum of the number of hits and misses is equal to the number of instruction fetches f i in an MO x i :</p><formula xml:id="formula_5">f i = Hit(x i ) + Miss(x i )<label>(4)</label></formula><p>For a given input data set, the number of instruction fetches f i within an MO x i is a constant and is independent of the memory hierarchy. Substituting the terms Miss(x i ) from equation (3) and Hit(x i ) from equation (4) in equation (2) and rearranging derives the following equation:</p><formula xml:id="formula_6">E Cache (x i ) = f i * E Cache hit +<label>(5)</label></formula><p>∑</p><formula xml:id="formula_7">x j ∈N i Miss(x i , x j ) * (E Cache miss -E Cache hit )</formula><p>Observing the above equation, we find that the first term is a constant while the second term is variable which depends on the overall program code layout and the memory hierarchy. We would like to point out that the approach <ref type="bibr" target="#b11">[12]</ref> only considered the constant term in its energy model. Consequently, could not optimize the overall memory energy consumption.</p><p>Since there are no misses when an MO x i is present in the scratchpad, we can deduce the following energy equation: <ref type="bibr" target="#b5">(6)</ref> where E SP hit is the energy per access of the scratchpad.</p><formula xml:id="formula_8">E SP (x i ) = f i * E SP hit</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Algorithm</head><p>Once we have created the conflict graph G annotated with vertex and edge weights, energy consumption of the memory objects can be computed. Now, the problem is to select a subset of memory objects which minimize the number of conflict edges and the overall energy consumption of the system. The subset is bounded in size by the scratchpad size. In the simplest form, when every node and edge has a unit weight, the problem is reduced to finding a Maximum Independent Set <ref type="bibr" target="#b5">[6]</ref>. Unfortunately, even in the simplest form the problem is NP-complete <ref type="bibr" target="#b5">[6]</ref>. We will present an Integer Linear Programming ILP based solution, as it can be easily extended to handle complex memory hierarchies and requires an acceptable computation time using a commercial ILP solver. Moreover, the problem can be elegantly represented using a set of inequations.</p><p>In order to explain the algorithm we need to define a number of variables. The binary variable l(x i ) denotes the location of the memory object in the memory hierarchy:</p><formula xml:id="formula_9">l(x i ) = 0, if x i is present on scratchpad 1, otherwise<label>(7)</label></formula><p>Miss(x i , x j ) is the number misses of MO x i caused due to conflict with MO x j . Since a memory object present in the scratchpad does not conflict with other memory objects, we can represent Miss(x i , x j ) as follows:</p><formula xml:id="formula_10">Miss(x i , x j ) = 0, if x j is present on scratchpad m i j , otherwise<label>(8)</label></formula><p>where m i j is the weight of the edge e i j connecting vertex x i to x j . Function Miss(x i , x j ) can be reformulated using the location variable l(x j ) and represented as:</p><formula xml:id="formula_11">Miss(x i , x j ) = l(x j ) * m i j (9)</formula><p>Similarly, the location variable l(x i ) can be used to reformulate the energy equation (1) denoting the energy consumed by the memory object.</p><formula xml:id="formula_12">E(x i ) = [1 -l(x i )] * E SP (x i ) + l(x i ) * E Cache (x i )<label>(10)</label></formula><p>We substitute the energy equations for E Cache and E SP from equations ( <ref type="formula" target="#formula_6">5</ref>) and ( <ref type="formula">6</ref>), respectively, into the above equation.</p><p>By rearranging the terms we transform the equation <ref type="bibr" target="#b9">(10)</ref> into the following form.</p><formula xml:id="formula_13">E(x i )= f i * E SP hit + (11) f i * [E Cache hit -E SP hit ] * l(x i ) + [E Cache miss -E Cache hit ] * [ ∑ j∈N i l(x j ) * l(x i ) * m i j ]</formula><p>Observing the above equations, we find the last term is a quadratic degree term. This can be justified by the fact that the number of misses of a memory object x i not only depends upon its location but also upon the location of the conflicting memory objects x j . Prior to formulating an ILP problem, we need to linearize the above equation. This can be achieved by replacing the expression l(x i ) * l(x j ) by an additional variable L(x i , x j ) in the following equation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E(x</head><formula xml:id="formula_14">i ) = f i * E SP hit + (12) f i * [E Cache hit -E SP hit ] * l(x i ) + [E Cache miss -E Cache hit ] * [ ∑ j∈N i L(x i , x j ) * m i j ]</formula><p>In order to prevent the linearizing variable L(x i , x j ) from taking arbitrary values, the following linearization constraints are added to the set of constraints.</p><formula xml:id="formula_15">l(x i ) -L(x i , x j ) ≥ 0 (13) l(x j ) -L(x i , x j ) ≥ 0 (14) l(x i ) + l(x j ) -2 * L(x i , x j ) ≤ 1 (<label>15</label></formula><formula xml:id="formula_16">)</formula><p>The best set of memory objects which fits into the scratchpad and minimizes the total energy consumption now has to be identified. The objective function E Total denotes the total energy consumed by the system.</p><formula xml:id="formula_17">E Total = ∑ x i ∈X E(x i )<label>(16)</label></formula><p>The scratchpad size constraint can be modeled as follows:</p><formula xml:id="formula_18">∑ x i ∈X [1 -l(x i )] * S(x i ) ≤ scratchpadsize<label>(17)</label></formula><p>The size S(x i ) of memory object x i is computed without considering the appended NOP instructions. These NOP instructions are stripped away from the memory objects prior to allocating them to the scratchpad. A commercial ILP Solver <ref type="bibr" target="#b4">[5]</ref> is used to obtain an optimal subset of memory objects which minimizes the objective function. The number of vertices |V | of the conflict graph G is equal to the number of memory objects, which is bounded by the number of basic blocks in the program code. The number of linearizing variables is equal to the number of edges |E| in the conflict graph G. Hence, the number of variables in the ILP problem is equal to |V | + |E| and is bounded by O(|V | 2 ). Nevertheless, the maximum runtime of the ILP solver for our set of real-life benchmarks (upto 19.5kBytes program size) was found to be less than a second.</p><p>Our ILP formulation can be easily extended to handle complex memory hierarchies. For example, if we had more than one scratchpad at the same horizontal level in the memory hierarchy, then we only need to repeat inequation (17) for every scratchpad. An additional constraint ensuring that a memory object is assigned to at most one scratchpad is also required. If we had I-caches at different levels (e.g. L1, L2) in the memory hierarchy, we need not do anything, as the algorithm tries to minimize the L1 I-cache misses. The L2 I-cache misses, being a subset of the L1 I-cache misses, are thus also minimized. Consequently, the energy consumption of the whole memory hierarchy is minimized. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Experimental Setup</head><p>The experimental setup consists of an ARM7T processor core, an onchip cache, an onchip scratchpad and an offchip main memory. We want to compare the effect of allocation techniques for scratchpad on the energy consumption of the instruction memory subsystem. The cacti cache model was used to calculate the energy consumption per access for onchip 0.5µm technology cache, loop cache and scratchpad memories. The loop cache was assumed to contain a maximum of 4 loops. The energy consumption of the main memory was measured from the evaluation board.</p><p>Experiments were conducted according to the workflow presented in figure <ref type="figure" target="#fig_2">3</ref>. In the first step, the benchmarks programs are compiled using an energy optimizing C compiler. Trace generation <ref type="bibr" target="#b13">[14]</ref> is a well known I-cache performance optimization technique. Hence, for a fair comparison, traces are generated for both the allocation techniques. either CASA or the scratchpad allocation algorithm <ref type="bibr" target="#b12">[13]</ref> allocates memory objects to the scratchpad memory. The generated machine code is then fed into ARMulator <ref type="bibr" target="#b0">[1]</ref> to obtain the instruction trace. Our custom memory hierarchy simulator <ref type="bibr" target="#b7">[8]</ref> based upon the instruction trace, memory hierarchy and the energy cost model, computes the aggregate energy consumed by the memory subsystem.</p><p>For the loop cache configuration, the workflow is similar to scratchpad workflow. The loop cache allocation algorithm <ref type="bibr" target="#b11">[12]</ref> is utilized for assigning loops and functions to the loop cache. The energy consumed by the memory subsystem is computed similarly, using the appropriate memory hierarchy. The runtime overhead of CASA compared against the other two approaches was negligible. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Results</head><p>A subset of benchmarks from the Mediabench suite were used to substantiate our claims on energy savings by the proposed algorithm. The size of the scratchpad/loop cache was varied while the rest of instruction memory subsystem was kept invariant and the number of hits and misses to the various levels of memory hierarchy were counted. Based upon the gathered information and the energy model (subsection 3.4), energy consumption was computed. Figure <ref type="figure">4</ref> displays the parameters of CASA algorithm for the MPEG benchmark against the published algorithm by Steinke et al. <ref type="bibr" target="#b12">[13]</ref>. Both the algorithms assign memory objects to the scratchpad incorporated in the presented memory hierarchy. A direct mapped 2kB I-cache was chosen for these experiments. All results are shown as the percentage of the algorithm's <ref type="bibr" target="#b12">[13]</ref> parameters, denoted as 100%. The first fact to observe is that the number of I-cache accesses are higher, while the number of scratchpad accesses are lower than for Steinke's algorithm. This might beguile the reader that CASA might cause an increase in energy consumption. However, this is incorrect since Steinke's algorithm tries to reduce energy consumption by increasing the number of accesses to the energy efficient scratchpad. In contrast, CASA Next we compare (see figure <ref type="figure">5</ref>) scratchpad allocated with CASA against loop cache preloaded with Ross's <ref type="bibr" target="#b11">[12]</ref> algorithm. Loop cache results are denoted as 100% in figure <ref type="figure">5</ref>. For small sizes (128 and 256 bytes), the number of accesses to loop cache are higher than those to scratchpad. However, as we increase the size, loop cache's performance is restricted by the maximum number of preloadable memory objects. On the other hand, the scratchpad can be preloaded with any number of memory objects. Consequently, we observe a higher percentage of scratchpad accesses. Also using CASA, the number of I-cache misses for scratchpad are substantially lower than those for loop cache. Consequently, scratchpad is able to reduce energy consumption at an average of 26% against loop cache.</p><p>Finally, table <ref type="table" target="#tab_0">1</ref> summarizes the energy consumption using CASA for scratchpad. Instruction cache of size 2kB, 1kB and 128 Bytes was assumed for the mpeg, g721 and adpcm benchmarks, respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.">Conclusion and Future Work</head><p>In this paper we presented a generic cache-behavior based scratchpad allocation technique. The technique reduced the energy consumption of the system against a published algorithm. We also demonstrated that the simple scratchpad memory allocated with the presented technique is better than a loop cache. The overall average energy savings of scratchpad allocated with our approach against scratchpad and loop cache allocated with their respective allocation algorithms are 21.1% and 28.6% respectively. We intend to extend the approach by considering preloading of data and dynamic copying (overlay) of memory objects on the scratchpad.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 .</head><label>1</label><figDesc>Figure 1. System architecture: (a) scratchpad (b) loop cache</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 .</head><label>2</label><figDesc>Figure 2. Conflict graph</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 .</head><label>3</label><figDesc>Figure 3. Experimental workflow</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 4 .Figure 5 .</head><label>45</label><figDesc>Figure 4. Comparison of CASA against Steinke's algorithm for MPEG benchmark.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 .</head><label>1</label><figDesc>Overall energy savingstries to reduce I-cache misses by assigning conflicting memory objects to the scratchpad. Since I-cache misses are the major source of energy consumption, CASA is able to conserve up to 60% energy against Steinke's algorithm.</figDesc><table><row><cell>Benchmark</cell><cell>Mem Size</cell><cell cols="3">Energy Consumption (µJ)</cell><cell cols="2">Improvement(%)</cell></row><row><cell>(size)</cell><cell>(Bytes)</cell><cell cols="5">SP (CASA) SP (Steinke) LC (Ross) CASA vs. Steinke SP (CASA) vs. LC</cell></row><row><cell>adpcm</cell><cell>64</cell><cell>3398.37</cell><cell>3261.04</cell><cell>3779.80</cell><cell>-4.2</cell><cell>10.1</cell></row><row><cell>(1 kByte )</cell><cell>128</cell><cell>1694.71</cell><cell>2052.12</cell><cell>2702.20</cell><cell>17.4</cell><cell>37.3</cell></row><row><cell></cell><cell>256</cell><cell>224.55</cell><cell>856.83</cell><cell>1480.59</cell><cell>73.8</cell><cell>84.8</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>29.0</cell><cell>44.1</cell></row><row><cell>g721</cell><cell>128</cell><cell>7493.75</cell><cell>8011.68</cell><cell>8343.61</cell><cell>4.0</cell><cell>10.2</cell></row><row><cell>(4.7 kBytes)</cell><cell>256</cell><cell>6640.65</cell><cell>6510.00</cell><cell>6734.41</cell><cell>-2.0</cell><cell>1.4</cell></row><row><cell></cell><cell>512</cell><cell>4941.53</cell><cell>4951.91</cell><cell>5616.16</cell><cell>0.2</cell><cell>12.0</cell></row><row><cell></cell><cell>1024</cell><cell>2106.53</cell><cell>3033.11</cell><cell>4707.76</cell><cell>30.6</cell><cell>55.2</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>8.2</cell><cell>19.7</cell></row><row><cell>mpeg</cell><cell>128</cell><cell>7554.88</cell><cell>10364.46</cell><cell>10918.01</cell><cell>27.1</cell><cell>30.8</cell></row><row><cell>(19.5 kBytes)</cell><cell>256</cell><cell>7521.28</cell><cell>9744.85</cell><cell>8624.61</cell><cell>22.8</cell><cell>12.8</cell></row><row><cell></cell><cell>512</cell><cell>3904.27</cell><cell>9502.60</cell><cell>5189.06</cell><cell>58.9</cell><cell>24.8</cell></row><row><cell></cell><cell>1024</cell><cell>3400.70</cell><cell>3518.72</cell><cell>5261.94</cell><cell>3.4</cell><cell>35.4</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>28.0</cell><cell>26.0</cell></row></table></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">Advanced RISC Machines Ltd</title>
		<author>
			<persName><surname>Arm</surname></persName>
		</author>
		<ptr target="http://www.arm.com/armtech/ARM10Thumb" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">An Optimal Memory Allocation Scheme for Scratch-Pad-Based Embedded Systems</title>
		<author>
			<persName><forename type="first">O</forename><surname>Avissar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Barua</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Stewart</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Embedded Computing Systems</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="6" to="26" />
			<date type="published" when="2002-11">Nov. 2002</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Scratchpad Memory: A Design Alternative for Cache On-chip Memory in Embedded Systems</title>
		<author>
			<persName><forename type="first">R</forename><surname>Banakar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Steinke</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B.-S</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of the 10th International Symposium on Hardware/Software Codesign</title>
		<meeting>of the 10th International Symposium on Hardware/Software Codesign<address><addrLine>Estes Park, CO</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2002-05">May. 2002</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Architectural and Compiler Support for Energy Reduction in the Memory Hierarchy of High Performance Microprocessors</title>
		<author>
			<persName><forename type="first">N</forename><surname>Bellas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Hajj</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Polychronopoulos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of the ISLPED</title>
		<meeting>of the ISLPED<address><addrLine>Monterey, CA, USA</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="1998-08">Aug. 1998</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title/>
		<author>
			<orgName type="collaboration">CPLEX. CPLEX limited www.cplex.com</orgName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">Computers and Intractability: A Guide To the Theory of NP-Completeness</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">R</forename><surname>Garey</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">S</forename><surname>Johnson</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1979">1979</date>
			<publisher>W. H. Freeman and Company</publisher>
			<pubPlace>New York</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Analytical Energy Dissipation Models for Low Power Caches</title>
		<author>
			<persName><forename type="first">M</forename><surname>Kamble</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Ghosh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of the ISLPED</title>
		<meeting>of the ISLPED<address><addrLine>Monterey, CA, USA</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="1997-08">Aug. 1997</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title/>
		<author>
			<persName><forename type="first">Memsim</forename><surname>Dept</surname></persName>
		</author>
		<ptr target="http://ls12.cs.uni-dortmund.de/research/memsim/" />
		<imprint/>
		<respStmt>
			<orgName>of Computer Science XII, Univ. of Dortmund</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">Motorola ColdFire MCF5XXX processor family</title>
		<author>
			<persName><surname>Motorola</surname></persName>
		</author>
		<ptr target="http://e-www.motorola.com/" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Profile Guided Code Positioning</title>
		<author>
			<persName><forename type="first">P</forename><surname>Pettis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Hansen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of the ACM SIGPLAN&apos;90 Conference on Programming Language Design and Implementation</title>
		<meeting>of the ACM SIGPLAN&apos;90 Conference on Programming Language Design and Implementation</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="1990-06">Jun. 1990</date>
			<biblScope unit="page" from="16" to="27" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">Memory Issues In Embedded Systems-on-chip</title>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">R</forename><surname>Panda</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><forename type="middle">D</forename><surname>Dutt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Nicolau</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1999">1999</date>
			<publisher>Kluwer Academic Publishers</publisher>
			<pubPlace>Norwell, MA</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Exploiting Fixed Programs in Embedded Systems: A Loop Cache Example</title>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">C A</forename><surname>Gordon-Ross</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Vahid</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In IEEE Computer Architecture Letters</title>
		<imprint>
			<date type="published" when="2002-01">Jan. 2002</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Assigning Program and Data Objects to Scratchpad for Energy Reduction</title>
		<author>
			<persName><forename type="first">S</forename><surname>Steinke</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Wehmeyer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">S</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of the DATE Conference</title>
		<meeting>of the DATE Conference<address><addrLine>Paris, France</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2002-03">Mar. 2002</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Optimal Code Placement of Embedded software for Instruction Caches</title>
		<author>
			<persName><forename type="first">H</forename><surname>Tomiyama</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Yasuura</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of the 9th European Design and Test Conference ET&amp;TC&apos;96</title>
		<meeting>of the 9th European Design and Test Conference ET&amp;TC&apos;96<address><addrLine>Paris, France</addrLine></address></meeting>
		<imprint>
			<date type="published" when="1996-03">Mar. 1996</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">CACTI: An Enhanced Cache Access and Cycle Time Model</title>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">J E</forename><surname>Wilton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><forename type="middle">P</forename><surname>Jouppi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Journal of Solid-State Circuits</title>
		<imprint>
			<biblScope unit="volume">31</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="677" to="688" />
			<date type="published" when="1996-05">May 1996</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
