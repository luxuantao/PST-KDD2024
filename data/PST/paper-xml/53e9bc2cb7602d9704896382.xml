<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">An Augmented Lagrangian Method for Total Variation Video Restoration</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author role="corresp">
							<persName><roleName>Student Member, IEEE</roleName><forename type="first">Stanley</forename><forename type="middle">H</forename><surname>Chan</surname></persName>
							<email>h5chan@ucsd.edu</email>
							<affiliation key="aff0">
								<orgName type="department">Department of Electrical and Computer Engineering</orgName>
								<orgName type="institution">University of California</orgName>
								<address>
									<addrLine>San Diego, La Jolla</addrLine>
									<postCode>92093-0112</postCode>
									<region>CA</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><roleName>Student Member, IEEE</roleName><forename type="first">Ramsin</forename><surname>Khoshabeh</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Electrical and Computer Engineering</orgName>
								<orgName type="institution">University of California</orgName>
								<address>
									<addrLine>San Diego, La Jolla</addrLine>
									<postCode>92093-0112</postCode>
									<region>CA</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><roleName>Student Member, IEEE</roleName><forename type="first">Kristofor</forename><forename type="middle">B</forename><surname>Gibson</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Electrical and Computer Engineering</orgName>
								<orgName type="institution">University of California</orgName>
								<address>
									<addrLine>San Diego, La Jolla</addrLine>
									<postCode>92093-0112</postCode>
									<region>CA</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Philip</forename><forename type="middle">E</forename><surname>Gill</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Electrical and Computer Engineering</orgName>
								<orgName type="institution">University of California</orgName>
								<address>
									<addrLine>San Diego, La Jolla</addrLine>
									<postCode>92093-0112</postCode>
									<region>CA</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><roleName>Fellow, IEEE</roleName><forename type="first">Truong</forename><forename type="middle">Q</forename><surname>Nguyen</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Electrical and Computer Engineering</orgName>
								<orgName type="institution">University of California</orgName>
								<address>
									<addrLine>San Diego, La Jolla</addrLine>
									<postCode>92093-0112</postCode>
									<region>CA</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Sina</forename><forename type="middle">S H</forename><surname>Farsiu</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">Department of Mathematics</orgName>
								<orgName type="institution">University of California</orgName>
								<address>
									<addrLine>San Diego, La Jolla</addrLine>
									<postCode>92093-0112</postCode>
									<region>CA</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">R</forename><surname>Chan</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">Department of Mathematics</orgName>
								<orgName type="institution">University of California</orgName>
								<address>
									<addrLine>San Diego, La Jolla</addrLine>
									<postCode>92093-0112</postCode>
									<region>CA</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">K</forename><forename type="middle">B</forename><surname>Khoshabeh</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">Department of Mathematics</orgName>
								<orgName type="institution">University of California</orgName>
								<address>
									<addrLine>San Diego, La Jolla</addrLine>
									<postCode>92093-0112</postCode>
									<region>CA</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">T</forename><forename type="middle">Q</forename><surname>Gibson</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">Department of Mathematics</orgName>
								<orgName type="institution">University of California</orgName>
								<address>
									<addrLine>San Diego, La Jolla</addrLine>
									<postCode>92093-0112</postCode>
									<region>CA</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><surname>Nguyen</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">Department of Mathematics</orgName>
								<orgName type="institution">University of California</orgName>
								<address>
									<addrLine>San Diego, La Jolla</addrLine>
									<postCode>92093-0112</postCode>
									<region>CA</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">An Augmented Lagrangian Method for Total Variation Video Restoration</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">C29C3DBBB9508A41A39CA71A514415EA</idno>
					<idno type="DOI">10.1109/TIP.2011.2158229</idno>
					<note type="submission">received October 29, 2010; revised February 24, 2011 and April 26, 2011; accepted May 05, 2011. Date of publication May 31, 2011; date of current version October 19, 2011.</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.3" ident="GROBID" when="2023-07-28T03:51+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Alternating direction method (ADM)</term>
					<term>augmented Lagrangian</term>
					<term>hot-air turbulence</term>
					<term>total variation (TV)</term>
					<term>video deblurring</term>
					<term>video disparity</term>
					<term>video restoration</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>This paper presents a fast algorithm for restoring video sequences. The proposed algorithm, as opposed to existing methods, does not consider video restoration as a sequence of image restoration problems. Rather, it treats a video sequence as a space-time volume and poses a space-time total variation regularization to enhance the smoothness of the solution. The optimization problem is solved by transforming the original unconstrained minimization problem to an equivalent constrained minimization problem. An augmented Lagrangian method is used to handle the constraints, and an alternating direction method is used to iteratively find solutions to the subproblems. The proposed algorithm has a wide range of applications, including video deblurring and denoising, video disparity refinement, and hot-air turbulence effect reduction.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>I. INTRODUCTION</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Video Restoration Problems</head><p>I MAGE RESTORATION is an inverse problem where the objective is to recover a sharp image from a blurry and noisy observation. Mathematically, a linear shift invariant imaging system is modeled as <ref type="bibr" target="#b0">[1]</ref> (1) where is a vector denoting the unknown (potentially sharp) image of size , is a vector denoting the observed image, is a vector denoting the noise, and matrix is a linear transformation representing convolution operation. The goal of image restoration is to recover from .</p><p>Standard single-image restoration has been studied for more than half a century. Popular methods such as Wiener deconvolution <ref type="bibr" target="#b0">[1]</ref>, Lucy Richardson deconvolution <ref type="bibr" target="#b1">[2]</ref>, <ref type="bibr" target="#b2">[3]</ref>, and regularized least squares minimization <ref type="bibr" target="#b3">[4]</ref>, <ref type="bibr" target="#b4">[5]</ref> have already been implemented in MATLAB and FIJI <ref type="bibr" target="#b5">[6]</ref>. Advanced methods such as variational methods are also becoming mature <ref type="bibr" target="#b6">[7]</ref>- <ref type="bibr" target="#b10">[11]</ref>.</p><p>While single-image restorations still have a room for improvement, we consider in this paper the video restoration problem. The key difference between an image and a video is the additional time dimension. Consequently, video restoration has some unique features that do not exist in an image restoration.</p><p>1) Motion information Motion deblurring requires motion vector field, which can be estimated from a video sequence using conventional methods such as block matching <ref type="bibr" target="#b11">[12]</ref> and optical flow <ref type="bibr" target="#b12">[13]</ref>.</p><p>While it is also possible to remove motion blur based on a single image, for example, <ref type="bibr" target="#b13">[14]</ref>- <ref type="bibr" target="#b17">[18]</ref>, the performance is limited to a global motion or, at most, one to two objects by using sophisticated object segmentation algorithms.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>2) Spatial variance versus spatial invariance</head><p>For a class of spatially variant image restoration problems (in particular motion blur), the convolution matrix is not a block-circulant matrix. Therefore, Fourier transforms cannot be utilized to efficiently find a solution. Videos, in contrast, allow us to transform a sequence of spatially variant problems to a spatially invariant problem (See the next section for more discussions). As a result, a huge gain in speed can be realized.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>3) Temporal consistency</head><p>Temporal consistency is concerned about the smoothness of the restored video along the time axis. Although smoothing can be spatially performed (as in the case of single image restoration), temporal consistency cannot be guaranteed if these methods are applied to a video in a frame-by-frame basis. Because of these unique features of a video, we seek a video restoration algorithm that utilizes motion information, exploits the spatially invariant properties, and enforces spatial and temporal consistency.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Related Work</head><p>There are many works on the problem of video restoration, particularly in the domain of video superresolution. In <ref type="bibr" target="#b18">[19]</ref>- <ref type="bibr" target="#b20">[21]</ref>, video superresolution is formulated in a regularized least square minimization framework, in which the bilateral total variation (TV) is used as the regularization function. Later, in <ref type="bibr" target="#b21">[22]</ref> and <ref type="bibr" target="#b22">[23]</ref>, the concept of kernel regression to the video restoration problem is applied. Similar approaches can be also found in <ref type="bibr" target="#b23">[24]</ref>, where Ng et al. considered isotropic TV as the a regularization function and modified <ref type="bibr" target="#b0">(1)</ref> to incorporate the geometric warp caused by motion. In <ref type="bibr" target="#b24">[25]</ref>, <ref type="bibr">Belekos et al.</ref> proposed a novel prior that utilizes the motion vector field in updating the regularization parameters so that the prior is both spatially and temporally adaptive to the data. A recent work by Chan and Nguyen <ref type="bibr" target="#b25">[26]</ref> has considered a regularization function of the residue between the current solution and the motion compensated version of the previous solution.</p><p>It is worth noting that most of the aforementioned methods recover a video in a frame-by-frame basis. 1 Additionally, all of these methods assume that the blur kernel is spatially invariant. While this assumption is valid for many superresolution scenarios where multiple shots of the same object are used to fuse a higher resolution image, it is invalid when the blur is caused by object motions. As a result, they are unable to handle the spatially variant motion blur kernel.</p><p>Our proposed algorithm is inspired by the concept of "space-time volume," which is first introduced in the early 90s by JÃ¤hne <ref type="bibr" target="#b26">[27]</ref>, and rediscovered by Wexler, Shechtman, Caspi, and Irani <ref type="bibr" target="#b27">[28]</ref>, <ref type="bibr" target="#b28">[29]</ref>. The idea of space-time volume is to stack the frames of a video to form a 3-D data structure known as the space-time volume. This allows one to transform the spatially variant motion blur problem to a spatially invariant problem. By imposing regularization functions along the spatial and temporal directions, respectively, both spatial and temporal smoothness can be enforced.</p><p>The main drawback of space-time minimization is that the size of a space-time volume is much larger than that of a single image (or five frames in the case of <ref type="bibr" target="#b24">[25]</ref>). Therefore, the authors of <ref type="bibr" target="#b28">[29]</ref> only considered a Tikhonov regularized least square minimization ([29, eq. ( <ref type="formula">3</ref>)]) in which a closed-form solution exists. More sophisticated regularization functions such as TV and bilateral TV do not seem possible under this framework for these nondifferentiable functions are difficult to efficiently solve.</p><p>This paper investigates the TV regularization functions in space-time minimization. In particular, we consider the following two problems: minimize</p><p>which is known as the TV/L2 minimization and minimize <ref type="bibr" target="#b2">(3)</ref> which is known as the TV/L1 minimization. Unless specified, norms and are the conventional vector 2-norm squares and the vector 1-norm, respectively. TV-norm can either be the anisotropic TV norm (4) 1 A version of <ref type="bibr" target="#b24">[25]</ref> is able to simultaneously process multiple frames, but in practice, it only supports five frames at once.</p><p>or the isotropic TV norm <ref type="bibr" target="#b4">(5)</ref> where operators , , and are the forward finite-difference operators along the horizontal, vertical, and temporal directions, respectively. Here, are constants, and denotes the th component of the vector . More details on these two equations will be discussed in Section II-C.</p><p>The proposed algorithm is based on the augmented Lagrangian method, which is an old method that has recently drawn significant attention <ref type="bibr" target="#b9">[10]</ref>, <ref type="bibr" target="#b10">[11]</ref>, <ref type="bibr" target="#b29">[30]</ref>. Most of the existing augmented Lagrangian methods for image restoration follow from Eckstein and Bertsekas' operator splitting method <ref type="bibr" target="#b30">[31]</ref>, which can be traced back to the work of Douglas and Rachford <ref type="bibr" target="#b31">[32]</ref>, and the proximal point algorithm by Rockafellar <ref type="bibr" target="#b32">[33]</ref>, <ref type="bibr" target="#b33">[34]</ref>. Recently, the operator splitting method has been proven to be equivalent to the splitting Bregman iteration for some problems <ref type="bibr" target="#b34">[35]</ref>, <ref type="bibr" target="#b35">[36]</ref>. However, there is no work on extending the augmented Lagrangian method to space-time minimization.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Contributions</head><p>The contribution of this paper is summarized as follows. 1) We extend the existing augmented Lagrangian method to solve space-time TV minimization problems ( <ref type="formula" target="#formula_0">2</ref>) and (3). Augmented Lagrangian method was previously used to image restoration only <ref type="bibr" target="#b9">[10]</ref>, <ref type="bibr" target="#b10">[11]</ref>. 2) Half-quadratic penalty parameter is updated according to constraint violation. This leads to faster rate of convergence, compared with methods using a fixed parameter <ref type="bibr" target="#b9">[10]</ref>. 3) Because of the space-time data structure, our proposed algorithm is able to handle spatially variant motion blur problems (object motion blur). Existing methods such as <ref type="bibr" target="#b18">[19]</ref>- <ref type="bibr" target="#b25">[26]</ref> are unable to do so. 4) Compared with <ref type="bibr" target="#b28">[29]</ref>, which is also a space-time minimization method, our method achieves TV/L1 and TV/L2 minimization quality, whereas <ref type="bibr" target="#b28">[29]</ref> only achieves Tikhonov least square minimization quality. 5) In terms of speed, we achieve significantly faster computational speed, compared with existing methods. Typical run time to deblur and denoise a 300 400 gray-scaled video is a few second per frame on a personal computer (PC) (MATLAB). This implies the possibility of real-time processing on a graphics-processing unit. 6) The proposed algorithm supports a wide range of applications. a) Video deblurring: With the assistance of frame rate up-conversion algorithms, the proposed method can remove spatially variant motion blur for real video sequences; b) Video disparity: Occlusion errors and temporal inconsistent estimates in the video disparity can be handled by the proposed algorithm without any modification; c) Hot-air turbulence: The algorithm can be directly used to deblur and remove hot-air turbulence effects.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D. Organization</head><p>This paper is an extension of two recently accepted conference papers <ref type="bibr" target="#b36">[37]</ref>, <ref type="bibr" target="#b37">[38]</ref>. The organization of this paper is as follows: Section II consists of notations and background materials. The algorithms are discussed in Section III. Section IV discusses three applications of the proposed algorithm, namely, 1) video deblurring, 2) video disparity refinement, and 3) hot-air turbulence effects reduction. A concluding remark is given in Section V.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>II. BACKGROUND AND NOTATION</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Notation</head><p>A video signal is represented by a 3-D function , where denotes the coordinate in space and denotes the coordinate in time. Suppose that each frame of the video has rows, columns, and there are frames, then, the discrete samples of for , , and form a 3-D tensor of size . For the purpose of discussing numerical algorithms, we use matrices and vectors. To this end, we stack the entries of into a column vector of size , according to the lexicographic order. We use the bold letter to represent the vectorized version of the space-time volume , i.e., where represents the vectorization operator.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Three-Dimensional Convolution</head><p>The 3-D convolution is a natural extension of the conventional 2-D convolution. Given space-time volume and the blur kernel , the convolved signal is given by . Convolution is a linear operation; therefore, it can be expressed using matrices. More precisely, we define the convolution matrix associated with a blur kernel as the linear operator that maps signal to following the rule, i.e., <ref type="bibr" target="#b5">(6)</ref> Assuming periodic boundaries <ref type="bibr" target="#b38">[39]</ref>, the convolution matrix is a triple block-circulant matrix-it has a block-circulant structure, and within each block, there is a submatrix of block circulant with circulant block. Circulant matrices are diagonalizable using discrete Fourier transform (DFT) matrices <ref type="bibr" target="#b39">[40]</ref>, <ref type="bibr" target="#b40">[41]</ref>:</p><p>Fact 1: If is a triple block-circulant matrix, then it can be diagonalized by the 3-D DFT matrix as where is the Hermitian operator and is a diagonal matrix storing the eigenvalues of .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Forward-Difference Operators</head><p>We define operator as a collection of three suboperators , where , , and are the firstorder forward finite-difference operators along the horizontal, vertical, and temporal directions, respectively. The definitions of each individual suboperators are with periodic boundary conditions.</p><p>In order to have greater flexibility in controlling the forward difference along each direction, we introduce three scaling factors as follows. We define scalars , , and and multiply them with , , and , respectively, so that . With , the anisotropic TV norm and the isotropic TV are defined according to ( <ref type="formula">4</ref>) and ( <ref type="formula">5</ref>), respectively. When and , is the 2-D TV of (in space). When and , is the 1-D TV of (in time). By adjusting , , and , we can control the relative emphasis put on individual terms , , and . Note that is equivalent to the vector 1-norm on , i.e., . Therefore, for notation simplicity, we use instead. For , although using the vector 2-norm definition, we still define to align with the definition of . However, this will be made clear if confusion arises.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>III. PROPOSED ALGORITHM</head><p>The proposed algorithm belongs to the family of operator splitting methods <ref type="bibr" target="#b9">[10]</ref>, <ref type="bibr" target="#b10">[11]</ref>, <ref type="bibr" target="#b30">[31]</ref>. Therefore, instead of repeating the details, we focus on the modifications made to the 3-D data structure. Additionally, our discussion is focused on the anisotropic TV, i.e.,</p><p>. The isotropic TV, can be similarly derived.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. TV/L2 Problem</head><p>The core optimization problem that we solve is the following TV/L2 minimization: minimize <ref type="bibr" target="#b6">(7)</ref> where is a regularization parameter. To solve problem <ref type="bibr" target="#b6">(7)</ref>, we first introduce intermediate variables and transform problem (7) into an equivalent problem, i.e., minimize subject to <ref type="bibr" target="#b7">(8)</ref> The augmented Lagrangian of problem ( <ref type="formula">8</ref>) is <ref type="bibr" target="#b8">(9)</ref> where is a regularization parameter associated with the quadratic penalty term and is the Lagrange multiplier associated with the constraint . In ( <ref type="formula">9</ref>), intermediate variable and Lagrange multiplier can be respectively partitioned as and (10) The idea of the augmented Lagrangian method is to find a saddle point of , which is also the solution of the original problem <ref type="bibr" target="#b6">(7)</ref>. To this end, we use the alternating direction method to iteratively solve the following subproblems:</p><formula xml:id="formula_1">(11)<label>(12) (13)</label></formula><p>We now investigate these subproblems one by one.</p><p>1) -Subproblem: By dropping indexes , the solution of problem <ref type="bibr" target="#b10">(11)</ref> is found by considering the normal equation as follows: <ref type="bibr" target="#b13">(14)</ref> The convolution matrix in ( <ref type="formula">14</ref>) is a triple block-circulant matrix, and therefore, by Fact 1, can be diagonalized using the 3-D DFT matrix. Hence, <ref type="bibr" target="#b13">(14)</ref> has the following solution: <ref type="bibr" target="#b14">(15)</ref> where denotes the 3-D Fourier transform operator. The matrices , , , and can be precalculated outside the main loop. Therefore, the complexity of solving <ref type="bibr" target="#b13">(14)</ref> is in the order of operations, which is the complexity of the 3-D Fourier transforms, and is the number of elements of the space-time volume . 2) -Subproblem: Problem ( <ref type="formula" target="#formula_1">12</ref>) is known as the -subproblem, which can be solved using a shrinkage formula <ref type="bibr" target="#b41">[42]</ref>. Letting (analogous definitions for and ), is given by sign ( <ref type="formula">16</ref>)</p><p>Analogous solutions for and can be also derived.</p><p>In case of isotropic TV, the solution is given by <ref type="bibr" target="#b41">[42]</ref> (</p><p>where , and is a small constant . Here, the multiplication and divisions are componentwise operations.</p><p>3) Algorithm: Algorithm 1 shows the pseudocode of the TV/L2 algorithm.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Algorithm 1 Algorithm for TV/L2 minimization problem</head><p>Input data and .</p><p>Input parameters , , , and .</p><p>Set parameters default and default .</p><p>Initialize , , , .</p><p>Compute the matrices , , , and .</p><p>while not converge do 1. Solve the -subproblem ( <ref type="formula">11</ref>) using <ref type="bibr" target="#b14">(15)</ref>.</p><p>2. Solve the -subproblem ( <ref type="formula" target="#formula_1">12</ref>) using ( <ref type="formula">16</ref>).</p><p>3. Update the Lagrange multiplier using (13).</p><p>4. Update according to <ref type="bibr" target="#b23">(24)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Check convergence:</head><p>if then break end if end while</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. TV/L1 Problem</head><p>TV/L1 problem can be solved by introducing two intermediate variables, i.e., and , and modifying problem (3) as minimize subject to <ref type="bibr" target="#b17">(18)</ref> The augmented Lagrangian of ( <ref type="formula">18</ref>) is given by . Here, variable is the Lagrange multiplier associated with constraint , and variable is the Lagrange multiplier associated with the constraint . Moreover, and can be partitioned as in <ref type="bibr" target="#b9">(10)</ref>. Parameters and are two regularization parameters. Subscripts " " and " " stand for "objective" and "regularization," respectively. . The image is blurred by a Gaussian blur kernel of size 9 9 and . Addition Gaussian noise is added to the image so that the BSNR is 40 dB.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>1) -Subproblem:</head><p>The -subproblem of TV/L1 is minimize <ref type="bibr" target="#b18">(19)</ref> which can be solved by considering the following normal equation:</p><formula xml:id="formula_3">yielding (20)</formula><p>2) -Subproblem: The -subproblem of TV/L1 is the same as that of TV/L2. Therefore, the solution is given by <ref type="bibr" target="#b15">(16)</ref>.</p><p>3 </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Parameters</head><p>In this subsection, we discuss the choice of parameters.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>1) Choosing :</head><p>The regularization parameter trades off the least square error and the TV penalty. Large values of tend to give sharper results, but noise will be amplified. Small values of give less noisy results, but the image may be smoothed. The choice of is not known prior to solving the minimization. Recent advances in the operator-splitting methods have considered constrained minimization problems <ref type="bibr" target="#b10">[11]</ref> so that can be replaced by an estimate of the noise level (the noise estimation is performed using a third party algorithm). However, from our experience, it is often easier to choose than to estimate the noise level for the noise characteristic of a video is never exactly known. Empirically, a reasonable for a natural image (and video sequence) typically lies in the range .  . The image is blurred by a Gaussian blur kernel of size 9 9 and of the pixels are corrupted by salt and pepper noise. Image source: <ref type="bibr" target="#b42">[43]</ref>.</p><p>2) Choosing : One of the major differences between the proposed algorithm and FTVd 4.0 <ref type="bibr" target="#b34">[35]</ref> <ref type="foot" target="#foot_0">2</ref> is the update of . In <ref type="bibr" target="#b34">[35]</ref>, is a fixed constant. However, as mentioned in <ref type="bibr" target="#b43">[44]</ref>, the method of multipliers can exhibit a faster rate of convergence by adapting the following parameter update scheme:</p><formula xml:id="formula_4">if otherwise. (<label>24</label></formula><formula xml:id="formula_5">)</formula><p>Here, condition specifies the constraint violation with respect to constant . The intuition is that the quadratic penalty is a convex surface added to the original objective function so that the problem is guaranteed to be strongly convex <ref type="bibr" target="#b32">[33]</ref>. Ideally, residue should decrease as increases. However, if is not decreasing for some reasons, one can increase the weight of penalty , relative to the objective, so that is forced to be reduced. Therefore, given and , where and , <ref type="bibr" target="#b23">(24)</ref> makes sure that the constraint violation is asymptotically decreasing. In the steady state, as , becomes a constant <ref type="bibr" target="#b45">[46]</ref>. The update for in TV/L1 follows a similar approach.</p><p>The initial value of is chosen to be within the range of <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b9">10]</ref>. This value cannot be large (in the order of 100) because the role of the quadratic surface is to perturb the original objective function so that it becomes strongly convex. If the initial value of is too large, the solution of the original problem may not be found. However, cannot be too small either; otherwise, the effect of the quadratic surface becomes negligible. Empirically, we find that is robust to most restoration problems.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D. Convergence</head><p>Fig. <ref type="figure" target="#fig_3">3</ref> illustrates the convergence profile of the TV/L2 algorithm in a typical image recovery problem. In this test, the image "cameraman.tif" (size 256 256; gray scaled) is blurred by a Gaussian blur kernel of size 9 9 and . Gaussian noise is added so that the blurred signal-to-noise ratio (BSNR) is 40 dB. To visualize the effects of the parameter update scheme, we set the initial value of to be , and let . Referring to <ref type="bibr" target="#b23">(24)</ref>, is increased by a factor of if the condition is sat- isfied. Note that <ref type="bibr" target="#b34">[35]</ref> (FTVd 4.0) is a special case when , whereas the proposed algorithm allows the user to vary .</p><p>In Fig. <ref type="figure" target="#fig_3">3</ref>, the -axis is the objective value for the th iteration, and the -axis is iteration number . As shown in the figure, an appropriate choice of significantly improves the rate of convergence. However, if is too large, the algorithm is not converging to the solution. Empirically, we find that is robust to most of the image and video problems.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E. Sensitivity Analysis</head><p>Table I illustrates the sensitivity of the algorithm to parameters , , and . In this test, 20 images are blurred by a Gaussian blur kernel of size <ref type="bibr" target="#b8">9</ref> 9, with variance . The BSNR is 30 dB. For each image, two of the three parameters ( , , and ) are fixed at their default values, i.e., , , and , whereas one of them is varying within the range specified in Table <ref type="table" target="#tab_2">I</ref>. The stopping criteria of the algorithm is , , and for all images. The maximum peak signal-to-noise ratio (PSNR), minimum PSNR, and the difference are reported in Table <ref type="table" target="#tab_2">I</ref>. Referring to the values, it can be calculated that the average maximum-to-minimum PSNR differences among all 20 images for , , and are 0.311, 0.208, and 0.357 dB, respectively. For an average PSNR difference in the order of 0.3 dB, the perceivable difference is small.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>F. Comparison With Existing Operator-Splitting Methods</head><p>The proposed algorithm belongs to the class of operator splitting methods. Table II summarizes the differences between the proposed method and some existing methods. 4   </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>IV. APPLICATIONS</head><p>In this section, we demonstrate three applications of the proposed algorithm, namely, 1) video deblurring, 2) video disparity refinement, and 3) video restoration for videos distorted by hot-air turbulence. Due to limited space, more results are available at http://videoprocessing.ucsd.edu/stanleychan/deconvtv.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Video Deblurring 1) Spatially Invariant Blur:</head><p>We first consider the class of spatially invariant blur. In this problem, the th observed image is related to the true image as Note that the spatially invariant blur kernel is assumed to be identical for all time .</p><p>The typical method to solve a spatially invariant blur is to consider the model as 4 The speed comparison is based on deblurring "lena.bmp" (512 512; gray scaled), which is blurred by a Gaussian blur kernel of size 9 9, , and BSNR dB. The machine used is Intel Qual Core at 2.8 GHz, with 4-GB random access memory (RAM), and Windows 7/MATLAB 2010. Comparisons between FTVd 4.0 and the proposed method are based on . If (default setting of FTVd 4.0), then the run time are 1.56 and 1.28 s for FTVd 4.0 and the proposed method, respectively. and apply a frame-by-frame approach to individually recover . In <ref type="bibr" target="#b25">[26]</ref>, the authors considered the following minimization: minimize where is the solution of the th frame and is the motion compensation operator that maps the coordinates of to the coordinates of . Operators are the spatial forward finite-difference operators oriented at angles 0 , 45 , 90 , and 135 . Regularization parameters and control the relative emphasis put on the spatial and temporal smoothness.</p><p>Another method to solve the spatially invariant blur problem is to apply the multichannel approach by modeling the imaging process as <ref type="bibr" target="#b23">[24]</ref>, <ref type="bibr" target="#b24">[25]</ref> for</p><p>, where is the size of the temporal window (typically ranged from 1 to 3).</p><p>is the motion compensation operator that maps the coordinates of to the coordinates of . The th frame can be recovered by solving the following minimization <ref type="bibr" target="#b23">[24]</ref>: minimize <ref type="bibr" target="#b24">(25)</ref> where is a constant and is the isotropic TV on the th frame. The method presented in <ref type="bibr" target="#b24">[25]</ref> replaces the objective function by a weighted least squares and the isotropic TV regularization function by a weighted 2-norm on gradient. The weights are adaptively updated (using residue and motion vector field) in each iteration, and therefore, the regularization function is nonstationary, both spatially and temporally.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>TABLE II COMPARISONS BETWEEN THE OPERATOR-SPLITTING METHODS FOR TV/L2 MINIMIZATION TABLE III COMPARISONS BETWEEN THE VIDEO RESTORATION METHODS</head><p>A drawback of these methods is that the image recovery result heavily depends on the accuracy of motion estimation and compensation. In particular, in occlusion areas, the assumption that is a one-to-one mapping <ref type="bibr" target="#b46">[47]</ref> fails to hold. Thus, is not a full-rank matrix, and . As a result, minimizing can lead to a serious error. There are methods to reduce the error caused by rank deficiency of , for example, the concept of unobservable pixel introduced in <ref type="bibr" target="#b23">[24]</ref>, but the restoration result depends on the effectiveness of how the unobservable pixels are selected.</p><p>Another drawback of these methods is the computation time. For spatially invariant blur, blur operator is a block-circulant matrix. However, in the multichannel model, the operator is not a block-circulant matrix. The block-circulant property is a critical factor to speed as it allows the use of Fourier transform methods. For methods in <ref type="bibr" target="#b23">[24]</ref> and <ref type="bibr" target="#b24">[25]</ref>, conjugate gradient (CG) is used to solve the minimization task. While the total number of CG iterations may be few, the per-iteration run time can be long.</p><p>Table <ref type="table" target="#tab_2">III</ref> illustrates the differences between various video restoration methods.</p><p>Our approach to solve spatially invariant blur problem shares the same insight as <ref type="bibr" target="#b28">[29]</ref>, which does not consider motion compensation. The temporal error is handled by spatio-temporal TV . An intuition to this approach is that the temporal difference can be classified as temporal edge and temporal noise. The temporal edge is the intensity change caused by object movements, whereas the temporal noise is the artifact generated in the minimization process. Similar to the spatial TV, the temporal TV preserves the temporal edges while reducing the temporal noise. Moreover, the space-time volume preserves the block-circu- lant structure of the operator, thus leading to significantly faster computation.</p><p>Table <ref type="table" target="#tab_2">IV</ref>, and Figs. <ref type="figure" target="#fig_4">4</ref> and<ref type="figure" target="#fig_5">5</ref> show the comparisons between <ref type="bibr" target="#b23">[24]</ref>, <ref type="bibr" target="#b25">[26]</ref>, and <ref type="bibr" target="#b28">[29]</ref> and the proposed method on spatially invariant blur. The four testing video sequences are blurred by a Gaussian blur kernel of size 9 9 with . Additive Gaussian noise are added so that the BSNR is 30 dB.</p><p>The specific settings of the methods are as follows. For <ref type="bibr" target="#b28">[29]</ref>, we consider the following minimization: minimize and set the parameters empirically for the best recovery quality: and . For <ref type="bibr" target="#b23">[24]</ref>, instead of  <ref type="table" target="#tab_2">IV</ref>).</p><p>using the CG presented in this paper, we use a modification of the proposed augmented Lagrangian method to speed up the computation. Specifically, in solving the -subproblem, we used CG (LSQR <ref type="bibr" target="#b47">[48]</ref>) to accommodate the nonblock-circulant operator . The motion estimation is performed using the benchmark full search (exhaustive search) with 0.5-pixel accuracy. The block size is 8 8, and the search range is 16 16. Motion compensation is performed by coordinate transform according to the motion vectors (bilinear interpolation for half pixels). The threshold for unobservable pixels <ref type="bibr" target="#b23">[24]</ref> is set as 6 (out of 255), and the regularization parameter is [see <ref type="bibr" target="#b24">(25)</ref>]. We use the previous and the next frame for the model, i.e., and let (Using (1, 1, 1) tends to give worse results). For <ref type="bibr" target="#b25">[26]</ref>, the regularization parameters are also empirically chosen for the best recovery quality: and . To compare these methods, we apply TV/L2 (Algorithm 1) with the following parameters (same for all four videos): and . All other parameters take the default setting: , , and . The algorithm terminates if .  <ref type="table" target="#tab_2">IV</ref>).</p><p>In Table <ref type="table" target="#tab_2">IV</ref>, three quantities are used to evaluate the performance of the algorithms. PSNR measures the image fidelity. Spatial TV is defined as for each frame, and temporal TV is defined as for each frame <ref type="bibr" target="#b25">[26]</ref>. The average (over all frames) PSNR, , and are listed in Table <ref type="table" target="#tab_2">IV</ref>. Referring to the results, it is shown that the proposed algorithm produces the highest PSNR values while keeping and at a low level. It is worth noting that <ref type="bibr" target="#b28">[29]</ref> is equivalent to the 3-D Wiener deconvolution (regularized). Therefore, there exists a closed-form solution, but the result looks blurrier than the other methods. Among the four methods, both <ref type="bibr" target="#b23">[24]</ref> and <ref type="bibr" target="#b25">[26]</ref> use motion estimation and compensation. However, <ref type="bibr" target="#b23">[24]</ref> is more sensitive to the motion estimation error-motion estimation error in some fast-moving areas are amplified in the deblurring step. Reference <ref type="bibr" target="#b25">[26]</ref> is more robust to motion estimation error, but the computation time is significantly longer than the proposed method. The run time of <ref type="bibr" target="#b23">[24]</ref> and <ref type="bibr" target="#b25">[26]</ref> are approximately 100 s per frame (per color channel), whereas the proposed algorithm only requires approximately 2 s per frame (per color channel). These statistics are based on recovering videos of size 288 352, using a PC with Intel Qual Core at 2.8 GHz, with 4-GB RAM, and Windows 7/MATLAB 2010. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>2) Spatially Variant Motion Blur:</head><p>The proposed algorithm can be used to remove spatially variant motion blur. However, since motion-blurred videos often have low temporal resolution, frame rate up-conversion algorithms are needed to first increase the temporal resolution before applying the proposed method (see <ref type="bibr" target="#b28">[29]</ref> for detailed explanations). To this end, we apply <ref type="bibr" target="#b48">[49]</ref> to upsample the video by a factor of 8. Consequently, the motion blur kernel can be modeled as if</p><p>, and otherwise where in this case. Fig. <ref type="figure" target="#fig_6">6</ref> shows frame no. 146 of the video sequence "Market Place," and Fig. <ref type="figure" target="#fig_7">7</ref> shows frame no. 28 of the video sequence "Super Loop." The videos are captured by a Panasonic TM-700 video recorder with resolution 1920 1080p at 60 fps. For computational speed, we down sampled the spatial resolution by a factor of 4 (so the resolution is 480 270). The parameters of the proposed algorithm are empirically chosen as and . There are not many relevant video motion deblurring algorithms for comparison (or unavailable to be tested). Therefore, we are only able to show the results of <ref type="bibr" target="#b28">[29]</ref>, using parameters and . As shown in Figs. <ref type="figure" target="#fig_6">6</ref> and<ref type="figure" target="#fig_7">7</ref>, the proposed algorithm produces a significantly higher quality result than <ref type="bibr" target="#b28">[29]</ref>. We also tested for a range of parameters and for <ref type="bibr" target="#b28">[29]</ref>. However, we observe that the results are either oversharpened (serious ringing artifacts) or undersharpened (not enough deblurring).</p><p>3) Limitation: The algorithm requires considerably less memory than other TV minimization algorithms such as interior point methods. However, for high-definition videos, the proposed algorithm still has a memory issue as the size of the space-time volume is large. While one can use fewer frames  to lower the memory demand, trade off in the recovery quality should be expected.</p><p>Another bottleneck of the proposed algorithm is the sensitivity to the frame-rate conversion algorithm. At object boundaries where the motion estimation algorithm fails to provide accurate estimates, the estimation error in the deblurring step will be amplified. This typically happens to areas with nonuniform and rapid motion. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Video Disparity Refinement</head><p>1) Problem Description: Our second example is disparity map refinement. Disparity is proportional to the reciprocal of the distance between the camera and the object (i.e., depth). Disparity maps are useful for many stereo-video processing applications, including object detection in 3-D space, saliency for stereo videos, stereo coding, and view synthesis There are numerous papers on generating one disparity map based on a pair of stereo images <ref type="bibr" target="#b49">[50]</ref>. However, all of these methods cannot be extended to videos because the energy functions are considered in a frame-by-frame basis. Although there are works in enforcing temporal consistency for adjacent frames, such as <ref type="bibr" target="#b50">[51]</ref> and <ref type="bibr" target="#b51">[52]</ref>, the computational complexity is high.</p><p>We propose to estimate the video disparity in two steps. In the first step, we combine the locally adaptive support weight <ref type="bibr" target="#b52">[53]</ref> and the cross-bilateralateral grid <ref type="bibr" target="#b53">[54]</ref> to generate an initial disparity estimate. Since this method is a frame-by-frame method, spatial and temporal consistency is poor. In the second step, we consider the initial video disparity as a space-time volume and solve the TV/L1 minimization problem, i.e., minimize There are two reasons for choosing TV/L1 instead of TV/L2 in refining video disparity. First, disparity is a piecewise constant function with quantized levels, and across the flat regions, there are sharp edges. As shown in Fig. <ref type="figure" target="#fig_8">8</ref> (bottom), the estimation error behaves like outliers in a smooth function. Therefore, to reduce the estimation error, one can consider a robust curve fitting as it preserves the shape of the data while suppressing the outliers.</p><p>The second reason for using TV/L1 is that the 1-norm is related to the notion of percentage of bad pixels, which is a quantity commonly used to evaluate disparity estimation algorithms <ref type="bibr" target="#b49">[50]</ref>. Given a ground truth disparity , the number of bad pixels of an estimated disparity is the cardinality of the set for some threshold . In the absence of ground truth, the same idea can be used with a reference disparity (e.g., ). In this case, the cardinality of the set , denoted by , is the number of bad pixels of with respect to (w.r.t) . Therefore, minimizing is equivalent to minimizing the number of bad pixels of w.r.t. . However, this problem is nonconvex and is NP-hard. In order to alleviate the computational difficulty, we set so that , and convexify by . Therefore, can be regarded as the convexification of the notion of percentage bad pixels.</p><p>2) Video Results: Two real videos ("Horse" and "Old Timers") are tested for the proposed algorithm. These stereo videos are downloaded from http://sp.cs.tut.fi/mo-bile3dtv/stereo-video/. Fig. <ref type="figure" target="#fig_9">9</ref> illustrates the results. The first row in Fig. <ref type="figure" target="#fig_9">9</ref> shows the left view of the stereo video. The second row shows the results of applying <ref type="bibr" target="#b52">[53]</ref>, <ref type="bibr" target="#b53">[54]</ref> to the stereo video. Note that we are implementing a spatio-temporal version of <ref type="bibr" target="#b53">[54]</ref>, which uses adjacent frames to enhance the temporal consistency. However, the estimated disparity is still noisy, particularly around the object boundaries. The third row shows the result of applying the proposed TV/L1 minimization to the initial disparity estimated in the second row. It should be noted that the proposed TV/L1 minimization improves not only the flat interior region but also the object boundary (e.g., the arm of the man in "Old Timers" sequence), which is an area that <ref type="bibr" target="#b52">[53]</ref> and <ref type="bibr" target="#b53">[54]</ref> are unable to handle. 3) Image Results: The effectiveness of the proposed algorithm can be further elaborated by comparing to the 99 benchmark methods on Middlebury stereo evaluation website <ref type="bibr" target="#b49">[50]</ref>. For all the 99 methods on Middlebury stereo evaluation website, we download their results and apply the proposed algorithm to improve the spatial smoothness. Note that the proposed algorithm is ready for this test because an image is a single-frame video. In this case, we set . Fig. <ref type="figure" target="#fig_10">10</ref> shows the results for two of the 99 methods (randomly chosen) for the data set "Tsukuba," and Fig. <ref type="figure" target="#fig_11">11</ref> shows the percentage of error reduction (in terms of the number of bad pixels, with threshold 1) by applying the proposed algorithm to all methods on the Middlebury database. The higher bars in the plots indicate that the proposed algorithm reduces the error by a greater amount. It is shown that the errors are typically reduced by a large margin of over 10%. While there is less error reduction for some data sets, it is important to note that error reduction is always nonnegative. In other words, the proposed algorithm always improves the initial disparity estimate. Furthermore, for every algorithm, we provide improvement in at least one of the image sets.</p><p>Limitations: A limitation of the proposed algorithm is that it is unable to handle large and consistent error results from poor initial disparity estimation algorithm. This particularly happens in large occlusion areas, repeating texture regions, or frames consisting of rapid motions. We are currently seeking methods to feedback the TV/L1 result to the initial disparity estimation so that the algorithm is more robust to these errors.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Videos Distorted by Hot-Air Turbulence</head><p>1) Problem Description: Our third example is the stabilization of videos distorted by hot-air turbulence effects. In the pres-ence of hot-air turbulence, the refractive index along the transmission path of the light ray is spatially and temporally varying <ref type="bibr" target="#b54">[55]</ref>. Consequently, the path differences and, hence, the phases of the light rays are also spatially and temporally varying. As a result, the observed image is distorted by geometric warping, motion blur, and, sometimes, out-of-focus blur. This type of distortion is generally known as the hot-air turbulence effect.</p><p>There are various methods to overcome imaging through hot-air turbulence. For example, the speckle imaging technique <ref type="bibr" target="#b54">[55]</ref> assumes that the refractive index is randomly changing but is also statistically stationary <ref type="bibr" target="#b55">[56]</ref>, <ref type="bibr" target="#b56">[57]</ref>. Consequently, by averaging enough number of frames, the geometric distortion will be smoothed out. Then, a deconvolution algorithm can be used to remove the blur.</p><p>The drawback of the speckle imaging technique is that the average operation makes the deblurring process challenging. Therefore, Zhu and Milanfar <ref type="bibr" target="#b59">[60]</ref> and Shimizu et al. <ref type="bibr" target="#b60">[61]</ref> proposed to first compensate the geometric distortion using nonrigid registration <ref type="bibr" target="#b61">[62]</ref> and then deblur the images using deconvolution algorithms. The limitation is that nonrigid registration works well only when the geometric distortion can be adjusted by all the control points in the grid <ref type="bibr" target="#b61">[62]</ref>. However, imaging through hot-air turbulence contains both large-area distortion (perceived as waving) and small disturbance (perceived as jittering). If nonrigid registration has to be used to compensate small disturbance, then the number of control points will be huge, making the computation not practical. There are other methods such as lucky frame/region fusion approach <ref type="bibr" target="#b62">[63]</ref>, <ref type="bibr" target="#b63">[64]</ref>. However, these methods cannot effectively handle small disturbance either.</p><p>Using the same methodology as we used for video deblurring, we consider the video as a space-time volume and minimize the TV/L2 problem. Our intuition is that the small hot-air  turbulence can be regarded as temporal noise, whereas the object movement is regarded as temporal edge. Under this framework, spatially invariant blur can be also incorporated. If the input video originally has a low contrast, a preprocessing step using gray level grouping (GLG) <ref type="bibr" target="#b57">[58]</ref>, <ref type="bibr" target="#b58">[59]</ref> can be used (See Fig. <ref type="figure" target="#fig_12">12</ref>).</p><p>2) Experiments: Fig. <ref type="figure" target="#fig_13">13</ref> shows the snapshots (zoom in) of a video sequence "Acoustic Explorer." In this example, GLG is applied to the input videos so that contrast is enhanced. Then, the proposed algorithm is used to reduce the hot-air turbulence effect. A Gaussian blur kernel is assumed in both examples, where the variance is empirically determined. Comparing the video quality before and after applying the proposed method, fewer jittering such as artifacts are observed in the processed videos. While this may not be apparent by viewing the still images, the improvement is significant in the 24 fps videos. <ref type="foot" target="#foot_2">5</ref>Fig. <ref type="figure" target="#fig_14">14</ref> shows the comparisons without the contrast enhancement by GLG. Referring to the figures, the proposed algorithm does not only reduce the unstable hot-air turbulence effects, it also improves the blur. The word "Empire State" could not be clearly seen in the input sequence, but it becomes sharper in the processed sequence.</p><p>3) Limitation: The aforementioned experiments indicate that the proposed algorithm is effective for reducing small hot-air turbulence effects. However, for large-area geometric distortions, nonrigid registration is needed. In addition, the general turbulence distortion is spatially and temporally varying, meaning that the point spread function cannot be modeled as one Gaussian function. This issue is an open problem. V. CONCLUSION In this paper, we have proposed a video deblurring/denoising algorithm that minimizes a TV optimization problem for spatial-temporal data. The algorithm transforms the original unconstrained problem to an equivalent constrained problem and uses an augmented Lagrangian method to solve the constrained problem. With the introduction of spatial and temporal regularization to the spatial-temporal data, the solution of the algorithm is both spatially and temporally consistent.</p><p>Applications of the algorithm include video deblurring, disparity refinement, and turbulence removal. For video deblurring, the proposed algorithm restores motion-blurred video sequences. The average PSNR is improved, and the spatial and temporal TVs are maintained at an appropriate level, meaning that the restored videos are spatially and temporally consistent.</p><p>For disparity map refinement, the algorithm removes flickering in the disparity map and preserves the sharp edges in the disparity map. For turbulence removal, the proposed algorithm stabilizes and deblurs videos taken under the influence of hot-air turbulence.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 1 .</head><label>1</label><figDesc>Fig. 1. TV/L2 image recovery using different choices . The optimal (in terms of PSNR compared to the reference) is. The image is blurred by a Gaussian blur kernel of size 9 9 and . Addition Gaussian noise is added to the image so that the BSNR is 40 dB.</figDesc><graphic coords="5,84.00,63.12,432.00,90.00" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head></head><label></label><figDesc>Figs. 1 and 2 show the recovery results by using different values of . In the case of TV/L1 minimization, is typically lying in the range [0.1, 10].</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 2 .</head><label>2</label><figDesc>Fig. 2. TV/L1 image recovery using different choices . The optimal (in terms of PSNR compared to the reference) is. The image is blurred by a Gaussian blur kernel of size 9 9 and of the pixels are corrupted by salt and pepper noise. Image source:<ref type="bibr" target="#b42">[43]</ref>.</figDesc><graphic coords="6,79.98,64.14,432.00,90.00" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Fig. 3 .</head><label>3</label><figDesc>Fig. 3. Convergence profile of the proposed algorithm for deblurring the image "cameraman.tif". (Four colored curves) The rate of convergence using different values of , where is the multiplication factor for updating .</figDesc><graphic coords="6,331.02,210.12,196.98,148.98" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Fig. 4 .</head><label>4</label><figDesc>Fig. 4. "News" sequence; frame 100. (a) Original image (cropped for better visualization). (b) Blurred by a Gaussian blur kernel of size 9 9, , and BSNR dB. (c)-(f) Results by various methods (see TableIV).</figDesc><graphic coords="9,46.02,67.14,235.98,372.00" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Fig. 5 .</head><label>5</label><figDesc>Fig. 5. "Salesman" sequence; frame 10. (a) Original image (cropped for better visualization). (b) Blurred by a Gaussian blur kernel of size 9 9, , and BSNR dB. (c)-(f) Results by various methods (see TableIV).</figDesc><graphic coords="9,309.00,65.10,235.98,372.00" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Fig. 6 .</head><label>6</label><figDesc>Fig. 6. "Market Place" sequence; frame 146. (Top) The original observed video sequences. (Middle) Result of [29]. (Bottom) Result of the proposed method.</figDesc><graphic coords="10,43.02,65.16,243.00,265.98" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Fig. 7 .</head><label>7</label><figDesc>Fig. 7. "Super Loop" sequence; frame 28. (Top) The original observed video sequences. (Middle) Result of [29]. (Bottom) Result of the proposed method.</figDesc><graphic coords="10,307.02,65.16,243.00,265.98" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Fig. 8 .</head><label>8</label><figDesc>Fig. 8. (Top) Before applying the proposed TV/L1 algorithm. (Middle) After applying the proposed TV/L1 algorithm. (Bottom) Time evolution of the disparity value (normalized) of a pixel.</figDesc><graphic coords="10,306.00,374.16,246.00,235.98" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>Fig. 9 .</head><label>9</label><figDesc>Fig. 9. Video disparity estimation. (First row) Left view of the stereo video. (Second row) Initial disparity estimate. (Third row) Refinement using the proposed method with parameters , , , , , and . (Last row) Zoom-in comparisons. (a) "Old Timers" sequence. (b) "Horse" sequence.</figDesc><graphic coords="11,58.98,64.14,472.98,172.98" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head>Fig. 10 .</head><label>10</label><figDesc>Fig. 10. Image disparity refinement on algorithms 8 and 78 (randomly chosen) from Middlebury for "Tsukuba." (Red box) Before applying the proposed method. (Blue box) After applying the proposed method.is found exhaustively with increment 0.1, , , , , and .</figDesc><graphic coords="11,309.00,296.10,235.98,111.00" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_11"><head>Fig. 11 .</head><label>11</label><figDesc>Fig. 11. Percentage error reduction (in terms of number of bad pixels) by applying the proposed algorithm to all the 99 methods on the Middlebury stereo database.</figDesc><graphic coords="12,92.04,65.16,409.92,274.98" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_12"><head>Fig. 12 .</head><label>12</label><figDesc>Fig. 12. Hot-air turbulence removal for the sequence "Acoustic Explorer" using the proposed method to reduce the effect of hot-air turbulence. (a) A frame of the original video sequence. (b) Step 1: Apply GLG [58], [59] to the input. (c) Step 2: Apply the proposed method to the results of Step 1.</figDesc><graphic coords="13,49.02,63.12,238.98,73.02" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_13"><head>Fig. 13 .</head><label>13</label><figDesc>Fig. 13. Zoom-in of "Acoustic Explorer" sequence; frames 25-28 (object is 2 mi from the camera). (Top) Input video sequence with contrast enhanced by GLG. (Bottom) Processed video by applying the proposed method to the output of GLG.</figDesc><graphic coords="13,49.98,214.14,223.98,117.96" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_14"><head>Fig. 14 .</head><label>14</label><figDesc>Fig. 14. Snapshot of "Empire State" sequence. (Left) Input video sequence without GLG. (Right) Processed video by applying GLG and the proposed method.</figDesc><graphic coords="13,310.02,64.14,235.98,84.00" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0"><head></head><label></label><figDesc></figDesc><graphic coords="7,66.00,94.14,446.10,216.00" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0"><head></head><label></label><figDesc></figDesc><graphic coords="8,91.98,91.14,420.00,121.98" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0"><head></head><label></label><figDesc></figDesc><graphic coords="8,49.98,254.16,504.00,107.94" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>TABLE I SENSITIVITY</head><label>I</label><figDesc>ANALYSIS OF PARAMETERS. MAXIMUM AND MINIMUM PSNR (IN DECIBELS) FOR A RANGE OF , , AND . IF A PARAMETER IS NOT THE VARIABLE, IT IS FIXED AT DEFAULT VALUES , , AND</figDesc><table /></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_0"><p>The most significant difference is that FTVd 4.0 supports only images, whereas the proposed algorithm supports videos.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3" xml:id="foot_1"><p>It should be noted that the optimization problem is identical for all parameter settings. Therefore, the correlation between the PSNR and visual quality is high.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="5" xml:id="foot_2"><p>Videos are available at http://videoprocessing.ucsd.edu/~stanleychan/deconvtv</p></note>
		</body>
		<back>

			<div type="funding">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>The work of S. Chan was supported by Croucher Foundation Scholarship, Hong Kong. The work of R. Khoshabeh was supported by the University of California, San Diego, under Program CalIT2 CSRO. The work of K. Gibson was support by the Space and Naval Warfare Systems Center Pacific (SSC Pacific) Naval Innovative Science and Engineering Program. The Associate Editor coordinating the review of this manuscript and approving it for publication was Prof.</p></div>
			</div>

			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>As an undergraduate, he specialized in computer design and graduated with highest honors (summa cum laude). In graduate school, his emphasis has been on computer vision and 3-D. His thesis work centers on the application of 3-D in the realm of surgical practice. His collaborators include two world-renowned surgeons and an expert in digital signal processing, all of which have guided him toward his academic goals. His interests include image and video processing, machine learning, robotics, biology and chemistry, and any opportunities in which he may help mankind through science and technology.</p><p>Mr. Khoshabeh was a recipient of California Institute of Telecommunications and Information Technology Strategic Research Opportunities Fellowship. </p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<author>
			<persName><forename type="first">R</forename><surname>Gonzalez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Woods</surname></persName>
		</author>
		<title level="m">Digital Image Processing</title>
		<meeting><address><addrLine>Englewood Cliffs, NJ</addrLine></address></meeting>
		<imprint>
			<publisher>Prentice-Hall</publisher>
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">An iterative technique for the rectification of observed distributions</title>
		<author>
			<persName><forename type="first">L</forename><surname>Lucy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Astron. J</title>
		<imprint>
			<biblScope unit="volume">79</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="745" to="754" />
			<date type="published" when="1974-06">Jun. 1974</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Bayesian-based iterative method of image restoration</title>
		<author>
			<persName><forename type="first">W</forename><surname>Richardson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Opt. Soc. Amer</title>
		<imprint>
			<biblScope unit="volume">62</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="55" to="59" />
			<date type="published" when="1972-01">Jan. 1972</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Regularized constrained total least-squares image restoration</title>
		<author>
			<persName><forename type="first">V</forename><surname>Mesarovic</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Galatsanos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Katsaggelos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Image Process</title>
		<imprint>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="1096" to="1108" />
			<date type="published" when="1995-08">Aug. 1995</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<author>
			<persName><forename type="first">P</forename><surname>Hansen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Nagy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>O'leary</surname></persName>
		</author>
		<title level="m">Deblurring Images: Matrices, Spectra, and Filtering (Fundamentals of Algorithms 3)</title>
		<meeting><address><addrLine>Philadelphia, PA</addrLine></address></meeting>
		<imprint>
			<publisher>SIAM</publisher>
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<author>
			<persName><surname>Fiji</surname></persName>
		</author>
		<ptr target="http://pacific.mpi-cbg.de/wiki/index.php/Fiji" />
		<title level="m">Fiji Is Just ImageJ</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Nonlinear total variation based noise removal algorithms</title>
		<author>
			<persName><forename type="first">L</forename><surname>Rudin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Osher</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Fatemi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Phys. D</title>
		<imprint>
			<biblScope unit="volume">60</biblScope>
			<biblScope unit="issue">1-4</biblScope>
			<biblScope unit="page" from="259" to="268" />
			<date type="published" when="1992-11">Nov. 1992</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">A nonlinear primal-dual method for total variation-based image restoration</title>
		<author>
			<persName><forename type="first">T</forename><surname>Chan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Golub</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Mulet</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">SIAM J. Sci. Comput</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="1964" to="1977" />
			<date type="published" when="1999-11">Nov. 1999</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">An algorithm for total variation minimization and applications</title>
		<author>
			<persName><forename type="first">A</forename><surname>Chambolle</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Math. Imaging Vis</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="issue">1/2</biblScope>
			<biblScope unit="page" from="89" to="97" />
			<date type="published" when="2004-03">Jan. -Mar. 2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">An efficient TVL1 algorithm for deblurring multichannel images corrupted by impulsive noise CAAM</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Yin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<idno>TR-0812</idno>
		<imprint>
			<date type="published" when="2008-09">Sep. 2008</date>
			<pubPlace>Houston, TX</pubPlace>
		</imprint>
		<respStmt>
			<orgName>Rice Univ.</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Fast image recovery using variable splitting and constrained optimization</title>
		<author>
			<persName><forename type="first">M</forename><surname>Afonso</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Bioucas-Dias</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Figueiredo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Image Process</title>
		<imprint>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="issue">9</biblScope>
			<biblScope unit="page" from="2345" to="2356" />
			<date type="published" when="2010-09">Sep. 2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<author>
			<persName><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Ostermann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<title level="m">Video Processing and Communications</title>
		<meeting><address><addrLine>Englewood Cliffs, NJ</addrLine></address></meeting>
		<imprint>
			<publisher>Prentice-Hall</publisher>
			<date type="published" when="2002">2002</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">Generalized image matching by the method of differences</title>
		<author>
			<persName><forename type="first">B</forename><surname>Lucas</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1984">1984</date>
			<pubPlace>Pittsburgh, PA</pubPlace>
		</imprint>
		<respStmt>
			<orgName>Carnegie Mellon Univ</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">Ph.D. dissertation</note>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">High-quality motion deblurring from a single image</title>
		<author>
			<persName><forename type="first">Q</forename><surname>Shan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Agarwala</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Trans. Graph. (SIGGRAPH)</title>
		<imprint>
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="1" to="73" />
			<date type="published" when="2008-08">Aug. 2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Motion from blur</title>
		<author>
			<persName><forename type="first">S</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Wu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE CVPR</title>
		<meeting>IEEE CVPR</meeting>
		<imprint>
			<date type="published" when="2008">2008</date>
			<biblScope unit="page" from="1" to="8" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Blind motion deblurring using image statistics</title>
		<author>
			<persName><forename type="first">A</forename><surname>Levin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. NIPS</title>
		<meeting>NIPS</meeting>
		<imprint>
			<date type="published" when="2006">2006</date>
			<biblScope unit="page" from="841" to="848" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Removing non-uniform motion blur from images</title>
		<author>
			<persName><forename type="first">S</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Matsushita</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE ICCV</title>
		<meeting>IEEE ICCV</meeting>
		<imprint>
			<date type="published" when="2007">2007</date>
			<biblScope unit="page" from="1" to="8" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Single image motion deblurring using transparency</title>
		<author>
			<persName><forename type="first">J</forename><surname>Jia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE CVPR</title>
		<meeting>IEEE CVPR</meeting>
		<imprint>
			<date type="published" when="2007">2007</date>
			<biblScope unit="page" from="1" to="8" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Fast and robust multi-frame super-resolution</title>
		<author>
			<persName><forename type="first">S</forename><surname>Farsiu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Robinson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Elad</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Milanfar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Image Process</title>
		<imprint>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page" from="1327" to="1344" />
			<date type="published" when="2004-10">Oct. 2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Multi-frame demosaicing and super-resolution of color images</title>
		<author>
			<persName><forename type="first">S</forename><surname>Farsiu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Elad</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Milanfar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Image Process</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="141" to="159" />
			<date type="published" when="2006-01">Jan. 2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Video-to-video dynamic super-resolution for grayscale and color sequences</title>
		<author>
			<persName><forename type="first">S</forename><surname>Farsiu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Elad</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Milanfar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">EURASIP J. Appl. Signal Process</title>
		<imprint>
			<biblScope unit="page">232</biblScope>
			<date type="published" when="2006">2006. 2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Deblurring using regularized locally adaptive kernel regression</title>
		<author>
			<persName><forename type="first">H</forename><surname>Takeda</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Farsiu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Milanfar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Image Process</title>
		<imprint>
			<biblScope unit="volume">17</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="550" to="563" />
			<date type="published" when="2008-04">Apr. 2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Super-resolution without explicit subpixel motion estimation</title>
		<author>
			<persName><forename type="first">H</forename><surname>Takeda</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Milanfar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Protter</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Elad</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Image Process</title>
		<imprint>
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="issue">9</biblScope>
			<biblScope unit="page" from="1958" to="1975" />
			<date type="published" when="2009-09">Sep. 2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">A total variation regularization based super-resolution reconstruction algorithm for digital video</title>
		<author>
			<persName><forename type="first">M</forename><surname>Ng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Lam</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">EURASIP J. Adv. Signal Process</title>
		<imprint>
			<biblScope unit="page" from="1" to="16" />
			<date type="published" when="2007">2007. 2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Maximum a posteriori video super-resolution using a new multichannel image prior</title>
		<author>
			<persName><forename type="first">S</forename><surname>Belekos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Galatsanos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Katsaggelos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Image Process</title>
		<imprint>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="1451" to="1464" />
			<date type="published" when="2010-06">Jun. 2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">LCD motion blur: Modeling, analysis and algorithm</title>
		<author>
			<persName><forename type="first">S</forename><surname>Chan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Nguyen</surname></persName>
		</author>
		<ptr target="http://videoprocessing.ucsd.edu/~stanleychan/" />
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Image Process</title>
		<imprint>
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<author>
			<persName><forename type="first">B</forename><surname>JÃ¤hne</surname></persName>
		</author>
		<title level="m">Spatio-Temporal Image Processing: Theory and Scientific Applications</title>
		<meeting><address><addrLine>New York</addrLine></address></meeting>
		<imprint>
			<publisher>Springer-Verlag</publisher>
			<date type="published" when="1993">1993</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Space-time completion of video</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Wexler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Shechtman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Irani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Pattern Anal. Mach. Intell</title>
		<imprint>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="463" to="476" />
			<date type="published" when="2007-03">Mar. 2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Space-time super-resolution</title>
		<author>
			<persName><forename type="first">E</forename><surname>Shechtman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Caspi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Irani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Pattern Anal. Mach. Intell</title>
		<imprint>
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="531" to="545" />
			<date type="published" when="2005-04">Apr. 2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">A fast total variation minimization method for image restoration</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Ng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Wen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">SIAM Multiscale Model Simul</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="774" to="795" />
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">On the Douglas-Rachford splitting method and the proximal point algorithm for maximal monotone operators</title>
		<author>
			<persName><forename type="first">J</forename><surname>Eckstein</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Bertsekas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Math. Program</title>
		<imprint>
			<biblScope unit="volume">55</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="293" to="318" />
			<date type="published" when="1992-06">Jun. 1992</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">On the numerical solution of heat conduction problems in two and three space variables</title>
		<author>
			<persName><forename type="first">J</forename><surname>Douglas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Rachford</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Trans. Amer. Math. Soc</title>
		<imprint>
			<biblScope unit="volume">82</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="421" to="439" />
			<date type="published" when="1956-07">Jul. 1956</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Augmented Lagrangians and applications of the proximal point algorithm in convex programming</title>
		<author>
			<persName><forename type="first">R</forename><surname>Rockafellar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Math. Oper. Res</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="97" to="116" />
			<date type="published" when="1976-05">May 1976</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Monotone operators and proximal point algorithm</title>
		<author>
			<persName><forename type="first">R</forename><surname>Rockafellar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">SIAM J. Control Optim</title>
		<imprint>
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="877" to="898" />
			<date type="published" when="1976">1976</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<monogr>
		<title level="m" type="main">Alternating direction algorithms for total variation deconvolution in image reconstruction Nanjing Univ</title>
		<author>
			<persName><forename type="first">M</forename><surname>Tao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Yang</surname></persName>
		</author>
		<ptr target="http://www.optimiza-tion-online.org/DB_FILE/2009/11/2463.pdf" />
		<imprint>
			<date type="published" when="2009">2009</date>
			<pubPlace>Nanjing, China</pubPlace>
		</imprint>
	</monogr>
	<note type="report_type">Tech. Rep. TR0918</note>
</biblStruct>

<biblStruct xml:id="b35">
	<monogr>
		<title level="m" type="main">Applications of Lagrangian-based alternating direction methods and connections to split Bregman UCLA</title>
		<author>
			<persName><forename type="first">E</forename><surname>Esser</surname></persName>
		</author>
		<ptr target="ftp://ftp.math.ucla.edu/pub/camreport/cam09-31.pdf" />
		<imprint>
			<date type="published" when="2009">2009</date>
			<pubPlace>Los Angeles, CA</pubPlace>
		</imprint>
	</monogr>
	<note type="report_type">Tech. Rep. 09-31</note>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">An augmented Lagrangian method for video restoration</title>
		<author>
			<persName><forename type="first">S</forename><surname>Chan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Khoshabeh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Gibson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Gill</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Nguyen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE ICASSP</title>
		<meeting>IEEE ICASSP</meeting>
		<imprint>
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Spatio-temporal consistency in video disparity estimation</title>
		<author>
			<persName><forename type="first">R</forename><surname>Khoshabeh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Chan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Nguyen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE ICASSP</title>
		<meeting>IEEE ICASSP</meeting>
		<imprint>
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Numerical optimization methods for image restoration</title>
		<author>
			<persName><forename type="first">B</forename><surname>Kim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Dept. Manage. Sci. Eng., Stanford Univ</title>
		<imprint>
			<date type="published" when="2002-12">Dec. 2002</date>
			<pubPlace>Stanford, CA</pubPlace>
		</imprint>
	</monogr>
	<note type="report_type">Ph.D. dissertation</note>
</biblStruct>

<biblStruct xml:id="b39">
	<monogr>
		<title level="m" type="main">Matrix Computation, 2nd ed</title>
		<author>
			<persName><forename type="first">G</forename><surname>Golub</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Van Loan</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1989">1989</date>
			<publisher>Johns Hopkins Univ. Press</publisher>
			<pubPlace>Baltimore, MD</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<monogr>
		<title level="m" type="main">Iterative Methods for Toeplitz Systems</title>
		<author>
			<persName><forename type="first">M</forename><surname>Ng</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2004">2004</date>
			<publisher>Oxford Univ. Press</publisher>
			<pubPlace>London, U.K.</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">An efficient algorithm for total variation regularization with applications to the single pixel camera and compressive sensing</title>
		<author>
			<persName><forename type="first">C</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Dept. Comput. Appl. Math., Rice Univ</title>
		<imprint>
			<date type="published" when="2009">2009</date>
			<pubPlace>Houston, TX</pubPlace>
		</imprint>
	</monogr>
	<note type="report_type">M.S. thesis</note>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Spectral matting</title>
		<author>
			<persName><forename type="first">A</forename><surname>Levin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Rav-Acha</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Lischinski</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Pattern Anal. Mach. Intell</title>
		<imprint>
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page" from="1699" to="1712" />
			<date type="published" when="2008-10">Oct. 2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<monogr>
		<title level="m" type="main">A method for nonlinear constraints in minimization problems</title>
		<author>
			<persName><forename type="first">M</forename><surname>Powell</surname></persName>
		</author>
		<editor>Optimization, R. Fletcher</editor>
		<imprint>
			<date type="published" when="1969">1969</date>
			<publisher>Academic</publisher>
			<biblScope unit="page" from="283" to="298" />
			<pubPlace>New York</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<monogr>
		<title level="m" type="main">The split Bregman algorithm for L1 regularized problems UCLA</title>
		<author>
			<persName><forename type="first">T</forename><surname>Goldstein</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Osher</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2008">2008</date>
			<biblScope unit="page" from="8" to="29" />
			<pubPlace>Los Angeles, CA</pubPlace>
		</imprint>
	</monogr>
	<note type="report_type">Tech. Rep</note>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Multiplier methods: A survey</title>
		<author>
			<persName><forename type="first">D</forename><surname>Bertsekas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Automatica</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="133" to="145" />
			<date type="published" when="1976-03">Mar. 1976</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Multichannel regularized iterative restoration of motion compensated image sequences</title>
		<author>
			<persName><forename type="first">M</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Galatsanos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Katsaggelos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Vis. Commun. Image Represent</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="244" to="258" />
			<date type="published" when="1996-09">Sep. 1996</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">LSQR: An algorithm for sparse linear equations and sparse least squares</title>
		<author>
			<persName><forename type="first">C</forename><surname>Paige</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Saunders</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Trans. Math. Softw</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="43" to="71" />
			<date type="published" when="1982-03">Mar. 1982</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Fast one-pass motion compensated frame interpolation in high-definition video processing</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Nguyen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE ICIP</title>
		<meeting>IEEE ICIP</meeting>
		<imprint>
			<date type="published" when="2009-11">Nov. 2009</date>
			<biblScope unit="page" from="369" to="372" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<monogr>
		<title/>
		<author>
			<persName><forename type="first">Middleburry</forename><surname>Stereo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dataset</forename></persName>
		</author>
		<ptr target="http://bj.middlebury.edu/~schar/stereo/web/results.php" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Disparity estimation and virtual view synthesis from stereo video</title>
		<author>
			<persName><forename type="first">J</forename><surname>Oh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Kuo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE ISCAS</title>
		<meeting>IEEE ISCAS</meeting>
		<imprint>
			<date type="published" when="2007">2007</date>
			<biblScope unit="page" from="993" to="996" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Disparity estimation algorithm for stereo video coding based on edge detecetion</title>
		<author>
			<persName><forename type="first">J</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Bao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Xia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE WCSP</title>
		<meeting>IEEE WCSP</meeting>
		<imprint>
			<date type="published" when="2009">2009</date>
			<biblScope unit="page" from="1" to="5" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">Locally adaptive support-weight approach for visual correspondence search</title>
		<author>
			<persName><forename type="first">K</forename><surname>Yoon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Kweon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE CVPR</title>
		<meeting>IEEE CVPR</meeting>
		<imprint>
			<date type="published" when="2005">2005</date>
			<biblScope unit="page" from="924" to="931" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">Real-time spatiotemporal stereo matching using the dual-cross-bilateral grid</title>
		<author>
			<persName><forename type="first">C</forename><surname>Richardt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Orr</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Davies</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Criminisi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Dodgson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ECCV</title>
		<meeting>ECCV</meeting>
		<imprint>
			<date type="published" when="2010">2010</date>
			<biblScope unit="page" from="510" to="523" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<monogr>
		<title level="m" type="main">Imaging Through Turbulence</title>
		<author>
			<persName><forename type="first">M</forename><surname>Roggemann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Welsh</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1996">1996</date>
			<publisher>CRC Press</publisher>
			<pubPlace>Boca Raton, FL</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<monogr>
		<author>
			<persName><forename type="first">J</forename><surname>Goodman</surname></persName>
		</author>
		<title level="m">Statistical Optics</title>
		<meeting><address><addrLine>New York</addrLine></address></meeting>
		<imprint>
			<publisher>Wiley-Interscience</publisher>
			<date type="published" when="2000">2000</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<monogr>
		<author>
			<persName><forename type="first">J</forename><surname>Goodman</surname></persName>
		</author>
		<title level="m">Introduction to Fourier Optics, 4</title>
		<meeting><address><addrLine>Englewood, CO</addrLine></address></meeting>
		<imprint>
			<publisher>Roberts &amp; Company Publishers</publisher>
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
	<note>th ed.</note>
</biblStruct>

<biblStruct xml:id="b57">
	<analytic>
		<title level="a" type="main">Gray-level grouping (GLG): An automatic method for optimized image contrast enhancement-Part I</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Abidi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Page</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Abidi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Image Process</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="2290" to="2302" />
			<date type="published" when="2006-08">Aug. 2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b58">
	<analytic>
		<title level="a" type="main">Gray-level grouping (GLG): An automatic method for optimized image contrast enhancement-Part II</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Abidi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Page</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Abidi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Image Process</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="2303" to="2314" />
			<date type="published" when="2006-08">Aug. 2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b59">
	<analytic>
		<title level="a" type="main">Image reconstruction from videos distorted by atmospheric turbulence</title>
		<author>
			<persName><forename type="first">X</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Milanfar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Vis. Inf. Process. Commun</title>
		<imprint>
			<biblScope unit="volume">7543</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="754" to="784S" />
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b60">
	<analytic>
		<title level="a" type="main">Super-resolution from image sequence under influence of hot-air optical turbulence</title>
		<author>
			<persName><forename type="first">M</forename><surname>Shimizu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Yoshimura</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Tanaka</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Okutomi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE CVPR</title>
		<meeting>IEEE CVPR</meeting>
		<imprint>
			<date type="published" when="2008">2008</date>
			<biblScope unit="page" from="1" to="8" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b61">
	<analytic>
		<title level="a" type="main">Spline-based image registration</title>
		<author>
			<persName><forename type="first">R</forename><surname>Szeliski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Coughlan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Int. J. Comput. Vis</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="199" to="218" />
			<date type="published" when="1997-04">Mar./Apr. 1997</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b62">
	<analytic>
		<title level="a" type="main">Probability of getting a lucky short-exposure image through turbulence</title>
		<author>
			<persName><forename type="first">D</forename><surname>Fried</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Opt. Soc. Amer</title>
		<imprint>
			<biblScope unit="volume">68</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page" from="1651" to="1657" />
			<date type="published" when="1978-12">Dec. 1978</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b63">
	<analytic>
		<title level="a" type="main">Automated video enhancement from a stream of atmospherically-distorted images: The lucky-region fusion approach</title>
		<author>
			<persName><forename type="first">M</forename><surname>Aubailly</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Vorontsov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Carhat</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Valley</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. SPIE</title>
		<meeting>SPIE</meeting>
		<imprint>
			<date type="published" when="2009">2009</date>
			<biblScope unit="volume">7463</biblScope>
			<biblScope unit="page" from="746" to="776C" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
