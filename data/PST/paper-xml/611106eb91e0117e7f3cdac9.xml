<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Pioneering Chiplet Technology and Design for the AMD EPYC™ and Ryzen™ Processor Families</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Samuel</forename><surname>Naffziger</surname></persName>
							<affiliation key="aff0">
								<address>
									<region>Inc</region>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Noah</forename><surname>Beck</surname></persName>
							<affiliation key="aff0">
								<address>
									<region>Inc</region>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Thomas</forename><surname>Burd</surname></persName>
							<affiliation key="aff0">
								<address>
									<region>Inc</region>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Kevin</forename><surname>Lepak</surname></persName>
							<affiliation key="aff0">
								<address>
									<region>Inc</region>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Gabriel</forename><forename type="middle">H</forename><surname>Loh</surname></persName>
							<affiliation key="aff0">
								<address>
									<region>Inc</region>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Mahesh</forename><surname>Subramony</surname></persName>
							<affiliation key="aff0">
								<address>
									<region>Inc</region>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Sean</forename><surname>White</surname></persName>
							<affiliation key="aff0">
								<address>
									<region>Inc</region>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Pioneering Chiplet Technology and Design for the AMD EPYC™ and Ryzen™ Processor Families</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="DOI">10.1109/ISCA52012.2021.00014</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.2" ident="GROBID" when="2023-01-01T13:26+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Chiplets</term>
					<term>Moore&apos;s Law</term>
					<term>Processors</term>
					<term>Modular</term>
					<term>Industry</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>For decades, Moore's Law has delivered the ability to integrate an exponentially increasing number of devices in the same silicon area at a roughly constant cost. This has enabled tremendous levels of integration, where the capabilities of computer systems that previously occupied entire rooms can now fit on a single integrated circuit.</p><p>In recent times, the steady drum beat of Moore's Law has started to slow down. Whereas device density historically doubled every 18-24 months, the rate of recent silicon process advancements has declined. While improvements in device scaling continue, albeit at a reduced pace, the industry is simultaneously observing increases in manufacturing costs.</p><p>In response, the industry is now seeing a trend toward reversing direction on the traditional march toward more integration. Instead, multiple industry and academic groups are advocating that systems on chips (SoCs) be "disintegrated" into multiple smaller "chiplets." This paper details the technology challenges that motivated AMD to use chiplets, the technical solutions we developed for our products, and how we expanded the use of chiplets from individual processors to multiple product families.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>I. INTRODUCTION</head><p>Moore's Law had set the pace for the semiconductor industry for decades with a reliable generation-upon-generation increase in transistor density and a corresponding reduction in cost per transistor <ref type="bibr" target="#b17">[18]</ref>. One direct consequence of Moore's Law was the steady miniaturization and integration of complex computer systems into fewer components, from roomsized computers in the first half of last century to today's mobile and wearable devices.</p><p>In recent years, the pace of Moore's Law has slowed. While new silicon process nodes continue to be introduced, the cadence has decreased compared to historical rates. However, the challenges for the semiconductor industry span much more than having to wait a little longer for more transistors. Some of these challenges include rising manufacturing costs for upfront expenses like mask sets as well as cost per chip, increased complexity of design rules in leadingedge nodes, and the architectural challenges of meeting the relentless demand for more and more computational power. We will discuss these trends and challenges in greater detail in Section II, but it is the simultaneous combination of all these challenges that compelled the reassessment of the traditional integration story.</p><p>Over the last several years, AMD has been writing a new post-Moore's Law story that revises the historical trend of integrating more functionality per silicon chip and instead is disintegrating the traditional monolithic silicon chip into multiple smaller "chiplets." Section III explains the AMD chiplet approach and in particular how it addresses the challenges of a post-Moore's Law world.</p><p>While there has been a range of activities and research in partitioning systems of chips (SoCs) into multiple silicon die over the years <ref type="bibr" target="#b3">[4]</ref>[5] <ref type="bibr" target="#b7">[8]</ref>[19] <ref type="bibr" target="#b29">[30]</ref>, and the concept of multichip modules (MCMs) dates back even further <ref type="bibr" target="#b6">[7]</ref> <ref type="bibr" target="#b21">[22]</ref>[26] <ref type="bibr" target="#b30">[31]</ref>, AMD is taking the theory of chiplet-based architectures and applying it to design real, high-volume, commercially-successful products. In Section IV, we explain how we started deploying the chiplet approach in the highperformance CPU server space. However, the post-Moore's Law challenges are not limited to the server market, and Sections V and VI discuss how coordinated chiplet designs enable significant reuse to effectively deliver solutions across a range of markets. Section VII reflects on some of the key learnings regarding what has made the chiplet approach such a success for AMD.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>II. CHIPLETS MOTIVATION</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. The Insatiable Demand for More Compute</head><p>The world's computational demands have been increasing exponentially over time. Figure <ref type="figure">1</ref> shows historical performance trends both at the component level <ref type="bibr" target="#b27">[28]</ref> and the system level (using the world's fastest supercomputers as a representative example) <ref type="bibr" target="#b28">[29]</ref>. The performance growth of the top supercomputers has been increasing at a rate faster than Moore's Law, approximately doubling in peak floating-point operations per second (FLOPS) every 1.2 years. With the advent of the world's first exascale supercomputers <ref type="bibr" target="#b12">[13]</ref> <ref type="bibr" target="#b14">[15]</ref>, the global computing ecosystem's desire for more computational power does not appear to be slowing down.</p><p>The recent scramble to understand and find effective mitigations for the SARS-CoV-2019 virus provides a very concrete and societally-relevant example of just how much more could be accomplished if the world had more, and more powerful, computational resources on hand <ref type="bibr" target="#b0">[1]</ref>. The thirst for more compute is also clear in other areas such as seen with the explosion of machine learning (ML) and the massive computational demands that come with both the training of and inferencing with the latest algorithms and models. The number of parameters in the largest ML models over the past couple of years from GPT-1 <ref type="bibr" target="#b22">[23]</ref> to DeepSpeed <ref type="bibr" target="#b23">[24]</ref> has roughly been doubling every 0.2 years, and more recently the Switch Transformer model has broken the trillion parameter threshold <ref type="bibr" target="#b9">[10]</ref>. Similarly, the amount of computational power required to train leading-edge ML models appears to be doubling approximately every 3.4 months, reflecting both increases in the model sizes and computational changes in the underlying algorithms <ref type="bibr" target="#b20">[21]</ref>. We strongly believe that the need for more compute will continue for the foreseeable future, but the computing industry faces several challenges that we now discuss in more detail.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Moore's Law is Breaking Down</head><p>The original formulation of Moore's Law was not about processor performance but rather an observation about device density, and in particular that it was doubling approximately every year <ref type="bibr" target="#b17">[18]</ref>. Over the past few decades, the rate has settled to be closer to a doubling every 18-24 months. However, for recent process nodes, the rate at which new technologies are introduced has slowed down. Figure <ref type="figure" target="#fig_1">2</ref> shows AMD internal estimates for the approximate introduction dates of major process nodes over approximately the last fifteen years <ref type="bibr" target="#b26">[27]</ref>. From the 90nm node down to 22nm, the introduction of new technologies followed a steady cadence of approximately one generation every two years. Starting with the 14nm node, we observed that the inter-node gap extended to approximately three years, and from there to the next major node the gap increased again to more than three years. The exact timing on the availability of future nodes is not yet entirely known, but it is unlikely that the industry will return to the predictable pace of yesterday's Moore's Law in a sustainable manner.</p><p>While not explicitly called out in Moore's original paper <ref type="bibr" target="#b10">[11]</ref>, an important economic consequence of Moore's Law was that each generation's device density increase came at roughly the same cost as the previous generation. Not only does device density increase exponentially, but the cost-pertransistor similarly exhibited a corresponding exponential decrease. However, over the past decade and more, the cost to manufacture an integrated chip has steadily been climbing, with a sharp increase in the latest generations due to reasons such as increased mask layers (e.g., for multiple patterning), more challenging and complex manufacturing (e.g., advanced metallurgy, new materials), and more. Figure <ref type="figure">3</ref> shows the cost for a yielded 250mm 2 die over time, normalized to the 45nm process node ("yielded" die costs includes the amortization of the expense for any defective chips) <ref type="bibr" target="#b27">[28]</ref>. The 10nm process node in Figure <ref type="figure">3</ref> was omitted because AMD transitioned from 14nm technology directly to 7nm. So not only are processor manufacturers waiting longer for each new process node, but they must also pay more when the technology becomes available.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Bigger Chips Will Not Save Us</head><p>If the rate of introduction of new, denser silicon technologies is slowing down, one possible approach is to instead build larger chips to achieve the desired higher overall device counts. In fact, this approach has already been utilized in some segments, as Figure <ref type="figure" target="#fig_2">4</ref> shows the die-size trends over time for GPUs and server CPUs <ref type="bibr" target="#b26">[27]</ref>. At the high end of the market, the higher average selling prices of products can offset the higher manufacturing costs of larger chips. However, the industry is now running up against the lithographic reticle  limit, which is a practical ceiling on how large silicon die can be manufactured.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D. The Product Portfolio Multiplier</head><p>The above discussions regarding the challenges of leadingedge silicon technologies apply not only to a single chip design, but rather the impacts may be multiplied over a company's broader portfolio of products. As an example, consider a server CPU lineup that consists of five products with 16, 24, 32, 48, and 64 cores. Each of these products could represent a separate tapeout, which comes with their own mask sets, yield and cost profiles, etc. Beyond the silicon costs, there are many additional upfront costs that must be accounted for on a per-SoC basis. For example, each unique chip requires its own physical design (e.g., floorplanning, power delivery, clocking), test and debug, validation, firmware, power and thermal management optimization, etc. Given a finite engineering budget, a possible consequence of rising costs in a post-Moore's Law world could be the reduction in the number of products that a company could offer at a time when customer demand for more and differentiated products continues to grow.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>III. CHIPLETS TO EXTEND MOORE'S LAW</head><p>The overall problem statement is that semiconductor companies need to continue delivering products with a larger number of transistors to provide customers with more functionality and computing capability at market-friendly price points, but the unraveling of Moore's Law delays the availability of the new process nodes that would deliver the additional devices, and the costs are also increasing.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. The Chiplet Approach</head><p>The overall idea with chiplets is to take what would normally be a monolithic, single-die SoC, and then partition it into multiple smaller die or "chiplets" and then "reintegrate" them with some form of in-package interconnect to enable the collective to operate as a single, logical SoC. The reason why this approach can economically make sense is that the cost of silicon is not a linear function of chip area. For example, a chip with T/2 transistors may cost considerably less than half the cost of a chip with T transistors. In general, if an SoC with T transistors can be partitioned into n separate chiplets, such that the combination of those n chiplets provides the equivalent functionality of the original T-transistor SoC while the sum of the costs of the individual chiplets plus any additional costs for reintegration (e.g., additional packaging expenses) still comes in lower than the cost of a monolithic T-transistor SoC, then a chiplet implementation for this SoC may be worthwhile.</p><p>For very large SoCs, for example those approaching the reticle limit (Figure <ref type="figure" target="#fig_2">4</ref>), the yield rates of such large chips can be economically very challenging. The top of Figure <ref type="figure" target="#fig_3">5</ref> shows an abstract depiction of a conventional manufacturing flow using monolithic SoCs. Each chip is constructed on the wafer using standard lithographic procedures to build the front-end transistors and the back-end metal layers. After chip construction, each SoC die undergoes a test procedure to determine functionality, also commonly called "known good die" (KGD) testing. Each unrepairable manufacturing fault can result in an entire SoC's worth of silicon being discarded (thereby burdening the remaining functional parts with the added costs). The SoC die that are functional can then be assembled with the final packaging solution, which results in some number of yielded processors that can now be sold.</p><p>With a chiplet-based approach, the lower portion of Figure <ref type="figure" target="#fig_3">5</ref> shows how the same hypothetical SoC has been partitioned into chiplets where each chiplet has approximately one quarter of the original SoC functionality (e.g., one fourth the core count). Each chiplet is manufactured using the same standard lithographic procedures as in the monolithic case to produce to a larger number of smaller chiplets. The individual chiplets then undergo KGD testing. Now, for the same fault distribution as in the monolithic case, each potential defect results in discarding only approximately one-fourth of the amount of silicon. The chiplets can be individually tested and then reassembled and packaged into the complete final SoCs. The overall result is that each wafer can yield a significantly larger number of functional SoCs.  Beyond functional testing, individual chiplets can also be tested for maximum performance (e.g., clock speed) <ref type="bibr" target="#b5">[6]</ref> <ref type="bibr" target="#b13">[14]</ref>. Figure <ref type="figure" target="#fig_3">5</ref> shows a few light-colored/yellow chiplets that indicate samples that can achieve higher frequencies, for example, due to parametric variations in devices across the wafer. These faster chiplets can be identified, collected together, and assembled into premium parts with all fast cores. In contrast, a monolithic chip may only have a fraction of its cores that fall within the region of the wafer with faster transistors, and as such it becomes statistically far less likely to find a monolithic chip with all fast cores. Beyond raw functional yield rates, chiplet-based assembly can also increase the supply of higher-performing products.</p><p>In addition to the yield and cost argument illustrated in Figure <ref type="figure" target="#fig_3">5</ref>, chiplets have other potential benefits. Early in the introduction of a new process technology node, the yield rates are often lower than compared to a more mature process. As such, trying to build large SoCs early in the lifetime of a new technology can be even more economically challenging. However, by utilizing a collection of smaller chiplets that yield at substantially higher rates, this can enable one to transition to a new process node earlier than would make sense with a conventional monolithic design.</p><p>Another advantage of chiplets is that the lithographic reticle limit only applies to individual chips, but multiple chiplets can be assembled such that their cumulative silicon area exceeds that possible with monolithic designs. Commercial examples of these and other advantages will be further illustrated in later sections.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Chiplets are Not a Free Lunch</head><p>While the chiplet approach to SoC construction has a lot of potential advantages, it also introduces some new costs and complexities. A chiplet design requires more engineering work upfront to appropriately partition the SoC into the right number and right kinds of chiplets. There are a combinatorial number of possibilities for partitioning an SoC, but not all may satisfy cost constraints, performance requirements, ease of IP and silicon reuse, and more.</p><p>Chiplets also require new inter-chiplet communication paths <ref type="bibr" target="#b5">[6]</ref>. Compared to on-chip metal, these interconnects involve longer routes with potentially higher impedances, lower available bandwidth, higher power consumption, and/or higher latency. The interconnect overhead may also include circuits for crossing voltage and timing domains, protocol conversions, and/or serializer-deserializers (SerDes), and these circuits all represent additional power and silicon area overheads that would not have been present in a monolithic design.</p><p>In addition to the inter-chip communication interfaces, other circuits and functionality may also need to be replicated on a per-chiplet basis. Some examples include test and debug interfaces (e.g., for testing individual die prior to assembly), clock generation and distribution, power management, onchip temperature sensors, certain types of I/O (e.g., USB, SATA), and more. As a result, the total silicon area of a single monolithic T-transistor SoC, Area(SoC(T)), will typically be less than the total area of n chiplets where each chiplet has a die area of Area(SoC(T/n)+K), where K represents the additional overheads discussed above (e.g., inter-chip interfaces, test circuitry). However, using more total silicon area for n separate chiplets may still result in lower total cost compared to a monolithic approach (Section III-A).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Why Chiplets Now?</head><p>The fundamental idea of SoC partitioning is not new, and multi-chip module (MCM) technologies have been around for decades <ref type="bibr" target="#b6">[7]</ref>[9] <ref type="bibr" target="#b21">[22]</ref>[26] <ref type="bibr" target="#b30">[31]</ref>. However, many past MCM applications have been sequestered to relatively narrower use cases and markets. What is now new is that the post-Moore's Law challenges discussed in Section II are changing the semiconductor industry landscape and creating pressures toward adopting chiplet approaches even for mainstream, high-volume markets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>IV. CASE STUDY: AMD EPYC™ PROCESSORS</head><p>AMD EPYC™ server CPUs are our first products to utilize a modern chiplet-based design methodology. In this section, we discuss two generations of AMD EPYC™ processors to highlight the evolution in the approach and share some of the design decisions and benefits.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. First-generation AMD EPYC™ Processor</head><p>AMD market analysis and our product definition process set a target of 32 cores for our first-generation AMD EPYC™ processors, formerly codenamed "Naples," to compete in the server CPU market. In addition to raw core count, we also targeted an SoC supporting eight DDR4 memory channels and 128 lanes of PCIe® gen3 I/O to provide industry-leading memory and I/O bandwidths, both of which are frequently high priorities for server, cloud, and enterprise use cases. Figure <ref type="figure" target="#fig_4">6</ref>(a) shows a schematic of a hypothetical monolithic 32-core processor. Based on our internal analysis and product planning exercises, such a processor would have required 777mm 2 of die area in a 14nm process. While still within the reticle limit and therefore technically manufacturable, such a large chip would have been very costly and put the product in a potentially uncompetitive position.</p><p>The first-generation AMD EPYC™ processor utilized a design with four identical chiplets, codenamed "Zeppelin," shown in Figure <ref type="figure" target="#fig_4">6</ref>(b) <ref type="bibr">[3][6]</ref>. Each chiplet provides eight "Zen" CPU cores, two channels of DDR4 memory, and 32 lanes of PCIe such that the combination of the four chiplets meets our product requirements. Additional die area was also required to implement our Infinity Fabric™ interconnect between the four chiplets <ref type="bibr" target="#b5">[6]</ref> as well as other per-chip circuitry as discussed in Section III-B. As a result, each chiplet had a die area of 213mm 2 in a 14nm process, for a total aggregate die area of 4u213mm 2 = 852mm 2 . This represents a ~10% die area overhead compared to the hypothetical monolithic 32core chip. Based on AMD-internal yield modeling using historical defect density data for a mature process technology, we estimated that the final cost of the quad-chiplet design is only approximately 0.59u of the monolithic approach despite consuming approximately 10% more total silicon.</p><p>Beyond the cost savings for the 32-core product, the chiplet approach also provides a flexible platform for reusing the chiplets across multiple product offerings. For example, after testing individual chiplets, some may have four or fewer cores that have been rendered inoperable due to manufacturing defects <ref type="bibr" target="#b19">[20]</ref>. However, these four "harvested" chiplets (each still with four functional cores) can be assembled into a 16-core processor. Due to the chiplet-based design, this 16core processor can also be "fully featured" with all eight DDR4 memory channels and 128 PCIe I/O lanes. Network and storage-oriented use cases may only need a moderate number of CPU cores to keep the storage and/or networking fully utilized; any additional cores beyond that represent unnecessary costs for the end user.</p><p>Harvesting is highly advantageous for several reasons. The first is that we can utilize a higher fraction of the total number of chips per wafer even in the face of some manufacturing defects. The second is that with just a single chiplet design (i.e., one mask set, one tapeout), we can deliver multiple different products that traditionally would have required multiple separate unique SoCs. Third, the chiplet approach makes it more practical to offer products with a full complement of memory and I/O capabilities. In contrast, a dedicated 16-core monolithic SoC might not by itself be able to tolerate the cost burden of the additional die area needed for so many memory channels and I/O lanes. However, the lower cost of the individual chiplets combined with the ability to amortize the additional memory and I/O interfaces over multiple products can make this an economically viable approach while enabling valuable product differentiation to meet customer needs.</p><p>The multi-chiplet design of the first-generation AMD EPYC™ processor introduces additional interconnect latency when chiplets need to communicate across the Infinity Fab-ric™ on-package (IFOP) interconnect, which are implemented as point-to-point links directly on the organic package substrate <ref type="bibr" target="#b5">[6]</ref>. The IFOP links utilize custom high-speed SerDes circuits. Compared to SerDes for off-package I/O like PCIe gen3, which consumes approximately 11pJ per bit, the IFOP SerDes have been carefully optimized for shorter package substrate route lengths and achieves a power efficiency of ~2pJ per bit. Transmitting data over the IFOP links still represents a power overhead compared to a monolithic chip, where on-chip interconnect power is typically much less than 1pJ per bit, with the exact power cost depending on the route length and other factors.</p><p>For the eight CPU cores on a given chiplet, only two out of the eight total DDR4 memory channels are resident on the same chiplet. This means that in the absence of non-uniform memory access (NUMA) data management and thread pinning, some memory requests must be serviced by "remote" memory channels, as shown in Figure <ref type="figure" target="#fig_5">7</ref>.</p><p>Based on AMD internal testing with requests that generate DRAM page misses, the typical latency for a memory request to a local memory channel (i.e., on the same chiplet) was measured to be 90ns, whereas accesses to remote memory channels (i.e., different chiplet, same socket) incur a latency of 141ns. The additional latency is due to the round-trip combination of the IFOP links and additional hops through each chiplet's local data fabric (also known as a network-on-chip or NoC). For a memory access pattern uniformly interleaved across the eight memory channels, this results in an average memory latency of 128ns. These intra-socket NUMA effects are one of the items (among others) that our second-generation AMD EPYC™ processor addresses.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Second-generation AMD EPYC™ Processor</head><p>The first-generation AMD EPYC™ processor was very well received by the market, and we set our sights to be even more aggressive for our second generation of server CPU products <ref type="bibr" target="#b19">[20]</ref>. The timing of the second-generation AMD EPYC™ processors also aligned with the dawn of 7nm silicon technology, which provided both benefits and challenges that required new innovations in our chiplet methodology.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>1) 7nm Benefits and Cost Challenges:</head><p>Compared to the 14nm process technology used for the first-generation AMD EPYC™ processor, the 7nm process that was becoming available was very promising from a device perspective. Based on AMD-internal analysis of a 14nm product ported to a 7nm node with a similar implementation flow and design methodology, we projected that we could obtain a doubling in transistor density for the core logic. The 7nm devices also delivered significant improvements in transistor speed and power efficiency. From the same study, we projected that at the same power consumption, the 7nm version could deliver over 25% more performance, or for the same performance, power could be reduced by approximately one half. Starting from our previous 32-core design point, the increase in device density with a corresponding power-efficiency improvement meant that a 64-core product might be within striking distance. However, Figure <ref type="figure">3</ref> showed that the transition to the 7nm technology node also comes with increasing die cost, and so further innovation was required.</p><p>2) Hybrid Multi-die Architecture: We initially started by analyzing the "Zeppelin" chiplet and considered a hypothetical 7nm version of it. We found that such an approach would be challenging to make work with the higher cost of the latest silicon technology. In particular, high-performance server products demand a lot of memory and I/O, which occupy a significant fraction of the first-generation chiplet. Unfortunately, most of these structures do not scale well with shrinking device geometries either due to the analog devices or from being limited by the bump pitch of the external I/O connections. As shown in Figure <ref type="figure" target="#fig_6">8</ref>, the CPU cores, L3 cache, and other logic account for approximately 56% of the "Zeppelin" chiplet area. Rather than halving the die size from going to 7nm, we would only achieve approximately a 28% reduction (i.e., 56%/2 + 44%=72%). When considering the relative cost increase of transitioning from 14nm to 7nm, this 28% die size reduction might not be sufficient to even reach a cost break-even point compared to the original 14nm "Zeppelin" chiplet.</p><p>Instead, the second-generation AMD EPYC™ processor, formerly codenamed "Rome," utilizes a dual-chiplet approach. The first chiplet, called the I/O die (IOD), was implemented in a mature and cost-effective 12nm process. The IOD has a size of 416mm 2 with 8.34 billion transistors, and it contains the full complement of eight DDR4 memory channels, 128 lanes of PCIe gen4 I/O, other I/O such as USB and SATA, the SoC data fabric, and other system-level functionality. The second chiplet, the core-complex die (CCD), was implemented in the leading edge 7nm node. Each CCD is only ~74mm 2 in size, leading to very good yield rates even in the early days of a new process node. Figure <ref type="figure" target="#fig_7">9</ref>(a) shows how one IOD can be assembled with up to eight CCDs. Each CCD provides eight "Zen 2" CPU cores, and so all together this arrangement can enable an impressive 64 cores in a single socket. The CCD attempts to utilize as much of the highperformance, but more costly, 7nm silicon for the functions that benefit from the advanced devices, namely the eight CPU cores and the L3 cache. Only a small portion of the CCD area is consumed for the IFOP, which is placed centrally on the chiplet to minimize distance from the L3 cache banks. Figure <ref type="figure" target="#fig_7">9</ref>(b) shows how the CCD improves the utilization of the 7nm chiplet so that the CPU cores and L3 caches now account for 86% of the total chiplet area.</p><p>3) Packaging Technology Decisions: AMD was among the first companies to commercially introduce silicon interposer technologies starting with the AMD Radeon™ R9 "Fury" GPUs with high-bandwidth memory (HBM) in 2015 <ref type="bibr" target="#b15">[16]</ref>. A   Even accounting for some load imbalance across the CCDs, a single CCD's IFOP would still be expected to observe no more than a few tens of GB/s of traffic, and in fact each link can support approximately 55GB/s of effective bandwidth. Point-to-point links in the package substrate routing layers are more than sufficient to handle this modest level of bandwidth. In contrast, a single HBM stack can deliver hundreds of GB/s of memory bandwidth, which far exceeds the capabilities of the organic package substrate, and this is why HBM-enabled GPU products need a higher-bandwidth solution such as silicon interposers <ref type="bibr" target="#b1">[2]</ref>[16] <ref type="bibr" target="#b16">[17]</ref>.</p><p>The second factor against silicon interposers for our chiplet-based processors is the reach of the interposer-based interconnects. While interposers can provide great signal density for very high bandwidths, the lengths of the signals are limited and as such constrain the connections to edge-toedge links. The reach of interposer-based interconnects can in principle be extended using wider metal routes and greater spacing between routes, but this would decrease the effective bandwidth per interface because fewer total routes could be supported for a fixed width of routing tracks. This argument also applies to silicon bridge technologies <ref type="bibr" target="#b11">[12]</ref>. The next subsection describes the challenges of providing sufficient IFOP bandwidth across the package substrate. Figure <ref type="figure" target="#fig_9">10</ref> illustrates a hypothetical interposer-based processor design. The edgeconnectivity constraint would limit the architecture to only four CCDs, which would render the product concept to be far less compelling. Even if interconnect reach was not a limiting factor, the IOD and the eight CCDs would require so much area that the underlying interposer would greatly exceed the reticle limit (while a passive interposer does not contain any transistors, the metal layers are still lithographically created and therefore must stay within the same reticle field constraints). Figure <ref type="figure" target="#fig_9">10</ref> shows the placement where an additional CCD would have to be, which is both outside the boundary of a maximum-sized interposer and too far for the unbuffered interposer routes to reach while supporting required bandwidths. Recent advancements in silicon interposer manufacturing have enabled reticle stitching to create very large interposers <ref type="bibr" target="#b10">[11]</ref>, but such an approach would have been cost prohibitive for this market segment. Last, the silicon interposer itself adds more cost to the overall solution. A CCD with the twice the core count could have been used, but that would have resulted in lower yield and decreased configurability. For all these reasons, routing IFOP directly across the package substrate was chosen for this product family.</p><p>The total area consumed by multiple chiplets is typically greater than a monolithic chip with equivalent functionality. While this could theoretically cause a corresponding increase in the overall package size, the size of the SP3 processor package used by AMD EPYC™ processors is primarily determined by the large number of package pins required to support the eight DDR memory channels, 128 lanes of PCIe plus other miscellaneous I/O, and all the power and ground connections.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>4) Chiplet-Package Co-design Challenges:</head><p>While routing on the package substrate was the only practical option, that does not imply that it was simple to do. This section discusses some of the challenges and the solutions related to package-level routing and power delivery. These are topics that are often outside the attention of many computer architecture researchers, but we wish to highlight how high-level architecture decisions can have major downstream impacts on the rest of the overall design.</p><p>The package routing layers are already heavily utilized not only for IFOP but also for external I/O connections, escaping out the multiple DDR4 channels, and delivering power and ground across the entire package. Figure <ref type="figure" target="#fig_11">11(a)</ref> shows a schematic view of the first-generation AMD EPYC™ processor's package routing for the DDR4 memory buses, PCIe I/O lanes, and IFOP. Figure <ref type="figure" target="#fig_11">11(b)</ref> shows how the package routing resources have already been consumed by the first-generation AMD EPYC™ processor <ref type="bibr" target="#b2">[3]</ref>. To enable the necessary package-level connectivity for our second-generation architecture, significant co-design was required between our packaging and silicon teams.</p><p>Figure <ref type="figure" target="#fig_11">11</ref>(c) shows the package routing for the secondgeneration AMD EPYC™ processor. The overall layout and floorplan for the CCDs, the IOD, and the package routing had to be coordinated from the outset. Note that to provide our customers with the option for seamless upgrades, the first and second-generation AMD EPYC™ processors are designed to be socket compatible. However, that creates a challenge to providing connectivity to all the CCDs because we cannot increase the package size to make room for more routes. There are four "inner" CCDs directly adjacent to the central IOD, and then there are four more "outer" CCDs toward the top and bottom edges of the package. The enabling co-design innovation was finding a way to route the IFOP directly underneath the CCDs. Figure <ref type="figure" target="#fig_11">11(c)</ref> shows how this opens clear routing paths between the two columns of CCDs to allow the PCIe routes to escape to the top and bottom edges of the package. In a similar fashion, the multiple channels of DDR4 can escape directly from the IOD to the right and left edges of the package.</p><p>Unfortunately, routing underneath the CCDs only works if there are available routing resources in the package.</p><p>Figure <ref type="figure" target="#fig_10">12</ref>(a) shows a simplified view of the VDDM power delivery on the "Zeppelin" chiplet (VDDM is a regulated power supply rail supporting the L3 cache SRAM arrays) <ref type="bibr" target="#b24">[25]</ref>. The real core supply (RVDD) is not shown for clarity, but it is also distributed in the package layer as input to the on-chip regulators. VDDM is delivered by a low-drop out (LDO) linear regulator, which drives the power down to the thick copper layers in the package substrate to help distribute VDDM across the entire area of the L3 cache. The power can then be delivered back up to power the L3 cache. The lowresistance package layers are very effective for power distribution, but unfortunately this is where we also want to route IFOP for the second-generation architecture.</p><p>Figure <ref type="figure" target="#fig_10">12</ref>(b) shows how the package and CCD were jointly co-designed to simultaneously address power delivery requirements while freeing up the necessary package routing resources to enable IFOP to tunnel underneath the CCDs. The VDDM distribution was brought on to the CCD and it instead utilizes redistribution layer (RDL) metal. The challenge was that the RDL metal is more resistive than the thick copper layers in the package, and so the span (power distribution distance) of the LDOs had to be reduced. Another packaging-related challenge was that the chiplets manufactured in different process nodes normally would have utilized different bump pitches to connect the chiplets to the package substrate. In particular, the 12nm IOD bump pitch is 150Pm, while the 7nm CCD bump is 130Pm. The result would have been that the different types of chiplets would be at different heights after assembly, which could potentially be problematic for providing a level and uniform interface to the cooling solution. To address this, we engineered a copper pillar-based bump for the IOD, which was also compatible with the 7nm CCDs, to ensure uniform postassembly chiplet heights. The transition to a copper-pillar package interface also provided denser bump pitches and enabled higher maximum current (electromigration) limits.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>5) Improved Memory Performance:</head><p>The first-generation AMD EPYC™ processor's organization with memory and The overall layout of the second-generation AMD EPYC™ processor's chiplets resembles a star topology, which provides much more uniform memory access latencies. Each memory request from a CCD takes a direct hop to the IOD, and then from there the high-performance data fabric routes the request to one of the eight targeted memory channels. Note that some memory channels are still closer or farther from each of the CCDs and so some NUMA effects remain, but they are greatly reduced compared to the prior generation approach. Figure <ref type="figure" target="#fig_13">13</ref> shows the overall IOD data fabric topology. The data fabric utilizes a hybrid ring-ladder topology, where the demand memory requests are routed along the external ring, and the I/O traffic moves along the interior "ladder" of the data fabric ( IS in the figure is for inter-socket (IS) traffic for symmetric multiprocessing (SMP) platforms). Figure <ref type="figure" target="#fig_13">13</ref> also shows the latencies from one of the CCDs to memory channels in each of the four corners of the IOD (measured on an AMD EPYC™ 7002 Series processor with DDR2933 memory with DRAM page-missing traffic at low load). In contrast to the first-generation organization where some memory requests could be locally serviced on a given chiplet without any IFOP hops, in the second-generation approach all requests always require an IFOP hop to get from the CCD to the IOD. Despite this mandatory IFOP hop, the "local" memory access latency (labeled c in Figure <ref type="figure" target="#fig_13">13</ref>) is only 4ns slower than the same-chiplet memory access in the first-generation architecture (i.e., 94ns versus 90ns) due to intense engineering efforts to squeeze out cycles along the memory paths. The path to the next-closest pair of memory channels d incurs approximately 3ns of additional latency roundtrip, or approximately 4 FCLKs (fabric clocks). In the worst case to access the memory channels on the opposite corner of the IOD f, the data fabric adds approximately 7 FCLKs in each direction, or approximately 20ns roundtrip assuming a 1.46GHz FCLK. Note that this is a ~30ns improvement over the inter-chiplet memory latency of the first-generation AMD EPYC™ processor. For a memory access pattern uniformly distributed across all eight channels, the average memory latency improves by ~24ns (~19% reduction), and the difference between nearest and farthest memory channels improves from ~51ns down to ~20ns, which represents a ~61% reduction in the variance/range of in-socket NUMA effects of the overall design.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>6) Overall Cost and Performance:</head><p>An objective of the second-generation AMD EPYC™ processor was to enable an architecture that delivered scalable performance with a cost structure that also scaled linearly with the capability of the system (in contrast to monolithic chips where the cost scales super-linearly with SoC function/die size). Figure <ref type="figure" target="#fig_14">14</ref> shows the relative cost of five different possible configurations ranging from 64 cores down to 16 cores. For comparison, we also show the projected cost for hypothetical monolithic SoCs with equivalent core counts. Note that no cost is shown for a monolithic 64-core SoC because the total chip size would be greater than 1000mm 2 , which greatly exceeds the reticle limit.</p><p>The chart illustrates a few important trends that really demonstrate the power and value of our chiplet methodology. The first is that across all configurations, the final silicon cost is significantly lower than any of the monolithic equivalents. The second is that the cost scales linearly with a gentle slope  as the core count is varied. The bottom portion of Figure <ref type="figure" target="#fig_14">14</ref> also illustrates how the different core counts can be achieved by simply depopulating CCDs from the package. This visually shows how with only two tapeouts (and only one of those being in the leading-edge 7nm node), we are able to flexibly enable an entire server product stack including the 64-core option that would otherwise be both technologically and economically impractical to manufacture.</p><p>Figure <ref type="figure" target="#fig_3">15</ref> shows a comparison between the first-generation and second-generation AMD EPYC™ processors. The chiplet approach enabled a doubling in total core count per socket. The total number of transistors more than doubled, with a total count of over 38 billion devices, although total silicon area in the package only increased a modest 18%. This metric simply counts total mm 2 of silicon and does not treat the denser 7nm silicon any differently than 14nm, and so the relatively small increase in total silicon highlights the density advantage of the 7nm process node. Finally, Figure <ref type="figure" target="#fig_3">15</ref> shows the overall performance as measured on SPEC-rate® for double-socket (2P) server platforms 1 . The performance uplift is a combination of the doubled core count, higher clock speeds, higher IPC of the newer "Zen 2" microarchitecture, and a higher supported power limit (TDP) for the second-generation products.</p><p>Beyond enabling new levels of performance, another benefit of our combined CCD and IOD approach is that it enables more flexible inventory management. With conventional monolithic SoCs, the relatively long lead time of modern silicon manufacturing forces a company to forecast how many of each part should be ordered. If too few of a popular part gets ordered, then a company could potentially face shortages which could potentially lead to missed revenue opportunities as well as customers potentially buying products from competitors. If too many parts are ordered, then a company may be faced with excess inventory that might become challenging to move. The second-generation AMD EPYC™ processor enables a later-binding approach, where we can wait until after the chips have been returned from manufacturing to assemble the chiplets into 16, 24, …, 64-core parts. If market demands shift and we need to produce more of one part or another, we have the potential to be agile and assemble more or fewer chiplets to increase the supply of the desired products.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>V. CASE STUDY: AMD RYZEN™ PROCESSORS</head><p>The chiplet-based architecture of the second-generation AMD EPYC™ processor was a significant feat of siliconpackage co-design and proved to be highly effective for addressing the needs of our server, enterprise, and high-performance computing customers. However, AMD also produces many products to serve a wide range of other target markets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. AMD Ryzen™ Processor Organization</head><p>The "Zeppelin" chiplet is a complete SoC with cores, memory, I/O, and all system functionality for standalone operation. For the first-generation AMD Ryzen™ processors, we were able to take a single "Zeppelin" chiplet and put it in a client AM4 package to provide a desktop processor with eight cores, two DDR4 memory channels, and 24 lanes of I/O.  We later used the silicon from the second-generation AMD EPYC™ processor to create the third-generation AMD Ryzen™ processor with two CCDs and a "client IOD" (cIOD), shown in Figure <ref type="figure" target="#fig_4">16</ref>. The CCDs directly utilize the same silicon design as that used by the server products. The cIOD is a new die, but it heavily leverages the server IOD design. Figure <ref type="figure" target="#fig_4">16</ref> shows how the 125mm 2 , 2.09 billion transistor cIOD is effectively a quarter-sized version of the server IOD, which features two DDR channels, 32 lanes of PCIe I/O, and two IFOP ports to the CCDs. result is an industryleading 16-core high-performance desktop processor without requiring a new 7nm tapeout and a cost-effective and highlyleveraged cIOD. Analogous to the impracticality of a monolithic 64-core server processor, a 16-core desktop processor would likely not be economically practical if implemented as a monolithic die, but chiplets make it possible. Similar to the server products, the overall architecture of the third-generation AMD Ryzen™ processor enables product definition flexibility by simply reducing the number of CCDs to one.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Overall Cost and Performance</head><p>The modular chiplet design approach of the second-generation AMD Ryzen™ processor provides the same types of cost savings and scalability benefits as we were able to achieve with the second-generation AMD EPYC™ processors. Figure <ref type="figure" target="#fig_16">17</ref> shows the relative die cost of both 16-core (two CCDs) and 8-core (one CCD) chiplet implementations compared to hypothetical monolithic 7nm designs. These results only show the silicon costs and do not reflect the additional engineering benefits from reusing the CCDs and server IOD IP in terms of amortization of both design and verification efforts, product configurability, and time-to-market.</p><p>Figure <ref type="figure" target="#fig_17">18</ref> shows a comparison between first-and secondgeneration AMD Ryzen™ processors. Similar to the comparison of the AMD EPYC™ processors, the chiplet approach and 7nm technology enabled a doubling of core count. Another similar trend is the doubling in overall transistor count while total silicon area increases by only 28%. Performance is reported using the Cinebench R20 benchmark. The single-threaded (1T) performance improvements come primarily from enhancements to the "Zen 2" core microarchitecture and increased clock speeds (both parts were measured with the same 105W TDP). Maximum 1T clock frequency is very difficult to increase, and the "Zen 2" microarchitecture combined with 7nm enabled a critical 400MHz improvement. The all-cores (NT) performance results highlight the benefits of our chiplet approach. The maximum all-cores clock speed is similar between the two generations of processors 2 , but the doubling in core count is responsible for the majority of the NT performance gains, with the rest coming from the core IPC uplift, memory system improvements, power-performance efficiency optimizations, and other enhancements.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Additional Chiplet Optimizations</head><p>In Section III we discussed how prior to assembly, individual chiplets can be tested to match up parts with similar performance. However, parametric variations still exist within each individual chiplet, leading to some potential differences between cores. This effect can be magnified by the higher core counts that our chiplet design approach enables, and we have observed up to 200MHz variations in the maximum clock frequency (Fmax) across cores in the same CCD. While legacy boost techniques did not take advantage of the faster cores, we now utilize an algorithm that characterizes each CCD's cores at boot time to generate a list of cores in order of Fmax capabilities. As shown in Figure <ref type="figure" target="#fig_18">19</ref>, this list is made   available to the operating system (OS), so that when the OS runs a single high-performance thread it can schedule the program to the highest performing core. Each eight-core CCD consists of a pair of core complexes (CCX) each with an L3 cache and four associated cores. Like the characterization, we also determine the best-performing CCXs, which allows the OS to schedule threads from a multithreaded workload to the fastest CCX.</p><p>The core and CCX characterization processes occur every time a system is booted. This enables the characterization processes to adapt to the specific system that the processor is seated in; for example, systems may be equipped with different cooling solutions or board-level components such as voltage regulators themselves may vary across instances. An additional benefit of boot-time characterization is that as processors age, the preferred core may change over time and so we can continue to deliver the highest performance possible for each part by selecting the fastest core under the current circumstances.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>VI. EXPANDING THE CHIPLET APPROACH</head><p>The chiplet-based architectures of the second-generation AMD EPYC™ processor and AMD Ryzen™ processor were highly effective for addressing the needs of our server and client offerings. However, chiplets also provided opportunities in a range of other target markets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. AMD Ryzen™ Threadripper™ Processors</head><p>AMD identified opportunities for extremely high-performance, high-core-count processors for high-end desktops (HEDT) and workstations. In particular, many professional content creation tasks such as computer-aided design and high-resolution video rendering (e.g., for Hollywood feature movies) require massive CPU resources to, for example, reduce movie rendering times for higher end-user productivity. Figure <ref type="figure" target="#fig_20">20</ref> shows how AMD leveraged the first-generation AMD EPYC™ processor by depopulating two of the four chiplets and replacing them with dummy silicon die to preserve the mechanical integrity of the overall package, combined with some additional customizations to the package, firmware, BIOS, etc., to create first-generation AMD Ryzen™ Threadripper™ processors with up to 16 cores. In subsequent generations, additional chiplets were enabled, and later we also utilized the chiplet designs from the second-generation AMD EPYC™ processor for 64-core HEDT offerings. This is another interesting example of how creative co-design of both silicon chiplets and packaging enabled AMD to quickly react to emerging market data and new opportunities.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. AMD EPYC™ Embedded Processors</head><p>The high-performance embedded processor market has a range of applications with requirements for substantial I/O needs coupled with a range of desired core counts. Example applications include embedded networking, storage, medical imaging, and industrial control solutions. Figure <ref type="figure" target="#fig_21">21</ref> shows how our "Zeppelin" chiplet designs combined with packaging optimized for high-performance embedded use cases can produce additional products appropriate for this market. The first generation of AMD EPYC™ Embedded 3000 series processors include single-and dual-chiplet configurations with up to 16 cores in a package designed to be appropriate for the target embedded market form factors. Again, a single chiplet design can enable multiple product options. In a similar fashion, chiplet designs from the second-generation AMD EPYC™ server processors have been leveraged for the AMD EPYC™ 7000 series embedded processors.   </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. The AMD X570 Chipset</head><p>Motherboards for high-performance processors such as those demanded by gamers, content creators, and other enthusiasts typically utilize chipsets to provide a full suite of I/O capabilities. Features often include additional PCIe lanes, USB ports, NVMe interfaces, and support for more SATA drives. To provide chipset support for the third-generation AMD Ryzen™ processor, we were able to directly leverage the cIOD without any CCDs in a standalone package. This is enabled by robust harvesting techniques for the cIOD chiplets, secure firmware, and other non-silicon support. Figure <ref type="figure" target="#fig_19">22</ref> illustrates the silicon reuse, which provides the platform with 16 PCIe gen4 I/O lanes, up to 12 SATA ports, and twelve USB ports all in addition to the PCIe, USB, and storage I/O from the AMD Ryzen™ processor itself.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>VII. CONCLUSIONS</head><p>Chiplet-based design has transformed architecture at AMD. Figure <ref type="figure" target="#fig_1">23</ref> shows an illustration for how our chiplet-enabled approach enables a wide variety of products using a few individual chiplet designs. We have been able to creatively combine chiplets with a range of packaging solutions to produce a rich portfolio of products across a wide range of markets, create solutions that would be infeasible with monolithic designs (e.g., 64-core server), and do so while delivering great performance and value.</p><p>This paper aimed to demonstrate to the community how modular architectures can address a range of challenges in a post-Moore's Law world. At the same time, we wanted to provide a glimpse into how technology and architecture decisions not only consider traditional metrics like performance, power, and cost, but also how the decisions can help and support the construction of a diverse portfolio of products. Examples of engineering challenges such as the required silicon-package co-design for under-CCD routing are reminders that one does not simply take disparate pieces of silicon and "glue" them into a complete system. Significant thought, planning, collaboration, engineering, and creativity are needed to successfully bring all the pieces together.</p><p>In addition to the technical challenges, implementing such a widespread chiplet approach across so many market segments requires an incredible amount of partnership and trust across technology teams, business units, and our external partners. The product roadmaps across markets must be carefully coordinated and mutually scheduled to ensure that the right silicon is available at the right time for the launch of each product. Unexpected challenges and obstacles can arise, and world-class and highly passionate AMD engineering teams across the globe have risen to each occasion. The success of the AMD chiplet approach is as much a feat of engineering as it is a testament to the power of teams with diverse skills and expertise working together toward a shared set of goals and a common vision.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 3 .Figure 1 .</head><label>31</label><figDesc>Figure 3. Normalized cost per chip versus technology node.</figDesc><graphic url="image-1.png" coords="2,315.23,84.32,241.57,102.07" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 .</head><label>2</label><figDesc>Figure 2. Process technology node introduction over time.</figDesc><graphic url="image-4.png" coords="2,54.23,245.90,241.57,93.01" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 4 .</head><label>4</label><figDesc>Figure 4. Die size increases for large SoCs over time; die sizes are reaching the reticle limit and further increases are not feasible.</figDesc><graphic url="image-5.png" coords="3,55.13,73.94,241.87,117.55" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 5 .</head><label>5</label><figDesc>Figure 5. Illustrative construction of processors using (top) monolithic die and (bottom) reassembled chiplets. The light/yellow chiplets represent chiplets that can run at higher clock speeds.</figDesc><graphic url="image-6.png" coords="3,320.63,73.94,240.19,103.15" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 6 .</head><label>6</label><figDesc>Figure 6. Hypothetical monolithic 32-core chip compared to an assembly of four eight-core chiplets. (a) (b)</figDesc><graphic url="image-7.png" coords="4,314.57,78.14,240.55,115.93" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 7 .</head><label>7</label><figDesc>Figure 7. Connectivity of the four "Zeppelin" chiplets and memory latencies for local, remote, and average requests.</figDesc><graphic url="image-9.png" coords="5,54.23,75.50,242.05,166.45" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 8 .</head><label>8</label><figDesc>Figure 8. A significant fraction of the "Zeppelin" chiplet area is dominated by DDR PHYs, SerDes, and other I/O that hardly reduce in size from moving to a 7nm process.</figDesc><graphic url="image-10.png" coords="6,52.49,74.24,242.59,81.49" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure 9 .</head><label>9</label><figDesc>Figure 9. (a) Second-generation AMD EPYC™ processor consisting of a 12nm IOD die and up to eight 7nm CCDs, (b) CCD detail showing majority of chiplet area occupied by cores and L3 cache.</figDesc><graphic url="image-11.png" coords="6,56.99,199.52,233.77,140.41" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head></head><label></label><figDesc>our chiplet-based products is why we chose to use package substrate routing rather than the higherdensity interconnects enabled by silicon interposers. There are several factors that drove the decision to not use silicon interposers for our chiplet-based processors. The first is the communication requirements of our chiplets. With eight CCDs and eight memory channels, on average each chiplet's IFOP only needs to handle approximately one DDR4 channel's worth of bandwidth. Using DDR4-2933 as an example, a single channel would correspond to ~23.5 GB/s of peak bandwidth.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>Figure 10 .</head><label>10</label><figDesc>Figure 10. Hypothetical silicon interposer-based chiplet architecture. Maximum interposer size and unbuffered interposer routing limits configuration to four CCDs.</figDesc><graphic url="image-12.png" coords="7,95.81,73.28,157.39,124.51" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head>Figure 12 (</head><label>12</label><figDesc>b) shows how the LDOs have been reoriented and distributed along the sides of the L3 cache. The LDOs drive power down to the RDL, and then only need to fan out a relatively shorter distance through the resistive RDL routes. This carefully choreographed chip-package layout kept the VDDM IR drop to within 10mV while freeing up the package routing layers (labeled "Signal Routes" at the bottom of Figure 12(b)) for IFOP to pass through.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_11"><head>Figure 11 .</head><label>11</label><figDesc>Figure 11. (a) Schematic view of first-generation AMD EPYC™ processor package routing for DDR (red), I/O (orange), and Infinity Fabric™ links (cyan), (b) multi-layer package routing layout of the first-generation AMD EPYC™ processor package, and (c) annotated package routing layout of the secondgeneration AMD EPYC™ processor package. (a) (b) (c)</figDesc><graphic url="image-13.png" coords="8,54.23,72.86,319.51,210.43" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_12"><head>Figure 12 .</head><label>12</label><figDesc>Figure 12. (a) "Zen" CPU VDDM distribution via the package plane, (b) "Zen 2" CPU VDDM distribution via RDL only.</figDesc><graphic url="image-16.png" coords="9,56.39,83.72,250.09,160.15" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_13"><head>Figure 13 .</head><label>13</label><figDesc>Figure 13. IOD data fabric topology and measured memory latencies to the four quadrants.</figDesc><graphic url="image-17.png" coords="9,321.89,72.20,220.09,191.71" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_14"><head>Figure 14 .</head><label>14</label><figDesc>Figure 14. Normalized processor costs as core counts vary compared to hypothetical monolithic die, and visualization of core-count configuration by varying the number of CCDs.</figDesc><graphic url="image-18.png" coords="10,76.19,73.94,220.09,171.97" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_15"><head>Figure 15 .Figure 16 .</head><label>1516</label><figDesc>Figure 15. Comparison of first-and second-generation AMD EPYC™ processors.</figDesc><graphic url="image-19.png" coords="10,55.19,290.24,247.09,68.71" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_16"><head>Figure 17 .</head><label>17</label><figDesc>Figure 17. Normalized AMD Ryzen™ processor costs as core counts vary compared to hypothetical monolithic die.</figDesc><graphic url="image-21.png" coords="11,70.97,76.34,211.63,97.51" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_17"><head>Figure 18 .</head><label>18</label><figDesc>Figure 18. Comparison of first-and second-generation AMD Ryzen™ processors.</figDesc><graphic url="image-22.png" coords="11,314.09,88.34,240.97,76.87" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_18"><head>Figure 19 .</head><label>19</label><figDesc>Figure 19. Boot-time characterization of each CCD's cores and core complexes (CCX) enable better utilization of the many cores in the system.</figDesc><graphic url="image-23.png" coords="11,343.13,534.74,185.29,120.97" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_19"><head>Figure 22 .</head><label>22</label><figDesc>Figure 22. Reuse of the cIOD chiplet enables a fully-featured PCIe 4.0 chipset for AMD Ryzen™ processor motherboards.</figDesc><graphic url="image-24.png" coords="12,316.73,564.98,240.49,122.23" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_20"><head>Figure 20 .</head><label>20</label><figDesc>Figure 20. First-generation AMD Ryzen™ Threadripper™ processor created with two "Zeppelin" chiplets, two dummy die, and new package routes.</figDesc><graphic url="image-25.png" coords="12,94.91,72.20,169.87,115.57" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_21"><head>Figure 21 .</head><label>21</label><figDesc>Figure 21. AMD EPYC™ 3000 Series embedded processors utilizing one or two chiplets.</figDesc><graphic url="image-26.png" coords="12,332.15,72.20,208.45,113.59" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0"><head></head><label></label><figDesc></figDesc><graphic url="image-27.png" coords="13,61.13,73.76,491.77,256.39" type="bitmap" /></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" xml:id="foot_0">Authorized licensed use limited to: Tsinghua University. Downloaded on December 31,2022 at 07:18:24 UTC from IEEE Xplore. Restrictions apply.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" xml:id="foot_1">Authorized licensed use limited to: Tsinghua University. Downloaded on 31,2022 UTC from IEEE Xplore. Restrictions apply.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" xml:id="foot_2">Authorized licensed use limited to: Tsinghua University. Downloaded on December 31,2022 at 07:18:24</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" xml:id="foot_3">Authorized licensed use limited to: Tsinghua University. Downloaded on December Restrictions apply.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" xml:id="foot_4">Authorized licensed use limited to: Tsinghua University. Downloaded on December 31,2022 Xplore. Restrictions apply.</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>ACKNOWLEDGMENTS</head><p>The authors would like to thank the anonymous reviewers for their constructive feedback and suggestions for this paper. Figure 23. Mapping of AMD chiplets to multiple families of high-performance products.</p><p>The authors also deeply thank the multiple worldwide AMD teams that have worked tirelessly to bring all of these products to life and whose hard work is represented in this paper.</p></div>
			</div>

			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Authorized licensed use limited to: Tsinghua University. Downloaded apply.</p><p>ATTRIBUTION AMD, the AMD Arrow logo, EPYC, Instinct, Ryzen, Threadripper, Infinity Fabric, Radeon, and combinations thereof are trademarks of Advanced Micro Devices, Inc. PCIe is a copyright of the PCI-SIG Corporation, and SPEC, Specint, and SPECrate are copyrights of the Standard Performance Evaluation Corporation. Other product names used in this publication are for identification purposes only and may be trademarks of their respective companies.</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">Shared Resilience: AMD Response to COVID-19</title>
		<ptr target="https://www.amd.com/en/corporate/amd-covid-19-re-sponse" />
		<imprint>
			<date type="published" when="2020">2020</date>
			<publisher>Advanced Micro Devices, Inc</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">AMD Instinct™ MI100 Accelerator</title>
		<ptr target="https://www.amd.com/en/products/server-accelerators/instinct-mi100" />
		<imprint>
			<publisher>Advanced Micro Devices, Inc</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Zeppelin&quot;: An SoC for Multichip Architectures</title>
		<author>
			<persName><forename type="first">Noah</forename><surname>Beck</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sean</forename><surname>White</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Milam</forename><surname>Paraschou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Samuel</forename><surname>Naffziger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Solid-State Circuits Conference</title>
				<imprint>
			<date type="published" when="2018-02">February 2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">3D Processing Technology and its Impact on IA32 Microprocessors</title>
		<author>
			<persName><forename type="first">Bryan</forename><surname>Black</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Donald</forename><surname>Nelson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Clair</forename><surname>Webb</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nick</forename><surname>Samra</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Computer Design</title>
				<imprint>
			<date type="published" when="2004-10">October 2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Die Stacking (3D) Microarchitecture</title>
		<author>
			<persName><forename type="first">Bryan</forename><surname>Black</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Murali</forename><surname>Annavaram</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ned</forename><surname>Brekelbaum</surname></persName>
		</author>
		<author>
			<persName><forename type="first">John</forename><surname>Devale</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lei</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Gabriel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Don</forename><surname>Loh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Patrick</forename><surname>Mccauley</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Donald</forename><forename type="middle">W</forename><surname>Morrow</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Daniel</forename><surname>Nelson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Paul</forename><surname>Pantuso</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jeff</forename><surname>Reed</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sadasivan</forename><surname>Rupley</surname></persName>
		</author>
		<author>
			<persName><forename type="first">John</forename><surname>Shankar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Clair</forename><surname>Paul Shen</surname></persName>
		</author>
		<author>
			<persName><surname>Webb</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Symposium on Microarchitecture</title>
				<imprint>
			<date type="published" when="2006-12">December 2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Zeppelin&quot;: An SoC for Multichip Architectures</title>
		<author>
			<persName><forename type="first">Thomas</forename><surname>Burd</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Noah</forename><surname>Beck</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sean</forename><surname>White</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Milam</forename><surname>Paraschou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nathan</forename><surname>Kalyanasundharam</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gregg</forename><surname>Donley</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alan</forename><surname>Smith</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Larry</forename><surname>Hewitt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Samuel</forename><surname>Naffziger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Journal of Solid-state Circuits</title>
		<imprint>
			<biblScope unit="volume">54</biblScope>
			<biblScope unit="issue">1</biblScope>
			<date type="published" when="2019-01">January 2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Blade Computing with the AMD Opteron™ Processor</title>
		<author>
			<persName><forename type="first">Pat</forename><surname>Conway</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nathan</forename><surname>Kalyanasundharam</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gregg</forename><surname>Donley</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kevin</forename><surname>Lepak</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bill</forename><surname>Hughes</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Hot Chips 21 Symposium</title>
				<imprint>
			<date type="published" when="2009-08">August 2009</date>
		</imprint>
	</monogr>
	<note>Magny-cours&quot;)</note>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Centip3De: a Manycore Prototype Exploring 3D Integration and Near-threshold Computing</title>
		<author>
			<persName><forename type="first">Ronald</forename><forename type="middle">G</forename><surname>Dreslinksi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><surname>Fick</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bharan</forename><surname>Giridhar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gyouhou</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sangwon</forename><surname>Seo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Matthew</forename><surname>Fojtik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sudhir</forename><surname>Satpathy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yoonmyung</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Daeyeon</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nurrachman</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><surname>Wieckowski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gregory</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dennis</forename><surname>Sylvester</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><surname>Blaauw</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Trevor</forename><surname>Mudge</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Communications of the ACM</title>
		<imprint>
			<biblScope unit="volume">56</biblScope>
			<biblScope unit="issue">11</biblScope>
			<date type="published" when="2013-11">November 2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Design Considerations and Packaging of a Pentium® Pro Processor Based MultiChip Module for High Performance Workstation and Servers</title>
		<author>
			<persName><forename type="first">Gary</forename><surname>Dudeck</surname></persName>
		</author>
		<author>
			<persName><forename type="first">John</forename><surname>Dudeck</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Symposium on IC/Package Design Integration</title>
				<imprint>
			<date type="published" when="1998-02">February 1998</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">Switch Transformers: Scaling to Trillion Parameter Models with Simple and Efficient Sparsity</title>
		<author>
			<persName><forename type="first">William</forename><surname>Fedus</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Barret</forename><surname>Zoph</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Noam</forename><surname>Shazeer</surname></persName>
		</author>
		<idno>arXiv 2101.03961</idno>
		<imprint>
			<date type="published" when="2021-01">January 2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Large Area Interposer Lithography</title>
		<author>
			<persName><forename type="first">Warren</forename><surname>Flack</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Robert</forename><surname>Hsieh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gareth</forename><surname>Kenyon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Manish</forename><surname>Ranjan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">John</forename><surname>Slabbekoorn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andy</forename><surname>Miller</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Eric</forename><surname>Beyne</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Medhat</forename><surname>Toukhy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pinghung</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yi</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chunwei</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE 64 th Electronic Components and Technology Conference</title>
				<imprint>
			<date type="published" when="2014-05">May 2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<author>
			<persName><forename type="first">David</forename><surname>Greenhill</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ron</forename><surname>Ho</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><surname>Lewis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Herman</forename><surname>Schmit</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kok</forename><surname>Hong Chan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andy</forename><surname>Tong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sean</forename><surname>Atsatt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dana</forename><surname>How</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Peter</forename><surname>Mcelheny</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Keith</forename><surname>Duwel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jeffrey</forename><surname>Schulz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Darren</forename><surname>Faulkner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gopal</forename><surname>Iyer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">George</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hee</forename><surname>Kong Phoon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Han</forename><forename type="middle">Wooi</forename><surname>Lim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wei-Yee</forename><surname>Koay</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ty</forename><surname>Garibay</surname></persName>
		</author>
		<title level="m">A 14nm 1GHz FPGA with 2.5D Transceiver Integration</title>
				<imprint>
			<publisher>IEEE International Solid-State Circuits Conference</publisher>
			<date type="published" when="2017-02">February 2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">Cray to deliver record-setting Frontier supercomputer at ORNL</title>
		<author>
			<persName><forename type="first">Hewlett</forename><surname>Packard</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Enterprise</forename></persName>
		</author>
		<ptr target="https://www.hpe.com/us/en/news-room/press-release/2019/05/cray-to-deliver-record-setting-frontier-su-percomputer-at-ornl.html" />
		<imprint>
			<date type="published" when="2019-05">May 2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Enabling Interposer-based Disintegration of Multi-core Processors</title>
		<author>
			<persName><forename type="first">Ajaykumar</forename><surname>Kannan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Natalie</forename><forename type="middle">Enright</forename><surname>Jerger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Gabriel</surname></persName>
		</author>
		<author>
			<persName><surname>Loh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Symposium on Microarchitecture</title>
				<imprint>
			<date type="published" when="2015-12">December 2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">LLNL and HPE to partner with AMD on El Capitan, projected as world&apos;s fastest supercomputer</title>
		<ptr target="https://www.llnl.gov/news/llnl-and-hpe-partner-amd-el-capitan-pro-jected-worlds-fastest-supercomputer" />
		<imprint>
			<date type="published" when="2020-03">March 2020</date>
		</imprint>
		<respStmt>
			<orgName>Lawrence Livermore National Laboratory</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">AMD&apos;s Next Generation GPU and High Bandwidth Memory Architecture: FURY</title>
		<author>
			<persName><forename type="first">Joe</forename><surname>Macri</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Hot Chips 27 Symposium</title>
				<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">AMD&apos;s Radeon Next Generation GPU</title>
		<author>
			<persName><forename type="first">Michael</forename><surname>Mantor</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ben</forename><surname>Sander</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Hot Chips 29 Symposium</title>
				<imprint>
			<date type="published" when="2017-08">August 2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">Cramming More Components onto Integrated Circuits</title>
		<author>
			<persName><forename type="first">Gordon</forename><surname>Moore</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1965-04">April 1965</date>
			<biblScope unit="volume">38</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">Symposium on Architectural Support for Programming Languages and Operating Systems</title>
		<author>
			<persName><forename type="first">Shashidhar</forename><surname>Mysore</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Banit</forename><surname>Agrawal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Navin</forename><surname>Srivastava</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sheng-Chih</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kaustav</forename><surname>Banerjee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Timothy</forename><surname>Sherwood</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2006-10">October 2006</date>
		</imprint>
	</monogr>
	<note>Introspective 3D Chips</note>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">AMD Chiplet Architecture for High-performance Server and Desktop Products</title>
		<author>
			<persName><forename type="first">Samuel</forename><surname>Naffziger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kevin</forename><surname>Lepak</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Milam</forename><surname>Paraschou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mahesh</forename><surname>Subramony</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020-02">February 2020</date>
		</imprint>
	</monogr>
	<note>IEEE International Solid-State Circuits Conference</note>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">AI and Compute</title>
		<author>
			<persName><surname>Openai</surname></persName>
		</author>
		<ptr target="https://openai.com/blog/ai-and-compute/" />
		<imprint>
			<date type="published" when="2018-05">May 2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Tuning the Pentium Pro Microarchitecture</title>
		<author>
			<persName><forename type="first">David</forename><surname>Papworth</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Micro</title>
		<imprint>
			<biblScope unit="volume">16</biblScope>
			<date type="published" when="1996-04">April 1996</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Improving Language Understanding by Generative Pre-Training</title>
		<author>
			<persName><forename type="first">Alec</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Karthik</forename><surname>Narasimhan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tim</forename><surname>Salimans</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">OpenAI</title>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">ZeRO: Memory Optimization Towards Training A Trillion Parameter Models</title>
		<author>
			<persName><forename type="first">Samyam</forename><surname>Rajbhandari</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jeff</forename><surname>Rasley</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Olatunji</forename><surname>Ruwase</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuxiong</forename><surname>He</surname></persName>
		</author>
		<idno>ArXiv 1910.02054</idno>
		<imprint>
			<date type="published" when="2019-10">October 2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Zen: An Energy-efficient High-performance x86 Core</title>
		<author>
			<persName><forename type="first">Teja</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alex</forename><surname>Schaefer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Deepesh</forename><surname>Sundar Rangarajan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Carson</forename><surname>John</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Russell</forename><surname>Henrion</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Miguel</forename><surname>Schreiber</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stephen</forename><surname>Rodriguez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Samuel</forename><surname>Kosonocky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Amy</forename><surname>Naffziger</surname></persName>
		</author>
		<author>
			<persName><surname>Novak</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Journal of Solid-State Circuits</title>
		<imprint>
			<biblScope unit="volume">53</biblScope>
			<biblScope unit="issue">1</biblScope>
			<date type="published" when="2018-01">January 2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">POWER5 System Microarchitecture</title>
		<author>
			<persName><forename type="first">Balaram</forename><surname>Sinharoy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ronald</forename><forename type="middle">N</forename><surname>Kalla</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Joel</forename><forename type="middle">M</forename><surname>Tendler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Richard</forename><forename type="middle">J</forename><surname>Eickemeyer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jody</forename><forename type="middle">B</forename><surname>Joyner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IBM Journal of Research and Development</title>
		<imprint>
			<biblScope unit="volume">49</biblScope>
			<biblScope unit="issue">4/5</biblScope>
			<date type="published" when="2005-09">July/September 2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Multi-chip Technologies to Unleash Computing Performance Gains Over the Next Decade</title>
		<author>
			<persName><forename type="first">Lisa</forename><forename type="middle">T</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Samuel</forename><surname>Naffziger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mark</forename><surname>Papermaster</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Electron Devices Meeting</title>
				<imprint>
			<date type="published" when="2017-12">December 2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<title level="m" type="main">Plenary Talk, DARPA Electronics Resurgence Initiative Summit</title>
		<author>
			<persName><forename type="first">Lisa</forename><forename type="middle">T</forename><surname>Su</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019-07">July 2019</date>
		</imprint>
	</monogr>
	<note>Delivering the Future of High-performance Computing</note>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<title/>
		<author>
			<persName><forename type="first">Top500</forename><surname>List</surname></persName>
		</author>
		<ptr target="http://www.top500.org" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Architecting Microprocessor Components in 3D Design Space</title>
		<author>
			<persName><forename type="first">Balaji</forename><surname>Vaidyanathan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W-L</forename><surname>Hung</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Feng</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuan</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Vijaykrishnan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">J</forename><surname>Irwin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on VLSI Design</title>
				<imprint>
			<date type="published" when="2007-01">January 2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">POWER2: Next Generation of the RISC System/6000 Family</title>
		<author>
			<persName><forename type="first">Steven</forename><forename type="middle">W</forename><surname>White</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sudhir</forename><surname>Dhawan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IBM Journal of Research and Development</title>
		<imprint>
			<biblScope unit="volume">38</biblScope>
			<biblScope unit="issue">5</biblScope>
			<date type="published" when="1994-09">September 1994</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
