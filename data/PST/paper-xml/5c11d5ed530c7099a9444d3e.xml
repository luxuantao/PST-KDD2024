<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Compiling machine learning programs via high-level tracing</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Roy</forename><surname>Frostig</surname></persName>
							<email>frostig@google.com</email>
						</author>
						<author>
							<persName><forename type="first">Matthew</forename><forename type="middle">James</forename><surname>Johnson</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Chris</forename><surname>Leary</surname></persName>
							<email>leary@google.com</email>
						</author>
						<title level="a" type="main">Compiling machine learning programs via high-level tracing</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-01-03T08:31+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>We describe JAX, a domain-specific tracing JIT compiler for generating high-performance accelerator code from pure Python and Numpy machine learning programs. JAX uses the XLA compiler infrastructure to generate optimized code for the program subroutines that are most favorable for acceleration, and these optimized subroutines can be called and orchestrated by arbitrary Python. Because the system is fully compatible with Autograd, it allows forward-and reverse-mode automatic differentiation of Python functions to arbitrary order. Because JAX supports structured control flow, it can generate code for sophisticated machine learning algorithms while maintaining high performance. We show that by combining JAX with Autograd and Numpy we get an easily programmable and highly performant ML system that targets CPUs, GPUs, and TPUs, capable of scaling to multi-core Cloud TPUs.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">INTRODUCTION</head><p>Unlocking machine FLOPs has powered the explosion of progress in machine learning. Since the landmark work of AlexNet on dual-GPUs <ref type="bibr" target="#b4">[5]</ref>, the field has come a long way both in the number of FLOPs available to researchers and the ease with which these FLOPs can be harnessed. The JAX compiler aims to push further in this direction, enabling researchers to write Python programs-leaning on familiar and convenient libraries like Numpy <ref type="bibr" target="#b11">[12]</ref> for numerics and Autograd <ref type="bibr" target="#b5">[6]</ref> for automatic differentiation-that are automatically compiled and scaled to leverage accelerators and supercomputers.</p><p>The two goals of maximizing access to machine FLOPs and of facilitating research-friendly programmability are often in tension. Dynamic languages like Python offer convenient programming <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b7">8]</ref> but are too unconstrained to enable optimized code generation. Meanwhile, effective hardware acceleration requires much more static information <ref type="bibr" target="#b12">[13,</ref><ref type="bibr" target="#b13">14]</ref>. Emerging supercomputer platforms like the NVIDIA DGX-1 and the Google Cloud TPU present new hardware capabilities that magnify the programmability challenge.</p><p>We can start to address this tension with an empirical observation: ML workloads often consist of large, accelerable, pure-andstatically-composed (PSC) subroutines orchestrated by dynamic logic. Roughly speaking, a function is pure when it does not have side effects. It is statically-composed, relative to a set of primitive functions, when it can be represented as a static data dependency graph on those primitives. Provided the primitive functions are themselves accelerable, e.g. when they comprise array-level numerical kernels and restricted control flow, PSC routines are prime candidates for acceleration: they delineate chunks of the original Python program from which all unused dynamism can be stripped out.</p><p>The JAX system is a just-in-time (JIT) compiler that generates code for PSC subroutines via high-level tracing together with the XLA compiler infrastructure. Tracing in JAX is high-level both in the sense (i) that it is implemented as user-level code within the source language, rather than as part of the source language's implementation, and (ii) that the trace primitives are not VM-level operations on basic data, but rather library-level numerical functions on array-level data, like matrix multiplies, convolutions, reductions along axes, elementwise operations, and multidimensional indexing and slicing <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b8">9,</ref><ref type="bibr" target="#b10">11]</ref>. JAX is built atop the same tracing library used within Autograd, which, being designed for self-closure, recognizes its own operations as primitives. JAX also has Numpy's numerical functions among its primitives. As a result, it generates code for Python functions written in familiar Numpy and that involve arbitrary-order forward-and reverse-mode automatic differentiation. On the back end, JAX uses XLA for array-level program optimization and code generation. Whereas other systems focus on providing easy access to a fixed set of hand-written, target-specific numerical kernels, JAX provides a means of composition for all of XLA's supported target architectures: by trace-compiling PSC routines, JAX automatically stages out new kernels from existing ones.</p><p>The acronym JAX stands for "just after execution", since to compile a function we first monitor its execution once in Python.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">SYSTEM DESIGN</head><p>The design of JAX is informed by the observation that ML workloads are typically dominated by PSC subroutines. In light of this observation, JAX trace formation simply requires users to annotate the PSC entry point; that is, a Python function for which the PSC assumption holds. This design exploits the property that these functions are often straightforward to identify in machine learning code, making them simple for the machine learning researcher to annotate with JAX's jit_ps decorator. While manual annotation presents a challenge for non-expert users and for "zero workload knowledge" optimization, it provides immediate benefits to experts and, as a systems research project, demonstrates the power of the PSC assumption. JAX trace caching creates a monomorphic signature for the parameters to the traced computation, such that newly encountered array element types, array dimensions, or tuple members trigger a re-compilation.</p><p>On a trace cache miss, JAX executes the corresponding Python function and traces its execution into a graph of primitive functions with static data dependencies. Existing primitives comprise not only array-level numerical kernels, including Numpy functions and additional functions like convolutions and windowed reductions, but also restricted control flow functions, like a functional while_loop and cond (if-then-else). These control flow primitives are less familiar than syntactic Python control flow constructs, but they allow users to stage control flow into the compiled computation by preserving the PSC property. Finally, JAX includes some primitives for functional distributed programming, like iterated_map_reduce. The set of primitives is defined in Python and is extensible; new primitives must simply be annotated with a translation rule that builds corresponding XLA computations.</p><p>To generate code, JAX translates the trace into XLA HLO, an intermediate language that models highly accelerable array-level numerical programs. Broadly speaking, JAX can be seen as a system that lifts the XLA programming model into Python and enables its use for accelerable subroutines while still allowing dynamic orchestration. See Listing 2 for some example translation rules. Listing 2: JAX translation rules from primitives to XLA HLO.</p><p>Finally, JAX is fully compatible with Autograd. See Listing 1, which compiles a gradient function for neural network training.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">EXAMPLES AND EXPERIMENTS</head><p>Array-level fusion. To demonstrate the array-level code optimization and operation fusion that JAX and XLA provide, we compiled a single fully-connected neural network layer with SeLU nonlinearity <ref type="bibr" target="#b3">[4]</ref> and show the JAX trace and XLA HLO graphs in Figure <ref type="figure">1</ref>.</p><p>Truncated Newton-CG optimization on CPU. As a CPU benchmark, we implemented a truncated Newton-CG optimization algorithm, which performs approximate Newton-Raphson updates using a conjugate gradient (CG) algorithm in its inner loop. The inner CG algorithm is truncated when the residual norm is decreased below some threshold or a maximum number of inner iterations is reached. We compared the Python execution time with the JAXcompiled runtime on CPU using a single thread and several small example optimization problems, including a convex quadratic, a hidden Markov model (HMM) marginal likelihood, and a logistic regression. The XLA compile times were slow for some CPU examples but are likely to improve significantly in the future, and the speedups for the warmed-up code (Table <ref type="table" target="#tab_1">1</ref>) are substantial.     Training a convolutional network on GPU. We implemented an all-conv CIFAR-10 network <ref type="bibr" target="#b9">[10]</ref>, involving only convolutions and ReLU activations. We JAX-compiled a single stochastic gradient descent (SGD) update step and called it from a pure Python loop, reporting the minimum average wall-clock step time across 100 trials in Table <ref type="table">2</ref>. As a reference, we implemented the same algorithm in TensorFlow and invoked it within a similar Python loop. The benchmarking environment was CUDA 8 driver 384.111 on an HP Z420 workstation.</p><p>TF:GPU JAX:GPU t exec 40.2 msec 41.8 msec relative 1x 1.04x Table <ref type="table">2</ref>: Timing (msec) for a JAX convnet step on GPU.</p><p>Cloud TPU scalability. JAX parallelization of global batch on Cloud TPU cores exhibits linear speedup (Figure <ref type="figure" target="#fig_4">2</ref>, left). At fixed minibatch / replica, t exec is minimally impacted by replica count (within 2ms, right). We used a Cloud TPU configuration with four chips and two cores per chip <ref type="bibr" target="#b2">[3]</ref>, so R = 2 is more efficient than R = 8, as on-chip communication is faster than between chips. The anomaly at R = 4 is due to an implementation detail of XLA's all-reduce.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head></head><label></label><figDesc>def xla_add(xla_builder, xla_args, np_x, np_y): return xla_builder.Add(xla_args[0], xla_args[1]) def xla_sinh(xla_builder, xla_args, np_x): b, xla_x = xla_builder, xla_args[0] return b.Div(b.Sub(b.Exp(xla_x), b.Exp(b.Neg(xla_x))), b.Const(2)) def xla_while(xla_builder, xla_args, cond_fun, body_fun, init_val): xla_cond = trace_computation(cond_fun, args=(init_val,)) xla_body = trace_computation(body_fun, args=(init_val,)) return xla_builder.While(xla_cond, xla_body, xla_args[-1]) jax.register_translation_rule(numpy.add, xla_add) jax.register_translation_rule(numpy.sinh, xla_sinh) jax.register_translation_rule(while_loop, xla_while)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure</head><label></label><figDesc>Figure 1: XLA HLO fusion for a layer with SeLU nonlinearity. The gray box indicates all ops are fused into the GEMM. Python JAX speedup convex quadratic 4.12 sec 0.036 sec 114x hidden Markov model fit 7.79 sec 0.057 sec 153x logistic regression fit 3.62 sec 1.19 sec 3x Table 1: Timing (sec) for Truncated Newton-CG on CPU.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: Scaling on a Cloud TPU for ConvNet training step.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 1 :</head><label>1</label><figDesc>1: XLA HLO fusion for a layer with SeLU nonlinearity. The gray box indicates all ops are fused into the GEMM. Timing (sec) for Truncated Newton-CG on CPU.</figDesc><table><row><cell></cell><cell></cell><cell></cell><cell cols="2">Python</cell><cell>JAX</cell><cell>speedup</cell></row><row><cell></cell><cell></cell><cell cols="4">convex quadratic 4.12 sec 0.036 sec</cell><cell>114x</cell></row><row><cell></cell><cell cols="5">hidden Markov model fit 7.79 sec 0.057 sec</cell><cell>153x</cell></row><row><cell></cell><cell cols="5">logistic regression fit 3.62 sec 1.19 sec</cell><cell>3x</cell></row><row><cell></cell><cell cols="2">Speedup v Replicas</cell><cell></cell><cell></cell><cell cols="2">Execution Time v Replicas</cell></row><row><cell></cell><cell cols="3">global batch=1024, slope=0.97</cell><cell></cell><cell cols="2">global batch=R*128</cell></row><row><cell>texec @ R=1 / texec</cell><cell>2 4 8</cell><cell></cell><cell>Execution Time (ms)</cell><cell>21.0 21.5 22.0</cell><cell></cell></row><row><cell></cell><cell>1 2</cell><cell>4</cell><cell>8</cell><cell></cell><cell>1 2</cell><cell>4</cell><cell>8</cell></row><row><cell></cell><cell></cell><cell>Replica Count (R)</cell><cell></cell><cell></cell><cell cols="2">Replica Count (R)</cell></row></table></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">Tensorflow: Large-scale machine learning on heterogeneous distributed systems</title>
		<author>
			<persName><forename type="first">Mart?n</forename><surname>Abadi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ashish</forename><surname>Agarwal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Paul</forename><surname>Barham</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Eugene</forename><surname>Brevdo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhifeng</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Craig</forename><surname>Citro</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Greg</forename><forename type="middle">S</forename><surname>Corrado</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andy</forename><surname>Davis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jeffrey</forename><surname>Dean</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Matthieu</forename><surname>Devin</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1603.04467</idno>
		<imprint>
			<date type="published" when="2016">2016. 2016</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
	<note>and others</note>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Parallelizing Julia with a Non-Invasive DSL</title>
		<author>
			<persName><forename type="first">Hai</forename><surname>Todd A Anderson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lindsey</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ehsan</forename><surname>Kuper</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jan</forename><surname>Totoni</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tatiana</forename><surname>Vitek</surname></persName>
		</author>
		<author>
			<persName><surname>Shpeisman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">LIPIcs-Leibniz International Proceedings in Informatics</title>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="volume">74</biblScope>
		</imprint>
		<respStmt>
			<orgName>Schloss Dagstuhl-Leibniz-Zentrum fuer Informatik</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<author>
			<persName><forename type="first">Jeff</forename><surname>Dean</surname></persName>
		</author>
		<ptr target="http://learningsys.org/nips17/assets/slides/dean-nips17.pdf" />
		<title level="m">Machine Learning for Systems and Systems for Machine Learning</title>
		<imprint>
			<date type="published" when="2017">2017. 2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title/>
		<author>
			<persName><forename type="first">G?nter</forename><surname>Klambauer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Thomas</forename><surname>Unterthiner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andreas</forename><surname>Mayr</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sepp</forename><surname>Hochreiter</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1706.02515</idno>
		<imprint>
			<date type="published" when="2017">2017. 2017</date>
		</imprint>
	</monogr>
	<note type="report_type">Self-Normalizing Neural Networks. arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Imagenet classification with deep convolutional neural networks</title>
		<author>
			<persName><forename type="first">Alex</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Geoffrey</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems (NIPS)</title>
		<imprint>
			<date type="published" when="2012">2012</date>
			<biblScope unit="page" from="1097" to="1105" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<author>
			<persName><forename type="first">Dougal</forename><surname>Maclaurin</surname></persName>
		</author>
		<title level="m">Modeling, Inference and Optimization with Composable Differentiable Procedures</title>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
	<note>Ph.D. Dissertation. Harvard University</note>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">Autograd: Reverse-mode differentiation of native Python</title>
		<author>
			<persName><forename type="first">Dougal</forename><surname>Maclaurin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><surname>Duvenaud</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Matthew</forename><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ryan</forename><forename type="middle">P</forename><surname>Adams</surname></persName>
		</author>
		<ptr target="https://github.com/HIPS/autograd" />
		<imprint>
			<date type="published" when="2015">2015. 2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title/>
		<author>
			<persName><forename type="first">Adam</forename><surname>Paszke</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sam</forename><surname>Gross</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Soumith</forename><surname>Chintala</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gregory</forename><surname>Chanan</surname></persName>
		</author>
		<ptr target="https://github.com/pytorch/pytorch" />
		<imprint>
			<date type="published" when="2017">2017. 2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title/>
		<author>
			<persName><forename type="first">Jarrett</forename><surname>Revels</surname></persName>
		</author>
		<ptr target="https://github.com/jrevels/Cassette.jl" />
		<imprint>
			<date type="published" when="2017">2017. 2017</date>
		</imprint>
	</monogr>
	<note type="report_type">Casette.jl</note>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<author>
			<persName><forename type="first">Jost</forename><surname>Tobias Springenberg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alexey</forename><surname>Dosovitskiy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Thomas</forename><surname>Brox</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Martin</forename><surname>Riedmiller</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1412.6806</idno>
		<title level="m">Striving for simplicity: The all convolutional net</title>
		<imprint>
			<date type="published" when="2014">2014. 2014</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">HPAT: high performance analytics with scripting ease-of-use</title>
		<author>
			<persName><forename type="first">Ehsan</forename><surname>Totoni</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Todd</forename><forename type="middle">A</forename><surname>Anderson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tatiana</forename><surname>Shpeisman</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1611.04934</idno>
		<imprint>
			<date type="published" when="2016">2016. 2016</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">The NumPy array: a structure for efficient numerical computation</title>
		<author>
			<persName><forename type="first">Chris</forename><surname>St?fan Van Der Walt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gael</forename><surname>Colbert</surname></persName>
		</author>
		<author>
			<persName><surname>Varoquaux</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computing in Science &amp; Engineering</title>
		<imprint>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="page" from="22" to="30" />
			<date type="published" when="2011">2011. 2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">DLVM: A modern compiler framework for neural network DSLs</title>
		<author>
			<persName><forename type="first">Richard</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lane</forename><surname>Schwartz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vikram</forename><surname>Adve</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS 2017 Autodiff Workshop</title>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">XLA -TensorFlow</title>
		<author>
			<persName><forename type="first">Tensorflow</forename><surname>Xla</surname></persName>
		</author>
		<author>
			<persName><surname>Teams</surname></persName>
		</author>
		<ptr target="https://developers.googleblog.com/2017/03/xla-tensorflow-compiled.html" />
		<imprint>
			<date type="published" when="2017">2017. 2017</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
