<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">PromptKG: A Prompt Learning Framework for Knowledge Graph Representation Learning and Application</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date type="published" when="2022-10-01">1 Oct 2022</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Xin</forename><surname>Xie</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Zhoubo</forename><surname>Li</surname></persName>
							<email>zhoubo.li@zju.edu.cn</email>
						</author>
						<author>
							<persName><forename type="first">Xiaohan</forename><surname>Wang</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Shumin</forename><surname>Deng</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Feiyu</forename><surname>Xiong</surname></persName>
							<email>feiyu.xfy@alibaba-inc.com</email>
						</author>
						<author>
							<persName><forename type="first">Huajun</forename><surname>Chen</surname></persName>
							<email>huajunsir@zju.edu.cn</email>
						</author>
						<author>
							<persName><forename type="first">Ningyu</forename><surname>Zhang</surname></persName>
							<email>zhangningyu@zju.edu.cn</email>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="laboratory">AZFT Joint Lab for Knowledge Engine Hangzhou</orgName>
								<orgName type="institution">Zhejiang University</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff1">
								<orgName type="laboratory">AZFT Joint Lab for Knowledge Engine Hangzhou</orgName>
								<orgName type="institution">Zhejiang University</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff2">
								<orgName type="institution">National University</orgName>
								<address>
									<country>Singapore Singapore</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff3">
								<orgName type="laboratory">Alibaba Group Hangzhou</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff4">
								<orgName type="laboratory">AZFT Joint Lab for Knowledge Engine Hangzhou</orgName>
								<orgName type="institution">Zhejiang University</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff5">
								<orgName type="laboratory">AZFT Joint Lab for Knowledge Engine Hangzhou</orgName>
								<orgName type="institution">Zhejiang University</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff6">
								<orgName type="department">Conference acronym &apos;XX</orgName>
								<address>
									<postCode>03-05, 2018</postCode>
									<settlement>June, Woodstock</settlement>
									<region>NY</region>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">PromptKG: A Prompt Learning Framework for Knowledge Graph Representation Learning and Application</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2022-10-01">1 Oct 2022</date>
						</imprint>
					</monogr>
					<idno type="arXiv">arXiv:2210.00305v1[cs.CL]</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-01-03T09:10+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Knowledge Graphs (KGs) often have two characteristics: heterogeneous graph structure and text-rich entity/relation information. KG representation models should consider graph structures and text semantics, but no comprehensive open-sourced framework is mainly designed for KG regarding informative text description. In this paper, we present PromptKG, a prompt learning framework for KG representation learning and application that equips the cutting-edge text-based methods, integrates a new prompt learning model and supports various tasks (e.g., knowledge graph completion, question answering, recommendation, and knowledge probing). PromptKG is publicly open-sourced at https://github.com/zjunlp/PromptKG with long-term technical support.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">INTRODUCTION</head><p>Knowledge Graphs (KGs) encode real-world facts as structured data and have drawn significant attention from academia, and the industry <ref type="bibr" target="#b18">[19]</ref>. KG representation learning aims to project the relations and entities into a continuous vector space, which can promote the knowledge reasoning ability and feasibly be applied to downstream tasks: question answering <ref type="bibr" target="#b8">[9]</ref>, recommender system <ref type="bibr" target="#b16">[17]</ref> and so on. Previous embedding-based knowledge graph representation methods, such as TransE <ref type="bibr" target="#b1">[2]</ref>, embed the relational knowledge into a vector space and then optimize the target object by leveraging a pre-defined scoring function to those vectors. A few remarkable open-sourced and long-term maintained KG representation toolkits have been developed, such as OpenKE <ref type="bibr" target="#b4">[5]</ref>, LibKGE <ref type="bibr" target="#b2">[3]</ref>, PyKEEN <ref type="bibr" target="#b0">[1]</ref>, CogKGE <ref type="bibr" target="#b5">[6]</ref>. Nevertheless, these embedding-based methods are restricted in expressiveness regarding the shallow network architectures without using any side information.</p><p>By comparison, text-based methods <ref type="bibr" target="#b14">[15]</ref> incorporate available texts for knowledge representation learning. With the development of prompt learning, lots of text-based models <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b13">14]</ref> have been proposed, which can obtain promising performance with pre-trained language models (PLMs) and take advantage of allocating a fixed memory footprint for those large-scale real-world KGs. However, there is no comprehensive open-sourced framework particularly designed for KG representation with prompt learning at present, Figure <ref type="figure">2</ref>: A prompt learning method for KG representation learning in PromptKG and those KG can be applied to tasks such as KGC, QA, recommendation and knowledge probing. Entity_t refers to the target tail entity, answer entity, recommended items, and target tail entity for different tasks, which follows the pre-train (obtain the embedding) and fine-tune paradigm (task-specific tuning).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Pre-trained Language Model</head><p>which makes it challenging to try out new methods and make rigorous comparisons for previous approaches.</p><p>In this paper, we share with the community an open-sourced prompt learning framework for KG representation learning and application called PromptKG (MIT License), which supports various cutting-edge text-based KG representation models. Besides, we equip PromptKG with a simple-yet-effective prompt learning method, which utilizes contextual [MASK] embedding as knowledge representations (through KG pre-training) for downstream tasks (task-specific fine-tuning with KG representations as shown in Figure <ref type="figure">2</ref>). PromptKG supports diverse tasks including KG completion, question answering, recommendation, and knowledge probing (LAMA). Empirically, we demonstrate that PromptKG can yield better or comparable performance on seven datasets. We also provide tutorial notebooks for beginners. We will provide maintenance to meet new tasks, new requests, and fix bugs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">TEXT-BASED KG REPRESENTATION</head><p>In this section, we detail two major types of text-based KG representation (discrimination-based and generation-based) and the proposed prompt learning method for KG representation learning, which are all integrated into PromptKG. </p><formula xml:id="formula_0">Model PLM Support Task Complexity KGBERT [15] MLM KGC O (|?| 2 |E | 2 |R|) SimKGC [13] MLM KGC O (|?/2| 2 |E |(1 + |R|)) StAR [12] MLM KGC O (|?/2| 2 |E |(1 + |R|)) KGT5 [9] Seq2Seq KGC QA O (|?/2| 3 |E ||R|) GenKGC [14] Seq2Seq KGC O (|?/2| 3 |E ||R|) ?NN-KGE [18] MLM KGC LAMA O (|?| 2 |E ||R|)</formula><p>Notation. Given a triple (?, ?, ?), we define their natural language descriptions as</p><formula xml:id="formula_1">? ? = {? ? 1 , ? ? 2 , ..., ? ? |? | } , ? ? = {? ? 1 , ? ? 2 , ..., ? ? |? | } and ? ? = {? ? 1 , ? ? 2 , ..., ? ? |? | }.</formula><p>We denote the embedding of token ? as ?.</p><p>Discrimination-based methods. There are two kinds of models based on the discrimination method: one (e.g., KG-BERT <ref type="bibr" target="#b14">[15]</ref>, PKGC <ref type="bibr" target="#b6">[7]</ref>) utilizes a single encoder to encode triples of KG with text description; others <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b12">13]</ref> leverage siamese encoder (two-tower models) with PLMs to encode entities and relations respectively. For the first kind, the score of each triple is expressed as:</p><formula xml:id="formula_2">????? (?, ?, ?) = TransformerEnc(? ? , ? ? , ? ? ),<label>(1)</label></formula><p>where TransformerEnc is the BERT model followed by a binary classifier. However, these models have to iterate all the entities calculating scores to decide the correct one, which is computationintensive, as shown in Table <ref type="table" target="#tab_0">1</ref>. In contrast, two-tower models like StAR <ref type="bibr" target="#b11">[12]</ref> and SimKGC <ref type="bibr" target="#b12">[13]</ref> usually encode ??, ? ? and ? to obtain the embeddings. Then, they use a score function to predict the correct tail entity from the candidates, denoted by:</p><formula xml:id="formula_3">????? (??, ? ?, ?) = cos(? ??,? ? , ? ? ).<label>(2)</label></formula><p>Generation-based methods. Generation-based models formulate KG completion or other KG-intensive tasks as sequence-to-sequence generation. Given a triple with the tail entity missing (?, ?, ?), models are fed with ?? ? , ? ? ? and then output ? ? . In the training procedure, generative models maximize the conditional probability:</p><formula xml:id="formula_4">????? (?, ?, ?) = |? | ?=1 ? (? ? ? |? ? 1 , ? ? 2 , ..., ? ? ?-1 ; ?? ? , ? ? ?).<label>(3)</label></formula><p>To guarantee the consistency of decoding sequential schemas and tokens in KG, GenKGC <ref type="bibr" target="#b13">[14]</ref> proposes an entity-aware hierarchical decoder to constrain ? ? . In addition, inspired by prompt-learning, GenKGC takes triples with the same relation as demonstrations to implicitly encode structured knowledge. Besides, KGT5 <ref type="bibr" target="#b8">[9]</ref> proposes to pre-train generation-based PLMs from scratch with text descriptions for KG representation.</p><p>A prompt learning method for KG representation learning. We introduce the technical details of the proposed prompt learning method for KG representation learning, which shares the same architecture as normal discrimination PLMs. Note that there are two modules in the normal PLMs: a word embedding layer to embed the token ids into semantic space and an encoder to generate contextaware token embedding. Here, we take the masked language model and treat entities and relations as special tokens in the 'word embedding layer'. As shown in Figure <ref type="figure">2</ref>, the model predicts the correct tail entity with the sequence of head entity and relation token and their descriptions. For the entity/relation embedding, we freeze the encoder layer, only tuning the entity embedding layer, to optimize the loss function:</p><formula xml:id="formula_5">L = -1 | E | ? ? ?E I(? ? = ? ? ) log ? [MASK] = ? ? | ? ? ; ? ,<label>(4)</label></formula><p>where ? represents the parameters of the model, ? ? and ? ? is the description and the embedding of entity ?.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">TOOLKIT DESIGN</head><p>We introduce the design principle of PromptKG as: 1) Unified KG Encoder: PromptKG utilizes a unified encoder to pack graph structure and text semantics; 2) Model Hub: PromptKG is integrated with many cutting-edge text-based KG representation models; 3) Flexible Downstream Tasks: PromptKG disentangles KG representation learning and downstream tasks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Unified KG Encoder</head><p>As shown in Figure <ref type="figure" target="#fig_0">1</ref>, a unified KG encoder represents graph structure and text semantics, supporting different types of text-based KG representation methods. For the discrimination-based method, the input is built on the plain text description as:</p><formula xml:id="formula_6">? hr pair = [CLS] ? ? [SEP] ? ? [SEP] ? tail = [CLS] ? ? [SEP].</formula><p>For the generation-based model, we leverage the tokens in ? ? and ? ? to optimize the model with label ? ? . When predicting the head entity, we add a special token [reverse] in the input sequence for reverse reasoning. Referring to the proposed prompt learning method, we represent entities and relations in KG with special tokens and obtain the input as:</p><formula xml:id="formula_7">? = [CLS]? ? [Entity h] [SEP] ? ? [SEP] [MASK] [SEP],<label>(5)</label></formula><p>where [Entity h] represents the special token to the head entity. To encode the graph structure, we sample 1-hop neighbor entities and concatenate their tokens as input for implicit structure information. With such a unified KG encoder, PromptKG can encode both heterogeneous graph structure and text-rich semantic information.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Model Hub</head><p>As shown in Figure <ref type="figure" target="#fig_0">1</ref>, PromptKG consists of a Model Hub which supports many representative text-based KG representation models. For example, KG-BERT <ref type="bibr" target="#b14">[15]</ref> uses BERT to score the triple with their descriptions. Comparing to its high time complexity, StAR <ref type="bibr" target="#b11">[12]</ref> and SimKGC <ref type="bibr" target="#b12">[13]</ref> both introduce a tower-based method to precompute entity embeddings and retrieve top-? entities efficiently. Further, GenKGC <ref type="bibr" target="#b13">[14]</ref> and KGT5 <ref type="bibr" target="#b8">[9]</ref> treat knowledge graph completion as sequence-to-sequence generation. Besides, ?NN-KGE <ref type="bibr" target="#b17">[18]</ref> is a KG representation model which linearly interpolates its entity distribution by k-nearest neighbors. Note that all the model </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Applying to Downstream Tasks</head><p>We take the proposed prompt learning for KG representation learning as an example <ref type="foot" target="#foot_0">1</ref> and introduce the technical details of applying to downstream tasks as shown in Figure <ref type="figure">2</ref>. For knowledge graph completion, we feed the model with the textual information ?? ? , ? ? ? of the head entity and the relation, then obtain the target tail entity via mask token prediction. For question answering, we feed the model with the question written in natural language concatenated with a [MASK] token to obtain the special token of the target answer (entity). For recommendation, we take the user's interaction history as sequential input <ref type="bibr" target="#b9">[10]</ref> with entity embeddings and then leverage the mask token prediction to obtain recommended items. For the knowledge probing task, we adopt entity embedding as additional knowledge to help the model better reason through the sentence and predict the token in the masked position following PELT <ref type="bibr" target="#b15">[16]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">EXPERIMENTS</head><p>To evaluate PromptKG, we conduct experiments on four tasks as shown in Table <ref type="table" target="#tab_1">2</ref>. 1) KG completion (link prediction) with textual information is a direct downstream task of KG representation; 2) question answering is an intuitive knowledge-intensive task; 3) recommendation involves items aligned to entities in real-world KGs and thus can benefit from KG representation; 4) knowledge probing (LAMA) analyzes the factual and commonsense knowledge contained in language models using cloze-style questions. All datasets and detailed hyper-parameters are all available on the Github for reproducibility.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Knowledge Graph Completion</head><p>For the KG completion task, we conduct link prediction experiments on two datasets WN18RR <ref type="bibr" target="#b3">[4]</ref> and FB15k-237 <ref type="bibr" target="#b10">[11]</ref>, and evaluate the models in PromptKG with hits1 and MRR metrics. From Table <ref type="table" target="#tab_1">2</ref>, we observe that discrimination-based method SimKGC (previous state-of-the-art) achieves higher performance than other baselines. Generation-based models like KGT5 <ref type="bibr" target="#b8">[9]</ref> and GenKGC <ref type="bibr" target="#b13">[14]</ref> also yield comparable results and show potential abilities in KG representation. ?NN-KGE <ref type="bibr" target="#b17">[18]</ref> can obtain promising performance (the best hits1 score) by computing the nearest neighbors based on the distance in the entity embedding space from the knowledge store and a two-step training strategy.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Question Answering</head><p>KG is known to be helpful for the task of question answering. We apply PromptKG to question answering and conduct experiments on the MetaQA dataset. Due to computational resource limits, we only evaluate the 1-hop inference performance. From Table <ref type="table" target="#tab_1">2</ref>, KGT5 in PromptKG yields the best performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Recommendation</head><p>For the recommendation task, we conduct experiments on a wellestablished version ML-20m 2 . Linkage of ML-20m and Freebase offered by KB4Rec <ref type="bibr" target="#b19">[20]</ref> is utilized to obtain textual descriptions of movies in ML-20m. With movie embeddings pre-trained on these descriptions, we conduct experiments on sequential recommendation task following the settings of BERT4Rec <ref type="bibr" target="#b9">[10]</ref>. We notice that PromptKG is confirmed to be effective for the recommendation compared with BERT4Rec.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Knowledge Probing</head><p>Knowledge probing <ref type="bibr" target="#b7">[8]</ref> examines the ability of LMs (BERT, RoBERTa, etc.) to recall facts from their parameters. We conduct experiments on LAMA using pre-trained BERT (bert-base-uncased) and RoBERTa (roberta-base) models. To prove that entity embedding enhanced by KGs helps LMs grab more factual knowledge from PLMs, we train a pluggable entity embedding module following PELT <ref type="bibr" target="#b15">[16]</ref>.</p><p>As shown in Table <ref type="table" target="#tab_1">2</ref>, the performance boosts while we use the entity embedding module. Since there is no subject entity annotated in Squad and no URI of the subject entity in Concept Net for entity alignment, we only apply the entity embedding module to the remaining data in LAMA. PromptKG will support a unified API accessible for enhanced entity embedding in the future.</p><p>2 https://grouplens.org/datasets/movielens/20m/</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">CONCLUSION AND FUTURE WORK</head><p>We propose PromptKG, a prompt learning framework for knowledge graph representation learning and application. PromptKG establishes a unified toolkit with well-defined modules and easyto-use interfaces to support research on using PLMs on KGs. Both for researchers and developers, PromptKG provides effective and efficient training code and supports downstream tasks. In the future, we will continue to integrate more models and tasks (e.g., dialogue) into PromptKG to facilitate the research progress of the KG.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: Architecture of PromptKG.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 :</head><label>1</label><figDesc>Inference efficiency comparison. |?| is the length of the triple description. |?/2| can be seen as the length of entity tokens. |E | and |R| are the numbers of all unique entities and relations in the graph respectively. Usually, |E | exceeds hundreds of thousands and is much greater than |R|.</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 :</head><label>2</label><figDesc>Hits1 and MRR (%) results on KGC, question answering, recommendation and knowledge probing tasks. refers to the result are borrowed from origin papers.</figDesc><table><row><cell>Task</cell><cell>Dataset</cell><cell>Method</cell><cell cols="2">hits1 MRR</cell></row><row><cell></cell><cell></cell><cell>KG-BERT</cell><cell>4.1</cell><cell>21.6</cell></row><row><cell></cell><cell></cell><cell>StAR</cell><cell>24.3</cell><cell>40.1</cell></row><row><cell></cell><cell></cell><cell>SimKGC</cell><cell>42.5</cell><cell>60.8</cell></row><row><cell></cell><cell>WN18RR</cell><cell>KGT5</cell><cell>17.9</cell><cell>-</cell></row><row><cell></cell><cell></cell><cell>GenKGC</cell><cell>39.6</cell><cell>-</cell></row><row><cell></cell><cell></cell><cell>?NN-KGE</cell><cell>52.5</cell><cell>57.9</cell></row><row><cell>KG Completion</cell><cell></cell><cell>KG-BERT</cell><cell>-</cell><cell>-</cell></row><row><cell></cell><cell></cell><cell>StAR</cell><cell>20.5</cell><cell>29.6</cell></row><row><cell></cell><cell></cell><cell>SimKGC</cell><cell>22.6</cell><cell>30.1</cell></row><row><cell></cell><cell>FB15k-237</cell><cell>KGT5</cell><cell>10.8</cell><cell>-</cell></row><row><cell></cell><cell></cell><cell>GenKGC</cell><cell>19.2</cell><cell>-</cell></row><row><cell></cell><cell></cell><cell>?NN-KGE</cell><cell>28.0</cell><cell>37.3</cell></row><row><cell></cell><cell></cell><cell>GT query</cell><cell>63.3</cell><cell>-</cell></row><row><cell>Question Answering</cell><cell>MetaQA</cell><cell>PullNet</cell><cell>65.1</cell><cell>-</cell></row><row><cell></cell><cell></cell><cell>KGT5</cell><cell>67.8</cell><cell>-</cell></row><row><cell>Recommendation</cell><cell>ML-20m</cell><cell>BERT4Rec PromptKG</cell><cell>34.4 37.3</cell><cell>47.9 50.5</cell></row><row><cell></cell><cell></cell><cell>BERT</cell><cell>28.6</cell><cell>37.7</cell></row><row><cell></cell><cell>TREx</cell><cell>RoBERTa</cell><cell>19.9</cell><cell>27.8</cell></row><row><cell></cell><cell></cell><cell cols="2">PromptKG (RoBERTa) 22.1</cell><cell>29.8</cell></row><row><cell></cell><cell></cell><cell>BERT</cell><cell>13.2</cell><cell>23.5</cell></row><row><cell></cell><cell>Squad</cell><cell>RoBERTa</cell><cell>13.4</cell><cell>24.6</cell></row><row><cell>Knowledge Probing</cell><cell></cell><cell>PromptKG (RoBERTa)</cell><cell>-</cell><cell>-</cell></row><row><cell></cell><cell></cell><cell>BERT</cell><cell>10.3</cell><cell>17.3</cell></row><row><cell></cell><cell>Google RE</cell><cell>RoBERTa</cell><cell>7.6</cell><cell>12.8</cell></row><row><cell></cell><cell></cell><cell>PromptKG (RoBERTa)</cell><cell>8.1</cell><cell>14.2</cell></row><row><cell></cell><cell></cell><cell>BERT</cell><cell>15.1</cell><cell>23.1</cell></row><row><cell></cell><cell>Concept Net</cell><cell>RoBERTa</cell><cell>17.8</cell><cell>25.4</cell></row><row><cell></cell><cell></cell><cell>PromptKG (RoBERTa)</cell><cell>-</cell><cell>-</cell></row><row><cell cols="5">implementations in Model Hub are modularized; thus, flexible to</cell></row><row><cell cols="2">debug and add new models.</cell><cell></cell><cell></cell><cell></cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0"><p>Some models still under fast development in Model Hub can not be directly applied to downstream tasks.</p></note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">PyKEEN 1.0: A Python Library for Training and Evaluating Knowledge Graph Embeddings</title>
		<author>
			<persName><forename type="first">Mehdi</forename><surname>Ali</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Max</forename><surname>Berrendorf</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Charles</forename><forename type="middle">Tapley</forename><surname>Hoyt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Laurent</forename><surname>Vermue</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sahand</forename><surname>Sharifzadeh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jens</forename><surname>Volker Tresp</surname></persName>
		</author>
		<author>
			<persName><surname>Lehmann</surname></persName>
		</author>
		<ptr target="http://jmlr.org/papers/v22/20-825.html" />
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="page" from="1" to="6" />
			<date type="published" when="2021">2021. 2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Translating Embeddings for Modeling Multi-relational Data</title>
		<author>
			<persName><forename type="first">Antoine</forename><surname>Bordes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nicolas</forename><surname>Usunier</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alberto</forename><surname>Garc?a-Dur?n</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jason</forename><surname>Weston</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Oksana</forename><surname>Yakhnenko</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of NeurIPS</title>
		<meeting>of NeurIPS</meeting>
		<imprint>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">LibKGE -A knowledge graph embedding library for reproducible research</title>
		<author>
			<persName><forename type="first">Samuel</forename><surname>Broscheit</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Daniel</forename><surname>Ruffinelli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Adrian</forename><surname>Kochsiek</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Patrick</forename><surname>Betz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rainer</forename><surname>Gemulla</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of EMNLP</title>
		<meeting>of EMNLP</meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Convolutional 2D Knowledge Graph Embeddings</title>
		<author>
			<persName><forename type="first">Tim</forename><surname>Dettmers</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pasquale</forename><surname>Minervini</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pontus</forename><surname>Stenetorp</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sebastian</forename><surname>Riedel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of AAAI</title>
		<meeting>of AAAI</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">OpenKE: An Open Toolkit for Knowledge Embedding</title>
		<author>
			<persName><forename type="first">Xu</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shulin</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xin</forename><surname>Lv</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yankai</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhiyuan</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Maosong</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Juanzi</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of EMNLP</title>
		<meeting>of EMNLP</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">CogKGE: A Knowledge Graph Embedding Toolkit and Benchmark for Representing Multi-source and Heterogeneous Knowledge</title>
		<author>
			<persName><forename type="first">Zhuoran</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tianyi</forename><surname>Men</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hongbang</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhitao</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dianbo</forename><surname>Sui</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chenhao</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhipeng</forename><surname>Xue</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yubo</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jun</forename><surname>Zhao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ACL</title>
		<meeting>ACL</meeting>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Do Pre-trained Models Benefit Knowledge Graph Completion? A Reliable Evaluation and a Reasonable Approach</title>
		<author>
			<persName><forename type="first">Xin</forename><surname>Lv</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yankai</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yixin</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lei</forename><surname>Hou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Juanzi</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhiyuan</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Peng</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jie</forename><surname>Zhou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Findings of the ACL</title>
		<editor>
			<persName><forename type="first">Preslav</forename><surname>Smaranda Muresan</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Aline</forename><surname>Nakov</surname></persName>
		</editor>
		<editor>
			<persName><surname>Villavicencio</surname></persName>
		</editor>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Language Models as Knowledge Bases?</title>
		<author>
			<persName><forename type="first">Fabio</forename><surname>Petroni</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tim</forename><surname>Rockt?schel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sebastian</forename><surname>Riedel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Patrick</forename><surname>Lewis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Anton</forename><surname>Bakhtin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuxiang</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alexander</forename><surname>Miller</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of EMNLP</title>
		<meeting>of EMNLP</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Sequence-to-Sequence Knowledge Graph Completion and Question Answering</title>
		<author>
			<persName><forename type="first">Apoorv</forename><surname>Saxena</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Adrian</forename><surname>Kochsiek</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rainer</forename><surname>Gemulla</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of ACL</title>
		<meeting>of ACL</meeting>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Rec: Sequential Recommendation with Bidirectional Encoder Representations from Transformer</title>
		<author>
			<persName><forename type="first">Fei</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jun</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jian</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Changhua</forename><surname>Pei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiao</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wenwu</forename><surname>Ou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Peng</forename><surname>Jiang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of CIKM</title>
		<meeting>CIKM</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="volume">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Representing Text for Joint Embedding of Text and Knowledge Bases</title>
		<author>
			<persName><forename type="first">Kristina</forename><surname>Toutanova</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Danqi</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Patrick</forename><surname>Pantel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hoifung</forename><surname>Poon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pallavi</forename><surname>Choudhury</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><surname>Gamon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of EMNLP</title>
		<meeting>of EMNLP</meeting>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Structure-Augmented Text Representation Learning for Efficient Knowledge Graph Completion</title>
		<author>
			<persName><forename type="first">Bo</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tao</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Guodong</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tianyi</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ying</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yi</forename><surname>Chang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of WWW</title>
		<meeting>of WWW</meeting>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">SimKGC: Simple Contrastive Knowledge Graph Completion with Pre-trained Language Models</title>
		<author>
			<persName><forename type="first">Liang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wei</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhuoyu</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jingming</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of ACL</title>
		<meeting>of ACL</meeting>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">From Discrimination to Generation: Knowledge Graph Completion with Generative Transformer</title>
		<author>
			<persName><forename type="first">Xin</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ningyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhoubo</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shumin</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hui</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Feiyu</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mosha</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Huajun</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of WWW</title>
		<meeting>of WWW</meeting>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">KG-BERT: BERT for Knowledge Graph Completion</title>
		<author>
			<persName><forename type="first">Liang</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chengsheng</forename><surname>Mao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuan</forename><surname>Luo</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019">2019. 2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">A Simple but Effective Pluggable Entity Lookup Table for Pre-trained Language Models</title>
		<author>
			<persName><forename type="first">Deming</forename><surname>Ye</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yankai</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Peng</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Maosong</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhiyuan</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of ACL</title>
		<meeting>of ACL</meeting>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">AliCG: Fine-grained and Evolvable Conceptual Graph Construction for Semantic Search at Alibaba</title>
		<author>
			<persName><forename type="first">Ningyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qianghuai</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shumin</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiang</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hongbin</forename><surname>Ye</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hui</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Huaixiao</forename><surname>Tou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gang</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhao</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nengwei</forename><surname>Hua</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Huajun</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of KDD</title>
		<meeting>of KDD</meeting>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<author>
			<persName><forename type="first">Ningyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xin</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiang</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shumin</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chuanqi</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fei</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xu</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Huajun</forename><surname>Chen</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2201.05575</idno>
		<title level="m">Reasoning through memorization: Nearest neighbor knowledge graph embeddings</title>
		<imprint>
			<date type="published" when="2022">2022. 2022</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">DeepKE: A Deep Learning Based Knowledge Extraction Toolkit for Knowledge Base Population</title>
		<author>
			<persName><forename type="first">Ningyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xin</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Liankuan</forename><surname>Tao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Haiyang</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hongbin</forename><surname>Ye</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shuofei</forename><surname>Qiao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xin</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiang</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhoubo</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lei</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of EMNLP Demonstration</title>
		<meeting>EMNLP Demonstration</meeting>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">KB4Rec: A Data Set for Linking Knowledge Bases with Recommender Systems</title>
		<author>
			<persName><forename type="first">Gaole</forename><surname>Wayne Xin Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kunlin</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hong-Jian</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jin</forename><surname>Dou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Siqi</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ji-Rong</forename><surname>Ouyang</surname></persName>
		</author>
		<author>
			<persName><surname>Wen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Data Intelligence</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="121" to="136" />
			<date type="published" when="2019">2019. 2019</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
