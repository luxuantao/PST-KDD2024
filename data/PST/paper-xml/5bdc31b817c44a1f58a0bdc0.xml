<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Action-Agnostic Human Pose Forecasting</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Hsu-Kuang</forename><surname>Chiu</surname></persName>
							<email>hkchiu@cs.stanford.edu</email>
							<affiliation key="aff0">
								<orgName type="institution">Stanford University</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Ehsan</forename><surname>Adeli</surname></persName>
							<email>eadeli@cs.stanford.edu</email>
							<affiliation key="aff0">
								<orgName type="institution">Stanford University</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Borui</forename><surname>Wang</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Stanford University</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">De-An</forename><surname>Huang</surname></persName>
							<email>dahuang@cs.stanford.edu</email>
							<affiliation key="aff0">
								<orgName type="institution">Stanford University</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Juan</forename><forename type="middle">Carlos</forename><surname>Niebles</surname></persName>
							<email>jniebles@cs.stanford.edu</email>
							<affiliation key="aff0">
								<orgName type="institution">Stanford University</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Action-Agnostic Human Pose Forecasting</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">06896058AA7A3C2CFD56F5D565A15EFD</idno>
					<idno type="DOI">10.1109/WACV.2019.00156</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.3" ident="GROBID" when="2023-07-27T09:59+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Forecasting human dynamics is a very interesting but challenging task with several prospective applications in robotics, health-care, among others. Researchers have recently developed methods for human pose forecasting; but unfortunately, they often introduce a number of simplification assumptions. For instance, previous work either focuses only on short-term or long-term predictions, while sacrificing one or the other. Furthermore, they use the activity labels as part of the training process, and require them to be available at testing time. These simplifications limit the usage of such pose forecasting models for real-world applications. To overcome these limitations, we propose a new action-agnostic method for short-and long-term human pose forecasting. Our triangular-prism recurrent neural network (TP-RNN) models the hierarchical and multi-scale characteristics of human dynamics. Our model captures the latent hierarchical structure in human pose sequences by encoding temporal dependencies with different time-scales. We run an extensive set of experiments on Human 3.6M and Penn Action datasets and show that our method outperforms baseline and stateof-the-art methods quantitatively and qualitatively. Code is available at https://github.com/eddyhkchiu/pose_ forecast_wacv/.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Humans are able to predict how their surrounding environment may change and how other people move. This inclination and aptitude is crucial to make social life and interaction with others attainable <ref type="bibr" target="#b3">[4]</ref>. As such, to create machines that can interact with humans seamlessly, it is very important to provide them with the ability of predicting short-and long-term future of human dynamics based on the immediate present and past. Recently, computer vision researchers attempted predicting human dynamics from images <ref type="bibr" target="#b11">[11]</ref>, or through time in videos <ref type="bibr" target="#b8">[8,</ref><ref type="bibr" target="#b31">31,</ref><ref type="bibr" target="#b37">37]</ref>. Human dynamics are usually defined as the temporal evolution of a set of structured body joints or poses <ref type="bibr" target="#b33">[33]</ref>. Therefore, predicting human dynamics refers to predicting the course of changes in human poses <ref type="bibr" target="#b11">[11,</ref><ref type="bibr" target="#b18">18,</ref><ref type="bibr" target="#b24">24,</ref><ref type="bibr" target="#b36">36,</ref><ref type="bibr" target="#b40">40,</ref><ref type="bibr" target="#b43">43]</ref>. Detecting and predicting poses has been of interest in the computer vision community <ref type="bibr" target="#b27">[27,</ref><ref type="bibr" target="#b33">33,</ref><ref type="bibr" target="#b43">43]</ref>. Some recent methods for forecasting human poses focus in the near future <ref type="bibr" target="#b11">[11,</ref><ref type="bibr" target="#b18">18,</ref><ref type="bibr" target="#b24">24,</ref><ref type="bibr" target="#b36">36,</ref><ref type="bibr" target="#b40">40]</ref>. Martinez et al. <ref type="bibr" target="#b23">[23]</ref> notes that although prior work has achieved advancements in pose forecasting, the methods often fail to generate realistic human poses in short-term predictions, and in many cases they do not outperform the zero-velocity predictor that repeats the very last seen pose as predictions for the future. Ghosh et al. <ref type="bibr" target="#b16">[16]</ref>, with reference to <ref type="bibr" target="#b23">[23]</ref>, attributes this finding to the side-effects of the common use of curriculum learning (such as in <ref type="bibr" target="#b11">[11]</ref>) for temporal forecasting. With this context, some prior work focuses on short-term forecasting of human poses <ref type="bibr" target="#b18">[18,</ref><ref type="bibr" target="#b23">23]</ref>, while some other exclusively aims at long-term predictions <ref type="bibr" target="#b8">[8,</ref><ref type="bibr" target="#b16">16,</ref><ref type="bibr" target="#b36">36]</ref>. However, most prior methods achieve reasonable performance by incorporating action labels as extra data annotation in their models. That is, they either train pose forecasters on each action class separately <ref type="bibr" target="#b15">[15,</ref><ref type="bibr" target="#b18">18]</ref> or incorporate the action labels as an input to the model. They conclude that the use of action labels improves the results <ref type="bibr" target="#b23">[23]</ref>. We observe that although it may be possible to acquire action labels for training samples, the assumption that action labels are available for testing videos is unrealistic and makes these models unusable for real-world applications, as action labels are not available during testing <ref type="bibr" target="#b0">[1]</ref>. Unlike such prior work, our method learns a pose forecaster without explicit use of the action class. We propose an action-agnostic model for pose forecasting by implicitly encoding the shortand long-term dependencies within actions.</p><p>In this paper, we propose a new recurrent neural network (RNN) model for forecasting human poses in both shortand long-term settings. To model human dynamics and to capture the latent hierarchical structure in the temporal pose sequences, we encode the temporal dependencies of different time-scales in a hierarchical interconnected sequence of RNN cells. Our proposed Triangular-Prism Recurrent Neural Network (TP-RNN) contains a new multi-phase hierarchical multi-scale RNN architecture tailored for modeling human dynamics in visual scenes. Unlike the original hierarchical multi-scale RNNs (HM-RNN) for representation of natural language sequences <ref type="bibr" target="#b12">[12]</ref>, our architecture uses a redefined hierarchy and multi-scale interconnections to accommodate human dynamics. We note that sequences of human poses through time involve hierarchical and multiscale structures, as movements of different body-parts (and joints) depend on each other. Furthermore, each of these parts (and joints) have distinct motion patterns and hence different temporal scales for particular activities. Taking 'walking' as an example, arms and legs move in a shorter temporal scale, or more frequently, compared to the torso, which is in a longer temporal scale (see Fig. <ref type="figure" target="#fig_0">1</ref>). Learning the hierarchical multi-scale dynamics of changes in human poses enables TP-RNN to construct an implicit encoding of short-and long-term dependencies within action classes, and hence be able to predict future sequences without the demand for the supervising signal from action labels.</p><p>Our model takes body joint velocities as inputs. These velocities are calculated as differences between the current and the immediate previous poses. Our model produces output predictions in the same space of velocities. As opposed to prior work <ref type="bibr" target="#b15">[15,</ref><ref type="bibr" target="#b18">18,</ref><ref type="bibr" target="#b40">40]</ref> that focuses on predicting sequences of poses in the form of either joint angles <ref type="bibr" target="#b15">[15]</ref> or joint locations <ref type="bibr" target="#b18">[18]</ref>, we argue that forecasting in the velocity space boosts prediction power since human poses change slightly in narrow time-steps. Note that in <ref type="bibr" target="#b23">[23]</ref> residual connections are applied on top of RNN, and the model produces residuals as outputs, with poses as input. However, our method uses velocities as both inputs and outputs and shows significantly improved forecasting results.</p><p>To evaluate our method, we run an extensive set of experiments on Human 3.6M <ref type="bibr" target="#b17">[17]</ref> and Penn Action <ref type="bibr" target="#b46">[46]</ref> datasets, and compare the results with several baseline and state-ofthe-art algorithms on these datasets. The comparison shows that our method outperforms others in terms of the mean angle error (MAE) on Human 3.6M and the Percentage of Correct Keypoint (PCK) score on Penn Action. Our actionagnostic method leads to superior results in cases of both short-and long-term predictions (some are visualized in Fig. <ref type="figure" target="#fig_0">1</ref>) even in comparison to the methods designed specifically for short-or long-term predictions or methods that use action labels as inputs to their models.</p><p>In summary, the contributions of this paper are three-fold: <ref type="bibr" target="#b0">(1)</ref> we propose an action-agnostic model that trains the pose forecaster without explicit use of action classes; (2) we propose a new model, TP-RNN for forecasting human dynamics, which implicitly encodes the action classes and does not require external action labels during training; (3) we show that operating in the velocity space, by using pose velocities as both inputs and outputs of the network, improves the results.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Work</head><p>In this Section, we review the relevant literature on human motion, activity, and pose forecasting, along with prior work on hierarchical and multi-scale RNNs (and Long Short-Term Memory cells, i.e., LSTMs). Predicting Motion and Human Dynamics: The majority of the recent work on motion representation has mainly focused on anticipating the future at the pixel level. For instance, generative adversarial networks (GANs) were used to generate video pixels <ref type="bibr" target="#b25">[25,</ref><ref type="bibr" target="#b37">37]</ref>, and RNNs for anticipating future video frames <ref type="bibr" target="#b22">[22]</ref>. To predict dense trajectories, Walker et al. <ref type="bibr" target="#b39">[39]</ref> used a CNN, and others have used random forests <ref type="bibr" target="#b28">[28]</ref> or variational auto-encoders <ref type="bibr" target="#b38">[38]</ref>. Other work targeted predicting the future in forms of semantic labels <ref type="bibr" target="#b20">[20,</ref><ref type="bibr" target="#b21">21,</ref><ref type="bibr" target="#b38">38]</ref> or activity labels <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b5">6,</ref><ref type="bibr" target="#b34">34,</ref><ref type="bibr" target="#b40">40]</ref>. Human dynamics, however, could be better characterized by 2D <ref type="bibr" target="#b10">[10,</ref><ref type="bibr" target="#b29">29,</ref><ref type="bibr" target="#b30">30]</ref> or 3D <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b26">26,</ref><ref type="bibr" target="#b35">35,</ref><ref type="bibr" target="#b45">45]</ref> poses, and several works attempted to detect these poses from images or videos <ref type="bibr" target="#b32">[32,</ref><ref type="bibr" target="#b35">35,</ref><ref type="bibr" target="#b45">45]</ref>. Modeling human motions is commonly defined in two different ways: probabilistic and state transition models, such as Bayesian and Gaussian processes <ref type="bibr" target="#b42">[42]</ref> or hidden Markov models <ref type="bibr" target="#b44">[44]</ref>, and deep learning methods <ref type="bibr" target="#b16">[16,</ref><ref type="bibr" target="#b18">18,</ref><ref type="bibr" target="#b23">23]</ref>, in particular RNNs and LSTMs. For instance, Jain et al. <ref type="bibr" target="#b18">[18]</ref> proposed a structural RNN to cast an arbitrary spatio-temporal graph as a RNN and use it for modeling human pose in temporal video sequences. In this work, we propose a new multi-phase hierarchical multi-scale RNN for modeling human dynamics to forecast poses. Human Pose Forecasting: Forecasting human poses in images and video sequences is relatively new compared to predicting image or video pixels. Although it can be a very useful task with great applications, such as predictive surveillance and patient monitoring, just recently researchers have paid more attention to it <ref type="bibr" target="#b8">[8,</ref><ref type="bibr" target="#b11">11,</ref><ref type="bibr" target="#b18">18,</ref><ref type="bibr" target="#b23">23,</ref><ref type="bibr" target="#b24">24,</ref><ref type="bibr" target="#b36">36,</ref><ref type="bibr" target="#b40">40]</ref>.</p><p>Specifically, Chao et al. <ref type="bibr" target="#b11">[11]</ref> proposed a 3D Pose Forecasting Network (3D-PFNet) for forecasting human dynamics from static images. Their method integrates recent advances on single-image human pose estimation and sequence prediction. In another work, <ref type="bibr" target="#b24">[24]</ref> introduced a method to predict 3D positions of the poses, given their 2D locations. Barsoum et al. <ref type="bibr" target="#b8">[8]</ref> proposed a sequence-to-sequence model for the task of probabilistic pose prediction, trained with an improved Wasserstein GAN <ref type="bibr" target="#b4">[5]</ref>. Walker et al. <ref type="bibr" target="#b40">[40]</ref> proposed a method based on variational autoencoders and GANs to predict possible future human movements (i.e., poses) and then predict future frames. Fragkiadaki et al. <ref type="bibr" target="#b15">[15]</ref> proposed two architectures for the task of pose prediction, one denoted by LSTM-3LR (3 layers of LSTM cells) and the second one as ERD (Encoder-Recurrent-Decoder). These two models are based on a sequence of LSTM units. Martinez et al. <ref type="bibr" target="#b23">[23]</ref> used a variation of RNNs to model human motion with the goal of learning time-dependent representations for human motion prediction synthesis in a short-term. Three key modifications to recent RNN models were introduced, in the architecture, loss function, and the training procedures. Bütepage et al. <ref type="bibr" target="#b9">[9]</ref> proposed an encoding-decoding network that learns to predict future 3D poses from the immediate past poses, and classify the pose sequences into action classes. These two methods incorporate a high-level supervision in the form of action labels, which itself improves the performance. However, in many real world applications of human motion analysis there are no motion or activity labels available during inference time. Hierarchical Multi-Scale RNNs: Our proposed architecture is inspired by the hierarchical multi-scale recurrent neural networks (HM-RNN) introduced in <ref type="bibr" target="#b12">[12]</ref>. HM-RNN builds on multi-scale RNNs <ref type="bibr" target="#b19">[19]</ref> that model high-level abstraction changes slowly with temporal coherency while low-level abstraction has quickly changing features sensitive to the precise local timing <ref type="bibr" target="#b14">[14]</ref>. This architecture is able to learn the latent representation of natural language sequences in different hierarchies (e.g., words, phrases, and sentences) to build character-level language models for predicting future sequences <ref type="bibr" target="#b7">[7,</ref><ref type="bibr" target="#b14">14]</ref>. We observe that multi-scale temporal information at different hierarchical levels can be beneficial in modeling human dynamics. However, it is difficult to adopt this approach directly because we do not have clear-cut temporal boundaries as in natural language data.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Triangular-Prism RNN (TP-RNN)</head><p>As discussed earlier, sequences of human poses can be subsumed under hierarchical and multi-scale structures, since movements of different body parts hinge on movements of other parts. Also, each part often has distinct motion patterns and hence different temporal scales when performing particular activities. Therefore, in contrast to the classical single-layer LSTM or RNN architectures (such as in <ref type="bibr" target="#b18">[18]</ref>) or stacked LSTMs (e.g., in <ref type="bibr" target="#b13">[13,</ref><ref type="bibr" target="#b41">41]</ref>), we introduce multi-phase hierarchical multi-scale upper layers of LSTM sequences to better learn the longer-term temporal relationships between different time-steps in a series of different granularities.</p><p>The inputs and outputs of the model, as mentioned earlier, are velocities. Let the pose in time t be identified by P t , then the velocity in time t can be defined as V t = P t -P t-1 . Therefore, for any given granularity coefficient K and the number of levels M , we define a multi-phase hierarchical multi-scale RNN with scale K and M levels. On the first level, we have a regular LSTM sequence taking the velocity information at each time-step as inputs. Then, on the second level, we define K different sequences of LSTM units, with each sequence only taking the inputs from the LSTM units on the first level at time-steps that are congruent modulo K. For example, if K = 2, then we have two LSTM sequences at level 2, with the first one taking inputs from t = {1, 3, 5, . . . } and the second one from t = {2, 4, 6, . . .} (see Fig. <ref type="figure" target="#fig_1">2</ref> for illustrations). Note that these LSTMs in the same level of the hierarchy share weights and this shifting scheme is actually used as a data augmentation strategy to learn longer-term dependencies in a more reliable way. Similarly, if we define a third level in the hierarchy, for each of the K LSTM sequences on the second level, we will have K different LSTM sequences each taking congruentmodulo-K inputs from it, resulting in a total of K 2 LSTM sequences in the third level. This process of spawning new higher-level LSTM sequences over the hierarchy continues for M -1 levels, which will have K M -1 LSTM sequences in the M th level. Therefore, logically, we have a total of</p><formula xml:id="formula_0">(K M -1 + K M -2 + . . . + K 3 + K 2 + K + 1)</formula><p>LSTM sequences in the whole architecture, while only M different ones are kept physically. Note, the LSTM sequences in each level share weights. For the sampling stage, we introduce a two-layer fully-connected network to generate the velocity predictions given the velocity at the current time-step and the corresponding hidden units across all hierarchies. Fig. <ref type="figure" target="#fig_1">2</ref> illustrates an example of our architecture for K = M = 2.</p><p>Our hierarchical model is inspired by HM-RNN <ref type="bibr" target="#b12">[12]</ref>, however, as discussed earlier, we cannot directly apply HM-RNN to our task, due to the differences between human dynamics and natural language sequences. TP-RNN models short-term dependencies in lower levels of the hierarchy and long-term temporal dependencies in higher levels, and hence can capture the latent hierarchical structure in different time-scales. Different from HM-RNN and unlike language models, since human dynamics do not have natural boundaries in the sequences, TP-RNN uses the outputs of all hierarchy levels to represent the sequence and predict the next element in the sequence. Moreover, instead of having only one RNN layer in each level of the hierarchies, as we move up in the hierarchy, TP-RNN decreases the resolution by one unit but has multiple RNN layers in the higher levels, to capture the temporal dynamics from different phases of its lower hierarchy level. All RNN layers in each single hierarchy level share parameters; although this scheme does Past Future Past Future not increase model parameters, shifting phases from each lower level to create their immediate higher level RNNs helps in augmenting the data and learning better models at each level. Therefore, as we go up in the hierarchy, more parallel RNN layers are incorporated, and hence we chose the name triangular-prism RNN (see Fig. <ref type="figure" target="#fig_1">2</ref>). This architecture design provides the following advantages over HM-RNN for modeling human dynamics: (1) the lowest layer RNN can learn the finest grained scale motion dynamics without the interference from the higher levels, and higher levels capture different characteristics of the dynamics each at a certain scale;</p><p>(2) during the prediction of each time-step, we have the most up-to-date RNN outputs from different hierarchies, each of which carries temporal motion information with different scales. On the contrary, HM-RNN does not provide the most up-to-date larger scale temporal information when the current prediction time-step is not right after a boundary.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Experiments</head><p>We evaluate our method on two challenging datasets. The results are analyzed and compared with baseline and stateof-the-art techniques, both quantitatively and qualitatively.</p><p>In our architecture, we use LSTMs with hidden size 1024 as the RNN cells (the orange and the green blocks in Fig. <ref type="figure" target="#fig_1">2</ref>). For the final pose velocity generation networks (the red blocks in Fig. <ref type="figure" target="#fig_1">2</ref>), we use 2 fully-connected layers with hidden sizes 256 and 128, followed by a Leaky-ReLU nonlinearity layer. The training setup is similar to <ref type="bibr" target="#b23">[23]</ref>. The optimization uses mini-batch stochastic gradient descent with batch size 16, clipping the gradients up to 2 -norm value of 5. The learning rate is initialized to 0.01 and decayed along the training iterations. We train for 100,000 iterations and record the performance coverage in the end.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Datasets</head><p>To test the performance of our model for human pose forecasting, we run extensive experiments using Human 3.6M <ref type="bibr" target="#b17">[17]</ref> and Penn Action <ref type="bibr" target="#b46">[46]</ref> datasets. Human 3.6M dataset: The Human 3.6M dataset <ref type="bibr" target="#b17">[17]</ref> is one of the largest publicly available datasets of human motion capture data. This dataset contains video sequences of a total of 15 different human activity categories, each performed by seven actors in two different trials. The videos were recorded at 50Hz (i.e., 20ms between each two consecutive pose frames). Following previous work <ref type="bibr" target="#b18">[18,</ref><ref type="bibr" target="#b23">23]</ref>, in our experiments, we downsample the pose sequence by 2. In the dataset, each pose is represented as exponential map representations of 32 human joints in the 3D space, and during evaluation, we employ the measurement of the Euclidean distance between the ground-truth pose and our predicted pose in the angle space as the error metric. Consistent with the previous work, we also use Subject 5 as the test data and Subjects 1, 6, 7, 8, 9, 11 as training. Similar to <ref type="bibr" target="#b18">[18,</ref><ref type="bibr" target="#b23">23]</ref>, we train our models using the past 50 frames (2000ms) as the input sequence, and forecast the future 25 frames (1000ms). The training loss is calculated by the mean angle error (MAE) from each of the predicted future frames. Penn Action dataset: The second dataset we experiment on is the Penn Action dataset <ref type="bibr" target="#b46">[46]</ref>, which contains 2326 video sequences of 15 different actions and human joint annotations for each sequence. Each human pose is represented by 13 human joint coordinates in the 2D space. Following the same data split of <ref type="bibr" target="#b11">[11,</ref><ref type="bibr" target="#b46">46]</ref>, 1258 video sequences are used for training and the remaining 1068 video sequences are used for testing. For this dataset, the previous state-of-the-art, 3D-PFNet <ref type="bibr" target="#b11">[11]</ref>, takes the first frame image as input, and outputs the poses extracted from that frame and the future 15 frames, resulting in total of 16 frame poses. The model performance is evaluated using PCK@0.05 <ref type="bibr" target="#b11">[11]</ref>. For the experiments on this dataset, we use a single pose in a past frame (ignoring the frame image) as the input, and the outputs are the predictions of poses of the future 16 frames. Although the input format of <ref type="bibr" target="#b11">[11]</ref> is slightly different from ours, it is still a fair comparison of pose forecasting capabilities.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Results</head><p>Baseline Methods: We use the following recent research to compare with: ERD <ref type="bibr" target="#b15">[15]</ref>, LSTM-3LR <ref type="bibr" target="#b15">[15]</ref>, SRNN <ref type="bibr" target="#b18">[18]</ref>, Dropout-AutoEncoder <ref type="bibr" target="#b16">[16]</ref>, 3D-PFNet <ref type="bibr" target="#b11">[11]</ref>, and Residual <ref type="bibr" target="#b23">[23]</ref>. Similar to <ref type="bibr" target="#b23">[23]</ref>, we include the zero-velocity model as a naïve baseline for comparison. We also include our implementations of different LSTM-based models as part of the comparison (i.e., conducting ablation tests).</p><p>First set of our experiments compares the single layer LSTM model with pose as the input (denoted by Single Layer (Pose)) and the same model but with velocity as the input (Single Layer (Vel.)), to demonstrate that conducting the experiments in the velocity space and feeding it into LSTM sequences can better capture the human motion dynamics. As mentioned earlier, when both inputs and outputs are all velocities with similar small numerical scales, it is easier for the model to be trained. In the second set of experiments, we build multiple 2-Layer LSTM models with different architectures using velocity as the input, including the most basic one that simply stacks 2 layers of LSTMs (Stacked 2-Layer (Vel.)), commonly called multi-layer LSTM <ref type="bibr" target="#b13">[13,</ref><ref type="bibr" target="#b41">41]</ref>. On top of the basic model, we build further extensions with hierarchical and multi-scale structures: two independent LSTMs (Double-scale (Vel.)), which, unlike the regular multi-layer LSTM, its higher level one does not use the output from the lower level as the input. Instead, the higher level LSTM input is the larger scale of velocity, i.e., the velocity calculated by the pose sequence only at the odd time-steps, or only at the even time-steps. The next model (Double-scale (Hier., Vel.)) is similar to HM-RNN <ref type="bibr" target="#b12">[12]</ref>, but with slight modification of setting the higher level LSTM scale to a constant number 2, due to the fact that there is no natural boundary in human motion sequences. Another model (Double-scale (Phase, Vel.)) has multiple phases in the higher level LSTMs, capturing larger scale velocity information, rather than using the lower level LSTM outputs. Finally, we implement our proposed model (TP-RNN) with double scale setting. Note, to showcase the superiority of the proposed technique we report the results for K = M = 2 in TP-RNN, which demonstrates that without the need to increase the network parameters, our network already outperforms all other methods. However, we also conduct an experiment for analyzing the effect of the number of levels in TP-RNN, and show models with more hierarchies can lead to even better results. Comparison on Human 3.6M dataset: Previous literature published their performance numbers on either short-term (up to 400ms) <ref type="bibr" target="#b23">[23]</ref> or long-term (up to 1000ms) <ref type="bibr" target="#b18">[18]</ref> predictions. Besides, some of them only report the prediction on a small set of actions (i.e., 'walking', 'eating', 'smoking', and 'discussion') <ref type="bibr" target="#b15">[15,</ref><ref type="bibr" target="#b18">18]</ref>, while others report the results for all 15 actions <ref type="bibr" target="#b23">[23]</ref> in the Human 3.6M dataset <ref type="bibr" target="#b17">[17]</ref>. To compare with all the above different settings, for each of our architectures, we train a single action-agnostic model using sequence data from all of the 15 actions, without any supervision from the ground-truth action labels. We use the loss over each forecasted frame up to 1000ms (25 frames). We follow the settings of <ref type="bibr" target="#b18">[18,</ref><ref type="bibr" target="#b23">23]</ref> for the length of the input seed observed pose, which is 2000ms or 50 frames.</p><p>Table <ref type="table">1</ref> shows the MAE for 'walking', 'eating', 'smoking', and 'discussion' for short-term predictions. Our model (TP-RNN) outperforms all the baseline results, including the current state-of-the-art, Residual model <ref type="bibr" target="#b23">[23]</ref>, in short-term forecasting. In the Residual model <ref type="bibr" target="#b23">[23]</ref>, pose information is used to predict the velocity of the next frame. Note that the numerical scale of velocity is much smaller compared to the pose. On the contrary, in our proposed model, velocity information of the past is fed into the models to predict the next velocity. Therefore, the scales of inputs and outputs are the same, which potentially puts the neural network in an easier path to train. For actions with large movements, like 'Walking' and 'Eating', our model outperforms the baselines and the state-of-the-art by a large margin. However, similar to the prior work, our method has hard time to forecast 'difficult-to-predict' actions like 'Smoking' and 'Discussion'. Although, our results in those activities are also superior to all other methods, they are close to the zero-velocity baseline. Our proposed TP-RNN is setting a new state-of-the-art for pose forecasting on this dataset without the need of action labels at test time. Furthermore, it conducts both short-and long-term predictions simultaneously without sacrificing the accuracy of either end.</p><p>Table <ref type="table" target="#tab_1">2</ref> shows the MAE for the same set of the four actions in the long-term prediction task. The state-of-the-art model, Residual <ref type="bibr" target="#b23">[23]</ref>, does not report the long-term prediction performance results, therefore we use its open-source implementation code to collect the results for long-term predictions. Note that when changing the training loss from short-term predictions to long-term predictions, this model sacrifices the prediction accuracy in the short-term timerange (less than 400ms) in order to gain the extra long-term (400ms to 1000ms) prediction ability.</p><p>The long-term prediction of the Residual model <ref type="bibr" target="#b23">[23]</ref> still outperforms other prior works in most of the cases. Another strong previous work in long-term forecasting is the Dropout-AutoEncoder model <ref type="bibr" target="#b16">[16]</ref>, which generates the best 1000ms Table <ref type="table">1</ref>: MAE for four action classes in the short-term forecasting experiment (prior-work results from <ref type="bibr" target="#b23">[23]</ref>). In each column, the best obtained results are typeset in boldface and the second best are underlined. AA: Action-Agnostic, N/A: Not Applicable.   prediction for the 'Discussion' action among all other models. Similar to short-term predictions, our proposed velocitybased model outperforms all the baseline and state-of-theart methods, except for the Dropout-AutoEncoder model in 1000ms prediction with respect to only the 'Discussion' action. Note that our model conducts an action-agnostic forecasting and does not sacrifice the short-term or long-term predictions. Our results in comparison with other methods that are either trained for each action separately <ref type="bibr" target="#b15">[15,</ref><ref type="bibr" target="#b18">18]</ref> or only target short-or long-term predictions <ref type="bibr" target="#b16">[16,</ref><ref type="bibr" target="#b23">23]</ref> show better overall performance. As our models are trained using all 15 actions in Human 3.6M <ref type="bibr" target="#b17">[17]</ref>, without any extra supervision from the action labels, we further evaluate the proposed method by reporting the average MAE for all time-points across all 15 action categories. In Tables <ref type="table" target="#tab_2">3</ref> and<ref type="table" target="#tab_3">4</ref>, we compare our results with the current state-of-the-art model <ref type="bibr" target="#b23">[23]</ref>, which is the only previous research experimented on all 15 action classes. Table <ref type="table" target="#tab_2">3</ref> shows the long-term forecasting results of the remaining 11 action categories, not included in Table <ref type="table" target="#tab_1">2</ref>. As can be seen, our proposed TP-RNN model performs better than <ref type="bibr" target="#b23">[23]</ref> in most of the action categories. Table <ref type="table" target="#tab_3">4</ref> summarizes the short-term and long-term results from the current state-of-the-art model <ref type="bibr" target="#b23">[23]</ref> (from the paper and the code, both when including action labels as inputs to the model or not), and the results from our proposed velocity-based models by showing the average MAE for all time-points across all 15 action categories. Our models outperform the Residual model, in both short-term and long-term prediction tasks. We use the average MAE metric to also conduct the ablation analysis by evaluating the difference between each of our model extensions with hierarchical and multi-scale architectures. We can see that basic multi-layer LSTM model 'Stacked 2-Layer (Vel.)' does not improve the overall performance compared with single layer LSTM model 'Single Layer (Vel.)'. After we include the multi-scale idea in our models, we can see the improvement over the single layer model 'Single Layer (Vel.)' and the basic multi-layer single-scale For the long-term time-range, the model directly adapted from HM-RNN <ref type="bibr" target="#b12">[12]</ref> 'Double-scale (Hier., Vel.)' does not improve the performance, because the original design of the HM-RNN model is for the natural language data with obvious boundaries in the sequence. We mitigate this limitation with our proposed TP-RNN model, which multiple phases of LSTM on the higher levels for capturing the up-todate larger scale temporal information. TP-RNN provides best quantitative results, as shown in Table <ref type="table" target="#tab_3">4</ref>. In summary, Table <ref type="table" target="#tab_3">4</ref> shows that the previous state-ofthe-art (i.e., Residual <ref type="bibr" target="#b23">[23]</ref>) needs to compromise between short-and long-term forecasting accuracy. However, TP-RNN model performs better, especially for the long-term forecasting (1000ms in future) without sacrificing the shortterm accuracy. In terms of numbers, our long-term forecasting improvement is 1.83-1.71 1.83 ∝ 6.56%, which is not negligible. Besides, Table <ref type="table" target="#tab_1">2</ref> shows that for certain action categories with hierarchical multi-scale motions (e.g., 'Walking'), long-term forecasting improvement is even more significant: 0.96-0.77 0.96 ∝ 19.79%. To further test the significance of the improvements, we conduct a one-tailed paired t-test between our average results and those of <ref type="bibr" target="#b23">[23]</ref>. The p-value of the test equals 0.0002, which by all conventional criteria the difference between the two sets of results is considered to be extremely statistically significant.</p><p>Deeper Hierarchical Structure of TP-RNN: In the previous subsections, we showed that even with the most basic architectural settings of M = 2 and K = 2, TP-RNN already outperforms the state-of-the-art. In this section, we further experiment and analyze the effect of increasing the number of levels M . Fig. <ref type="figure" target="#fig_2">3</ref>(b) shows the average MAE of the long-term forecasting (1000ms) results from TP-RNN with different numbers of levels: M ∈ {2, 3, 4, 5}. In general, increasing the number of levels of TP-RNN improves the longterm forecasting accuracy, which is in accordance with our hypothesis that higher hierarchical levels are able to better capture long-term human motion dynamics and thus improve the long-term forecasting accuracy. In addition, Fig <ref type="figure" target="#fig_2">3(a)</ref> shows similar results for short-term forecasting (400ms), which also indicates that the performance improves slightly when increasing the number of levels. However, when we increase the number of TP-RNN levels to 4 for short-term or 5 for long-term, we see a decline in the performance. The reason is that the LSTM cell(s) at the 5 th level only update once at every 2 5-1 = 16 time-step, which is too long, given that we only predict the future 25 frames with very few updates for M = 5. Besides, with deeper hierarchical structures, TP-RNN has more trainable parameters and therefore is prone to overfitting (requires more data).</p><p>Qualitative Evaluations and Visualization: Fig. <ref type="figure" target="#fig_3">4</ref> shows the visualization of pose sequences for our method, in comparisons with the Residual method <ref type="bibr" target="#b23">[23]</ref>, for the sequences for the actions 'Walking' and 'Smoking'. As it can be seen in the figure, our predictions (the middle row) are visually closer to the ground-truth (last row) compared to the state-of-the-art Residual method <ref type="bibr" target="#b23">[23]</ref>. This also supports the quantitative results shown in Table <ref type="table" target="#tab_1">2</ref>, as our method steadily outperforms <ref type="bibr" target="#b23">[23]</ref> for the 'walking' activity: TP-RNN's MAE was 0.25 at 80ms while <ref type="bibr" target="#b23">[23]</ref> led to an MAE of 0.32, similarly, ours was 0.75 and 0.77 in 560ms and 1000ms while <ref type="bibr" target="#b23">[23]</ref> obtained 0.86 and 0.96, respectively. A similar conclusion can be made for predictions of the 'smoking' activity. Although, this activity has slight amounts of movement, still our method can cap-  ture better dynamics, if we look at the results closely. For instance, using our method, the distance of the subject's hand from the torso and face are better predicted in long-term and leg movements are more precisely forecasted, especially in shorter-term predictions. More visualizations of the forecasted poses by TP-RNN were visualized in Fig. <ref type="figure" target="#fig_0">1</ref>, in which one can simply observe how the patterns of poses change through time, in comparison with the ground-truth.</p><p>Comparison on Penn Action Dataset: We trained TP-RNN on Penn Action dataset <ref type="bibr" target="#b46">[46]</ref> and here we compare its results with the previous state-of-the-art on this dataset, 3D-PFNet <ref type="bibr" target="#b11">[11]</ref>. The input to our velocity-based TP-RNN is the pose in a single frame, and we set the initial velocity to 0 (denoted by TP-RNN w/o init vel.), in order to have a fair comparison with <ref type="bibr" target="#b11">[11]</ref>. The model performance is evaluated using PCK@0.05 as in <ref type="bibr" target="#b11">[11]</ref>. PCK calculates the percentage of joint locations correctly predicted by the model. With the threshold 0.05, a joint location is counted as correctly predicted if the normalized distance between its predicted and ground-truth locations is less than 0.05. The results are shown in Table <ref type="table" target="#tab_4">5</ref>. Our model performs significantly better than 3D-PFNet <ref type="bibr" target="#b11">[11]</ref> (a p-value of 0.0419 &lt; 0.05 significance threshold), with the results shown in Table <ref type="table" target="#tab_4">5</ref>. For further comparison, we used the open-sourced code of <ref type="bibr" target="#b23">[23]</ref> with the necessary modifications (the same setting as our TP-RNN), and experimented on the Penn Action dataset. Additionally, we further experiment by including the initial velocity in the input, to demonstrate that the importance of the velocity information. The additional initial velocity is estimated using the difference between the current pose and the previous pose, our TP-RNN model generates even better forecasting results, as shown in the last row in Table <ref type="table" target="#tab_4">5</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Conclusion</head><p>In this paper, inspired by the success of hierarchical multiscale RNN (HM-RNN) frameworks in the natural language processing applications, we proposed a new model to encode different hierarchies in human dynamics at different timescales. Our model trains a set of RNNs (as LSTM sequences) at each hierarchy with different time-scales. Within each level of the hierarchy, RNNs share their learnable weights, since they are all learning a same concept in a same timescale but with different phases, i.e., different starting points of the sequence. This dramatically decreases the number of parameters to be learned in the model, while involving as much data as possible to train the higher level RNNs. As a result, the lowest layer can learn the finest grained scale motion dynamics, and higher levels capture different characteristics of the dynamics each at a certain time-scale. Furthermore, we set up a more rigorous but realistic experimental settings by conducting an action-agnostic forecasting and predicting both short-and long-term sequences simultaneously. In this in contrast to the prior work, which often limited their settings. Despite these strict settings, our results on the Human 3.6M dataset and the Penn Action dataset show superior performance for TP-RNN to the baseline and state-of-theart methods, in terms of both quantitative evaluations and qualitative visualizations.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: Ground-truth pose sequences (top) and our forecasted sequences (bottom). Solid colors indicate later timesteps and faded colors indicate earlier ones. Changes in the predicted and ground-truth poses resemble similar patterns. Furthermore, body-part movement patterns show that different parts depend on each other, but with varied temporal scales. Hence, hierarchical multi-scale modeling may encode the latent structures of human dynamics.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: Architecture of TP-RNN with K = 2 and M = 2. Left: 3D view of the triangular-prism RNN. Right: 2D projection.</figDesc><graphic coords="4,237.92,174.45,139.94,102.31" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: Average MAEs of short-and long-term forecasting using TP-RNN with different levels: M ∈ {2, 3, 4, 5}.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 4 :</head><label>4</label><figDesc>Figure 4: Visualization of long-term pose-forecasting (up to 1000ms) for the actions 'Walking' (top) and 'Smoking' (bottom), downsampled by a factor of 2 (13 forecasted poses out of 25 are visualized). The purple poses are ground-truth data, including past (in the left side) and future time frames (in the bottom). The blue and red poses are the predictions of [23] and our method (TP-RNN), respectively.</figDesc><graphic coords="8,62.28,282.62,134.18,78.31" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 :</head><label>2</label><figDesc>MAE for four action classes in the long-term forecasting experiments (prior-work results from SRNN<ref type="bibr" target="#b18">[18]</ref>, Dropout-AutoEncoder<ref type="bibr" target="#b16">[16]</ref>, and code from<ref type="bibr" target="#b23">[23]</ref>). In each column, the best obtained results are typeset with boldface and the second best are underlined. AA: Action-Agnostic, N/A: Not Applicable, Dropout-AE: Dropout-AutoEncoder. 3LR<ref type="bibr" target="#b15">[15]</ref> 1.<ref type="bibr" target="#b18">18</ref> 1.50 1.67 1.81 2.20 1.36 1.79 2.29 2.49 2.82 2.05 2.34 3.10 3.24 3.42 2.25 2.33 2.45 2.48 2.93 SRNN [18] 1.08 1.34 1.60 1.90 2.13 1.35 1.71 2.12 2.28 2.58 1.90 2.30 2.90 3.21 3.23 1.67 2.03 2.20 2.39 2.43 Dropout-AE [16] 1.00 1.11 1.39 1.55 1.39 1.31 1.49 1.86 1.76 2.01 0.92 1.03 1.15 1.38 1.77 1.11 1.20 1.38 1.53 1.73 Residual [23] 0.32 0.54 0.72 0.86 0.96 0.25 0.42 0.64 0.94 1.30 0.33 0.60 1.01 1.23 1.83 0.34 0.74 1.04 1.43 1.75 Zero-velocity N/A 0.39 0.68 0.99 1.35 1.32 0.27 0.48 0.73 1.04 1.38 0.26 0.48 0.97 1.02 1.69 0.31 0.67 0.94 1.41 1.96 TP-RNN (Ours) 0.25 0.41 0.58 0.74 0.77 0.20 0.33 0.53 0.84 1.14 0.26 0.48 0.88 0.98 1.66 0.30 0.66 0.98 1.39 1.74</figDesc><table><row><cell>AA</cell><cell>Walking</cell><cell>Eating</cell><cell>Smoking</cell><cell>Discussion</cell></row><row><cell>milliseconds</cell><cell cols="4">80 160 320 560 1000 80 160 320 560 1000 80 160 320 560 1000 80 160 320 560 1000</cell></row><row><cell>ERD [15]</cell><cell cols="4">1.30 1.56 1.84 2.00 2.38 1.66 1.93 2.88 2.36 2.41 2.34 2.74 3.73 3.68 3.82 2.67 2.97 3.23 3.47 2.92</cell></row><row><cell>LSTM-</cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 3 :</head><label>3</label><figDesc>Long-term forecasting MAE comparison for the remaining 11 actions in Human 3.6 dataset.</figDesc><table><row><cell></cell><cell>Directions</cell><cell>Greeting</cell><cell>Talking on the phone</cell><cell>Posing</cell></row><row><cell cols="5">millisec 80 160 320 400 560 1000 80 160 320 400 560 1000 80 160 320 400 560 1000 80 160 320 400 560 1000</cell></row><row><cell>[23]</cell><cell cols="4">0.44 0.69 0.83 0.94 1.03 1.49 0.53 0.88 1.29 1.45 1.72 1.89 0.61 1.12 1.57 1.74 1.59 1.92 0.47 0.87 1.49 1.76 1.96 2.35</cell></row><row><cell cols="5">TP-RNN 0.38 0.59 0.75 0.83 0.95 1.38 0.51 0.86 1.27 1.44 1.72 1.81 0.57 1.08 1.44 1.59 1.47 1.68 0.42 0.76 1.29 1.54 1.75 2.47</cell></row><row><cell></cell><cell>Purchases</cell><cell>Sitting</cell><cell>Sitting down</cell><cell>Taking photo</cell></row><row><cell cols="5">millisec 80 160 320 400 560 1000 80 160 320 400 560 1000 80 160 320 400 560 1000 80 160 320 400 560 1000</cell></row><row><cell>[23]</cell><cell cols="4">0.60 0.86 1.24 1.30 1.58 2.26 0.44 0.74 1.19 1.40 1.57 2.03 0.51 0.93 1.44 1.65 1.94 2.55 0.33 0.65 0.97 1.09 1.19 1.47</cell></row><row><cell cols="5">TP-RNN 0.59 0.82 1.12 1.18 1.52 2.28 0.41 0.66 1.07 1.22 1.35 1.74 0.41 0.79 1.13 1.27 1.47 1.93 0.26 0.51 0.80 0.95 1.08 1.35</cell></row><row><cell></cell><cell>Waiting</cell><cell>Walking dog</cell><cell>Walking together</cell><cell>Average of all 15</cell></row><row><cell cols="5">millisec 80 160 320 400 560 1000 80 160 320 400 560 1000 80 160 320 400 560 1000 80 160 320 400 560 1000</cell></row><row><cell>[23]</cell><cell cols="4">0.34 0.65 1.09 1.28 1.61 2.27 0.56 0.95 1.28 1.39 1.68 1.92 0.31 0.61 0.84 0.89 1.00 1.43 0.43 0.75 1.11 1.24 1.42 1.83</cell></row><row><cell cols="5">TP-RNN 0.30 0.60 1.09 1.31 1.71 2.46 0.53 0.93 1.24 1.38 1.73 1.98 0.23 0.47 0.67 0.71 0.78 1.28 0.37 0.66 0.99 1.11 1.30 1.71</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 4 :</head><label>4</label><figDesc>Comparison of average MAE across all 15 actions of Human 3.6 dataset with prior and baseline models (ablation study). In each column, the best results are typeset in bold and the second best are underlined. For 'Residual<ref type="bibr" target="#b23">[23]</ref>', both short-term (from paper) and long-term (from code) results are reported. AA: Action-Agnostic.</figDesc><table><row><cell></cell><cell cols="2">AA 80 160 320 400 560 1000</cell></row><row><cell>Residual [23] (short)</cell><cell>0.36 0.67 1.02 1.15 -</cell><cell>-</cell></row><row><cell>Residual [23] (short)</cell><cell>0.39 0.72 1.08 1.22 -</cell><cell>-</cell></row><row><cell>Residual [23] (long)</cell><cell cols="2">0.43 0.75 1.11 1.24 1.42 1.83</cell></row><row><cell>Residual [23] (long)</cell><cell cols="2">0.42 0.73 1.09 1.23 1.42 1.84</cell></row><row><cell>Zero-velocity</cell><cell cols="2">-0.40 0.71 1.07 1.21 1.42 1.85</cell></row><row><cell>Single Layer (Pose)</cell><cell cols="2">0.49 0.83 1.20 1.34 1.53 1.92</cell></row><row><cell>Single Layer (Vel.)</cell><cell cols="2">0.39 0.67 1.00 1.13 1.32 1.73</cell></row><row><cell cols="3">Stacked 2-Layer (Vel.) 0.38 0.66 1.01 1.13 1.32 1.74</cell></row><row><cell>Double-scale (Vel.)</cell><cell cols="2">0.37 0.66 0.99 1.11 1.30 1.73</cell></row><row><cell cols="3">Double-scale (Hier., Vel.) 0.37 0.66 1.00 1.12 1.32 1.76</cell></row><row><cell cols="3">Double-scale (Phase, Vel.) 0.37 0.66 1.00 1.12 1.31 1.72</cell></row><row><cell>TP-RNN (Ours)</cell><cell cols="2">0.37 0.66 0.99 1.11 1.30 1.71</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 5 :</head><label>5</label><figDesc>Comparison with prior works on the Penn Action dataset in terms of PCK@0.05. Best results are typset in bold, and the second best are underlined. TP-RNN is tested with or without incorporating an initial pose velocity (details in the text).</figDesc><table><row><cell>Future frame</cell><cell>1</cell><cell>2</cell><cell>3</cell><cell>4</cell><cell>5</cell><cell>6</cell><cell>7</cell><cell>8</cell><cell>9</cell><cell>10</cell><cell>11</cell><cell>12</cell><cell>13</cell><cell>14</cell><cell>15</cell><cell>16</cell></row><row><cell>Residual [23]</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Acknowledgements We thank Panasonic for their support.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Multi-label discriminative weakly-supervised human activity recognition and localization</title>
		<author>
			<persName><forename type="first">E</forename><surname>Adeli-Mosabbeb</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Cabral</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>De La Torre</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Fathy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Asian Conference on Computer Vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="241" to="258" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Encouraging lstms to anticipate actions very early</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">S</forename><surname>Aliakbarian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><forename type="middle">S</forename><surname>Saleh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Salzmann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Fernando</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Petersson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Andersson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="280" to="289" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Monocular 3d pose estimation and tracking by detection</title>
		<author>
			<persName><forename type="first">M</forename><surname>Andriluka</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Roth</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Schiele</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision and Pattern Recognition (CVPR), 2010 IEEE Conference on</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2010">2010</date>
			<biblScope unit="page" from="623" to="630" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">Non-verbal communication in human social interaction</title>
		<author>
			<persName><forename type="first">M</forename><surname>Argyle</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1972">1972</date>
			<publisher>Cambridge U. Press</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title/>
		<author>
			<persName><forename type="first">M</forename><surname>Arjovsky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Chintala</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Bottou</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1701.07875</idno>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
	<note type="report_type">Wasserstein gan. arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Structured prediction with short/longrange dependencies for human activity recognition from depth skeleton data</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">M</forename><surname>Arzani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Fathy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Aghajan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">A</forename><surname>Azirani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Raahemifar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Adeli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Intelligent Robots and Systems (IROS)</title>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m">IEEE/RSJ International Conference on</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="560" to="567" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">End-to-end attention-based large vocabulary speech recognition</title>
		<author>
			<persName><forename type="first">D</forename><surname>Bahdanau</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Chorowski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Serdyuk</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Brakel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Acoustics, Speech and Signal Processing (ICASSP), 2016 IEEE International Conference on</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="4945" to="4949" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<author>
			<persName><forename type="first">E</forename><surname>Barsoum</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Kender</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1711.09561</idno>
		<title level="m">Hp-gan: Probabilistic 3d human motion prediction via gan</title>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Deep representation learning for human motion prediction and classification</title>
		<author>
			<persName><forename type="first">J</forename><surname>Bütepage</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">J</forename><surname>Black</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Kragic</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Kjellström</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page">2017</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Realtime multiperson 2d pose estimation using part affinity fields</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Simon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S.-E</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Sheikh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Forecasting human dynamics from static images</title>
		<author>
			<persName><forename type="first">Y.-W</forename><surname>Chao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Price</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Cohen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Hierarchical multiscale recurrent neural networks</title>
		<author>
			<persName><forename type="first">J</forename><surname>Chung</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Ahn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In ICLR</title>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Long-term recurrent convolutional networks for visual recognition and description</title>
		<author>
			<persName><forename type="first">J</forename><surname>Donahue</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Hendricks</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Guadarrama</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Rohrbach</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Venugopalan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Saenko</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="2625" to="2634" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Hierarchical recurrent neural networks for long-term dependencies</title>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">El</forename><surname>Hihi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="1996">1996</date>
			<biblScope unit="page" from="493" to="499" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Recurrent network models for human dynamics</title>
		<author>
			<persName><forename type="first">K</forename><surname>Fragkiadaki</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Levine</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Felsen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision (ICCV), 2015 IEEE International Conference on</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="4346" to="4354" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">Learning human motion models for long-term predictions</title>
		<author>
			<persName><forename type="first">P</forename><surname>Ghosh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Aksan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Hilliges</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1704.02827</idno>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Human3.6M: Large scale datasets and predictive methods for 3d human sensing in natural environments</title>
		<author>
			<persName><forename type="first">C</forename><surname>Ionescu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Papava</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Olaru</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Sminchisescu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE transactions on pattern analysis and machine intelligence</title>
		<imprint>
			<biblScope unit="volume">36</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="1325" to="1339" />
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Structuralrnn: Deep learning on spatio-temporal graphs</title>
		<author>
			<persName><forename type="first">A</forename><surname>Jain</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">R</forename><surname>Zamir</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Savarese</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Saxena</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="5308" to="5317" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">A clockwork rnn</title>
		<author>
			<persName><forename type="first">J</forename><surname>Koutnik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Greff</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Schmidhuber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="1863" to="1871" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">A hierarchical representation for future action prediction</title>
		<author>
			<persName><forename type="first">T</forename><surname>Lan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T.-C</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Savarese</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="689" to="704" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Predicting deeper into the future of semantic segmentation</title>
		<author>
			<persName><forename type="first">P</forename><surname>Luc</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Neverova</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Couprie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Verbeek</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">of: ICCV 2017-International Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page">10</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Geometry-based next frame prediction from monocular video</title>
		<author>
			<persName><forename type="first">R</forename><surname>Mahjourian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Wicke</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Angelova</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Intelligent Vehicles Symposium (IV)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2017">2017. 2017</date>
			<biblScope unit="page" from="1700" to="1707" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">On human motion prediction using recurrent neural networks</title>
		<author>
			<persName><forename type="first">J</forename><surname>Martinez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">J</forename><surname>Black</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Romero</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">A simple yet effective baseline for 3d human pose estimation</title>
		<author>
			<persName><forename type="first">J</forename><surname>Martinez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Hossain</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Romero</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">J</forename><surname>Little</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Deep multi-scale video prediction beyond mean square error</title>
		<author>
			<persName><forename type="first">M</forename><surname>Mathieu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Couprie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Vnect: Realtime 3d human pose estimation with a single rgb camera</title>
		<author>
			<persName><forename type="first">D</forename><surname>Mehta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Sridhar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Sotnychenko</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Rhodin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Shafiei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H.-P</forename><surname>Seidel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Casas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Theobalt</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Transactions on Graphics (TOG)</title>
		<imprint>
			<biblScope unit="volume">36</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page">44</biblScope>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<title level="m" type="main">3d human pose estimation using 2d-data and an alternative phase space representation</title>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">B</forename><surname>Moeslund</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Granum</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2000">2000. 2000</date>
		</imprint>
	</monogr>
	<note>Procedure Humans</note>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Déja vu</title>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">L</forename><surname>Pintea</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">C</forename><surname>Van Gemert</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">W</forename><surname>Smeulders</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="172" to="187" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Randomized trees for human pose detection</title>
		<author>
			<persName><forename type="first">G</forename><surname>Rogez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Rihan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Ramalingam</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Orrite</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">H</forename><surname>Torr</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision and Pattern Recognition</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2008">2008. 2008</date>
			<biblScope unit="page" from="1" to="8" />
		</imprint>
	</monogr>
	<note>CVPR 2008. IEEE Conference on</note>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Lcr-net: Localization-classification-regression for human pose</title>
		<author>
			<persName><forename type="first">G</forename><surname>Rogez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Weinzaepfel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Schmid</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR 2017-IEEE Conference on Computer Vision &amp; Pattern Recognition</title>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
		<title level="m" type="main">Anticipation in human-robot cooperation: A recurrent neural network approach for multiple action sequences prediction</title>
		<author>
			<persName><forename type="first">P</forename><surname>Schydlo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Rakovic</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Jamone</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Santos-Victor</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1802.10503</idno>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Real-time human pose recognition in parts from single depth images</title>
		<author>
			<persName><forename type="first">J</forename><surname>Shotton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Fitzgibbon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Cook</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Sharp</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Finocchio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Moore</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Kipman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Blake</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision and Pattern Recognition (CVPR), 2011 IEEE Conference on</title>
		<imprint>
			<publisher>Ieee</publisher>
			<date type="published" when="2011">2011</date>
			<biblScope unit="page" from="1297" to="1304" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Human pose estimation</title>
		<author>
			<persName><forename type="first">L</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="362" to="370" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Online localization and prediction of actions and interactions</title>
		<author>
			<persName><forename type="first">K</forename><surname>Soomro</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Idrees</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Shah</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Lifting from the deep: Convolutional 3d pose estimation from a single image</title>
		<author>
			<persName><forename type="first">D</forename><surname>Tome</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Russell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Agapito</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR 2017 Proceedings</title>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="2500" to="2509" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<monogr>
		<author>
			<persName><forename type="first">S</forename><surname>Toyer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Cherian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Gould</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1707.09240</idno>
		<title level="m">Human pose forecasting via deep markov models</title>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Generating videos with scene dynamics</title>
		<author>
			<persName><forename type="first">C</forename><surname>Vondrick</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Pirsiavash</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Torralba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances In Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="613" to="621" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">An uncertain future: Forecasting from static images using variational autoencoders</title>
		<author>
			<persName><forename type="first">J</forename><surname>Walker</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Doersch</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Hebert</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="835" to="851" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Dense optical flow prediction from a static image</title>
		<author>
			<persName><forename type="first">J</forename><surname>Walker</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Hebert</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision (ICCV), 2015 IEEE International Conference on</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="2443" to="2451" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">The pose knows: Video forecasting by generating pose futures</title>
		<author>
			<persName><forename type="first">J</forename><surname>Walker</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Marino</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Hebert</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2017 IEEE International Conference on Computer Vision (ICCV)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="3352" to="3361" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">A long short-term memory model for answer sentence selection in question answering</title>
		<author>
			<persName><forename type="first">D</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Nyberg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing</title>
		<meeting>the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing</meeting>
		<imprint>
			<publisher>Short Papers</publisher>
			<date type="published" when="2015">2015</date>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="707" to="712" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Gaussian process dynamical models for human motion</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">M</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">J</forename><surname>Fleet</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Hertzmann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE transactions on pattern analysis and machine intelligence</title>
		<imprint>
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="283" to="298" />
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Convolutional pose machines</title>
		<author>
			<persName><forename type="first">S.-E</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Ramakrishna</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Kanade</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Sheikh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="4724" to="4732" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Leveraging hierarchical parametric networks for skeletal joints based action segmentation and recognition</title>
		<author>
			<persName><forename type="first">D</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Shao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="724" to="731" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Accurate 3d pose estimation from a single depth image</title>
		<author>
			<persName><forename type="first">M</forename><surname>Ye</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Pollefeys</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision (ICCV), 2011 IEEE International Conference on</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2011">2011</date>
			<biblScope unit="page" from="731" to="738" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">From actemes to action: A strongly-supervised representation for detailed action understanding</title>
		<author>
			<persName><forename type="first">W</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Derpanis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
