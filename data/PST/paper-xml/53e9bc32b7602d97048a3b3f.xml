<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Lifting Factorization-Based Discrete Wavelet Transform Architecture Design</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Wenqing</forename><surname>Jiang</surname></persName>
							<email>wjiang@c-cube.com</email>
						</author>
						<author>
							<persName><forename type="first">Antonio</forename><surname>Ortega</surname></persName>
							<email>ortega@sipi.usc.edu</email>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="department">Department of Elec-trical Engineering-Systems</orgName>
								<orgName type="institution" key="instit1">W. Jiang was with the Integrated Media System Center</orgName>
								<orgName type="institution" key="instit2">University of Southern California</orgName>
								<address>
									<postCode>90089</postCode>
									<settlement>Los Angeles</settlement>
									<region>CA</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff1">
								<orgName type="institution">C-Cube MicroSystems Inc</orgName>
								<address>
									<postCode>95035</postCode>
									<settlement>Milpitas</settlement>
									<region>CA</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff2">
								<orgName type="department" key="dep1">Integrated Media System Center</orgName>
								<orgName type="department" key="dep2">Department of Elec-trical Engineering-Systems</orgName>
								<orgName type="institution">University of Southern California</orgName>
								<address>
									<postCode>90089</postCode>
									<settlement>Los Angeles</settlement>
									<region>CA</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Lifting Factorization-Based Discrete Wavelet Transform Architecture Design</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">1335C42BD75F1B3109419F2D4659A975</idno>
					<note type="submission">received November 9, 1999; revised September 26, 2000. This research was supported in part by the Integrated Media Systems Center, a National Science Foundation Engineering Research Center, under Cooperative</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.3" ident="GROBID" when="2023-07-28T15:25+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Boundary postprocessing</term>
					<term>discrete wavelet transform</term>
					<term>overlap-state</term>
					<term>parallel algorithm</term>
					<term>sequential algorithm</term>
					<term>split-and-merge</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>In this paper, two new system architectures, overlapstate sequential and split-and-merge parallel, are proposed based on a novel boundary postprocessing technique for the computation of the discrete wavelet transform (DWT). The basic idea is to introduce multilevel partial computations for samples near data boundaries based on a finite state machine model of the DWT derived from the lifting scheme. The key observation is that these partially computed (lifted) results can also be stored back to their original locations and the transform can be continued anytime later as long as these partial computed results are preserved. It is shown that such an extension of the in-place calculation feature of the original lifting algorithm greatly helps to reduce the extra buffer and communication overheads, in sequential and parallel system implementations, respectively. Performance analysis and experimental results show that, for the Daubechies (9,7) wavelet filters, using the proposed boundary postprocessing technique, the minimal required buffer size in the line-based sequential DWT algorithm [1] is 40% less than the best available approach. In the parallel DWT algorithm we show 30% faster performance than existing approaches.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>since in these systems cheap but slower communication links are used (as compared with dedicated parallel systems) <ref type="bibr" target="#b11">[12]</ref>, <ref type="bibr" target="#b7">[8]</ref>, <ref type="bibr" target="#b12">[13]</ref>, <ref type="bibr" target="#b13">[14]</ref>.</p><p>The major difficulty in achieving an efficient DWT architecture design (both in terms of memory and communication) is that, with the exception of trivial Haar filters, the DWT is not a block transform. When data has to be processed one block (or one image scanline) at a time in sequential systems <ref type="bibr" target="#b0">[1]</ref>, <ref type="bibr" target="#b5">[6]</ref> or partitioned over multiple processors in parallel systems <ref type="bibr" target="#b3">[4]</ref>, <ref type="bibr" target="#b7">[8]</ref>, correct DWT computation near data boundaries requires extra buffer and/or extra communication compared to that needed for a block transform such as the discrete cosine transform (DCT). In standard FFT-based filtering approaches, such a boundary issue can be easily handled with appropriate data overlapping (e.g., the overlap-save or overlap-add approaches <ref type="bibr" target="#b14">[15]</ref>). However, because the DWT consists of recursive filtering operations on multilevel downsampled data sequences, direct application of the overlapping techniques can be very costly in terms of memory and/or inter-processor communication.</p><p>Consider, for example, a -level wavelet decomposition of a -point input sequence to be performed using two processors (assuming is even for simplicity). In this case, either the two processors are given sufficient overlapped data to carry on the whole computation without communicating with each other, or alternatively, they have to communicate samples after each level of the decomposition has been computed. The first approach, overlapping, requires that input data near the block boundaries be given to both processors. Since each processor has to compute its own transform for multiple decomposition levels, this overlap can be quite large. As given in the analysis of the spatially segmented wavelet transform (SSWT) by Kossentini <ref type="bibr" target="#b15">[16]</ref>, the buffer size for a -level decomposition of a -point sequence is ( is the filter length). As one can see, the overlap increases exponentially with the increase of decomposition level , which can become significant if long wavelet filters are used and the number of levels of decomposition is large. Notice that the in-place lifting algorithm <ref type="bibr" target="#b16">[17]</ref> is already assumed to be used in our work and the focus of this paper is on the reduction of memory at boundaries below the level, , required in a standard lifting approach. To the best of our knowledge, reduction of boundary memory has not been addressed within a lifting framework until recently.</p><p>As one alternative, the overlapping technique has been also applied at each decomposition level rather than once for all as in SSWT. Examples of this approach include the recursive pyramid algorithm (RPA) by Vishwanath <ref type="bibr" target="#b2">[3]</ref>, and the reduced line-based compression system by Chrysafis et al. <ref type="bibr" target="#b0">[1]</ref>, <ref type="bibr" target="#b9">[10]</ref>. These approaches reduce significantly the buffer size to for a -level decomposition of a -point sequence. Unfortunately, this is still a quite large overlap for some applications, for example, the line-based wavelet image compression system 1051-8215/01$10.00 Â©2001 IEEE described in <ref type="bibr" target="#b0">[1]</ref>, <ref type="bibr" target="#b9">[10]</ref>. In that system, image lines are stored in memory only while they are used to generate output coefficients and are released from memory when no longer needed. This leads to and the amount of memory is image lines (due to line downsampling) at each stage. Consider a color image of size , such that each color component sample is stored as a 4 bytes floating point number for DWT computation. In this case, one image scanline requires 48 kB. Using the Daubechies (9,7) wavelet filterbank ( ), for a three-level decomposition, the total memory would be 588 kB. In this paper, we propose a novel technique which can help to reduce the memory to only 296 kB.</p><p>In the second approach, nonoverlapping,to parallel DWT implementation, input data is not overlapped so the memory requirement is relaxed. But boundary samples need to be exchanged at each decomposition level. Such an approach is used, for example, in the design of mesh and hypercube parallel DWT architectures by Fridman et al. <ref type="bibr" target="#b3">[4]</ref>. Their analysis shows that, for a -level wavelet decomposition, data exchanges are needed between neighboring processors <ref type="bibr" target="#b3">[4]</ref> (see Fig. <ref type="figure" target="#fig_0">1</ref> for a three-level example). In order to reduce the communication overhead, Yang et al. <ref type="bibr" target="#b11">[12]</ref> proposed to use boundary extensions in their DWT system configured from a cluster of SGI workstations. This, however, computes incorrect wavelet coefficients near data boundaries, which causes performance degradation in low-bit rate image coding applications <ref type="bibr" target="#b17">[18]</ref>.</p><p>This provides the motivation to study the problem of blockbased DWT computation and its implications on memory and communication overhead in practical system designs. In this paper, we present a novel technique, boundary postprocessing, which can help to achieve significant memory and communication savings. The idea is motivated by the standard overlap-add technique which first performs filtering operations on neighboring data blocks independently and completes the computation later by summing the partial boundary results together <ref type="bibr" target="#b14">[15]</ref>. We extend this idea to the case of multilevel wavelet decompositions using the lifting framework formulated by Daubechies and Sweldens <ref type="bibr" target="#b16">[17]</ref>. In the proposed approach, the DWT is modeled as a finite-state machine, in which each sample is updated progressively from the initial state (the original data sample) to the final state (the wavelet coefficient) with the help of samples in its local neighborhood. Obviously, samples near data boundaries cannot be fully updated due to lack of data from neighboring blocks. Rather than leaving them unchanged, as was done in previous approaches, we propose to update these samples into intermediate states and preserve these partially transformed results (state information) for later processing. The FSM model thus ensures that correct transform can still be achieved for these boundary samples using the preserved state information. Because of the partial computation and the state preservation, we will show that the buffer size in sequential algorithms and the communication overhead in parallel algorithms can be reduced.</p><p>Some recent works have also explored (independently of our work) the use of lifting factorizations for memory savings in sequential DWT implementations <ref type="bibr" target="#b18">[19]</ref>- <ref type="bibr" target="#b20">[21]</ref>. The novelty of our work is that, first, we introduce partial computations for boundary samples at multiple decomposition levels for memory savings and second, we propose that processors exchange data after multilevel decompositions for communication savings. Application of the proposed boundary postprocessing technique results in two new DWT system architectures, the overlap-state sequential, and the split-and-merge parallel. We will show how the proposed technique can be used to reduce the memory requirement and the interprocessor communication overhead in the architecture designs.</p><p>We mention that, throughout this paper, we focus on the Mallat tree-structured <ref type="bibr" target="#b21">[22]</ref> multilevel octave-band wavelet decomposition system with critical sampling using a two-channel wavelet filterbank. The extensions of our work to other DWT systems, including standard DWTs <ref type="bibr" target="#b22">[23]</ref>, multichannel wavelet filterbank, and wavelet packet decompositions are straightforward. The rest of the paper is organized as follows. In the next section, the finite state machine model is introduced for the DWT and the boundary postprocessing technique for the transform near block boundaries is presented. The proposed sequential and parallel architectures are given in Sections III and IV, respectively, along with performance analysis and experimental results. Section V concludes our work.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>II. FINITE-STATE MACHINE MODEL FOR DWT</head><p>In this section, we first introduce the FSM model for DWT based on the lifting factorization. Then we discuss a postprocessing technique for DWT computation near block boundaries.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. The Finite-State Machine Model</head><p>The polyphase matrix of any FIR wavelet filterbank has a factorization form ([17, Theorem 7]) as Consider the input as a column vector, and define the intermediate states in the process of transformation , where is the result of applying the operation to , and where the initial input is . There are two important observations. 1) Every time we generate , we only need to store this set of values, i.e., we do not need to know any of the other , for , in order to compute the output (the final wavelet coefficients).</p><p>2) Each sample is updated using samples only from the other polyphase component and itself. Consequently, each sample can be updated independently, fully or partially, and written back to its original location, an extended in-place computation feature of the lifting algorithm. For example, the updating of (may only be partially updated because insufficient boundary data is available) will not affect the updating of at this stage. For the polyphase matrix factorization, this requires that the elementary matrix can only be in the form of lower/upper triangular matrices with constants on the diagonal entries. This key property of the lifting factorization guarantees that the FSM structure applies. Thus, the wavelet transform based on the polyphase factorization can be modeled as a finite state machine, as depicted in Fig. <ref type="figure">2</ref>, where each elementary matrix updates the FSM state to the next higher level . The significance of such a FSM model is that the transform can be stopped at any intermediate stage. As long as the state information is preserved, the computation can be continued at any later time. This is the key reason that one can defer the transform for block boundary samples and still can obtain the correct result.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Boundary Postprocessing</head><p>In Fig. <ref type="figure">3</ref>, we show one updating operation for input sequence where even-indexed samples are updated as . Denote ( ), then where and are respectively the contributions from the anticausal and causal part of filter . Now let us consider the computations near the block boundary at point . Take sample , for example. It is obvious that it cannot be updated into state because the anticausal filtering result cannot be computed due to lack of data samples from the other block, i.e., samples , as shown in Fig. <ref type="figure">3(a)</ref>.</p><p>A typical approach used in most existing DWT algorithms is to buffer and wait until the future samples are available, see, e.g., <ref type="bibr" target="#b2">[3]</ref>, <ref type="bibr" target="#b0">[1]</ref>, <ref type="bibr" target="#b3">[4]</ref>. This direct buffering approach, however, forces one also to buffer whatever samples are necessary in the current block for the computation of , i.e., samples in this case. Clearly, for the future updating of sample , the buffer should at least be large enough to hold three samples.</p><p>Observing the fact that is readily available (all samples needed are in the same block as ), we propose to first update into and then buffer this partially computed result at the same location of . Notice that can be over-written (i.e., in-place computation) because no samples in the current block or other block need the original value for their updating operations as described before. As soon as samples become available, can be computed and can be updated into . Clearly, in this case we only need a one sample size buffer to store the partially computed result at sample location for the future updating. Thus compared to the case of direct buffering of , buffering partially transformed coefficients helps to reduce the memory requirement.</p><p>The same analysis also applies for other samples at both sides of the block boundary. The end state after the application of is shown in Fig. <ref type="figure">3(b</ref>). As one can see, the physical boundary splits into two and extends inwards in both blocks. The next stage state transition will operate against these two new boundaries. Notice that locations of these two new boundaries can be derived easily using the filtering tap information and before the stage updating operation. As a matter of fact, given the lifting factorization of the polyphase matrix , one knows exactly the end state number of each sample in the input data sequence. Therefore, the state number of each sample needs not to be stored for future processing.</p><p>Consequently, the buffer size at stage , , is only determined by the number of samples that need to be stored. For given filters , where is the number of taps of the longest of the two filters. Assuming a total of state transitions, then the total buffer size is . For the (9,7) filters using the factorization given in <ref type="bibr" target="#b16">[17]</ref>, one can obtain and thus . That is, only four partially transformed samples need to be preserved rather than seven samples as before. We emphasize that the buffer size reduction is based on a typical assumption in in-place lifting algorithms that each sample (including original data samples and partially transformed samples) needs the same amount of storage space. Since lifting factorization of a given polyphase matrix is not unique, one would choose the factorization which gives the minimum if the amount of memory is limited.</p><p>We thus call this approach of preserving intermediate state information and completing the transform later the boundary postprocessing technique. Fig. <ref type="figure" target="#fig_3">4</ref> shows an example three-level wavelet decomposition using the proposed technique. Compared to the approach shown in Fig. <ref type="figure" target="#fig_0">1</ref>, one can see that, only one data exchange is necessary between the two blocks. The amount of data exchanged is also reduced due to the partial computation as described above.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>III. OVERLAP-STATE SEQUENTIAL ARCHITECTURE</head><p>In Fig. <ref type="figure" target="#fig_4">5</ref>, the proposed overlap-state sequential DWT system architecture is shown. The input data sequence is segmented into nonoverlapping blocks of length and fed into the DWT/FSM one block at a time. The state information is overlapped between consecutive blocks. The computed wavelet coefficients are concatenated together to give the final result.</p><p>As shown in the last section, the memory requirement depends on the specific lifting factorization used in the implementation. Table <ref type="table" target="#tab_0">I</ref> provides memory requirements for commonly used wavelet filterbanks, including the Daubechies (9,7) filters, the (2,10) filters, and the cubic B-Splines CDF(4,2) filters; their factorizations can be found in <ref type="bibr" target="#b23">[24]</ref>. A direct extension of 1-D DWT becomes the strip-sequential 2-D DWT as shown in Fig. <ref type="figure" target="#fig_5">6</ref>, where the input data is transformed one strip at a time with state information overlapped only vertically (boundary extensions are used horizontally). The buffer size for the state information in a -level decomposition can be computed as (4)  where is the data width (subscript " " stands for bottom side of the data strip) is the number of rows partially computed at level .</p><p>A special case of this strip-sequential 2-D DWT system is the line-based DWT described in <ref type="bibr" target="#b0">[1]</ref>, <ref type="bibr" target="#b9">[10]</ref>, which assumes all completely transformed coefficients are not buffered. Based on this assumption, for one level decomposition using a -tap filterbank, only rows need to be buffered in SSWT or RPA (if counting the two new input rows, the total buffer size should be ). The memory requirement in the proposed system is always upper-bounded by that of RPA and can have substantial reduction (approximately 40%) for commonly used wavelet filterbanks, as shown in Table <ref type="table" target="#tab_1">II</ref>. For example, using the (9,7) filters, the system only needs to buffer four rows (i.e.,</p><p>) of data at each level while RPA needs to buffer 7 rows of data. The constant is due to line downsampling at each decomposition level. Refer to <ref type="bibr" target="#b23">[24]</ref> for more details on memory requirements and other 2-D DWT systems (e.g., the block sequential system).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>IV. SPLIT-AND-MERGE PARALLEL ARCHITECTURE</head><p>In Fig. <ref type="figure" target="#fig_6">7</ref>, the proposed parallel DWT architecture is shown. The input data is uniformly segmented into nonoverlapping  blocks and allocated onto available processors. Each processor computes its own allocated data up to the required wavelet decomposition level. This stage is called Split. The output from this stage consists of: 1) completely transformed coefficients and 2) the state information (partially updated boundary samples). In the second stage, Merge, a one-way communication is initiated and the state information is transfered to the neighboring processors. The state information from the neighbor processor is then combined together with its own corresponding state information to complete the whole DWT transform. This is further illustrated by Fig. <ref type="figure">8</ref>, where it can be seen how the operations in each processor are carried as far as possible in the split operation, while in the merge operation the processor will combine the available information to update the partially computed outputs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Communication Delay</head><p>The communication delay is the time used for communicating data between adjacent processors. Let be the communication setup time, e.g., the handshake time in an asynchronous communication protocol. For a level wavelet decomposition, the total communication <ref type="bibr" target="#b3">[4]</ref> is <ref type="bibr" target="#b4">(5)</ref> where is the time to transfer one data sample and is the number of boundary samples exchanged to adjacent processor at each level. In the proposed approach, however, only one communication setup is necessary to communicate the state information between adjacent processors. Furthermore, the size of the state information at each decomposition level is upper Fig. <ref type="figure">8</ref>. An example split-and merge parallel DWT architecture using the Daubechies (9, 7) filters for two level decompositions. Shaded boxes represent partially updated samples to be exchanged between processors in the merge stage. Notice that samples {25, 26, 27} are left unprocessed for clarity and the block boundary can be at any one of these three samples.</p><p>bounded by <ref type="bibr" target="#b23">[24]</ref>. So the upper bound of the communication delay is <ref type="bibr" target="#b5">(6)</ref> As one can see, using the boundary postprocessing approach, the communication overhead for link setup is reduced. Notice that the comparison is based on the assumption that all data samples which have to be buffered and exchanged (be it an original data sample or a partially computed intermediate sample) require the same storage space. This is a reasonable assumption when the in-place lifting algorithm is used to compute the wavelet transform. The reduction of the communication time certainly contributes to the total DWT computation time reduction. However, we mention that the overall contribution depends on the relative weight of the communication overhead in the total DWT computation. In general, more gain can be achieved for parallel systems with slow communication links.</p><p>To test the efficiency of the proposed parallel architecture, four different DWT algorithms are implemented using two SUN ULTRA-1 workstations running the LAM 6.1 parallel platform developed in Ohio Supercomputer Center <ref type="bibr" target="#b13">[14]</ref>. The algorithms compared are the sequential lifting algorithm, parallel standard algorithm (subsample-filtering approach), parallel lifting scheme, and our proposed parallel scheme. The test wavelet filters are the (9,7) filters and the test image is the Lena grayscale image of size . A 2-D separable wavelet transform is implemented with strip data partition <ref type="bibr" target="#b3">[4]</ref> between processors (refer to Fig. <ref type="figure" target="#fig_5">6</ref>). We implemented all the algorithms ourselves with no specific optimizations on the codes except for the compiler optimization. Further improvement of algorithms performances is certainly possible through more rigorous code optimizations. To compare the performances, the relative speedup is computed and the averaged results over 50 running instances for 1-5 decomposition levels are given in Table <ref type="table" target="#tab_2">III</ref>.</p><p>It can be seen from the results that our proposed parallel algorithm can significantly reduce the DWT computation time even compared with the fastest available parallel algorithm, i.e., the parallel lifting algorithm. Notice that the improvement is not linear with the increase of the decomposition level. The reason is that, though the communication overhead increases with the decomposition level, the total numerical computation also increases. Another observation is that the improvement of the proposed algorithm at one level decomposition is even greater than that at multiple level decompositions. It suggests that, in our experimental system setup, the gain due to saved amount of exchanged data is greater than that due to saved number of com-munication setups, though, future work is needed to further investigate this issue.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>V. CONCLUSIONS</head><p>To conclude, we have proposed a new boundary postprocessing technique for the DWT computation near block boundaries. Performance analysis and experimental results show that the auxiliary buffer size for boundary DWT and the communication overhead can be significantly reduced by using the proposed technique. The results presented here can be easily extended to 2-D or higher dimensional wavelet transforms by using separable transform approaches <ref type="bibr" target="#b23">[24]</ref>.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 1 .</head><label>1</label><figDesc>Fig. 1. Example dataflow chart of a three-level wavelet decomposition. Solid lines: completely transformed data. Dashed lines: boundary samples from the neighboring block. Operations 1,3,5: communicate boundary data samples to neighboring blocks. Operations 2,4,6: transform for current level.</figDesc><graphic coords="2,50.94,62.28,228.48,126.24" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head></head><label></label><figDesc>polynomials, which are called prediction and updating operations, respectively. Without loss of generality, we use to represent the elementary matrices. That is or Let us consider the time domain filtering operations corresponding to . By definition, we have where are the two polyphase components of filter . Each corresponds to two time domain filters or (2) where and are the low-and high-pass filters in the analysis filterbank, respectively. In time domain, this corresponds to In time domain, this corresponds to (3) For a -point input sequence (assume is even), and . The state transition in vector form for the upper triangular elementary matrix (similar form for the lower triangular elementary matrix) is . . . . . . . . . . . . Obviously, each and every elementary matrix in the factorization of the polyphase matrix can be modeled as such a state transition process.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 2 .Fig. 3 .</head><label>23</label><figDesc>Fig. 2. State transition diagram of DWT as a FSM.</figDesc><graphic coords="3,301.14,143.10,251.76,118.80" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Fig. 4 .</head><label>4</label><figDesc>Fig. 4. Example dataflow chart of a three-level wavelet decomposition using the proposed boundary postprocessing technique. Solid lines: completely transformed data. Dashed lines: partially transformed data. Operation 1: each block transforms its own allocated data independently and state information is buffered. Operation 2: state information is communicated to neighboring blocks. Operation 3: complete transform for the boundary data samples.</figDesc><graphic coords="4,319.20,62.28,217.92,144.36" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Fig. 5 .</head><label>5</label><figDesc>Fig. 5. The proposed sequential architecture for DWT.</figDesc><graphic coords="4,305.82,291.36,244.80,155.16" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Fig. 6 .</head><label>6</label><figDesc>Fig. 6. The strip-sequential 2-D DWT system on data of size W 2 H. The input is transformed one strip at a time from top to bottom. State information is overlapped vertically only.</figDesc><graphic coords="5,40.74,185.64,245.76,235.08" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Fig. 7 .</head><label>7</label><figDesc>Fig. 7. Proposed parallel architecture for DWT.</figDesc><graphic coords="5,301.56,187.98,250.32,157.92" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0"><head></head><label></label><figDesc></figDesc><graphic coords="6,130.86,62.28,331.68,283.44" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>TABLE I COMPARISON</head><label>I</label><figDesc>OF MEMORY REQUIREMENTS IN 1-D DWT N: SEQUENCE LENGTH. L: FILTER LENGTH. J: DECOMPOSITION LEVEL. B: NUMBER OF PARTIALLY COMPUTED SAMPLES AT EACH LEVEL</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>TABLE II COMPARISON</head><label>II</label><figDesc>OF MEMORY REQUIREMENTS IN LINE-BASED SYSTEMS W: ROW WIDTH. B: NUMBER OF PARTIALLY COMPUTED SAMPLES AT EACH LEVEL. = (2 0 1). = (1 0 2 )</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>TABLE III DWT</head><label>III</label><figDesc>RUNNING TIME AND SPEEDUP OF DIFFERENT PARALLEL ALGORITHMS</figDesc><table /><note><p>(IN SECONDS). S: SEQUENTIAL. P: PARALLEL. L: LIFTING</p></note></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Line based, reduced memory, wavelet image compression</title>
		<author>
			<persName><forename type="first">C</forename><surname>Chrysafis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Ortega</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Data Compression Conf</title>
		<meeting>Data Compression Conf</meeting>
		<imprint>
			<date type="published" when="1998">1998</date>
			<biblScope unit="page" from="398" to="407" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Fast algorithms for discrete and continuous wavelet transforms</title>
		<author>
			<persName><forename type="first">O</forename><surname>Rioul</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Duhamel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Inform. Theory</title>
		<imprint>
			<biblScope unit="volume">38</biblScope>
			<biblScope unit="page" from="569" to="586" />
			<date type="published" when="1992-03">Mar. 1992</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">The recursive pyramid algorithm for the discrete wavelet transform</title>
		<author>
			<persName><forename type="first">M</forename><surname>Vishwanath</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Signal Processing</title>
		<imprint>
			<biblScope unit="volume">42</biblScope>
			<biblScope unit="page" from="673" to="676" />
			<date type="published" when="1994-03">Mar. 1994</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">On the scalability of 2-D discrete wavelet transform alogrithms</title>
		<author>
			<persName><forename type="first">J</forename><surname>Fridman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">S</forename><surname>Manolakos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Multidimensional Syst. Signal Processing</title>
		<imprint>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="185" to="217" />
			<date type="published" when="1997">1997</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">A memory system supporting the efficient SIMD computation of the two dimensional DWT</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">A</forename><surname>Trenas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Lopez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">L</forename><surname>Zapata</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ICASSP</title>
		<meeting>ICASSP</meeting>
		<imprint>
			<date type="published" when="1998">1998</date>
			<biblScope unit="page" from="1521" to="1524" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">A parallel architecture for DWT based on lifting factorization</title>
		<author>
			<persName><forename type="first">W</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Ortega</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. SPIE Parallel and Distributed Methods for Image Processing III</title>
		<meeting>SPIE Parallel and Distributed Methods for Image essing III</meeting>
		<imprint>
			<date type="published" when="1999-10">Oct. 1999</date>
			<biblScope unit="volume">3817</biblScope>
			<biblScope unit="page" from="2" to="13" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Implementations of the discrete wavelet transform: complexity, memory, and parallelization issues</title>
		<author>
			<persName><forename type="first">A</forename><surname>Ortega</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Fernandez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Chrysafis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. SPIE: Wavelet Applications in Signal and Image Processing VII</title>
		<meeting>SPIE: Wavelet Applications in Signal and Image essing VII</meeting>
		<imprint>
			<date type="published" when="1999-10">Oct. 1999</date>
			<biblScope unit="volume">3813</biblScope>
			<biblScope unit="page" from="386" to="400" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Efficient DWT system architecture design using filterbank factorizations</title>
		<author>
			<persName><forename type="first">W</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Ortega</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ICIP</title>
		<meeting>ICIP</meeting>
		<imprint>
			<date type="published" when="1999-10">Oct. 1999</date>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="749" to="753" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title/>
	</analytic>
	<monogr>
		<title level="j">Proc. IEEE: Special Issue on Wavelets</title>
		<imprint>
			<biblScope unit="issue">4</biblScope>
			<date type="published" when="1996">1996</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Line based, reduced memory, wavelet image compression</title>
		<author>
			<persName><forename type="first">C</forename><surname>Chrysafis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Ortega</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Image Processing</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="page" from="378" to="389" />
			<date type="published" when="2000-05">May 2000</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">IMAS integrated controller electronics</title>
		<author>
			<persName><forename type="first">J</forename><surname>Bowers</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Keith</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Aranki</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Tawel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Jet Propulsion Laboratory</title>
		<imprint>
			<date type="published" when="1998">1998</date>
			<pubPlace>Pasadena, CA</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Coarse-grained parallel algorithms for multidimensional wavelet transforms</title>
		<author>
			<persName><forename type="first">L</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Misra</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Supercomputing</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="issue">1/2</biblScope>
			<biblScope unit="page" from="99" to="118" />
			<date type="published" when="1998">1998</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">A case for NOW (Networks of Workstations)</title>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">E</forename><surname>Anderson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">E</forename><surname>Culler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">A</forename><surname>Patterson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Micro</title>
		<imprint>
			<biblScope unit="volume">151</biblScope>
			<biblScope unit="page" from="54" to="64" />
			<date type="published" when="1995-02">Feb. 1995</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title/>
		<author>
			<persName><surname>Lam/Mpi</surname></persName>
		</author>
		<ptr target="http://www.mpi.nd.edu/lam" />
		<imprint/>
		<respStmt>
			<orgName>University of Notre Dame, Notra Dame</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">Fast Algorithms for Digital Signal Processing</title>
		<author>
			<persName><forename type="first">R</forename><surname>Blahut</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1985">1985</date>
			<publisher>Addison-Wesley</publisher>
			<pubPlace>Reading, MA</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">Spatially segmented wavelet transform</title>
		<author>
			<persName><forename type="first">F</forename><surname>Kossentini</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1998">1998</date>
		</imprint>
	</monogr>
	<note>UBC, ISOIEC JTC 1SC29WG1 WG1N868</note>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Factoring wavelet transforms into lifting steps</title>
		<author>
			<persName><forename type="first">I</forename><surname>Daubechies</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Sweldens</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Fourier Anal. Appl</title>
		<imprint>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="247" to="269" />
			<date type="published" when="1998">1998</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Report on core experiment codeff1 Complexity reduction of SSWT</title>
	</analytic>
	<monogr>
		<title level="m">Motorola Australia, UBC, ISOIEC JTC 1SC29WG1 WG1N881</title>
		<imprint>
			<date type="published" when="1998">1998</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Low memory line-based wavelet trasform using lifting scheme</title>
		<author>
			<persName><forename type="first">C</forename><surname>Chrysafis</surname></persName>
		</author>
		<idno>ISOIEC JTC 1SC29WG1 WG1N978</idno>
	</analytic>
	<monogr>
		<title level="j">HP Labs</title>
		<imprint>
			<date type="published" when="1998">1998</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Low memory line-based wavelet transform using lifting scheme</title>
		<author>
			<persName><forename type="first">P</forename><surname>Onno</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Canon Research Center France, ISOIEC JTC 1SC29WG1 WG1N1013</title>
		<imprint>
			<date type="published" when="1998">1998</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Optimal memory organization for scalable texture codecs in MPEG-4</title>
		<author>
			<persName><forename type="first">G</forename><surname>Lafruit</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Nachtergaele</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Bormans</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Engels</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Bolsens</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Circuits Syst. Video Technol</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="page" from="218" to="243" />
			<date type="published" when="1999-03">Mar. 1999</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">A theory for multiresolution signal decomposition: The wavelet representation</title>
		<author>
			<persName><forename type="first">S</forename><surname>Mallat</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Pattern Anal. Machine Intell</title>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="page" from="674" to="693" />
			<date type="published" when="1989">1989</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Fast wavelet transforms and numerical algorithms I</title>
		<author>
			<persName><forename type="first">G</forename><surname>Beylkin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Coifman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Rokhlin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">CPAM</title>
		<imprint>
			<biblScope unit="volume">44</biblScope>
			<biblScope unit="page" from="141" to="183" />
			<date type="published" when="1991">1991</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Discrete wavelet transform system architecture design using filterbank factorization</title>
		<author>
			<persName><forename type="first">W</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Ortega</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Univ. Southern California</title>
		<imprint>
			<date type="published" when="1999">1999</date>
			<pubPlace>Los Angeles, CA</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
