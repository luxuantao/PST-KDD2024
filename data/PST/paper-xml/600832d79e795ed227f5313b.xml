<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Under review as a conference paper at ICLR 2021 RAPID NEURAL ARCHITECTURE SEARCH BY LEARNING TO GENERATE GRAPHS FROM DATASETS</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<title level="a" type="main">Under review as a conference paper at ICLR 2021 RAPID NEURAL ARCHITECTURE SEARCH BY LEARNING TO GENERATE GRAPHS FROM DATASETS</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<note type="submission">Under review as a conference paper at ICLR 2021</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-01-03T09:16+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Despite the success of recent Neural Architecture Search (NAS) methods on various tasks which have shown to output networks that largely outperform humandesigned networks, conventional NAS methods have mostly tackled the optimization of searching for the network architecture for a single task (dataset), which does not generalize well across multiple tasks (datasets). Moreover, since such task-specific methods search for a neural architecture from scratch for every given task, they incur a large computational cost, which is problematic when the time and monetary budget are limited. In this paper, we propose an efficient NAS framework that is trained once on a database consisting of datasets and pretrained networks and can rapidly search a neural architecture for a novel dataset. The proposed MetaD2A (Meta Dataset-to-Architecture) model can stochastically generate graphs (architectures) from a given set (dataset) via a cross-modal latent space learned with amortized meta-learning. Moreover, we also propose a metaperformance predictor to estimate and select the best architecture from those sampled from MetaD2A. The experimental results demonstrate that our model metalearned on subsets of ImageNet-1K and architectures from NAS-Bench 201 search space successfully generalizes to multiple benchmark datasets including CIFAR-10 and CIFAR-100, with the search time of less than 30 GPU seconds on CIFAR-10. We believe that the MetaD2A proposes a new research direction for rapid NAS as well as ways to utilize the knowledge from rich databases of datasets and architectures accumulated over the past years.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">INTRODUCTION</head><p>The rapid progress in the design of neural architectures has largely contributed to the success of deep learning on many applications <ref type="bibr" target="#b25">(Krizhevsky et al., 2012;</ref><ref type="bibr" target="#b6">Cho et al., 2014;</ref><ref type="bibr" target="#b15">He et al., 2016;</ref><ref type="bibr" target="#b46">Szegedy et al.;</ref><ref type="bibr" target="#b47">Vaswani et al., 2017;</ref><ref type="bibr" target="#b54">Zhang et al., 2018)</ref>. However, due to the vast search space, designing a novel neural architecture requires a time-consuming trial-and-error search by human experts. To tackle such inefficiency in the manual architecture design process, researchers have proposed various Neural Architecture Search (NAS) methods that automatically search for optimal architectures, achieving models with impressive performances on various tasks that outperform human-designed counterparts <ref type="bibr" target="#b2">(Baker et al., 2017;</ref><ref type="bibr" target="#b55">Zoph &amp; Le, 2017;</ref><ref type="bibr" target="#b20">Kandasamy et al., 2018;</ref><ref type="bibr">Liu et al., 2018a;</ref><ref type="bibr" target="#b36">Luo et al., 2018;</ref><ref type="bibr" target="#b41">Pham et al., 2018;</ref><ref type="bibr" target="#b34">Liu et al., 2019;</ref><ref type="bibr" target="#b50">Xu et al., 2020;</ref><ref type="bibr" target="#b5">Chen et al., 2020)</ref>.</p><p>Recently, large benchmarks for NAS (NAS-101, NAS-201) <ref type="bibr" target="#b51">(Ying et al., 2019;</ref><ref type="bibr" target="#b10">Dong &amp; Yang, 2020)</ref> have been introduced, which provide databases of architectures and their performances on benchmark datasets. Yet, most conventional NAS methods cannot benefit from the availability of such databases, due to their task-specific nature which requires repeatedly training the model from scratch for each new dataset (See Figure <ref type="figure">1 (a)</ref>). Thus, searching for an architecture for a new task (dataset) may require a large number of computations, which may be problematic when the time and monetary budget are limited. How can we then exploit the vast knowledge of neural architectures that have been already trained on a large number of datasets, to better generalize over an unseen task?</p><p>To this end, we propose an efficient NAS framework that is trained once from a database containing datasets and their corresponding neural architectures and then generalizes to multiple datasets for searching neural architectures, by learning to generate a neural architecture from a given dataset. The proposed MetaD2A (Meta Dataset-to-Architecture) framework consists of a set encoder and a graph Figure <ref type="figure">1</ref>: Comparison of our method to conventional NAS approaches (a) Task-specific NAS methods require enormous search time as the search needs to be done from scratch for each task. (b) Meta-NAS methods can adapt to new tasks by leveraging meta-knowledge, but they do not scale well to large datasets and networks due to inefficiency in the meta-learning framework. (c) Our method can rapidly generate set-specific neural architecture candidates for a given dataset with amortized meta-learning of the cross-modal latent space which maps the dataset to architecture, with the set encoder and the graph decoder. Then, our performance predictor selects the best architecture among the generated architectures based on predicted performance.</p><p>decoder, which are used to learn a cross-modal latent space for datasets and neural architectures via amortized inference. For a new dataset, MetaD2A stochastically generates neural architecture candidates from set-dependent latent representations which are encoded from a new dataset and selects the final neural architecture based on their accuracies predicted by a performance predictor, which is also trained with amortized meta-learning (See Figure <ref type="figure">1 (c)</ref>).</p><p>Several Meta-NAS methods <ref type="bibr" target="#b31">(Lian et al., 2019;</ref><ref type="bibr" target="#b11">Elsken et al., 2020;</ref><ref type="bibr" target="#b44">Shaw et al., 2019)</ref> have studied the transferability of NAS to unseen datasets based on gradient-based meta-learning framework. However, their expensive multiple unrolling gradient steps for one meta-update of each task require large computational cost, which limits the applications of those Meta-NAS methods to small scale tasks such as few-shot classification (See Figure <ref type="figure">1 (b)</ref>). On the other hand, the proposed MetaD2A can output a set-dependent neural architecture for a large dataset, through an amortized inference which encodes the dataset as a latent embedding and decodes it as a directed acyclic graph.</p><p>We meta-learn the proposed MetaD2A on MetaImageNet, which is compiled to contain subsets of ImageNet-1K, and neural architectures from the NAS-Bench201 search space. Then we validate it on the architecture search problems for unseen subsets of ImageNet-1K. On this experiment, our model obtains a neural architecture largely outperforming all baselines for a given task within 27.03 GPU seconds on average. Further, our model meta-learned on MetaImageNet can generalize to search for neural architectures on multiple unseen datasets such as MNIST, SVHN, CIFAR-10, CIFAR-100, Aircraft, and Oxford-IIIT Pets. The results show that our model achieves state-of-theart performance on all datasets. Moreover, our model can search for a neural architecture within 22.47 GPU seconds on average, on CIFAR10.</p><p>To summarize, our contribution in this work is threefold:</p><p>? We propose a novel NAS framework, MetaD2A, which rapidly searches for a neural architecture on a new dataset, by sampling architectures from latent embeddings of the given dataset, then selecting the best one based on their predicted performances. ? To this end, we propose to learn a cross-modal latent space of datasets and architectures, by performing amortized meta-learning of it using a set encoder and a graph decoder on subsets of ImageNet-1K. ? The meta-learned our model successfully searches for neural architectures on multiple unseen datasets and achieves state-of-the-art performance on them in NAS-Bench201 search space, especially searching for architectures within 22.47 GPU seconds on CIFAR10.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">RELATED WORK</head><p>Neural Architecture Search (NAS) NAS is an automated architecture search process which aims to overcome the suboptimality of manual architecture designs when exploring the extensive search space for optimal neural architectures. NAS methods can be roughly categorized into reinforcement learning-based methods <ref type="bibr" target="#b55">(Zoph &amp; Le, 2017;</ref><ref type="bibr" target="#b56">Zoph et al., 2018;</ref><ref type="bibr" target="#b41">Pham et al., 2018)</ref>, evolutionary algorithm-based methods <ref type="bibr" target="#b42">(Real et al., 2019)</ref>, and gradient-based methods <ref type="bibr" target="#b34">(Liu et al., 2019;</ref><ref type="bibr" target="#b4">Cai et al., 2019;</ref><ref type="bibr" target="#b36">Luo et al., 2018;</ref><ref type="bibr">Dong &amp; Yang, 2019b;</ref><ref type="bibr" target="#b5">Chen et al., 2020;</ref><ref type="bibr" target="#b50">Xu et al., 2020;</ref><ref type="bibr" target="#b13">Fang et al., 2020)</ref>. Among existing approaches, perhaps the most relevant approach to ours is NAO <ref type="bibr" target="#b36">(Luo et al., 2018)</ref>, which maps DAGs onto a continuous latent embedding space. However, while NAO performs graph reconstruction for a single task, ours generates data-dependent Directed Acyclic Graphs (DAGs) across multiple tasks. Besides NAO, FNA <ref type="bibr" target="#b13">(Fang et al., 2020)</ref> focuses on reducing the adaptation time by using parameter remapping from the pre-trained network, while ours reduces the search time by generating graphs via latent space. Another important open problem in NAS is reducing the tremendous computational cost resulting from the large search space <ref type="bibr" target="#b4">(Cai et al., 2019;</ref><ref type="bibr">Liu et al., 2018a;</ref><ref type="bibr" target="#b41">Pham et al., 2018;</ref><ref type="bibr" target="#b34">Liu et al., 2019;</ref><ref type="bibr" target="#b5">Chen et al., 2020)</ref>. GDAS <ref type="bibr">(Dong &amp; Yang, 2019b)</ref> tackles this by optimizing sampled sub-graphs of DAG. PC-DARTS <ref type="bibr" target="#b50">(Xu et al., 2020)</ref> reduces GPU overhead and search time by partially selecting channel connections. However, due to the task-specific nature of those methods, they should train a model from the scratch for each new unseen task repeatably and each will take a few GPU hours.</p><p>Meta-learning Meta-learning (learning to learn) aims to train a model to generalize over a distribution of tasks, such that it can rapidly adapt to a new task <ref type="bibr" target="#b48">(Vinyals et al., 2016;</ref><ref type="bibr" target="#b45">Snell et al., 2017;</ref><ref type="bibr" target="#b14">Finn et al., 2017;</ref><ref type="bibr" target="#b39">Nichol et al., 2018;</ref><ref type="bibr">Lee et al., 2019b;</ref><ref type="bibr" target="#b17">Hou et al., 2019)</ref>. Recently, LEO <ref type="bibr" target="#b43">(Rusu et al., 2019)</ref> proposed a scalable meta-learning framework which learns the latent generative representations of model parameters for a given data in low-dimensional space for few-shot classification.</p><p>Similarly to LEO <ref type="bibr" target="#b43">(Rusu et al., 2019)</ref>, our method learns a low-dimensional latent embedding space, but we learn a cross-modal space for both datasets and models for task-dependent model generation.</p><p>Neural Architecture Search with Meta-Learning Recent NAS methods with gradient-based meta-learning <ref type="bibr" target="#b11">(Elsken et al., 2020;</ref><ref type="bibr" target="#b31">Lian et al., 2019;</ref><ref type="bibr" target="#b44">Shaw et al., 2019)</ref> have shown promising results on adapting to different tasks. However, they are only applicable on small scale tasks such as few-shot classification tasks <ref type="bibr" target="#b11">(Elsken et al., 2020;</ref><ref type="bibr" target="#b31">Lian et al., 2019)</ref> and require high-computation time, due to the multiple unrolling gradient steps for one meta-update of each task. While some attempt to bypass the bottleneck with a first-order approximation <ref type="bibr" target="#b31">(Lian et al., 2019;</ref><ref type="bibr" target="#b44">Shaw et al., 2019)</ref> or parallel computations with GPUs <ref type="bibr" target="#b44">(Shaw et al., 2019)</ref>, but their scalability is intrinsically limited due to gradient updates over a large number of tasks. To tackle such a scalability issue, we perform amortized inference over the multiple tasks by encoding a dataset as the low-dimensional latent vector and exploit fast GNN propagation instead of the expensive gradient update.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">METHOD</head><p>Our goal is to output a high-performing neural architecture for a given dataset rapidly by learning the prior knowledge obtained from the rich database consisting of datasets and its neural architectures. To this end, we propose Meta Dataset-to-Architecture (MetaD2A) framework which learns the cross-modal latent space of datasets and their neural architectures. Further, we introduce a metaperformance predictor, which predicts accuracies of given architectures on an unseen input dataset to select the best neural architecture for the dataset. Figure <ref type="figure" target="#fig_0">2</ref> illustrates the overview of our framework.</p><p>To formally define the problem, let us assume that we have a database of N ? number of tasks, where we randomly obtain tasks ? from task distribution p(? ) to train for the model. G trained on D. Each node v ? V of G then represents a computational operator (e.g. 3-by-3 convolution, mean-pooling, . . . ) and each edge (u, v) ? E represents a computation flow from an output of the operator u to input of the operator v. The whole framework consists of a set encoder, a graph decoder, and a meta-performance predictor parameterized by ?, ?, and ?, respectively. The dataset D is embedded into a latent space Z as a vector z through the set encoder q ? and then the graph decoder p ? estimates the distribution and generates a new graph G from z which are sampled from prior the p(z). We learn both components q ? (z|D) and p ? (G|z), generalizing over the task distribution p(? ). After training, for an unseen dataset D, our model with ? and ? can easily generate DAGs { ?i } ns i=1 as candidates, where n s is the number of sampled architectures. This is done by decoding the latent embeddings {? i |? i ? Z} ns i=1 sampled from the prior p(z) consisting of ? and ? generated from the set encoder for given D. The predictor f ? (s|D, G) is also learned from the task distribution p(? ) in the meta-training stage and then selects the best performing neural architecture for D from { ?i } ns i=1 based on their predicted accuracies { ?i } ns i=1 in the meta-test.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">LEARNING TO GENERATE GRAPHS FROM DATASETS</head><p>Dataset Encoding To compress the entire instances from a dataset D into a single latent code z, the set encoder should process input sets of any size and summarize consistent information agnostically to the order of the instances (permutation-invariance). While simple functions such as mean/sum-pooling can fulfill those requirements, they cannot model high-order interactions between the set elements to accurately generate complex DAGs such as neural architectures. Further, those non-hierarchical poolings consider all elements equally, and thus cannot accurately model individual classes in the given dataset, which may lead to poor performance <ref type="bibr" target="#b27">(Lee et al., 2020)</ref>.</p><p>Therefore, we introduce a novel Hierarchical Set Pooling for dataset encoding which stacks two permutation-invariant encoders with learnable parameters, that can extract information related to generating set-conditioned DAGs. The lower-level intra-class encoder captures the class prototypes that reflect label information, and the high-level inter-class encoder considers the relationship between class prototypes and aggregates them into a latent vector. Specifically, for a given dataset D, we construct the subset for class  <ref type="bibr">(Lee et al., 2019a)</ref>, where the former learns the features for each element in the set using self-attention while the latter pools the input features into k representative vectors. Please see Section A.1 of the Appendix for more details. We set k = 1 to produce a single vector, and stack SAB twice followed by PMA for both set encoders as follows (See Figure <ref type="figure" target="#fig_1">3 (a)</ref>):</p><formula xml:id="formula_0">c as {x|x ? B c } ? R b?dx ,</formula><formula xml:id="formula_1">v c = IntraSetPool {x|x ? B c } = PMA SAB SAB {x|x ? B c } (1) h e = InterSetPool {v c } C c=1 = PMA SAB SAB {v c } C c=1 (2)</formula><p>Then the latent code z ? R 1?dz can be sampled from a dataset-conditioned Gaussian distribution with diagonal covariance where NN ? , NN ? are single linear layers:</p><formula xml:id="formula_2">z ? q ? (z|D) = N (?, ? 2 ), where ?, ? = NN ? (h e ), NN ? (h e )<label>(3)</label></formula><p>Graph Decoding To generate a DAG G describing a neural architecture from the latent code z, we use a Graph Neural Network(GNN)-based decoder p ? (G|z). While GNN-based decoders have been actively used to generate graph-structured data <ref type="bibr">(Liu et al., 2018b;</ref><ref type="bibr" target="#b32">Liao et al., 2019;</ref><ref type="bibr" target="#b19">Jin et al., 2020)</ref>, they are suboptimal for our goal since they generally target undirected graphs and therefore ignore the topological order of DAGs. Thus, we use a GNN with asynchronous message passing (GNN-AMP) which allows message passing to happen only along the topological order of the DAGs <ref type="bibr" target="#b53">(Zhang et al., 2019)</ref>. The graph decoder starts from an initial hidden state h v0 = NN init (z), where NN init is an MLP followed by tanh. For i th node v i according to topological order, we compute the probability of each operation type o vi ? R 1?no over n o operations, given the current graph state as the last hidden node h</p><formula xml:id="formula_3">G := h vi . That is, o vi = NN node (h G )</formula><p>, where NN node is an MLP followed by softmax. When the predicted v i type is the end-of-graph, we stop the decoding process and connect all leaf nodes to v i . Otherwise we update hidden state h</p><formula xml:id="formula_4">(t)</formula><p>vi at time step t as follows:</p><formula xml:id="formula_5">h (t+1) vi = UPDATE(i, m (t) vi ), where m (t) vi = u?V in v i AGGREGATE(h (t) u )<label>(4)</label></formula><p>The function UPDATE is a gated recurrent unit (GRU) <ref type="bibr" target="#b6">(Cho et al., 2014)</ref>, i is the order of v i , and m</p><formula xml:id="formula_6">(t)</formula><p>vi is the incoming message to v i . The function AGGREGATE consists of mapping and gating functions with MLPs, where V in vi is a set of predecessors with incoming edges to v i . For all previously processed nodes v j |j = i-1, ..., 1 , we decide whether to link an edge from v j to v i by sampling the edge based on edge connection probability e {vj ,vi} = NN edge (h j , h i ) , where NN edge is a MLP followed by sigmoid. We update h vi by Eq. ( <ref type="formula" target="#formula_5">4</ref>) whenever a new edge is connected to v i . For meta-test, we select the operation with the max probability for each node and edges with e {vj ,vi} &gt; 0.5. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Meta-training Strategy for NAS</head><p>We meta-learn the model using set-amortized inference, by maximizing the approximated evidence lower bound (ELBO) for each task ? as follows:</p><formula xml:id="formula_7">max ?,? E z?q ? (z|D) log p ? G|z -? ? L ? KL q ? z|D ||p z (5)</formula><p>where each dimension of prior p(z) factorizes into N (0, 1). L ? KL is the KL divergence between two multivariate Gaussian distributions which has a simple closed form (Kingma &amp; Welling, 2014) and ? is the scalar weighting value. Using the reparameterization trick on z, we can optimize the above objective by stochastic gradient variational Bayes <ref type="bibr" target="#b22">(Kingma &amp; Welling, 2014)</ref>. The expectation of the log-likelihood of (5) can be rewritten with negative cross-entropy loss -L ? CE for nodes and binary cross-entropy loss -L ? BCE for edges, and we slightly modify it using the generated set-dependent graph G and the ground truth graph G as the input as follows:</p><formula xml:id="formula_8">-L ? ?,? D, G = vi?V -L ? CE ?vi , o vi + vj ?Vi -L ? BCE ?{vj,vi} , e {vj ,vi}<label>(6)</label></formula><p>We substitute the log-likelihood term of the equation ( <ref type="formula">5</ref>) such as equation ( <ref type="formula" target="#formula_8">6</ref>) and take negative to the equation ( <ref type="formula">5</ref>) to convert it to the resultant objective L ? ?,? (D, G) as follows:</p><formula xml:id="formula_9">min ?,? ? ?p(? ) L ? ?,? D, G = min ?,? ? ?p(? ) L ? ?,? D, G + ? ? L ? KL q ? z|D ||p z (7)</formula><p>We learn our MetaD2A generator by minimizing the final objective to learn ?, ?, which are shared across all tasks. In the meta-test stage, for a unseen dataset D, we can obtain set-dependent DAGs { ?i } ns i=1 , with the meta-trained generator parameterized by ? * , ? * , by feeding D as the input. Thus our method can generate neural architecture(s) for the novel dataset via set-level amortized inference.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">META-PERFORMANCE PREDICTOR</head><p>We design a meta-performance predictor f ? to select the best architecture from the architectures generated by MetaD2A generator, based on their predicted accuaracies on the target task. Since our predictor should accurately predict the model performance for any datasets unlike the predictors used in task-specific NAS, it also meta-learned. The meta-performance predictor consists of a dataset encoder and a graph encoder, followed by two linear layers with relu. For dataset encoding, we use the Hierarchical Set Pooling of Section 3.1 which takes D as an input, using only the mean ? from Eq. ( <ref type="formula" target="#formula_2">3</ref>) as an output. We perform a bi-directional encoding of the given graph following GNN-AMP <ref type="bibr" target="#b53">(Zhang et al., 2019)</ref>, where given G, we sequentially perform message passing for nodes from the predecessors following the topological order of the DAG G and then reverse the node orders and perform an additional encoding process (See Figure <ref type="figure" target="#fig_1">3</ref> (c), and Section A.3 of the appendix for more details). We concatenate the outputs of both the bi-directional graph encoder and the set encoder, and feed them to two linear layers with relu to predict accuracy. We train the predictor f ? to minimize the MSE loss between the predicted accuracy and the true accuracy s i of the model on each task:</p><formula xml:id="formula_10">min ?,? ? ?p(? ) L ? ? (s, D, G) = min ?,? ? ?p(? ) (s -f ? (D, G)) 2<label>(8)</label></formula><p>In the meta-test, the predictor f ? * (? i | D, ?i ) predicts accuracies {? i } ns i=1 for a given D and { ?i } ns i=1 and then select the neural architecture having the highest predicted accuracy among { ?i } ns i=1 . To our knowledge, the performance predictors proposed in the existing NAS works need re-training for the new dataset since they have no module to handle multiple datasets but only encoders for graphs. On the other hand, our proposed meta-performance predictor consists of a dataset encoder and graph encoder to address such a problem with one training. Moreover, the proposed metaperformance predictor learns meta-knowledge accumulated from multiple tasks of MetaImageNet. Thus, the meta-learned predictor can predict performance for the new unseen dataset without training. As the number of target datasets increases, the time complexity of other predictors linearly increases while ours needs a constant time complexity of O(1).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">COMPARISION WITH CONVENTIONAL META-NAS</head><p>Existing meta-NAS approaches <ref type="bibr" target="#b11">(Elsken et al., 2020;</ref><ref type="bibr" target="#b31">Lian et al., 2019;</ref><ref type="bibr" target="#b44">Shaw et al., 2019)</ref> mainly target few-shot classification and cannot scale to such a large task with a large number of samples, since they rely on gradient-based meta-learning with rollout gradient steps. For few-shot learning, taking a few gradient steps is more than enough since there are only a few samples from which we can compute the gradients, but for large-scale learning, taking few gradient steps with minibatch sampling will only consider a very small portion of the samples in the task, which will be largely insufficient for the model to converge. Thus the model needs to take a large number of gradient steps with a large number of samples, however, this is extremely costly and almost infeasible when computing second-order derivatives as in MAML. On the contrary, MetaD2A can handle the largeway many-shot problem tackled by general classification tasks, since it does not require any rollout gradient steps when inferring the architecture. For conventional few-shot settings (5-way 5-shot), the number of samples per task is 25. Contrarily, the number of samples of CIFAR-100 used in MetaD2A is 100 ? 20 = 2, 000 which is 80 times larger. Further, the number of instances(e.g. 20) can be set to a much larger number. Overcoming such a limitation of conventional meta-NAS methods, and meta-learning in general, is one of the main contributions of this work, which is highly novel, in both the perspective of NAS and meta-learning. Further, to compare MetaD2A with meta-NAS methods, we perform experiments on few-shot tasks in the Section D of the Appendix.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">EXPERIMENT</head><p>We conduct extensive experiments to demonstrate the effectiveness and efficiency of MetaD2A.</p><p>Search Space We explore the same search space of NAS-Bench-201 <ref type="bibr" target="#b10">(Dong &amp; Yang, 2020)</ref>, cellbased NAS, for all experiments. (See the Section B of the Appendix for more details).</p><p>Dataset We validate our method on multiple datasets as follows. 1) MetaImageNet: To meta-learn our model on multiple tasks, we compiled a new dataset consisting of multiple subsets of ImageNet-1K <ref type="bibr" target="#b7">(Deng et al., 2009)</ref> and their neural architectures. We downsample the image size to 32?32 and split classes into 600/400 for meta-training/meta-test, respectively. For each task of metatraining/meta-test, we make a dataset by sampling 20 classes randomly from meta-training/metatest with an average of 26K images, we search set-specific architecture on the sampled dataset using GDAS <ref type="bibr">(Dong &amp; Yang, 2019b)</ref> or random search, and we obtain its accuracy by training the searched architecture on the dataset. We collect N ? =1,679/3,752 meta-training tasks for the MetaD2A generator/predictor and 20/400 meta-test tasks for them, respectively. We apply our model trained from MetaImageNet to multiple benchmark datasets such as 2) MNIST (LeCun &amp; Cortes, 2010), 3) SVHN <ref type="bibr" target="#b38">(Netzer et al., 2011)</ref>, 4) CIFAR-10 ( <ref type="bibr" target="#b24">Krizhevsky et al., 2009)</ref>, 5) CIFAR-100 <ref type="bibr" target="#b24">(Krizhevsky et al., 2009)</ref>, 6) Aircraft <ref type="bibr" target="#b37">(Maji et al., 2013)</ref>, and 7) Oxford-IIIT Pets <ref type="bibr" target="#b40">(Parkhi et al., 2012)</ref>. (For detailed descriptions of the baselines, datasets and experimental setup, see Appendix, Section C) Table <ref type="table">1</ref>: Performance of our model and baselines on unseen MetaImageNet tasks. We validate all models on 20 unseen tasks from MetaImageNet. We report the mean accuracies and 95% confidence intervals. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">SEARCH RESULTS ON METAIMAGENET</head><p>In Table <ref type="table">1</ref>, we report the performance of baselines and our model trained on the training set of MetaImageNet, on unseen tasks of MetaImageNet. Note that for the unseen MetaImageNet tasks, all baseline models * require to train the NAS model from scratch on each individual task . On the contrary, our model can generate set-dependent architectures with the amortized inference through the architecture generator. In the inference process, we sample 1000 architecture candidates from the MetaD2A generator and select and train one architecture with the highest predicted accuracy computed by the meta-performance predictor. We report the average accuracy of over 20 unseen tasks with 3 different random seeds for all models. For our model, we report both the search time (27.03s) averaged on the 20 tasks, and meta-training time (26964.18s) ? . We calculate the search expense by with the hourly cost of 1.46$/h, of Tesla P100 on Google Cloud. All models are run on an Nvidia 2080ti GPU. The result shows that the proposed MetaD2A outperforms all baselines in both the accuracy and search time on unseen tasks, with fast search time (27.03s). We further report the accuracy over the search time in Figure <ref type="figure" target="#fig_4">4</ref> (c). Once meta-training is done, our model can generate architectures on any new tasks without training the NAS framework again, which can be done at extremely low cost ($0.01 for each dataset), which makes it highly practical.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">SEARCH RESULTS ON MULTIPLE DATASETS</head><p>Table <ref type="table" target="#tab_0">2</ref> shows that our model meta-trained on the MetaImageNet dataset can generalize to multiple datasets such as MNIST, SVHN, CIFAR-10, CIFAR-100, Aircraft, and Oxford-IIIT Pets. Since MetaD2A can output set-adaptive architectures on the target datasets, there is no training cost for the unseen tasks. MetaD2A infers fast, as Table <ref type="table" target="#tab_1">3</ref> shows, such as 22.47(s) for CIFAR10 and it has relatively comparable parameters 1.05M for CIFAR10. On CIFAR10 and CIFAR100, we report the mean accuracies over 10 runs of the search process by retrieving accuracies of searched architectures from NAS-Bench-201. Specifically, following SETN <ref type="bibr">(Dong &amp; Yang, 2019a)</ref>, we retrieve the accuracies of N architecture candidates from the NAS-bench-201 and report the highest final accu-  Qualitative analysis We further perform several qualitative analysis in Figure <ref type="figure" target="#fig_4">4</ref>. We first visualize the dataset and architecture embeddings in the latent search space in Figure <ref type="figure" target="#fig_4">4</ref> (a). To embed graphs, we combine the graph encoder of Section 3.2 and the graph decoder of Section 3.1 and train it for graph reconstruction on MetaImageNet training tasks. We observe that the architecture whose embeddings are closer to the given dataset achieves better performance on it, compared to architecture embedded far away. For example, the final accuracy of the closest architecture point for the Aircraft dataset is 50.87% while the farthest architecture only achieves 16.77%. In Figure <ref type="figure" target="#fig_4">4</ref> (b), we visualize the latent vectors of the multiple datasets. Similar datasets, such as CIFAR-10 and CIFAR-100 are embedded closely in the latent space, while datasets dissimilar to them, such as MNIST and SVHN, are embedded far away. This enables generating architectures that perform better on each dataset.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">EFFECTIVENESS OF METAD2A</head><p>Now, we verify the efficacy of each component of MetaD2A with further analysis. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Ablation Study on MetaD2A</head><p>We train different variations of our model on the MetaImageNet, and test on CIFAR10, CIFAR100, and Aircraft in Table <ref type="table" target="#tab_4">5</ref> with the same experimental setup as the main experiments in Table <ref type="table" target="#tab_0">2</ref>. We observe that combining the performance predictor to the random architecture sampler (Random w/ PP.) enhances the accuracy of the final architecture on all datasets. The MetaD2A generator without the performance predictor (MetaD2A w/o PP.) outperforms the simple random architecture sampler (Random), especially by 15.3% on Aircraft, which demonstrates the effectiveness of MetaD2A over the random sampler. Finally, MetaD2A combined with the performance predictor outperforms all baselines, especially by 22.16% on Aircraft, suggesting that our MetaD2A generator can sample architectures that are more relevant to the given task. We further report the accuracy of the retrieved architecture on CIFAR10 and CIFAR100 with a different number of samples in Figure <ref type="figure">5</ref>, which shows that our model requires only a small number of samples to obtain a good neural architecture.   We further analyze our performance predictor using the Pearson correlation coefficient on the validation tasks of MetaImageNet to measure the linear correlation between the actual performance and the predicted performance (higher the better). Table <ref type="table" target="#tab_5">6</ref> shows that using both the dataset and the computational graph of the target architecture as inputs, instead of using one, leads to better performance. Moreover, the proposed encoder yields models with a higher correlation than the mean pooling, which suggests that the higher-order interaction between the input instances is useful for graph generation. Finally, using hierarchical set encoding and bi-directional graph encoding both clearly improve the performance of the predictor. We also further evaluate the predictor performance for both the Pearson correlation coefficient and the accuracy of the searched architecture on CIFAR10 and CIFAR100 under the same experiment condition of the Table <ref type="table" target="#tab_0">2</ref>. To separate the effect of the predictor, we use random sampling instead of MetaD2A generator for this experiment. Using a larger number of tasks for training improves the performance of the predictor in both measures, suggesting that we can further improve the performance of MetaD2A by collecting more training pairs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">CONCLUSION</head><p>We proposed a novel NAS framework, MetaD2A (Meta Dataset-to-Architecture), that can output a neural architecture for an unseen dataset. The MetaD2A generator learns a dataset-to-architecture transformation over a database of datasets and neural architectures by encoding each dataset using a set encoder and generating each neural architecture with a graph decoder. While the model can generate a novel architecture given a new dataset in an amortized inference, we further learn a meta-performance predictor to select the best architecture for the dataset among multiple sampled architectures. The experimental results show that our method outperforms conventional NAS methods on various datasets with very small search time (less than a minute) as it generalizes well across datasets. We believe that our work is a meaningful step towards building a practical NAS system for real-world scenarios, where we need to handle diverse datasets while minimizing the search cost.</p><p>h f . The graph encoding process with GNN-AMP can be regarded as a mimic of forwarding propagation of computation flows of neural networks. Inspired by the forward-backward propagation algorithm of neural network, we exploit Bi-directional encoding <ref type="bibr" target="#b53">(Zhang et al., 2019)</ref> which reverses the node orders to perform the encoding process. In this case, the final node becomes the starting point (See Figure <ref type="figure" target="#fig_1">3 (c)</ref>). Thus, the backward graph encoder outputs h b , which is the last hidden states of the starting node. We concatenate the outputs h f of the forward graph encoder and h b of the backward graph encoder as the final output of the Bi-directional graph encoding.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B SEARCH SPACE</head><p>Following the NAS-Bench-201 <ref type="bibr" target="#b10">(Dong &amp; Yang, 2020)</ref>, We explore the search space consisting of 15,625 possible cell-based neural architectures for all experiments. Macro skeleton is stacked with one stem cell, three stages consisting of 5 cells for each, and a residual block <ref type="bibr" target="#b15">(He et al., 2016)</ref> between stages. The stem cell consists of 3-by-3 convolution with 16 channels and cells of the first, second and third stages have 16, 32 and 64, respectively. Residual blocks have convolution layer with the stride 2 for down-sampling. A fully connected layer is attached to the macro skeleton for classification. Each cell is DAG which consists of the fixed 4 nodes and the fixed 6 edge connections.</p><p>For each edge connection, NAS models select one of 5 operation candidates such as zerorize, skip connection, 1-by-1 convolution, 3-by-3 convolution, and 3-by-3 average pooling. To effectively encode the operation information as the node features, we represent edges of graphs in NAS-Bench-201 as nodes, and nodes of them as edges. Additionally, we add a starting node and an ending node to the cell during training. All nodes which have no predecessors (suceessors) are connected to the starting (ending) node, which we delete after generating the full neural architectures.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C EXPERIMENTAL SETUP</head><p>C.1 DATASET 1) MetaImageNet 447 neural architectures out of the 1,679 tasks used for training MetaD2A generator are searched with GDAS <ref type="bibr">(Dong &amp; Yang, 2019b)</ref>. We observed that the MetaD2A generator trained with tasks including neural architectures retrieved from GDA tend to converge to generate similar cell structures even for different tasks. Thus, we use additional 1,232 training tasks to increase the exploration diversity with the random search as follows: We first randomly sample five architecture candidates from 1000 architectures, which are in the rank of top-1000 accuracy on the ImageNet-16-120 from NAS-Bench-201 <ref type="bibr" target="#b10">(Dong &amp; Yang, 2020)</ref>. Then we train them on the dataset of the given task and select the architecture with the highest accuracy among the five. Although architectures searched in this manner are not optimal for the corresponding tasks, we observe that neural architectures generated from the latent space formed through training on those tasks significantly improve the search efficiency compared with the random sampling, as denoted in Table <ref type="table" target="#tab_4">5</ref>. Further, considering collectable database in real-world scenarios are usually not optimal, we believe that this is more realistic approach. To collect tasks for MetaD2A predictor, we randomly sample a subset from the meta-training set/meta-test set of MetaImageNet for meta-training/meta-test, respectively, and a neural architecture from the search space of NAS-Bench201. Then we train the neural architecture on the given subset to get accuracy. In this way, we simply collect 3,752/400 tasks for meta-training/meta-test, respectively. 2) MNIST (LeCun &amp; Cortes, 2010): This is a standard image classification dataset which contains 70K 28?28 grey colored images that describe 10 digits. We upsample the images to 32?32 pixels to satisfy the minimum required pixel size of the NAS-Bench 201 due to the residual blocks in the macro skeleton. We use the training/test split from the original dataset, where 60K images are used for training and 10K are used for test. 3) SVHN <ref type="bibr" target="#b38">(Netzer et al., 2011)</ref>: This dataset consists of 32?32 color images where each has a digit with a natural scene background. The number of classes is 10 denoting from digit 1 to 10 and the number of training/test images is 73257/26032, respectively. 4) <ref type="bibr">CIFAR-10 (Krizhevsky et al., 2009)</ref>: This dataset is a popular benchmark dataset for NAS, which consists of 32?32 colour images from 10 general object classes. The training set consists of 50K images, 5K for each class, and the test set consists of 10K images, 1K for each class. 5) CIFAR-100 <ref type="bibr" target="#b24">(Krizhevsky et al., 2009)</ref>: This dataset consists of colored images from 100 fine-grained general object classes. Each class has 500/100 images for training and test, respectively. 6) Aircraft <ref type="bibr" target="#b37">(Maji et al., 2013)</ref> This is fine-grained classification benchmark dataset containing 10K images from 30 different aircraft classes. We resize all images into 32?32. 7) Oxford-IIIT Pets <ref type="bibr" target="#b40">(Parkhi et al., 2012)</ref> This dataset is for fine-grained classification which has 37 breeds of pets with roughly 200 instances for each class. There is no split file provided, so we use the 85% of the dataset for training and the other 15% are as a test set. We also resize all images into 32?32. For CIFAR10 and CIFAR100, we used the training, validation, and test splits from the NAS-Bench-201, and use random validation/test splits for MNIST, SVHN, Aircraft, and Oxford-IIIT Pets by splitting the test set into two subsets of the same size. The validation set is used to update the searching algorithms as a supervision signal and the test set is used to evaluate the performance of the searched architectures.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C.2 BASELINES</head><p>We now briefly describe the baseline models and our MetaD2A model. 1) RSPS <ref type="bibr" target="#b30">(Li &amp; Talwalkar, 2019)</ref> This method is a combination of random search and weight sharing, which trains randomly sampled sub-graphs from weight shared DAG of the search space. The method then selects the best performing sub-graph among the sampled ones as the final neural architecture. 2) ENAS <ref type="bibr" target="#b41">(Pham et al., 2018)</ref> This is an RL-based NAS method, which introduces the controller to select a sub-graph from a large computational graph, which allows efficient architecture search by sharing parameters among the child models and providing empirical performance as the reward. 3) DARTS <ref type="bibr" target="#b34">(Liu et al., 2019)</ref> This gradient-based NAS method speeds up the search time by introducing differentiable architecture representations with continuous relaxation of the discrete computational graph. We use the first-order DARTS in all experiments. 4) SETN <ref type="bibr">(Dong &amp; Yang, 2019a</ref>) SETN is an one-shot NAS method, which selectively samples competitive child candidates by learning to evaluate the quality of the candidates based on the validation loss. 5) GDAS <ref type="bibr">(Dong &amp; Yang, 2019b)</ref> This is a Gumbel-Softmax based differentiable neural architecture sampler, which is trained to minimize the validation loss with the architecture sampled from DAGs. 6) PC-DARTS <ref type="bibr" target="#b50">(Xu et al., 2020)</ref> This is a gradient-based NAS which partially samples channels to apply operations, to improve the efficiency of NAS in terms of memory usage and search time compared to DARTS. We exploit the code at https://github.com/yuhuixu1993/PC-DARTS. 7) DrNAS <ref type="bibr" target="#b5">(Chen et al., 2020)</ref> This is a NAS approach that introduces Dirichlet distribution to approximate the architecture distribution, to enhance the generalization performance of differentiable architecture search. We use the code at https://github.com/xiangning-chen/DrNAS. We report the results on CIFAR10 and CIFAR100 in this paper using the provided code from the authors on the split set of NAS-Bench 201 while their reported results in the paper of the authors are 94.37 and 73.51, respectively on random training/test splits on CIFAR10 and CIFAR100. 8) ResNet <ref type="bibr" target="#b15">(He et al., 2016)</ref> This is a convolutional network which connects the output of previous layer as input to the current layer. It has achieved impressive performance on many challenging image tasks. We use ResNet56 in all experiments. 9) REA <ref type="bibr" target="#b42">(Real et al., 2019)</ref> This is an evolutional-based search method by using aging based tournament selection, showing evolution can work in NAS. 10) RS <ref type="bibr" target="#b3">(Bergstra &amp; Bengio, 2012)</ref> This is based on random search and we randomly samples architectures until the total time of training and evaluation reaches the budget. 11) REINFORCE (Williams, 1992) This is a RL-based NAS. We reward the model with the validation accuracy after 12 epochs of training. 12) BOHB <ref type="bibr" target="#b12">(Falkner et al., 2018)</ref> This combines the strengths of tree-structured parzen estimator based baysian optimization and hyperband, performing better than standard baysian optimization methods. 13) DVAE <ref type="bibr" target="#b53">(Zhang et al., 2019)</ref> This uses bayesian optimization to search for the best performing DAGs by using the latent space produced by D-VAE. We use the code at https://github.com/muhanzhang/D-VAE 14) FNA <ref type="bibr" target="#b13">(Fang et al., 2020)</ref> This starts from a pretrained seed network and adapts it to a target dataset using a differentiable NAS. When searching and training the architecture, it uses parameter remapping. We make the seed network by retrieving the best-performing architecture from NAS-Bench-201 and train the network on CIFAR-100. The total cost for pre-training is 6856(s). Here we use DARTS for architecture search and we use the same procedure of parameter remapping. 15) MetaD2A (Ours) This is our meta-NAS framework described in section 3, which can stochastically generate task-dependent computational graphs from a given dataset, and use the performance predictor to select the best performing candidates.</p><p>We follow the same settings of NAS-Bench-201 <ref type="bibr" target="#b10">(Dong &amp; Yang, 2020)</ref> for all baselines and use the code at https://github.com/D-X-Y/AutoDL-Projects except for 6), 7) and 13).  To demonstrate the set-dependent predictor on multiple tasks, we compare our preidctor (MetaD2A-GD) with other competitive setindependent predictors such as linaer layer-based, LSTM-based <ref type="bibr" target="#b16">(Hochreiter &amp; Schmidhuber, 1997)</ref>, <ref type="bibr">GCN-based (Kipf &amp; Welling, 2016)</ref> predictors. Note that the proposed predictor can accumulate meta-knowledge from multiple tasks to handle multiple datasets with once training while none of other set-independent predictors can do. Also, we add our predictor with graph encoder (MetaD2A-G) which is set-independent. We train with them on the multiple tasks of MetaImageNet and then compute Pearson correlation coefficient between the predicted accuracy and the true accuracy on 400 unseen MetaImageNet tasks. The higher score means the predictor estimates better the true accuracy. We observe that linear layersbased or LSTM-based predictors fail to be learned on multiple tasks. We conjecture the reason is that they are too simple and ignore the change in accuracy dependent on data even the same graph. GCN-based predictor quite successfully works well even it also set-independent. However, our MetaD2A-DG, which can consider both input dataset and graph, outperforms the GCN predictor. Even MetaD2A-G, which uses graph encoder only performs better than the GCN predictor due to the asynchronous message passing. Our predictor can search architecture for new dataset instantly by designed for capturing both dataset and graph and trained with amortized learning to accumulate meta-knowledge.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D.2 PERFORMANCE ON FEW-SHOT CLASSIFICATION TASK</head><p>Method Params (K) MiniImageNet 5-way 1-shot 5-way 5-shot MAML <ref type="bibr" target="#b14">(Finn et al., 2017)</ref> 32.9 48.70 63.11 MAML++ <ref type="bibr" target="#b0">(Antoniou et al., 2018)</ref> 32.9 52.15 68.32 AutoMeta <ref type="bibr" target="#b21">(Kim et al., 2018)</ref> 28 49.58 65.09 BASE <ref type="bibr" target="#b44">(Shaw et al., 2019)</ref> 1200 -66.20 T-NAS++ <ref type="bibr" target="#b31">(Lian et al., 2019)</ref> 26.5 54.11 69.59 MetaNAS <ref type="bibr" target="#b11">(Elsken et al., 2020)</ref>  We further compared our method against Meta-NAS methods <ref type="bibr" target="#b21">(Kim et al., 2018;</ref><ref type="bibr" target="#b11">Elsken et al., 2020;</ref><ref type="bibr" target="#b31">Lian et al., 2019;</ref><ref type="bibr" target="#b44">Shaw et al., 2019)</ref> on few-shot classification tasks, which are the main setting existing Meta-NAS methods consider. Note that we can generate an architecture for each task almost instantly with MetaD2A for the few-shot task. Following <ref type="bibr" target="#b11">(Elsken et al., 2020;</ref><ref type="bibr" target="#b31">Lian et al., 2019)</ref>, we adopt bi-level optimization (e.g., MAML framework) to meta-learn initial weight on a meta-training set of mini-imagenet.</p><p>As shown in the Table <ref type="table" target="#tab_7">9</ref>, the few-shot classification results on MiniImageNet further clearly show the MetaD2A's effectiveness over existing Meta-NAS methods, as well as conventional meta-learning without NAS.  </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: (a) Overview of MetaD2A MetaD2A generator with ? and ? meta-learns the set-dependent DAGs representations on the training tasks, which are the subset of ImageNet-1K. MetaD2A predictor with ? metalearns to predict performance across the multiple tasks. We transfer MetaD2A to new target datasets to search for the set-specific architecture within a minute. (b) T-SNE visualization of a learned cross-modal latent space.</figDesc><graphic url="image-69.png" coords="3,451.25,88.42,50.66,54.28" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: (a) Hierarchical Set Pooling is the attention-based module to extract a set latent vector for DAG generation considering class label information from D. (b) GNN-AMP Graph Decoder generates the setdependent DAG from the latent vector with asynchronous message passing. (c) GNN-AMP Graph Encoder for the performance predictor summarizes the input DAG into a single vector with bi-directional encoding scheme.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head></head><label></label><figDesc>where B c ? X c and ||B c || = b by random sampling. We input the sub-dataset into the IntraSetPool, the intra-class encoder, to encode class prototype v c ? R 1?dv c for each class c = 1, ..., C: v c = IntraSetPool {x|x ? B c } . Then we further feed the class-specific set representations {v c } C c=1 into the InterSetPool, the inter-class encoder, to generate the dataset representation h e ? R 1?d he : h e = InterSetPool {v c } C c=1 . Both the set encoders are comprised of two building blocks: Set Attention Block (SAB) and Pooling by Multi-head Attention (PMA)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head></head><label></label><figDesc>Fig 3(b) illustrates the decoder. For more details, please see Section A.2.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 4 :</head><label>4</label><figDesc>Figure 4: Qualitative Examples (a) T-SNE vis. of z of datasets (stars) and graphs (grey circles). (b) T-SNE vis. of z of several datasets. (c) Mean Accuarcy over Search Time on unseen MetaImageNet tasks. The actual architectures and more examples of (a) and (b) are in the Section D of the Appendix.racy for each run. While N = 1000 in SETN, MetaD2A searches for an optimal architecture using a smaller number of samples (N = 40), reliably returning 94.37 and 73.51 over 10 runs, in very small confidence interval. N = 10 is sufficient to obtain the model that outperforms the baselines as shown in Figure5. On MNIST, SVHN, Aircraft, and Oxford-IIIT Pets, we search one architecture by MetaD2A model and report the accuracy averaging over 3 runs with different seeds. Such rapid search of REA, RS, REINFORCE and BOHB is only possible where all of the accuracies are precomputed like NAS-Bench201 so that it can retrieve instantly on the target dataset, therefore, it is difficult to apply them to other non-benchmark datasets. These experiments show the transferability of MetaD2A, as obtaining higher accuracy against all baselines on multiple datasets. Especially, we observe that MetaD2A which is learned over multiple tasks benefit to search set-dependent neural architectures for fine-grained datasets such as Aircraft and Oxford-IIIT Pets.</figDesc><graphic url="image-73.png" coords="8,410.09,217.51,89.10,70.01" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 6 :</head><label>6</label><figDesc>Figure 6: Acc. and Corr. over training task size</figDesc><graphic url="image-76.png" coords="9,308.80,454.57,96.34,72.26" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>DFigure 7 :</head><label>7</label><figDesc>Figure 7: (a) and (b) T-SNE vis. of z of several datasets. Each marker indicates the subset {x|x ? Bc} with different seed. We randomly choose the task in MetaImageNet. As the task of MetaImageNet which is constructed from ImageNet can consist of similar classes of other datasets or not, the embedding can be spread over the space as in (a) or drawn far from the other datasets' embedding as in (b).</figDesc><graphic url="image-78.png" coords="17,115.79,599.57,190.07,85.44" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Figure 8 :Figure 9 :</head><label>89</label><figDesc>Figure 8: (a) T-SNE vis. of Aircraft, CIFAR-10, CIFAR-100 and Graph embeddings. (b) The farthest architecture point of AIRCRAFT (c) The closest architecture point of AIRCRAFT (d) The farthest architecture point of CIFAR-10 (e) The closest architecture point of CIFAR-10 (f) The farthest architecture point of CIFAR-100 (g) The closest architecture point of CIFAR-100</figDesc><graphic url="image-81.png" coords="18,174.01,248.28,66.01,99.01" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 2 :</head><label>2</label><figDesc>Performance on multiple datasets We transfer MetaD2A to target datasets after training on MetaIm-ageNet, while other baselines are trained to search neural architectures for each dataset from scratch. We report accuracies ? with 95% confidence intervals on multiple datasets and parameters on CIFAR10.</figDesc><table><row><cell>Method</cell><cell>Transfer</cell><cell cols="2">Search Time (sec)</cell><cell cols="2">Search Expense ($)</cell><cell>Params (M)</cell><cell cols="2">MetaImageNet Top-1 Accuracy Top-5 Accuracy</cell></row><row><cell>ResNet (He et al., 2016)</cell><cell>N</cell><cell>N/A</cell><cell></cell><cell></cell><cell>N/A</cell><cell>0.86</cell><cell></cell><cell>67.12?1.23</cell><cell>91.03?0.68</cell></row><row><cell>ENAS (Pham et al., 2018)</cell><cell>N</cell><cell cols="2">247479.12</cell><cell></cell><cell>100.37</cell><cell>0.07</cell><cell></cell><cell>28.05?1.25</cell><cell>63.90?1.54</cell></row><row><cell>RSPS (Li &amp; Talwalkar, 2019)</cell><cell>N</cell><cell>11009.18</cell><cell></cell><cell></cell><cell>4.46</cell><cell>0.28</cell><cell></cell><cell>60.66?1.56</cell><cell>89.37?0.90</cell></row><row><cell>DARTS (Liu et al., 2019)</cell><cell>N</cell><cell>9714.14</cell><cell></cell><cell></cell><cell>3.94</cell><cell>0.10</cell><cell></cell><cell>43.02?3.74</cell><cell>76.70?2.72</cell></row><row><cell>SETN (Dong &amp; Yang, 2019a)</cell><cell>N</cell><cell>30738.71</cell><cell></cell><cell></cell><cell>12.47</cell><cell>0.36</cell><cell></cell><cell>60.87?1.54</cell><cell>89.03?0.78</cell></row><row><cell>GDAS (Dong &amp; Yang, 2019b)</cell><cell>N</cell><cell>25272.03</cell><cell></cell><cell></cell><cell>10.25</cell><cell>0.67</cell><cell></cell><cell>64.59?1.36</cell><cell>89.89?0.76</cell></row><row><cell>PC-DARTS (Xu et al., 2020)</cell><cell>N</cell><cell>10721.59</cell><cell></cell><cell></cell><cell>8.39</cell><cell>0.44</cell><cell></cell><cell>57.62?4.66</cell><cell>83.60?4.97</cell></row><row><cell>DrNAS (Chen et al., 2020)</cell><cell>N</cell><cell>10949.95</cell><cell></cell><cell></cell><cell>4.44</cell><cell>1.00</cell><cell></cell><cell>67.48?1.05</cell><cell>91.36?0.63</cell></row><row><cell>MetaD2A (Ours)</cell><cell>Y</cell><cell cols="2">27.03 / 26964.18  ?</cell><cell></cell><cell>0.01/10.94</cell><cell>0.72</cell><cell></cell><cell>69.36?2.16</cell><cell>91.91?1.21</cell></row><row><cell>Method</cell><cell>Params (M)</cell><cell>MNIST</cell><cell>SVHN</cell><cell></cell><cell cols="3">Target Dataset CIFAR-10 CIFAR-100</cell><cell>Aircraft</cell><cell>Oxford-IIIT Pets</cell></row><row><cell>Optimal</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell></cell><cell>94.37</cell><cell>73.51</cell><cell></cell><cell>-</cell><cell>-</cell></row><row><cell>ResNet (He et al., 2016)</cell><cell>0.86</cell><cell cols="3">99.67?0.01 96.13?0.19</cell><cell>93.97?0.00</cell><cell cols="2">70.86?0.00</cell><cell>47.01?1.16</cell><cell>25.58?3.43</cell></row><row><cell>REA (Real et al., 2019)</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell></cell><cell>93.92?0.30</cell><cell cols="2">71.84?0.99</cell><cell>-</cell><cell>-</cell></row><row><cell>RS (Bergstra &amp; Bengio, 2012)</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell></cell><cell>93.70?0.36</cell><cell cols="2">71.04?1.07</cell><cell>-</cell><cell>-</cell></row><row><cell>REINFORCE (Williams, 1992)</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell></cell><cell>93.85?0.37</cell><cell cols="2">71.71?1.09</cell><cell>-</cell><cell>-</cell></row><row><cell>BOHB (Falkner et al., 2018)</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell></cell><cell>93.61?0.52</cell><cell cols="2">70.85?1.28</cell><cell>-</cell><cell>-</cell></row><row><cell>DVAE (Zhang et al., 2019)</cell><cell>0.86</cell><cell>-</cell><cell>-</cell><cell></cell><cell>85.53?1.31</cell><cell cols="2">51.10?3.99</cell><cell>-</cell><cell>-</cell></row><row><cell>RSPS (Li &amp; Talwalkar, 2019)</cell><cell>-</cell><cell cols="3">99.63?0.02 96.17?0.12</cell><cell>84.07?3.61</cell><cell cols="2">52.31?5.77</cell><cell>42.19?3.88</cell><cell>22.91?1.65</cell></row><row><cell>DARTS (Liu et al., 2019)</cell><cell>-</cell><cell cols="4">98.82?0.69 65.71?25.88 54.30?0.00</cell><cell cols="2">15.61?0.00</cell><cell>22.50?0.04</cell><cell>18.14?5.00</cell></row><row><cell>SETN (Dong &amp; Yang, 2019a)</cell><cell>-</cell><cell cols="3">99.69?0.04 96.02?0.12</cell><cell>87.64?0.00</cell><cell cols="2">59.05?0.24</cell><cell>44.84?3.96</cell><cell>25.17?1.68</cell></row><row><cell>GDAS (Dong &amp; Yang, 2019b)</cell><cell>-</cell><cell cols="3">99.64?0.04 95.57?0.57</cell><cell>93.61?0.09</cell><cell cols="2">70.70?0.30</cell><cell>53.52?0.48</cell><cell>24.02?2.75</cell></row><row><cell>PC-DARTS (Xu et al., 2020)</cell><cell>1.18</cell><cell cols="3">99.66?0.04 95.40?0.67</cell><cell>93.19?0.69</cell><cell cols="2">68.89?0.23</cell><cell>26.33?3.40</cell><cell>19.25?8.52</cell></row><row><cell>DrNAS (Chen et al., 2020)</cell><cell>1.53</cell><cell cols="3">99.60?0.04 96.28?0.01</cell><cell>93.76?0.00</cell><cell cols="2">71.50?0.75</cell><cell>40.45?9.06</cell><cell>26.73?2.61</cell></row><row><cell>FNA (Fang et al., 2020)</cell><cell>0.07</cell><cell cols="3">99.51?0.16 94.19?0.12</cell><cell>70.41?0.77</cell><cell>-</cell><cell></cell><cell>26.43?3.37</cell><cell>13.57?0.41</cell></row><row><cell>MetaD2A (Ours)</cell><cell>1.05</cell><cell cols="3">99.70?0.18 96.56?0.13</cell><cell>94.37?0.01</cell><cell cols="2">73.51?0.00</cell><cell>60.31?0.74</cell><cell>38.74?0.08</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 3 :</head><label>3</label><figDesc>Architecture Search Time on multiple datasets We report architecture search time(s) of each model on various datasets. After once meta-trained on MetaImageNet, we transfer MetaD2A to target datasets, while other baselines are trained to search neural architectures for each dataset from scratch.</figDesc><table><row><cell>Method</cell><cell>MNIST</cell><cell>SVHN</cell><cell cols="2">Target Dataset CIFAR-10 CIFAR-100</cell><cell>Aircraft</cell><cell>Oxford-IIIT Pets</cell></row><row><cell>RSPS (Li &amp; Talwalkar, 2019)</cell><cell cols="2">22457.20 27962.50</cell><cell>10200.10</cell><cell>18841.70</cell><cell>18697.40</cell><cell>3360.20</cell></row><row><cell>DARTS (Liu et al., 2019)</cell><cell cols="2">22398.40 26900.60</cell><cell>8007.13</cell><cell>18203.70</cell><cell>3624.70</cell><cell>2469.40</cell></row><row><cell>SETN (Dong &amp; Yang, 2019a)</cell><cell cols="2">69656.20 85189.60</cell><cell>30220.70</cell><cell>58808.70</cell><cell>18564.40</cell><cell>8625.20</cell></row><row><cell cols="3">GDAS (Dong &amp; Yang, 2019b) 60186.50 71595.00</cell><cell>25077.20</cell><cell>51580.00</cell><cell>18508.20</cell><cell>6965.40</cell></row><row><cell>PC-DARTS (Xu et al., 2020)</cell><cell cols="2">24857.80 31124.50</cell><cell>10395.00</cell><cell>19951.50</cell><cell>3524.30</cell><cell>2844.60</cell></row><row><cell>DrNAS (Chen et al., 2020)</cell><cell cols="2">44131.00 52791.00</cell><cell>21760.00</cell><cell>34529.00</cell><cell>32422.00</cell><cell>6019.50</cell></row><row><cell>FNA (Fang et al., 2020)</cell><cell cols="2">22830.40 27087.90</cell><cell>9462.80</cell><cell>-</cell><cell>3972.80</cell><cell>2619.50</cell></row><row><cell>MetaD2A (Ours)</cell><cell>32.95</cell><cell>30.20</cell><cell>22.47</cell><cell>68.58</cell><cell>34.51</cell><cell>38.79</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 4 :</head><label>4</label><figDesc>Generative Abilities</figDesc><table><row><cell cols="3">Validity Uniqueness Novelty</cell></row><row><cell>1.0000</cell><cell>0.3519</cell><cell>0.6731</cell></row></table><note><p>Uniqueness, and Novelty. Each is defined as how often the model can generate valid neural architectures from the prior distribution, the proportion of unique graphs out of the valid generations, and the proportion of valid generations that are not included in the training set, respectively. The result shows MetaD2A can generate 67.31% new graphs that do not belong to the training set and can generate 35.19% various graphs, not picking alwaysperforming-well architecture.</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 5 :</head><label>5</label><figDesc>Ablation study of MetaD2A Figure 5: Accuracy over the number of arch. sampling Study of The Meta-Performance Predictor and the Set Encoding</figDesc><table><row><cell>Input Type</cell><cell>Encoder</cell><cell>Decoder</cell><cell>Pearson</cell></row><row><cell cols="2">Data Graph Mean Set Hier.</cell><cell>Bi-dir.</cell><cell>Corr. Coeff.</cell></row><row><cell></cell><cell></cell><cell></cell><cell>0.3699</cell></row><row><cell></cell><cell></cell><cell></cell><cell>0.6518</cell></row><row><cell></cell><cell></cell><cell></cell><cell>0.7299</cell></row><row><cell></cell><cell></cell><cell></cell><cell>0.7647</cell></row><row><cell></cell><cell></cell><cell></cell><cell>0.7663</cell></row><row><cell></cell><cell></cell><cell></cell><cell>0.7976</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 6 :</head><label>6</label><figDesc>Ablation Study of Predictor.</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 8 :</head><label>8</label><figDesc>Predictor Comparison</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 9 :</head><label>9</label><figDesc>Performance on few-shot classification task</figDesc><table><row><cell></cell><cell>30</cell><cell>49.7</cell><cell>62.1</cell></row><row><cell>MetaD2A (Ours)</cell><cell>28.9</cell><cell>54.71</cell><cell>70.59</cell></row></table></figure>
		</body>
		<back>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A META DATASET-DEPENDENT ARCHITECTURE GENERATION MODEL</head><p>A.1 BUILDING BLOCKS OF THE SET ENCODER Set Attention Block (SAB) is an attention-based block, which makes the features of all of the instances in the set reflect the relations between itself and others such as: SAB(X) = LN(H + MLP(H)) where H = LN(X + MH(X, X, X))</p><p>where LN and MLP is the layer normalization <ref type="bibr" target="#b1">(Ba et al., 2016)</ref> and the multilayer perceptron respectively, and H ? R n Bc ?d H is computed with multi-head attention MH(Q, K, V ) <ref type="bibr" target="#b47">(Vaswani et al., 2017)</ref> which queries, keys, and values are elements of input set X.</p><p>Features encoded from the SAB layers can be pooled by PMA on learnable seed vectors S ? R k?d S to produce k vectors by slightly modifying H calculation of (9):</p><p>While k can be any size (i.e. k=1,2,10,16), we set k = 1 for generating the single latent vector. For extracting consistent information not depending the order and the size of input elements, encoding functions should be constructed by stacking permutation-equivariant layers E, which satisfies below condition for any permutation ? on a set X <ref type="bibr" target="#b52">(Zaheer et al., 2017)</ref>:</p><p>Since all of the components in SAB and PMA are row-wise computation functions, SAB and PMA is permutation equivarint by definition (11).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.2 GRAPH DECODING</head><p>We use a GNN-based decoder with asynchronous message passing (GNN-AMP) which allows message passing to happen only along the topological order of the DAGs <ref type="bibr" target="#b53">(Zhang et al., 2019)</ref>. The graph decoder starts from an initial hidden state h v0 = NN init (z), where NN init is an MLP with tanh output and it finally completes building DAGs by adding a node iteratively. We assume that a node without any predecessors/successors is a start/end node, and if multiple nodes meet aforementioned conditions, we create a virtual starts/end node.</p><p>To generate the i th node v i , we compute the operation type o vi ? R 1?no over n o operations based on the current graph state h G := h vi-1 and then predict whether the edge exists between the node v i and other existing nodes. h vi is updated following equation ( <ref type="formula">4</ref>) when a new edge is added.</p><p>During the propagation process of GNN operator, h</p><p>vi is continuously encoded by aggregating the graph-level information. Following <ref type="bibr" target="#b53">(Zhang et al., 2019)</ref>, when we compute the edge probability e {vj ,vi} , we consider nodes v j |j = i -1, ..., 1 in the reverse order to reflect information from nodes close to v i to the root node when deciding whether edge connection. Note that the proposed process guarantees the generation of directed acyclic graph since directed edge is always created from existing nodes to a new node. Note that while DVAE <ref type="bibr" target="#b53">(Zhang et al., 2019)</ref> demonstrates their performance on the simple task such as graph-to-graph reconstruction, we aim a more challenging problem of generating a DAG from a latent embedding of a dataset.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.3 BI-DIRECTIONAL GRAPH ENCODING</head><p>Graph encoding model is based on GNN-AMP. For a given graph candidate G, we sequentially perform message passing for nodes from the predecessors following the topological order of the DAG G. We iteratively update hidden states h (t) vi using the equation ( <ref type="formula">12</ref>) by feeding in its predecessors' hidden states {u ? V in vi }.</p><p>For starting node v 0 which the set of predecessors is the empty, we output the zero vector as the hidden state of v 0 . We use the last hidden states of the ending node as the output of the graph encoder We adopt the teacher forcing training strategy <ref type="bibr" target="#b18">(Jin et al., 2018)</ref>, which performs the current decoding process after correcting the decoded graph as the true graph until the previous step. This strategy is only used during meta-training and we progress subsequent generation based on the currently decoded graph part without the true graph information in the meta-test. We use mini-batch gradient descent to train the model with Eq. ( <ref type="formula">7</ref>). The values of hyperparameters which we used for MetaD2A model in this paper are described in Table <ref type="table">7</ref>. For unseen datasets of MetaImageNet tasks, MNIST, SVHN, Aircraft, and Oxford-IIIT Pets, we sample n s = 1000 neural architecture candidates by MetaD2A generator, select the best one neural architecture (N = 1) with the highest predicted accuracy computed by the meta-performance predictor, and report the mean accuracy of trained the search neural architecture run with 3 different random seeds. For CIFAR10 and CIFAR100, we sample n s = 1000 neural architecture candidates by MetaD2A generator, select N = 40 neural architecture candidates in order of high predicted accuracies computed by the meta-performance predictor, retrieve the real accuracies of N = 40 neural architecture candidates from NAS-Bench 201 <ref type="bibr" target="#b10">(Dong &amp; Yang, 2020)</ref>, and report the highest real accuracy among real accuracies as the final performance of the searched neural architecture by MetaD2A. To train searched neural architectures for all datasets, we follow the hyperparameter setting of NAS-Bench-201 <ref type="bibr" target="#b10">(Dong &amp; Yang, 2020)</ref>, which is used for training searched neural architectures on CIFAR10 and CIFAR100. While we report accuracy after training 50 epoch for MNIST, the accuracy of 200 epoch are reported for all datasets except MNIST.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D.5 VISUALIZATION OF GRAPHS WITH DIVERSE EDGES</head><p>We further show that our model is able to generate graphs with diverse edges. We train our MetaD2A with pairs of MetaImageNet and randomly sampled graph. Following <ref type="bibr" target="#b53">Zhang et al. (2019)</ref>, each graph consists of 6 layers, which each node is sampled from 6 operations (e.g. 3?3 and 5?5 convolutions, 3?3 and 5 ? 5 depthwise-separable convolutions, 3?3 max pooling, and 3?3 average pooling), and edge connections are various between nodes. As depicted in Figure <ref type="figure">10</ref>, the model can decode various graphs of several edge connection types. </p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">How to train your maml</title>
		<author>
			<persName><forename type="first">Antreas</forename><surname>Antoniou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Harrison</forename><surname>Edwards</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Amos</forename><surname>Storkey</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1810.09502</idno>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title/>
		<author>
			<persName><forename type="first">Jimmy</forename><surname>Lei Ba</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jamie</forename><surname>Ryan Kiros</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Geoffrey</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1607.06450</idno>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
	<note type="report_type">Layer normalization. arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Designing neural network architectures using reinforcement learning</title>
		<author>
			<persName><forename type="first">Bowen</forename><surname>Baker</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Otkrist</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nikhil</forename><surname>Naik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ramesh</forename><surname>Raskar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations (ICLR)</title>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Random search for hyper-parameter optimization</title>
		<author>
			<persName><forename type="first">James</forename><surname>Bergstra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="281" to="305" />
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">ProxylessNAS: Direct neural architecture search on target task and hardware</title>
		<author>
			<persName><forename type="first">Han</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ligeng</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Song</forename><surname>Han</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations (ICLR)</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<author>
			<persName><forename type="first">Xiangning</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ruochen</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Minhao</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaocheng</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Cho-Jui</forename><surname>Hsieh</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2006.10355</idno>
		<title level="m">Drnas: Dirichlet neural architecture search</title>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Learning phrase representations using rnn encoder-decoder for statistical machine translation</title>
		<author>
			<persName><forename type="first">Kyunghyun</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bart</forename><surname>Van Merri?nboer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Caglar</forename><surname>Gulcehre</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dzmitry</forename><surname>Bahdanau</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fethi</forename><surname>Bougares</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Holger</forename><surname>Schwenk</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">ImageNet: A Large-Scale Hierarchical Image Database</title>
		<author>
			<persName><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L.-J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">One-shot neural architecture search via self-evaluated template network</title>
		<author>
			<persName><forename type="first">Xuanyi</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yi</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision (CVPR)</title>
		<meeting>the IEEE International Conference on Computer Vision (CVPR)</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note>a</note>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Searching for a robust neural architecture in four gpu hours</title>
		<author>
			<persName><forename type="first">Xuanyi</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yi</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on computer vision and pattern recognition (CVPR)</title>
		<meeting>the IEEE Conference on computer vision and pattern recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note>b</note>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Nas-bench-201: Extending the scope of reproducible neural architecture search</title>
		<author>
			<persName><forename type="first">Xuanyi</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yi</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations (ICLR)</title>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Meta-learning of neural architectures for few-shot learning</title>
		<author>
			<persName><forename type="first">Thomas</forename><surname>Elsken</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Benedikt</forename><surname>Staffler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jan</forename><surname>Hendrik Metzen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Frank</forename><surname>Hutter</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition (CVPR)</title>
		<meeting>the IEEE conference on computer vision and pattern recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">Bohb: Robust and efficient hyperparameter optimization at scale</title>
		<author>
			<persName><forename type="first">Stefan</forename><surname>Falkner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aaron</forename><surname>Klein</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Frank</forename><surname>Hutter</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1807.01774</idno>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<author>
			<persName><forename type="first">Jiemin</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuzhu</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kangjian</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qian</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuan</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wenyu</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xinggang</forename><surname>Wang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2001.02525</idno>
		<title level="m">Fast neural network adaptation via parameter remapping and architecture search</title>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Model-agnostic meta-learning for fast adaptation of deep networks</title>
		<author>
			<persName><forename type="first">Chelsea</forename><surname>Finn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pieter</forename><surname>Abbeel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sergey</forename><surname>Levine</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning (ICML)</title>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shaoqing</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Long short-term memory</title>
		<author>
			<persName><forename type="first">Sepp</forename><surname>Hochreiter</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J?rgen</forename><surname>Schmidhuber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural computation</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="1735" to="1780" />
			<date type="published" when="1997">1997</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Cross attention network for few-shot classification</title>
		<author>
			<persName><forename type="first">Ruibing</forename><surname>Hou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hong</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">A</forename><surname>Bingpeng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shiguang</forename><surname>Shan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xilin</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems (NeurIPS)</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Junction tree variational autoencoder for molecular graph generation</title>
		<author>
			<persName><forename type="first">Wengong</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Regina</forename><surname>Barzilay</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tommi</forename><surname>Jaakkola</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning (ICML)</title>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">Hierarchical generation of molecular graphs using structural motifs</title>
		<author>
			<persName><forename type="first">Wengong</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Regina</forename><surname>Barzilay</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tommi</forename><surname>Jaakkola</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2002.03230</idno>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Neural architecture search with bayesian optimisation and optimal transport</title>
		<author>
			<persName><forename type="first">Kirthevasan</forename><surname>Kandasamy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Willie</forename><surname>Neiswanger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jeff</forename><surname>Schneider</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Barnabas</forename><surname>Poczos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Eric</forename><forename type="middle">P</forename><surname>Xing</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems (NeurIPS)</title>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">Auto-meta: Automated gradient based meta learner search</title>
		<author>
			<persName><forename type="first">Jaehong</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sangyeul</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sungwan</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Moonsu</forename><surname>Cha</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jung</forename><surname>Kwon Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Youngduck</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yongseok</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dong-Yeon</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiwon</forename><surname>Kim</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1806.06927</idno>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">Auto-encoding variational bayes</title>
		<author>
			<persName><forename type="first">P</forename><surname>Diederik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Max</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName><surname>Welling</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1312.6114</idno>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">Semi-supervised classification with graph convolutional networks</title>
		<author>
			<persName><forename type="first">N</forename><surname>Thomas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Max</forename><surname>Kipf</surname></persName>
		</author>
		<author>
			<persName><surname>Welling</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1609.02907</idno>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title level="m" type="main">Learning multiple layers of features from tiny images</title>
		<author>
			<persName><forename type="first">Alex</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Geoffrey</forename><surname>Hinton</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Imagenet classification with deep convolutional neural networks</title>
		<author>
			<persName><forename type="first">Alex</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Geoffrey</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems (NIPS)</title>
		<imprint>
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<author>
			<persName><forename type="first">Yann</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Corinna</forename><surname>Cortes</surname></persName>
		</author>
		<ptr target="http://yann.lecun.com/exdb/mnist/" />
		<title level="m">MNIST handwritten digit database</title>
		<imprint>
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Learning to balance: Bayesian meta-learning for imbalanced and out-of-distribution tasks</title>
		<author>
			<persName><forename type="first">Hae</forename><surname>Beom</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lee</forename></persName>
		</author>
		<author>
			<persName><forename type="first">Hayeon</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Donghyun</forename><surname>Na</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Saehoon</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Minseop</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Eunho</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sung</forename><forename type="middle">Ju</forename><surname>Hwang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations (ICLR)</title>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Set transformer: A framework for attention-based permutation-invariant neural networks</title>
		<author>
			<persName><forename type="first">Juho</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yoonho</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jungtaek</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Adam</forename><surname>Kosiorek</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Seungjin</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yee</forename><forename type="middle">Whye</forename><surname>Teh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning (ICML)</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note>a</note>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Meta-learning with differentiable convex optimization</title>
		<author>
			<persName><forename type="first">Kwonjoon</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Subhransu</forename><surname>Maji</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Avinash</forename><surname>Ravichandran</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stefano</forename><surname>Soatto</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note>b</note>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Random search and reproducibility for neural architecture search</title>
		<author>
			<persName><forename type="first">Liam</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ameet</forename><surname>Talwalkar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Uncertainty in Artificial Intelligence</title>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="367" to="377" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Towards fast adaptation of neural architectures with meta learning</title>
		<author>
			<persName><forename type="first">Dongze</forename><surname>Lian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yin</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yintao</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yanxiong</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Leyu</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Peilin</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Junzhou</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shenghua</forename><surname>Gao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations (ICLR)</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Efficient graph generation with graph recurrent attention networks</title>
		<author>
			<persName><forename type="first">Renjie</forename><surname>Liao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yujia</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yang</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shenlong</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Charlie</forename><surname>Nash</surname></persName>
		</author>
		<author>
			<persName><forename type="first">William</forename><forename type="middle">L</forename><surname>Hamilton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><surname>Duvenaud</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Raquel</forename><surname>Urtasun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Richard</forename><surname>Zemel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems (NeurIPS)</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Progressive neural architecture search</title>
		<author>
			<persName><forename type="first">Chenxi</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Barret</forename><surname>Zoph</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Maxim</forename><surname>Neumann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jonathon</forename><surname>Shlens</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wei</forename><surname>Hua</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Li-Jia</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Li</forename><surname>Fei-Fei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alan</forename><surname>Yuille</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jonathan</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kevin</forename><surname>Murphy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV)</title>
		<meeting>the European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note>a</note>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Darts: Differentiable architecture search</title>
		<author>
			<persName><forename type="first">Hanxiao</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Karen</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yiming</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations (ICLR)</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Constrained graph variational autoencoders for molecule design</title>
		<author>
			<persName><forename type="first">Qi</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Miltiadis</forename><surname>Allamanis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Marc</forename><surname>Brockschmidt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alexander</forename><surname>Gaunt</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems (NeurIPS)</title>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note>b</note>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Neural architecture optimization</title>
		<author>
			<persName><forename type="first">Renqian</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fei</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tao</forename><surname>Qin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Enhong</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tie-Yan</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems (NeurIPS)</title>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<monogr>
		<author>
			<persName><forename type="first">Subhransu</forename><surname>Maji</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Esa</forename><surname>Rahtu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Juho</forename><surname>Kannala</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Matthew</forename><surname>Blaschko</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrea</forename><surname>Vedaldi</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1306.5151</idno>
		<title level="m">Fine-grained visual classification of aircraft</title>
		<imprint>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b38">
	<monogr>
		<title level="m" type="main">Reading digits in natural images with unsupervised feature learning</title>
		<author>
			<persName><forename type="first">Yuval</forename><surname>Netzer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tao</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Adam</forename><surname>Coates</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alessandro</forename><surname>Bissacco</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bo</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrew</forename><forename type="middle">Y</forename><surname>Ng</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<monogr>
		<title level="m" type="main">On first-order meta-learning algorithms</title>
		<author>
			<persName><forename type="first">Alex</forename><surname>Nichol</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Joshua</forename><surname>Achiam</surname></persName>
		</author>
		<author>
			<persName><forename type="first">John</forename><surname>Schulman</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1803.02999</idno>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Cats and dogs</title>
		<author>
			<persName><forename type="first">O</forename><forename type="middle">M</forename><surname>Parkhi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Vedaldi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">V</forename><surname>Jawahar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Efficient neural architecture search via parameter sharing</title>
		<author>
			<persName><forename type="first">Hieu</forename><surname>Pham</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Melody</forename><forename type="middle">Y</forename><surname>Guan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Barret</forename><surname>Zoph</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jeff</forename><surname>Quoc V Le</surname></persName>
		</author>
		<author>
			<persName><surname>Dean</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning (ICML)</title>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Regularized evolution for image classifier architecture search</title>
		<author>
			<persName><forename type="first">Esteban</forename><surname>Real</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alok</forename><surname>Aggarwal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yanping</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Quoc V</forename><surname>Le</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the aaai conference on artificial intelligence (AAAI)</title>
		<meeting>the aaai conference on artificial intelligence (AAAI)</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<monogr>
		<title level="m" type="main">Oriol Vinyals, Razvan Pascanu, Simon Osindero, and Raia Hadsell. Meta-learning with latent embedding optimization</title>
		<author>
			<persName><forename type="first">Andrei</forename><forename type="middle">A</forename><surname>Rusu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dushyant</forename><surname>Rao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jakub</forename><surname>Sygnowski</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Meta architecture search</title>
		<author>
			<persName><forename type="first">Albert</forename><surname>Shaw</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wei</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Weiyang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Le</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bo</forename><surname>Dai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems (NeurIPS)</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Prototypical networks for few-shot learning</title>
		<author>
			<persName><forename type="first">Jake</forename><surname>Snell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kevin</forename><surname>Swersky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Richard</forename><surname>Zemel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems (NIPS)</title>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<monogr>
		<title/>
		<author>
			<persName><forename type="first">Christian</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wei</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yangqing</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pierre</forename><surname>Sermanet</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Scott</forename><surname>Reed</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dragomir</forename><surname>Anguelov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dumitru</forename><surname>Erhan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vincent</forename><surname>Vanhoucke</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrew</forename><surname>Rabinovich</surname></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Attention is all you need</title>
		<author>
			<persName><forename type="first">Ashish</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Noam</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Niki</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jakob</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Llion</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aidan</forename><forename type="middle">N</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">? Ukasz</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Illia</forename><surname>Polosukhin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems (NIPS)</title>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Matching networks for one shot learning</title>
		<author>
			<persName><forename type="first">Oriol</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Charles</forename><surname>Blundell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Timothy</forename><surname>Lillicrap</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Daan</forename><surname>Wierstra</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems (NIPS)</title>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Simple statistical gradient-following algorithms for connectionist reinforcement learning</title>
		<author>
			<persName><forename type="first">Williams</forename><surname>Ronald</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Machine learning</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="issue">3-4</biblScope>
			<biblScope unit="page" from="229" to="256" />
			<date type="published" when="1992">1992</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Pc-darts: Partial channel connections for memory-efficient architecture search</title>
		<author>
			<persName><forename type="first">Yuhui</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lingxi</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaopeng</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xin</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Guo-Jun</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qi</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hongkai</forename><surname>Xiong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations (ICLR)</title>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Nasbench-101: Towards reproducible neural architecture search</title>
		<author>
			<persName><forename type="first">Chris</forename><surname>Ying</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aaron</forename><surname>Klein</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Eric</forename><surname>Christiansen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Esteban</forename><surname>Real</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kevin</forename><surname>Murphy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Frank</forename><surname>Hutter</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning (ICML)</title>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="7105" to="7114" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">Deep sets</title>
		<author>
			<persName><forename type="first">Manzil</forename><surname>Zaheer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Satwik</forename><surname>Kottur</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Siamak</forename><surname>Ravanbakhsh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Barnabas</forename><surname>Poczos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ruslan</forename><forename type="middle">R</forename><surname>Salakhutdinov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alexander</forename><forename type="middle">J</forename><surname>Smola</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems (NIPS)</title>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">D-vae: A variational autoencoder for directed acyclic graphs</title>
		<author>
			<persName><forename type="first">Muhan</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shali</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhicheng</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Roman</forename><surname>Garnett</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yixin</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems (NeurIPS)</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">Shufflenet: An extremely efficient convolutional neural network for mobile devices</title>
		<author>
			<persName><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xinyu</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mengxiao</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<analytic>
		<title level="a" type="main">Neural architecture search with reinforcement learning</title>
		<author>
			<persName><forename type="first">Barret</forename><surname>Zoph</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Quoc</surname></persName>
		</author>
		<author>
			<persName><surname>Le</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations (ICLR)</title>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<analytic>
		<title level="a" type="main">Learning transferable architectures for scalable image recognition</title>
		<author>
			<persName><forename type="first">Barret</forename><surname>Zoph</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vijay</forename><surname>Vasudevan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jonathon</forename><surname>Shlens</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Quoc V</forename><surname>Le</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition (CVPR)</title>
		<meeting>the IEEE conference on computer vision and pattern recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
