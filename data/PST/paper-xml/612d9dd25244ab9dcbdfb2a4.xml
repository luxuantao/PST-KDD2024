<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Single Node Injection Attack against Graph Neural Networks</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date type="published" when="2021-10-03">3 Oct 2021</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Shuchang</forename><surname>Tao</surname></persName>
							<email>taoshuchang18z@ict.ac.cn</email>
							<affiliation key="aff0">
								<orgName type="department">Data Intelligence System Research Center</orgName>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="department">Institute of Computing Technology</orgName>
								<orgName type="institution">Chinese Academy of Sciences</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
							<affiliation key="aff4">
								<orgName type="institution">University of Chinese Academy of Sciences</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Qi</forename><surname>Cao</surname></persName>
							<email>caoqi@ict.ac.cn</email>
							<affiliation key="aff0">
								<orgName type="department">Data Intelligence System Research Center</orgName>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="department">Institute of Computing Technology</orgName>
								<orgName type="institution">Chinese Academy of Sciences</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Huawei</forename><surname>Shen</surname></persName>
							<email>shenhuawei@ict.ac.cn</email>
							<affiliation key="aff0">
								<orgName type="department">Data Intelligence System Research Center</orgName>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="department">Institute of Computing Technology</orgName>
								<orgName type="institution">Chinese Academy of Sciences</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
							<affiliation key="aff4">
								<orgName type="institution">University of Chinese Academy of Sciences</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Junjie</forename><surname>Huang</surname></persName>
							<email>huangjunjie17s@ict.ac.cn</email>
							<affiliation key="aff0">
								<orgName type="department">Data Intelligence System Research Center</orgName>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="department">Institute of Computing Technology</orgName>
								<orgName type="institution">Chinese Academy of Sciences</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
							<affiliation key="aff4">
								<orgName type="institution">University of Chinese Academy of Sciences</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Yunfan</forename><surname>Wu</surname></persName>
							<email>wuyunfan19b@ict.ac.cn</email>
							<affiliation key="aff0">
								<orgName type="department">Data Intelligence System Research Center</orgName>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="department">Institute of Computing Technology</orgName>
								<orgName type="institution">Chinese Academy of Sciences</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
							<affiliation key="aff4">
								<orgName type="institution">University of Chinese Academy of Sciences</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Xueqi</forename><surname>Cheng</surname></persName>
							<affiliation key="aff2">
								<orgName type="department">CAS Key Laboratory of Network Data Science and Technology</orgName>
							</affiliation>
							<affiliation key="aff3">
								<orgName type="department">Institute of Computing Technology</orgName>
								<orgName type="institution">Chinese Academy of Sciences</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
							<affiliation key="aff4">
								<orgName type="institution">University of Chinese Academy of Sciences</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Single Node Injection Attack against Graph Neural Networks</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2021-10-03">3 Oct 2021</date>
						</imprint>
					</monogr>
					<idno type="DOI">10.1145/3459637.3482393</idno>
					<idno type="arXiv">arXiv:2108.13049v2[cs.LG]</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.2" ident="GROBID" when="2023-01-01T13:30+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Node injection attack</term>
					<term>Adversarial attack</term>
					<term>Graph neural networks</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Node injection attack on Graph Neural Networks (GNNs) is an emerging and practical attack scenario that the attacker injects malicious nodes rather than modifying original nodes or edges to affect the performance of GNNs. However, existing node injection attacks ignore extremely limited scenarios, namely the injected nodes might be excessive such that they may be perceptible to the target GNN. In this paper, we focus on an extremely limited scenario of single node injection evasion attack, i.e., the attacker is only allowed to inject one single node during the test phase to hurt GNN's performance. The discreteness of network structure and the coupling effect between network structure and node features bring great challenges to this extremely limited scenario. We first propose an optimization-based method to explore the performance upper bound of single node injection evasion attack. Experimental results show that 100%, 98.60%, and 94.98% nodes on three public datasets are successfully attacked even when only injecting one node with one edge, confirming the feasibility of single node injection evasion attack. However, such an optimization-based method needs to be re-optimized for each attack, which is computationally unbearable. To solve the dilemma, we further propose a Generalizable Node Injection Attack model, namely G-NIA, to improve the attack efficiency while ensuring the attack performance. Experiments are conducted across three well-known GNNs. Our proposed G-NIA significantly outperforms state-of-the-art baselines and is 500 times faster than the optimization-based method when inferring.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>CCS CONCEPTS</head><p>• Information systems → Data mining.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Single Node Injection Attack</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">INTRODUCTION</head><p>Graph Neural Networks (GNNs) have achieved remarkable performance in many graph mining tasks such as node classification <ref type="bibr" target="#b13">[14,</ref><ref type="bibr" target="#b18">19,</ref><ref type="bibr" target="#b33">34,</ref><ref type="bibr" target="#b37">38,</ref><ref type="bibr" target="#b38">39]</ref>, graph classification <ref type="bibr" target="#b27">[28]</ref>, cascade prediction <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b25">26]</ref> and recommendation systems <ref type="bibr" target="#b9">[10]</ref>. Most GNNs follow a message passing scheme <ref type="bibr" target="#b36">[37]</ref>, learning the representation of a node by aggregating representations from its neighbors iteratively. Despite their success, GNNs are proved to be vulnerable to adversarial attacks <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b7">8,</ref><ref type="bibr" target="#b45">46]</ref>, which are attracting increasing research interests. Imperceptible perturbations on graph structure or node attributes can easily fool GNNs both at training time (poisoning attacks) as well as inference time (evasion attacks) <ref type="bibr" target="#b46">[47]</ref>. Pioneer attack methods for GNNs generally modify the structure <ref type="bibr" target="#b21">[22,</ref><ref type="bibr" target="#b45">46,</ref><ref type="bibr" target="#b46">47]</ref> or attributes <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b45">46]</ref> of existing nodes in the original graph, which requires high authority and makes the attack difficult to apply. The emerging node injection attack focuses on a more practical scenario. However, existing node injection attacks ignore extremely limited scenarios for adversarial attacks, namely the injected nodes might be excessive such that they may be perceptible or detected by both the target victim system or the target nodes. In addition, existing node injection attacks generally focus on poisoning scenarios, i.e., assuming that the target GNN is trained/retrained on the injected malicious nodes with labels. Such assumption is hard to satisfy in real scenarios since we may have no ideas about the label standard of target victim systems.</p><p>In this paper, we focus on a practical and extremely limited scenario of single node injection evasion attack, i.e., the attacker is only allowed to inject one single node during the test phase to hurt the performance of GNNs. Single node injection evasion attack has the advantages of being difficult to detect and low in cost. Taking social networks as an example, it is very difficult for both the detection system and the user to detect the addition of one single malicious follower, especially compared with existing node injection methods <ref type="bibr" target="#b30">[31]</ref> which lead to a sudden emergence of multiple followers. In addition, the focused evasion attack scenario does not require the vicious nodes to be present in the training set, nor does it force GNN to train on the unknown label, making it more realistic than the poisoning attack.</p><p>The key of single node injection evasion attack is to spread malicious attributes to the proper nodes along with the network structure. However, the discreteness of network structure and the coupling effect between network structures and node features bring great challenges to this extremely limited scenario. We first propose an optimization-based method, namely OPTI, to explore the performance upper bound of single node injection evasion attack. To directly optimize the continuous/discrete attributes and the discrete edges of the malicious node, we extend the Gumbel-Top-𝑘 technique to solve this problem. Experimental results show that OPTI can successfully attack 100%, 98.60%, 94.98% nodes on three public datasets, confirming the feasibility of single node evasion injection attack. Nevertheless, OPTI needs to be re-optimized for each attack, which is computationally unbearable. In addition, the information learned in each optimization is discarded, which hinders the generalization of the injection attack.</p><p>To solve the dilemma, we further propose a Generalizable Node Injection Attack model, namely G-NIA, to improve the attack efficiency while ensuring the attack performance. G-NIA generates the discrete edges also by Gumbel-Top-𝑘 following OPTI and captures the coupling effect between network structure and node features by a sophisticated designed model. Comparing with OPTI, G-NIA shows advantages in the following two aspects. One is the representation ability. G-NIA explicitly models the most critical feature propagation via jointly modeling. Specifically, the malicious attributes are adopted to guide the generation of edges, modeling the influence of attributes and edges better. The other is the high efficiency. G-NIA adopts a model-based framework, utilizing useful information of attacking during model training, as well as saving a lot of computational cost during inference without re-optimization.</p><p>To provide an intuitive understanding of single node evasion injection attack, we give a real example about the attack performance of our proposed generalizable node injection attack model in Figure <ref type="figure" target="#fig_0">1</ref>. Specifically, we construct a subgraph of the social network Reddit. The attacker makes a total of 9 nodes misclassified even in an extremely limited scenario, i.e., injecting one node and one edge. Indeed, we observe remarkable performances when applying our proposed G-NIA model on real large-scale networks. Experimental results on Reddit and ogbn-products show that our G-NIA achieves the misclassification rate of 99.9%, 98.8%, when only injecting one node and one edge. The results reveal that G-NIA significantly outperforms state-of-the-art methods on all the GNN models and network datasets, and is much more efficient than OPTI. The results demonstrate the representation ability and high efficiency of G-NIA.</p><p>To sum up, our proposal owns the following main advantages:</p><p>( </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">RELATED WORK 2.1 Adversarial Attacks on Graphs</head><p>GNNs have shown exciting results on many graph mining tasks, e.g., node classification <ref type="bibr" target="#b13">[14,</ref><ref type="bibr" target="#b19">20]</ref>, network representation learning <ref type="bibr" target="#b26">[27]</ref>, graph classification <ref type="bibr" target="#b27">[28]</ref>. However, they are proved to be sensitive to adversarial attacks <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b15">16,</ref><ref type="bibr" target="#b17">18,</ref><ref type="bibr" target="#b23">24,</ref><ref type="bibr" target="#b29">30,</ref><ref type="bibr" target="#b44">45,</ref><ref type="bibr" target="#b46">47]</ref>. Attack methods can perturb both the graph structure, node attributes, and labels <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b31">32,</ref><ref type="bibr" target="#b39">40]</ref>. Specifically, Nettack <ref type="bibr" target="#b45">[46]</ref> modifies node attributes and the graph structure guided by the gradient. RL-S2V <ref type="bibr" target="#b7">[8]</ref> uses reinforcement learning to flip edges. Others make perturbations via approximation techniques <ref type="bibr" target="#b34">[35]</ref>, label flipping <ref type="bibr" target="#b43">[44]</ref>, and exploratory strategy <ref type="bibr" target="#b21">[22]</ref>.</p><p>In the real world, modifying existing edges or attributes is intractable, due to no access to the database storing the graph data <ref type="bibr" target="#b30">[31]</ref>. Node injection attack <ref type="bibr" target="#b30">[31,</ref><ref type="bibr" target="#b35">36]</ref> focuses on a more practical scenario, which only injects some malicious nodes without modifying the existing graph. Pioneering researches focus on node injection poisoning attacks and have some problematic disadvantages which cannot be overlooked in an evasion attack scenario. Node Injection Poisoning Attack (NIPA) uses a hierarchical Q-learning network to sequentially generate the labels and edges of the malicious nodes <ref type="bibr" target="#b30">[31]</ref>. Nevertheless, NIPA fails to generate the attributes of injected nodes, which are extremely important in evasion attacks, resulting in its poor performance. Approximate Fast Gradient Sign method (AFGSM) provides an approximate closed-form attack solution to GCN with a lower time cost <ref type="bibr" target="#b35">[36]</ref>, but AFGSM is difficult to handle other GNNs, which limits its application. In addition, AFGSM cannot deal with the continuous attributed graphs which are common in the real world. Such approximation also leads to an underutilization of node attributes and structure.</p><p>There is still a lack of an effective method that simultaneously considers the attributes and structure of the injected node.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Attack in the Extremely Limited Scenario</head><p>Extremely Limited Scenario is firstly proposed in one-pixel attack by Su et al. <ref type="bibr" target="#b28">[29]</ref> in the computer vision area. The attacker can only modify one pixel to make the image misclassified. Su et al. propose a method for generating one-pixel adversarial perturbations based on differential evolution. This work starts the discussion of extremely limited scenario in adversarial learning area <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b10">11,</ref><ref type="bibr" target="#b20">21,</ref><ref type="bibr" target="#b32">33]</ref>.</p><p>Recently, in the graph adversarial learning area, Finkelshtein et al. <ref type="bibr" target="#b10">[11]</ref> propose the single-node attack to show that GNNs are vulnerable to the extremely limited scenario. The attacker aims to make the target node misclassified by modifying the attributes of a selected node. However, it is not realistic because modifying an existing node requires high authority and makes it difficult to apply. Up to now, there is still a lack of a practical method conducted in the extremely limited scenario in graph adversarial learning.</p><p>Our work fills the two gaps by proposing a single node injection evasion attack scenario and proposing G-NIA to ensure the effectiveness and efficiency of the model at the same time.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">PRELIMINARIES</head><p>This section introduces the task of node classification, together with GNNs to tackle this task.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">GNN for Node Classification</head><p>Node classification Given an attributed graph 𝐺 = (𝑉 , 𝐸, 𝑋 ), 𝑉 = {1, 2, ..., 𝑛} is the set of 𝑛 nodes, 𝐸 ⊆ 𝑉 × 𝑉 is the edge set, and 𝑋 ∈ R 𝑛×𝑑 is the attribute matrix with 𝑑-dimensional features. We denote network structure as adjacency matrix 𝐴. Given 𝑉 𝑙 ⊆ 𝑉 labelled by a class set K, the goal is to assign a label for each unlabelled node by the classifier 𝒁 = 𝑓 𝜃 (𝐺), where 𝒁 ∈ R 𝑛×𝐾 is the probability distribution, and 𝐾 = |K | is the number of classes. Graph neural networks. A typical GNN layer contains a feature transformation and an aggregation operation <ref type="bibr" target="#b22">[23,</ref><ref type="bibr" target="#b40">41,</ref><ref type="bibr" target="#b41">42]</ref> as:</p><formula xml:id="formula_0">𝑋 ′ 𝑖𝑛 = 𝑓 𝑡𝑟𝑎𝑛𝑠 (𝑋 𝑖𝑛 ) ; 𝑋 𝑜𝑢𝑡 = 𝑓 𝑎𝑔𝑔 𝑋 ′ 𝑖𝑛 ; 𝐺<label>(1)</label></formula><p>where 𝑋 𝑖𝑛 ∈ R 𝑛×𝑑 𝑖𝑛 and 𝑋 𝑜𝑢𝑡 ∈ R 𝑛×𝑑 𝑜𝑢𝑡 are the input and output features of a GNN layer with 𝑑 𝑖𝑛 and 𝑑 𝑜𝑢𝑡 dimensions. The feature transformation 𝑓 𝑡𝑟𝑎𝑛𝑠 (•) transforms 𝑋 𝑖𝑛 to 𝑋 ′ 𝑖𝑛 ∈ R 𝑛×𝑑 𝑜𝑢𝑡 ; and the feature aggregation 𝑓 𝑎𝑔𝑔 (• ; 𝐺) updates features by aggregating transformed features via 𝐺. Different GNNs have varied definitions of 𝑓 𝑡𝑟𝑎𝑛𝑠 (•) and 𝑓 𝑎𝑔𝑔 (• ; 𝐺). Taking the GCN as an example: 𝑋 ′ 𝑖𝑛 = 𝑋 𝑖𝑛 𝑾 ; 𝑋 𝑜𝑢𝑡 = D− 1 2 Ã D− 1 2 𝑋 ′ 𝑖𝑛 , where Ã = 𝐴 + 𝐼 , D is the degree matrix, 𝑾 ∈ R 𝑑 𝑖𝑛 ×𝑑 𝑜𝑢𝑡 is a weight matrix for feature transformation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Problem Definition</head><p>Single node injection evasion attack aims to make the original nodes misclassified in an extremely limited scenario, i.e., the attacker is only allowed to inject one single node in the test phase without modifying the existing nodes and edges in the original graph. We formalize the objective function as: max</p><formula xml:id="formula_1">𝐺 ′ ∑︁ 𝑡 ∈𝑉 𝑡𝑎𝑟 I 𝑓 𝜃 * 𝐺 ′ , 𝑡 ≠ 𝑦 𝑡 𝐺 ′ = 𝑉 ∪ 𝒗 𝑖𝑛 𝑗 , 𝐸 ∪ 𝒆 𝑖𝑛 𝑗 , 𝑋 ⊕ 𝒂 𝑖𝑛 𝑗 𝑠.𝑡 . 𝜃 * = arg min 𝜃 L train (𝑓 𝜃 (𝐺), 𝑦) ,<label>(2)</label></formula><p>where I(•) is an indicator function, 𝑉 𝑡𝑎𝑟 is the target nodes which can be a single node as well as multiple nodes, 𝑦 𝑡 is the ground truth label of node 𝑡, and 𝑓 𝜃 * is the attacked GNN model which has been trained on the original graph 𝐺. The perturbed graph 𝐺 ′ includes the malicious injected node 𝒗 𝑖𝑛 𝑗 , together with its edges 𝒆 𝑖𝑛 𝑗 and attributes 𝒂 𝑖𝑛 𝑗 . ⊕ refers to the concatenation operation. To ensure the imperceptibility, we define an injected edge budget i.e., ∥𝒆 𝑖𝑛 𝑗 ∥ &lt; Δ. And the malicious attributes are also limited by the maximum and minimum values of the existing attributes for continuous attributed graphs, and the maximum 𝐿 0 norm for discrete ones.</p><p>Node injection evasion attack takes the advantage of the feature propagation to spread the malicious attributes to the proper nodes without retraining. Feature propagation depends on the coupling effect of attributes and edges. Therefore, the core is to generate proper attributes and edges of the injected nodes, while considering their coupling effect between each other. Nevertheless, generating attributes and edges faces their respective challenges.</p><p>Challenges: For edges, the challenge comes from the discrete nature of edges. The resulting non-differentiability hinders backpropagating gradients to guide the optimization of injected edges. For attributes, different network datasets vary greatly. Most networks, such as social networks and co-purchasing networks, have continuous attributes to denote more meanings, while commonly used citation networks in researches have discrete node attributes to represent the appearance of keywords. But so far, few studies have focused on continuous attribute graphs Also, discrete attribute generation faces the same challenge as edge generation. It is worth noting that no node injection attack method can handle both two kinds of attributed graphs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">OPTIMIZATION-BASED NODE INJECTION ATTACK</head><p>In this section, we first propose an optimization-based approach, which is simple but effective. We evaluate it on three well-known datasets. Experimental results indicate that this method explores the performance upper bound. These results prove that the single node injection attack is sufficient for the targeted evasion attack, and it is time to explore the efficiency of the attack.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Optimization-based Approach</head><p>To tackle the single node injection evasion attack problem, we propose an optimization-based approach, namely OPTI, to directly optimize the attributes and edges of the vicious node. Specifically, we first initialize the vicious attributes and edges as free variables and inject them into the original graph. Then, we feed the perturbed graph into a surrogate GNN model to obtain the output. After that, we calculate the loss by Equation <ref type="formula" target="#formula_10">8</ref>and optimize the free variables until convergence. The joint optimization helps to depict the coupling effect between the attributes and edges of the vicious node. To overcome the gradient challenge of discrete variables as </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Attack Performance</head><p>We execute the single node injection evasion attack on three datasets. For each attack, the attacker only injects one node and one edge to attack one target node during the inference phase. Every node in the whole graph is selected as a target node, separately. We will elaborate on the settings and datasets in Section 6.1. Here, we just tend to illustrate the attack performance.</p><p>As shown in Figure <ref type="figure" target="#fig_1">2</ref>, our proposed method OPTI significantly outperforms state-of-the-art AFGSM on all three datasets. OPTI makes 100%, 98.60%, 94.98% nodes misclassified on Reddit, ogbnproducts, and Citeseer, even when injecting only one single node and one single edge. The results show that OPTI can attack almost all target nodes successfully. It also confirms the feasibility and effectiveness of the single node injection evasion attack, thus we do not need to inject more nodes on this attack scenario.</p><p>Even though OPTI achieves excellent results, it is difficult to be executed in reality. Because it is a case-by-case method, which means a distinct optimization is executed for each attack. The optimization is computationally expensive, and no knowledge can be saved and reused. What's worse, these limitations exist in almost all attack methods in the graph adversarial learning area. Therefore, it is time to explore the efficiency of single node injection attack.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">GENERALIZABLE NODE INJECTION ATTACK</head><p>In this section, we first introduce the generalization ability in the graph adversarial attack area. Then, we creatively propose a generalizable attack model G-NIA to carry out the node injection attack, which generates attack plans through the knowledge learning from the original graph.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Generalization in Graph Adversarial Attack</head><p>Generalization refers to the adaptability of machine learning algorithms to fresh (unseen) samples. In the field of graph adversarial learning, generalization is also important, especially in targeted attacks <ref type="bibr" target="#b1">[2]</ref>. For example, the above optimization-based method is almost impossible to tackle a real-life attack, even though it achieves superb attack performance. Because the huge computation brought by the case-by-case optimization is intolerable.</p><p>To solve the dilemma, we propose to model the optimization process by a generalizable model. The generalizable model should have the ability of both fitting and generalization. The fitting ability refers to having high attack performance during training, fitting OPTI as much as possible. Note that the fitting ability is the fundamental condition for generalization. A parametric attack model with poor attack performance does not meet the standard of generalization. The generalization ability refers to the performance and efficiency of (fresh) test data which is unseen in the training phase. The efficiency is naturally guaranteed by the parametric model since the knowledge has been preserved by the parameters. Note that, unlike those one-off optimization methods, the model can be saved and reused. A well-trained model can learn the information that is implicit in the data. Thus, the model can attack successfully on unseen fresh nodes, as long as they obey the assumption of independent and identical distribution.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Framework of Generalizable Node Injection Attack Model</head><p>We design and propose Generalizable Node Injection Attack model, namely G-NIA, to perform the single node injection evasion attack on graph data, which brings the generalization ability while keeping the high attack performance as the above OPTI. Figure <ref type="figure" target="#fig_2">3</ref> illustrates the overall framework of the proposed model G-NIA. The upper part shows the workflow of our proposed G-NIA during training. Specifically, G-NIA consists of three components: attribute generation, edge generation, and optimization. The first two components generate proper attributes and edges for the malicious injected node, and the last one performs an attack on the surrogate GNN and calculates the loss to optimize G-NIA. The lower part shows how G-NIA attacks the original graph in the inference phase. G-NIA first generates the malicious attributes, then generates edges, and uses the obtained adversarial graph 𝐺 ′ to attack the target nodes. Next, we will provide the fine details of each component in our proposed model G-NIA.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3">Attribute Generation</head><p>The key point of the node injection evasion attack is that the injected node propagates the malicious attributes to proper nodes through feature aggregation. This process requires close cooperation between the attributes and edges of the malicious node. Among them, attributes are essential for attack performance, since attributes bring additional information to a successful attack.</p><p>The goal of the attack is to make target nodes misclassified, which is inherently related to target nodes as well as class information, including the label class, i.e., 𝑦 𝑡 , and the class that is most likely to be misclassified by a surrogate GNN, i.e., 𝑘 𝑡 . Specifically, target nodes provide necessary information about who we are going to attack, and the class information tells a possible way to conduct the attack, i.e., changing their predicted labels from 𝑦 𝑡 to 𝑘 𝑡 . Based on these, we adopt the representation of target nodes 𝒓 𝑡 as well as the representation of label class and misclassified class, i.e., 𝒖 𝑡 = {𝒖 𝑦 𝑡 , 𝒖 𝑘 𝑡 }, to generate attributes. For the representation of target nodes, 𝒓 𝑡 is calculated by the surrogate GNN 𝑓 . For the representation of label class 𝒖 𝑡 , we adopt the 𝑦 𝑡 -th column of the feature transformation weights, i.e., 𝒖 𝑦 𝑡 = 𝑾 [:,𝑦 𝑡 ] and 𝒖 𝑘 𝑡 = 𝑾 [:,𝑘 𝑡 ] , where 𝑾 = 𝑾 0 𝑾 1 in a GNN with two layers. Because feature transformation is a mapping between the attribute space and label space.</p><p>Feeding the above representations, we utilize two layer neural networks F 𝑎 and a mapping function G 𝑎 to generate attribute of the malicious injected node 𝒂 𝑖𝑛 𝑗 :</p><formula xml:id="formula_2">𝒂 𝑖𝑛 𝑗 = G 𝑎 F 𝑎 𝑉 𝑡𝑎𝑟 , 𝑓 , 𝐺; 𝜃 * F 𝑎 𝑉 𝑡𝑎𝑟 , 𝑓 , 𝐺; 𝜃 * = 𝜎 [𝒓 𝑡 ∥𝒖 𝑡 ] 𝑾 𝑎 0 + 𝒃 𝑎 0 𝑾 𝑎 1 + 𝒃 𝑎 1 ,<label>(3)</label></formula><p>where 𝜃 = 𝑾 𝑎 0 , 𝑾 𝑎 1 , 𝒃 𝑎 0 , 𝒃 𝑎 1 are the trainable weights. G 𝑎 maps the output of F 𝑎 to the designated attributes space of the original graph, making the attributes similar to existing nodes. For continuous attributes, G 𝑎 contains a sigmoid function and a scaler to stretch to the designated attributes space. For discrete attributes, G 𝑎 is the Gumbel-Top-𝑘 technique, which will be introduced in Section 5.4.2.</p><p>Comparing with other approaches, G-NIA is more flexible. Our model can handle high-dimensional attributes generation for both discrete and continuous attributed graphs, but AFGSM can only deal with discrete attributes and NIPA even cannot generate attributes.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.4">Edge Generation</head><p>The malicious edges help the malicious node to spread its attributes to the required candidate nodes. Here, the candidate nodes are limited within the target nodes and their first-order neighbors, making the injected node to be at least the second-order neighbors of the target nodes. Because most GNNs with good performance contain two layers of feature propagation. Owing to the limited injected edge budget Δ, we score the candidate nodes and select the best Δ ones to connect to the injected node. The scores of candidate nodes measures the impact of connecting certain candidate nodes to the malicious node on the attack performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.4.1">Jointly modeling.</head><p>To capture the coupling effect between network structure and node features, we jointly model the malicious attributes and edges. Specifically, we use malicious attributes to guide the generation of malicious edges, jointly modeling the attributes and edges of the injected node. According to this, we use the representation of candidate nodes 𝒓 𝑐 and the representation of the malicious node 𝒓 𝑖𝑛 𝑗 to obtain scores. Specifically, 𝒓 𝑖𝑛 𝑗 is computed by feature transformation of the surrogate GNN, i.e., 𝒓 𝑖𝑛 𝑗 = 𝑓 𝑡𝑟𝑎𝑛𝑠 (𝒂 𝑖𝑛 𝑗 ), since the malicious node does not have structure yet. (The brown arrow from malicious attributes to the of edge generation in Figure <ref type="figure" target="#fig_2">3</ref>.)</p><p>As we mentioned before, the goal of the attack is closely tied to two elements, target nodes, and class information. The representations of the above two elements, i.e. 𝒓 𝑡 and 𝒖 𝑡 , are also included in the input of neural networks, and they are both repeated 𝑚 times, where 𝑚 is the number of candidate nodes.</p><p>With all the mentioned inputs, we adopt two layer neural networks F 𝑒 to obtain the score of candidate nodes, and use Gumbel-Top-𝑘 technique G 𝑒 to discretize the scores as follows:</p><formula xml:id="formula_3">𝒆 𝑖𝑛 𝑗 = G 𝑒 F 𝑒 𝒂 𝑖𝑛 𝑗 , 𝑉 𝑡𝑎𝑟 , 𝑓 , 𝐺; 𝜃 *<label>(4)</label></formula><formula xml:id="formula_4">F 𝑒 𝒂 𝑖𝑛 𝑗 , 𝑉 𝑡𝑎𝑟 , 𝑓 , 𝐺; 𝜃 * = 𝜎 𝒓 𝑖𝑛 𝑗 ∥𝒓 𝑡 ∥𝒖 𝑡 ∥𝒓 𝑐 𝑾 𝑒 0 + 𝒃 𝑒 0 𝑾 𝑒 1 + 𝒃 𝑒 1 , where 𝜃 = 𝑾 𝑒 0 , 𝑾 𝑒 1 , 𝒃 𝑒 0 , 𝒃 𝑒</formula><p>1 are the trainable weights.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.4.2">Gumbel-Top-𝑘 technique.</head><p>To tackle the discreteness of network structure, we adopt the Gumbel-Top-𝑘 technique to solve the optimization of edges. Gumbel distribution G 𝑖 is used to model the distribution of the extremum of a number of samples of various distributions <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b12">13]</ref>. Formally,</p><formula xml:id="formula_5">G 𝑖 = − log (− log (𝑈 𝑖 ))</formula><p>where</p><formula xml:id="formula_6">𝑈 𝑖 ∼ Uniform(0, 1) is Uniform distribution. Gumbel-Softmax is: Gumbel-Softmax(𝒛) 𝑖 = exp 𝒛 𝑖 +G 𝑖 𝜏 𝑛 𝑗=1 exp 𝒛 𝑗 +G 𝑗 𝜏 , (<label>5</label></formula><formula xml:id="formula_7">)</formula><p>where 𝜏 is the temperature controlling the smoothness, 𝒛 is the output of the neural network F 𝑒 . Gumbel-Softmax brings exploration to the edge selection through Gumbel distribution G 𝑖 . To encourage exploring, we further use 𝜖 to control the strength of exploration:</p><formula xml:id="formula_8">Gumbel-Softmax(𝒛, 𝜖) 𝑖 = exp 𝒛 𝑖 +𝜖G 𝑖 𝜏 𝑛 𝑗=1 exp 𝒛 𝑗 +𝜖G 𝑗 𝜏 .<label>(6)</label></formula><p>Gumbel-Top-𝑘 function G 𝑒 is:</p><formula xml:id="formula_9">G 𝑒 (𝒛) = 𝑘 ∑︁ 𝑗=1 Gumbel-Softmax(𝒛 ⊙ 𝑚𝑎𝑠𝑘 𝑗 , 𝜖),<label>(7)</label></formula><p>where 𝑘 is the budget of edges (discrete attributes), 𝑚𝑎𝑠𝑘 𝑗 filters the selected edges/attributes to ensure they will not be selected again. Note that the obtained vector is sharp but not completely discrete, which benefits training. In the test phase, we discretize the vector compulsorily to obtain discrete edges. And we do the same for discrete attributes. Gumbel-Top-𝑘 technique can solve the optimization problem of high-dimensional discrete attributes. But it is difficult for reinforcement learning methods to deal with huge discrete action spaces. It may be the reason why NIPA <ref type="bibr" target="#b30">[31]</ref> cannot generate attributes. In addition, Gumbel-Top-𝑘 does not affect training stability. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.5">Optimization</head><p>After generating the malicious attributes and edges, we inject the malicious node into the original graph 𝐺 to obtain the perturbed graph 𝐺 ′ . We feed 𝐺 ′ into the surrogate GNN model and compute the attack loss L 𝑎𝑡𝑘 : min</p><formula xml:id="formula_10">𝐺 ′ L 𝑎𝑡𝑘 𝑓 𝜃 * 𝐺 ′ , 𝑉 𝑡𝑎𝑟 = ∑︁ 𝑡 ∈𝑉 𝑡𝑎𝑟 𝒁 ′ 𝑡,𝑦 𝑡 − max 𝑘≠𝑦 𝑡 𝒁 ′ 𝑡,𝑘 .<label>(8)</label></formula><p>Attack loss L 𝑎𝑡𝑘 is used to guide the training process. We iteratively optimize the attack loss by gradient descent until convergence.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">EXPERIMENTS 6.1 Experimental Settings</head><p>6.1.1 Datasets. To illustrate the wide adaptability of the proposed G-NIA to different application scenarios, we conduct experiments on three different types of network datasets: a social network Reddit <ref type="bibr" target="#b13">[14,</ref><ref type="bibr" target="#b42">43]</ref>, a products co-purchasing network ogbn-products <ref type="bibr" target="#b14">[15]</ref> and a commonly used citation network Citeseer <ref type="bibr" target="#b3">[4]</ref>. Specifically, in Reddit, each node represents a post, with word vectors as attributes and community as the label, while each edge represents the postto-post relationship. In ogbn-products, a node represents a product sold in Amazon with the word vectors of product descriptions as attributes and the product category as the label, and edges between two products indicate that the products are purchased together.</p><p>In Citeseer, a node represents a paper with sparse bag-of-words as attributes and paper class as the label, and the edge represents the citation relationship. Note that the attributes of the first two datasets are continuous and Citeseer has discrete attributes. G-NIA can effectively attack both types of attributed graphs. Due to the high complexity of some GNN models, it is difficult to apply them to very large graphs with more than 200k nodes. Thus, we keep the subgraphs of Reddit and ogbn-products for experiments. For Reddit, we randomly sample 12k nodes as a subgraph and then select the largest connected components (LCC) of the subgraph. For ogbn-products, we randomly sample 55k nodes as a subgraph and then also keep the LCC. Following the settings of most previous attack methods <ref type="bibr" target="#b30">[31,</ref><ref type="bibr" target="#b35">36,</ref><ref type="bibr" target="#b45">46,</ref><ref type="bibr" target="#b46">47]</ref>, experiments are conducted on the LCC for all the three network datasets. The statistics of each dataset are summarized in Table <ref type="table" target="#tab_1">1</ref>.</p><p>6.1.2 Baseline Methods. Since the node injection attack is an emerging type of attack, which is firstly proposed on <ref type="bibr" target="#b30">[31]</ref>, only two researches are focusing on this area. To demonstrate the effectiveness of our proposed model G-NIA, we design a random method and two heuristic-based methods as our baselines. We also compare G-NIA with the two state-of-the-art node injection attack methods.</p><p>• Random. We randomly sample a node from the whole graph and take its attributes as the attributes of the malicious node to make the attack difficult to be detected. As for edges, we randomly select Δ nodes from the target nodes and their neighbors to ensure the injected node can influence the target node.</p><p>• Heuristic-based methods. We design two heuristic methods from the perspective of attributes and edges. (1) MostAttr. For the attributes of the malicious node, we use the attributes of a node randomly selected from the class that is most likely to be misclassified, i.e. 𝑘 𝑡 . The edge selection is the same as Random. (2) PrefEdge. We adopt the preferential attachment mechanism <ref type="bibr" target="#b30">[31]</ref> to inject edges. Specifically, it connects the nodes with a probability proportional to the degree of the nodes, which preferentially injects edges to high-degree nodes from the targets nodes and their neighbors. The attributes are obtained by randomly sampling, the same as Random.</p><p>• NIPA. NIPA <ref type="bibr" target="#b30">[31]</ref> is a node injection poisoning attack method, which generates the label and edges of the injected node via deep reinforcement learning. Since the code is not released, we reproduce the code and adapt it to the evasion attack scenario. For attributes, it originally applies a Gaussian noise N (0, 1) on the average attributes of existing nodes. We concern it is too weak for the evasion attack. Hence, we compute the average of attributes on the generated label and apply a Gaussian noise N (0, 1) added on to them. Afterward, we discretize or rescale the output, which is the same as our method, to obtain the malicious attributes.</p><p>• AFGSM. AFGSM <ref type="bibr" target="#b35">[36]</ref> is a targeted node injection attack, which calculates the approximation of the optimal closed-form solutions for attacking GCN. It sets the edges (discrete attributes) with the largest 𝑏 approximate gradients to 1 where 𝑏 is the budget. For continuous attributed graph, we do not limit the budget of attributes and set all the attributes with positive gradients to 1, to maximize its attack ability as much as possible. Note that AFGSM has already outperformed existing attack methods (such as Nettack <ref type="bibr" target="#b45">[46]</ref> and Metattack <ref type="bibr" target="#b46">[47]</ref>), thus, we just compare our proposed G-NIA with the state-of-the-art AFGSM.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.1.3">Experimental Configuration.</head><p>We conduct experiments to attack across three well-known GNNs (GCN, GAT, and APPNP). The hyper-parameters of them are the same with original works <ref type="bibr" target="#b18">[19,</ref><ref type="bibr" target="#b19">20,</ref><ref type="bibr" target="#b33">34]</ref>. We randomly split 80% nodes for the training phase, and the rest 20% for the inference phase. We also randomly select 20% nodes from the training split as a validation set, which are used for hyperparameters tuning and early stopping. The train/validation/test split is consistent in training GNN and the proposed G-NIA, which means G-NIA first trains on the nodes of the training set as the target nodes, and then validates on the nodes of the validation set, finally evaluates the performance on the test set.</p><p>For our proposed model G-NIA, we employ RMSprop Optimizer <ref type="bibr" target="#b11">[12]</ref> to train it for at most 2000 epochs and stop training if the misclassification rate of the validation set does not increase for 100 consecutive epochs. Learning rate is tuned over {1 −5 , 1 −4 , 1 −3 }, and the temperature coefficient 𝜏 of Gumbel-Top-𝑘 over {0.01, 0.1, ..., 100}. We use exponential decay for the exploration coefficient 𝜖. For the baseline methods, the settings are the same as described in the respective papers <ref type="bibr" target="#b29">[30,</ref><ref type="bibr" target="#b35">36]</ref>.</p><p>To demonstrate the capability of G-NIA, we design two scenarios for single node injection evasion attack, one is to attack a single target node, namely Single Target Attack, and the other is to attack multiple target nodes, namely Multiple Targets Attack. Note that existing target node injection attacks, such as AFGSM, To ensure a low attack cost, we strictly limit the injected edge budget Δ of the malicious node. For Single Target Attack, Δ = 1, which means the attacker is only allowed to inject one single edge. For Multiple Targets Attack, the edge budget is no more than Δ = min 𝑛 𝑡 𝐷 avg , 0.5 * 𝑚 , where 𝑚 is the number of candidate nodes, 𝑛 𝑡 is the number of target nodes, 𝐷 is the average degree which is shown in Table <ref type="table" target="#tab_1">1</ref>. And the injected attributes are also forced to be consistent with the original graph. This change is to make the settings more realistic.</p><p>All experiments run on a server with Intel Xeon E5-2640 CPU, Tesla K80, and 128GB RAM, which installs Linux CentOS 7.1.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.2">Performance of Single Target Attack</head><p>To evaluate the effectiveness and broad applicability of our proposed G-NIA, we conduct experiments on the Single Target Attack scenario on three benchmark datasets across three representative GNNs, i.e., GCN, GAT, and APPNP. In this scenario, the goal of the attacker is to attack one target node. The attacker is only allowed to inject one single node and one single edge to minimize the attack cost. We compare our proposed G-NIA model with the above heuristic and state-of-the-art baselines. The misclassification of the clean graph, i.e., Clean, is also included here as a lower bound.</p><p>Table <ref type="table" target="#tab_2">2</ref> shows the misclassification rate in the test phase. We can see that all the methods, including Random, increase the misclassification rate across three GNNs comparing with Clean. The heuristic baselines perform consistently on three GNNs. MostAttr performs better than Random. Comparing with Random, MostAttr only samples attributes from the most likely class 𝑘 𝑡 . This indicates that well-designed malicious attributes benefit the attack performance. However, PrefEdge even leads to a decrease compared with Random and is similar to Clean. The result indicates that the preferential attachment mechanism is not suited for node injection attacks.</p><p>For the state-of-art node injection attack baselines, NIPA performs well on the continuous attributed graphs, Reddit and ogbnproducts, while on the discrete attributed graph Citeseer, it performs even similar to Random. The result of Citeseer is different from what is shown in the original paper <ref type="bibr" target="#b30">[31]</ref> because we discretize the attributes to avoid the malicious node as outliers, which is performed for all methods. We also evaluate our reproduced code on the original poisoning attack settings, and the results show that our code without discretizing the attributes can reproduce the attack performance in the paper. We believe the performance on that paper is brought by the continuous malicious attributes. AFGSM performs best among all the baselines, especially on Citeseer dataset. This is because AFGSM can only generate discrete attributes. Its good performance indicates the effectiveness of the closed-form solution for attacking GCN proposed by AFGSM.</p><p>Our proposed model G-NIA outperforms all the baselines (or performs the same as the state-of-the-art baseline) on all datasets attacking all the three GNNs. On continuous attributed graphs, G-NIA shows tremendous attack performance, making nearly all the nodes misclassified. Specifically, G-NIA achieves 99.90% and 98.76% misclassification rate on Reddit and ogbn-products on GCN. As for GAT and APPNP, G-NIA also achieves the misclassification rate of more than 91%. On the discrete attributed graph Citeseer, G-NIA still performs better than state-of-the-art AFGSM on GCN, which indicates that G-NIA learns the knowledge neglected by the approximate optimal solution of AFGSM. Attacking GAT, G-NIA regains a significant advantage, indicating its adaptive capacity for different models. Attacking APPNP, G-NIA performs the same as AFGSM. This may be because that APPNP has multiple layers (more than 2 layers) of feature propagation but our malicious node selects the edge from only the one-order neighbors of the target node, making some information being neglected. Our G-NIA shows its superiority on the continuous attributed graph, and still has room to be improved on the discrete attributed graph.</p><p>In this setting, OPTI achieves misclassification rates of 100% on Reddit, 98.60% on ogbn-products, and 86.97% on Citeseer when attacking GCN. Note that the misclassification rate in Section 4.2 is 94.98% on Citeseer. The difference comes from the number of non-zero attributes of the vicious node being the maximum value of the original nodes (Section 4.2) or the average value (here). Our model G-NIA performs comparably with OPTI on all three datasets without re-optimization. The results reveal the attack performance and generalization ability of our proposed G-NIA.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.3">Performance of Multiple Targets Attack</head><p>To further demonstrate the attack ability of G-NIA, we conduct experiments on the Multiple Targets Attack scenario. It is a difficult scenario, where the attacker is required to make all nodes in the group of multiple targets misclassified with only one injected node. Each group contains three target nodes, and their distance is at most two hops. It is a reasonable scenario since the attacker may inject malicious information into one community for each attack. Note that the 𝒓 𝑡 is the average of the representation of all target nodes in each group, and so is 𝒖 𝑡 Also, we make sure there is no intersection between groups. We select all groups that meet the criteria from each dataset, i.e. 2959 groups in Reddit, 2620 groups in ogbn-products, 578 groups in Citeseer. We randomly split 80% of groups for training and the other 20% for tests. We also randomly split 20% from the training split as a validation set. Owing to space limitations, we only report experimental results on GCN on three datasets. And the OPTI is still not included due to the expensively computational cost.</p><p>The baselines show similar performance to that in the above Single Target Attack on Reddit and ogbn-products. However, on Citeseer, the misclassification rate of heuristic baselines and NIPA decreases, comparing with Clean. This may be because we only increase the edge budget rather than the most critical attribute budget. In addition, the discrete attributed graph is more difficult to attack, since the attribute budget is small. On continuous attributed graphs, MostAttr performs better than Random, while PrefEdge shows less effectiveness. NIPA achieves better results on the continuous attributed graph. AFGSM performs best among baselines on all datasets.</p><p>For our model, G-NIA outperforms all the baselines on all datasets. G-NIA shows great attack performance, making 92.62%, 95.42%, and 63.51% nodes misclassified successfully on Reddit, ogbn-products, and Citeseer. We can see that G-NIA has a great advantage on continuous attributed graphs, and can successfully attack almost all nodes. On the discrete attributed graph Citeseer, G-NIA still performs better than the state-of-the-art AFGSM.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.4">Black-box Attack</head><p>To explore G-NIA's possibility in the real world, we execute the black-box attack. We adopt the most limited black-box scenario, i.e., the attacker has no access to any information of the victim GNN model (the type, parameters, and outputs of GNN). Specifically, all the attack methods use a surrogate GCN to obtain the malicious nodes and are evaluated on completely unknown models. Here, we use GAT and APPNP to evaluate the attack methods. Since it is a very hard scenario, we only compare G-NIA to the state-of-the-art baselines, i.e., AFGSM and NIPA.</p><p>As shown in Table <ref type="table" target="#tab_4">4</ref>, the performance of NIPA is still unsatisfactory. AFGSM performs well whose misclassification rate is even 1% higher than our model on Citeseer. On Reddit and ogbn-products, our proposed G-NIA also achieves the misclassification rate of over 90% in this black-box scenario, significantly outperforming all the baselines. The result shows that G-NIA can maintain its advantages attacking unknown GNN models on continuous attributed graphs, while on discrete data, there is room for improvement.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.5">Efficiency Analysis</head><p>The main reason that we require a generalizable model for the node injection attack is to avoid the repeating optimization for each attack and improve the efficiency of the attack. To clarify the efficiency of our proposed method G-NIA, we report the running time of G-NIA, OPTI, and state-of-the-art baselines. The other heuristic baselines are not included in the experiments, considering their poor attack performance. The experiments are conducted on Single Target Attack on GCN on Reddit. We profile the node injection attack on a server with GPU Tesla K80 and show the results in Table <ref type="table">5</ref>. Note that running time refers to the average inference time of each node injection attack.</p><p>As shown in Figure <ref type="figure" target="#fig_5">5</ref>, OPTI takes about 1.17 seconds to complete each attack, tens or even hundreds of times longer than other methods. This is because that OPTI has to run a distinct optimization for each attack which is the reason why we need to further improve the attack efficiency.</p><p>Our proposed model G-NIA is significantly more efficient than OPTI. For each attack, our model can generate the attributes and edges of the injected node in 0.00253 seconds, nearly 500 times faster than OPTI. Moreover, G-NIA only takes 5 seconds to complete the node injection attacks on all test data, but OPTI needs 2333 seconds which is unbearable. In addition, G-NIA is 10 times faster than AFGSM. Because AFGSM has to loop many times to generate attributes, while G-NIA and NIPA can directly infer the results since they are both parametric models. This gap will be even more significant as the number of attacks increases. The results prove the efficiency of our G-NIA model when inferring.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.6">Case Study</head><p>Our proposed G-NIA achieves great attack performances. Especially on two continuous graphs, the misclassification rate is close to 100%. We wonder whether it injects a weird node that is very different  from the original ones. Therefore, we visualize the attributes of the original nodes and the malicious node on Reddit and ogbn-products, to have an intuitive understanding of the malicious node injected by G-NIA. We adopt the standard t-SNE technique <ref type="bibr" target="#b24">[25]</ref> to visualize high-dimensional attributes.</p><p>The attribute visualizations on Reddit and ogbn-products are illustrated in Figure <ref type="figure" target="#fig_4">4</ref>. For each dataset, we show two attack cases on the Single Target Attack scenario. Specifically, for each case, we randomly sample a target node and attack it via G-NIA. In the four attack cases, the malicious node is mix up with the original nodes, further confirming that the single injected node is imperceptible and undetectable. The results demonstrate that our attribute restriction and edge budget on the attack is helpful, and the injected node looks quite similar to existing nodes.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.7">Ablation Study</head><p>We analyze the influence of each part of G-NIA through experiments and compare their attack performances when the corresponding part is removed. Specifically, we remove attribute generation and edge generation respectively, and replace them with the Random strategy, namely G-NIA w/o Attr and G-NIA w/o Edge. We also investigate the influence of jointly modeling by removing the attribute guidance for edges, i.e., G-NIA w/o Joint. That is, we replace 𝒓 𝑖𝑛 𝑗 in the input of edge generation with an all-zero vector. Note that we take the experimental results of Single Target Attack on GCN on Reddit as an illustration.</p><p>As shown in Figure <ref type="figure" target="#fig_5">5</ref>, we find that the lack of any part damages the performance of G-NIA a lot. Among them, G-NIA w/o Attr performs worst, indicating that the most important part is attribute generation, which is consistent with our analysis in Section 5.3. G-NIA w/o Edge also loses half of the attack performance, implying the necessity of edge generation. Besides, G-NIA w/o Joint only achieves a 62% misclassification rate, which demonstrates the importance of jointly modeling. In other words, it makes sense to use malicious attributes to guide the generation of edges. In a word, the experimental results prove that each part of G-NIA is necessary and helpful to improve the attack performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">CONCLUSIONS</head><p>In this paper, we focus on an extremely limited scenario of single node injection evasion attack. We first present an effective optimization-based approach (OPTI) to explore the upper bound of attack performance. However, the case-by-case optimization is not of practical significance, due to its computationally expensive cost. To solve the dilemma, we propose a generalizable node injection attack model (G-NIA) to balance the attack performance and efficiency. G-NIA models the optimization process via a parametric model, to preserve the learned attack strategy and reuse it when inferring. We conduct extensive attack experiments on three representative GNNs (GCN, GAT, APPNP) and three well-known datasets. The experimental results reveal that G-NIA performs comparable with OPTI, but is much more efficient without re-optimization. With only injecting one node, G-NIA shows tremendous attack performances on Single Target Attack and Multiple Targets Attack tasks, outperforming state-of-the-art baselines. In addition, G-NIA can also carry out black-box attacks.</p><p>Although G-NIA is effective and efficient, there are still some remaining issues for future work. The first is whether the injected node is easy to be detected by some defense methods. If so, it is important to design and inject a more imperceptible malicious node. The second is whether G-NIA is effective on the robust GNNs <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b16">17,</ref><ref type="bibr" target="#b47">48]</ref> with adversarial training or robust training. We will explore these problems in the future.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: Comparison before and after single node injection evasion attack on Reddit.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: Misclassification rate of single node injection attack on GCN on three datasets. Clean refers to the clean graph. AFGSM is the state-of-the-art node injection attack method. OPTI is our proposed jointly optimization method.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: The overall architecture of our G-NIA model we mentioned above, we adopt the Gumbel-Top-𝑘 technique to aid in optimizing, which will be specifically introduced in Section 5.4.2.</figDesc><graphic url="image-13.png" coords="4,118.59,68.82,378.86,205.82" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head></head><label></label><figDesc>Attributes visualization on ogbn-products</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 4 :</head><label>4</label><figDesc>Figure 4: Case study on two continuous attributed graphs, Reddit and ogbn-products. The red cross indicates the injected node and the blue point refers to the original nodes.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 5 :</head><label>5</label><figDesc>Figure 5: Ablation study. Misclassification rate on clean graph and perturbed graphs attacked by G-NIA variants.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>1) Effectiveness: When only injecting one single node and one single edge, the proposed OPTI can successfully attack 100%, 98.60%, 94.98% nodes on a social network, a co-purchasing network, and a citation network. Our G-NIA can achieve comparable results during inferring without re-optimization like OPTI. Both of them significantly outperform state-ofthe-art methods on all three datasets.(2) Efficiency: The proposed model G-NIA is approximately 500 times faster than OPTI in the inference phase. (3) Flexibility: Extensive experiments are conducted on three kinds of network datasets across three well-known GNN models in two scenarios, to demonstrate the effectiveness of our proposed G-NIA model. In addition, G-NIA is also capable in the black-box scenario.</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 1 :</head><label>1</label><figDesc>Statistics of the evaluation datasets</figDesc><table><row><cell>Dataset</cell><cell>Type</cell><cell>𝑁 𝐿𝐶𝐶</cell><cell>𝑑</cell><cell cols="2">𝐾 𝐷 𝑎𝑣𝑔</cell></row><row><cell>Reddit</cell><cell>Social network</cell><cell cols="4">10,004 602 41 3.7</cell></row><row><cell cols="6">ogbn-products Co-purchasing network 10,494 100 35 7.4</cell></row><row><cell>Citeseer</cell><cell>Citation network</cell><cell cols="3">2,110 3,703 6</cell><cell>4.5</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2 :</head><label>2</label><figDesc>Misclassification rate of Single Target Attack across various GNNs on Reddit, ogbn-products and Citeseer</figDesc><table><row><cell></cell><cell></cell><cell>GCN</cell><cell></cell><cell></cell><cell>GAT</cell><cell></cell><cell></cell><cell>APPNP</cell><cell></cell></row><row><cell></cell><cell cols="9">Reddit ogbn-products Citeseer Reddit ogbn-products Citeseer Reddit ogbn-products Citeseer</cell></row><row><cell>Clean</cell><cell>8.15%</cell><cell>21.01%</cell><cell>22.04%</cell><cell>8.45%</cell><cell>24.34%</cell><cell>21.09%</cell><cell>6.65%</cell><cell>18.58%</cell><cell>19.91%</cell></row><row><cell>Random</cell><cell>11.33%</cell><cell>27.65%</cell><cell>24.05%</cell><cell>10.45%</cell><cell>29.60%</cell><cell>22.23%</cell><cell>8.05%</cell><cell>22.44%</cell><cell>20.83%</cell></row><row><cell cols="2">MostAttr 13.53%</cell><cell>31.82%</cell><cell>27.80%</cell><cell>12.30%</cell><cell>33.18%</cell><cell>24.41%</cell><cell>9.95%</cell><cell>26.56%</cell><cell>22.80%</cell></row><row><cell>PrefEdge</cell><cell>8.53%</cell><cell>23.05%</cell><cell>22.75%</cell><cell>8.70%</cell><cell>26.76%</cell><cell>21.64%</cell><cell>6.77%</cell><cell>20.11%</cell><cell>20.21%</cell></row><row><cell>NIPA</cell><cell>10.54%</cell><cell>22.20%</cell><cell>22.99%</cell><cell>10.19%</cell><cell>25.49%</cell><cell>21.56%</cell><cell>9.55%</cell><cell>20.10%</cell><cell>19.91%</cell></row><row><cell>AFGSM</cell><cell>68.57%</cell><cell>85.76%</cell><cell>84.60%</cell><cell>74.11%</cell><cell>80.28%</cell><cell>57.11%</cell><cell>45.43%</cell><cell>74.18%</cell><cell>41.47%</cell></row><row><cell>G-NIA</cell><cell>99.90%</cell><cell>98.76%</cell><cell cols="2">85.55% 99.85%</cell><cell>93.23%</cell><cell cols="2">73.70% 91.05%</cell><cell>98.67%</cell><cell>41.47%</cell></row><row><cell cols="5">can only attack one target node. Thus, we adjust it to adapt to the</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="3">scenario of Multiple Targets Attack.</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 3 :</head><label>3</label><figDesc>Misclassification rate of Multiple Targets Attack on GCN on Reddit, ogbn-products, and Citeseer</figDesc><table><row><cell></cell><cell cols="3">Reddit ogbn-products Citeseer</cell></row><row><cell>Clean</cell><cell>3.89%</cell><cell>16.28%</cell><cell>19.70%</cell></row><row><cell>Random</cell><cell>5.43%</cell><cell>21.69%</cell><cell>16.49%</cell></row><row><cell cols="2">MostAttr 6.40%</cell><cell>24.08%</cell><cell>18.19%</cell></row><row><cell>PrefEdge</cell><cell>5.59%</cell><cell>22.02%</cell><cell>16.24%</cell></row><row><cell>NIPA</cell><cell>13.96%</cell><cell>21.56%</cell><cell>16.09%</cell></row><row><cell>AFGSM</cell><cell>51.86%</cell><cell>87.34%</cell><cell>63.22%</cell></row><row><cell>G-NIA</cell><cell>92.62%</cell><cell>95.42%</cell><cell>63.51%</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 4 :</head><label>4</label><figDesc>Black-box Attack: Misclassification rate of Single Target Attack on GAT and APPNP on Reddit, ogbn-products, and Citeseer 10 −2 1.47 × 10 −3 1.17 2.53 × 10 −3</figDesc><table><row><cell></cell><cell></cell><cell cols="3">Reddit ogbn-products Citeseer</cell></row><row><cell></cell><cell>Clean</cell><cell>8.45%</cell><cell>24.34%</cell><cell>21.09%</cell></row><row><cell>GAT</cell><cell cols="2">NIPA AFGSM 45.83% 12.54%</cell><cell>25.92% 74.85%</cell><cell>21.09% 38.63%</cell></row><row><cell></cell><cell>G-NIA</cell><cell>94.60%</cell><cell>95.00%</cell><cell>36.49%</cell></row><row><cell></cell><cell>Clean</cell><cell>6.65%</cell><cell>18.58%</cell><cell>19.91%</cell></row><row><cell>APPNP</cell><cell cols="2">NIPA AFGSM 33.38% 9.90%</cell><cell>21.15% 67.65%</cell><cell>20.14% 30.09%</cell></row><row><cell></cell><cell>G-NIA</cell><cell>99.85%</cell><cell>99.14%</cell><cell>28.91%</cell></row><row><cell cols="5">Table 5: Running time comparison on Single Target Attack</cell></row><row><cell cols="2">on GCN on Reddit</cell><cell></cell><cell></cell></row><row><cell></cell><cell>AFGSM</cell><cell>NIPA</cell><cell>OPTI</cell><cell>G-NIA</cell></row><row><cell cols="2">Time (s) 2.72 ×</cell><cell></cell><cell></cell></row></table></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>ACKNOWLEDGMENTS</head><p>This work is funded by the National Key R&amp;D Program of China (2020AAA0105200) and the National Natural Science Foundation of China under Grant Nos. 62102402, 91746301 and U1911401. Huawei Shen is also supported by Beijing Academy of Artificial Intelligence (BAAI) under the grant number BAAI2019QN0304 and K.C. Wong Education Foundation.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Threat of Adversarial Attacks on Deep Learning in Computer Vision: A Survey</title>
		<author>
			<persName><forename type="first">Naveed</forename><surname>Akhtar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ajmal</forename><forename type="middle">S</forename><surname>Mian</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Access</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page" from="14410" to="14430" />
			<date type="published" when="2018">2018. 2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Learning to Attack: Adversarial Transformation Networks</title>
		<author>
			<persName><forename type="first">S</forename><surname>Baluja</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ian</forename><forename type="middle">S</forename><surname>Fischer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 30nd AAAI Conference on Artificial Intelligence (AAAI &apos;18)</title>
				<meeting>the 30nd AAAI Conference on Artificial Intelligence (AAAI &apos;18)</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Adversarial Attacks on Node Embeddings via Graph Poisoning</title>
		<author>
			<persName><forename type="first">Aleksandar</forename><surname>Bojchevski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stephan</forename><surname>Günnemann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 36th International Conference on Machine Learning (ICML &apos;19)</title>
				<meeting>the 36th International Conference on Machine Learning (ICML &apos;19)</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="695" to="704" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Certifiable Robustness to Graph Perturbations</title>
		<author>
			<persName><forename type="first">Aleksandar</forename><surname>Bojchevski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stephan</forename><surname>Günnemann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
				<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="page" from="8319" to="8330" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Efficient Robustness Certificates for Discrete Data: Sparsity-Aware Randomized Smoothing for Graphs, Images and More</title>
		<author>
			<persName><forename type="first">Aleksandar</forename><surname>Bojchevski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Johannes</forename><surname>Klicpera</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stephan</forename><surname>Günnemann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 37th International Conference on Machine Learning (ICML &apos;20)</title>
				<meeting>the 37th International Conference on Machine Learning (ICML &apos;20)</meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="11647" to="11657" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Popularity Prediction on Social Platforms with Coupled Graph Neural Networks</title>
		<author>
			<persName><forename type="first">Qi</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Huawei</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jinhua</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bingzheng</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xueqi</forename><surname>Cheng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 13th International Conference on Web Search and Data Mining (WSDM &apos;20)</title>
				<meeting>the 13th International Conference on Web Search and Data Mining (WSDM &apos;20)</meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="70" to="78" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">Xiangnan He, and Zibin Zheng. 2020. A Survey of Adversarial Learning on Graphs</title>
		<author>
			<persName><forename type="first">Liang</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jintang</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiaying</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tao</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zengxu</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kun</forename><surname>Xu</surname></persName>
		</author>
		<idno>ArXiv abs/2003.05730</idno>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Adversarial Attack on Graph Structured Data</title>
		<author>
			<persName><forename type="first">Hanjun</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hui</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tian</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xin</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lin</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jun</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Le</forename><surname>Song</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 35th International Conference on Machine Learning (ICML &apos;18)</title>
				<meeting>the 35th International Conference on Machine Learning (ICML &apos;18)</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="1123" to="1132" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Weighted random sampling with a reservoir</title>
		<author>
			<persName><forename type="first">P</forename><surname>Efraimidis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Spirakis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Inf. Process. Lett</title>
		<imprint>
			<biblScope unit="volume">97</biblScope>
			<biblScope unit="page" from="181" to="185" />
			<date type="published" when="2006">2006. 2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Graph Neural Networks for Social Recommendation</title>
		<author>
			<persName><forename type="first">Wenqi</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yao</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qing</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuan</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Eric</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiliang</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dawei</forename><surname>Yin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The World Wide Web Conference (WWW &apos;19</title>
				<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="417" to="426" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">Single-Node Attack for Fooling Graph Neural Networks</title>
		<author>
			<persName><forename type="first">Ben</forename><surname>Finkelshtein</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chaim</forename><surname>Baskin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Evgenii</forename><surname>Zheltonozhskii</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Uri</forename><surname>Alon</surname></persName>
		</author>
		<idno>ArXiv abs/2011.03574</idno>
		<imprint>
			<date type="published" when="2020">2020. 2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">Generating Sequences With Recurrent Neural Networks</title>
		<author>
			<persName><forename type="first">A</forename><surname>Graves</surname></persName>
		</author>
		<idno>ArXiv abs/1308.0850</idno>
		<imprint>
			<date type="published" when="2013">2013. 2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">Statistical theory of extreme values and some practical applications: a series of lectures</title>
		<author>
			<persName><forename type="first">Emil</forename><surname>Julius</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gumbel</forename></persName>
		</author>
		<imprint>
			<date type="published" when="1954">1954</date>
			<publisher>US Government Printing Office</publisher>
			<biblScope unit="volume">33</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Inductive Representation Learning on Large Graphs</title>
		<author>
			<persName><forename type="first">William</forename><surname>Hamilton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhitao</forename><surname>Ying</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jure</forename><surname>Leskovec</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
				<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="page" from="1024" to="1034" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Open Graph Benchmark: Datasets for Machine Learning on Graphs</title>
		<author>
			<persName><forename type="first">Weihua</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Matthias</forename><surname>Fey</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Marinka</forename><surname>Zitnik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuxiao</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hongyu</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bowen</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michele</forename><surname>Catasta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jure</forename><surname>Leskovec</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
				<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page">33</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Certified Robustness of Community Detection against Adversarial Structural Perturbation via Randomized Smoothing</title>
		<author>
			<persName><forename type="first">Jinyuan</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Binghui</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaoyu</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Neil</forename><surname>Zhenqiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gong</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of The Web Conference</title>
				<meeting>The Web Conference</meeting>
		<imprint>
			<publisher>WWW &apos;20</publisher>
			<date type="published" when="2020">2020. 2020</date>
			<biblScope unit="page" from="2718" to="2724" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Robust Training of Graph Convolutional Networks via Latent Perturbation</title>
		<author>
			<persName><forename type="first">Hongwei</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xinhua</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Machine Learning and Knowledge Discovery in Databases: European Conference (ECML/PKDD</title>
				<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="394" to="411" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">Adversarial Attacks and Defenses on Graphs: A Review and Empirical Study</title>
		<author>
			<persName><forename type="first">Wei</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yaxin</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Han</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yiqi</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiliang</forename><surname>Tang</surname></persName>
		</author>
		<idno>ArXiv abs/2003.00653</idno>
		<imprint>
			<date type="published" when="2020">2020. 2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Semi-Supervised Classification with Graph Convolutional Networks</title>
		<author>
			<persName><forename type="first">Thomas</forename><forename type="middle">N</forename><surname>Kipf</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Max</forename><surname>Welling</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
				<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
	<note>ICLR &apos;17</note>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Predict then Propagate: Graph Neural Networks meet Personalized PageRank</title>
		<author>
			<persName><forename type="first">Johannes</forename><surname>Klicpera</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aleksandar</forename><surname>Bojchevski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stephan</forename><surname>Günnemann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations (ICLR &apos;19)</title>
				<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">Exploring Adversarial Examples: Patterns of One-Pixel Attacks</title>
		<author>
			<persName><forename type="first">David</forename><surname>Kügler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alexander</forename><surname>Distergoft</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Arjan</forename><surname>Kuijper</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Mukhopadhyay</surname></persName>
		</author>
		<ptr target="MLCN/DLF/iMIMIC@MICCAI" />
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Exploratory Adversarial Attacks on Graph Neural Networks</title>
		<author>
			<persName><forename type="first">C</forename><surname>Xixun Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jia</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Haibo</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yanan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bin</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">20th IEEE International Conference on Data Mining (ICDM &apos;20)</title>
				<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="791" to="800" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">A Unified View on Graph Neural Networks as Graph Signal Denoising</title>
		<author>
			<persName><forename type="first">Yao</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaorui</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tong</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yozen</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiliang</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Neil</forename><surname>Shah</surname></persName>
		</author>
		<idno>ArXiv abs/2010.01777</idno>
		<imprint>
			<date type="published" when="2020">2020. 2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Graph Adversarial Attack via Rewiring</title>
		<author>
			<persName><forename type="first">Yao</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Suhang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tyler</forename><surname>Derr</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lingfei</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiliang</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 27th ACM SIGKDD International Conference on Knowledge Discovery &amp; Data Mining (KDD &apos;21)</title>
				<meeting>the 27th ACM SIGKDD International Conference on Knowledge Discovery &amp; Data Mining (KDD &apos;21)</meeting>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="1161" to="1169" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Visualizing Data using t-SNE</title>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">V D</forename><surname>Maaten</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Geoffrey</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="page" from="2579" to="2605" />
			<date type="published" when="2008">2008. 2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">DeepInf: Social Influence Prediction with Deep Learning</title>
		<author>
			<persName><forename type="first">Jiezhong</forename><surname>Qiu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jian</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hao</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuxiao</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kuansan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jie</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 24th ACM SIGKDD International Conference on Knowledge Discovery &amp; Data Mining (KDD &apos;18)</title>
				<meeting>the 24th ACM SIGKDD International Conference on Knowledge Discovery &amp; Data Mining (KDD &apos;18)</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="2110" to="2119" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Struc2vec: Learning Node Representations from Structural Identity</title>
		<author>
			<persName><forename type="first">F</forename><forename type="middle">R</forename><surname>Leonardo</surname></persName>
		</author>
		<author>
			<persName><surname>Ribeiro</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">P</forename><surname>Pedro</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Daniel</forename><forename type="middle">R</forename><surname>Saverese</surname></persName>
		</author>
		<author>
			<persName><surname>Figueiredo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 23rd ACM SIGKDD International Conference on Knowledge Discovery &amp; Data Mining (KDD &apos;17)</title>
				<meeting>the 23rd ACM SIGKDD International Conference on Knowledge Discovery &amp; Data Mining (KDD &apos;17)</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="385" to="394" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">A persistent weisfeiler-lehman procedure for graph classification</title>
		<author>
			<persName><forename type="first">Bastian</forename><surname>Rieck</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christian</forename><surname>Bock</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Karsten</forename><surname>Borgwardt</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 36th International Conference on Machine Learning (ICML &apos;19)</title>
				<meeting>the 36th International Conference on Machine Learning (ICML &apos;19)</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="5448" to="5458" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">One Pixel Attack for Fooling Deep Neural Networks</title>
		<author>
			<persName><forename type="first">Jiawei</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Danilo</forename><forename type="middle">Vasconcellos</forename><surname>Vargas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Sakurai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Evolutionary Computation</title>
		<imprint>
			<biblScope unit="volume">23</biblScope>
			<biblScope unit="page" from="828" to="841" />
			<date type="published" when="2019">2019. 2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<title level="m" type="main">Adversarial Attack and Defense on Graph Data: A Survey</title>
		<author>
			<persName><forename type="first">Lichao</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ji</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Philip</forename><forename type="middle">S</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bo</forename><surname>Li</surname></persName>
		</author>
		<idno>ArXiv abs/1812.10528</idno>
		<imprint>
			<date type="published" when="2018">2018. 2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Adversarial Attacks on Graph Neural Networks via Node Injections: A Hierarchical Reinforcement Learning Approach</title>
		<author>
			<persName><forename type="first">Yiwei</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Suhang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xian-Feng</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tsung-Yu</forename><surname>Hsieh</surname></persName>
		</author>
		<author>
			<persName><surname>Vasant</surname></persName>
		</author>
		<author>
			<persName><surname>Honavar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of The Web Conference</title>
				<meeting>The Web Conference</meeting>
		<imprint>
			<publisher>WWW &apos;20</publisher>
			<date type="published" when="2020">2020. 2020</date>
			<biblScope unit="page" from="673" to="683" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Adversarial Immunization for Certifiable Robustness on Graphs</title>
		<author>
			<persName><forename type="first">Shuchang</forename><surname>Tao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Huawei</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qi</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Liang</forename><surname>Hou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xueqi</forename><surname>Cheng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 14th ACM International Conference on Web Search and Data Mining (WSDM&apos;21</title>
				<meeting>the 14th ACM International Conference on Web Search and Data Mining (WSDM&apos;21</meeting>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="698" to="706" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
		<title level="m" type="main">Understanding the One Pixel Attack: Propagation Maps and Locality Analysis</title>
		<author>
			<persName><forename type="first">Danilo</forename></persName>
		</author>
		<author>
			<persName><forename type="first">Vasconcellos</forename><surname>Vargas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiawei</forename><surname>Su</surname></persName>
		</author>
		<idno>ArXiv abs/1902.02947</idno>
		<imprint>
			<date type="published" when="2020">2020. 2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Graph Attention Networks</title>
		<author>
			<persName><forename type="first">Petar</forename><surname>Veličković</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Guillem</forename><surname>Cucurull</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Arantxa</forename><surname>Casanova</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Adriana</forename><surname>Romero</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pietro</forename><surname>Liò</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations (ICLR &apos;18)</title>
				<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Attacking Graph-Based Classification via Manipulating the Graph Structure</title>
		<author>
			<persName><forename type="first">Binghui</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Neil</forename><surname>Zhenqiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gong</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2019 ACM SIGSAC Conference on Computer and Communications Security (CCS &apos;19</title>
				<meeting>the 2019 ACM SIGSAC Conference on Computer and Communications Security (CCS &apos;19</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="2023" to="2040" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<monogr>
		<title level="m" type="main">Scalable Attack on Graph Data by Injecting Vicious Nodes</title>
		<author>
			<persName><forename type="first">Jihong</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Minnan</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fnu</forename><surname>Suya</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jundong</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zijiang</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qinghua</forename><surname>Zheng</surname></persName>
		</author>
		<idno>ArXiv abs/2004.13825</idno>
		<imprint>
			<date type="published" when="2020">2020. 2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">AM-GCN: Adaptive Multi-Channel Graph Convolutional Networks</title>
		<author>
			<persName><forename type="first">Xiao</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Meiqi</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Deyu</forename><surname>Bo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Peng</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chuan</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jian</forename><surname>Pei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 26th ACM SIGKDD International Conference on Knowledge Discovery &amp; Data Mining (KDD &apos;20)</title>
				<meeting>the 26th ACM SIGKDD International Conference on Knowledge Discovery &amp; Data Mining (KDD &apos;20)</meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="1243" to="1253" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Graph convolutional networks using heat kernel for semi-supervised learning</title>
		<author>
			<persName><forename type="first">Bingbing</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Huawei</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qi</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Keting</forename><surname>Cen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xueqi</forename><surname>Cheng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 28th International Joint Conference on Artificial Intelligence (IJCAI &apos;19)</title>
				<meeting>the 28th International Joint Conference on Artificial Intelligence (IJCAI &apos;19)</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="1928" to="1934" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Graph Wavelet Neural Network</title>
		<author>
			<persName><forename type="first">Bingbing</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Huawei</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qi</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yunqi</forename><surname>Qiu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xueqi</forename><surname>Cheng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations (ICLR &apos;19)</title>
				<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Adversarial Attacks and Defenses in Images, Graphs and Text: A Review</title>
		<author>
			<persName><forename type="first">Han</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yao</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Haochen</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Debayan</forename><surname>Deb</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hui</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiliang</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Anil</forename><forename type="middle">K</forename><surname>Jain</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Int. J. Autom. Comput</title>
		<imprint>
			<biblScope unit="volume">17</biblScope>
			<biblScope unit="page" from="151" to="178" />
			<date type="published" when="2020">2020. 2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">How Powerful are Graph Neural Networks</title>
		<author>
			<persName><forename type="first">Keyulu</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Weihua</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Leskovec</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Jegelka</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations (ICLR &apos;19)</title>
				<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Identityaware Graph Neural Networks</title>
		<author>
			<persName><forename type="first">Jiaxuan</forename><surname>You</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jonathan</forename><surname>Gomes-Selman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rex</forename><surname>Ying</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jure</forename><surname>Leskovec</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 33nd AAAI Conference on Artificial Intelligence (AAAI &apos;21)</title>
				<meeting>the 33nd AAAI Conference on Artificial Intelligence (AAAI &apos;21)</meeting>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">GraphSAINT: Graph Sampling Based Inductive Learning Method</title>
		<author>
			<persName><forename type="first">Hanqing</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hongkuan</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ajitesh</forename><surname>Srivastava</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rajgopal</forename><surname>Kannan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Viktor</forename><surname>Prasanna</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations (ICLR &apos;20)</title>
				<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Adversarial Label-Flipping Attack and Defense for Graph Neural Networks</title>
		<author>
			<persName><forename type="first">Mengmei</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Linmei</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chuan</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiao</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">20th IEEE International Conference on Data Mining (ICDM &apos;20</title>
				<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="791" to="800" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Comparing and detecting adversarial attacks for graph deep learning</title>
		<author>
			<persName><forename type="first">Yingxue</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mark</forename><surname>Khan</surname></persName>
		</author>
		<author>
			<persName><surname>Coates</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Representation Learning on Graphs and Manifolds Workshop, Int. Conf. Learning Representations</title>
				<meeting>Representation Learning on Graphs and Manifolds Workshop, Int. Conf. Learning Representations<address><addrLine>New Orleans, LA, USA</addrLine></address></meeting>
		<imprint/>
	</monogr>
	<note>RLGM @ ICLR &apos;19</note>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Adversarial attacks on neural networks for graph data</title>
		<author>
			<persName><forename type="first">Daniel</forename><surname>Zügner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Amir</forename><surname>Akbarnejad</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stephan</forename><surname>Günnemann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 24th ACM SIGKDD International Conference on Knowledge Discovery &amp; Data Mining (KDD &apos;18)</title>
				<meeting>the 24th ACM SIGKDD International Conference on Knowledge Discovery &amp; Data Mining (KDD &apos;18)</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="2847" to="2856" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Adversarial Attacks on Graph Neural Networks via Meta Learning</title>
		<author>
			<persName><forename type="first">Daniel</forename><surname>Zügner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stephan</forename><surname>Günnemann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations (ICLR &apos;19)</title>
				<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Certifiable Robustness and Robust Training for Graph Convolutional Networks</title>
		<author>
			<persName><forename type="first">Daniel</forename><surname>Zügner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stephan</forename><surname>Günnemann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 25th ACM SIGKDD International Conference on Knowledge Discovery &amp; Data Mining (KDD &apos;19</title>
				<meeting>the 25th ACM SIGKDD International Conference on Knowledge Discovery &amp; Data Mining (KDD &apos;19</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="246" to="256" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
