<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Seed-Guided Topic Discovery with Out-of-Vocabulary Seeds</title>
				<funder ref="#_3jcuQuQ">
					<orgName type="full">INCAS</orgName>
				</funder>
				<funder ref="#_39bjUAN #_mV4W2Ff">
					<orgName type="full">US DARPA KAIROS</orgName>
				</funder>
				<funder ref="#_cad2E2g">
					<orgName type="full">IIS</orgName>
				</funder>
				<funder ref="#_qZVSgkY #_Ahpv4fZ #_qJYpfpQ #_quav9zV">
					<orgName type="full">National Science Foundation</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Yu</forename><surname>Zhang</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">University of Illinois at Urbana-Champaign</orgName>
								<address>
									<region>IL</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Yu</forename><surname>Meng</surname></persName>
							<email>yumeng5@illinois.edu</email>
							<affiliation key="aff0">
								<orgName type="institution">University of Illinois at Urbana-Champaign</orgName>
								<address>
									<region>IL</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Xuan</forename><surname>Wang</surname></persName>
							<email>swang@cs.washington.edu</email>
							<affiliation key="aff0">
								<orgName type="institution">University of Illinois at Urbana-Champaign</orgName>
								<address>
									<region>IL</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Sheng</forename><surname>Wang</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution">University of Washington</orgName>
								<address>
									<settlement>Seattle</settlement>
									<region>WA</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Jiawei</forename><surname>Han</surname></persName>
							<email>hanj@illinois.edu</email>
							<affiliation key="aff0">
								<orgName type="institution">University of Illinois at Urbana-Champaign</orgName>
								<address>
									<region>IL</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Seed-Guided Topic Discovery with Out-of-Vocabulary Seeds</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-01-03T08:32+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Discovering latent topics from text corpora has been studied for decades. Many existing topic models adopt a fully unsupervised setting, and their discovered topics may not cater to users' particular interests due to their inability of leveraging user guidance. Although there exist seed-guided topic discovery approaches that leverage user-provided seeds to discover topicrepresentative terms, they are less concerned with two factors: (1) the existence of out-ofvocabulary seeds and (2) the power of pretrained language models (PLMs). In this paper, we generalize the task of seed-guided topic discovery to allow out-of-vocabulary seeds. We propose a novel framework, named SEE-TOPIC, wherein the general knowledge of PLMs and the local semantics learned from the input corpus can mutually benefit each other. Experiments on three real datasets from different domains demonstrate the effectiveness of SEETOPIC in terms of topic coherence, accuracy, and diversity. 1</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Automatically discovering informative and coherent topics from massive text corpora is central to text analysis through helping users efficiently digest a large collection of documents <ref type="bibr" target="#b11">(Griffiths and Steyvers, 2004</ref>) and advancing downstream applications such as summarization <ref type="bibr" target="#b35">(Wang et al., 2009</ref><ref type="bibr" target="#b36">(Wang et al., , 2022))</ref>, classification <ref type="bibr" target="#b3">(Chen et al., 2015;</ref><ref type="bibr">Meng et al., 2020b)</ref>, and generation <ref type="bibr" target="#b19">(Liu et al., 2021)</ref>.</p><p>Unsupervised topic models have been the mainstream approach to topic discovery since the proposal of pLSA <ref type="bibr" target="#b12">(Hofmann, 1999)</ref> and LDA <ref type="bibr" target="#b2">(Blei et al., 2003)</ref>. Despite their encouraging performance in finding informative latent topics, these topics may not reflect user preferences well, mainly due to their unsupervised nature. For example, given a collection of product reviews, a user may be specifically interested in product categories Table <ref type="table">1</ref>: Three datasets <ref type="bibr" target="#b4">(Cohan et al., 2020;</ref><ref type="bibr" target="#b21">McAuley and Leskovec, 2013;</ref><ref type="bibr" target="#b40">Zhang et al., 2017)</ref> from different domains and their topic categories (i.e., seeds). Red: Seeds never seen in the corpus (i.e., out-of-vocabulary). In all three datasets, a large proportion of seeds are outof-vocabulary. (e.g., "books", "electronics"), but unsupervised topic models may generate topics containing different sentiments (e.g., "good", "bad"). To consider users' interests and needs, seed-guided topic discovery approaches <ref type="bibr" target="#b13">(Jagarlamudi et al., 2012;</ref><ref type="bibr" target="#b10">Gallagher et al., 2017;</ref><ref type="bibr">Meng et al., 2020a)</ref> have been proposed to find representative terms for each category based on user-provided seeds or category names.<ref type="foot" target="#foot_1">2</ref> However, there are still two less concerned factors in these approaches.</p><p>The Existence of Out-of-Vocabulary Seeds. Previous studies <ref type="bibr" target="#b13">(Jagarlamudi et al., 2012;</ref><ref type="bibr" target="#b10">Gallagher et al., 2017;</ref><ref type="bibr">Meng et al., 2020a)</ref> assume that all user-provided seeds must be in-vocabulary (i.e., appear at least once in the input corpus), so that they can utilize the occurrence statistics or Skip-Gram embedding methods <ref type="bibr" target="#b24">(Mikolov et al., 2013)</ref> to model seed semantics. However, user-interested categories can have specific or composite descriptions, which may never appear in the corpus. Table <ref type="table">1</ref> shows three datasets from different domains: sci-entific papers, product reviews, and social media posts. In each dataset, documents can belong to one or more categories, and we list the category names provided by the dataset collectors. These seeds should reflect their particular interests. In all three datasets, we have a large proportion of seeds (45% in SciDocs, 60% in Amazon, and 78% in Twitter) that never appear in the corpus. Some category names are too specific (e.g., "chronic respiratory diseases", "nightlife spot") to be exactly matched, others are the composition of multiple entities (e.g., "hepatitis a/b/c/e", "neoplasms (cancer)", "clothing, shoes and jewelry"). 3   The Power of Pre-trained Language Models. Techniques used in previous studies are mainly based on LDA variants <ref type="bibr" target="#b13">(Jagarlamudi et al., 2012)</ref> or context-free embeddings <ref type="bibr">(Meng et al., 2020a)</ref>. Recently, pre-trained language models (PLMs) such as BERT <ref type="bibr" target="#b7">(Devlin et al., 2019)</ref> have achieved significant improvement in a wide range of text mining tasks. In topic discovery, the generic representation power of PLMs learned from web-scale corpora (e.g., Wikipedia or PubMed) may complement the information a model can obtain from the input corpus. Moreover, out-of-vocabulary seeds usually have meaningful in-vocabulary components (e.g., "night" and "life" in "nightlife spot", "health" and "care" in "health and personal care"). The optimized tokenization strategy of PLMs <ref type="bibr" target="#b30">(Sennrich et al., 2016;</ref><ref type="bibr" target="#b37">Wu et al., 2016)</ref> can help segment the seeds into such meaningful components (e.g., "nightlife" ? "night" and "##life"), and the contextualization power of PLMs can help infer the correct meaning of each component (e.g., "##life" and "care") in the category name. Therefore, PLMs are much needed in handling out-of-vocabulary seeds and effectively learning their semantics.</p><p>Contributions. Being aware of these two factors, in this paper, we study seed-guided topic discovery in the presence of out-of-vocabulary seeds. Our proposed SEETOPIC framework consists of two modules: (1) The general representation module 3 One possible idea to deal with composite seeds is to split them into multiple seeds. However, there are many possible ways to express the conjunctions (e.g., "/ ", "()", "," and "and" in Table <ref type="table">1</ref>), which may require manual tuning. Besides, simple chunking rules will induce splits that break the semantics of the original composition (e.g., "professional and other places" may be split into "professional" and "other places"). Moreover, even after the split, some seeds are still out-ofvocabulary. Therefore, we propose to use PLMs to tackle out-of-vocabulary seeds in a unified way. In experiments, we will show that our model is able to tackle composite seeds. For example, given the seed "hepatitis a/b/c/e", we can find terms relevant to "hepatitis b" and "hepatitis c" (see Table <ref type="table">4</ref>). uses a PLM to derive the representation of each term (including out-of-vocabulary seeds) based on the general linguistic knowledge acquired through pre-training. (2) The seed-guided local representation module learns in-vocabulary term embeddings specific to the input corpus and the given seeds. In order to optimize the learned representations for topic coherence, which is commonly reflected by pointwise mutual information (PMI) <ref type="bibr" target="#b26">(Newman et al., 2010)</ref>, our objective implicitly maximizes the PMI between each word and its context, the documents it appears, as well as the category it belongs to. The learning of the two modules is connected through an iterative ensemble ranking process, in which the general knowledge of PLMs and the term representations specifically learned from the target corpus conditioned on the seeds can complement each other.</p><p>To summarize, this study makes three contributions. ( <ref type="formula" target="#formula_1">1</ref>) Task: we propose to study seedguided topic discovery in the presence of out-ofvocabulary seeds. (2) Framework: we design a unified framework that jointly models general knowledge through PLMs and local corpus statistics through embedding learning. (3) Experiment: extensive experiments on three datasets demonstrate the effectiveness of SEETOPIC in terms of topic coherence, accuracy, and diversity.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Problem Definition</head><p>As shown in Table <ref type="table">1</ref>, we assume a seed can be either a single word or a phrase. Given a corpus D, we use V D to denote the set of terms appearing in D. In accordance with the assumption of category names, each term can also be a single word or a phrase. In practice, given a raw corpus, one can use existing phrase chunking tools <ref type="bibr" target="#b20">(Manning et al., 2014;</ref><ref type="bibr" target="#b31">Shang et al., 2018)</ref> to detect phrases in it. After phrase chunking, if a category name is still not in V D , we define it as out-of-vocabulary. Problem Definition. Given a corpus D = {d 1 , ..., d |D| } and a set of category names C = {c 1 , ..., c |C| } where some category names are outof-vocabulary, the task is to find a set of invocabulary terms S i = {w 1 , ..., w S } ? V D for each category c i such that each term in S i is semantically close to c i and far from other categories c j (?j = i).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">The SEETOPIC Framework</head><p>In this section, we first introduce how we model general and local text semantics using a PLM mod-ule and a seed-guided embedding learning module, respectively. Then, we present the iterative ensemble ranking process and our overall framework.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Modeling General Text Semantics using a PLM</head><p>PLMs such as BERT <ref type="bibr" target="#b7">(Devlin et al., 2019)</ref> aim to learn generic language representations from webscale corpora (e.g., Wikipedia or PubMed) that can be applied to a wide variety of text-related applications. To transfer such general knowledge to our topic discovery task, we employ a PLM to encode each category name and each in-vocabulary term to a vector. To be specific, given a term w ? C ? V D , we input the sequence "[CLS] w [SEP]" into the PLM. Here, w can be a phrase containing multiple words, and each word can be out of the PLM's vocabulary. To deal with this, most PLMs use a pre-trained tokenizer <ref type="bibr" target="#b30">(Sennrich et al., 2016;</ref><ref type="bibr" target="#b37">Wu et al., 2016)</ref> to segment each unseen word into frequent subwords. Then, the contextualization power of PLMs will help infer the correct meaning of each word/subword, so as to provide a more precise representation of the whole category.</p><p>After LM encoding, following <ref type="bibr" target="#b32">(Sia et al., 2020;</ref><ref type="bibr" target="#b34">Thompson and Mimno, 2020;</ref><ref type="bibr" target="#b17">Li et al., 2020)</ref>, we take the output of all tokens from the last layer and average them to get the term embedding e w . In this way, even if a seed c i is out-of-vocabulary, we can still obtain its representation e c i .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Modeling Local Text Semantics in the Input Corpus</head><p>The motivation of topic discovery is to discover latent topic structures from the input corpus. Therefore, purely relying on general knowledge in the PLM is insufficient because topic discovery results should adapt to the input corpus D. Now, we introduce how we learn another set of embeddings</p><formula xml:id="formula_0">{u w |w ? V D } from D.</formula><p>Previous studies on embedding learning assume that the semantic of a term is similar to its local context <ref type="bibr" target="#b24">(Mikolov et al., 2013)</ref>, the document it appears <ref type="bibr" target="#b33">(Tang et al., 2015;</ref><ref type="bibr">Xun et al., 2017a)</ref>, and the category it belongs to <ref type="bibr">(Meng et al., 2020a)</ref>. Inspired by these studies, we propose the following embedding learning objective.</p><formula xml:id="formula_1">J = d?D w i ?d w j ?C(w i ,h) p(wj|wi) context + d?D w?d p(d|w) document + c i ?C w?S i p(ci|w) category ,<label>(1)</label></formula><p>where</p><formula xml:id="formula_2">p(z|w) = exp(u T w vz) z exp(u T w v z )</formula><p>, (z can be wj, d, or ci).</p><p>(2)</p><p>In this objective, u w i (and v w j ), v d , v c i are the embedding vectors of terms, documents, and categories, respectively.</p><formula xml:id="formula_3">C(w i , h) is the set of context terms of w i in d. Specifically, if d = w 1 w 2 ...w L , then C(w i , h) = {w j |i -h ? j ? i + h, j = i},</formula><p>where h is the context window size.</p><p>Note that the last term in Eq. ( <ref type="formula" target="#formula_1">1</ref>) encourages the similarity between each category c i and its representative terms S i . Here, we adopt an iterative process to gradually update category-representative terms. Initially, S i consists of just a few invocabulary terms similar to c i according to the PLM. At each iteration, the size of S i will increase to contain more category-discriminative terms (the selection criterion of these terms will be introduced in the next section), and we need to encourage their proximity with c i in the next iteration.</p><p>Directly optimizing the full softmax in Eq. ( <ref type="formula">2</ref>) is costly. Therefore, we adopt the negative sampling strategy <ref type="bibr" target="#b24">(Mikolov et al., 2013)</ref> for efficient approximation.</p><p>Interpreting the Objective. In topic modeling studies, pointwise mutual information (PMI) <ref type="bibr" target="#b26">(Newman et al., 2010</ref>) is a standard evaluation metric for topic coherence <ref type="bibr" target="#b14">(Lau et al., 2014;</ref><ref type="bibr" target="#b29">R?der et al., 2015)</ref>. <ref type="bibr" target="#b16">Levy and Goldberg (2014)</ref> prove that the Skip-Gram embedding model is implicitly factorizing the PMI matrix. Following their proof, we can show that maximizing Eq. ( <ref type="formula" target="#formula_1">1</ref>) is implicitly doing the following factorization:</p><formula xml:id="formula_4">U T w [Vw; V d ; Vc] = [Xww; X wd ; Xwc],<label>(3)</label></formula><p>where the columns of</p><formula xml:id="formula_5">U w , V w , V d , V c are u w i , v w j , v d , v c i , respectively (w i , w j ? V D , d ? D, c i ? C);</formula><p>X ww , X wd , and X wc are PMI matrices.</p><formula xml:id="formula_6">Xww = log #D(wi, wj) ? ?D #D(wi) ? #D(wj) ? b w i ,w j ?V D , X wd = log # d (w) ? ?D #D(w) ? ? d ? b w?V D , d?D , Xwc = xw,c i w?V D , c i ?C , where xw,c i = log |C| b , if w ? Si, -?, if w ? Sj (?j = i).<label>(4)</label></formula><p>Here, # D (w i , w j ) denotes the number of cooccurrences of w i and w j in a context window in D; # D (w) denotes the number of occurrences of w in D; ? D is the total number of terms in D; # d (w) denotes the number of times w occurs in d; ? d is the total number of terms in d; b is the number of negative samples. (For the derivation of Eq. ( <ref type="formula" target="#formula_4">3</ref>), please refer to Appendix A.) To summarize, the learned local representations u w are implicitly optimized for topic coherence, where term co-occurrences are measured in context, document, and category levels.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Ensemble Ranking</head><p>We have obtained two sets of term embeddings that model text semantics from different angles: {e w |w ? C ? V D } carries the PLM's knowledge, while {u w |w ? V D } models the input corpus as well as user-provided seeds. We now propose an ensemble ranking method to leverage information from both sides to grab more discriminative terms for each category.</p><p>Given a category c i and its current term set S i , we first calculate the scores of each term w ? V D .</p><formula xml:id="formula_7">scoreG(w|Si) = 1 |Si| w ?S i cos(ew, e w ), scoreL(w|Si) = 1 |Si| w ?S i cos(uw, u w ).</formula><p>(5)</p><p>The subscript "G" here means "general", while "L" means "local". Then, we sort all terms by these two scores, respectively. Each term w will hence get two rank positions rank G (w) and rank L (w). We propose the following ensemble score based on the reciprocal rank:</p><formula xml:id="formula_8">score(w|Si) = 1 2 1 rankG(w) ? + 1 2 1 rankL(w) ? 1/? . (6)</formula><p>Here, 0 &lt; ? ? 1 is a constant. In practice, instead of ranking all terms in the vocabulary, we only check the top-M results in the two ranking lists. If a term w is not among the top-M according to score G (w) (resp., score L (w)), we set rank G (w) = +? (resp., rank L (w) = +?). In fact, when ? = 1, Eq. ( <ref type="formula">6</ref>) becomes the arithmetic mean of the two reciprocal ranks w) . This is essentially the mean reciprocal rank (MRR) commonly used in ensemble ranking, where a high position in one ranking list can largely compensate a low position in the other. In contrast, when ? ? 0, Eq. ( <ref type="formula">6</ref>) becomes the geometric mean of the two reciprocal ranks (see Appendix B), where two ranking lists both have the "veto power" (i.e., a term needs to be ranked as top-M in both ranking lists to obtain a non-zero ensemble score). In experiment, we set ? = 0.1 and show it outperforms MRR (i.e., ? = 1) in our topic discovery task.</p><formula xml:id="formula_9">1 rank G (w) and 1 rank L (</formula><p>After computing the ensemble score score(w|S i ) for each w, we update S i . To guarantee that each S i is category-discriminative, we do not allow any term to belong to more than one category. Therefore, we gradually expand each S i by turns. At the beginning, we reset S 1 = ... = S |C| = ?. When it is S i 's turn, we add one term S i according to the following criterion:</p><formula xml:id="formula_10">Si ? Si ? { arg max w?V D \(S 1 ?...?S |C| )</formula><p>score(w|Si)}. (7)</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Overall Framework</head><p>We summarize the entire SEETOPIC framework in Algorithm 1. To deal with out-of-vocabulary category names, we first utilize a PLM to find their nearest in-vocabulary terms as the initial categorydiscriminative term set S i (Lines 1-7). After initialization,</p><formula xml:id="formula_11">|S i | = N (?1 ? i ? |C|).</formula><p>Note that for an in-vocabulary category name c i ? V D , itself will be added to the initial S i as the top-1 similar in-vocabulary term. After getting the initial S i , we update it by T iterations (Lines 8-16). At each iteration, according to the up-to-date S 1 , S 2 , ..., S |C| , we relearn embeddings u w , v w , v d , and v c i using Eq. (1) (Line 10). The two set of embeddings, {e w |w ? C ? V D } (computed at Line 1) and {u w |w ? V D } (updated at Line 10), are then leveraged to perform ensemble ranking (Lines 11-12). Based on the ensemble score score(w|S i ), we update S i using Eq. ( <ref type="formula">7</ref>) (Lines 13-16). After the t-th iteration, Amazon review belongs to one or more product categories. We use the subset sampled by <ref type="bibr" target="#b42">Zhang et al. (2020</ref><ref type="bibr" target="#b41">Zhang et al. ( , 2022))</ref>, which contains 10 categories and 100K reviews. (3) Twitter <ref type="bibr" target="#b40">(Zhang et al., 2017)</ref> 6 is a crawl of geo-tagged tweets in New York City from August 2014 to November 2014. The dataset collectors link these tweets with Foursquare's POI database and assign them to 9 POI categories. We take these category names as input seeds.</p><p>Seeds used in the three datasets are shown in Table <ref type="table">1</ref>. Dataset statistics are summarized in Table 2. For all three datasets, we use AutoPhrase <ref type="bibr" target="#b31">(Shang et al., 2018)</ref> 7 to perform phrase chunking in the corpus, and we remove words and phrases occurring less than 3 times.</p><p>Previous studies <ref type="bibr" target="#b13">(Jagarlamudi et al., 2012;</ref><ref type="bibr">Meng et al., 2020a)</ref> have tried some other datasets (e.g., RCV1, 20 Newsgroups, NYT, and Yelp). However, the category names they use in these datasets are 4 https://github.com/allenai/scidocs 5 http://jmcauley.ucsd.edu/data/amazon/index.html 6 https://github.com/franticnerd/geoburst 7 https://github.com/shangjingbo1226/AutoPhrase all picked from in-vocabulary terms. Therefore, we do not consider these datasets for evaluation in our task settings. Following <ref type="bibr" target="#b32">(Sia et al., 2020)</ref>, we adopt a 60-40 train-test split for all three datasets. The training set is used as the input corpus D, and the testing set is used for calculating topic coherence metrics (see evaluation metrics for details).</p><p>Compared Methods. We compare our SEETOPIC framework with the following methods, including seed-guided topic modeling methods, seedguided embedding learning methods, and PLMs.</p><p>(1) SeededLDA <ref type="bibr" target="#b13">(Jagarlamudi et al., 2012)</ref> 8 is a seed-guided topic modeling method. It improves LDA by biasing topics to produce input seeds and by biasing documents to select topics relevant to the seeds they contain. (2) Anchored CorEx <ref type="bibr" target="#b10">(Gallagher et al., 2017)</ref> 9 is a seed-guided topic modeling method. It incorporates user-provided seeds by balancing between compressing the input corpus and preserving seed-related information. (3) Labeled ETM (Dieng et al., 2020) 10 is an embedding-based topic modeling method. It incorporates distributed representation of each term. Following <ref type="bibr">(Meng et al., 2020a)</ref>, we retrieve representative terms according to their embedding similarity with the category name. (4) CatE <ref type="bibr">(Meng et al., 2020a)</ref> 11 is a seed-guided embedding learning method for discriminative topic discovery. It takes category names as input and jointly learns term embedding and specificity from the input corpus. Category-discriminative terms are then selected based on both embedding similarity with the category and specificity. ( <ref type="formula">5</ref>) BERT <ref type="bibr" target="#b7">(Devlin et al., 2019)</ref> 12 is a PLM. Following Lines 1-7 in Algorithm 1, we use BERT to encode each input category name and each term to a vector, and then perform similarity search to directly find all repre- Since BioBERT is specifically trained for biomedical text mining tasks, we report its performance on the SciDocs dataset only. ( <ref type="formula">7</ref>) SEETOPIC-NoIter is a variant of our SEETOPIC framework. In Algorithm 1, after initialization (Lines 1-7), it executes Lines 9-16 only once (i.e., T = 1) to find all representative terms.</p><p>Here, all seed-guided topic modeling and embedding baselines (i.e., SeededLDA, Anchored CorEx, CatE, and Labeled ETM) can only take in-vocabulary seeds as input. For a fair comparison, we run Lines 1-7 in Algorithm 1 to get the initial representative in-vocabulary terms for each category, and input these terms as seeds into the baselines. In other words, all compared methods use BERT/BioBERT to initialize their term sets.</p><p>Evaluation Metrics. We evaluate topic discovery results from three different angles: topic coherence, term accuracy, and topic diversity.</p><p>(1) NPMI <ref type="bibr" target="#b14">(Lau et al., 2014</ref>) is a standard metric in topic modeling to measure topic coherence. Within each topic, it calculates the normalized pointwise mutual information for each pair of terms in S i .</p><formula xml:id="formula_12">NPMI = 1 |C| |C| i=1 1 |S i | 2 w j ,w k ?S i log P (w j ,w k ) P (w j )P (w k ) -log P (wj, w k ) ,<label>(8)</label></formula><p>where P (w j , w k ) is the probability that w j and w k co-occur in a document; P (w j ) is the marginal probability of w j . 14</p><p>(2) LCP <ref type="bibr" target="#b25">(Mimno et al., 2011)</ref> is another standard metric to measure topic coherence. It calculates the pairwise log conditional probability of top-ranked 13 https://huggingface.co/dmis-lab/biobert-v1.1</p><p>14 When calculating Eqs. ( <ref type="formula" target="#formula_12">8</ref>) and ( <ref type="formula" target="#formula_13">9</ref>), to avoid log 0, we use P (wj, w k ) + and P (w) + to replace P (wj, w k ) and P (w), respectively, where = 1/|D|. terms.</p><formula xml:id="formula_13">LCP = 1 |C| |C| i=1 1 |S i | 2 w j ,w k ?S i j&lt;k log P (wj, w k ) P (wj) . (<label>9</label></formula><formula xml:id="formula_14">)</formula><p>Note that PMI <ref type="bibr" target="#b26">(Newman et al., 2010)</ref> is also a standard metric for topic coherence. We do observe that SEETOPIC outperforms baselines in terms of PMI in most cases. However, since our local embedding step is implicitly optimizing a PMI-like objective, we no longer use it as our evaluation metric.</p><p>(3) MACC <ref type="bibr">(Meng et al., 2020a)</ref> measures term accuracy. It is defined as the proportion of retrieved terms that actually belong to the corresponding category according to the category name.</p><formula xml:id="formula_15">MACC = 1 |C| |C| i=1 1 |Si| w j ?S i 1(wj ? ci),<label>(10)</label></formula><p>where 1(w j ? c i ) is the indicator function of whether w j is relevant to category c i . MACC requires human evaluation, so we invite five annotators to perform independent annotation. The reported MACC score is the average MACC of the five annotators. A high inter-annotator agreement is observed, with Fleiss' kappa (Fleiss, 1971) being 0.856, 0.844, and 0.771 on SciDocs, Amazon, and Twitter, respectively.</p><p>(4) Diversity (Dieng et al., 2020) measures the mutual exclusivity of discovered topics. It is the percentage of unique terms in all topics, which corresponds to our task requirement that each retrieved term is discriminatively close to one category and far from the others.</p><formula xml:id="formula_16">Diversity = | |C| i=1 Si| |C| i=1 |Si| . (<label>11</label></formula><formula xml:id="formula_17">)</formula><p>Experiment Settings. We use BioBERT as the PLM on SciDocs, and BERT-base-uncased as the PLM on Amazon and Twitter. The embedding dimension of u w is 768 (the same as e w ); the number of negative samples b = 5. In ensemble ranking, the length of the general/local ranking list M = 100; the hyperparameter ? in Eq. ( <ref type="formula">6</ref>) is set as 0.1; the number of iterations T = 4; after each iteration, we increase the size of S i by N = 3. We use the top-10 ranked terms in each topic for final evaluation (i.e., |S i | = 10 in Eqs. ( <ref type="formula" target="#formula_12">8</ref>)-( <ref type="formula" target="#formula_16">11</ref>)).</p><p>Experiments are run on Intel Xeon E5-2680 v2 @ 2.80GHz and one NVIDIA GeForce GTX 1080.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Performance Comparison</head><p>Table <ref type="table" target="#tab_3">3</ref> shows the performance of all methods. We run each experiment 3 times with the average score reported. To show statistical significance, we conduct a two-tailed unpaired t-test to compare SEE-TOPIC and each baseline. (The performance of BERT and BioBERT is deterministic according to our usage. When comparing SEETOPIC with them, we conduct a two-tailed Z-test instead.) The significance level is also marked in Table <ref type="table" target="#tab_3">3</ref>.</p><p>We have the following observations from Table <ref type="table" target="#tab_3">3</ref>. (1) Our SEETOPIC model performs consistently well. In fact, it achieves the highest score in 8 columns and the second highest in the remaining 4 columns. (2) Classical seed-guided topic modeling baselines (i.e., SeededLDA and Anchored CorEx) perform not well in respect of NPMI (topic coherence) and MACC (term accuracy). Embeddingbased topic discovery approaches (i.e., Labeled ETM and CatE) make some progress, but they still significantly underperform the PLM-empowered SEETOPIC model on SciDocs and Amazon. (3) SEETOPIC consistently performs better than SEE-TOPIC-NoIter on all three datasets, indicating the positive contribution of the proposed iterative process. (4) SEETOPIC guarantees the mutual exclusivity of S 1 , ..., S |C| . In comparison, SeededLDA, Labeled ETM, and BERT cannot guarantee such mutual exclusivity. In-vocabulary vs. Out-of-vocabulary. Figure <ref type="figure" target="#fig_1">1</ref> compares the MACC scores of different seedguided topic discovery methods on in-vocabulary categories and out-of-vocabulary categories. We find that the performance improvement of SEE-TOPIC upon baselines on out-of-vocabulary categories is larger than that on in-vocabulary ones. For example, on Amazon, SEETOPIC underperforms CatE on in-vocabulary categories but outperforms CatE on out-of-vocabulary ones; on Twitter, the gap between SEETOPIC and baselines be- comes much more evident on out-of-vocabulary categories. Note that all baselines in Figure <ref type="figure" target="#fig_1">1</ref> do not utilize the power of PLMs, so this observation validates our claim that PLMs are helpful in tackling out-of-vocabulary seeds.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Parameter Study</head><p>We study the effect of two important hyperparameters: ? (the hyperparameter in ensemble ranking) and T (the number of iterations). We vary the value of ? in {0.1, 0.3, 0.5, 0.7, 0.9, 1} (SEETOPIC uses ? = 0.1 by default) and the value of T in {1, 2, 3, 4, 5} (SEETOPIC uses T = 4 by default, and SEETOPIC-NoIter is the case when T = 1).</p><p>Figure <ref type="figure">2</ref> shows the change of model performance measured by NPMI and LCP.</p><p>Table <ref type="table">4</ref>: Top-5 representative terms retrieved by different algorithms for three out-of-vocabulary categories from SciDocs, Amazon, and Twitter. : at least 3 of the 5 annotators judge the term as relevant to the seed. : at most 2 of the 5 annotators judge the term as relevant to the seed. , in most cases, the performance of SEETOPIC deteriorates as ? increases from 0.1 to 0.9. Thus, setting ? = 0.1 always leads to competitive NPMI and LCP scores on the three datasets. Although ? = 1 is better than ? = 0.9, its performance is still suboptimal in comparison with ? = 0.1. This finding indicates that replacing the mean reciprocal rank (i.e., ? = 1) with our proposed Eq. ( <ref type="formula">6</ref>) is reasonable. According to Figures <ref type="figure">2(c</ref>) and 2(d), SEETOPIC usually performs better when there are more iterations. On SciDocs and Twitter, the scores start to converge after T = 4. Besides, more iterations will result in longer running time. Overall, we believe setting T = 4 strikes a good balance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Case Study</head><p>Finally, we show the terms retrieved by different methods as a case study. From each of the three datasets, we select an out-of-vocabulary category and show its topic discovery results in Table <ref type="table">4</ref>. We mark a retrieved term as correct () if at least 3 of the 5 annotators judge the term as relevant to the seed. Otherwise, we mark the term as incorrect ().</p><p>For the category "hepatitis a/b/c/e" from Sci-Docs, SeededLDA and Anchored CorEx can only find very general medical terms, which are relevant to all seeds in SciDocs and thus inaccurate; Labeled ETM and CatE find terms about "alanine aminotransferase", whose elevation suggest not only hepatitis but also other diseases like diabetes and heart failure, thus not discriminative either; BioBERT and SEETOPIC, with the power of a PLM, can accurately pick terms relevant to "hepatitis b" and "hepatitis c". For the category "sports and outdoors" from Amazon, SeededLDA and Anchored CorEx again find very general terms, most of which are not category-discriminative; Labeled ETM and CatE are able to pick more specific terms such as "cars and tracks", but they still make mistakes; BERT, as a PLM, can accurately find terms that have lexical overlap with the category name (e.g., "outdoorsmen", "sporting events"), meanwhile such terms are less diverse; SEETOPIC-NoIter starts to discover more concrete terms than BERT (e.g., "indoor soccer", "bike riding") by leveraging local text semantics; the full SEETOPIC model, with an iterative updating process, can find more specific and informative terms (e.g., "canoeing", "picnics", and "rafting"). For the category "travel and trans-port" from Twitter, both BERT and CatE make mistakes by including the term "natural history"; SEETOPIC-NoIter, without an iterative update process, also includes this error; the full SEETOPIC model finally excludes this error and achieves the highest accuracy in the retrieved top-5 terms among all compared methods.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Related Work</head><p>Seed-Guided Topic Discovery. Seed-guided topic models aim to leverage user-provided seeds to discover underlying topics according to users' interests. Early studies take LDA <ref type="bibr" target="#b2">(Blei et al., 2003)</ref> as the backbone and incorporate seeds into model learning. For example, <ref type="bibr" target="#b0">Andrzejewski et al. (2009)</ref> consider must-link and cannot-link constraints among seeds as priors. SeededLDA <ref type="bibr" target="#b13">(Jagarlamudi et al., 2012)</ref> encourages topics to contain more seeds and encourages documents to select topics relevant to the seeds they contain. Anchored CorEx <ref type="bibr" target="#b10">(Gallagher et al., 2017)</ref> extracts maximally informative topics by jointly compressing the corpus and preserving seed relevant information. Recent studies start to utilize embedding techniques to learn better word semantics. For example, CatE <ref type="bibr">(Meng et al., 2020a)</ref> explicitly encourages distinction among retrieved topics via category-name guided embedding learning. However, all these models require the provided seeds to be in-vocabulary, mainly because they focus on the input corpus only and are not equipped with general knowledge of PLMs.</p><p>Embedding-Based Topic Discovery. A number of studies extend LDA to involve word embedding. The common strategy is to adapt distributions in LDA to generate real-valued data (e.g., Gaussian LDA <ref type="bibr" target="#b6">(Das et al., 2015)</ref>, LFTM <ref type="bibr" target="#b27">(Nguyen et al., 2015)</ref>, Spherical HDP <ref type="bibr" target="#b1">(Batmanghelich et al., 2016)</ref>, and CGTM <ref type="bibr">(Xun et al., 2017b)</ref>). Some other studies think out of the LDA backbone. For example, TWE <ref type="bibr" target="#b18">(Liu et al., 2015)</ref> uses topic structures to jointly learn topic embeddings and improve word embeddings. CLM <ref type="bibr">(Xun et al., 2017a)</ref> collaboratively improves topic modeling and word embedding by coordinating global and local contexts. ETM (Dieng et al., 2020) models word-topic correlations via word embeddings to improve the expressiveness of topic models. More recently, <ref type="bibr" target="#b32">Sia et al. (2020)</ref> show that directly clustering word embeddings (e.g., word2vec or BERT) also generates good topics; <ref type="bibr" target="#b34">Thompson and Mimno (2020)</ref> further find that BERT and GPT-2 discover high-quality topics, but RoBERTa does not. These models are unsupervised and hard to be applied to seed-guided settings. In contrast, our SEETOPIC framework joint leverages PLMs, word embeddings, and seed information.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Conclusions and Future Work</head><p>In this paper, we study seed-guided topic discovery in the presence of out-of-vocabulary seeds. To understand and make use of in-vocabulary components in each seed, we utilize the tokenization and contextualization power of PLMs. We propose a seed-guided embedding learning framework inspired by the goal of maximizing PMI in topic modeling, and an iterative ensemble ranking process to jointly leverage general knowledge of the PLM and local signals learned from the input corpus. Experimental results show that SEETOPIC outperforms seed-guided topic discovery baselines and PLMs in terms of topic coherence, term accuracy, and topic diversity. A parameter study and a case study further validate some design choices in SEETOPIC.</p><p>In the future, it would be interesting to extend SEETOPIC to seed-guided hierarchical topic discovery, where parent and child information in the input category hierarchy may help infer the meaning of out-of-vocabulary nodes.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A The Embedding Learning Objective</head><p>In Section 3.2, we propose the following embedding learning objective: J = d?D w i ?d w j ?C(w i ,h) exp(u T w i vw j )</p><p>w ?V D exp(u T w i v w )</p><formula xml:id="formula_18">J context + d?D w?d exp(u T w v d ) d ?D exp(u T w v d ) J document + c i ?C w?S i exp(u T w vc i ) c ?C exp(u T w v c ) J category . (<label>12</label></formula><formula xml:id="formula_19">)</formula><p>Now we prove that maximizing J is implicitly performing the factorization in Eq. ( <ref type="formula" target="#formula_4">3</ref>). <ref type="bibr" target="#b16">Levy and Goldberg (2014)</ref> have proved that maximizing J context is implicitly doing the following factorization. </p><p>We follow their approach to consider the other two terms J document and J category in Eq. ( <ref type="formula" target="#formula_18">12</ref>). Using the negative sampling strategy to rewrite J document , we get </p><p>where ?(?) is the sigmoid function. Following <ref type="bibr" target="#b16">(Levy and Goldberg, 2014;</ref><ref type="bibr" target="#b28">Qiu et al., 2018)</ref>, we assume the negative sampling distribution ? ? d . 15  Then, the objective becomes (</p><formula xml:id="formula_22">)<label>15</label></formula><p>For a specific term-document pair (w, d), we consider its effect in the objective: (17) 15 In practice, the negative sampling distribution ? ? 3/4 d , but related studies <ref type="bibr" target="#b16">(Levy and Goldberg, 2014;</ref><ref type="bibr" target="#b28">Qiu et al., 2018)</ref> usually assume a linear relationship in their derivation.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head></head><label></label><figDesc>|S i | = (t + 1)N (?1 ? i ? |C|). Complexity Analysis. The time complexity of using the PLM is O((|C| + |V D |)? PLM ), where ? PLM is the complexity of encoding one term via the PLM. The total complexity of local embedding is O(T ? D (h+|C|)b) because in each iteration 1 ? t ? T , every w ? D interacts with every other term in the context window of size h, its belonging document, and each category c i ? C, and each update involves b negative samples. The total complexity of ensemble ranking is O(T |V D ||C||S i |) as in each iteration 1 ? t ? T , we compute scores between each w ? V D and each w ? S i (?c i ? C). 4 Experiments 4.1 Experimental Setup Datasets. We conduct experiments on three public datasets from different domains: (1) SciDocs (Cohan et al., 2020) 4 is a large collection of scientific papers supporting diverse evaluation tasks. For the MeSH classification task (Coletti and Bleich, 2001), about 23K medical papers are collected, each of which is assigned to one of the 11 common disease categories derived from the MeSH vocabulary. We use the title and abstract of each paper as documents and the 11 category names as seeds. (2) Amazon (McAuley and Leskovec, 2013) 5 contains product reviews from May 1996 to July 2014. Each</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: MACC of seed-guided topic discovery methods on in-vocabulary categories and out-of-vocabulary categories.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head></head><label></label><figDesc>hbv dna (), serum hbv dna (), serum alanine aminotransferase (), alanine aminotransferase alt (), below detection limit () CatE chronic hepatitis b virus hbv infection (), hepatitis b e antigen hbeag (), hepatitis b virus hbv dna (), normal alanine aminotransferase (), hbeag-negative chronic hepatitis b () BioBERT hepatitis b virus hbv dna (), chronic hepatitis b virus hbv infection (), hepatitis b e antigen hbeag (), hepatitis b virus hbv infection (), chronic hepatitis c virus hcv () SEETOPIC-NoIter hepatitis b virus hbv dna (), hepatitis b e antigen hbeag (), chronic hepatitis b virus hbv infection (), hepatitis b surface antigen hbsag (), hbeag-negative chronic hepatitis b () SEETOPIC hepatitis b virus hbv infection (), hbeag-negative chronic hepatitis b (), hepatitis c virus hcv-infected (), hepatitis b virus hbv dna (), chronic hepatitis c virus hcv () Dataset: Amazon, Category Name: sports and outdoors SeededLDA use (), good (), one (), product (), like () Anchored CorEx sports (), use (), size (), wear (), fit () Labeled ETM cars and tracks (), tracks and cars (), search options (), championships (), cool bosses () CatE outdoorsmen (), outdoor activities (), cars and tracks (), foot support (), offers plenty () BERT cars and tracks (), outdoor activities (), outdoorsmen (), sports (), sporting events () SEETOPIC-NoIter outdoorsmen (), outdoor activities (), cars and tracks (), indoor soccer (), bike riding () SEETOPIC canoeing (), picnics (), bike rides (), bike riding (), rafting () Dataset: Twitter, Category Name: travel and transport SeededLDA nyc (), new york (), line (), high (), time square () Anchored CorEx new york (), post photo (), new (), day (), today () Labeled ETM tourism (), theview (), file (), morning view (), gma () CatE maritime (), tourism (), natural history (), scenery (), elevate () BERT maritime (), tourism (), natural history (), olive oil (), baggage claim () SEETOPIC-NoIter maritime (), tourism (), natural history (), scenery (), navy () SEETOPIC wildlife (), scenery (), maritime (), highlinepark (), aquarium () According to Figures 2(a) and 2(b)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head></head><label></label><figDesc>u T w i vw j = log #D(wi, wj) ? ?D #D(wi) ? #D(wj) ? b , i.e., U T w Vw = Xww.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head></head><label></label><figDesc>w?V D d?D # d (w) log ?(u T w v d )+bE d log ?(-u T w v d ) ,</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>J</head><label></label><figDesc>w,d = # d (w) log ?(u T w v d )+#D(w) b ? ? d ?D log ?(-u T w v d ).(16)Let x w,d = u T w v d . To maximize J w,d , we should have0 = ?J w,d ?x w,d = # d (w)?(-x w,d ) -#D(w) ? b ? ? d ?D ?(x w,d ).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head></head><label></label><figDesc>Algorithm 1: SEETOPIC Input: A text corpus D = {d1, ..., d |D| }, a set of seeds C = {c1, ..., c |C| }, and a PLM. Output: (S1, ..., S |C| ), where each Si is a set of category-discriminative terms for ci. 1 Compute {ew|w ? C ? VD} using the PLM;</figDesc><table><row><cell cols="2">2 // Initialize Si;</cell><cell></cell><cell></cell></row><row><cell cols="2">3 S1, ..., S |C| ? ?;</cell><cell></cell><cell></cell></row><row><cell cols="2">4 for n ? 1 to N do</cell><cell></cell><cell></cell></row><row><cell>5</cell><cell cols="2">for i ? 1 to |C| do</cell><cell></cell></row><row><cell>6</cell><cell>wn ?</cell><cell>arg max</cell><cell>cos(ew, ec i );</cell></row><row><cell></cell><cell cols="2">w?V D \(S 1 ?...?S |C| )</cell><cell></cell></row><row><cell>7</cell><cell cols="2">Si ? Si ? {wn};</cell><cell></cell></row><row><cell cols="3">8 // Update Si for T iterations;</cell><cell></cell></row><row><cell cols="2">9 for t ? 1 to T do</cell><cell></cell><cell></cell></row><row><cell>10</cell><cell cols="3">Learn {uw|w ? VD} from the input corpus D</cell></row><row><cell></cell><cell cols="3">and the up-to-date representative terms</cell></row><row><cell></cell><cell cols="3">S1, ..., S |C| according to Eq. (1);</cell></row><row><cell>11</cell><cell cols="3">scoreG(w|Si) and scoreL(w|Si) ? Eq. (5);</cell></row><row><cell>12</cell><cell cols="2">score(w|Si) ? Eq. (6);</cell><cell></cell></row></table><note><p>13 S1, ..., S |C| ? ?; 14 for n ? 1 to (t + 1)N do 15 for i ? 1 to |C| do 16 Si ? Eq. (7); 17 Return (S1, ..., S |C| );</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2 :</head><label>2</label><figDesc>Dataset Statistics.</figDesc><table><row><cell>Dataset</cell><cell>SciDocs</cell><cell>Amazon</cell><cell>Twitter</cell></row><row><cell>#Documents</cell><cell>23,473</cell><cell>100,000</cell><cell>135,529</cell></row><row><cell>#In-vocabulary Terms (After Phrase Chunking)</cell><cell>55,897</cell><cell>56,942</cell><cell>17,577</cell></row><row><cell>Avg Doc Length</cell><cell>239.8</cell><cell>119.0</cell><cell>6.7</cell></row><row><cell>#Seeds</cell><cell>11</cell><cell>10</cell><cell>9</cell></row><row><cell>#Out-of-vocabulary Seeds (After Phrase Chunking)</cell><cell>5</cell><cell>6</cell><cell>7</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 3 :</head><label>3</label><figDesc>NPMI, LCP, MACC, and Diversity of compared algorithms on three datasets. NPMI and LCP measure topic coherence; MACC measures term accuracy; Diversity (abbreviated to Div.) measures topic diversity. Bold: the highest score. Underline: the second highest score. * : significantly worse than SEETOPIC (p-value &lt; 0.05).</figDesc><table><row><cell>Methods</cell><cell>NPMI</cell><cell cols="2">SciDocs LCP MACC</cell><cell>Div.</cell><cell>NPMI</cell><cell cols="2">Amazon LCP MACC</cell><cell>Div.</cell><cell>NPMI</cell><cell cols="2">Twitter LCP MACC</cell><cell>Div.</cell></row><row><cell>SeededLDA</cell><cell>0.056  *  *</cell><cell>-0.616</cell><cell cols="3">0.156  *  *  0.451  *  *  0.070  *  *</cell><cell>-0.753</cell><cell cols="6">0.147  *  *  0.393  *  *  0.013  *  *  -2.254  *  *  0.195  *  *  0.696  *  *</cell></row><row><cell cols="4">Anchored CorEx 0.106  *  *  -1.090  *  *  0.264  *  *</cell><cell>1.000</cell><cell cols="3">0.134  *  *  -0.982  *  0.333  *  *</cell><cell>1.000</cell><cell cols="3">0.090  *  *  -2.192  *  *  0.233  *  *</cell><cell>1.000</cell></row><row><cell>Labeled ETM</cell><cell cols="7">0.334  *  -0.775  *  *  0.458  *  *  0.961  *  0.308  *  *  -1.051  *  *  0.585  *  *</cell><cell>1.000</cell><cell cols="3">0.305  *  -1.098  *  *  0.268  *  *</cell><cell>0.989</cell></row><row><cell>CatE</cell><cell cols="3">0.345  *  -0.725  *  *  0.633  *  *</cell><cell>1.000</cell><cell cols="3">0.317  *  *  -0.844  *  *  0.856  *</cell><cell>1.000</cell><cell>0.356</cell><cell>-0.827</cell><cell>0.483  *  *</cell><cell>1.000</cell></row><row><cell>BERT</cell><cell cols="7">0.313  *  *  -0.841  *  *  0.740  *  *  0.891  *  *  0.294  *  *  -1.093  *  *  0.832  *  *</cell><cell>1.000</cell><cell cols="2">0.313  *  *  -1.044  *  *</cell><cell>0.627</cell><cell>0.944  *  *</cell></row><row><cell>BioBERT</cell><cell cols="2">0.309  *  *  -0.852  *  *</cell><cell>0.938</cell><cell>0.982  *  *</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell cols="3">SEETOPIC-NoIter 0.341  *  *  -0.768  *  *</cell><cell>0.887</cell><cell>1.000</cell><cell cols="2">0.322  *  *  -0.986  *  *</cell><cell>0.892</cell><cell>1.000</cell><cell>0.318</cell><cell>-1.004  *  *</cell><cell>0.618</cell><cell>1.000</cell></row><row><cell>SEETOPIC</cell><cell>0.358</cell><cell>-0.634</cell><cell>0.909</cell><cell>1.000</cell><cell>0.342</cell><cell>-0.696</cell><cell>0.904</cell><cell>1.000</cell><cell>0.320</cell><cell>-0.907</cell><cell>0.633</cell><cell>1.000</cell></row><row><cell cols="6">sentative terms. (6) BioBERT (Lee et al., 2020) 13</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="6">is a PLM. It is used in the same way as BERT.</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note><p>* * : significantly worse than SEETOPIC (p-value &lt; 0.01).</p></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0"><p>The code and datasets are available at https://github.com/yuzhimanhua/SeeTopic.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_1"><p>In this paper, we use "seeds" and "category names" interchangeably.</p></note>
		</body>
		<back>

			<div type="acknowledgement">
<div><head>Acknowledgments</head><p>We thank anonymous reviewers for their valuable and insightful feedback. Research was supported in part by <rs type="funder">US DARPA KAIROS</rs> Program No. <rs type="grantNumber">FA8750-19-2-1004</rs>, SocialSim Program No. <rs type="grantNumber">W911NF-17-C-0099</rs>, and <rs type="funder">INCAS</rs> Program No. <rs type="grantNumber">HR001121C0165</rs>, <rs type="funder">National Science Foundation</rs> <rs type="grantNumber">IIS-19-56151</rs>, <rs type="grantNumber">IIS-17-41317</rs>, and <rs type="funder">IIS</rs> <rs type="grantNumber">17-04532</rs>, and the <rs type="institution">Molecule Maker Lab Institute</rs>: <rs type="programName">An AI Research Institutes program</rs> supported by <rs type="funder">NSF</rs> under Award No. <rs type="grantNumber">2019897</rs>, and the <rs type="institution">Institute for Geospatial Understanding</rs> through an <rs type="grantName">Integrative Discovery Environment (I-GUIDE</rs>) by <rs type="funder">NSF</rs> under Award No. <rs type="grantNumber">2118329</rs>. Any opinions, findings, and conclusions or recommendations expressed herein are those of the authors and do not necessarily represent the views, either expressed or implied, of <rs type="affiliation">DARPA</rs> or the <rs type="institution">U.S. Government</rs>.</p></div>
			</div>
			<listOrg type="funding">
				<org type="funding" xml:id="_39bjUAN">
					<idno type="grant-number">FA8750-19-2-1004</idno>
				</org>
				<org type="funding" xml:id="_mV4W2Ff">
					<idno type="grant-number">W911NF-17-C-0099</idno>
				</org>
				<org type="funding" xml:id="_3jcuQuQ">
					<idno type="grant-number">HR001121C0165</idno>
				</org>
				<org type="funding" xml:id="_qZVSgkY">
					<idno type="grant-number">IIS-19-56151</idno>
				</org>
				<org type="funding" xml:id="_Ahpv4fZ">
					<idno type="grant-number">IIS-17-41317</idno>
				</org>
				<org type="funding" xml:id="_cad2E2g">
					<idno type="grant-number">17-04532</idno>
					<orgName type="program" subtype="full">An AI Research Institutes program</orgName>
				</org>
				<org type="funding" xml:id="_qJYpfpQ">
					<idno type="grant-number">2019897</idno>
					<orgName type="grant-name">Integrative Discovery Environment (I-GUIDE</orgName>
				</org>
				<org type="funding" xml:id="_quav9zV">
					<idno type="grant-number">2118329</idno>
				</org>
			</listOrg>

			<div type="availability">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>8 https://github.com/vi3k6i5/GuidedLDA 9 https://github.com/gregversteeg/corex_topic 10 https://github.com/adjidieng/ETM 11 https://github.com/yumeng5/CatE 12 https://huggingface.co/bert-base-uncased</p></div>
			</div>

			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>That is,</p><p>Therefore, e x w,d = -1 (which is invalid) or</p><p>Similarly, for J category , the objective can be rewritten as</p><p>Following the derivation of J document , we get</p><p>Putting Eqs. ( <ref type="formula">13</ref>), (19), and (21) together gives us Eq. (3).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B The Ensemble Ranking Function</head><p>In Section 3.3, we propose the following ensemble ranking function:</p><p>Now we prove this ranking function is a generalization of the arithmetic mean reciprocal rank (i.e., MRR) and the geometric mean reciprocal rank:</p><p>The case of ? ? 1 is trivial. When ? ? 0, we aim to show that</p><p>In fact, let r G = 1 rank G (w) and r L = 1 rank L (w) .</p><p>The third line is obtained by applying L'Hopital's rule.</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Incorporating domain knowledge into topic modeling via dirichlet forest priors</title>
		<author>
			<persName><forename type="first">David</forename><surname>Andrzejewski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaojin</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mark</forename><surname>Craven</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML&apos;09</title>
		<imprint>
			<date type="published" when="2009">2009</date>
			<biblScope unit="page" from="25" to="32" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Nonparametric spherical topic modeling with word embeddings</title>
		<author>
			<persName><forename type="first">Kayhan</forename><surname>Batmanghelich</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ardavan</forename><surname>Saeedi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Karthik</forename><surname>Narasimhan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sam</forename><surname>Gershman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL&apos;16</title>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="537" to="542" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Latent dirichlet allocation</title>
		<author>
			<persName><forename type="first">Andrew</forename><forename type="middle">Y</forename><surname>David M Blei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><forename type="middle">I</forename><surname>Ng</surname></persName>
		</author>
		<author>
			<persName><surname>Jordan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">JMLR</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page" from="993" to="1022" />
			<date type="published" when="2003">2003</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Dataless text classification with descriptive lda</title>
		<author>
			<persName><forename type="first">Xingyuan</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yunqing</forename><surname>Xia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jin</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">John</forename><surname>Carroll</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI&apos;15</title>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="2224" to="2231" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Specter: Document-level representation learning using citation-informed transformers</title>
		<author>
			<persName><forename type="first">Arman</forename><surname>Cohan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sergey</forename><surname>Feldman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Iz</forename><surname>Beltagy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Doug</forename><surname>Downey</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Daniel</forename><forename type="middle">S</forename><surname>Weld</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL&apos;20</title>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="2270" to="2282" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Medical subject headings used to search the biomedical literature</title>
		<author>
			<persName><forename type="first">H</forename><surname>Margaret</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Howard</forename><forename type="middle">L</forename><surname>Coletti</surname></persName>
		</author>
		<author>
			<persName><surname>Bleich</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">JAMIA</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="317" to="323" />
			<date type="published" when="2001">2001</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Gaussian lda for topic models with word embeddings</title>
		<author>
			<persName><forename type="first">Rajarshi</forename><surname>Das</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Manzil</forename><surname>Zaheer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chris</forename><surname>Dyer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL&apos;15</title>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="795" to="804" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Bert: Pre-training of deep bidirectional transformers for language understanding</title>
		<author>
			<persName><forename type="first">Jacob</forename><surname>Devlin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ming-Wei</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kenton</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kristina</forename><surname>Toutanova</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NAACL-HLT&apos;19</title>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="4171" to="4186" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Topic modeling in embedding spaces</title>
		<author>
			<persName><forename type="first">Francisco Jr</forename><surname>Adji B Dieng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><forename type="middle">M</forename><surname>Ruiz</surname></persName>
		</author>
		<author>
			<persName><surname>Blei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">TACL</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page" from="439" to="453" />
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Measuring nominal scale agreement among many raters</title>
		<author>
			<persName><forename type="first">L</forename><surname>Joseph</surname></persName>
		</author>
		<author>
			<persName><surname>Fleiss</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Psychological Bulletin</title>
		<imprint>
			<biblScope unit="volume">76</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page">378</biblScope>
			<date type="published" when="1971">1971</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Anchored correlation explanation: Topic modeling with minimal domain knowledge</title>
		<author>
			<persName><forename type="first">Ryan J</forename><surname>Gallagher</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kyle</forename><surname>Reing</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><surname>Kale</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Greg</forename><surname>Ver</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Steeg</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">TACL</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page" from="529" to="542" />
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<author>
			<persName><forename type="first">L</forename><surname>Thomas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mark</forename><surname>Griffiths</surname></persName>
		</author>
		<author>
			<persName><surname>Steyvers</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Finding scientific topics</title>
		<imprint>
			<date type="published" when="2004">2004</date>
			<biblScope unit="volume">101</biblScope>
			<biblScope unit="page" from="5228" to="5235" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Probabilistic latent semantic indexing</title>
		<author>
			<persName><forename type="first">Thomas</forename><surname>Hofmann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SIGIR&apos;99</title>
		<imprint>
			<date type="published" when="1999">1999</date>
			<biblScope unit="page" from="50" to="57" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Incorporating lexical priors into topic models</title>
		<author>
			<persName><forename type="first">Jagadeesh</forename><surname>Jagarlamudi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hal</forename><surname>Daum?</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Raghavendra</forename><surname>Udupa</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EACL&apos;12</title>
		<imprint>
			<date type="published" when="2012">2012</date>
			<biblScope unit="page" from="204" to="213" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Machine reading tea leaves: Automatically evaluating topic coherence and topic model quality</title>
		<author>
			<persName><forename type="first">Han</forename><surname>Jey</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><surname>Lau</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Timothy</forename><surname>Newman</surname></persName>
		</author>
		<author>
			<persName><surname>Baldwin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EACL&apos;14</title>
		<imprint>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="530" to="539" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Biobert: a pre-trained biomedical language representation model for biomedical text mining</title>
		<author>
			<persName><forename type="first">Jinhyuk</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wonjin</forename><surname>Yoon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sungdong</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Donghyeon</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sunkyu</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chan</forename><surname>Ho</surname></persName>
		</author>
		<author>
			<persName><forename type="first">So</forename></persName>
		</author>
		<author>
			<persName><forename type="first">Jaewoo</forename><surname>Kang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Bioinformatics</title>
		<imprint>
			<biblScope unit="volume">36</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="1234" to="1240" />
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<author>
			<persName><forename type="first">Omer</forename><surname>Levy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yoav</forename><surname>Goldberg</surname></persName>
		</author>
		<title level="m">Neural word embedding as implicit matrix factorization. NIPS&apos;14</title>
		<imprint>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="2177" to="2185" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">On the sentence embeddings from pre-trained language models</title>
		<author>
			<persName><forename type="first">Bohan</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hao</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Junxian</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mingxuan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yiming</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lei</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP&apos;20</title>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="9119" to="9130" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Tat-Seng Chua, and Maosong Sun</title>
		<author>
			<persName><forename type="first">Yang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhiyuan</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI&apos;15</title>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="2418" to="2424" />
		</imprint>
	</monogr>
	<note>Topical word embeddings</note>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Graphine: A dataset for graph-aware terminology definition generation</title>
		<author>
			<persName><forename type="first">Zequn</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shukai</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yiyang</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ruiyi</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ming</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sheng</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP&apos;21</title>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="3453" to="3463" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">The stanford corenlp natural language processing toolkit</title>
		<author>
			<persName><forename type="first">Mihai</forename><surname>Christopher D Manning</surname></persName>
		</author>
		<author>
			<persName><forename type="first">John</forename><surname>Surdeanu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jenny</forename><forename type="middle">Rose</forename><surname>Bauer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Steven</forename><surname>Finkel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><surname>Bethard</surname></persName>
		</author>
		<author>
			<persName><surname>Mc-Closky</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL&apos;14, System Demonstrations</title>
		<imprint>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="55" to="60" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Hidden factors and hidden topics: understanding rating dimensions with review text</title>
		<author>
			<persName><forename type="first">Julian</forename><surname>Mcauley</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jure</forename><surname>Leskovec</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">RecSys&apos;13</title>
		<imprint>
			<date type="published" when="2013">2013</date>
			<biblScope unit="page" from="165" to="172" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Discriminative topic mining via categoryname guided text embedding</title>
		<author>
			<persName><forename type="first">Yu</forename><surname>Meng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiaxin</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Guangyuan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zihan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chao</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiawei</forename><surname>Han</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">WWW&apos;20</title>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="2121" to="2132" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Text classification using label names only: A language model self-training approach</title>
		<author>
			<persName><forename type="first">Yu</forename><surname>Meng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yunyi</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiaxin</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chenyan</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Heng</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chao</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiawei</forename><surname>Han</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP&apos;20</title>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="9006" to="9017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Distributed representations of words and phrases and their compositionality</title>
		<author>
			<persName><forename type="first">Tomas</forename><surname>Mikolov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kai</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Greg</forename><forename type="middle">S</forename><surname>Corrado</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jeff</forename><surname>Dean</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS&apos;13</title>
		<imprint>
			<date type="published" when="2013">2013</date>
			<biblScope unit="page" from="3111" to="3119" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Optimizing semantic coherence in topic models</title>
		<author>
			<persName><forename type="first">David</forename><surname>Mimno</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hanna</forename><surname>Wallach</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Edmund</forename><surname>Talley</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Miriam</forename><surname>Leenders</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrew</forename><surname>Mccallum</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP&apos;11</title>
		<imprint>
			<date type="published" when="2011">2011</date>
			<biblScope unit="page" from="262" to="272" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Automatic evaluation of topic coherence</title>
		<author>
			<persName><forename type="first">David</forename><surname>Newman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jey</forename></persName>
		</author>
		<author>
			<persName><forename type="first">Han</forename><surname>Lau</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Karl</forename><surname>Grieser</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Timothy</forename><surname>Baldwin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NAACL-HLT&apos;10</title>
		<imprint>
			<date type="published" when="2010">2010</date>
			<biblScope unit="page" from="100" to="108" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Improving topic models with latent feature word representations</title>
		<author>
			<persName><forename type="first">Richard</forename><surname>Dat Quoc Nguyen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lan</forename><surname>Billingsley</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mark</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName><surname>Johnson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">TACL</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page" from="299" to="313" />
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Network embedding as matrix factorization: Unifying deepwalk, line, pte, and node2vec</title>
		<author>
			<persName><forename type="first">Jiezhong</forename><surname>Qiu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuxiao</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hao</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jian</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kuansan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jie</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">WSDM&apos;18</title>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="459" to="467" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Exploring the space of topic coherence measures</title>
		<author>
			<persName><forename type="first">Michael</forename><surname>R?der</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andreas</forename><surname>Both</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alexander</forename><surname>Hinneburg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">WSDM&apos;15</title>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="399" to="408" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Neural machine translation of rare words with subword units</title>
		<author>
			<persName><forename type="first">Rico</forename><surname>Sennrich</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Barry</forename><surname>Haddow</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alexandra</forename><surname>Birch</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL&apos;16</title>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="1715" to="1725" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Automated phrase mining from massive text corpora</title>
		<author>
			<persName><forename type="first">Jingbo</forename><surname>Shang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jialu</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Meng</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiang</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Clare</forename><forename type="middle">R</forename><surname>Voss</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiawei</forename><surname>Han</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE TKDE</title>
		<imprint>
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page" from="1825" to="1837" />
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
		<author>
			<persName><forename type="first">Suzanna</forename><surname>Sia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ayush</forename><surname>Dalmia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sabrina</forename><forename type="middle">J</forename><surname>Mielke</surname></persName>
		</author>
		<title level="m">Tired of topic models? clusters of pretrained word embeddings make for fast and good topics too! In EMNLP&apos;20</title>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="1728" to="1736" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Pte: Predictive text embedding through large-scale heterogeneous text networks</title>
		<author>
			<persName><forename type="first">Jian</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Meng</forename><surname>Qu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qiaozhu</forename><surname>Mei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">KDD&apos;15</title>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="1165" to="1174" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<monogr>
		<title level="m" type="main">Topic modeling with contextualized word representation clusters</title>
		<author>
			<persName><forename type="first">Laure</forename><surname>Thompson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><surname>Mimno</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2010.12626</idno>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Multi-document summarization using sentence-based topic models</title>
		<author>
			<persName><forename type="first">Dingding</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shenghuo</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tao</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yihong</forename><surname>Gong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL&apos;09</title>
		<imprint>
			<date type="published" when="2009">2009</date>
			<biblScope unit="page" from="297" to="300" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<monogr>
		<title level="m" type="main">Textomics: A dataset for genomics data summary generation</title>
		<author>
			<persName><forename type="first">Mu-Chun</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zixuan</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sheng</forename><surname>Wang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
	<note>In ACL&apos;22</note>
</biblStruct>

<biblStruct xml:id="b37">
	<monogr>
		<author>
			<persName><forename type="first">Yonghui</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mike</forename><surname>Schuster</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhifeng</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Quoc</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mohammad</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wolfgang</forename><surname>Norouzi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Maxim</forename><surname>Macherey</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuan</forename><surname>Krikun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qin</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Klaus</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><surname>Macherey</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1609.08144</idno>
		<title level="m">Google&apos;s neural machine translation system: Bridging the gap between human and machine translation</title>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Collaboratively improving topic discovery and word embeddings by coordinating global and local contexts</title>
		<author>
			<persName><forename type="first">Guangxu</forename><surname>Xun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yaliang</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jing</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aidong</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">KDD&apos;17</title>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="535" to="543" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">A correlated topic model using word embeddings</title>
		<author>
			<persName><forename type="first">Guangxu</forename><surname>Xun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yaliang</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wayne</forename><forename type="middle">Xin</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jing</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aidong</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IJCAI&apos;17</title>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="4207" to="4213" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">React: Online multimodal embedding for recencyaware spatiotemporal activity modeling</title>
		<author>
			<persName><forename type="first">Chao</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Keyang</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Quan</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fangbo</forename><surname>Tao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Luming</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tim</forename><surname>Hanratty</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiawei</forename><surname>Han</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SI-GIR&apos;17</title>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="245" to="254" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Motifclass: Weakly supervised text classification with higher-order metadata information</title>
		<author>
			<persName><forename type="first">Yu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shweta</forename><surname>Garg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yu</forename><surname>Meng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiusi</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiawei</forename><surname>Han</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">WSDM&apos;22</title>
		<imprint>
			<date type="published" when="2022">2022</date>
			<biblScope unit="page" from="1357" to="1367" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Minimally supervised categorization of text with metadata</title>
		<author>
			<persName><forename type="first">Yu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yu</forename><surname>Meng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiaxin</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Frank</forename><forename type="middle">F</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xuan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiawei</forename><surname>Han</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SIGIR&apos;20</title>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="1231" to="1240" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
