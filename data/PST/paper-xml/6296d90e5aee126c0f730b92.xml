<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Rethinking Graph Neural Networks for Anomaly Detection</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Jianheng</forename><surname>Tang</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Jiajin</forename><surname>Li</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Ziqi</forename><surname>Gao</surname></persName>
						</author>
						<title level="a" type="main">Rethinking Graph Neural Networks for Anomaly Detection</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.2" ident="GROBID" when="2022-12-25T13:09+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Graph Neural Networks (GNNs) are widely applied for graph anomaly detection. As one of the key components for GNN design is to select a tailored spectral filter, we take the first step towards analyzing anomalies via the lens of the graph spectrum. Our crucial observation is the existence of anomalies will lead to the 'rightshift' phenomenon, that is, the spectral energy distribution concentrates less on low frequencies and more on high frequencies. This fact motivates us to propose the Beta Wavelet Graph Neural Network (BWGNN). Indeed, BWGNN has spectral and spatial localized band-pass filters to better handle the 'right-shift' phenomenon in anomalies. We demonstrate the effectiveness of BWGNN on four large-scale anomaly detection datasets. Our code and data are released at https://github.com/squareRoot3/ Rethinking-Anomaly-Detection.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>An anomaly or an outlier is a data object that deviates significantly from the majority of the objects, as if it was generated by a different mechanism <ref type="bibr" target="#b21">(Han et al., 2011)</ref>. As a well-established problem, anomaly detection has received much attention due to its vast applicable tasks, e.g., cyber security <ref type="bibr" target="#b46">(Ten et al., 2011)</ref>, fraud detection <ref type="bibr" target="#b37">(Ngai et al., 2011)</ref>, health monitoring <ref type="bibr" target="#b3">(Bao et al., 2019)</ref>, device failure detection <ref type="bibr" target="#b44">(Sipple, 2020)</ref>, to name a few. As graph data becomes ubiquitous in the Web era, graph information often plays a vital role in identifying fraudulent users or activities, e.g., friendship relations in a social network and transaction records on a financial platform. Consequently, as a crucial research direction, graph-based anomaly detection is necessary to be further explored <ref type="bibr" target="#b38">(Noble &amp; Cook, 2003;</ref><ref type="bibr" target="#b33">Ma et al., 2021)</ref>. 1 Hong Kong University of Science and Technology (Guangzhou) 2 Hong Kong University of Science and Technology 3 Stanford University. Correspondence to: Jia Li &lt;jialee@ust.hk&gt;.</p><p>Proceedings of the 39 th International Conference on Machine Learning, Baltimore, Maryland, USA, PMLR 162, 2022. Copyright 2022 by the author(s).</p><p>Nowadays, Graph Neural Networks (GNNs) act as popular approaches for mining structural data, and are naturally applied for the graph anomaly detection task <ref type="bibr" target="#b2">(Bandyopadhyay et al., 2019;</ref><ref type="bibr" target="#b26">Kumar et al., 2018)</ref>. Unfortunately, the vanilla GNNs are not well-suited for anomaly detection and suffer from the over-smoothing issue <ref type="bibr" target="#b29">(Li et al., 2018;</ref><ref type="bibr" target="#b50">Wu et al., 2019)</ref>. When GNN aggregates information from the node neighborhoods, it also averages the representations of anomalies and makes them less distinguishable. Thus, by intentionally connecting with large amounts of benign neighborhoods, the anomalous nodes may attenuate their suspiciousness, which results in the poor performance of the plain GNNs.</p><p>To remedy the issue, several GNN models have been proposed. We categorize the existing methods into three classes. That is, (1) applying attention mechanisms to correlate different neighbors through various views <ref type="bibr" target="#b48">(Wang et al., 2019a;</ref><ref type="bibr" target="#b8">Cui et al., 2020;</ref><ref type="bibr" target="#b30">Liu et al., 2021a)</ref>, (2) using the resampling strategy to aggregate neighborhood information selectively <ref type="bibr" target="#b15">(Dou et al., 2020;</ref><ref type="bibr" target="#b32">Liu et al., 2020;</ref><ref type="bibr" target="#b31">2021b)</ref>, (3) designing auxiliary losses to enhance the network training power <ref type="bibr" target="#b12">(Ding et al., 2019;</ref><ref type="bibr" target="#b54">Zhao et al., 2020;</ref><ref type="bibr">2021)</ref>. All these methods analyze the anomaly detection from the graph spatial domain.</p><p>There are few works that address this problem from the spectral domain. Nevertheless, choosing a tailored spectral filter is a key component of GNN design, as the spectral filter determines the expressive power of GNN <ref type="bibr" target="#b1">(Balcilar et al., 2020;</ref><ref type="bibr" target="#b22">He et al., 2021)</ref>.</p><p>To fill the above gap, we want to answer the question in this paper-how to choose a tailored spectral filter in GNN for anomaly detection? We take the first step towards analyzing anomalies via the lens of the graph spectrum (i.e., after the graph Fourier transform of node attributes). From Figure <ref type="figure" target="#fig_0">1</ref>, we observe that low-frequency energy is gradually transferred to the high-frequency part, when the degree of anomaly becomes larger. We summarize this phenomenon as the 'right-shift' of spectral energy distribution, and further prove it on a Gaussian anomaly model in a rigorous manner. We validate the 'right-shift' phenomenon in a variety of graphs with synthetic or real-world anomalies. Through our analysis, we justify the necessity of spectral localized band-pass filters in graph anomaly detection.</p><p>Based on these findings, we propose Beta Wavelet Graph arXiv: <ref type="bibr">2205.15508v1 [cs.</ref>LG] 31 May 2022 Neural Network (BWGNN) to better tackle the 'right-shift' phenomenon of graph anomalies. Although the frequency responses of existing works <ref type="bibr" target="#b51">(Wu et al., 2021;</ref><ref type="bibr" target="#b22">He et al., 2021;</ref><ref type="bibr" target="#b5">Bo et al., 2021)</ref> can cover nearly all frequency profiles, these GNNs with adaptive filters cannot guarantee to be band-pass and spectral-localized <ref type="bibr" target="#b1">(Balcilar et al., 2020)</ref>. In fact, the design of frequency response in those works are not tailored to anomaly detection. Therefore, we invoke Hammond's graph wavelet theory <ref type="bibr" target="#b20">(Hammond et al., 2011)</ref> to develop our new graph neural network architecture. In contrast with the widely used Heat kernels <ref type="bibr" target="#b14">(Donnat et al., 2018;</ref><ref type="bibr" target="#b52">Xu et al., 2019a;</ref><ref type="bibr" target="#b28">Li et al., 2021)</ref>, the crux of our method is to propose the Beta kernel to address higher frequency anomalies via multiple flexible, spatial/spectral-localized, and band-pass filters.</p><formula xml:id="formula_0">=1 =2 =5 =20 0 0.5 1 1.5 2 (a) 0% 20% 40% 60% 80% 100% =1 =2 =5 =20 =0% =1% =5% =20% 0 0.5 1 1.5 2 (b) =0% =1% =5% =20% =1 =2 =5 =20 0 0.5 1 1.5 2 (c) =1 =2 =5 =20 =0% =1% =5% =20% 0 0.5 1 1.5 2 (d) =0% =1% =5% =20%</formula><p>To further facilitate this line of research, we release two large-scale real-world datasets as new graph anomaly detection benchmarks, including the T-Finance dataset based on a transaction network and the T-Social dataset based on a social network. Together with them, we conduct extensive experiments on four datasets in both supervised and semisupervised settings. The proposed BWGNN shows superior performance over widely used graph neural networks and state-of-the-art graph anomaly detection methods.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Spectral Analysis of Graph Anomaly</head><p>To start with, we provide some necessary preliminaries of graph anomaly detection in Section 2.1. In Section 2.2, we give the theoretical insights of the 'right-shift' phenomenon and rigorously prove it on a Gaussian anomaly model. Towards that end, we validate our findings on graphs with synthetic anomalies in Section 2.3, and on real-world anomaly detection datasets in Section 2.4.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.">Preliminaries</head><p>Attributed Graph We define an attributed graph as G = {V, E, X}, where</p><formula xml:id="formula_1">V = {v 1 , v 2 , ⋯, v N } is the set of N nodes, E = {e ij }</formula><p>is the set of edges, and e ij = (v i , v j ) represents an unweighted edge between nodes v i and v j . Let A be the corresponding adjacency matrix, D be the degree matrix with D ii = ∑ j A ij . Each node v i has a d-dimensional feature vector X i ∈ R d , and the set of all node features is X = {X 1 , X 2 , ⋯, X N }. In some applications, G is a multirelational graph and multiple edge sets represent different relations between nodes.</p><p>Graph-based Anomaly Detection Let V a , V n be two disjoint subsets of V(i.e., V a ∩ V n = ∅), where V a represents all the nodes labeled as anomalous and V n represents all normal nodes. Graph-based anomaly detection is to classify unlabeled nodes in G into the normal or anomalous categories given the information of the graph structure E, node features X, and partial node labels {V a , V n }. In this paper, we focus on node anomalies and assume that all edges in G are trusted, leaving structural anomalies for future work. Usually, there are far more normal nodes than anomalous nodes ( V a &lt;&lt; V n ), thus graph-based anomaly detection can be regarded as an imbalanced binary node classification problem. The main difference is that anomaly detection focuses more on the unusual and deviated patterns in the dataset.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.">Theoretical Results</head><p>Problem Setup Let the Laplacian matrix L be defined as D − A (regular) or as</p><formula xml:id="formula_2">I − D −1 2 AD −1 2 (normalized),</formula><p>where I is an identity matrix. L is a symmetric matrix with eigenvalues, i.e., 0 = λ 1 ≤ ⋯ ≤ λ N and a corresponding orthonormal basis of eigenvectors U = (u 1 , u 2 , ⋯, u N ). Except for two endpoints λ 1 and λ N , we can split other eigenvalues into the low frequencies {λ 1 , λ 2 , ⋯, λ k } and high frequencies {λ k+1 , λ k+2 , ⋯, λ N } with an arbitrary threshold λ k .</p><p>Assume that</p><formula xml:id="formula_3">x = (x 1 , x 2 , ⋯, x N ) T ∈ R N is a signal on G, and x = (x 1 , x2 , ⋯, xN ) T = U T x is the graph Fourier transform of x. We denote x2 k Σ N i=1 x2</formula><p>i as the spectral energy distribution at λ k (1 ≤ k ≤ N ). We summarize our theoretical and empirical findings as</p><p>The existence of anomalies leads to the 'right-shift' of spectral energy, which means the spectral energy distribution concentrates less in low frequencies and more in high frequencies.</p><p>To further justify the insight, we prove this interesting finding based on a probabilistic anomaly model <ref type="bibr" target="#b21">(Han et al., 2011;</ref><ref type="bibr" target="#b18">Grubbs, 1969)</ref>. The graph features are assumed to be identically independent drawn from a Gaussian distribution, i.e., x ∼ N (µe N , σ 2 I N ), where µe N is an all-the-one vector. Here, the coefficient of variation σ µ can be regarded as the degree of anomalies in x, which is indeed a commonly used measure to describe the dispersion of a distribution <ref type="bibr" target="#b24">(Kendall et al., 1946;</ref><ref type="bibr" target="#b4">Bedeian &amp; Mossholder, 2000)</ref>. When the fraction of anomalies in x increases, σ µ becomes larger and indicates a more considerate degree of anomaly. Alternatively, if the distance between anomalies and the mean vector becomes larger, σ µ also increases, representing a larger degree of anomalies.</p><p>To quantify how the spectral energy distribution changes with respect to the degree of anomalies in x, we introduce a metric -energy ratio as below:</p><p>Definition 1 (Energy Ratio). For any 1 ≤ k ≤ N − 1, we define k-th low-frequency energy ratio as the accumulated energy distribution in the first k eigenvalues:</p><formula xml:id="formula_4">η k (x, L) = ∑ k i=1 x2 i ∑ N i=1 x2 i .</formula><p>A larger η k indicates that a larger part of the energy boils down to the first k eigenvalues. In the following proposition, we shed light on how the degree of anomalies in x will affect η k .</p><p>Proposition 2. If µ ≠ 0 and L = D − A, the expectation of the inverse of low-frequency energy ratio</p><formula xml:id="formula_5">E x [1 η k (x, L)]</formula><p>is monotonically increasing with the anomaly degree σ µ .</p><p>Proposition 2 indicates that an increased degree of anomaly enforces the spectral energy distribution to concentrate less on the low-frequency eigenvalues. The proof details can be found in Appendix A.</p><p>Unfortunately, the calculation of spectral energy ratio requires the eigen-decomposition of the graph Laplacian, which is time-consuming on large-scale graphs. To navigate such pitfalls, we introduce a more computational amenable metric to generally measure the effect of graph anomalies in the spectral domain.</p><p>Definition 3 (High-frequency Area). Suppose that the lowfrequency energy ratio curve f (t) is defined as</p><formula xml:id="formula_6">f (t) = η k (x, L) where t ∈ [λ k , λ k+1 ) and 1 ≤ k ≤ N − 1.</formula><p>The area between f (t) and g(t) = 1 is defined as the high-frequency area:</p><formula xml:id="formula_7">S high = ∫ λ N 0 1 − f (t)dt.</formula><p>Without the need of eigen-decomposition, S high can be computed by simple elementary manipulations:</p><formula xml:id="formula_8">S high = ∑ N k=1 λ k x2 k ∑ N k=1 x2 k = x T Lx x T x .<label>(1)</label></formula><p>We refer the reader to Appendix B for further details. According to (1), spectral energy on low frequencies contributes less to S high after multiplying a small eigenvalue. For example, S high = 0 if the entire spectral energy concentrates on λ 1 = 0, and S high increases when spectral energy shifts to larger eigenvalues. Therefore, we can use the change of S high to describe the 'right-shift' phenomenon in the whole spectrum.</p><p>Furthermore, (1) reveals that the energy distribution of x in the spectral domain is closely related to the smoothness of x in the spatial domain -x T Lx will be smaller if signal x has closer values between connected nodes, which also reflects a smaller anomaly degree. In Appendix A, we additionally proof that S high is monotonically increasing with the anomaly degree σ µ , which matches the result in Proposition 2.  <ref type="table" target="#tab_2">Social 5,781,065 73,105,508</ref> 3.01% 10 -5.36% (-13.2%) 0.08% (-0.02%)</p><formula xml:id="formula_9">0.0 0.5 1.0 1.5 2.0 Barabasi Albert graph 0% 20% 40% 60% 80% 100% 0.0 0.5 1.0 1.5 2.0 Minnesota road graph =1, =1 =0.5, =1 =1, =2 Figure 2. Spectral analysis of graph signal x ∼ N (µe N , σ 2 I N ).</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3.">Validation on Synthetic Anomalies</head><p>To present our theoretical findings in a more intuitive way, we demonstrate it on different graphs with synthetic anomalies. In Figure <ref type="figure" target="#fig_0">1</ref>, we study the effect of anomalies on two types of graph: a Barabási-Albert graph with 500 nodes on Figure <ref type="figure" target="#fig_0">1</ref>  We first assume normal and anomalous nodes follow different distributions to better visualize the change of spectral energy under different anomaly degrees. For both graphs, the one-dimensional feature of each normal node is drawn from N (1, 1), while the anomalous one is drawn from N (1, σ 2 ) (i.e., σ &gt; 1). We analyze two variations of anomalies: (i) the fraction of anomalies is fixed to 5%, and the standard deviation of anomalies changes (i.e., σ = 1, 2, 5, 20); and (ii) the standard deviation of anomalies is fixed to 5 and the fraction of anomalies changes (i.e., α = 0%, 1%, 5%, 20%).</p><p>In the top half of Figure <ref type="figure" target="#fig_0">1</ref>, we use blue circles to represent anomalous nodes in the spatial domain. Bigger blue nodes indicate a larger degree of anomaly. In the bottom half of Figure <ref type="figure" target="#fig_0">1</ref>, we display the energy distribution of x in the spectral domain with various anomaly degrees. The colored histograms reflect the proportion of spectral energy in different eigenvalue intervals such as [0, 0.5), and the curves represent f (t) in Definition 3.</p><p>We take the Barabási-Albert graph as an example. When the fraction of anomalies is 0%, most of the energy (60%) locates in the low frequency region (λ &lt; 0.5). By contrast, when the anomaly degree becomes larger -increasing σ and α, the ratio of spectral energy for λ &gt; 0.5 becomes larger in the histogram. Also, the low-frequency energy ratio curve is almost monotonically decreasing on the interval λ ∈ [0, 2].</p><p>The observation also holds for the Minnesota road graph.</p><p>Moreover, we further assume the features of all nodes are drawn from a single Gaussian distribution which strictly follows Proposition 2. We corroborate our theoretical findings on three cases: (1) the original features µ = 1, σ = 1, (2) features with a smaller mean value µ = 0.5, σ = 1, and</p><p>(3) features with a larger variance µ = 1, σ = 2. In Figure <ref type="figure">2</ref>, both increasing the variance and decreasing the mean value lead to the 'right-shift' of spectral energy distributions, which is consistent with the result in Proposition 2.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.4.">Validation on Real-world Anomalies</head><p>In real-world anomaly detection datasets, the node features may not strictly follow the Gaussian distribution. Nevertheless, we verify the generality of 'right-shift' on the real case. We introduce four industry datasets for different anomaly detection scenarios. We choose two widely used datasets in previous works <ref type="bibr" target="#b31">(Liu et al., 2021b;</ref><ref type="bibr" target="#b44">2020)</ref>, including the Amazon dataset <ref type="bibr" target="#b34">(McAuley &amp; Leskovec, 2013)</ref> for user anomaly detection and the YelpChi dataset <ref type="bibr" target="#b43">(Rayana &amp; Akoglu, 2015)</ref> for review anomaly detection. We further construct two large-scale real-world datasets as new graph anomaly detec-tion benchmarks, including the T-Finance dataset based on a transaction network and the T-Social dataset based on a social network. The statistics of these datasets are summarized in Table <ref type="table" target="#tab_0">1</ref>. More details about these datasets are in Section 4.1.</p><p>We first study the 'right-shift' phenomenon of anomalies in the Amazon dataset. We choose the last feature dimension and visualize the spectral energy in three views: (1) the origin graph, (2) the perturbed graph by dropping all anomalies, and (3) the perturbed graph by dropping the same number of random nodes. In Figure <ref type="figure" target="#fig_2">3</ref>, compared with the origin graph, the spectral energy in the subgraph without anomalies concentrates more in low frequencies (i.e., λ ∈ [0, 0.2)) and less in high frequencies (i.e., λ ∈ [0.8, 1.2)). By contrast, dropping random nodes does not have the same effect. The 'right-shift' phenomenon in this case indicates that the frequency band around λ = 1 has a strong connection with anomalies.</p><p>As the eigen-decomposition operation suffers from the heavy computational burden, especially on the other three large-scale datasets, we quantify the spectral effect by the high-frequency area introduced in Definition 3. We compare the original graph with two perturbed graphs. The relative change is calculated by ∆S high = ( Ŝhigh − S high ) S high , in which S high and Ŝhigh denote the high-frequency area of the original graph and that of two perturbed graphs respectively.</p><p>We compute ∆S high for each feature dimension and report the mean and lowest values in Table <ref type="table" target="#tab_0">1</ref>. Among all datasets, removing anomalies leads to the decrease of S high , while dropping nodes randomly has limited effects on S high . More specifically, on T-Social and Yelpchi, S high decreases more than 10% for the worst-case feature, which is impressive given that anomalies are few.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Methodology</head><p>The analysis in Section 2 shows that we need to focus on 'right-shift' effect when detecting graph anomalies. Unfortunately, most of the current GNNs are low-pass filters <ref type="bibr" target="#b39">(Nt &amp; Maehara, 2019;</ref><ref type="bibr" target="#b50">Wu et al., 2019)</ref> or adaptive filters <ref type="bibr" target="#b11">(Defferrard et al., 2016;</ref><ref type="bibr" target="#b22">He et al., 2021;</ref><ref type="bibr" target="#b13">Dong et al., 2021;</ref><ref type="bibr" target="#b27">Levie et al., 2018)</ref> that are neither guaranteed to be band-pass nor spectral-localized <ref type="bibr" target="#b1">(Balcilar et al., 2020)</ref>. Since highfrequency anomalies account for only a small fraction and most spectral energy still concentrates on low frequencies, these adaptive GNNs may degrade to low-pass filters in this task.</p><p>To overcome this drawback, we propose our new graph neural network architecture based on Hammond's graph wavelet theory <ref type="bibr" target="#b20">(Hammond et al., 2011)</ref>, which is band-pass in nature and can better address the 'right-shift' effect inheriting from anomalies. We first introduce the backgrounds of graph wavelet in Section 3.1. Then, we propose our graph Beta wavelet and demonstrate its good properties in Section 3.2. Based on Beta wavelets, we construct the Beta Wavelet Graph Neural Network (BWGNN) in Section 3.3. We discuss the differences between BWGNN and other related graph wavelets in Section 3.4.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Background: Hammond's Graph Wavelet</head><p>The graph wavelet transform defined in <ref type="bibr" target="#b20">(Hammond et al., 2011)</ref> starts with a "mother" wavelet ψ and employs a group of wavelets as bases, defined as W = (W ψ1 , W ψ2 , ⋯). Formally, applying W ψi on a graph signal x ∈ R N can be written as</p><formula xml:id="formula_10">W ψi (x) = U g i (Λ)U T x,<label>(2)</label></formula><p>where g i (⋅) is a kernel function in the spectral domain defined on [0, λ N ], and g i (Λ) = diag(g i (λ)). Although Equation ( <ref type="formula" target="#formula_10">2</ref>) is similar to the graph spectral convolution derived from Fourier transform, the kernel function g i in Hammond's graph wavelet transform should satisfy the following additional requirements.</p><p>• According to the Parseval theorem, the wavelet transform needs to meet the admissibility condition:</p><formula xml:id="formula_11">∞ 0 g i (w) 2 w dw = C g &lt; ∞,</formula><p>which means g i should satisfy g i (0) = g i (∞) = 0 and perform like a band-pass filter in the spectral domain. • The wavelet transform covers different frequency bands via band-pass filters of different scales {g 1 , g 2 , ...}.</p><p>To avoid the eigen-decomposition of the graph Laplacian L, the kernel function g i has to be a polynomial function, i.e., U g i (Λ)U T = g i (L) in most of the literature.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Beta Wavelet on Graph</head><p>Beta distribution often serves as a wavelet basis (De Oliveira &amp; De Araújo, 2015) in computer vision applications <ref type="bibr" target="#b0">(Amar et al., 2005;</ref><ref type="bibr" target="#b23">Jemai et al., 2010;</ref><ref type="bibr" target="#b16">ElAdel et al., 2016</ref>), but has not been utilized for mining graph data yet. Here we choose the Beta distribution as the graph kernel function and demonstrate that it meets the requirements of Hammond's graph wavelet.</p><p>The probability density function of Beta distribution admits:</p><formula xml:id="formula_12">β p,q (w) = 1 B(p+1,q+1) w p (1 − w) q if w ∈ [0, 1] 0 otherwise</formula><p>where p, q ∈ R + and B(p + 1, q + 1) = p!q! (p + q + 1)! is a constant. As the eigenvalues of the normalized graph Laplacian L satisfy λ ∈ [0, 2], we adopt β * p,q (w) = 1 2 β p,q ( w 2 ) to cover the complete spectral range of L. We further add the restrictions p, q ∈ N + to ensure β * (p, q) is a polynomial, such that fast computation can be conducted. Thus, our Beta wavelet transform W p,q can be written as:</p><formula xml:id="formula_13">W p,q = U β * p,q (Λ)U T = β * p,q (L) = ( L 2 ) p (I − L 2 ) q 2B(p + 1, q + 1)</formula><p>.</p><p>Let p + q = C be a constant and our Beta wavelet transform W is constructed by a group of C + 1 Beta wavelets with the same order:</p><formula xml:id="formula_14">W = (W 0,C , W 1,C−1 , ⋯, W C,0 ).<label>(3)</label></formula><p>In this equation, W 0,C is a low-pass filter and others are band-pass filters of different scales. Besides, when p &gt; 0, the kernel function β * p,q satisfies:</p><formula xml:id="formula_15">∞ 0 β * p,q (w) 2 w dw ≤ 2 0 dw 2B(p + 1, q + 1) &lt; ∞.</formula><p>Therefore, the proposed Beta wavelet transform W satisfies both two requirements of Hammond's graph wavelet in Section 2.1.</p><p>Figure <ref type="figure" target="#fig_3">4</ref> compares a group of the proposed Beta wavelets (C = 4) with the widely-used wavelet with Heat kernels (i.e., g(λ) = e −τ λ , τ ∈ {1, 3, 5, 10}). The first column shows that Heat kernels are all low-pass filters in the spectral domain, while Beta kernels contain various filter types including low-pass and band-pass.</p><p>The right two columns in Figure <ref type="figure" target="#fig_3">4</ref> visualize the effect of graph wavelet transform on a randomly generated kNN graph. The responses with an absolute value less than 0.001 are marked as grey. For different scales, responses of heat wavelets are all positive, while those of Beta wavelets can be positive or negative in different channels. The diversified responses of the Beta graph wavelet transform show that it can capture not only the similarities but also the differences of nodes within a subgraph, thus a better neighborhood flexibility. This property can make the representations of anomalies more distinguishable.</p><p>The classic wavelet transform is well-known for its locality in time and frequency domains. As an analogy, we present two propositions about the good locality of Beta graph wavelets in spectral and spatial domains:</p><p>Proposition 4 (Spectral Locality). Consider p &gt; 0, q &gt; 0 and X ∼ β * p,q where β * p,q is band-pass, the mean and variance of X are:</p><formula xml:id="formula_16">µ =E(X) = 2(p + 1) p + q + 2 σ =Var(X) = 4(p + 1)(q + 1) (p + q + 2) 2 (p + q + 3) .</formula><p>When p+q → ∞ satisfies p = cq, we have σ → 0 and µ = 2c c+1 can be any number between (0, 2).</p><p>Here, we use p = cq to ensure p and q do not deviate dramatically. Proposition 4 can be directly derived from the properties of the Beta distribution. It shows β * p,q can concentrate on any µ ∈ (0, 2), which means Beta graph wavelets can be tailored to any specific frequency band for anomaly detection.</p><p>Proposition 5 (Spatial Locality). Let v i , v j be two nodes on G, W p,q δ i [j] be the effect of a one-hot signal δ i ∈ R N on node v j after the wavelet transform. W p,q δ i is localized in (p+q)-hops of node v i . When the distance d G (v i , v j ) &gt; p+q, we have W p,q δ i [j] = 0.</p><p>Invoking Lemma 5.2 in <ref type="bibr" target="#b20">(Hammond et al., 2011)</ref>, we have</p><formula xml:id="formula_17">L n δ i [j] = 0 when d G (v i , v j ) &gt; n.</formula><p>Because W p,q is just a Corder polynomial of L, we complete the proof. Proposition 5 shows that the Beta graph wavelet also has a good spatial locality. Propositions 4 and 5 indicate that a larger C can provide a better spectral locality at the expense of a worse spatial locality and vice versa.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Beta Wavelet Graph Neural Network</head><p>Based on the introduced Beta graph wavelet, we propose the Beta Wavelet Graph Neural Network (BWGNN) for graph-based anomaly detection.</p><p>Different from GNN models <ref type="bibr" target="#b25">(Kipf &amp; Welling, 2017)</ref> in which multiple layers are stacked in a cascade fashion, BWGNN uses different wavelet kernels in parallel and then aggregates the corresponding filtering results. Specifically, BWGNN adopts the following propagation process:</p><formula xml:id="formula_18">Z i = W i,C−i (MLP(X)) H = AGG([Z 0 , Z 1 , ⋯, Z C ]),</formula><p>where MLP(⋅) denotes a multi-layer perceptron and AGG(⋅) can be a simple aggregation function such as summation or concatenation. W i,C−i from (3) denotes our wavelet kernels. The aggregated representation H is then fed to another MLP with the Sigmoid function to compute the abnormal probability p i . Weighted cross-entropy loss is used for the training of BWGNN:</p><formula xml:id="formula_19">L = i (γy i log(p i ) + (1 − y i ) log(1 − p i )), (<label>4</label></formula><formula xml:id="formula_20">)</formula><p>where γ is the ratio of anomaly labels (y i = 1) to normal labels (y i = 0).</p><p>The complexity of BWGNN is O(C E ), as β * p,q (L) is a polynomial function that can be computed recursively <ref type="bibr" target="#b11">(Defferrard et al., 2016)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4.">Discussion</head><p>We discuss the differences between the proposed BWGNN and other related wavelet GNNs. Heat kernel-based methods <ref type="bibr" target="#b52">(Xu et al., 2019a;</ref><ref type="bibr" target="#b14">Donnat et al., 2018;</ref><ref type="bibr" target="#b28">Li et al., 2021)</ref> do not essentially satisfy Hammond's graph wavelet theory <ref type="bibr" target="#b20">(Hammond et al., 2011)</ref> because they are not band-pass. To remedy this issue, BWGNN extends Hammond's graph wavelet to the learnable GNN framework and, for the first time, utilizes Beta distribution as the kernel function to generate graph wavelets. On another front, <ref type="bibr" target="#b35">(Min et al., 2020;</ref><ref type="bibr" target="#b17">Gama et al., 2019;</ref><ref type="bibr" target="#b36">Min et al., 2021)</ref> adopt the idea of diffusion wavelets <ref type="bibr" target="#b7">(Coifman &amp; Maggioni, 2006)</ref> to construct graph neural networks. Compared with them, the proposed Beta kernel can better handle higher frequency anomalies via multiple flexible, localized, and band-pass filters.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Experiments</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Experimental Setup</head><p>Datasets. We conduct experiments on four datasets introduced in Table <ref type="table" target="#tab_0">1</ref>. The YelpChi dataset <ref type="bibr" target="#b43">(Rayana &amp; Akoglu, 2015)</ref> aims to find the anomalous reviews which unjustly promote or demote certain products or businesses on Yelp.com. There are three edge types in the graph, including R-U-R (the reviews posted by the same user), R-S-R (the reviews under the same product with the same star rating), and R-T-R (the reviews under the same product posted in the same month). The Amazon dataset <ref type="bibr" target="#b34">(McAuley &amp; Leskovec, 2013)</ref> aims to find the anomalous users paid to write fake product reviews under the Musical Instrument category on Amazon.com. There are also three relations: U-P-U (users reviewing at least one same product), U-S-U (users having at least one same star rating within one week), and U-V-U (users with top-5% mutual review similarities). Furthermore, we release two real-world datasets T-Finance and T-Social. The T-Finance dataset aims to find the anomaly accounts in transaction networks. The nodes are unique anonymized accounts with 10-dimension features related to registration days, logging activities and interaction frequency. The edges in the graph represent two accounts that have transaction records. Human experts annotate nodes as anomalies if they fall into categories like fraud, money laundering and online gambling. The T-Social dataset aims to find the anomaly accounts in social networks. It has the same node annotations and features as T-Finance, while two nodes are connected if they maintain the friend relationship for more than three months. The size of T-Social is 100 times larger than that of YelpChi and Amazon.</p><p>Metrics. We choose two widely used metrics to measure the performance of all the methods, namely F1-macro and AUC. F1-macro is the unweighted mean of the F1-score of two classes, which neglects the imbalance ratio between normal and anomaly labels. AUC <ref type="bibr" target="#b9">(Davis &amp; Goadrich, 2006)</ref> is the area under the ROC Curve.</p><p>Baselines and Implementation Details. We compare BWGNN with three groups of baselines. The first group only considers node features and includes MLP and SVM <ref type="bibr" target="#b6">(Chang &amp; Lin, 2011)</ref>. The second group is general GNN models, including GCN <ref type="bibr" target="#b25">(Kipf &amp; Welling, 2017)</ref>, ChebyNet <ref type="bibr" target="#b11">(Defferrard et al., 2016)</ref>, GAT <ref type="bibr" target="#b47">(Veličković et al., 2017)</ref>, GIN <ref type="bibr" target="#b53">(Xu et al., 2019b</ref><ref type="bibr">), GraphSAGE (Hamilton et al., 2017)</ref>, and GWNN <ref type="bibr" target="#b52">(Xu et al., 2019a)</ref>. The third group is state-of-the-art methods for graph-based anomaly detection, including GraphConsis <ref type="bibr" target="#b32">(Liu et al., 2020)</ref>, CAREGNN <ref type="bibr" target="#b15">(Dou et al., 2020)</ref> and PC-GNN <ref type="bibr" target="#b31">(Liu et al., 2021b)</ref>. For the detailed baseline description, we refer the reader to Appendix C.</p><p>Since the graphs in YelpChi and Amazon are multirelational, we introduce two ways of dealing with heterogeneity in BWGNN. The first way is to treat all types of edges as the same. The second way is to perform graph propagation (3) for each relation separately and add a maximum pooling after that. We denote the first way as BWGNN (homo) and the second way as BWGNN (hetero).</p><p>We train all models except SVM for 100 epochs by Adam optimizer with a learning rate of 0.01, and save the model with the best Macro-F1 in validation. On the YelpChi, Amazon, and T-Finance datasets, the dimension h for representations and hidden states in all models are set to 64, and the order C in BWGNN is 2. We use concatenation as the AGG(⋅) function in BWGNN. The training ratio is 40% in the supervised scenario and 1% in the semi-supervised scenario, while the remaining data are split by 1:2 for validation and test. On the T-Social dataset, h is set to 64, C is set to 5, the supervised training ratio is 40%, and the semi-supervised training ratio is 0.01% (with only 17 labeled anomalies). The ratio of validation and test sets is 1:2. We report the average value and standard deviation of 10 runs on YelpChi and Amazon. On T-Finance and T-Social, we report the </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Performance Comparison</head><p>The results are reported in Table <ref type="table" target="#tab_1">2</ref> and Table <ref type="table" target="#tab_2">3</ref> respectively. The standard deviation is in Appendix D. In general, BWGNN achieves the best performance in all datasets except Amazon (1%), where PC-GNN obtains the best AUC score. For two datasets with multi-relation graphs, BWGNN (Hetero) performs better on YelpChi while BWGNN (Homo) is better on Amazon.</p><p>GraphConsis, CAREGNN, and PC-GNN are three state-ofthe-art methods for graph-based anomaly detection, while BWGNN outperforms them significantly with a much shorter training time. For example, on YelpChi (40%), BWGNN has 13.9% and 10.6% absolutely improvement in F1-Macro and AUC respectively when compared with PC-GNN. The performance improvement is even more significant on T-Social, proving both the superiority and the scalability of BWGNN on large graphs.</p><p>Among general GNN models (GCN, ChebyNet, GAT, GIN, GraphSAGE, and GWNN), GCN performs worst in most cases. It is consistent with our analysis that the low-pass filter is insufficient in distinguishing anomalies. Conversely, ChebyNet performs better because the learnable Chebyshev kernel in it can act as a band-pass filter.</p><p>Even though graph structure is ignored, MLP and SVM can also achieve comparable performance in some datasets. For instance, SVM outperforms many GNN approaches on Amazon. However, the performance is still lower than BWGNN, suggesting that neighborhood information is still important in anomaly detection, but needs to be used properly.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Sensitivity Analysis</head><p>The Order C in BWGNN. The order C is a crucial hyperparameter in BWGNN, as Beta wavelet is a C-order polynomial of L and is localized in C-hops of each node according to proposition 5. Figure <ref type="figure" target="#fig_4">5</ref> presents the F1-macro and AUC scores of BWGNN on two datasets when varying C from 1 to 5. On T-Social, higher C leads to better performances, while on T-Finance, there are no significant differences in results for C ≥ 2. One possible reason is that the graph in T-Social is much more sparse than that in T-Finance according to Table <ref type="table" target="#tab_0">1</ref>. Thus, a larger range of neighborhoods is required for T-Social.</p><p>Impact of Anomaly Degree. We evaluate the effect of different anomaly degrees on ChebyNet, CAREGNN and our BWGNN using the T-Finance dataset. We consider two variables in Section 2.2, including the standard deviation σ and the fraction α of the anomalies. We keep the mean value  and scale the σ of anomalous node features. To control α, we follow the attribute perturbation schema in <ref type="bibr" target="#b12">(Ding et al., 2019)</ref>. We generate different fractions of anomalies by replacing normal node attributes with anomalous ones.</p><p>Figure <ref type="figure" target="#fig_5">6</ref> compares the F1-macro and AUC scores of BWGNN, ChebyNet, and CAREGNN on T-Finance (1%) with different anomaly degrees. When σ increases, all three models perform better as the anomalies are more distinguishable. Among them, BWGNN is the fastest-growing method and reaches 99% F1-macro at σ = 4. When varying α, BWGNN consistently outperforms other methods and is robust to different anomaly degrees.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Conclusion</head><p>This work presents a novel analysis of graph anomalies in the spectral domain. We find that graph anomalies lead to As the first and second groups of baselines are not specifically designed for anomaly detection, they suffer from class imbalance and produce very few positive predictions. For fair comparisons, in training, we use the weighted cross-entropy Equation (4) in BWGNN. In validation, we search for the best binary classification threshold by adjusting it at 0.05 intervals to achieve the best F1-macro.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D. Additional Experimental Results</head><p>In Table <ref type="table" target="#tab_3">4</ref>, we report the average value and standard deviation of 10 runs on YelpChi and Amazon. </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 .</head><label>1</label><figDesc>Figure 1. The effect of graph anomalies in the spatial domain (top) and spectral domain (bottom) with different anomaly degrees. The cases (a,c) are related to different standard deviation of anomalies (type (i), σ = 1, 2, 5, 20) while the cases (b,d) are about different fraction of anomalies (type (ii), α = 0%, 1%, 5%, 20%).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head></head><label></label><figDesc>(a)-(b) and a Minnesota road graph with 2,642 nodes (Perraudin et al., 2014) in Figure 1 (c)-(d).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 .</head><label>3</label><figDesc>Figure 3. Comparison of the spectral energy distribution (left) and the energy ratio curve (right) between the original graph and two perturbed graphs in the Amazon dataset.</figDesc><graphic url="image-1.png" coords="4,307.44,207.37,234.00,117.00" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 4 .</head><label>4</label><figDesc>Figure 4. Comparison between Heat wavelets and Beta wavelets in the spectral domain (left) and the spatial domain (right).</figDesc><graphic url="image-2.png" coords="6,55.44,67.05,233.96,168.97" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 5 .</head><label>5</label><figDesc>Figure 5. The performance of BWGNN on T-Finance and T-Social with different order C.</figDesc><graphic url="image-3.png" coords="8,307.44,318.64,233.96,103.98" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 6 .</head><label>6</label><figDesc>Figure 6. Comparison of performance on T-Finance (1%) with different anomaly degrees controlled by σ and α.</figDesc><graphic url="image-4.png" coords="9,55.44,306.97,233.96,155.97" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head></head><label></label><figDesc>the 'right-shift' phenomenon of spectral energy distributions and further rigorously justify the observation on a vanilla probabilistic model. Inspired by this fact, we propose Beta Wavelet Graph Neural network (BWGNN) to better capture anomaly information on graph. BWGNN leverages Beta graph wavelet to generate band-pass filters with good locality in spectral and spatial domains. Empirical results on four datasets show the superiority and scalability of our model.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 .</head><label>1</label><figDesc>Summary of dataset statistics and the comparison of anomaly effect on spectral energy distributions. The number in the parentheses is the lowest value of ∆S high on each dataset.</figDesc><table><row><cell></cell><cell></cell><cell cols="2">Statistics</cell><cell></cell><cell cols="2">Anomaly Effect (∆S high )</cell></row><row><cell>Dataset</cell><cell># Nodes</cell><cell># Edges</cell><cell cols="2">Anomaly(%) # Features</cell><cell>Drop-Anomaly</cell><cell>Drop-Random</cell></row><row><cell>Amazon</cell><cell>11,944</cell><cell>4,398,392</cell><cell>6.87%</cell><cell>25</cell><cell cols="2">-0.59% (-4.19%) 0.15% (-0.39%)</cell></row><row><cell>YelpChi</cell><cell>45,954</cell><cell>3,846,979</cell><cell>14.53%</cell><cell>32</cell><cell cols="2">-1.61% (-15.4%) -0.04% (-1.14%)</cell></row><row><cell>T-Finance</cell><cell>39,357</cell><cell>21,222,543</cell><cell>4.58%</cell><cell>10</cell><cell cols="2">-0.34% (-0.92%) 0.09% (-0.01%)</cell></row><row><cell>T-</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 .</head><label>2</label><figDesc>Experimental results of all compared methods on YelpChi and Amazon with 1% and 40% training ratios.</figDesc><table><row><cell>Dataset</cell><cell cols="2">YelpChi (1%)</cell><cell cols="2">YelpChi (40%)</cell><cell cols="2">Amazon (1%)</cell><cell cols="2">Amazon (40%)</cell></row><row><cell cols="9">Metric F1-macro AUC F1-macro AUC F1-macro AUC F1-macro AUC</cell></row><row><cell>MLP</cell><cell>53.90</cell><cell>59.83</cell><cell>57.57</cell><cell>66.52</cell><cell>74.68</cell><cell>83.62</cell><cell>79.17</cell><cell>89.80</cell></row><row><cell>SVM</cell><cell>60.47</cell><cell>62.92</cell><cell>70.77</cell><cell>70.37</cell><cell>83.49</cell><cell>81.62</cell><cell>90.71</cell><cell>90.51</cell></row><row><cell>GCN</cell><cell>52.48</cell><cell>54.06</cell><cell>54.31</cell><cell>56.51</cell><cell>67.93</cell><cell>82.85</cell><cell>67.47</cell><cell>83.49</cell></row><row><cell>ChebyNet</cell><cell>63.13</cell><cell>73.48</cell><cell>65.72</cell><cell>78.19</cell><cell>85.74</cell><cell>87.60</cell><cell>91.94</cell><cell>94.64</cell></row><row><cell>GAT</cell><cell>50.27</cell><cell>50.95</cell><cell>54.64</cell><cell>57.20</cell><cell>60.84</cell><cell>73.45</cell><cell>83.18</cell><cell>89.90</cell></row><row><cell>GIN</cell><cell>57.57</cell><cell>64.73</cell><cell>62.85</cell><cell>74.09</cell><cell>68.69</cell><cell>78.83</cell><cell>69.26</cell><cell>80.56</cell></row><row><cell>GraphSAGE</cell><cell>58.41</cell><cell>67.58</cell><cell>65.49</cell><cell>78.31</cell><cell>70.78</cell><cell>75.37</cell><cell>74.17</cell><cell>86.95</cell></row><row><cell>GWNN</cell><cell>59.10</cell><cell>67.16</cell><cell>65.29</cell><cell>75.32</cell><cell>87.01</cell><cell>85.37</cell><cell>91.00</cell><cell>93.19</cell></row><row><cell>GraphConsis</cell><cell>56.79</cell><cell>66.41</cell><cell>58.70</cell><cell>69.83</cell><cell>68.59</cell><cell>74.11</cell><cell>75.12</cell><cell>87.41</cell></row><row><cell>CAREGNN</cell><cell>62.18</cell><cell>75.07</cell><cell>63.32</cell><cell>76.19</cell><cell>68.78</cell><cell>88.69</cell><cell>86.39</cell><cell>90.53</cell></row><row><cell>PC-GNN</cell><cell>59.82</cell><cell>75.47</cell><cell>63.00</cell><cell>79.87</cell><cell>79.86</cell><cell>90.40</cell><cell>89.56</cell><cell>95.86</cell></row><row><cell>BWGNN (Homo)</cell><cell>61.15</cell><cell>72.01</cell><cell>71.00</cell><cell>84.03</cell><cell>90.92</cell><cell>89.45</cell><cell>92.29</cell><cell>98.06</cell></row><row><cell>BWGNN (Hetero)</cell><cell>67.02</cell><cell>76.95</cell><cell>76.96</cell><cell>90.54</cell><cell>83.83</cell><cell>86.59</cell><cell>91.72</cell><cell>97.42</cell></row><row><cell cols="4">average value of 5 runs with different random seeds. Please</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="4">refer to Appendix C for more implementation details.</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 3 .</head><label>3</label><figDesc>Experimental results and the overall training time (seconds) on the T-Finance and T-Social datasets with different training ratios.</figDesc><table><row><cell>Dataset</cell><cell cols="2">T-Finance (1%)</cell><cell cols="3">T-Finance (40%)</cell><cell cols="2">T-Social (0.01%)</cell><cell cols="3">T-Social (40%)</cell></row><row><cell cols="5">Metric F1-macro AUC F1-macro AUC</cell><cell>Time</cell><cell cols="4">F1-macro AUC F1-macro AUC</cell><cell>Time</cell></row><row><cell>MLP</cell><cell>61.00</cell><cell>82.93</cell><cell>70.57</cell><cell cols="2">87.15 13.32</cell><cell>50.03</cell><cell>56.35</cell><cell>50.35</cell><cell>56.96</cell><cell>986</cell></row><row><cell>SVM</cell><cell>67.69</cell><cell>71.47</cell><cell>76.23</cell><cell cols="2">78.16 145.11</cell><cell>57.69</cell><cell>50.06</cell><cell>-</cell><cell>-</cell><cell>&gt;1 day</cell></row><row><cell>GCN</cell><cell>54.11</cell><cell>57.30</cell><cell>70.74</cell><cell cols="2">64.43 23.98</cell><cell>49.23</cell><cell>59.04</cell><cell>59.88</cell><cell>87.35</cell><cell>1294</cell></row><row><cell>ChebyNet</cell><cell>77.20</cell><cell>85.53</cell><cell>80.81</cell><cell cols="2">88.45 26.13</cell><cell>52.59</cell><cell>70.02</cell><cell>64.77</cell><cell>85.52</cell><cell>1711</cell></row><row><cell>GAT</cell><cell>53.15</cell><cell>52.04</cell><cell>53.86</cell><cell cols="2">73.00 181.62</cell><cell>46.25</cell><cell>44.35</cell><cell>69.01</cell><cell>89.06</cell><cell>1596</cell></row><row><cell>GIN</cell><cell>58.25</cell><cell>68.86</cell><cell>65.23</cell><cell cols="2">80.02 32.39</cell><cell>58.32</cell><cell>70.61</cell><cell>61.74</cell><cell>79.72</cell><cell>2195</cell></row><row><cell>GraphSAGE</cell><cell>59.03</cell><cell>66.35</cell><cell>52.71</cell><cell cols="2">67.12 35.91</cell><cell>57.91</cell><cell>59.69</cell><cell>59.77</cell><cell>70.80</cell><cell>2230</cell></row><row><cell>GWNN</cell><cell>70.64</cell><cell>86.68</cell><cell>71.58</cell><cell cols="2">86.57 27.25</cell><cell>50.81</cell><cell>56.14</cell><cell>58.72</cell><cell>73.77</cell><cell>1992</cell></row><row><cell>GraphConsis</cell><cell>71.73</cell><cell>90.28</cell><cell>73.46</cell><cell cols="2">91.42 264.41</cell><cell>52.45</cell><cell>65.29</cell><cell>56.55</cell><cell>71.25</cell><cell>3495</cell></row><row><cell>CAREGNN</cell><cell>73.32</cell><cell>90.50</cell><cell>77.55</cell><cell cols="2">92.16 572.41</cell><cell>55.82</cell><cell>71.20</cell><cell>56.26</cell><cell>71.86</cell><cell>9159</cell></row><row><cell>PC-GNN</cell><cell>62.06</cell><cell>90.76</cell><cell>63.18</cell><cell cols="2">91.23 736.55</cell><cell>51.14</cell><cell>59.84</cell><cell>52.17</cell><cell cols="2">68.45 13958</cell></row><row><cell>BWGNN</cell><cell>84.89</cell><cell>91.15</cell><cell>86.87</cell><cell cols="2">94.35 31.98</cell><cell>75.93</cell><cell>88.06</cell><cell>83.98</cell><cell>95.20</cell><cell>2707</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 4 .</head><label>4</label><figDesc>Experimental results (Mean ± Std.) of compared methods on the YelpChi and Amazon datasets with 1% and 40% training ratios. 90±0.23 59.83±0.40 57.57±0.89 66.52±1.09 74.68±1.25 83.62±1.76 79.17±1.26 89.80±1.04 SVM 60.47±0.24 62.92±0.92 70.77±0.01 70.37±0.04 83.49±1.39 81.62±3.53 90.71±0.04 90.51±0.07 GCN 52.48±0.50 54.06±0.72 54.31±0.77 56.51±1.09 67.93±1.42 82.85±0.71 67.47±0.52 83.49±0.47 ChebyNet 63.13±0.50 73.48±0.74 65.72±0.48 78.19±0.63 85.74±1.67 87.60±0.61 91.94±0.29 94.64±0.53 GAT 50.27±2.31 50.95±1.39 54.64±2.19 57.20±0.24 60.84±2.47 73.45±1.26 83.18±2.91 89.90±0.95 GIN 57.57±1.15 64.73±1.73 62.85±0.76 74.09±1.06 68.69±4.12 78.83±3.82 69.26±2.45 80.56±2.99 GraphSAGE 58.41±2.12 67.58±1.69 65.49±1.84 78.31±2.14 70.78±3.85 75.37±2.49 74.17±1.37 86.95±2.74 GWNN 59.10±6.53 67.16±11.44 65.29±6.67 75.32±8.97 87.01±1.98 85.37±2.32 91.00±0.27 93.19±2.22 GraphConsis 56.79±2.72 66.41±3.41 58.70±2.00 69.83±3.02 68.59±3.41 74.11±3.53 75.12±3.25 87.41±3.34 CAREGNN 62.18±1.39 75.07±3.88 63.32±0.94 76.19±2.92 68.78±1.68 88.69±3.58 86.39±1.73 90.53±1.67 PC-GNN 59.82±1.42 75.47±0.98 63.00±2.30 79.87±0.14 79.86±5.65 90.40±2.05 89.56±0.77 95.86±0.14 BWGNN (Homo) 61.15±0.41 72.01±0.48 71.00±0.91 84.03±0.98 90.92±0.78 89.45±0.33 92.29±0.44 98.06±0.45 BWGNN (Hetero) 67.02±0.50 76.95±1.38 76.96±0.89 90.54±0.49 83.83±3.79 86.59±2.62 91.72±0.84 97.42±0.48</figDesc><table><row><cell>Dataset</cell><cell cols="2">YelpChi (1%)</cell><cell cols="2">YelpChi (40%)</cell><cell cols="2">Amazon (1%)</cell><cell cols="2">Amazon (40%)</cell></row><row><cell cols="2">Metric F1-macro</cell><cell>AUC</cell><cell>F1-macro</cell><cell>AUC</cell><cell>F1-macro</cell><cell>AUC</cell><cell>F1-macro</cell><cell>AUC</cell></row><row><cell>MLP 53.</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0">https://github.com/dmlc/dgl</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_1">https://github.com/safe-graph/DGFraud</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3" xml:id="foot_2">https://github.com/YingtongDou/CARE-GNN</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4" xml:id="foot_3">https://github.com/PonderLY/PC-GNN</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgements</head><p>The work described in this paper was supported by grants from HKUST(GZ) under a Startup Grant and HKUST-GZU Joint Research Collaboration Fund (Project No.: GZU22EG05).</p></div>
			</div>

			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Supplementary of "Rethinking Graph Neural Networks for Anomaly Detection"</head><p>A. Proof of Proposition 2</p><p>Proof. By the rotation invariance of Gaussian distributions,</p><p>As the all-the-one vector is the eigenvector for λ 1 = 0, we have x1 ∼ N (µ √ n, σ 2 ) and xi ∼ N (0, σ 2 ), ∀i ≠ 1. For simplicity, we denote</p><p>σ , 1) and z i ∼ N (0, 1), ∀i ≠ 1.</p><p>As w is nonnegative, we just have to focus on the monotonicity of</p><p>t 2 +w dt with respect to ρ. Due to Lemma 6, we can conclude that f (ρ) is log-concave function and the maximum attains at 0. Hence, the expectation of the inverse of low-frequency energy ratio E x [1 η k (x, L)] is monotonically increasing with the anomaly degree σ µ .</p><p>t 2 +w dt is a log-concave function and the maximum attains at 0.</p><p>Proof. By the following two log-concave preserving rules, it is not hard to get our result.</p><p>• The product of log-concave functions is still a log-concave function.</p><p>Combining with the fact that the density function of Gaussian distributions is log-concave, it is easy to get f (ρ) is logconcave. We omitted the details here. The next step is to argue the optimal solution. As f (ρ) is differentiable, we can interchange the partial and integral here, that is,</p><p>Rethinking Graph Neural Networks for Anomaly Detection</p><p>Then,</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Proof of Equation (1)</head><p>Proof. Note that we have</p><p>x T Lx according to spectral graph theory <ref type="bibr" target="#b45">(Spielman, 2007)</ref>. The detailed derivation of Equation ( <ref type="formula">1</ref>) is:</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Baselines and Implementation Details</head><p>The first group only considers node features without graph relations:</p><p>• MLP: a multi-layer perceptron network consisting of two linear layers with activation functions.</p><p>• SVM: <ref type="bibr" target="#b6">(Chang &amp; Lin, 2011)</ref>: a support vector machine with the Radial Basis Function (RBF) kernel.</p><p>The second group is general GNN models for node classification:</p><p>• GCN (Kipf &amp; Welling, 2017): a graph convolutional network using the first-order approximation of localized spectral filters on graphs. • ChebyNet <ref type="bibr" target="#b11">(Defferrard et al., 2016</ref>): a graph convolutional network which restricts convolution kernel to a Chebyshev polynomial. • GAT <ref type="bibr" target="#b47">(Veličković et al., 2017)</ref>: a graph attention network that employs the attention mechanism for neighbor aggregation. • GIN <ref type="bibr" target="#b53">(Xu et al., 2019b)</ref>, a GNN model connecting to Weisfeiler-Lehman (WL) graph isomorphism test.</p><p>• GraphSAGE <ref type="bibr" target="#b19">(Hamilton et al., 2017)</ref>: a GNN model based on a fixed sample number of the neighbor nodes.</p><p>• GWNN <ref type="bibr" target="#b52">(Xu et al., 2019a)</ref>: a graph wavelet neural network using heat kernels to generate wavelet transforms.</p><p>The third group is state-of-the-art methods for graph-based anomaly detection:</p><p>• GraphConsis <ref type="bibr" target="#b32">(Liu et al., 2020)</ref>: a heterogeneous graph neural network which tackles context, feature and relation inconsistency problem in graph anomaly detection. • CAREGNN <ref type="bibr" target="#b15">(Dou et al., 2020)</ref>: a camouflage-resistant GNN which enhances the aggregation process with three unique modules against camouflages and reinforcement learning. • PC-GNN <ref type="bibr" target="#b31">(Liu et al., 2021b</ref>): a GNN-based imbalanced learning method to solve the class imbalance problem in graph-based fraud detection via resampling.</p><p>MLP is implemented by PyTorch <ref type="bibr" target="#b40">(Paszke et al., 2019)</ref>, and SVM is in Scikit-learn <ref type="bibr" target="#b41">(Pedregosa et al., 2011)</ref>. For GCN, ChebyNet, GAT, and GraphSAGE, we use the implementation of DGL 1 . GWNN is implemented by ourselves since the source code does not support the fast algorithm. For GraphConsis 2 , CAREGNN 3 , and PC-GNN 4 , we use the code provided by the authors. Our model is implemented based on PyTorch and DGL <ref type="bibr" target="#b49">(Wang et al., 2019b)</ref>. We conduct all the experiments on a high performance computing server running Ubuntu 20.04 with a Intel(R) Xeon(R) Gold 6226R CPU and 64GB memory.</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Beta wavelets. synthesis and application to lossy image compression</title>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">B</forename><surname>Amar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Zaied</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Alimi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in Engineering Software</title>
		<imprint>
			<biblScope unit="volume">36</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="459" to="474" />
			<date type="published" when="2005">2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">Analyzing the expressive power of graph neural networks in a spectral perspective</title>
		<author>
			<persName><forename type="first">M</forename><surname>Balcilar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Renton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Héroux</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Gaüzère</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Adam</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Honeine</surname></persName>
		</author>
		<editor>ICLR</editor>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Outlier aware network embedding for attributed networks</title>
		<author>
			<persName><forename type="first">S</forename><surname>Bandyopadhyay</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Lokesh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">N</forename><surname>Murty</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
				<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">Computer vision and deep learning-based data anomaly detection method for structural health monitoring. Structural Health Monitoring</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Bao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="page" from="401" to="421" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">On the use of the coefficient of variation as a measure of diversity</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">G</forename><surname>Bedeian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">W</forename><surname>Mossholder</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Organizational Research Methods</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="285" to="297" />
			<date type="published" when="2000">2000</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Beyond lowfrequency information in graph convolutional networks</title>
		<author>
			<persName><forename type="first">D</forename><surname>Bo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Shen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
				<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Libsvm: a library for support vector machines</title>
		<author>
			<persName><forename type="first">C.-C</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C.-J</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACM transactions on intelligent systems and technology (TIST)</title>
				<imprint>
			<date type="published" when="2011">2011</date>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="1" to="27" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">Diffusion wavelets. Applied and computational harmonic analysis</title>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">R</forename><surname>Coifman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Maggioni</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2006">2006</date>
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="page" from="53" to="94" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Deterrent: Knowledge guided graph attention network for detecting healthcare misinformation</title>
		<author>
			<persName><forename type="first">L</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Seo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Tabar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">KDD</title>
				<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="492" to="502" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">The relationship between precision-recall and roc curves</title>
		<author>
			<persName><forename type="first">J</forename><surname>Davis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Goadrich</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
				<imprint>
			<date type="published" when="2006">2006</date>
			<biblScope unit="page" from="233" to="240" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">Compactly supported one-cyclic wavelets derived from beta distributions</title>
		<author>
			<persName><forename type="first">H</forename><surname>De Oliveira</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>De Araújo</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1502.02166</idno>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Convolutional neural networks on graphs with fast localized spectral filtering</title>
		<author>
			<persName><forename type="first">M</forename><surname>Defferrard</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Bresson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Vandergheynst</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">NeurIPS</title>
		<imprint>
			<biblScope unit="page" from="3844" to="3852" />
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Deep anomaly detection on attributed networks</title>
		<author>
			<persName><forename type="first">K</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Bhanushali</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICDM. SIAM</title>
				<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Graph neural networks with adaptive frequency response filter</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Jalaian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><surname>Adagnn</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CIKM</title>
				<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="392" to="401" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Learning structural node embeddings via diffusion wavelets</title>
		<author>
			<persName><forename type="first">C</forename><surname>Donnat</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Zitnik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Hallac</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Leskovec</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">KDD</title>
				<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Enhancing graph neural network-based fraud detectors against camouflaged fraudsters</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Dou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">S</forename><surname>Yu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CIKM</title>
				<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="315" to="324" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Fast beta wavelet network-based feature extraction for image copy detection</title>
		<author>
			<persName><forename type="first">A</forename><surname>Eladel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Zaied</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">B</forename><surname>Amar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neurocomputing</title>
		<imprint>
			<biblScope unit="volume">173</biblScope>
			<biblScope unit="page" from="306" to="316" />
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Diffusion scattering transforms on graphs</title>
		<author>
			<persName><forename type="first">F</forename><surname>Gama</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Ribeiro</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Bruna</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
				<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Procedures for detecting outlying observations in samples</title>
		<author>
			<persName><forename type="first">F</forename><forename type="middle">E</forename><surname>Grubbs</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Technometrics</title>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1" to="21" />
			<date type="published" when="1969">1969</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">Inductive representation learning on large graphs</title>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">L</forename><surname>Hamilton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Ying</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Leskovec</surname></persName>
		</author>
		<editor>NeurIPS</editor>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="1025" to="1035" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Wavelets on graphs via spectral graph theory</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">K</forename><surname>Hammond</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Vandergheynst</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Gribonval</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Applied and Computational Harmonic Analysis</title>
		<imprint>
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="129" to="150" />
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<author>
			<persName><forename type="first">J</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Kamber</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Pei</surname></persName>
		</author>
		<title level="m">Data Mining: Concepts and Techniques</title>
				<imprint>
			<publisher>Morgan Kaufmann</publisher>
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
	<note>3rd edition</note>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">Learning arbitrary graph spectral filters via bernstein approximation</title>
		<author>
			<persName><forename type="first">M</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><surname>Bernnet</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2021">2021</date>
			<publisher>NeurIPS</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">An architecture of fast beta wavelet networks for image classification</title>
		<author>
			<persName><forename type="first">O</forename><surname>Jemai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Zaied</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">B</forename><surname>Amar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">M</forename><surname>Alimi</surname></persName>
		</author>
		<author>
			<persName><surname>Fbwn</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IJCNN</title>
				<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2010">2010</date>
			<biblScope unit="page" from="1" to="8" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title level="m" type="main">The advanced theory of statistics. The advanced theory of statistics</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">G</forename><surname>Kendall</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1946">1946</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Semi-supervised classification with graph convolutional networks</title>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">N</forename><surname>Kipf</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Welling</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
				<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Rev2: Fraudulent user prediction in rating platforms</title>
		<author>
			<persName><forename type="first">S</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Hooi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Makhija</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Faloutsos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Subrahmanian</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">WSDM</title>
				<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="333" to="341" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Graph convolutional neural networks with complex rational spectral filters</title>
		<author>
			<persName><forename type="first">R</forename><surname>Levie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Monti</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Bresson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">M</forename><surname>Bronstein</surname></persName>
		</author>
		<author>
			<persName><surname>Cayleynets</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Signal Processing</title>
		<imprint>
			<biblScope unit="volume">67</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="97" to="109" />
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Deconvolutional networks on graph data</title>
		<author>
			<persName><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Cheng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NeurIPS</title>
				<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="page" from="21019" to="21030" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Deeper insights into graph convolutional networks for semi-supervised learning</title>
		<author>
			<persName><forename type="first">Q</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Wu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
				<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="3538" to="3545" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Intention-aware heterogeneous graph attention networks for fraud transactions detection</title>
		<author>
			<persName><forename type="first">C</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Ao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">KDD</title>
				<imprint>
			<date type="published" when="2021">2021a</date>
			<biblScope unit="page" from="3280" to="3288" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Pick and choose: A gnn-based imbalanced learning approach for fraud detection</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Ao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Qin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Chi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>He</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Web Conference 2021</title>
				<meeting>the Web Conference 2021</meeting>
		<imprint>
			<date type="published" when="2021">2021b</date>
			<biblScope unit="page" from="3168" to="3177" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Alleviating the inconsistency problem of applying graph neural network to fraud detection</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Dou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">S</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Peng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SIGIR</title>
				<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="1569" to="1572" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">A comprehensive survey on graph anomaly detection with deep learning</title>
		<author>
			<persName><forename type="first">X</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Xue</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><forename type="middle">Z</forename><surname>Sheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Akoglu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Knowledge and Data Engineering</title>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">From amateurs to connoisseurs: modeling the evolution of user expertise through online reviews</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">J</forename><surname>Mcauley</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Leskovec</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">WWW</title>
				<imprint>
			<date type="published" when="2013">2013</date>
			<biblScope unit="page" from="897" to="908" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<monogr>
		<title level="m" type="main">Scattering GCN: overcoming oversmoothness in graph convolutional networks</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Min</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Wenkel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Wolf</surname></persName>
		</author>
		<editor>NeurIPS</editor>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Geometric scattering attention networks</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Min</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Wenkel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Wolf</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICASSP</title>
				<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">The application of data mining techniques in financial fraud detection: A classification framework and an academic review of literature</title>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">W</forename><surname>Ngai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><forename type="middle">H</forename><surname>Wong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Decision support systems</title>
		<imprint>
			<biblScope unit="volume">50</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="559" to="569" />
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Graph-based anomaly detection</title>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">C</forename><surname>Noble</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">J</forename><surname>Cook</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">KDD</title>
				<imprint>
			<date type="published" when="2003">2003</date>
			<biblScope unit="page" from="631" to="636" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<monogr>
		<author>
			<persName><forename type="first">H</forename><surname>Nt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Maehara</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1905.09550</idno>
		<title level="m">Revisiting graph neural networks: All we have is low-pass filters</title>
				<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Pytorch: An imperative style, high-performance deep learning library</title>
		<author>
			<persName><forename type="first">A</forename><surname>Paszke</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Gross</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Massa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Lerer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Bradbury</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Chanan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Killeen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Gimelshein</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Antiga</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">NeurIPS</title>
		<imprint>
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="page" from="8026" to="8037" />
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Scikit-learn: Machine learning in python</title>
		<author>
			<persName><forename type="first">F</forename><surname>Pedregosa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Varoquaux</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Gramfort</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Michel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Thirion</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Grisel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Blondel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Prettenhofer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Weiss</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Dubourg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of machine Learning research</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="page" from="2825" to="2830" />
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<monogr>
		<author>
			<persName><forename type="first">N</forename><surname>Perraudin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Paratte</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Shuman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Martin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Kalofolias</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Vandergheynst</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">K</forename><surname>Hammond</surname></persName>
		</author>
		<author>
			<persName><surname>Gspbox</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1408.5781</idno>
		<title level="m">A toolbox for signal processing on graphs</title>
				<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Collective opinion spam detection: Bridging review networks and metadata</title>
		<author>
			<persName><forename type="first">S</forename><surname>Rayana</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Akoglu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">KDD</title>
				<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="985" to="994" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Interpretable, multidimensional, multimodal anomaly detection with negative sampling for detection of device failure</title>
		<author>
			<persName><forename type="first">J</forename><surname>Sipple</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
				<imprint>
			<publisher>PMLR</publisher>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="9016" to="9025" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Spectral graph theory and its applications</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">A</forename><surname>Spielman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">48th Annual IEEE Symposium on Foundations of Computer Science (FOCS&apos;07)</title>
				<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2007">2007</date>
			<biblScope unit="page" from="29" to="38" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Anomaly detection for cybersecurity of the substations</title>
		<author>
			<persName><forename type="first">C.-W</forename><surname>Ten</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Hong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C.-C</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Smart Grid</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="865" to="873" />
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<monogr>
		<title/>
		<author>
			<persName><forename type="first">P</forename><surname>Veličković</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Cucurull</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Casanova</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Romero</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Lio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1710.10903</idno>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
	<note>Graph attention networks</note>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">A semi-supervised graph attentive network for financial fraud detection</title>
		<author>
			<persName><forename type="first">D</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Qi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICDM</title>
				<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2019">2019a</date>
			<biblScope unit="page" from="598" to="607" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<monogr>
		<title level="m" type="main">Deep graph library: A graphcentric, highly-performant package for graph neural networks</title>
		<author>
			<persName><forename type="first">M</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Ye</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Gan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Gai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Karypis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1909.01315</idno>
		<imprint>
			<date type="published" when="2019">2019b</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Simplifying graph convolutional networks</title>
		<author>
			<persName><forename type="first">F</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Souza</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Fifty</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Weinberger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
				<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="6861" to="6871" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<monogr>
		<title level="m" type="main">Beyond low-pass filtering: Graph convolutional networks with automatic filtering</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Zhang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2107.04755</idno>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">Graph wavelet neural network</title>
		<author>
			<persName><forename type="first">B</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Qiu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Cheng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In ICLR</title>
		<imprint>
			<date type="published" when="2019">2019a</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<monogr>
		<title level="m" type="main">How powerful are graph neural networks? ICLR</title>
		<author>
			<persName><forename type="first">K</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Leskovec</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Jegelka</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019">2019b</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">Error-bounded graph anomaly loss for gnns</title>
		<author>
			<persName><forename type="first">T</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Jiang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CIKM</title>
				<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="1873" to="1882" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<analytic>
		<title level="a" type="main">A synergistic approach for graph anomaly detection with pattern mining and feature learning</title>
		<author>
			<persName><forename type="first">T</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Shah</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Jiang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Neural Networks and Learning Systems</title>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
