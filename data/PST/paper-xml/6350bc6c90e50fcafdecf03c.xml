<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Museformer: Transformer with Fine-and Coarse-Grained Attention for Music Generation</title>
				<funder ref="#_egrfdFW">
					<orgName type="full">National Natural Science Foundation of China</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date type="published" when="2022-10-19">19 Oct 2022</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Botao</forename><surname>Yu</surname></persName>
							<email>btyu@foxmail.com</email>
							<affiliation key="aff0">
								<orgName type="laboratory">State Key Laboratory for Novel Software Technology</orgName>
								<orgName type="institution">Nanjing University</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Peiling</forename><surname>Lu</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Rui</forename><surname>Wang</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Wei</forename><surname>Hu</surname></persName>
							<affiliation key="aff0">
								<orgName type="laboratory">State Key Laboratory for Novel Software Technology</orgName>
								<orgName type="institution">Nanjing University</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Xu</forename><surname>Tan</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Wei</forename><surname>Ye</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">National Engineering Research Center for Software Engineering</orgName>
								<orgName type="institution">Peking University</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Shikun</forename><surname>Zhang</surname></persName>
							<email>zhangsk@pku.edu.cn</email>
							<affiliation key="aff1">
								<orgName type="department">National Engineering Research Center for Software Engineering</orgName>
								<orgName type="institution">Peking University</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Tao</forename><surname>Qin</surname></persName>
							<email>taoqin@microsoft.com</email>
						</author>
						<author>
							<persName><forename type="first">Tie-Yan</forename><surname>Liu</surname></persName>
							<email>tyliu@microsoft.com</email>
						</author>
						<author>
							<persName><forename type="first">Microsoft</forename><surname>Research</surname></persName>
						</author>
						<author>
							<affiliation key="aff2">
								<orgName type="institution">Microsoft Research Asia</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Museformer: Transformer with Fine-and Coarse-Grained Attention for Music Generation</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2022-10-19">19 Oct 2022</date>
						</imprint>
					</monogr>
					<idno type="arXiv">arXiv:2210.10349v1[cs.SD]</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-01-03T08:49+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Symbolic music generation aims to generate music scores automatically. A recent trend is to use Transformer or its variants in music generation, which is, however, suboptimal, because the full attention cannot efficiently model the typically long music sequences (e.g., over 10,000 tokens), and the existing models have shortcomings in generating musical repetition structures. In this paper, we propose Museformer, a Transformer with a novel fine-and coarse-grained attention for music generation. Specifically, with the fine-grained attention, a token of a specific bar directly attends to all the tokens of the bars that are most relevant to music structures (e.g., the previous 1st, 2nd, 4th and 8th bars, selected via similarity statistics); with the coarse-grained attention, a token only attends to the summarization of the other bars rather than each token of them so as to reduce the computational cost. The advantages are two-fold. First, it can capture both music structure-related correlations via the fine-grained attention, and other contextual information via the coarse-grained attention. Second, it is efficient and can model over 3? longer music sequences compared to its full-attention counterpart. Both objective and subjective experimental results demonstrate its ability to generate long music sequences with high quality and better structures. 1  </p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Symbolic music generation aims at generating music scores automatically and has drawn more and more attention in recent years <ref type="bibr" target="#b0">[1]</ref><ref type="bibr" target="#b1">[2]</ref><ref type="bibr" target="#b2">[3]</ref>. Since music can be represented in organized sequences of discrete tokens just like text, Transformer-based models, which have been demonstrated to work well on text generation <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b4">5]</ref>, are increasingly applied in music generation <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b5">[6]</ref><ref type="bibr" target="#b6">[7]</ref><ref type="bibr" target="#b7">[8]</ref><ref type="bibr" target="#b8">[9]</ref><ref type="bibr" target="#b9">[10]</ref><ref type="bibr" target="#b10">[11]</ref> and have made great success. While the self-attention mechanism empowers Transformer to capture the complex correlations in music, there are two ubiquitous challenges to solve for this task: 1) Long sequence modeling. Music sequences are typically very long, especially for multi-instrument polyphonic music where the lengths can usually exceed 10,000. The quadratic complexity of full attention limits its scalability to that length. 2) Music structure modeling. Music has its unique structures, where a piece Synth. can usually repeat some patterns of a previous piece, occasionally with some variations, after either a short or a long distance (see Figure <ref type="figure" target="#fig_1">1</ref> for an example). Successfully generating reasonable structures would make the music more realistic just like human-made music.</p><formula xml:id="formula_0">? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? = 120 ? ?</formula><p>Although many Transformer variants in natural language processing (NLP) <ref type="bibr" target="#b11">[12]</ref> have been proposed to handle long sequences (the first challenge), they cannot well model the music structures (the second challenge). According to the basic principles of these models, we can classify them into two types. The first type is called local focusing. Models of this type, e.g., Transformer-XL <ref type="bibr" target="#b12">[13]</ref> and Longformer <ref type="bibr" target="#b13">[14]</ref>, mainly focus on part of the input sequence, and drop the rest tokens to reduce the cost. However, the parts that they focus on cannot contain many of the essential ranges important to music structures, and directly dropping the rest tokens may lead to losing some important information.</p><p>The second type is called global approximation. Models of this type such as Linear Transformer <ref type="bibr" target="#b14">[15]</ref> utilize linearized attention or sequence compression over the whole input sequence to approximate the token pair-wise attention. While the approximation effectively reduces the complexity, they cannot accurately capture the correlations between related parts and accordingly are inadequate in generating repetition structures. We will review more existing long-sequence Transformers in ?2. Directly applying these models to music generation is suboptimal, and it is desirable to design an efficient model that can well model long music sequences as well as their structures.</p><p>In this paper, we propose to unify the above two types of models, which can well fit the characteristics of music. Our motivation is based on the observation that the importance is not uniformly distributed over the music sequence, and thus we do not need to treat all the tokens equally. Intuitively, to generate music with repetition structures, the most important information the model should directly refer to when generating a music bar, lies in those bars that tend to be repeated in the current bar.</p><p>We call these bars structure-related bars. For the other bars that are less important, approximation should do the trick. To this end, the proposed fine-and coarse-grained attention takes two different schemes towards different bars: with the fine-grained attention, tokens of the structure-related bars are directly attended to, to well learn the structure-related correlations; with the coarse-grained attention, information of the other bars are captured via summarization <ref type="bibr" target="#b15">[16]</ref>, rather than attending to each token of them, to decrease the computation and space complexity, and meanwhile retain the necessary information of these bars. The structure-related bars are selected according to the statistics on human-made music. They are not necessarily the most recent and consecutive bars but can include distant ones, to cater to long-term structures in music. Our main contributions in this paper are summarized as follows:</p><p>? We propose Museformer, a Transformer model with a novel fine-and coarse-grained attention for music generation. It captures the correlations of structure-related bars via fine-grained attention for learning the music structures, as well as the necessary and concentrated information of the other bars through their summarization via coarse-grained attention.</p><p>? We propose to select the structure-related bars based on similarity statistics on human-made music, which can help decrease the perplexity and generate music that exhibits better structures.</p><p>? The computation complexity and space complexity are reduced greatly to nearly linear in practice, which enables Museformer to scale up to long music sequences.</p><p>? Experimental results show that Museformer can generate music of full-song length with high quality and better structures.</p><p>2 Related Work</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Symbolic Music Generation</head><p>Symbolic music generation aims to exploit machines to compose music scores automatically. It has been attracting more and more people to work on it, and the solutions continuously evolve from rule-based models <ref type="bibr" target="#b0">[1]</ref> to probabilistic models <ref type="bibr" target="#b16">[17,</ref><ref type="bibr" target="#b17">18,</ref><ref type="bibr" target="#b1">2]</ref> to deep learning models <ref type="bibr" target="#b5">[6]</ref><ref type="bibr" target="#b6">[7]</ref><ref type="bibr" target="#b7">[8]</ref><ref type="bibr" target="#b8">[9]</ref><ref type="bibr" target="#b9">[10]</ref><ref type="bibr" target="#b10">[11]</ref>.</p><p>In recent years, the Transformer-based models have achieved great success in many text generation tasks <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b4">5]</ref>, thus have also been increasingly applied in the similar music generation tasks. Huang et al. <ref type="bibr" target="#b2">[3]</ref> apply Transformer in symbolic music generation for the first time and show that it can achieve better performance compared to previous deep learning models such as recurrent neural networks.</p><p>While Transformer shows promising results in music generation, the quadratic complexity of the attention mechanism limits its applications on the typically long music sequences. To resolve this problem, researchers on symbolic music generation come up with different solutions. One popular solution is to design a new representation method to represent musical information in fewer tokens, such as compound word <ref type="bibr" target="#b18">[19]</ref> and OctupleMIDI <ref type="bibr" target="#b19">[20]</ref>. While this solution indeed decreases the lengths of music sequences to a certain extent, full-song music sequences are still too long for Transformer to handle. Another solution is to use a long-sequence Transformer variant as the backbone. For example, Huang and Yang <ref type="bibr" target="#b20">[21]</ref> use Transformer-XL <ref type="bibr" target="#b12">[13]</ref>, and Hsiao et al. <ref type="bibr" target="#b18">[19]</ref> use Linear Transformer <ref type="bibr" target="#b14">[15]</ref>. Although these models can process long sequences, they are initially designed for NLP tasks and cannot well model the music structures. We will introduce and discuss more about the long-sequence Transformers in the following section.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Long-Sequence Transformers</head><p>Many Transformer variants have been proposed to tackle long-sequence tasks <ref type="bibr" target="#b11">[12]</ref>, which in general can be categorized into the following types. 1) Recurrent Transformer, which encodes sequences chunk by chunk <ref type="bibr" target="#b12">[13,</ref><ref type="bibr" target="#b21">22,</ref><ref type="bibr" target="#b22">23]</ref>. 2) Sparse attention, which sparsifies the attention layout with either predefined patterns <ref type="bibr" target="#b13">[14,</ref><ref type="bibr" target="#b23">24,</ref><ref type="bibr" target="#b24">25,</ref><ref type="bibr" target="#b15">16,</ref><ref type="bibr" target="#b25">26]</ref>, or with content-based patterns <ref type="bibr" target="#b26">[27]</ref><ref type="bibr" target="#b27">[28]</ref><ref type="bibr" target="#b28">[29]</ref><ref type="bibr" target="#b29">[30]</ref><ref type="bibr" target="#b30">[31]</ref>. 3) Linearized attention, which replaces the exponent of the inner product of features with the multiplication of feature maps <ref type="bibr" target="#b14">[15,</ref><ref type="bibr" target="#b31">[32]</ref><ref type="bibr" target="#b32">[33]</ref><ref type="bibr" target="#b33">[34]</ref>. 4) Compression-based attention, which reduces the number of queries or key-value pairs by compressing the contextual representations <ref type="bibr" target="#b34">[35]</ref><ref type="bibr" target="#b35">[36]</ref><ref type="bibr" target="#b36">[37]</ref>. In addition, some models also try to combine multiple techniques, such as Compressive Transformer <ref type="bibr" target="#b21">[22]</ref> that combines recurrent and compression-based methods, Poolingformer <ref type="bibr" target="#b35">[36]</ref> and Transformer-LS <ref type="bibr" target="#b37">[38]</ref> that combine sparse attention and compression-based methods.</p><p>Existing works on music generation directly adopt some of those long-sequence Transformers to process long music sequences, but it is suboptimal due to the unique structures of music. In general, music has many repeating or similar pieces, many of which are distant from each other, and the distance is measured by time units such as bar or beat instead of the number of tokens. Therefore, the receptive fields of the existing recurrent Transformers or sparse attention methods cannot cover the many ranges of structure-related content. Although the linearized or compression-based attention can cover the whole sequence, they do not precisely capture the correlation between each pair of tokens and may have shortcomings in generating repeating or similar music pieces.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Museformer</head><p>In order to well model long music sequences as well as their music structures, we propose Museformer. The general idea is that we do not need to focus on the whole sequence with the same importance level given that the complexity of pair-wise full attention is unacceptably high, but instead we combine two different attention schemes -fine-grained attention for the structure-related bars, and coarse-grained attention for the other bars. Museformer basically follows the original Transformer architecture <ref type="bibr" target="#b38">[39]</ref>,  and a novel fine-and coarse-grained attention (FC-Attention) is designed to replace the original self-attention module to tackle the challenges in long music sequence modeling.</p><formula xml:id="formula_1">1 s 1 s 2 s 2 s 3 s 3 s ?? ?? ?? 1,1 x 1,1 x 1,2 x 1,2 x 1,3 x 1,3 x 1,1 x 1,1 x 1,2 x 1,2 x 1,3 x 1,3 x 2,1 x 2,1 x 2,1 x 2,1 x 2,2 x 2,2 x 3,1 x 3,1 x 3,2 x 3,2 x 3,2 x 3,2 x 3,1 x 3,</formula><p>Museformer takes a music token sequence X = X 1 , . . . , X b as input, where b denotes the number of music bars, and the i-th bar X i = x i,1 , . . . , x i,|Xi| contains |X i | tokens. For the i-th bar where i = 1, . . . , b, we insert a summary token s i after it to facilitate local information aggregation in FC-Attention. The summary token sequence is denoted by S = s 1 , . . . , s b . After the insertion, the overall token sequence becomes X 1 , s 1 ? ? ? , X b , s b . The embedding layer embeds the overall token sequence into a vector space, and concatenates bar embeddings and beat embeddings as the positional information <ref type="bibr" target="#b9">[10]</ref>, followed by a linear projection. The Museformer layers then model the contextual representation, and the hidden state output by the last layer is fed into a softmax classifier to predict the next token.</p><p>In the following part, we first give a brief formulation of the attention mechanism ( ?3.1). Then, we introduce the details of FC-Attention ( ?3.2), followed by the selection of the structure-related bars ( ?3.3). Finally, we discuss the merits of Museformer ( ?3.4).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Preliminary: Attention</head><p>The attention mechanism <ref type="bibr" target="#b38">[39]</ref> is the basis of FC-Attention. It receives two sequential inputs, namely source X ? R nsrc?d and target X ? R ntgt?d , where n src and n tgt are the sequence lengths of source and target respectively, d is the embedding dimension. It computes contextual representation for each</p><formula xml:id="formula_2">x i ? R 1?d in target X as Attn(x i , X) = softmax x i W Q (XW K ) T ? d XW V ,<label>(1)</label></formula><p>where W Q , W K , W V ? R d?d are the trainable parameters. In practice, we employ the multi-head version of attention, but omit it from the equation for simplicity.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Fine-and Coarse-Grained Attention</head><p>The basic idea of FC-Attention is that, instead of directly attending to all the tokens which causes the quadratic complexity, a token of a specific bar only directly attends to the structure-related bars that are essential for generating structured music (fine-grained attention), and for the other bars, the token only attends to their summary tokens to obtain concentrated information (coarse-grained attention).</p><p>To achieve this, we first summarize the local information of each bar through the summarization step, and then aggregate the fine-grained and coarse-grained information through the aggregation step. Figure <ref type="figure" target="#fig_2">2</ref> visualizes the process.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Summarization</head><p>In the summarization step, we aggregate the information of each bar into the corresponding summary token. For the i-th bar, given the representation of summary token s i ? R 1?d , and that of the music tokens</p><formula xml:id="formula_3">X i = [x i,1 , . . . , x i,|Xi| ] ? R |Xi|?d , the summarization of this bar is si = Attn s i , [X i , s i ] ,<label>(2)</label></formula><p>where Attn(?) is defined in Equation <ref type="formula" target="#formula_2">1</ref>, [?] is the concatenation operation. In this step, each summary token attends to music tokens of its corresponding bar as well as the summary token itself.</p><p>Aggregation In the aggregation step, we aggregate the information of the tokens belonging to the structure-related bars or within the current bar, as well as the summarization of the other bars, so as to update the contextual representations of music tokens. The updated representation for x i,j is</p><formula xml:id="formula_4">xi,j = Attn x i,j , [X R(i) , X i,k?j , S R(i) ] ,<label>(3)</label></formula><p>where R(i) is the set of the indices of the structure-related bars with respect to the i-th bar, and R(i) is the set of the indices of other previous bars. X R(i) is the matrix formed by stacking</p><formula xml:id="formula_5">{X i | i ? R(i)}. Similarly, S R(i) is the matrix formed by stacking {s i | i ? R(i)}. X i,k?j is the matrix formed by stacking {x i,k | x i,k ? X i , k ? j}.</formula><p>In other words, the music token x i,j only attends to those music tokens that belong to the structure-related bars, and its previous tokens within the current bar. For the other bars, it only attends to their summary tokens.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Structure-Related Bar Selection</head><p>Considering that the structure-related bars are expected to contain those bars that are most likely to be repeated by the current bar to be generated, we propose to pinpoint them by similarity statistics, and the bars with high similarities should be selected. Specifically, for each song in the training set, we calculate the similarity between each pair of the bars, which is defined as</p><formula xml:id="formula_6">l i,j = |N (i) ? N (j)| |N (i) ? N (j)| ,<label>(4)</label></formula><p>where N (i) is the set of music notes within the i-th bar, and two notes are considered equal when their pitches, duration, and onset positions within their bars are all the same. The value of l i,j ranges from 0.0 to 1.0. If two bars are exactly the same, this value equals 1.0. We then calculate the average similarity of those bar pairs whose intervals are t over the training set D, which is formulated as</p><formula xml:id="formula_7">L t = Mean( D j=i+t l i,j ).<label>(5)</label></formula><p>In this paper, we call the distribution of the similarity with respect to the bar intervals the similarity distribution, and show that of the training data in Figure <ref type="figure" target="#fig_3">3</ref>. As we can see, it shows an obvious periodical pattern -a music bar tends to be more similar to its previous 2 bars, and also to the previous 4-th bar or its multipliers in most cases. We further conduct the similarity statistics on different datasets involving music of various genres and styles. The results shown in Appendix A interestingly indicate that this pattern is universally applicable to the music of the great diversity. We believe that it can be regarded as a general rule applicable to most music in our daily life.</p><p>Based on the statistical results, we carefully select 8 bars -the previous 1st, 2nd, 4th, 8th, 12th, 16th, 24th, and 32nd bars, as the default structured-related bars, since they can cover the most similar bars in most cases. The number of selected structure-related bars is a trade-off between efficiency and information richness. One can select more or other bars according to the computation resources and the specific similarity distribution of the used dataset. We have to admit that the selected bars cannot always cater to any song, but as long as it covers most cases, it can already enable Museformer to generate music with better structures.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Merits of Museformer</head><p>We explain why Museformer is suitable for the music generation task.</p><p>First, the receptive fields of the fine-grained attention comply with the characteristics of music and can cover most of the structure-related information. Unlike previous models that adopt fixed token-level patterns <ref type="bibr" target="#b13">[14,</ref><ref type="bibr" target="#b37">38]</ref> or learn content-based patterns <ref type="bibr" target="#b26">[27,</ref><ref type="bibr" target="#b30">31]</ref>, which may not cover the most important ranges to be referred to for generating music structures, we directly use the structure-related bars derived from human-made music. These bars can include both neighboring ones and distant ones, which enables Museformer to generate both short-term and long-term music structures.</p><p>Second, the coarse-grained attention can preserve necessary information for generating better music. In general, FC-Attention can be categorized into sparse attention, but unlike conventional sparse attention that simply drops a large amount of information, which may limit the model's capability, especially for music where mutual connections abound, the coarse-grained attention preserves the information of the other bars so as to provide rich clues for generation.</p><p>Third, the combination of the fine-and coarse-grained attention enables Museformer to handle long music sequences efficiently. Compared to the number of all the music tokens, the number of tokens in the structure-related bars and the summary tokens is much smaller, so the memory consumption and the running time are greatly reduced. It is essential for this task where sequences are quite long.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experiments</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Experiment Settings</head><p>We give a brief introduction to the experiment settings. Please refer to Appendix B for more details.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Dataset and Music Representation</head><p>We conduct our experiments on the widely used Lakh MIDI (LMD) dataset <ref type="bibr" target="#b39">[40]</ref>, which contains multi-instrument music in the format of MIDI. After preprocessing, the final dataset contains 29,940 songs (1,727 hours), each song contains 95 bars on average. Following <ref type="bibr" target="#b9">[10]</ref>, various instruments are merged into the 6 basic ones namely square synthesizer, piano, guitar, string, bass, and drum, with the square synthesizer playing the melody. We use a REMI-like representation method <ref type="bibr" target="#b20">[21]</ref> to transfer MIDIs into token sequences, where the musical information (instrument, bar line, note position, pitch, duration, etc.) is represented in separate tokens (see Figure <ref type="figure" target="#fig_1">1</ref> for an example). Each song is represented by 15,042 tokens on average, and each bar is represented by 158 tokens on average. We randomly split all the songs by 8/1/1 for training/validation/test, respectively.</p><p>Implementation Museformer is implemented with PyTorch<ref type="foot" target="#foot_0">2</ref> and fairseq <ref type="foot" target="#foot_1">3</ref> . For the efficient computation of FC-Attention, we write CUDA kernels to construct the attention layouts for each sample, and then transfer the layouts into a blocksparse form and compute with SparTA<ref type="foot" target="#foot_2">4</ref>  <ref type="bibr" target="#b40">[41]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Model and Training Configurations</head><p>The core parameters of Museformer include layer number = 4, hidden size = 512, number of attention heads = 8, and FFN hidden size = 2,048. During training, the batch size is set to 4 songs. Following <ref type="bibr" target="#b38">[39]</ref>, we use the Adam optimizer with ? 1 = 0.9, ? 2 = 0.98 and ? = 10 -9 , and the learning rate is warmed up over the first 16,000 steps linearly to a peak value of 5 ? 10 -4 , and then decreases based on the inverse square root of the steps. The L2 weight decay is set to 0.01. During inference, we use top-k sampling with k = 8. Generation continues until the end-of-sentence token is generated or it reaches the maximum length of 20,480.</p><p>Compared Models We compare Museformer with 4 representative Transformer-based models, most of which have been adopted in music generation:</p><p>? Music Transformer <ref type="bibr" target="#b2">[3]</ref>: a vanilla Transformer with a memory-efficient "skewing" relative position embedding implementation. ? Transformer-XL <ref type="bibr" target="#b12">[13]</ref>: a recurrent Transformer that encodes the sequence chunk by chunk and uses the gradient-stopped representations of previous chunks as the memory.</p><p>? Longformer <ref type="bibr" target="#b13">[14]</ref>: a model with a sliding window sparse attention.</p><p>? Linear Transformer <ref type="bibr" target="#b14">[15]</ref>: a model that uses a kernel-based attention of linear complexity.</p><p>All the compared models are set with comparable hyper-parameters as Museformer. For Music Transformer that uses the full attention and thus cannot model long sequences at once due to memory limit, we chunk each song into multiple samples during training, and apply the model to generate long sequences during validation and inference to test its generalization on long music sequences.</p><p>Objective Evaluation We use the following objective metrics to evaluate the models:</p><p>? Perplexity (PPL): a common metric to measure whether a generative model can correctly predict future tokens. The smaller, the better. To see the models' performances on different lengths, it is calculated on the first 1,024, 5,120, or 10,240 tokens of each sample. ? Similarity Error (SE): the error between the similarity distribution of training data and generated music, to evaluate the models' ability to generate music with realistic structures. It is defined as</p><formula xml:id="formula_8">SE = 1 T T t=1 | Lt -L t |,<label>(6)</label></formula><p>where Lt and L t are the average similarities (defined in Equation <ref type="formula" target="#formula_7">5</ref>) of the generated music and the training data respectively. We set T = 40 in our experiments, and Lt is calculated on 100 generated music pieces for each model. The smaller the value is, the more similar the structures of the generated music are to human-made music.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Subjective Evaluation</head><p>The most canonical way to evaluate a music generation model is the human listening test. We apply each model to randomly generate 100 music pieces, and invite 10 people, where 7 of them have music-related learning experiences, to score these music pieces. Specifically, for each participant, we randomly construct 5 groups, where each group contains 5 music pieces that are generated by Museformer and the 4 compared models, respectively. Participants are asked to score these music pieces from 1 (lowest) to 10 (highest) over the following subjective metrics:</p><p>? Musicality: whether it is pleasant and interesting, and real enough just like human-made music.</p><p>? Short-term structure: whether it shows good structures in neighboring content, such as good repetitions and reasonable development. ? Long-term structure: whether it shows good structures in long distances, such as song-level repetitions and long-distance connections. ? Overall: an overall score. We also calculate preference score based on this overall score, which is defined as the ratio of winning times (obtaining the highest overall score within a group).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Comparison with Previous Models</head><p>Table <ref type="table" target="#tab_1">1</ref> shows the results of the objective evaluation. As we can see: 1) Music Transformer achieves a comparable PPL as other models on short music sequences (1,024 tokens) but undergoes a severe deterioration on longer sequences, which implies that a model trained on short music sequences cannot Furthermore, we present the results of the subjective evaluation in Table <ref type="table" target="#tab_2">2</ref>, which show that Museformer achieves the best performance. Specifically, 1) Museformer gets the highest scores on all the metrics. 2) On the structure-related metrics, especially the long-term structure, Museformer exceeds other models by a large gap, indicating that the proposed FC-Attention can empower the model to capture the correlations in distant bars.</p><p>We further do pairwise comparisons over the subjective evaluation results. Table <ref type="table" target="#tab_3">3</ref> shows the number of wins/ties/losses based on the overall scores, as well as the p-values of the Wilcoxon signed rank test. Museformer obtains more wins than the compared models, and the p-values indicate that Museformer achieves statistically significant improvements (p &lt; 0.05). </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Ablation Study</head><p>We verify the effectiveness of the Museformer components as follows: 1) To see the effectiveness of the coarse-grained attention, we compare Museformer with the setting w/o coarse-grained, where each music token attends to the music tokens in the structure-related bars and its previous ones in the current bars, and no summary token of any bar. 2) To see the effectiveness of the structure-related bar selection for the fine-grained attention, we compare Museformer with the setting w/o bar selection, where we use the most recent 8 bars for the fine-grained attention instead of the structure-related bars selected via the similarity statistics.</p><p>From the results presented in the bottom block of Table <ref type="table" target="#tab_1">1</ref>, we can observe that: 1) Museformer consistently achieves better performances on both PPLs and SE compared to the two ablation settings, which demonstrates the effectiveness of the coarse-grained attention and the structure-related bar selection.</p><p>2) The coarse-grained attention preserves the necessary information of the bars other than the structured-related bars and can consistently help decrease the PPLs.</p><p>3) The contribution of the structure-related bar selection increases as the sequences get longer. This is reasonable because that in a longer music sequence, there tends to be more bars and more long-term structures, and the fine-grained attention can directly capture the correlations in the distant structure-related bars, which helps make more accurate predictions. 4) Embodied on SE, the bar selection is contributory to the structures of generated music. Please refer to Appendix C for more details. In addition to the above two settings, we have also tried disabling the fine-grained attention for previous bars, where a music token only directly attends to its previous tokens within its bar and attends to all the previous bars via the summary tokens. The result indicates that only summarized information and no precise token-level information for the previous bars is insufficient for the model to generate coherent music.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Complexity Analysis</head><p>Suppose the sequence length is n, the average bar number is b, the average bar length is m, and the number of selected structure-related bars is k. For FC-Attention, the time complexity of the summarization step is O(n), and that of the aggregation step is O (km + b)n , so the overall complexity is O (km + b)n = O (km + n/m)n . Although the complexity is still proportional to the square of n, the typically large divider m (mostly exceeds 100) greatly reduces the complexity. Thus, the specific complexity is between linear and quadratic. Note that the computation of FC-Attention depends on the input content, i.e., the number of bars and the number of tokens in each individual bar, the complexity cannot be precisely formulated, and the efficiency in real applications is more noteworthy.</p><p>To see the efficiency of FC-Attention in real applications, we train Museformer and its full attention counterpart on the validation set, and record their memory consumption and running time. With batch size = 1, we increase the maximum sequence length until we use up the 32GB memory of an Nvidia V100 GPU, and record the peak memory consumption <ref type="foot" target="#foot_3">5</ref> and the running time for one-epoch training.</p><p>Figure <ref type="figure" target="#fig_5">4</ref> shows the results. We observe that the memory consumption of Museformer increases at a nearly linear rate with respect to the maximum sequence length, and can process over 3? longer music sequences than its full attention counterpart, making it capable of generating full-song music. Museformer also achieves faster training time when the maximum sequence length is greater than around 5,000, which is common for music sequences.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5">Case Study</head><p>Figure <ref type="figure" target="#fig_6">5</ref> shows a snippet of a song generated by Museformer. As we can see, on the Strings track, the 13th -16th bars repeat the 9th -12th bars with an interval of 4 bars, which is a short-term structure. The 25th -32nd bars repeat the 9th -16th bars with an interval of 16 bars, and there are reasonable variations in the 27th bar compared to the 11th bar, which shows a long-term structure. This case demonstrates that Museformer can generate music with both short-term and long-term structures, and not only exact repetitions but also some variations. In addition to the two exhibited tracks, other tracks, such as piano and drum, have more variations on the two similar segments, which creates reasonable development of music. Please refer to our demo page for more information.    </p><formula xml:id="formula_9">? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ??</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusion</head><p>To solve the long sequence modeling and music structure modeling challenges in symbolic music generation, we propose Museformer with a novel fine-and coarse-grained attention. The fine-grained attention is applied to the structure-related bars for learning the structure-related correlations, and the coarse-grained attention is applied to the summarization of the other bars for getting a sketch of them. We propose to select the structure-related bars via bar-pair similarity statistics. Experimental results show that Museformer is efficient and can generate music with good quality and structures.</p><p>Museformer is not perfect yet, and we would like to discuss about its limitations and possible future explorations. First, since Museformer takes random samplings during inference and does not receive manual control, it can hardly ensure that every generated music piece is well-structured in an expected way. Techniques to enhance its reliability and controllability can be further explored. Furthermore, the musicality and creativity of the generated music are still far behind that of human-made music, which remains a problem for all the existing music generation models. We believe that more sophisticated music representation and large models trained on large-scale data can help alleviate this problem. Finally, we anticipate Museformer's adaptation to more tasks and domains. It can be easily and reasonably applied to music understanding tasks. For NLP tasks, since natural languages do not have the periodical patterns as music, how to determine the semantically related sentences, paragraphs, or documents is an interesting challenge to solve.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A Similarity Statistics on Different Datasets</head><p>We conduct the similarity statistics introduced in ?3.3 on the whole LMD dataset that we use, and exhibit the results in Figure <ref type="figure" target="#fig_7">6</ref>. We can observe that the periodical structure pattern, that a music bar tends to be more similar to its previous 2 bars, and also to the previous 4-th bar or its multipliers in most cases, is also satisfied on this set of music. Specifically, this rule holds on all of the instrument tracks except the drum track. This makes sense because the percussive instruments usually play the same rhythmic patterns over and over throughout a song. To see the structure pattern of music of different genres, we conduct the same similarity statistics on the Top-MAGD dataset <ref type="foot" target="#foot_4">6</ref> , which annotates altogether 13 music genres to the songs. Figure <ref type="figure" target="#fig_9">7</ref> shows that although different genres have their specific distributions, the general pattern is still applicable to all of these genres.  To see whether the pattern still holds on other styles of music, we conduct the statistics on the Symphony dataset <ref type="bibr" target="#b41">[42]</ref> and exhibit the distribution in Figure <ref type="figure" target="#fig_10">8</ref>. Since it is not easy to tell the melody tracks for the symphony music, the reported result is calculated over all the tracks. Due to the existence of some instruments that play more repetitions, e.g., drums, the differences among the similarities over the intervals are not that significant, but it still presents the same tendency. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B Details of Experiment Settings B.1 Dataset Construction</head><p>We use the LMD dataset <ref type="bibr" target="#b39">[40]</ref> in our experiments, and perform the following cleaning and processing to ensure the data quality:</p><p>? Track Compression: We first compress various tracks into 6 tracks <ref type="bibr" target="#b9">[10]</ref>, namely square synthesizer, piano, guitar, string, bass, and drum, with the square synthesizer playing the melody.</p><p>? Note Position and Duration Normalization: To ensure that the bar splitting is correct and the duration of notes recorded in MIDI files conforms to the musically perfect duration (e.g., quarter notes held for exact 1 beat, 8-th notes held for exact 0.5 beat), we use MuseScore<ref type="foot" target="#foot_5">7</ref> to normalize the note position and duration.</p><p>? Data Filtering: Since this dataset is crawled from the Internet and contains many samples of low quality, we use a set of heuristic rules presented in Table <ref type="table" target="#tab_5">4</ref> to filter them out and keep the samples of good quality and reasonable lengths.</p><p>? Pitch Normalization: We normalize the pitches to transfer the tonality to "C major" or "A minor". We then represent each MIDI file into a sequence of tokens using a REMI-like <ref type="bibr" target="#b20">[21]</ref> method. The bar lines are inferred automatically based on the time signature and the note onset positions. The statistics of the number of tokens and the number of bars are shown in Figure <ref type="figure" target="#fig_12">9</ref>. The average number   of tokens is 15,042, the average number of bars is 95, and the average number of tokens per bar is 158. Most samples are longer than 10,000 tokens, making it essential to design Museformer for efficiently modeling the long music sequences.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.2 Implementation Details</head><p>Museformer is equipped with a dynamic sparse attention mechanism, which requires constructing distinct attention layouts for each sample according to the token ranges of bars. Since the dynamic sparse attention layout is not regular as sliding windows <ref type="bibr" target="#b13">[14,</ref><ref type="bibr" target="#b24">25]</ref> or fixed-length blocks <ref type="bibr" target="#b23">[24]</ref>, it is very challenging to leverage the GPU parallel computation techniques to speed up training and decrease the memory consumption. Note that we cannot pad each bar into a fixed-length sequence because the lengths of bars vary drastically, and it would introduce a lot of padding tokens that lead to unacceptable sequence lengths. To achieve an efficient implementation, we utilize a blocksparse method that splits the attention layouts into fixed-size square blocks, and only computes those blocks where there is at least one query-key/value pair that expects computation. In our implementation, all the summary tokens of the input bars are put before the music tokens to facilitate the computation, and thus the summarization step and the aggregation step in FC-Attention can be transferred into computing summary-to-all (summary tokens attending to summary tokens and music tokens) and music-to-all (music tokens attending to summary tokens and music tokens) attention, respectively, and their attention layouts are shown in Figure <ref type="figure" target="#fig_1">10</ref>. We take the following 3 steps to compute each of the two attention processes: 1) attention layout generation to generate the full attention layout according to the bar ranges; 2) layout block-sparsification to transfer the full layout into a blocksprase layout; 3) blocksparse computation to compute the blocksparse multiplications and softmax operations.</p><p>Attention Layout Generation According to the bar splitting of each sample, we fill "true" on the corresponding areas of a Boolean tensor to construct the attention layout (shown on the left part of Figure <ref type="figure" target="#fig_1">10</ref>). To speed up the construction, we collect the begin and end indices of the bars ahead of time and write a CUDA kernel to fill it for all the bars simultaneously.</p><p>Layout Block-Sparsification As visualized in Figure <ref type="figure" target="#fig_1">10</ref>, the full attention layouts are blocksparsified with a fixed-size square (the block size set to 32 in our experiments), and diminished into the blocksparse layouts.</p><p>Blocksparse Computation We leverage SparTA <ref type="bibr" target="#b40">[41]</ref> to compute the blocksparse multiplications and softmax operations of attention. It only computes for the shaded areas on the blocksparse layouts in Figure <ref type="figure" target="#fig_1">10</ref>.</p><p>Since the attention layouts are the same for all the layers and heads in our setting, we only construct the attention layouts once for each sample, and cache them as well as the SparTA kernels for reuse, which saves a lot of memory and time. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.3 Detailed Model and Training Configurations</head><p>For Museformer and all the compared models, the basic model and training configurations introduced in ?4.1 are set the same. For Museformer involves two separate attention processes (summarization and aggregation), to make the parameter size comparable with other models, the projection matrices for projecting the targets (i.e., queries) are shared for the two attention processes, and we add different biases for summary tokens and music tokens respectively. It is the same for projecting the sources (i.e., keys and values), except that we use different projection parameters for SN(i) in Equation <ref type="formula" target="#formula_4">3</ref>. All the models have a comparable amount of trainable parameters: Museformer 16.1M, Music Transformer 16.6M, Transformer-XL 13.9M, Longformer 15.3M, Linear Transformer 13.2M, with acceptable differences due to different architectures and implementations.</p><p>All the models are trained on 4 Nvidia V100 32GB GPUs with fp16. Since in Museformer, a token directly attends to the tokens of 8 previous bars (the selected structure-related bars) and the current bar via the fine-grained attention, for a fair comparison, we set the window sizes for Longformer to 1,408, which is approximately the number of tokens that 9 bars contain. For Music Transformer that uses full attention and cannot process long sequences at once, we chunk the input sequence into blocks of fixed size 1,408, and manipulate the batch size and the update frequency to ensure that it is updated the comparable number of times within each epoch as other models. For Transformer-XL, the chunk size and the memory size are also set to 1,408. During inference, all the models are applied to generate sequences not shorter than 2,048 and not longer than 20,480, until the end-of-sentence token is generated. The top-k sampling is used, and k is set to 8.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C Similarity Distributions of Generated Music</head><p>Compared to other models (the compared models and the ablation settings), the music generated by Museformer has the similarity distribution most similar to that of the training data, as its SE reported in Table <ref type="table" target="#tab_1">1</ref> is the smallest. However, the value of SE may not comprehensively represent the structural characteristics, so we further display and discuss about the specific distributions in this section.</p><p>Figure <ref type="figure" target="#fig_1">11</ref> shows the similarity distributions. We can observe that: 1) The distribution of Museformer is very similar to that of the training data (shown in Figure <ref type="figure" target="#fig_3">3</ref>). The quantity is close, and the contour shows the same periodical pattern, i.e., the previous 2 bars, the previous 4-th bar as well as its multipliers, have relatively large similarities.</p><p>2) The distributions of Music Transformer and Linear Transformer show no periodical pattern. It implies that the Music Transformer model trained on short sequences cannot generate well-structured music of long lengths, and Linear Transformer cannot well capture the structure-related correlations even though its receptive field covers the whole sequence.</p><p>3) The distributions of Transformer-XL and Longformer show the tendency of the periodical pattern, and the similarity decreases as the interval increases in general. It indicates that the two models whose receptive fields only contain the most recent content have the ability to generate periodical repetitions of short distances but fall short for long-term structures. 4) It seems that compared to the quantity of the similarity, the contour (i.e., the periodical pattern) is more relevant to the human scoring on the structure-related metrics. The distributions of Transformer-XL and Longformer show the pattern, and their subjective scores on short-term and long-term structures (shown in Table <ref type="table" target="#tab_2">2</ref>) are relatively high, and are higher than Music Transformer and Linear Transformer whose distributions fail to show the periodical pattern. However, the quantity of the similarity may also influence human decisions. For example, the quantities of the similarities of Transformer-XL are relatively high, which indicates that too many repetitions are generated. For these samples, human scorers sometimes think them annoying and eventually give lower scores on musicality. </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: The music score of Twinkle, Twinkle, Little Star and its corresponding token representation. Every two consecutive bars on the Synth. track have the same rhythmic pattern, and the 9th -12th bars repeat the 1st -4th bars with an interval of 8 bars. Structures embodied as repetitions and variations are common in music.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: The fine-and coarse-grained attention. It shows a toy example of 3 bars, where for each bar, only the previous 1st bar is regarded as the structure-related bar.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: The similarity distribution of the training set used in our experiments. It is calculated over the melody track because melody best shows the structure of a song. We omit the similarity when t = 0 because the similarity between a bar and itself is always 1.0.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head></head><label></label><figDesc>Running time per iteration.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 4 :</head><label>4</label><figDesc>Figure 4: Memory consumption and running time of Museformer with FC-Attention compared to its full attention counterpart.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 5 :</head><label>5</label><figDesc>Figure 5: A snippet of a generated song.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure 6 :</head><label>6</label><figDesc>Figure 6: The similarity distribution of the LMD dataset we use.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>Figure 7 :</head><label>7</label><figDesc>Figure 7: The similarity distribution of the melody track of the different genres in TopMAGD.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head>Figure 8 :</head><label>8</label><figDesc>Figure 8: The similarity distribution of all tracks of the Symphony dataset.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_11"><head></head><label></label><figDesc>Statistics of number of bars.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_12"><head>Figure 9 :</head><label>9</label><figDesc>Figure 9: Length statistics. The vertical axes represent the ratios of those samples that are in the corresponding ranges.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_13"><head>? 1 ? 2 2 Figure 10 :</head><label>12210</label><figDesc>Figure10: Visualization of the blocksparse implementation. The full attention layouts are on the left, which are exactly the same layouts shown in Figure2bexcept that the summary tokens are put before the music tokens to facilitate our implementation. If the block size is 2, the corresponding blocksparse layouts are shown on the right. Note that it is only a toy example, and in real cases, the sequences are much longer, and the sparsity of the layouts is much larger what is depicted here.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_14"><head>5 )Figure 11 :</head><label>511</label><figDesc>Figure 11: The similarity distribution of the melody track of the music generated by different models.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 1 :</head><label>1</label><figDesc>The results of objective evaluation and ablation study. The numbers in the parentheses for PPLs are sequence lengths.</figDesc><table><row><cell></cell><cell cols="4">PPL (1,024) PPL (5,120) PPL (10,240) SE (%)</cell></row><row><cell>Music Transformer</cell><cell>1.66</cell><cell>1.77</cell><cell>2.55</cell><cell>2.49</cell></row><row><cell>Transformer-XL</cell><cell>1.64</cell><cell>1.45</cell><cell>1.43</cell><cell>15.66</cell></row><row><cell>Longformer</cell><cell>1.65</cell><cell>1.46</cell><cell>1.45</cell><cell>5.25</cell></row><row><cell>Linear Transformer</cell><cell>1.86</cell><cell>1.67</cell><cell>1.64</cell><cell>1.97</cell></row><row><cell>Museformer (ours)</cell><cell>1.64</cell><cell>1.41</cell><cell>1.35</cell><cell>0.95</cell></row><row><cell>w/o coarse-grained</cell><cell>1.65</cell><cell>1.42</cell><cell>1.38</cell><cell>1.08</cell></row><row><cell>w/o bar selection</cell><cell>1.65</cell><cell>1.43</cell><cell>1.39</cell><cell>6.39</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2 :</head><label>2</label><figDesc>The results of subjective evaluation. ST and LT stand for short-term and long-term respectively. For all the subjective metrics, mean ? standard deviation is reported. Pref stands for preference score.</figDesc><table><row><cell></cell><cell>Musicality</cell><cell>ST structure</cell><cell>LT structure</cell><cell>Overall</cell><cell>Pref</cell></row><row><cell>Music Transformer</cell><cell>6.00 ? 2.21</cell><cell>6.90 ? 1.76</cell><cell>5.30 ? 2.58</cell><cell>5.90 ? 1.90</cell><cell>0.20</cell></row><row><cell>Transformer-XL</cell><cell>6.10 ? 2.19</cell><cell>7.40 ? 1.81</cell><cell>6.26 ? 2.78</cell><cell>6.44 ? 2.01</cell><cell>0.34</cell></row><row><cell>Longformer</cell><cell>6.46 ? 1.81</cell><cell>7.60 ? 1.47</cell><cell>6.18 ? 2.54</cell><cell>6.44 ? 1.72</cell><cell>0.24</cell></row><row><cell cols="2">Linear Transformer 6.06 ? 1.99</cell><cell>6.92 ? 2.03</cell><cell>5.78 ? 2.64</cell><cell>6.30 ? 1.84</cell><cell>0.24</cell></row></table><note><p>Museformer (ours) 6.88 ? 1.95 7.86 ? 1.51 6.72 ? 2.74 7.12 ? 1.81 0.46 well generalize to long music sequences. Accordingly, an appropriate long-sequence Transformer model is needed for modeling the long music sequences. 2) Although the receptive field of Linear Transformer can cover the whole sequence, its PPLs show no superiority compared to other models, which may be because the kernel-based attention cannot accurately capture the complex correlations of music. 3) The proposed Museformer can consistently achieve the best PPLs on various sequence lengths, especially on a larger length, which demonstrates the effectiveness of Museformer on the music generation task. 4) The results on SE demonstrate that the music generated by Museformer has structures that are most similar to the human-made music. We provide the similarity distribution for each model and more discussion in Appendix C.</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 3 :</head><label>3</label><figDesc>The results of pairwise comparisons based on the subjective overall scores.</figDesc><table><row><cell></cell><cell cols="4">Wins Ties Losses p-value</cell></row><row><cell>Museformer VS Music Transformer</cell><cell>33</cell><cell>7</cell><cell>10</cell><cell>0.0003</cell></row><row><cell>Museformer VS Transformer-XL</cell><cell>25</cell><cell>14</cell><cell>11</cell><cell>0.0375</cell></row><row><cell>Museformer VS Longformer</cell><cell>28</cell><cell>8</cell><cell>14</cell><cell>0.0424</cell></row><row><cell>Museformer VS Linear Transformer</cell><cell>29</cell><cell>7</cell><cell>14</cell><cell>0.0128</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 4 :</head><label>4</label><figDesc>Our filtering rules.</figDesc><table><row><cell>Type</cell><cell>Rule</cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_0"><p>https://pytorch.org</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3" xml:id="foot_1"><p>https://github.com/pytorch/fairseq</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4" xml:id="foot_2"><p>https://github.com/microsoft/SparTA</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="5" xml:id="foot_3"><p>Obtained by torch.cuda.max_memory_allocated().</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="6" xml:id="foot_4"><p>http://www.ifs.tuwien.ac.at/mir/msd/TopMAGD.html</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="7" xml:id="foot_5"><p>https://musescore.org/</p></note>
		</body>
		<back>

			<div type="acknowledgement">
<div><head>Acknowledgments</head><p>This work is supported by <rs type="funder">National Natural Science Foundation of China</rs> (No. <rs type="grantNumber">61872172</rs>). We thank the <rs type="institution">SparTA group</rs>, the scorers in the subjective evaluation, and many other people for their kind help.</p></div>
			</div>
			<listOrg type="funding">
				<org type="funding" xml:id="_egrfdFW">
					<idno type="grant-number">61872172</idno>
				</org>
			</listOrg>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Automatic music composition with simple probabilistic generative grammars</title>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">A G</forename><surname>Salas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">F</forename><surname>Gelbukh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Calvo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><forename type="middle">G</forename><surname>Soria</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Polibits</title>
		<imprint>
			<biblScope unit="volume">44</biblScope>
			<biblScope unit="page" from="59" to="65" />
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">Classical music composition using state space models</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">K</forename><surname>Yanchenko</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Mukherjee</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1708.03822</idno>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Music transformer: Generating music with long-term structure</title>
		<author>
			<persName><forename type="first">C.-Z</forename><forename type="middle">A</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Simon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Hawthorne</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">M</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">D</forename><surname>Hoffman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Dinculescu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Eck</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of International Conference on Learning Representations (ICLR)</title>
		<meeting>International Conference on Learning Representations (ICLR)</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Improving language understanding by generative pre-training</title>
		<author>
			<persName><forename type="first">A</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Narasimhan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Salimans</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">OpenAI</title>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Pretrained language model for text generation: A survey</title>
		<author>
			<persName><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">X</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J.-R</forename><surname>Wen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of International Joint Conference on Artificial Intelligence (IJCAI)</title>
		<meeting>International Joint Conference on Artificial Intelligence (IJCAI)</meeting>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="4492" to="4499" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Lakhnes: Improving multiinstrumental music generation with cross-domain pre-training</title>
		<author>
			<persName><forename type="first">C</forename><surname>Donahue</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">H</forename><surname>Mao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><forename type="middle">E</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">W</forename><surname>Cottrell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">J</forename><surname>Mcauley</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of International Society for Music Information Retrieval Conference (ISMIR)</title>
		<meeting>International Society for Music Information Retrieval Conference (ISMIR)</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="685" to="692" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Symbolic music generation with transformer-gans</title>
		<author>
			<persName><forename type="first">A</forename><surname>Muhamed</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Yaddanapudi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Chi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Jackson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Suresh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><forename type="middle">C</forename><surname>Lipton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">J</forename><surname>Smola</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the AAAI Conference on Artificial Intelligence (AAAI)</title>
		<meeting>the AAAI Conference on Artificial Intelligence (AAAI)</meeting>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="page" from="408" to="417" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">A hierarchical latent vector model for learning long-term structure in music</title>
		<author>
			<persName><forename type="first">A</forename><surname>Roberts</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Engel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Raffel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Hawthorne</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Eck</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of International Conference on Machine Learning (ICML)</title>
		<meeting>International Conference on Machine Learning (ICML)</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="volume">80</biblScope>
			<biblScope unit="page" from="4364" to="4373" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">Telemelody: Lyric-to-melody generation with a template-based two-stage method</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Ju</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Qin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T.-Y</forename><surname>Liu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2109.09617</idno>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Popmag: Pop music accompaniment generation</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Qin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T.-Y</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ACM International Conference on Multimedia (MM)</title>
		<meeting>ACM International Conference on Multimedia (MM)</meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="1198" to="1206" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Songmass: Automatic song writing with pre-training and alignment constraint</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Sheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Ye</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Qin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the AAAI Conference on Artificial Intelligence (AAAI)</title>
		<meeting>the AAAI Conference on Artificial Intelligence (AAAI)</meeting>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="page">805</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">A survey of transformers</title>
		<author>
			<persName><forename type="first">T</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Qiu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2106.04554</idno>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Transformer-XL: Attentive language models beyond a fixed-length context</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Carbonell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Salakhutdinov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of Annual Meeting of the Association for Computational Linguistics (ACL)</title>
		<meeting>Annual Meeting of the Association for Computational Linguistics (ACL)</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="2978" to="2988" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">Longformer: The long-document transformer</title>
		<author>
			<persName><forename type="first">I</forename><surname>Beltagy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">E</forename><surname>Peters</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Cohan</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2004.05150</idno>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Transformers are RNNs: Fast autoregressive transformers with linear attention</title>
		<author>
			<persName><forename type="first">A</forename><surname>Katharopoulos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Vyas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Pappas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Fleuret</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of International Conference on Machine Learning (ICML)</title>
		<meeting>International Conference on Machine Learning (ICML)</meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="5156" to="5165" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">Bp-transformer: Modelling long-range context via binary partitioning</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Ye</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Gan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Qiu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1911.04070</idno>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Analysis and synthesis of palestrina-style counterpoint using markov chains</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">M</forename><surname>Farbood</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Sch?ner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of International Computer Music Conference (ICMC)</title>
		<meeting>International Computer Music Conference (ICMC)</meeting>
		<imprint>
			<date type="published" when="2001">2001</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Harmonising chorales in the style of johann sebastian bach</title>
		<author>
			<persName><forename type="first">M</forename><surname>Allan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Master&apos;s</title>
		<imprint>
			<date type="published" when="2002">2002</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Compound word transformer: Learning to compose full-song music over dynamic directed hypergraphs</title>
		<author>
			<persName><forename type="first">W.-Y</forename><surname>Hsiao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J.-Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y.-C</forename><surname>Yeh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y.-H</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the AAAI Conference on Artificial Intelligence (AAAI)</title>
		<meeting>the AAAI Conference on Artificial Intelligence (AAAI)</meeting>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="page" from="178" to="186" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">MusicBERT: Symbolic music understanding with large-scale pre-training</title>
		<author>
			<persName><forename type="first">M</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Ju</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Qin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T.-Y</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of Findings of the Association for Computational Linguistics (ACL Findings)</title>
		<meeting>Findings of the Association for Computational Linguistics (ACL Findings)</meeting>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="791" to="800" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Pop music transformer: Beat-based modeling and generation of expressive pop piano compositions</title>
		<author>
			<persName><forename type="first">Y.-S</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y.-H</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ACM International Conference on Multimedia (MM)</title>
		<meeting>ACM International Conference on Multimedia (MM)</meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="1180" to="1188" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Compressive transformers for long-range sequence modelling</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">W</forename><surname>Rae</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Potapenko</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">M</forename><surname>Jayakumar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Hillier</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">P</forename><surname>Lillicrap</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of International Conference on Learning Representations (ICLR)</title>
		<meeting>International Conference on Learning Representations (ICLR)</meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">ERNIE-Doc: A retrospective long-document modeling transformer</title>
		<author>
			<persName><forename type="first">S</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Shang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of Annual Meeting of the Association for Computational Linguistics and International Joint Conference on Natural Language Processing (ACL-IJCNLP)</title>
		<meeting>Annual Meeting of the Association for Computational Linguistics and International Joint Conference on Natural Language Processing (ACL-IJCNLP)</meeting>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="2914" to="2927" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">Generating long sequences with sparse transformers</title>
		<author>
			<persName><forename type="first">R</forename><surname>Child</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Gray</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1904.10509</idno>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Big bird: Transformers for longer sequences</title>
		<author>
			<persName><forename type="first">M</forename><surname>Zaheer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Guruganesh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">A</forename><surname>Dubey</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Ainslie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Alberti</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Ontanon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Pham</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Ravula</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Ahmed</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of Advances in Neural Information Processing Systems (NeurIPS)</title>
		<meeting>Advances in Neural Information Processing Systems (NeurIPS)</meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page">297</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Image transformer</title>
		<author>
			<persName><forename type="first">N</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Ku</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Tran</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of International Conference on Machine Learning (ICML)</title>
		<meeting>International Conference on Machine Learning (ICML)</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="volume">80</biblScope>
			<biblScope unit="page" from="4055" to="4064" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Reformer: The efficient transformer</title>
		<author>
			<persName><forename type="first">N</forename><surname>Kitaev</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Levskaya</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of International Conference on Learning Representations (ICLR)</title>
		<meeting>International Conference on Learning Representations (ICLR)</meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Efficient content-based sparse attention with routing transformers</title>
		<author>
			<persName><forename type="first">A</forename><surname>Roy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Saffar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Grangier</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Transactions of the Association for Computational Linguistics (TACL)</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="page" from="53" to="68" />
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Sac: Accelerating and structuring selfattention via sparse adaptive connection</title>
		<author>
			<persName><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Meng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of Advances in Neural Information Processing Systems (NeurIPS)</title>
		<editor>
			<persName><forename type="first">M</forename><surname>Larochelle</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">R</forename><surname>Ranzato</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">M</forename><surname>Hadsell</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">H</forename><surname>Balcan</surname></persName>
		</editor>
		<editor>
			<persName><surname>Lin</surname></persName>
		</editor>
		<meeting>Advances in Neural Information Processing Systems (NeurIPS)</meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page">8</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Sparse Sinkhorn attention</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Tay</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Bahri</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Metzler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D.-C</forename><surname>Juan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of International Conference on Machine Learning (ICML)</title>
		<meeting>International Conference on Machine Learning (ICML)</meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="volume">119</biblScope>
			<biblScope unit="page" from="9438" to="9447" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
		<title level="m" type="main">Smart bird: Learnable sparse attention for efficient and effective transformer</title>
		<author>
			<persName><forename type="first">C</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Jiao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Xie</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2108.09193</idno>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
		<title level="m" type="main">Masked language modeling for proteins via linearly scalable long-context transformers</title>
		<author>
			<persName><forename type="first">K</forename><surname>Choromanski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Likhosherstov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Dohan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Gane</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Sarlos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Hawkins</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Davis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Belanger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Colwell</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2006.03555</idno>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
		<title level="m" type="main">Rethinking attention with performers</title>
		<author>
			<persName><forename type="first">K</forename><surname>Choromanski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Likhosherstov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Dohan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Gane</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Sarlos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Hawkins</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Davis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Mohiuddin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Kaiser</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2009.14794</idno>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Random feature attention</title>
		<author>
			<persName><forename type="first">H</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Pappas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Yogatama</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Schwartz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Smith</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Kong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of International Conference on Learning Representations (ICLR)</title>
		<meeting>International Conference on Learning Representations (ICLR)</meeting>
		<imprint>
			<biblScope unit="page">2021</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<monogr>
		<title level="m" type="main">Linformer: Self-attention with linear complexity</title>
		<author>
			<persName><forename type="first">S</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">Z</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Khabsa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Ma</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2006.04768</idno>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Poolingformer: Long document modeling with pooling attention</title>
		<author>
			<persName><forename type="first">H</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Lv</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Duan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of International Conference on Machine Learning (ICML)</title>
		<meeting>International Conference on Machine Learning (ICML)</meeting>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="volume">139</biblScope>
			<biblScope unit="page">446</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Fast transformers with clustered attention</title>
		<author>
			<persName><forename type="first">A</forename><surname>Vyas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Katharopoulos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Fleuret</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of Advances in Neural Information Processing Systems (NeurIPS), H. Larochelle, M. Ranzato</title>
		<editor>
			<persName><forename type="first">R</forename><surname>Hadsell</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">M</forename><surname>Balcan</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">H</forename><surname>Lin</surname></persName>
		</editor>
		<meeting>Advances in Neural Information Processing Systems (NeurIPS), H. Larochelle, M. Ranzato</meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page" from="21" to="665" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Longshort transformer: Efficient transformers for language and vision</title>
		<author>
			<persName><forename type="first">C</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Ping</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Shoeybi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Goldstein</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Anandkumar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Catanzaro</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of Advances in Neural Information Processing Systems (NeurIPS)</title>
		<meeting>Advances in Neural Information Processing Systems (NeurIPS)</meeting>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="page" from="17" to="723" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Attention is all you need</title>
		<author>
			<persName><forename type="first">A</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">N</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Polosukhin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of Advances in Neural Information Processing Systems (NIPS)</title>
		<meeting>Advances in Neural Information Processing Systems (NIPS)</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="5998" to="6008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<monogr>
		<title level="m" type="main">Learning-based methods for comparing sequences, with applications to audio-to-midi alignment and matching</title>
		<author>
			<persName><forename type="first">C</forename><surname>Raffel</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
		<respStmt>
			<orgName>Columbia University</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Sparta: Deep-learning model sparsity via tensor-with-sparsity-attribute</title>
		<author>
			<persName><forename type="first">N</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Zhou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of USENIX Symposium on Operating Systems Design and Implementation</title>
		<meeting>USENIX Symposium on Operating Systems Design and Implementation</meeting>
		<imprint>
			<date type="published" when="2022">2022</date>
			<biblScope unit="page" from="213" to="232" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<monogr>
		<title level="m" type="main">Symphony generation with permutation invariant language model</title>
		<author>
			<persName><forename type="first">J</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Sun</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2205.05448</idno>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
