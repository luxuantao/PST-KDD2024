<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Graph Classification by Mixture of Diverse Experts</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Fenyu</forename><surname>Hu</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Liping</forename><surname>Wang</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Shu</forename><surname>Wu</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Liang</forename><surname>Wang</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Tieniu</forename><surname>Tan</surname></persName>
						</author>
						<title level="a" type="main">Graph Classification by Mixture of Diverse Experts</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.2" ident="GROBID" when="2022-12-25T12:35+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Graph classification is a challenging research problem in many applications across a broad range of domains. In these applications, it is very common that class distribution is imbalanced. Recently, Graph Neural Network (GNN) models have achieved superior performance on various real-world datasets. Despite their success, most of current GNN models largely overlook the important setting of imbalanced class distribution, which typically results in prediction bias towards majority classes. To alleviate the prediction bias, we propose to leverage semantic structure of dataset based on the distribution of node embedding. Specifically, we present Graph-DIVE, a general framework leveraging mixture of diverse experts (i.e., graph classifiers) for imbalanced graph classification. With a divide-andconquer principle, GraphDIVE employs a gating network to partition an imbalanced graph dataset into several subsets. Then each expert network is trained based on its corresponding subset. Experiments on real-world imbalanced graph datasets demonstrate the effectiveness of GraphDIVE.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Graph classification aims to identify class labels of graphs in a dataset, which is a critical and challenging problem for a broad range of real-world applications <ref type="bibr" target="#b25">(Huang et al., 2016;</ref><ref type="bibr" target="#b13">Fey et al., 2018;</ref><ref type="bibr" target="#b63">Zhang &amp; Chen, 2019;</ref><ref type="bibr" target="#b29">Jia et al., 2020)</ref>. For instance, in chemistry, a molecule could be represented as a graph, where nodes denote atoms, and edges represent chemical bonds. Correspondingly, the classification of molecular graphs can help predict target molecular properties <ref type="bibr" target="#b24">(Hu et al., 2020)</ref>. Graph Neural Network (GNN) models have achieved outstanding performance in graph classification <ref type="bibr" target="#b62">(Ying et al., 2018;</ref><ref type="bibr" target="#b59">Xu et al., 2019;</ref><ref type="bibr" target="#b57">Wang et al., 2020;</ref><ref type="bibr" target="#b7">Corso et al., 2020)</ref>. Most of existing GNN models first transform nodes into low-dimensional dense embeddings to learn discriminative graph attributive and structural features, and then summarize all node embeddings to obtain a global representation of the graph. Finally, Multi-Layer Perceptrons (MLPs) are used to facilitate end-to-end training. Nevertheless, current GNN models largely overlook the important setting of imbalanced class distribution, which is ubiquitous in practical graph classification scenarios. For example, in OGBG-MOLHIV dataset <ref type="bibr" target="#b24">(Hu et al., 2020)</ref>, only about 3.5% of molecules can inhibit HIV virus replication. Figure <ref type="figure" target="#fig_0">1</ref> presents graph classification results of GCN <ref type="bibr" target="#b33">(Kipf &amp; Welling, 2017)</ref> and GIN <ref type="bibr" target="#b59">(Xu et al., 2019)</ref> on this dataset. Considering either test accuracy or cross-entropy loss, the classification performance of minority class falls far behind that of majority class, which indicates the necessity of boosting GNN from the perspective of imbalanced learning.</p><p>However, apart from suffering the well-known learning bias towards majority classes <ref type="bibr" target="#b50">(Sun et al., 2009;</ref><ref type="bibr" target="#b21">He &amp; Garcia, 2009;</ref><ref type="bibr" target="#b30">Kang et al., 2019)</ref>, the class-imbalanced learning problem is exacerbated on graph classification due to the following reasons: (I) structure diversity; (II) poor applicability in multi-task classification setting. First, structure diversity and the related out-of-distribution problem is very ubiquitous in real-world graph datasets <ref type="bibr" target="#b23">(Hu et al., 2019)</ref>. For example, when scientists want to forecast COVID-19 property, it will be difficult because COVID-19 is different from all known virus. This means typical imbalanced learning methods, such as re-sampling <ref type="bibr" target="#b3">(Chawla et al., 2002)</ref> and re-weighting <ref type="bibr" target="#b25">(Huang et al., 2016)</ref>, may no longer work arXiv: <ref type="bibr">2103.15622v1 [cs.</ref>LG] 29 Mar 2021 on graph datasets. Because of the potential over-fitting to minority classes, these imbalanced strategies are sensitive to fluctuations of minor classes, resulting in unstable performance. Second, in more common cases, such as drug discovery <ref type="bibr" target="#b24">(Hu et al., 2020;</ref><ref type="bibr" target="#b0">Bécigneul et al., 2020;</ref><ref type="bibr" target="#b44">Pan et al., 2015)</ref> and functional brain analysis <ref type="bibr" target="#b45">(Pan et al., 2016)</ref>, graph datasets contain multiple classification tasks due to need for predicting various properties of a graph simultaneously. For example, Tox21 Challenge<ref type="foot" target="#foot_0">1</ref> aims to predict whether certain chemical compounds are active for twelve pathway assays, which can be viewed as twelve-task binary classification setting. Existing imbalanced learning methods are originally designed for single-task classification. Therefore, it is difficult to apply existing imbalanced learning strategies <ref type="bibr" target="#b3">(Chawla et al., 2002;</ref><ref type="bibr" target="#b30">Kang et al., 2019;</ref><ref type="bibr" target="#b31">Kim et al., 2020)</ref> to multi-task setting.</p><p>To this end, we propose a novel imbalanced Graph classification framework with DIVerse Experts, referred to as GraphDIVE for brevity. At first, we leverage a gating network to capture the semantic structure of the dataset. As illustrated in Figure <ref type="figure">2</ref>, the semantically similar graphs are grouped into the same subset. Then, multiple classifiers, which are referred to as experts, are trained based on their corresponding subsets. Due to the difference in semantic structure, samples of majority class and minority class tend to be concentrated in different subsets. Obviously, for the subset containing most of the minority class (please refer to Subset 2 in Figure <ref type="figure">2</ref>), the imbalance phenomenon is alleviated. As a result, the performance of minority class get promoted.</p><p>We systematically study the effect of GraphDIVE on public benchmarks and obtain the following key observations: (I) existing imbalanced learning strategies are difficult to offer satisfactory improvement on graph datasets because of the graph diversity problem, (II) the performance of existing GNNs on graph classification can be further improved by appropriately modeling the imbalanced distribution, (III) capturing semantic structure of datasets can address the structure diversity problem and alleviate the bias towards majority classes, (IV) different gates are generally necessary for multi-task setting. Apart from these observations, GraphDIVE achieves state-of-the-art results in both singletask and multi-task settings. As an instance, on HIV and BACE benchmarks, GraphDIVE achieves improvements over GCN by 2.09% and 4.24%, respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Work</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.">Graph Neural Networks</head><p>Over the past few years, we have witnessed the fast development of graph neural networks. They process permutation- invariant graphs with variable sizes and learn to extract discriminative features through a recursive process of transforming and aggregating representations from neighbors. At first, GNNs were introduced by <ref type="bibr" target="#b17">(Gori et al., 2005)</ref> as a form of recurrent neural networks. Then, <ref type="bibr" target="#b2">Bronstein et al. (2017)</ref> define the convolutional operations using Fourier transformation and Laplacian matrix. GCN <ref type="bibr" target="#b33">(Kipf &amp; Welling, 2017)</ref> approximate the Fourier transformation process by truncating the Chebyshev polynomial to the first-order neighborhood. GraphSAGE <ref type="bibr" target="#b18">(Hamilton et al., 2017)</ref> samples a fixed number of neighbors and employs several aggregation functions. GAT <ref type="bibr" target="#b55">(Veličković et al., 2018)</ref> aggregates information from neighbors by using attention mechanism. DR-GCN <ref type="bibr" target="#b49">(Shi et al., 2020)</ref> applies class-conditioned adversarial network to alleviate bias in imbalanced node classification task. Recently, <ref type="bibr" target="#b4">Chen et al. (2020)</ref> try to tackle the over-smoothing problem by using initial residual and identity mapping. Apart from innovation on convolution filters, there are also two main branches in the research of graph classification. For one thing, Graph pooling methods, such as DiffPool <ref type="bibr" target="#b62">(Ying et al., 2018)</ref>, Graph U-nets <ref type="bibr" target="#b14">(Gao &amp; Ji, 2019)</ref>, and self-attention pooling <ref type="bibr" target="#b36">(Lee et al., 2019)</ref>, are developed to extract more global information. <ref type="bibr" target="#b42">Mesquita et al. (2020)</ref> take a step further in understanding the effect of local pooling methods. For another, there is recently a growing class of graph isomorphic methods <ref type="bibr" target="#b59">(Xu et al., 2019;</ref><ref type="bibr" target="#b43">Morris et al., 2019;</ref><ref type="bibr" target="#b7">Corso et al., 2020)</ref> which aim to quantity representation power of GNNs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.">Class-imbalanced Learning</head><p>Re-sampling. Re-sampling methods aim to balance the class distribution by controlling each class's sample frequencies. It can be achieved by over-sampling or undersampling <ref type="bibr" target="#b3">(Chawla et al., 2002;</ref><ref type="bibr" target="#b19">Han et al., 2005;</ref><ref type="bibr" target="#b22">He et al., 2008)</ref>. Nevertheless, traditional random sampling methods usually cause over-fitting in minority classes or under-fitting in majority classes. Recently, <ref type="bibr" target="#b30">Kang et al. (2019)</ref> propose to use re-sampling strategy in a two-stage training scheme. Besides, <ref type="bibr" target="#b39">Liu et al. (2020)</ref>, <ref type="bibr" target="#b31">Kim et al. (2020), and</ref><ref type="bibr" target="#b5">Chou et al. (2020)</ref> generate augmented samples to supplement minority classes, which can also be viewed as re-sampling methods. However, it is a non-trivial task to apply augmentation on graphs with variable sizes. These re-sampling methods are also unable to produce multiple predictions simultaneously in multi-task setting because different tasks are usually with different class distributions.</p><p>Re-weighting. Re-weighting methods generally assign different weights to different samples. Traditional scheme re-weights classes proportionally to the inverse of the class frequency, which tends to make optimization difficult under extremely imbalanced settings <ref type="bibr" target="#b25">(Huang et al., 2016;</ref><ref type="bibr" target="#b26">2019a)</ref>. Another line of work assigns weights according to the properties of each training instance. FocalLoss <ref type="bibr" target="#b38">(Lin et al., 2017)</ref> lowers the weights of the well-classified samples. GHM <ref type="bibr" target="#b37">(Li et al., 2019)</ref> improves FocalLoss by further lowering the weights of very large gradients considering outliers. However, these two kinds of methods need prerequisite of domain experts to hand-craft the loss function in specific task, which may restrict their applicability. Recently, <ref type="bibr" target="#b8">Cui et al. (2019)</ref> introduce the effective number of samples to put larger weights on minority classes. <ref type="bibr" target="#b51">Tan et al. (2020)</ref> propose an equalization loss function that randomly ignores gradients from majority classes. LDAM <ref type="bibr" target="#b56">(Wallach et al., 2020)</ref> introduces a label-distribution-aware loss function that encourages larger margins for minority classes. Despite the simplicity in implementation, re-weighting methods do not consider the semantic structure of datasets. So, they may not handle the graph structure diversity problem, causing unstable predictions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3.">Mixture of Experts</head><p>Mixture of Experts (MoE) is mainly based on divide-andconquer principle, in which the problem space is first divided and then is addressed by specialized experts <ref type="bibr" target="#b28">(Jacobs et al., 1991)</ref>. MoE has been explored by several researchers and has witnessed success in a wide range of applications <ref type="bibr" target="#b54">(TRESP, 2001;</ref><ref type="bibr" target="#b6">Collobert et al., 2002;</ref><ref type="bibr" target="#b41">Masoudnia &amp; Ebrahimpour, 2014;</ref><ref type="bibr" target="#b10">Eigen et al., 2013)</ref>. In recent years, there is a surge of interest of in incorporating MoE models to address challenging tasks in natural language processing and computer vision. <ref type="bibr" target="#b47">Shazeer et al. (2017)</ref> introduce a sparsely-gated MoE network, which improves the performance of machine translation and language modeling. <ref type="bibr" target="#b48">Shen et al. (2019)</ref> evaluate the translation quality and diversity of MoE models. <ref type="bibr" target="#b11">Fedus et al. (2021)</ref> simplify the computational costs of MoE and make it possible to train models with trillion parameters. In computer vision, there are some frameworks <ref type="bibr" target="#b15">(Ge et al., 2015;</ref><ref type="bibr">2016)</ref> which have demonstrated the effectiveness of MoE in fine-grained classification. Contrary to existing MoE methods which focus on increasing model capacity, we find MoE are surprisingly overlooked in graph machine learning and that MoE are especially appropriate for imbalanced graph classification task. We also propose two variants considering posterior and prior distributions for the gating function. There are also concurrent MoE works <ref type="bibr" target="#b40">(Ma et al., 2018;</ref><ref type="bibr" target="#b46">Qin et al., 2020;</ref><ref type="bibr" target="#b52">Tang et al., 2020a</ref>) that involve multi-task learning.</p><p>However, they do not consider the important and ubiquitous class-imbalance problem.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Proposed Method: GaphDIVE</head><p>The technical core of GraphDIVE is the notion to leverage the semantic structure of the graph dataset based on the node embedding distributions. This notion encourages the GNN to group the structurally different but property-similar graphs into the same subset. Then the minority classes are more likely to be classified correctly by certain experts. Our method is similar to traditional MoE <ref type="bibr" target="#b28">(Jacobs et al., 1991)</ref>, but with a distinct motivation for imbalanced graph classification. In the following, we first present preliminaries and then describe the algorithmic details of GraphDIVE. Finally, we present two model variants for multi-task setting.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Preliminaries</head><formula xml:id="formula_0">Let D = {(G 1 , Y 1 ), . . . , (G n , Y n )} denote training data, where G i = (A i , X i , E i )</formula><p>denotes a graph containing adjacency matrix, node attribute matrix and edge attribute matrix respectively. Y i = (y 1 , . . . , y T ) represents the labels of G i across T tasks. The task of graph classification is to learn a mapping f : G i → Y i . In this paper, we only consider the binary classification situation that exists widely in practical applications <ref type="bibr" target="#b60">(Yanardag &amp; Vishwanathan, 2015;</ref><ref type="bibr" target="#b24">Hu et al., 2020)</ref>. For the class-imbalanced problem, the number of instances of majority class is far more than that of minority class.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Architecture</head><p>The ubiquitous class-imbalanced problem of graph datasets brings a huge challenge to existing GNNs because the classifier will inevitably produce a biased prediction towards majority classes <ref type="bibr" target="#b50">(Sun et al., 2009;</ref><ref type="bibr" target="#b21">He &amp; Garcia, 2009;</ref><ref type="bibr" target="#b53">Tang et al., 2020b)</ref>. We conjecture that it might be too difficult for one classifier to discriminate all graphs. Inspired by Mixture of Experts (MoE) <ref type="bibr" target="#b28">(Jacobs et al., 1991)</ref>, we propose to assign different experts to different subsets. As illustrated in Figure <ref type="figure" target="#fig_1">3a</ref>, GraphDIVE consists of the following components.</p><p>Feature extractor. Similar to the practice in <ref type="bibr" target="#b24">Hu et al. (2020)</ref>, we design a five-layer graph convolution network to extract graph features. Formally, at the k-th layer, the representation of the node v is:</p><formula xml:id="formula_1">h (k) v = COMBINE (k) h (k−1) v , m (k) v , m (k) v = AGGREGATE (k) h (k−1) v , h (k−1) u , e uv ,<label>(1)</label></formula><p>where u ∈ N (v) denotes the neighbors of node v, h</p><p>v is the representation of v at the k-th layer, e uv is the feature vector of the edge between u and v, and m aggregated to node v. On top of the graph feature extractor, we summarize the global representation of a graph G by using a graph average pooling layer (i.e., readout function):</p><formula xml:id="formula_3">x = P ooling [GN N (G)] ,<label>(2)</label></formula><p>where x ∈ R d , and d denotes the hidden dimension of graph embedding. Notably, GraphDIVE is generic to the choice of underlying GNN. Without loss of generality, in this paper, we choose two commonly used methods GCN <ref type="bibr" target="#b33">(Kipf &amp; Welling, 2017)</ref> and GIN <ref type="bibr" target="#b59">(Xu et al., 2019)</ref> as feature extractor.</p><p>Mixture of diverse experts. Under the assumption that one classifier is difficult to learn the desired mapping in skewed distribution, we adopt a gating network to decompose the imbalanced graph dataset into several subsets. Then a diverse set of individual networks, referred to as experts, are trained for discriminating graphs in their corresponding subsets. This divide-and-conquer strategy makes the learning process easier for each expert, and thus alleviates the bias towards majority class.</p><p>Formally, given a graph with global representation x and label y. We introduce a latent variable z ∈ {1, 2, . . . , M }, where M represents the number of experts. In GraphDIVE, we decompose the likelihood p(y|x; Θ) as</p><formula xml:id="formula_4">p(y|x; Θ) = M z=1 p(y, z|x; Θ) = M z=1 p(z|x; Θ)p(y|z, x; Θ),</formula><p>(3) where Θ = {W g ∈ R d×M , W e ∈ R d×2 } denotes learnable parameters of gating network and expert networks. M z=1 p(z|x; Θ) = 1, and p(z|x; Θ) is the output of the gating network, indicating the prior probability to assign x to the z-th expert. p(y|z, x; Θ) represents output distribution of the z-th expert.</p><p>For simplicity, we implement each expert with one linear projection layer followed by a sigmoid function:</p><formula xml:id="formula_5">p(y|z, x; Θ) = σ(x W e ).</formula><p>(4)</p><p>More specifically, the gating network generates an inputdependent soft partition of the dataset based on cosine similarity between graphs and gating prototypes:</p><formula xml:id="formula_6">p(z|x; Θ) = sof tmax x W z g τ ,<label>(5)</label></formula><p>where τ is the temperature hyper-parameter tuning the distribution of z, and W z g is the z-th gating prototype. Since samples of minority class and majority class are usually different in semantics, they are likely to be grouped into different subsets. For the subset containing most of the minority class, the imbalanced problem phenomenon is much alleviated.</p><p>Besides, unlike existing imbalanced learning strategies which suffer fluctuations in graph structure, GraphDIVE can also group structurally-different but semantically-similar graphs into same subsets. In other words, the semantic structure of the dataset is captured by gating network. Hence, the proposed method can alleviate the above-mentioned structure diversity problem of graph datasets.</p><p>Prior or posterior distribution. Apart from using prior probability in Eq. ( <ref type="formula">3</ref>), we also consider a model variant using posterior probability as expert weights. According to Bayes' theorem, posterior probability can be calculated as:</p><formula xml:id="formula_7">p(z|x, y; Θ) = p(z|x; Θ)p(y|z, x; Θ) z p(z |x; Θ)p(y|z , x; Θ) .<label>(6)</label></formula><p>As opposed to prior distribution which considers only graph features, this Bayesian extension considers the information from both graph labels and experts. For the convenience of expression, We refer to these two model variants as GraphDIVE-pri and GraphDIVE-post.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Optimization</head><p>We first present the optimization regime of Bayesian variant. Since the training objective is to maximize the loglikelihood, the loss function can be formulated as following:</p><formula xml:id="formula_8">L post = − M z=1 p(z|x, y; Θ) log p(y|x, z; Θ)<label>(7)</label></formula><p>Noticing the interdependence between posterior distribution and prediction distribution from experts, we propose to use EM algorithm <ref type="bibr" target="#b9">(Dempster et al., 1977)</ref> to optimize Eq. ( <ref type="formula" target="#formula_7">6</ref>) and Eq. ( <ref type="formula" target="#formula_8">7</ref>) iteratively:</p><p>E-step: estimate the weight p(z|x, y) for each expert according Eq. ( <ref type="formula" target="#formula_7">6</ref>) under current value of parameters Θ.</p><p>M-step: update parameters Θ using stochastic gradient descent algorithm, and the gradient ∇ log p(y, z|x) is weighted by the estimated p(z|x, y). The gradients are blocked in the computation of posterior distribution.</p><p>What is more, similar to the practice in <ref type="bibr" target="#b20">Hasanzadeh et al. (2020)</ref>, we introduce a Kullback-Leibler (KL) divergence regularization term KL(p(z|x, y)||p(z|x; Θ)) into the final loss function:</p><formula xml:id="formula_9">L post = − M z=1</formula><p>p(z|x, y; Θ) log p(y|x, z; Θ)</p><formula xml:id="formula_10">+ λ * KL(p(z|x, y; Θ)||p(z|x; Θ)),<label>(8)</label></formula><p>where λ is a hyper-parameter which controls the extent of regularization. The above KL term ensures the posterior distribution does not deviate too far from the prior distribution. So we choose to compute gradients in M-step based on Eq. ( <ref type="formula" target="#formula_10">8</ref>).</p><p>For the optimization of GraphDIVE-pri, the posterior distribution p(z|x i , y i ; Θ) in Eq. ( <ref type="formula" target="#formula_10">8</ref>) is replaced with prior distribution p(z|x i ; Θ). And regularization item becomes zero. In this case, the gating network and the expert network can be jointly optimized according to the following objective:</p><formula xml:id="formula_11">L pri = − M z=1 p(z|x; Θ) log p(y|x, z; Θ).<label>(9)</label></formula><p>In experiment, we find both variants fit graph data well and demonstrate superior generalization ability. And detailed comparison can be found in Section 5.3.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4.">Adaptation to multi-task scenario</head><p>In real-life applications, researchers are likely to be confronted with imbalanced graph classification in multi-task setting. Multi-task setting increases the difficulty of imbalanced learning as one graph might be of the majority class for some tasks while being of minority class for the other tasks. Existing imbalanced learning methods are originally designed for single-task classification, so it is a non-trivial task to adapt them for multi-task setting. In this paper, we consider two options for multi-task scenario, which are illustrated in Figure <ref type="figure" target="#fig_1">3b</ref> and Figure <ref type="figure" target="#fig_1">3c</ref>.</p><p>Option I: Shared gates. This is the simplest adaptation variant, which uses shared gating weights for different tasks (see Figure <ref type="figure" target="#fig_1">3b</ref>). Compared with the model in Figure <ref type="figure" target="#fig_1">3a</ref>, the only difference is that each expert generates predictions for different tasks. This variant has the advantage that it reduces model parameters, especially in settings with many experts. However, empirically this variant does not work well in our setting. Our reasoning is that it implicitly assumes training instances obey the same label distributions across all tasks.</p><p>Option II: Individual gates. Another natural option is to assign different groups of gating weights for different tasks, as illustrated in 3c. Compared to the shared gates solution, this variant has several additional gating networks. Notably, it models task relationships in a sophisticated way. For two less related tasks, the sharing expert weights will be penalized, resulting in different expert weights instead.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Theoretical Analysis</head><p>In this section, theoretical analysis is provided from variational inference perspective to help understand why Graph-Dive works.</p><p>Assuming that an observed graph x is related to a latent variable z = {1, 2, . . . , M }, and p(z|x) denotes the probability of x locating in the z-th sub-region of feature space. Let q(z|x, y) denote a variational distribution to infer latent variable given observed data, and p(y|x, z; Θ) denotes the distribution of the prediction of each expert. As for the KL regularization term, we consider the simple case of λ = 1.</p><p>Then we prove the following theorem.</p><p>Theorem 1. In GraphDIVE, optimizing the final loss is equivalent to the optimization of the lower bound of log p(y|x):</p><formula xml:id="formula_12">log p(y|x; Θ) ≥ E q(z|x,y) log p(y|x, z; Θ) − KL(q(z|x, y)||p(z|x; Θ))<label>(10)</label></formula><p>Proof.</p><p>KL(q(z|x, y)||p(z|x, y; Θ))</p><p>= E q(z|x,y) log q(z|x, y) p(z|x, y; Θ) = E q(z|x,y) log q(z|x, y)p(y|x; Θ) p(z, y|x; Θ)</p><p>= log p(y|x; Θ) + E q(z|x,y) log q(z|x, y) p(y|x, z; Θ)p(z|x; Θ) = log p(y|x) − E q(z|x,y) log p(y|x, z; Θ) + KL(q(z|x, y)||p(z|x; Θ)</p><p>Notice that KL(q(z|x, y)||p(z|x, y; Θ)) ≥ 0, which concludes the proof.</p><p>Eq. ( <ref type="formula" target="#formula_12">10</ref>) provides a lower bound of log p(y|x). Graph-DIVE optimizes the evidence lower bound (ELBO) from the perspective of variational inference. The more closer is p(z|x, y; Θ) to q(z|x, y), the tighter the lower bound is. The first item on the right hand encourages q(z|x, y) to be high for experts which make good predictions. And the second item is the Kullback-Leibler divergence between the variational distribution and the prior distribution output by the gating network. With this term, the gating network considers both graph labels and experts' capacity when partitioning graph datasets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Experiments</head><p>In this section, we provide extensive experimental results of GraphDIVE on imbalanced graph classification datasets under both single-task and multi-task settings. The experimental results demonstrate superior performance of GraphDIVE over state-of-the-art models. Besides, we present a case study to demonstrate how the gating mechanism improve classification performance of minority class.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.">Datasets</head><p>We conduct experiments on the recently released largescale datasets of Open Graph Benchmark 2 (OGB) <ref type="bibr" target="#b24">(Hu et al., 2020)</ref>, which are more realistic and challenging than traditional graph datasets. More specifically, we choose six molecular graph datasets from OGB: BACE, BBBP, HIV, SIDER, CLIONTOX, and TOX21. These datasets cover different complex chemical properties, such as inhibition of human β-secretase, and blood-brain barrier penetration. All these datasets contains two classes for each task. Here we give a brief statistics of these datasets in Table <ref type="table" target="#tab_1">1</ref>, and a more detailed description can be found in <ref type="bibr" target="#b24">Hu et al. (2020)</ref>. For the multi-class classification task, please refer to supplementary materials for details.</p><p>2 https://ogb.stanford.edu/ Each graph in molecular graph dataset represents a molecule, where nodes are atoms, and edges are chemical bonds. Each node contains a 9-dimensional attribute vector, including atomic number and chirality, as well as other additional atom features such as formal charge and whether the atom is in the ring. Moreover, each edge contains a 3-dimensional attribute vector, including bond type, bond stereochemistry, and an additional bond feature indicating whether the bond is conjugated.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.">Experimental Setup</head><p>For a fair comparison, we implement our method and all baselines in the same experimental settings as <ref type="bibr" target="#b24">Hu et al. (2020)</ref>. For both single-task and multi-task datasets, we follow the original scaffold train-validation-test split with the ratio of 80/10/10. The scaffold splitting separates structurally different molecules into different subsets, which provides a more realistic estimate of the model performance in experimental settings <ref type="bibr" target="#b58">(Wu et al., 2018)</ref>. We run ten times for each experiment with random seed ranging from 0 to 9, and report the mean and standard deviation of test ROC-AUC for all methods.</p><p>For hyper-parameter setting, we set the embedding dimension to 300, number of layers to 5, and employ the same GNN backbone network structure. We train the model using Adam optimizer <ref type="bibr" target="#b32">(Kingma &amp; Ba, 2015)</ref> with initial learning rate of 0.001. For HIV and TOX21 datasets, we train the network for 120 epochs in light of the scale of the dataset.</p><p>Moreover, for all the other datasets, we train the model for 100 epochs. According to the average performance on the validation dataset, we use grid-search to find the optimal value for M (i.e., the number of experts) and λ. We set the hyper-parameter space of M as <ref type="bibr">[2,</ref><ref type="bibr">3,</ref><ref type="bibr">4,</ref><ref type="bibr">5,</ref><ref type="bibr">6,</ref><ref type="bibr">7,</ref><ref type="bibr">8]</ref> and We implement all our models based on PyTorch Geometric <ref type="bibr">(Fey &amp; Lenssen, 2019)</ref> and run all our experiments on a single NVIDIA GeForce RTX 2080 Ti 12GB.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3.">Single-task Graph Classification</head><p>Setting and baselines. For single task, we choose BACE, BBBP and HIV datasets, which are initially single-task binary classification. Besides, at random, we pick the third task of SIDER dataset.</p><p>We consider two representative and competitive graph neural networks as feature extractors: GCN <ref type="bibr" target="#b33">(Kipf &amp; Welling, 2017)</ref> and GIN <ref type="bibr" target="#b59">(Xu et al., 2019)</ref>. To verify the effectiveness of our method, we also compare the following strong and competitive methods: FLAG <ref type="bibr" target="#b35">(Kong et al., 2020)</ref>, GSN <ref type="bibr" target="#b1">(Bouritsas et al., 2020)</ref>, WEGL <ref type="bibr" target="#b34">(Kolouri et al., 2021)</ref>. For all these methods, we use official implementation and follow the original setting.</p><p>In addition, we compare our method with state-of-the-art methods that are designed for imbalanced or long-tailed problems, including FocalLoss <ref type="bibr" target="#b38">(Lin et al., 2017)</ref>, LDAM <ref type="bibr" target="#b56">(Wallach et al., 2020)</ref>, GHM <ref type="bibr" target="#b37">(Li et al., 2019)</ref>, and Decoupling <ref type="bibr" target="#b30">(Kang et al., 2019)</ref>. For FocalLoss, we follow the original parameter setting. For LDAM and Decoupling, official implementation is adopted, and we carefully tune the hyper-parameters since there is considerable difference between the graph classification datasets and image classification datasets that these methods originally designed for. What is more, for GHM method, we set the number of bins to 30, and momentum η as 0.9.</p><p>Comparison with SOTA GNNs. We report the ROC-AUC score of state-of-the-art GNN models in Table <ref type="table">2</ref>. Overall, we can see that the proposed GraphDIVE shows strong performance across all four datasets. GraphDIVE consistently outperforms other GNN models. Particularly, GraphDIVE achieves up to 12.82% absolute improvement over GCN on SIDER-3 dataset. The strong performance verifies the effectiveness of the proposed mixture of experts framework.</p><p>Besides, comparing GraphDIVE-post and GraphDIVE-pri, we observe that GraphDIVE-pri performs better. We suppose the reason is that the calculation of posterior distribution introduces bias. As formulated in Eq. ( <ref type="formula" target="#formula_7">6</ref>), posterior distribution considers the capacity of each expert and graph labels. However, the imbalanced distribution of graph labels makes each expert focus more on majority class, hindering the performance of GraphDIVE. On the contrary, Graphpri reliefs the confounder bias from labels and achieves relatively better results. In the following text, without specification, we refer to GraphDIVE-pri as GraphDIVE for simplicity.</p><p>Comparison with SOTA imbalanced learning methods. We also compare GraphDIVE and other state-of-the-art class-imbalanced learning methods. The results are shown in Table <ref type="table" target="#tab_2">3</ref>. Firstly, we find that GraphDIVE outperforms baseline models (i.e., GCN and GIN) consistently by considerable margins, which implies that the performance of existing GNNs on graph classification can be further improved by appropriately modeling the imbalanced distribution.</p><p>It is also worth noting that the state-of-the-art imbalanced learning methods, such as LDAM and Decoupling, do not seem to offer significant or stable improvements over baseline models. For example, Focal loss performs better than GCN on BACE and HIV dataset, but it is inferior to baseline on BBBP and SIDER-3 dataset. We suppose the reason is that either re-sampling or re-weighting methods make the model focus more on minority class, resulting in potential over-fitting to minority class. When there are distinct differences in test graphs and train graphs, existing imbalanced learning methods may fail to generate accurate predictions. Compared with existing imbalanced learning methods, GraphDIVE usually achieves a higher ROC-AUC score and lower standard deviation. In other words, Graph-DIVE maintains remarkable and stable improvements across different datasets. We attribute this property to the gating and expert networks, which capture semantic structure of the dataset and possess the model superior generalization ability. We also present a case study in Section 5.5 to illustrate the effectiveness of GraphDIVE. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.4.">Multi-task Graph Classification</head><p>For multi-task, we choose three datasets, including TOX21, CLINTOX and SIDER. Since there is no prior imbalanced learning research on multi-task graph classification and existing imbalanced learning strategies are difficult to be adapted to multi-task setting, we only compare GraphDIVE and baseline models. The results in Table <ref type="table" target="#tab_3">4</ref> show Graph-DIVE still advance the performance over GCN and GIN. Notably, the improvements under multi-task setting are not as remarkable as those under single-task setting. This observation is expected because different tasks may require the shared graph representation to optimize towards different directions. So the multi-task imbalanced graph classification is a challenging research direction. We also note that the individual-gate variant (i.e., DIVE-IG) generally performs better than shared-gate variant (i.e., DIVE-SG). Considering that one graph might be of the majority class for some tasks while being of minority class for the other tasks, this result indicates that different gates are necessary for multi-task setting.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.5.">Effectiveness Analysis for Diverse Experts</head><p>To evaluate the effectiveness of the diverse experts in Graph-DIVE, we study whether and why diverse experts can alleviate prediction bias towards majority class. More specifically, we report the classification accuracy of minority class in Figure <ref type="figure" target="#fig_2">4</ref>. Besides, we visualize different experts' predictions on BACE dataset under the setting with three and four experts, respectively. According to the number of samples assigned to each expert, we present a pie chart in Figure <ref type="figure" target="#fig_3">5</ref>.</p><p>We have the following observations. First of all, existing state-of-the-art re-weighting and re-sampling methods, such as LDAM and Decoupling, have marginal improvements in minority class. This is as expected since they are prone to over-fitting to minority class and cannot perfectly solve the structure diversity problem. Secondly, we observe that the proposed GraphDIVE outperforms baseline and existing imbalanced learning methods by a remarkable margin. These improvements suggest that GraphDIVE can successfully alleviate the prediction bias towards majority class and boost performance for minority class.   Moreover, we take a further step to analyze why Graph-DIVE can alleviate the bias. As shown in Figure <ref type="figure" target="#fig_3">5</ref>, Class 0 and Class 1 denote the majority class and minority class, respectively. It can be observed that the classification of different classes relies on different experts. For example, for the setting with three experts, Expert 1 dominates the classification for the minority class while Expert 2 dominates the classification for the majority class. This phenomenon demonstrates that the proposed GraphDIVE successfully captured the semantic structure, i.e., each expert is responsible for a subset of graphs. For the expert which dominates the classification of minority class, its corresponding subset contains more minority class. So training procedure of this expert is less likely to be biased towards majority class.</p><p>We also investigate the impact of expert numbers. Please refer to supplementary materials for more detailed information. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.6.">Effect of gating mechanism</head><p>To verify the effectiveness of our proposed gating mechanism, we conduct an ablation experiment on three singletask datasets. We select GCN as a feature extractor and assign four experts. We replace the gating network with simple arithmetic mean operation on predictions of multiple experts. The model is called Dive-mean accordingly. As Table <ref type="table" target="#tab_5">5</ref> shows, compared with GraphDIVE, Dive-mean (i.e., DIVE-no-gate) reduces test ROC-AUC by relative 61.33% on average. Dive-mean may even degrade the performance on BBBP dataset. These results verify the effectiveness of the gating network. Notably, DIVE-mean almost shares the same number of parameters as GraphDIVE. Comparing GCN and DIVE-mean, it can also be seen that simply adding network parameters by introducing more experts cannot bring enough improvement.</p><p>To sum up, the above results suggest that both the expert network (refer to Section 5.5) and the gating network are needed to boost the performance of imbalanced graph classification.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Conclusion</head><p>In this paper, we have introduced GraphDIVE, a mixture of diverse experts framework for imbalanced graph classification. GraphDIVE employs a gating network to learn a soft partition of the dataset, in which process the semantically different graphs are grouped into different subsets. Then diverse experts are trained based on their corresponding subsets. Considering whether to use prior distribution or posterior distribution, we design two model variants and investigate their effectiveness. Besides, we also extend another two variants specially for multi-task setting. The theoretical analysis shows that GraphDIVE can optimize the exact evidence lower bound with the above divide-andconquer principle. We have conducted comprehensive experiments using various real-world datasets and practical settings. GraphDIVE consistently advances the performance over various baselines and imbalanced learning methods. Further studies exhibited the rationality and effectiveness of GraphDIVE.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Influence of Expert Number</head><p>We investigate the impact of expert number on GraphDIVE. To be more specific, we experiment with two to eight experts.</p><p>The results on single-task datasets are shown in Figure <ref type="figure">6</ref>. From the figure, it can be found that GraphDIVE outperforms baseline methods at most of the time, meaning that the expert numbers can be selected in a wide range. Besides, the performance improves with the increased number of experts at first, which demonstrates that more experts increase the model capacity. With the number of experts increasing, the experts which dominate the classification of minority class are more likely to generate unbiased predictions. For example, when the number of experts are less than eight, the performance of GraphDIVE-GCN increases monotonously with the number of experts increasing on BBBP dataset. Nevertheless, too many experts will inevitably introduce redundant parameters to the model, leading to over-fitting as well.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Complexity Comparison</head><p>In Table <ref type="table" target="#tab_6">6</ref>, we report the clock time of different methods on test set. For GraphDIVE, we employ GCN as a feature extractor and assign three experts on all datasets. It can be seen that GraphDIVE only introduces marginal computation complexity compared with GCN. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Experiments on multi-class Text Classification</head><p>In order to further verify that GraphDIVE is general for other imbalanced graph classification applications, we conduct experiments on text classification, which is an imbalanced multi-class graph classification task. Figure <ref type="figure">7</ref> shows the class distribution of the widely used text classification datasets. It can be seen that these datasets have a long-tailed label distribution. <ref type="bibr" target="#b61">Yao et al. (2019)</ref> create a corpus-level graph which treats both documents and words as nodes in a graph. They calculate point-wise mutual information as word-word edge weights and use normalized TF-IDF scores as worddocument edge weights. TextLevelGCN <ref type="bibr" target="#b27">(Huang et al., 2019b)</ref> produces a text level graph for each input text, and transforms text classification into graph classification task. TextLevelGCN fulfils the inductive learning of new words and achieves state-of-the-art results. Hence, we use TextLevelGCN<ref type="foot" target="#foot_1">3</ref>  <ref type="bibr" target="#b27">(Huang et al., 2019b)</ref> as a feature extractor and closely follow the experimental settings as <ref type="bibr" target="#b27">Huang et al. (2019b)</ref>. From Table <ref type="table" target="#tab_7">7</ref>, we can see that GraphDIVE achieves better results across different datasets. This result indicates that GraphDIVE is not only applicable to molecular graphs, but also is beneficial to the imbalanced multi-class graph classification in text domain. </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 .</head><label>1</label><figDesc>Figure 1. Test accuracy and cross-entropy loss on OGBG-HIV dataset.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 3 .</head><label>3</label><figDesc>Figure 3. Model variants in different settings. The latter two variants are designed for multi-task setting. The crossing of the dotted and solid lines of the same color indicates multiplication operation.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 4 .</head><label>4</label><figDesc>Figure 4. Classification accuracy of minor class on BACE and BBBP dataset.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 5 .</head><label>5</label><figDesc>Figure 5. The weights of experts output by gating network on BACE dataset. The classification of different classes relies on different experts.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 6 .Figure 7 .</head><label>67</label><figDesc>Figure 6. Mean ROC-AUC vs number of experts on four datasets: (a)BACE, (b)BBBP, (c)HIV, (d)SIDER-3</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 1 .</head><label>1</label><figDesc>Statistics of OGB Datasets</figDesc><table><row><cell>Dataset</cell><cell>Size</cell><cell cols="3"># tasks Avg. Size Positive Ratio</cell></row><row><cell>BACE</cell><cell>1513</cell><cell>1</cell><cell>34.1</cell><cell>45.6%</cell></row><row><cell>BBBP</cell><cell>2039</cell><cell>1</cell><cell>24.1</cell><cell>23.5%</cell></row><row><cell>HIV</cell><cell>41127</cell><cell>1</cell><cell>25.5</cell><cell>3.5%</cell></row><row><cell>SIDER-task-3</cell><cell>1427</cell><cell>1</cell><cell>33.6</cell><cell>1.5%</cell></row><row><cell>CLINTOX</cell><cell>1477</cell><cell>2</cell><cell>26.2</cell><cell>6.97%</cell></row><row><cell>SIDER</cell><cell>1427</cell><cell>27</cell><cell>33.6</cell><cell>25.14%</cell></row><row><cell>Tox21</cell><cell>7831</cell><cell>12</cell><cell>18.6</cell><cell>7.52%</cell></row><row><cell cols="5">Table 2. Summary of classification ROC-AUC (%) results under</cell></row><row><cell cols="2">single task setting.</cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>BACE</cell><cell>BBBP</cell><cell>HIV</cell><cell>SIDER-3</cell></row><row><cell>GCN</cell><cell>79.15±1.44</cell><cell cols="3">68.87±1.51 76.06±0.97 36.11±11.40</cell></row><row><cell>GIN</cell><cell>72.97±4.00</cell><cell>68.17±1.48</cell><cell>75.58±1.4</cell><cell>30.90±7.68</cell></row><row><cell>GCN+FLAG</cell><cell>80.53±1.43</cell><cell cols="2">70.04±0.82 76.83±1.02</cell><cell>46.90±7.22</cell></row><row><cell>GIN+FLAG</cell><cell>80.02±1.68</cell><cell cols="2">68.60±1.27 76.54±1.14</cell><cell>42.09±9.76</cell></row><row><cell>GSN</cell><cell>76.53±4.54</cell><cell cols="2">67.90±1.86 77.99±1.00</cell><cell>37.26±5.14</cell></row><row><cell>WEGL</cell><cell cols="4">78.06 ± 0.91 68.27 ± 0.99 77.57±1.11 48.88 ± 12.69</cell></row><row><cell cols="2">GraphDIVE-post 81.10±1.86</cell><cell cols="2">69.65±0.94 76.51±1.01</cell><cell>48.29±5.99</cell></row><row><cell>GraphDIVE-pri</cell><cell>83.39±0.8</cell><cell cols="2">70.33±1.24 78.15±1.28</cell><cell>48.93±3.79</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 3 .</head><label>3</label><figDesc>Comparison between GraphDIVE and other imbalanced-learning methods. The ROC-AUC (%) values are reported.</figDesc><table><row><cell></cell><cell></cell><cell>GCN</cell><cell></cell><cell></cell><cell></cell><cell>GIN</cell><cell></cell></row><row><cell></cell><cell>BACE</cell><cell>BBBP</cell><cell>HIV</cell><cell>SIDER-3</cell><cell>BACE</cell><cell>BBBP</cell><cell>HIV</cell><cell>SIDER-3</cell></row><row><cell></cell><cell cols="8">79.15±1.44 68.87±1.51 76.06±0.97 36.11±11.40 72.97±4.00 68.17±1.48 75.58±1.40 30.90±7.68</cell></row><row><cell cols="9">Focal Loss 81.08±2.02 67.90±1.16 76.56±1.15 29.36±10.30 72.36±4.59 66.10±1.65 77.00±1.10 26.90±9.03</cell></row><row><cell>GHM</cell><cell cols="8">80.51±1.54 67.04±1.26 75.33±1.44 19.90±6.99 70.93±4.74 67.71±2.26 74.21±1.07 38.57±9.60</cell></row><row><cell>LDAM</cell><cell cols="8">78.91±2.10 67.08±0.94 76.58±1.69 38.50±9.99 75.35±2.94 65.89±2.05 74.57±3.46 37.76±8.46</cell></row><row><cell cols="9">Decoupling 80.01±1.01 68.42±1.46 76.43±1.39 34.24±17.02 73.72±4.55 67.40±1.85 75.69±1.43 35.07±12.19</cell></row><row><cell cols="9">GraphDIVE 83.39±0.80 70.33±1.24 78.15±1.28 48.93±3.79 78.41±1.75 68.77±1.24 77.64±1.01 42.60±5.02</cell></row><row><cell cols="5">the hyper-parameter space of λ as [0.001, 0.01, 0.1, 1., 10],</cell><cell></cell><cell></cell><cell></cell></row><row><cell>respectively.</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 4 .</head><label>4</label><figDesc>Summary of classification ROC-AUC (%) results under multi-task setting.</figDesc><table><row><cell></cell><cell>CLINTOX</cell><cell>SIDER</cell><cell>TOX21</cell></row><row><cell>GCN</cell><cell cols="3">91.73±1.73 59.60±1.77 75.29±0.69</cell></row><row><cell cols="4">GCN-DIVE-SG 90.47±1.38 59.48±0.96 74.17±0.40</cell></row><row><cell cols="4">GCN-DIVE-IG 91.98±1.32 59.62±0.90 75.81±0.65</cell></row><row><cell>GIN</cell><cell cols="3">88.14±2.51 57.60±1.40 74.91±0.51</cell></row><row><cell cols="4">GIN-DIVE-SG 82.02±6.75 57.57±1.25 72.78±0.60</cell></row><row><cell cols="4">GIN-DIVE-IG 89.52±1.08 58.78±1.28 74.96±0.60</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 5 .</head><label>5</label><figDesc>Ablation study on gating network.</figDesc><table><row><cell></cell><cell>BACE</cell><cell>BBBP</cell><cell>SIDER-3</cell></row><row><cell>GCN</cell><cell cols="3">79.15±1.44 68.87±1.51 36.11±11.40</cell></row><row><cell cols="4">DIVE-no-gate 80.99±0.92 68.12±0.59 41.71±7.50</cell></row><row><cell>GraphDIVE</cell><cell cols="3">82.41±1.37 69.39±1.21 49.62±4.44</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 6 .</head><label>6</label><figDesc>Complexity comparison between GraphDIVE and other GNNs. Test time (millisecond) are reported.</figDesc><table><row><cell></cell><cell cols="2">BACE BBBP</cell><cell>HIV</cell><cell>SIDER-3</cell></row><row><cell>GCN</cell><cell>55.16</cell><cell>89.96</cell><cell>1248.01</cell><cell>54.98</cell></row><row><cell>GIN</cell><cell>41.81</cell><cell>59.01</cell><cell>1019.72</cell><cell>40.95</cell></row><row><cell cols="2">GCN+FLAG 86.76</cell><cell>90.22</cell><cell>1427.95</cell><cell>65.67</cell></row><row><cell>GIN+FLAG</cell><cell>60.56</cell><cell>83.07</cell><cell>1224.01</cell><cell>60.63</cell></row><row><cell>GSN</cell><cell cols="3">621.62 824.41 16100.56</cell><cell>640.24</cell></row><row><cell>GraphDIVE</cell><cell>61.12</cell><cell>93.92</cell><cell>1267.65</cell><cell>64.78</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 7 .</head><label>7</label><figDesc>Accuracy on text classification datasets. We run all models 10 times and report mean results.</figDesc><table><row><cell>Model</cell><cell>R8</cell><cell>R52</cell><cell>Ohsumed</cell></row><row><cell>CNN</cell><cell>95.7 ± 0.5</cell><cell>87.6 ± 0.5</cell><cell>58.4 ± 1.0</cell></row><row><cell>LSTM</cell><cell>96.1 ± 0.2</cell><cell>90.5 ± 0.8</cell><cell>51.1 ± 1.5</cell></row><row><cell>Bi-LSTM</cell><cell>96.3 ± 0.3</cell><cell>90.5 ± 0.9</cell><cell>49.3 ± 1.0</cell></row><row><cell>fastText</cell><cell>96.1 ± 0.2</cell><cell>92.8 ± 0.1</cell><cell>57.7 ± 0.5</cell></row><row><cell>Text-GCN</cell><cell>97.0 ± 0.1</cell><cell>93.7 ± 0.1</cell><cell>67.7 ± 0.3</cell></row><row><cell cols="4">TextLevelGCN 97.67 ± 0.2 94.05 ± 0.4 68.59 ± 0.7</cell></row><row><cell>GraphDIVE</cell><cell cols="3">97.91 ± 0.2 94.31 ± 0.3 69.27 ± 0.6</cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0">https://tripod.nih.gov/tox21/challenge/</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3" xml:id="foot_1">https://github.com/mojave-pku/TextLevelGCN</note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Optimal transport graph neural networks</title>
		<author>
			<persName><forename type="first">G</forename><surname>Bécigneul</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O.-E</forename><surname>Ganea</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Barzilay</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Jaakkola</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI Conference on Artificial Intelligence</title>
				<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">Improving graph neural network expressivity via subgraph isomorphism counting</title>
		<author>
			<persName><forename type="first">G</forename><surname>Bouritsas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Frasca</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Zafeiriou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">M</forename><surname>Bronstein</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2006.09252</idno>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">Geometric deep learning: going beyond euclidean data</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">M</forename><surname>Bronstein</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Bruna</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Szlam</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Vandergheynst</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
		<respStmt>
			<orgName>IEEE Signal Processing Magazine</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Smote: synthetic minority over-sampling technique</title>
		<author>
			<persName><forename type="first">N</forename><forename type="middle">V</forename><surname>Chawla</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">W</forename><surname>Bowyer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">O</forename><surname>Hall</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">P</forename><surname>Kegelmeyer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of artificial intelligence research</title>
		<imprint>
			<biblScope unit="volume">16</biblScope>
			<biblScope unit="page" from="321" to="357" />
			<date type="published" when="2002">2002</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Simple and deep graph convolutional networks</title>
		<author>
			<persName><forename type="first">M</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 37th International Conference on Machine Learning</title>
				<meeting>the 37th International Conference on Machine Learning</meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Remix: Rebalanced mixup</title>
		<author>
			<persName><forename type="first">H.-P</forename><surname>Chou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S.-C</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J.-Y</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D.-C</forename><surname>Juan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
				<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="95" to="110" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">A parallel mixture of svms for very large scale problems</title>
		<author>
			<persName><forename type="first">R</forename><surname>Collobert</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural computation</title>
		<imprint>
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="1105" to="1114" />
			<date type="published" when="2002">2002</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Principal neighbourhood aggregation for graph nets</title>
		<author>
			<persName><forename type="first">G</forename><surname>Corso</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Cavalleri</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Beaini</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Liò</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Veličković</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
				<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Classbalanced loss based on effective number of samples</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T.-Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Belongie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
				<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="9268" to="9277" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Maximum likelihood from incomplete data via the EM algorithm</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">P</forename><surname>Dempster</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><forename type="middle">M</forename><surname>Laird</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">B</forename><surname>Rubin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of the Royal Statistical Society: Series B</title>
		<imprint>
			<biblScope unit="volume">39</biblScope>
			<biblScope unit="page" from="1" to="38" />
			<date type="published" when="1977">1977</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">Learning factored representations in a deep mixture of experts</title>
		<author>
			<persName><forename type="first">D</forename><surname>Eigen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Ranzato</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1312.4314</idno>
		<imprint>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">Switch transformers: Scaling to trillion parameter models with simple and efficient sparsity</title>
		<author>
			<persName><forename type="first">W</forename><surname>Fedus</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Zoph</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Shazeer</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2101.03961</idno>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Fast graph representation learning with PyTorch Geometric</title>
		<author>
			<persName><forename type="first">M</forename><surname>Fey</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">E</forename><surname>Lenssen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR Workshop on Representation Learning on Graphs and Manifolds</title>
				<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Fast geometric deep learning with continuous b-spline kernels</title>
		<author>
			<persName><forename type="first">M</forename><surname>Fey</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">E</forename><surname>Lenssen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Weichert</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Müller</surname></persName>
		</author>
		<author>
			<persName><surname>Splinecnn</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
				<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="869" to="877" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Graph u-nets</title>
		<author>
			<persName><forename type="first">H</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Ji</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
				<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="2083" to="2092" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Subset feature learning for fine-grained category classification</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Ge</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Mccool</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Sanderson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Corke</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition Workshops</title>
				<meeting>the IEEE Conference on Computer Vision and Pattern Recognition Workshops</meeting>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="46" to="52" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Fine-grained classification via mixture of deep convolutional neural networks</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Ge</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Bewley</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Mccool</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Corke</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Upcroft</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Sanderson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Winter Conference on Applications of Computer Vision (WACV)</title>
				<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="1" to="6" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">A new model for learning in graph domains</title>
		<author>
			<persName><forename type="first">M</forename><surname>Gori</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Monfardini</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Scarselli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Joint Conference on Neural Networks</title>
				<imprint>
			<date type="published" when="2005">2005</date>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="729" to="734" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Inductive representation learning on large graphs</title>
		<author>
			<persName><forename type="first">W</forename><surname>Hamilton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Ying</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Leskovec</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
				<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Borderline-smote: a new over-sampling method in imbalanced data sets learning</title>
		<author>
			<persName><forename type="first">H</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W.-Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B.-H</forename><surname>Mao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International conference on intelligent computing</title>
				<imprint>
			<date type="published" when="2005">2005</date>
			<biblScope unit="page" from="878" to="887" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Bayesian graph neural networks with adaptive connection sampling</title>
		<author>
			<persName><forename type="first">A</forename><surname>Hasanzadeh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Hajiramezanali</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Boluki</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Duffield</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Narayanan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Qian</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
				<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="4094" to="4104" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Learning from imbalanced data</title>
		<author>
			<persName><forename type="first">H</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">A</forename><surname>Garcia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Knowledge and Data Engineering</title>
		<imprint>
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="issue">9</biblScope>
			<biblScope unit="page" from="1263" to="1284" />
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Adaptive synthetic sampling approach for imbalanced learning</title>
		<author>
			<persName><forename type="first">H</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">A</forename><surname>Garcia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><surname>Adasyn</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Joint Conference on Neural Network</title>
				<imprint>
			<date type="published" when="2008">2008</date>
			<biblScope unit="page" from="1322" to="1328" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Strategies for pre-training graph neural networks</title>
		<author>
			<persName><forename type="first">W</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Gomes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Zitnik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Pande</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Leskovec</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
				<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Open graph benchmark: Datasets for machine learning on graphs</title>
		<author>
			<persName><forename type="first">W</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Fey</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Zitnik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Catasta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Leskovec</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
				<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Learning deep representation for imbalanced classification</title>
		<author>
			<persName><forename type="first">C</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">C</forename><surname>Loy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
				<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="5375" to="5384" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Deep imbalanced learning for face recognition and attribute prediction</title>
		<author>
			<persName><forename type="first">C</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">C</forename><surname>Loy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE transactions on pattern analysis and machine intelligence</title>
				<imprint>
			<date type="published" when="2019">2019a</date>
			<biblScope unit="volume">42</biblScope>
			<biblScope unit="page" from="2781" to="2794" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Text level graph neural network for text classification</title>
		<author>
			<persName><forename type="first">L</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Houfeng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing</title>
				<meeting>the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing</meeting>
		<imprint>
			<publisher>EMNLP-IJCNLP</publisher>
			<date type="published" when="2019">2019b</date>
			<biblScope unit="page" from="3435" to="3441" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Adaptive mixtures of local experts</title>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">A</forename><surname>Jacobs</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">I</forename><surname>Jordan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">J</forename><surname>Nowlan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural Comput</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="79" to="87" />
			<date type="published" when="1991">1991</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Graphsleepnet: Adaptive spatial-temporal graph convolutional networks for sleep stage classification</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Ning</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Joint Conference on Artificial Intelligence</title>
				<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="1324" to="1330" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Decoupling representation and classifier for long-tailed recognition</title>
		<author>
			<persName><forename type="first">B</forename><surname>Kang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Rohrbach</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Gordo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Kalantidis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
				<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Imbalanced classification via major-to-minor translation</title>
		<author>
			<persName><forename type="first">J</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Jeong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Shin</surname></persName>
		</author>
		<author>
			<persName><surname>M2m</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
				<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="13896" to="13905" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">A method for stochastic optimization</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">P</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Ba</surname></persName>
		</author>
		<author>
			<persName><surname>Adam</surname></persName>
		</author>
		<idno>ICLR 2015</idno>
	</analytic>
	<monogr>
		<title level="m">3rd International Conference on Learning Representations</title>
				<editor>
			<persName><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</editor>
		<meeting><address><addrLine>San Diego, CA, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2015">May 7-9, 2015. 2015</date>
		</imprint>
	</monogr>
	<note>Conference Track Proceedings</note>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Semi-supervised classification with graph convolutional networks</title>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">N</forename><surname>Kipf</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Welling</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
				<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Wasserstein embedding for graph learning</title>
		<author>
			<persName><forename type="first">S</forename><surname>Kolouri</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Naderializadeh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">K</forename><surname>Rohde</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Hoffmann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
				<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<monogr>
		<title level="m" type="main">FLAG: adversarial data augmentation for graph neural networks</title>
		<author>
			<persName><forename type="first">K</forename><surname>Kong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Ghanem</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Taylor</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Goldstein</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2010.09891</idno>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Self-attention graph pooling</title>
		<author>
			<persName><forename type="first">J</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Kang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
				<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="3734" to="3743" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Gradient harmonized singlestage detector</title>
		<author>
			<persName><forename type="first">B</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI Conference on Artificial Intelligence</title>
				<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Focal loss for dense object detection</title>
		<author>
			<persName><forename type="first">T</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Dollár</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2017 IEEE International Conference on Computer Vision (ICCV)</title>
				<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="2999" to="3007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Deep representation learning on long-tailed data: A learnable embedding augmentation perspective</title>
		<author>
			<persName><forename type="first">J</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Dou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
				<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="2970" to="2979" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Modeling task relationships in multi-task learning with multi-gate mixture-of-experts</title>
		<author>
			<persName><forename type="first">J</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Yi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Hong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">H</forename><surname>Chi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 24th ACM SIGKDD International Conference on Knowledge Discovery &amp; Data Mining</title>
				<meeting>the 24th ACM SIGKDD International Conference on Knowledge Discovery &amp; Data Mining</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Mixture of experts: a literature survey</title>
		<author>
			<persName><forename type="first">S</forename><surname>Masoudnia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Ebrahimpour</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Artificial Intelligence Review</title>
		<imprint>
			<biblScope unit="volume">42</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="275" to="293" />
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Rethinking pooling in graph neural networks</title>
		<author>
			<persName><forename type="first">D</forename><surname>Mesquita</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Souza</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Kaski</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
				<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Weisfeiler and leman go neural: Higher-order graph neural networks</title>
		<author>
			<persName><forename type="first">C</forename><surname>Morris</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Ritzert</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Fey</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">L</forename><surname>Hamilton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">E</forename><surname>Lenssen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Rattan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Grohe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the AAAI Conference on Artificial Intelligence</title>
				<meeting>the AAAI Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="4602" to="4609" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Joint structure feature exploration and regularization for multitask graph classification</title>
		<author>
			<persName><forename type="first">S</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">Y</forename><surname>Philip</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Knowledge and Data Engineering</title>
		<imprint>
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="715" to="728" />
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Task sensitive feature exploration and learning for multitask graph classification</title>
		<author>
			<persName><forename type="first">S</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Cybernetics</title>
		<imprint>
			<biblScope unit="volume">47</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="744" to="758" />
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Multitask mixture of sequential experts for user activity streams</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Qin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Metzler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Qin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 26th ACM SIGKDD International Conference on Knowledge Discovery &amp; Data Mining</title>
				<meeting>the 26th ACM SIGKDD International Conference on Knowledge Discovery &amp; Data Mining</meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Outrageously large neural networks: The sparsely-gated mixture-of-experts layer</title>
		<author>
			<persName><forename type="first">N</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Mirhoseini</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Maziarz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Davis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Dean</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
				<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Mixture models for diverse machine translation: Tricks of the trade</title>
		<author>
			<persName><forename type="first">T</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Ott</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Auli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Ranzato</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
				<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="5719" to="5728" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Multiclass imbalanced graph convolutional network learning</title>
		<author>
			<persName><forename type="first">M</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Wilson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Joint Conference on Artificial Intelligence</title>
				<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="2879" to="2885" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Classification of imbalanced data: A review</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">K</forename><surname>Wong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">S</forename><surname>Kamel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Pattern Recognition and Artificial Intelligence</title>
		<imprint>
			<biblScope unit="volume">23</biblScope>
			<biblScope unit="issue">04</biblScope>
			<biblScope unit="page" from="687" to="719" />
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Equalization loss for long-tailed object recognition</title>
		<author>
			<persName><forename type="first">J</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Ouyang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Yin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Yan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
				<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="11662" to="11671" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">Progressive layered extraction (ple): A novel multi-task learning (mtl) model for personalized recommendations</title>
		<author>
			<persName><forename type="first">H</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Gong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Fourteenth ACM Conference on Recommender Systems</title>
				<imprint>
			<date type="published" when="2020">2020a</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">Long-tailed classification by keeping the good and removing the bad momentum causal effect</title>
		<author>
			<persName><forename type="first">K</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
				<imprint>
			<date type="published" when="2020">2020b</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<monogr>
		<title level="m" type="main">Mixtures of gaussian processes</title>
		<author>
			<persName><forename type="first">V</forename><surname>Tresp</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2001">2001</date>
			<biblScope unit="page" from="654" to="660" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<analytic>
		<title level="a" type="main">Graph Attention Networks</title>
		<author>
			<persName><forename type="first">P</forename><surname>Veličković</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Cucurull</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Casanova</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Romero</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Liò</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
				<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<analytic>
		<title level="a" type="main">Learning imbalanced datasets with label-distribution-aware margin loss</title>
		<author>
			<persName><forename type="first">H</forename><surname>Wallach</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Larochelle</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Beygelzimer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Alché-Buc</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Fox</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Garnett</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
				<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="1567" to="1578" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<analytic>
		<title level="a" type="main">Haar graph pooling</title>
		<author>
			<persName><forename type="first">Y</forename><forename type="middle">G</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Montufar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Zhuang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fan</forename></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
				<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="9952" to="9962" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b58">
	<analytic>
		<title level="a" type="main">Moleculenet: a benchmark for molecular machine learning</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Ramsundar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">N</forename><surname>Feinberg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Gomes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Geniesse</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">S</forename><surname>Pappu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Leswing</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Pande</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Chemical science</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="513" to="530" />
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b59">
	<analytic>
		<title level="a" type="main">How powerful are graph neural networks?</title>
		<author>
			<persName><forename type="first">K</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Leskovec</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Jegelka</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
				<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b60">
	<analytic>
		<title level="a" type="main">Deep graph kernels</title>
		<author>
			<persName><forename type="first">P</forename><surname>Yanardag</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Vishwanathan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Knowledge Discovery and Data Mining</title>
				<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="1365" to="1374" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b61">
	<analytic>
		<title level="a" type="main">Graph convolutional networks for text classification</title>
		<author>
			<persName><forename type="first">L</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Mao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Luo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the AAAI Conference on Artificial Intelligence</title>
				<meeting>the AAAI Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="7370" to="7377" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b62">
	<analytic>
		<title level="a" type="main">Hierarchical graph representation learning with differentiable pooling</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Ying</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>You</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Morris</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Hamilton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Leskovec</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
				<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="4800" to="4810" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b63">
	<analytic>
		<title level="a" type="main">Inductive matrix completion based on graph neural networks</title>
		<author>
			<persName><forename type="first">M</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
				<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
