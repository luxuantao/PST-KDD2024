<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">MixGCF: An Improved Training Method for Graph Neural Network-based Recommender Systems</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Tinglin</forename><surname>Huang</surname></persName>
							<email>tinglin.huang@zju.edu.cn</email>
							<affiliation key="aff0">
								<orgName type="institution">Zhejiang University</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Yuxiao</forename><surname>Dong</surname></persName>
							<email>yuxiaod@fb.com</email>
							<affiliation key="aff1">
								<orgName type="department">Facebook AI</orgName>
								<orgName type="institution">‚ô¶ Tsinghua University</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Ming</forename><surname>Ding</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Zhen</forename><surname>Yang</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Wenzheng</forename><surname>Feng</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Xinyu</forename><surname>Wang</surname></persName>
							<email>wangxinyu@zju.edu.cn</email>
							<affiliation key="aff0">
								<orgName type="institution">Zhejiang University</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Jie</forename><surname>Tang</surname></persName>
							<email>jietang@tsinghua.edu.cn</email>
						</author>
						<author>
							<affiliation key="aff2">
								<orgName type="institution">Tinglin Huang</orgName>
								<address>
									<addrLine>Ming Ding, Wenzheng Feng</addrLine>
									<settlement>Yuxiao Dong, Zhen Yang, Xinyu Wang, Jie Tang. 2021</settlement>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">MixGCF: An Improved Training Method for Graph Neural Network-based Recommender Systems</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="DOI">10.1145/3447548.3467408</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.2" ident="GROBID" when="2022-12-25T13:49+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Collaborative Filtering</term>
					<term>Recommender Systems</term>
					<term>Graph Neural Networks</term>
					<term>Negative Sampling</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Graph neural networks (GNNs) have recently emerged as state-ofthe-art collaborative filtering (CF) solution. A fundamental challenge of CF is to distill negative signals from the implicit feedback, but negative sampling in GNN-based CF has been largely unexplored. In this work, we propose to study negative sampling by leveraging both the user-item graph structure and GNNs' aggregation process. We present the MixGCF method-a general negative sampling plugin that can be directly used to train GNN-based recommender systems. In MixGCF, rather than sampling raw negatives from data, we design the hop mixing technique to synthesize hard negatives. Specifically, the idea of hop mixing is to generate the synthetic negative by aggregating embeddings from different layers of raw negatives' neighborhoods. The layer and neighborhood selection process are optimized by a theoretically-backed hard selection strategy. Extensive experiments demonstrate that by using MixGCF, state-of-the-art GNN-based recommendation models can be consistently and significantly improved, e.g., 26% for NGCF and 22% for LightGCN in terms of NDCG@20.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>CCS CONCEPTS</head><p>‚Ä¢ Information systems ‚Üí Recommender systems; ‚Ä¢ Mathematics of computing ‚Üí Graph algorithms.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">INTRODUCTION</head><p>Recommender systems have been widely used for avoiding information overload in applications as diverse as online shopping <ref type="bibr" target="#b52">[53]</ref>, social network <ref type="bibr" target="#b28">[29]</ref>, advertising <ref type="bibr" target="#b14">[15]</ref>, and Web search <ref type="bibr" target="#b16">[17]</ref>. Its goal is to provide users with personalized information feeding, that is, for each user, the problem of recommendation is to predict the items that she or he will consume.</p><p>Among the most promising techniques for this problem has been the usage of collaborative filtering (CF) <ref type="bibr" target="#b20">[21]</ref>, which models users' historical interactions with items to profile users and items for predicting future interactions. To date, the most prevalent CF solutions are to project users and items into the latent embedding space, such as matrix factorization <ref type="bibr" target="#b22">[23]</ref> and neural networks <ref type="bibr" target="#b13">[14]</ref> based techniques. To further improve the embedding quality, one prominent direction is to model user-item interactions as a graph and leverage graph neural networks (GNNs) <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b10">11,</ref><ref type="bibr" target="#b19">20]</ref> to incorporate structural information into the embeddings. Notably, the GNNbased recommendation models, such as PinSage <ref type="bibr" target="#b48">[49]</ref>, NGCF <ref type="bibr" target="#b42">[43]</ref>, and LightGCN <ref type="bibr" target="#b12">[13]</ref>, have generated state-of-the-art performance with Web-scale applications.</p><p>The typical flow of (GNN-based) recommender systems is relatively straightforward. Given a user-item interaction graph, it begins with defining an aggregation function over the structure to propagate neighborhood information, usually followed by a pooling operation for outputting both user and item embeddings. Similar to conventional recommendation methods, its objective function is designed to prefer an observed user-item pair (as positive) to unobserved ones (as negative pairs). Take the widely-adopted BPR loss <ref type="bibr" target="#b30">[31]</ref> for example, for each user and one of her positive items, we conduct negative sampling to pick one item as the negative from those she never interacts with.</p><p>Essentially, the negative samples play a decisive role in the performance of (GNN-based) recommendation models. Commonly, a uniform distribution is used for negative sampling <ref type="bibr" target="#b12">[13,</ref><ref type="bibr" target="#b42">43]</ref>. To improve the quality of negative samples, studies have attempted to design new sampling distributions for prioritizing informative negatives <ref type="bibr" target="#b16">[17,</ref><ref type="bibr" target="#b27">28,</ref><ref type="bibr" target="#b39">40,</ref><ref type="bibr" target="#b43">44,</ref><ref type="bibr" target="#b47">48,</ref><ref type="bibr" target="#b51">52]</ref>. In doing so, the model would be challenged and forced to distinguish their differences at a finer granularity. To improve negative sampling in GNNs, PinSage <ref type="bibr" target="#b48">[49]</ref> samples the negatives based on their PageRank scores and MCNS <ref type="bibr" target="#b47">[48]</ref> re-designs both positive and negative sampling distributions with their structural correlations in mind. However, these attempts in GNNs only focus on improving negative sampling in the discrete graph space, ignoring GNNs' unique neighborhood aggregation process in the embedding space.</p><p>Contributions. In this work, we propose to design negative sampling strategies for better training GNN-based recommender systems. We present a simple MixGCF framework for generating hard negative samples. Instead of directly sampling real negatives from the data, MixGCF takes inspirations from data augmentation and metric learning to synthesize negative samples by leveraging the underlying GNN-based recommender.</p><p>To make synthetic negatives hard for the recommendation models, MixGCF designs two strategies: positive mixing and hop mixing. In positive mixing, we introduce an interpolation mixing method to pollute the embeddings of raw negative samples by injecting information from positive ones into them. In hop mixing, we sample several raw negative samples, e.g., ùë£ ‚àí ùëñ , ùë£ ‚àí ùëó , and ùë£ ‚àí ùëò , in Figure <ref type="figure" target="#fig_0">1</ref> and generate the embedding of the synthetic negative ùë£ ‚àí by using their polluted embeddings aggregated from selected hops of their neighbors. For example, as indicated by the blue circles, hop 0, 1, and 2 are selected from ùë£ ‚àí ùëñ , ùë£ ‚àí ùëó , and ùë£ ‚àí ùëò , respectively. It is worth mentioning that the selection of which hop from whose neighborhood is guided by a designed hard selection strategy.</p><p>We perform extensive experiments on benchmark recommendation datasets. Experimental results show that by replacing their default negative sampler to MixGCF, the state-of-the-art GNN-based recommendation models can be consistently improved, such as average relative increase of 22% for LightGCN and 26% for NGCF in terms of NDCG@20. Furthermore, we compare MixGCF with various negative sampling techniques, the results of which demonstrate the empirical advantage of MixGCF over them.</p><p>In summary, our work makes the following contributions:</p><p>‚Ä¢ Introduce the idea of synthesizing negative samples rather than directly sampling negatives from the data for improving GNNbased recommender systems.</p><p>‚Ä¢ Present a general MixGCF framework with the hop mixing and positive mixing strategies that can be naturally plugged into GNN-based recommendation models. ‚Ä¢ Demonstrate the significant improvements that MixGCF brings to GNN recommenders, as well as its consistent outperformance over a diverse set of negative sampling techniques.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">PRELIMINARIES AND PROBLEM</head><p>In this section, we first review the overall process of graph neural network (GNN) based collaborative filtering (CF) for recommender systems. We then introduce the studied problem that concerns the training of the above process.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>2.1Graph Neural Networks for Recommendation</head><p>Commonly, the input of recommendation systems includes a set of users U = {ùë¢}, items V = {ùë£ }, and users' implicit feedback</p><formula xml:id="formula_0">O + = {(ùë¢, ùë£ + )|ùë¢ ‚àà U, ùë£ + ‚àà V},</formula><p>where each pair indicates an interaction between user ùë¢ and item ùë£ + . The goal is to estimate the user preference towards items.</p><p>Recently, studies have shown that GNN-based CF models offer promising results for this task <ref type="bibr" target="#b12">[13,</ref><ref type="bibr" target="#b42">43,</ref><ref type="bibr" target="#b48">49]</ref>. The main idea is to measure user ùë¢'s preference on item ùë£ based on their multi-hop neighbors. Specifically, these techniques generate the latent representations of the target user and item, the inner product of which is used to quantify the preference, that is, ≈∑(ùë¢, ùë£) = e * ùë¢ ‚ä§ e * ùë£ . Next, we briefly introduce the process of GNN-based CF, including aggregation, pooling, and its optimization with negative sampling.</p><p>Aggregation. Each user and item are associated with an initial embedding e ùë¢ and e ùë£ as its representation vector, respectively. In order to exploit the CF signal from neighbor nodes, GNN-based recommendation models apply different aggregation functions to propagate information over neighbors <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b12">13,</ref><ref type="bibr" target="#b36">37,</ref><ref type="bibr" target="#b42">43,</ref><ref type="bibr" target="#b48">49]</ref>. Take Light-GCN for example, its aggregation process is:</p><formula xml:id="formula_1">e (ùëô+1) ùë¢ = ùëñ ‚ààùëÅ ùë¢ 1 |ùëÅ ùë¢ ||ùëÅ ùëñ | e (ùëô)</formula><p>ùëñ , e</p><formula xml:id="formula_2">(ùëô+1) ùë£ = ùëó ‚ààùëÅ ùë£ 1 |ùëÅ ùë£ ||ùëÅ ùëó | e (ùëô) ùëó .<label>(1)</label></formula><p>where e</p><formula xml:id="formula_3">(ùëô)</formula><p>ùë¢ , e</p><formula xml:id="formula_4">(ùëô)</formula><p>ùë£ are the embeddings of user ùë¢ and item ùë£ at ùëô-th layer of GNN, N ùë¢ denotes the set of items that interact with user ùë¢, and N ùë£ denotes the set of users that interact with item ùë£. By stacking multiple aggregation layers, each user/item can gather the information from its higher-order neighbors. For simplicity, we use e (ùëô) as the ùëô-th layer embedding in the following sections.</p><p>Pooling. Different from GNNs for node classification <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b19">20]</ref>, where representations in the final layer are used, the GNN-based CF models usually adopt the pooling operation to generate the final representations for users and items. According to <ref type="bibr" target="#b46">[47]</ref>, this can help avoid over-smoothing and determine the importance of a node's subgraph information at different ranges.</p><p>Specifically, at the final layer ùêø, the pooling function is applied to generate the final user/item representations e * ùë¢ /e * ùë£ . For example, LightGCN uses sum-based pooling:</p><formula xml:id="formula_5">e * ùë¢ = ùêø ùëô=0 ùúÜ ùëô e (ùëô) ùë¢ , e * ùë£ = ùêø ùëô=0 ùúÜ ùëô e (ùëô) ùë£ ,<label>(2)</label></formula><p>and NGCF uses concat-based pooling:</p><formula xml:id="formula_6">e * ùë¢ = e (0) ùë¢ ||‚Ä¢ ‚Ä¢ ‚Ä¢ ||e (ùêø)</formula><p>ùë¢ , e * ùë£ = e (0)</p><formula xml:id="formula_7">ùë£ ||‚Ä¢ ‚Ä¢ ‚Ä¢ ||e (ùêø) ùë£ ,<label>(3)</label></formula><p>Optimization with Negative Sampling. The task of learning to rank is to provide a user with a ranked list of items by assuming that the items preferred by the user should rank higher than others. However, the ranked item list for each user oftentimes can only be inferred from the implicit feedback that only consists of the positive observations. One straightforward solution is to assume that users prefer the observed items over all unobserved ones. Due to the large size of unobserved items (usually in ùëÇ(|V | 2 )), the learning objective is usually simplified by negative sampling as the BPR loss <ref type="bibr" target="#b30">[31]</ref>:</p><formula xml:id="formula_8">max ùë£ + ,ùë£ ‚àí ‚àºùëì S (ùë¢) ùëÉ ùë¢ (ùë£ + &gt; ùë£ ‚àí |Œò)<label>(4)</label></formula><p>where ùë£ + and ùë£ ‚àí denote the positive and negative items, respectively, ùëÉ ùë¢ (ùëé &gt; ùëè) represents user ùë¢ prefers item ùëé over ùëè, ùëì S (ùë¢) is the distribution of negative sampling, and Œò is the parameter of the model. Most recommendation methods consider negative sample from a uniform distribution (ùëì S (ùë¢) as ùëì uniform (ùë¢)) <ref type="bibr" target="#b12">[13,</ref><ref type="bibr" target="#b13">14,</ref><ref type="bibr" target="#b28">29,</ref><ref type="bibr" target="#b30">31,</ref><ref type="bibr" target="#b42">43]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">The Negative Sampling Problem</head><p>According to the loss function in Eq. ( <ref type="formula" target="#formula_8">4</ref>), the negative sampling strategy plays a critical role in the model training of recommendation. Intuitively, the negative samples close to positive ones, a.k.a. hard negative samples, can make the models better learn the boundary between positive and negative instances <ref type="bibr" target="#b48">[49]</ref>. To this end, several attempts have been made to sample hard negatives to improve the optimization of general recommender systems <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b16">17,</ref><ref type="bibr" target="#b29">30,</ref><ref type="bibr" target="#b51">52]</ref>. However, negative sampling for GNN-based recommenders has remained largely unexplored. Notably, early attempts-PinSage <ref type="bibr" target="#b48">[49]</ref> and MCNS <ref type="bibr" target="#b47">[48]</ref>-focus on improving the sampling distributions at the discrete structure level, seeking better hard negative (raw) nodes in the graph. In this work, we ask the question of whether we can synthesize harder negative samples in the continuous space, based on the GNN underlying the recommender.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">THE MIXGCF METHOD</head><p>MixGCF is a general algorithm for negative sampling in GNNbased recommendation. It can be directly plugged into existing GNN-based recommendation algorithms, such as LightGCN and NGCF.</p><p>Instead of sampling real items from the data as negative ones <ref type="bibr" target="#b27">[28,</ref><ref type="bibr" target="#b30">31,</ref><ref type="bibr" target="#b39">40,</ref><ref type="bibr" target="#b47">48,</ref><ref type="bibr" target="#b51">52]</ref>, MixGCF proposes to synthesize informative (and fake) negative items based on the graph structure for training GNNbased CF recommendation models. Specifically, MixGCF introduces the positive mixing and hop mixing techniques to synthesize negative samples by mixing information from different local graphs.</p><p>The flow of MixGCF is illustrated in Figure <ref type="figure" target="#fig_1">2</ref>. In positive mixing, we develop an interpolation mixing method to inject information from positive samples to negative ones, making hard negative candidates. In hop mixing, we first leverage a hard negative selection strategy to extract unique information from each of the hard negatives generated above, and then use the pooling operation to combine the diverse information extracted for creating the fake but informative negative items.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Positive Mixing</head><p>Recall that in a ùêø-layer GNN, for each item ùë£, we can have ùêø + 1 embeddings for ùë£, each of which e (ùëô) ùë£ corresponds to the embedding that is aggregated with ùëô layers (0 ‚â§ ùëô ‚â§ ùêø).</p><p>To fake the negative ùë£ ‚àí ùëñ with its embedding e ùë£ ‚àí in Figure <ref type="figure" target="#fig_1">2</ref>, we first follows the convention <ref type="bibr" target="#b16">[17,</ref><ref type="bibr" target="#b48">49]</ref> to select ùëÄ negative items to form the candidate set M, with ùëÄ usually being much smaller than the number of items in the data. These ùëÄ negative items can form a candidate negative embedding set E = {e (ùëô)</p><formula xml:id="formula_9">ùë£ ùëö } of size ùëÄ √ó (ùêø + 1).</formula><p>A very recent study <ref type="bibr" target="#b16">[17]</ref> suggests that recommendation models usually operate on an input space that mainly comprises of easy negatives, therefore we propose to improve the quality of the embeddings E of candidate negatives. Inspired by ùëöùëñùë•ùë¢ùëù <ref type="bibr" target="#b17">[18,</ref><ref type="bibr" target="#b50">51]</ref>, we introduce the idea of positive mixing to inject positive information e ùë£ + into negative embeddings in E. ùëöùëñùë•ùë¢ùëù is an interpolation based data augmentation method, which enforces the model to output linearly between the training data. Specifically, for each candidate negative embedding e (ùëô) ùë£ ùëö ‚àà E, the positive mixing operation is formalized as:</p><formula xml:id="formula_10">e ‚Ä≤(ùëô) ùë£ ùëö = ùõº (ùëô) e (ùëô) ùë£ + + (1 ‚àí ùõº (ùëô) )e (ùëô)</formula><p>ùë£ ùëö , ùõº (ùëô) ‚àà (0, 1),</p><p>where ùõº (ùëô) is the mixing coefficient that is uniformly sampled for each hop ùëô. Note that the mixing coefficient of ùëöùëñùë•ùë¢ùëù is sampled from a beta distribution Beta(ùõΩ, ùõΩ), which has a heavy impact on the model's generalization ability <ref type="bibr" target="#b49">[50]</ref>. To decouple the impact, the mixing coefficient ùõº (ùëô) in our positive mixing is uniformly sampled from (0, 1) (cf. Section 4.3 for the empirical discussions on ùõº (ùëô) ).</p><p>Let E ‚Ä≤ be the enhanced embedding set for the candidate negatives M. Positive mixing enhances the negatives by (1) injecting positive information into negative samples, which can help enforce the optimization algorithm to exploit the decision boundary harder, and (2) introducing stochastic uncertainty into them with the random mixing coefficient.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Hop Mixing</head><p>With the embeddings E ‚Ä≤ = {e ‚Ä≤ (ùëô)</p><p>ùë£ ùëö } of candidate negative items enhanced by positive mixing, we present the hop mixing technique to generate the synthetic negative item ùë£ ‚àí and its embedding e ùë£ ‚àí . The main idea of hop mixing is to leverage the hierarchical (layerbased) aggregation process in GNNs.</p><p>Specifically, for each layer ùëô (0 ‚â§ ùëô ‚â§ ùêø), we sample one candidate negative embedding e ‚Ä≤ (ùëô) ) , which contains all the ùëô-th layer embeddings of the candidate negative items in M. Take ùêø=2 for example, we can sample e ‚Ä≤ (0)</p><formula xml:id="formula_12">ùë£ ùë• (1 ‚â§ ùë• ‚â§ ùëÄ) from E ‚Ä≤(ùëô</formula><p>ùë£ ùëé , e ‚Ä≤ (1) ùë£ ùëè , and e ‚Ä≤ (2) ùë£ ùëê from E ‚Ä≤ . Note that ùëé, ùëè, and ùëê are not necessary to be distinct.</p><p>The idea of hop mixing is then to combine all the ùêø+1 embeddings selected by layer to generate the representation e ùë£ ‚àí of the (fake) negative ùë£ ‚àí . Specifically, the representation is synthesized by fusing all candidate embeddings by the pooling operation:</p><formula xml:id="formula_13">e ùë£ ‚àí = ùëì pool e ‚Ä≤(0) ùë£ ùë• , ‚Ä¢ ‚Ä¢ ‚Ä¢ , e ‚Ä≤(ùêø) ùë£ ùë¶ ,<label>(6)</label></formula><p>where e ‚Ä≤ (ùëô) ùë£ ùë• denotes the ùëô-th layer embedding of ùë£ ùë• that is sampled at layer ùëô, and ùëì pool (‚Ä¢) applies the same pooling operation used in the current GNN-based recommender.</p><p>The essential question for hop mixing is how to effectively sample candidate embedding e ‚Ä≤ (ùëô)</p><p>ùë£ ùë• (1 ‚â§ ùë• ‚â§ ùëÄ) from E ‚Ä≤(ùëô) at each layer ùëô. Notably, a recent study on negative sampling for graph representation learning (MCNS) <ref type="bibr" target="#b47">[48]</ref> theoretically shows that the expected risk of the optimal parameter e ùëá ùë¢ e ùë£ between the expected loss ùêΩ (ùúÉ * ) and empirical loss ùêΩ (ùúÉ ùëá ) satisfies:</p><formula xml:id="formula_14">E ||(ùúÉ ùëá ‚àí ùúÉ * ) ùë¢ || 2 = 1 ùëá ( 1 ùëù ùëë (ùë£ |ùë¢) ‚àí 1 + 1 ùêæùëù ùëõ (ùë£ |ùë¢) ‚àí<label>1</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>ùêæ</head><p>), <ref type="bibr" target="#b6">(7)</ref> where ùëù ùëë (ùë£ |ùë¢), ùëù ùëõ (ùë£ |ùë¢) denote the estimated positive distribution and negative distribution, respectively, ùëá is the number of node pairs, and ùêæ is the number of negatives for each user recruiting in the loss. This derivation suggests that if ùëù ùëõ (ùë£ |ùë¢) is proportional to ùëù ùëë (ùë£ |ùë¢), the expected risk is only dependent on the ùëù ùëë (ùë£ |ùë¢), and the interaction probability between a user-item pair with a high inner product score can be estimated accurately. Based on the above theory, the suggested way for negative sampling is to select the negative according to the estimated positive distribution. Here we apply the inner product score to approximate the positive distribution and pick the candidate sample with the highest score, which is also called the hard negative select strategy <ref type="bibr" target="#b29">[30,</ref><ref type="bibr" target="#b51">52]</ref>. Formally, the hard selection strategy at the ùëô-th layer is implemented as:</p><formula xml:id="formula_15">e ‚Ä≤(ùëô) ùë£ ùë• = arg max e ‚Ä≤ (ùëô ) ùë£ùëö ‚àà E (ùëô ) ùëì Q (ùë¢, ùëô) ‚Ä¢ e ‚Ä≤(ùëô) ùë£ ùëö , (<label>8</label></formula><formula xml:id="formula_16">)</formula><p>where ‚Ä¢ is the inner product operation, and ùëì Q (ùë¢, ùëô) is a query mapping that returns an embedding related to the target user ùë¢ for the ùëô-th hop. The query in Eq. ( <ref type="formula" target="#formula_15">8</ref>) is dependent on the pooling module of the GNN used for recommendation. As discussed in Section 2, the mainstream pooling module in GNN-based recommendation <ref type="bibr" target="#b12">[13,</ref><ref type="bibr" target="#b42">43]</ref> can be categorized into sum-based and concat-based pooling operations. Therefore, there are two options for the inner product between the target user embedding e ùë¢ and the embedding of the synthesized negative e </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Optimization with MixGCF</head><p>Now, we can use the proposed MixGCF method as the negative sampling method ùëì S (‚Ä¢) in the loss function (Eq. ( <ref type="formula" target="#formula_8">4</ref>)) to optimize the parameters of the GNN-based recommendation model. Straightforwardly, the BPR loss function can be updated as</p><formula xml:id="formula_17">L BPR = (ùë¢,ùë£ + )‚àà O + e ùë£ ‚àí ‚àºùëì MixGCF (ùë¢,ùë£ + ) ln ùúé (e ùë¢ ‚Ä¢ e ùë£ ‚àí ‚àí e ùë¢ ‚Ä¢ e ùë£ + ),<label>(9)</label></formula><p>where ùúé(‚Ä¢) is the sigmoid function, O + is the set of the positive feedback, and e ùë£ ‚àí ‚àº ùëì MixGCF (ùë¢, ùë£ + ) represents that the instance (embedding) e ùë£ ‚àí is synthesized by the proposed MixGCF method.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Discussions on MixGCF</head><p>General Plugin. Observed from Eq. ( <ref type="formula" target="#formula_17">9</ref>), the proposed negative sampling method-MixGCF-can be naturally plugged into the ranking loss. In addition, MixGCF is a simple and non-parametric method. Altogether, these make MixGCF a general technique for improving a set of GNN-based recommendation models.</p><p>Data Augmentation. Unlike the prior efforts which elaborate versatile strategies to sample an existing negative item, MixGCF instead proposes the paradigm of synthesizing negative items based on the hop-wise sampling. Such a method could be also understood from the perspective of data augmentation, since the synthesized instance is conformed to but different from the existing instances <ref type="bibr" target="#b32">[33]</ref>. This enables the recommender to be trained on the more sophisticated data, leading to an improved generalization.</p><p>Approximation of Multiple Negatives. As shown in metric learning <ref type="bibr" target="#b15">[16,</ref><ref type="bibr" target="#b31">32,</ref><ref type="bibr" target="#b33">34]</ref>, recruiting multiple negative instances in loss function for each update can speed up the convergence of the underlying model and offer better performances. Instead of directly sampling multiple negatives, MixGCF naturally provides a low-cost approximation of them by hop mixing, potentially making it benefit from the metric learning conclusion above.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.5">Time Complexity</head><p>The time cost of MixGCF mainly comes from two parts. For the hop mixing module, the computational complexity of the hop-grained negative selection scheme is ùëÇ(ùëÄùêøùëë), where ùëÄ is the size of the negative candidate set, ùêø is the number of GNN layers, and ùëë is the embedding dimension. For the positive mixing module, the complexity of the linear combination is ùëÇ(ùëÄùêøùëë), which is equivalent to the hop mixing module's. Therefore, the time complexity of MixGCF is ùëÇ(ùëÄùêøùëë).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">EXPERIMENTS</head><p>We evaluate our proposed MixGCF method on three benchmark datasets with three representative GNN-based CF modes-LightGCN, NGCF, and PinSage-as the base recommenders. We compare MixGCF with other state-of-the-art negative sampling methods to demonstrate the superiority of our proposed method. We then conduct the hyper-parameter study and ablation study to analyze the behavior of MixGCF.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Experimental Settings</head><p>Dataset Description. In this paper, we evaluate our method on three benchmark datasets: Alibaba <ref type="bibr" target="#b47">[48]</ref>, Yelp2018 <ref type="bibr" target="#b42">[43]</ref>, and Amazon <ref type="bibr" target="#b47">[48]</ref>, which are publicly available. Each dataset consists of the user-item interactions solely, as summarized in Table <ref type="table" target="#tab_1">1</ref>. We follow the same settings described in <ref type="bibr" target="#b42">[43,</ref><ref type="bibr" target="#b47">48]</ref> to split the datasets into training, validation, and testing sets.</p><p>Evaluation Metrics. We choose the widely-used Recall@ùëÅ and NDCG@ùëÅ as the evaluation metrics (ùëÅ = 20 by default). Unlike the previous studies <ref type="bibr" target="#b13">[14,</ref><ref type="bibr" target="#b38">39,</ref><ref type="bibr" target="#b47">48]</ref> that conduct the sampled metrics, we compute Recall@ùëÅ and NDCG@ùëÅ by the all-ranking protocol <ref type="bibr" target="#b21">[22]</ref>. We report the average metrics of all users in each testing set.</p><p>Recommender. To verify the effectiveness of MixGCF, we perform experiments on three GNN-based recommenders as follows.</p><p>The detailed descriptions are in Section 2.1.</p><p>‚Ä¢ LightGCN <ref type="bibr" target="#b12">[13]</ref>: This is a state-of-the-art CF method. LightGCN claims that the design of NGCF is heavy and burdensome since each node in the user-item graph is only described by an ID. LightGCN omits the non-linear transformation and applies the sum-based pooling module to achieve better empirical performance.  <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b19">20]</ref>, NGCF applies the message-passing scheme on the bipartite useritem graph to exploit the high-order neighbors' information. To be specific, each node obtains the transformed representations of its multi-hop neighbors by recursively aggregating the message propagated from the adjacent neighbors. ‚Ä¢ PinSage <ref type="bibr" target="#b48">[49]</ref>: As an inductive variant of GNN, PinSage designs a random-walk based sampling method to sample neighbors for each node and proposes to sample hard negative based on PageRank score. We apply a sum-based pooling strategy to generate the embeddings of nodes.</p><p>Baselines. We compare the proposed method, MixGCF, with static (RNS), hard negative (DNS), GAN-based (IRGAN and AdvIR), and graph-based (MCNS) sampler, as follows:</p><p>‚Ä¢ RNS <ref type="bibr" target="#b30">[31]</ref>: Random negative sampling (RNS) strategy applies uniform distribution to sample negative items. It is independent of the recommender and is applied in various tasks.</p><p>‚Ä¢ DNS <ref type="bibr" target="#b51">[52]</ref>: Dynamic negative sampling (DNS) strategy is the stateof-the-art sampler <ref type="bibr" target="#b16">[17,</ref><ref type="bibr" target="#b29">30]</ref>, which adaptively selects the negative item scored highest by the recommender. Such the negative is viewed as the hard negative and can provide a large gradient to the parameters. ‚Ä¢ IRGAN <ref type="bibr" target="#b39">[40]</ref>: IRGAN integrates the recommender into a generative adversarial net (GAN) where the generator performs as a sampler to pick the negative for confusing the recommender.</p><p>‚Ä¢ AdvIR <ref type="bibr" target="#b27">[28]</ref>: AdvIR is also an adversarial sampler that incorporates adversarial sampling with adversarial training by adding adversarial perturbation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>‚Ä¢ MCNS [48]: Markov chain Monte Carlo negative sampling (MCNS)</head><p>is the pioneer to theoretically analyze the impact of negative sampling in link prediction. Based on the deduced theory, MCNS   proposes to sample negative by approximating the positive distribution and accelerate the process by Metropolis-Hastings algorithm.</p><p>Parameter Settings. We implement our MixGCF and recommendations models by PyTorch and will release our code upon acceptance. For each recommender, we fix the embedding size as 64, the optimizer as Adam <ref type="bibr" target="#b18">[19]</ref>, and use Xavier <ref type="bibr" target="#b8">[9]</ref> to initialize the parameters. We set the batch size as 1024 for NGCF, 2048 for Pin-Sage, and 2048 for LightGCN. We conduct a grid search to find the optimal settings for each recommender: the learning rate is searched in {0.0001, 0.0005, 0.001}, and ùêø 2 regularization is tuned in {10 ‚àí6 , 10    </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Performance Comparison</head><p>We report the detailed performance comparison in Table <ref type="table" target="#tab_2">2</ref> where we highlight the results of the best baselines (underlined) and our MixGCF (starred). The observations are as followed:</p><p>‚Ä¢ LightGCN consistently outperforms NGCF and PinSage by a large margin on three datasets, verifying that the nonlinearity and weight matrix are useless for collaborative filtering. ‚Ä¢ MixGCF yields the best performance on all the datasets. These improvements are attributed to the following reasons: (1) Through hop mixing technique, MixGCF augments the negative samples, which improves the generalization of recommender; (2) The synthesized hard negative that incorporates the different semantics information from multiple instances offers a informative gradient to the recommender. ‚Ä¢ The relative improvements on NGCF and PinSage are more significant than on LightGCN. Some possible reasons are: (1) The burdensome design offers a larger parameter space, meaning that NGCF and PinSage may benefit more from an informative negative;</p><p>(2) As a state-of-the-art CF model, LightGCN is capable of discriminating between the positive and easy negative item. Thus, the impact of synthesized hard negative to LightGCN is not as significant as the other two recommendation models. ‚Ä¢ DNS performs as the strongest baseline in most cases, which is consistent with previous studies <ref type="bibr" target="#b16">[17,</ref><ref type="bibr" target="#b29">30,</ref><ref type="bibr" target="#b43">44]</ref>. It reveals that selecting the hardest negative provides a meaningful gradient to guide the model.   Table <ref type="table">6</ref>: Impact of the number of negative instances (ùêæ).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Running</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Study of MixGCF</head><p>In this section, we first conduct an ablation study to investigate the effect of positive mixing. Towards the further analysis, we explore the influence of neighbor range, i.e., the number of aggregation modules. We then study how different size of candidate set and the distribution of the random coefficient in positive mixing affect the performance. In what follows, we extend the pairwise BPR loss to ùêæ-pair loss for analyzing the impact of the number of negative instances. For a fair comparison, we use the identical experimental settings as Section 4.2.</p><p>Impact of Presence of Positive Mixing. To verify the impact of positive mixing, we do an ablation study by disabling the positive mixing module (cf. Eq. ( <ref type="formula" target="#formula_11">5</ref>)) of MixGCF, termed MixGCF w/o p-m . The experimental results are shown in Table <ref type="table" target="#tab_3">3</ref> and we further plot the training curves of LightGCN, NGCF and PinSage in Figure <ref type="figure" target="#fig_4">3</ref>. The observations are as followed:</p><p>‚Ä¢ Aligning with Table <ref type="table" target="#tab_2">2</ref> in Section 4.2, removing the positive mixing module degrades the performance of recommenders in most cases, indicating the necessity of positive mixing. ‚Ä¢ Without positive mixing, MixGCF still outperforms DNS in most cases, manifesting that the hop-wise sampling is more effective than instance-wise sampling in GNN-based recommendation. ‚Ä¢ Comparing the training curves of MixGCF and MixGCF w/o p-m , it can be founded that the performance of MixGCF w/o p-m soars in the early phase but quickly reaches the peak and plumbs while MixGCF benefits from positive mixing and becomes more robust against the over-fitting.</p><p>Impact of Neighbor Range. To analyze the impact of the neighbor range, we vary ùêø in the range of {1, 2, 3} and summarize the results in Table <ref type="table" target="#tab_4">4</ref>. We observe that:</p><p>‚Ä¢ Increasing the neighbor range can improve the performance of recommenders in most cases. It is reasonable since considering the larger range of neighbors, more negative items can be integrated into the synthesized negative. ‚Ä¢ It can be found that MixGCF-1 achieves comparable or even higher performance to the MixGCF-2 and MixGCF-3 on PinSage. The possible reason is that the heavy design makes PinSage suffer from the risk of over-smoothing.</p><p>Impact of Candidate Set. We also conduct an experiment to analyze the influence of the size of the candidate set ùëÄ. We search ùëÄ in range of {8, 16, 32, 64} and sum up the results in Table <ref type="table" target="#tab_5">5</ref> where the best results is highlighted (starred). The observations are as followed:</p><p>‚Ä¢ Increasing the candidate set size enhances the accuracy of recommenders in most cases. For example, NGCF obtains the best performance on three datasets with ùëÄ = 64. ‚Ä¢ Interestingly, the improvements on Amazon is not significant as the other datasets when ùëÄ is increased. We attribute it to the different scale and distribution of these datasets.</p><p>Impact of Distribution of Positive Mixing. Since the random coefficient ùõº (ùëô) is the core of positive mixing, we conduct a experiment to investigate the impact of its distribution on Light-GCN. We explore different choices according to the relevant researches <ref type="bibr" target="#b17">[18,</ref><ref type="bibr" target="#b50">51]</ref>:</p><p>‚Ä¢ Beta distribution. ùëöùëñùë•ùë¢ùëù <ref type="bibr" target="#b50">[51]</ref> samples a mixing coefficient from Beta distribution Beta(ùõΩ, ùõΩ) to interpolates two samples. Following its experimental settings, we vary ùõΩ in range {0.2, 0.4}. ‚Ä¢ Gaussian distribution. Gaussian distribution Gaus(ùúá, ùúé) is one of the most prevalent distributions in machine learning. Here we fix ùúá = 0.5, ùúé = 0.1. ‚Ä¢ Uniform distribution. MoCHi <ref type="bibr" target="#b17">[18]</ref> serves for self-supervised learning approaches, and proposes creating a convex linear combination of negative feature and query feature <ref type="bibr" target="#b11">[12]</ref> for alleviating the false negative issue. It randomly samples a mixing coefficient from uniform distribution Uni(0, ùëé). Following its setting and our positive mixing, we fix ùëé = 0.5, 1 to study.</p><p>The experimental results are illustrated in Figure <ref type="figure" target="#fig_8">4</ref>. We have the following observations:</p><p>‚Ä¢ Our original setting in positive mixing, i.e., Uni(0, 1), yields the best performance in all the cases. Comparing with Uni(0, 0.5), the possible reason why Uni(0, 1) achieves a higher performance is that limiting the selecting range of random coefficient will reduce the search space of the model's parameters. ‚Ä¢ The beta distribution does not perform well on three datasets.</p><p>We attribute it to the requirement of a sophisticated selection of the hyper-parameter ùõΩ, which is known as a limitation of ùëöùëñùë•ùë¢ùëù.</p><p>Based on the above discussions, we find that sampling ùõº (ùëô) from uniform distribution leads to a good performance. Thus, to avoid complicating MixGCF, we fix the sampling distribution as Uni(0, 1).</p><p>Impact of Number of Negatives. We further conduct an experiment to investigate the influence of the number of negative instances ùêæ. Most of the prior work on negative sampling <ref type="bibr" target="#b27">[28,</ref><ref type="bibr" target="#b39">40,</ref><ref type="bibr" target="#b43">44,</ref><ref type="bibr" target="#b51">52]</ref> apply pairwise BPR loss to optimize the model. Inspired by the efforts on metric learning <ref type="bibr" target="#b31">[32,</ref><ref type="bibr" target="#b33">34]</ref>, we generalize the pairwise BPR loss to ùêæ-pair loss and study its effect on MixGCF. We vary ùêæ in range of {1, 2, 4, 8}. Note that when ùêæ = 1, the ùêæ-pair loss will degenerate into the BPR loss. The detailed description of ùêæ-pair loss is in Appendix A.2. We summarize the results in Table <ref type="table">6</ref>, and observe that increasing the ùêæ enhances the performance of the recommender consistently in most cases, verifying the effectiveness of the combination of MixGCF and ùêæ-pair loss. Such a phenomenon is expected since that the model distinguishes multiple synthesized negatives at the same time can lead to a more discriminative representation of the data and faster convergence.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">RELATED WORK 5.1 GNN-Based Recommendation</head><p>Recommendation system has been applied to many online services, such as E-commerce and social media <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b13">14,</ref><ref type="bibr" target="#b22">23,</ref><ref type="bibr" target="#b23">24]</ref>. To provide personalized information accurately, the recommendation system should estimate the preference of users towards myriad items. The most common paradigm is reconstructing the historical interactions by parameterizing the users and items.</p><p>In the recent years, GNN-based recommendation has gained considerable attention since the most information can be organized into a graph structure. To be specific, many recent works <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b12">13,</ref><ref type="bibr" target="#b34">35,</ref><ref type="bibr" target="#b36">37,</ref><ref type="bibr" target="#b42">43,</ref><ref type="bibr" target="#b48">49]</ref> consider the interactions between users and items as a bipartite graph, and utilize the graph neural network (GNN) to refine the representations of each node. For example, PinSage <ref type="bibr" target="#b48">[49]</ref> designs a random-walk based sampling method to sample neighborhoods and the visit counts are taken as the importance coefficients during aggregation. GC-MC <ref type="bibr" target="#b36">[37]</ref> models each node only with its adjacent neighbors by the aggregation module for the rating prediction task. NGCF <ref type="bibr" target="#b42">[43]</ref> exploits the multi-hop community information by recursively propagating the message and combines the representations of different layers to obtain a better performance. LightGCN <ref type="bibr" target="#b12">[13]</ref> argues the burdensome design of NGCF and simplifies the aggregation module by removing the nonlinear feature transformation.</p><p>To alleviate the cold-start issue and enhance the performance, some previous works take the side information, e.g., social network <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b28">29,</ref><ref type="bibr" target="#b45">46]</ref> and knowledge graph <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b37">38,</ref><ref type="bibr" target="#b38">39,</ref><ref type="bibr" target="#b41">42,</ref><ref type="bibr" target="#b44">45]</ref>, into account. For instance, GraphRec <ref type="bibr" target="#b6">[7]</ref> integrates the users into a social network and refines the representations of each node with a graph attention network. KGAT <ref type="bibr" target="#b41">[42]</ref> integrates the items into a knowledge graph and considers the interaction between users and items as a relation. It adopts the graph attention mechanism to exploit the high-order connections.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Negative Sampling for Recommendation</head><p>Negative sampling is first proposed to accelerate the training of skip-gram <ref type="bibr" target="#b25">[26]</ref>. In recent years, negative sampling has been studied in recommendation task for solving one-class problem <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b26">27]</ref>. To be specific, most interactions between users and items are in the form of implicit feedback, which only consists of positive feedback. Existing negative sampling methods roughly fall into four groups.</p><p>Static Sampler <ref type="bibr" target="#b25">[26,</ref><ref type="bibr" target="#b30">31]</ref> samples the negative item from a fixed distribution. Bayesian Personalized Ranking (BPR) <ref type="bibr" target="#b30">[31]</ref> applies uniform distribution to sample negative, which is one of the most prevalent methods. In addition, the negative sampling distribution of some previous work on network embedding <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b35">36]</ref> is proportional to the 3/4 power of the positive distribution. <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b27">28,</ref><ref type="bibr" target="#b39">40,</ref><ref type="bibr" target="#b40">41]</ref> is an adversarial sampler based on generative adversarial network (GAN). For example, IRGAN <ref type="bibr" target="#b39">[40]</ref> trains a generative adversarial network to play a min-max game with the recommender. The sampler performs as a generator and samples the negative to confuse the recommender. KBGAN <ref type="bibr" target="#b0">[1]</ref> applies the framework to knowledge graph embedding task by training two translation-based models. AdvIR <ref type="bibr" target="#b27">[28]</ref> adds the perturbation in adversarial sampling to make the model more robust.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>GAN-based Sampler</head><p>Hard Negative Sampler <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b16">17,</ref><ref type="bibr" target="#b29">30,</ref><ref type="bibr" target="#b51">52]</ref> adaptively picks the hardest negative by the current recommender. As a representative hard negative sampler, DNS <ref type="bibr" target="#b51">[52]</ref> selects the negative scored highest by the recommender. The main difference between MixGCF and DNS is the sampling level. To be specific, DNS is an instance-wise sampling method, which aims to sample a representation for one certain item. MixGCF is developed to serve the GNN-based recommender and is a hop-wise sampling method which samples the representations of each hop among the negative. Besides, MixGCF applies the positive mixing to improve the quality of negative candidates (Section 3.1). <ref type="bibr" target="#b43">[44,</ref><ref type="bibr" target="#b47">48,</ref><ref type="bibr" target="#b48">49]</ref> samples the negative based on the graph information. For example, MCNS <ref type="bibr" target="#b47">[48]</ref> derives a theory to quantify the impact of the negative sampling distribution, and based on the theory, it approximates the positive distribution with self-contrast approximation. KGPolicy <ref type="bibr" target="#b43">[44]</ref> incorporates knowledge graph into negative sampling and develop a reinforcement learning agent to sample high-quality negative items. PinSage <ref type="bibr" target="#b48">[49]</ref> samples the node according to their Personalized PageRank scores.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Graph-based Sampler</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">CONCLUSION</head><p>In this work, we study the GNN-based recommendations with the goal of improving the quality of negative samples. We devise a simple and non-parametric method: MixGCF, which is a general technique for the GNN-based recommendation models. Instead of sampling existing negative items, MixGCF integrates multiple negatives to synthesize a hard negative by positive mixing and hop mixing. We conduct experiments on public datasets and the results suggest that MixGCF can empower state-of-the-art GNN-based recommenders to achieve significant performance improvements over their original versions.</p><p>Capturing (negative) signals from data implicitly plays a crucial role in various fundamental learning tasks, such as contrastive learning and self-supervised learning. One promising direction is to bridge the developments from different topics for deriving general insights and inspiring perspectives. In the future, we would like to explore and further the techniques proposed in MixGCF for graph and relational data pre-training, so as to learn strongly generalizable representations of data. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.2 ùêæ-Pair Loss</head><p>Inspired by the prior researches on metric learning <ref type="bibr" target="#b15">[16,</ref><ref type="bibr" target="#b33">34]</ref>, we extend the pairwise BPR loss <ref type="bibr" target="#b30">[31]</ref> to ùêæ-pair loss aiming to enhance the performance of the model. Formally, the pairwise BPR loss is as follow: </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.3 Datasets</head><p>We use three publicly available datasets which vary in terms of domain, size, and sparsity to evaluate our proposed MixGCF.</p><p>‚Ä¢ Alibaba <ref type="bibr" target="#b47">[48]</ref> is collected from the Alibaba online shopping platform. The authors of MCNS organize the purchase record of selected users to construct the bipartite user-item graph. ‚Ä¢ Yelp2018 <ref type="bibr" target="#b42">[43]</ref> is from the 2018 edition of the Yelp challenge.</p><p>Each interaction of the graph is represented a restaurant or bar consumption record of a user. Besides, the authors applied the 10core setting to filter out the nodes with less than 10 interactions. ‚Ä¢ Amazon <ref type="bibr" target="#b47">[48]</ref> is first released in <ref type="bibr" target="#b24">[25]</ref> which contains several datasets of different products. The authors selected the "electronics" category to form a bipartite user-item graph whose time stamp spans from May 1996 to July 2014.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.4 Evaluation Metrics</head><p>In this paper, we conduct the all-ranking protocol and adopt two evaluation metrics: Recall@ùëÅ and NDCG@ùëÅ . The detailed introduction is shown as follow:</p><p>‚Ä¢ Recall@ùëÅ is the proportion of relevant items found in the topk recommendation. To be specific, we compute Recall@ùëÅ as follow:</p><p>( where ùêø ùëõ denotes the ùëõ-th term of ùêø, and ùõø(‚Ä¢) is the indicator function. (4) Calculate NDCG@ùëÅ = DCG@ùëÅ /IDCG@ùëÅ , where IDCG@ùëÅ denotes the ideal discounted cumulative gain.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: An illustration of hop mixing for synthesizing a negative item in MixGCF, where the user and item are represented by their ego-networks respectively, and the highlighted circles indicate the selected neighbors.</figDesc><graphic url="image-1.png" coords="1,317.96,213.66,252.18,152.39" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: An overview of MixGCF, where e (ùëô) denotes the ùëôth layer embedding of node ùëí, and e ‚Ä≤(ùëô) denotes the ùëô-th layer embedding generated by positive mixing.</figDesc><graphic url="image-2.png" coords="3,317.96,83.68,240.23,219.90" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head></head><label></label><figDesc>ùë£ ‚àí : ‚Ä¢ Sum-based pooling: e ùë¢ ‚Ä¢ e ùë£ ‚àí = ùêø ùëô=0 ùúÜ ùëô e ùë¢ ‚Ä¢ e (ùëô) ùë£ ‚àí ‚Ä¢ Concat-based pooling: e ùë¢ ‚Ä¢ e ùë£ ‚àí = ùêø ùëô=0 e (ùëô) ùë¢ ‚Ä¢ e (ùëô) ùë£ ‚àí . To make the selection process in Eq. (8) consistent with the pooling used in the GNN recommender, we let ùëì ùëÑ (ùë¢, ùëô) = e ùë¢ for sum-based pooling and ùëì ùëÑ (ùë¢, ùëô) = e (ùëô) ùë¢ for concat-based pooling, respectively.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head></head><label></label><figDesc>NDCG Recall NDCG LightGCN+MixGCF w/o p-m 0.0748 0.0347 0.0705 0.0581 0.0448 0.0212 NGCF+MixGCF w/o p-m 0.0479 0.0226 0.0674 0.0555 0.0332 0.0147 PinSage+MixGCF w/o p-m 0.0486 0.0231 0.0526 0.0432 0.0273 0</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: Impact of positive mixing.</figDesc><graphic url="image-9.png" coords="6,56.04,366.71,75.65,85.25" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>‚àí 5 ,</head><label>5</label><figDesc>10 ‚àí4 , 10 ‚àí3 }. The number of sampled neighbors in each layer of PinSage is fixed as 5. As for MCNS, the maximum depth of dfs is 100 by default. The candidate size ùëÄ in MixGCF and DNS are searched in {8, 16, 32, 64} and we report its effect on MixGCF in Section 4.3. The aggregation module number of the recommender ùêø is set as 3 by default, and we evaluate the impact of ùêø in Section 4.3. The detailed settings of MixGCF and recommenders are provided in Appendix A.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head></head><label></label><figDesc>NDCG Recall NDCGLightGCN+MixGCF-1 0.0651 0.0309 0.0684 0.0564 0.0403 0.0193 LightGCN+MixGCF-2 0.0726 0.0335 0.0707 0.0582 0.0438 0.0209 LightGCN+MixGCF-3 0.0763 0.0357 0.0713 0.0589 0.0460 0</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Figure 4 :</head><label>4</label><figDesc>Figure 4: Performance comparison over different distributions, i.e., Beta, Gaussian and Uniform distribution, of random coefficient (ùõº (ùëô) ) on three datasets.</figDesc><graphic url="image-14.png" coords="7,386.14,88.67,151.33,107.13" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head></head><label></label><figDesc>Implementation Details. Our implementation can be split into four parts: data iteration, embedding generation, negative sampling, and model learning. For each epoch, we randomly shuffle the training data and select the positive pairs with a size of ùêµ orderly until the end of the epoch. Then for each data batch, the current recommender generates the aggregated embeddings of users and items based on PyTorch 1.7.0 APIs. In this paper, we focus on the negative sampling part. The negative sampler picks/synthesizes a negative instance for each positive pair. After obtaining the negative items, we feed the representations of target users, positive items, and negative items into the loss function and update the parameters of the model based on the gradient.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head></head><label></label><figDesc>ùë£ + ,ùë£ ‚àí )‚àà O ‚àí ln ùúé(ùë¶ ùë¢,ùë£ + ‚àí ùë¶ ùë¢,ùë£ ‚àí ) where ùë¶ ùë¢,ùë£ denotes the matching score between user ùë¢ and item ùë£, and O = {(ùë¢, ùë£ + , ùë£ ‚àí )|(ùë¢, ùë£ + ) ‚àà O + , (ùë¢, ùë£ ‚àí ) / ‚àà O + } is the training dataset comprising of the interactions O + and the negative samples. Equivalently, we have L BPR = (ùë¢,ùë£ + ,ùë£ ‚àí )‚àà O ‚àí ln exp(ùë¶ ùë¢,ùë£ + ) exp(ùë¶ ùë¢,ùë£ + ) + exp(ùë¶ ùë¢,ùë£ ‚àí )The ùêæ-pair loss proposed in<ref type="bibr" target="#b33">[34]</ref> is formulated as:L ùêæ‚àípair = (ùë¢,ùë£ + ,ùë£ ‚àí 0 ,‚Ä¢‚Ä¢‚Ä¢,ùë£ ‚àí ùêæ )‚àà O ‚àíln exp(ùë¶ ùë¢,ùë£ + ) exp(ùë¶ ùë¢,ùë£ + ) + ùêæ ùëñ=0 exp(ùë¶ ùë¢,ùë£ ‚àí ùëñ ) where {ùë£ ‚àí 0 , ‚Ä¢ ‚Ä¢ ‚Ä¢ , ùë£ ‚àí ùêæ } denotes the set of ùêæ sampled negatives for each interaction pair.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_11"><head>1 )</head><label>1</label><figDesc>For each user ùë¢ in testing set, we compute the matching score with the rest of items {ùë£ 0 , ‚Ä¢ ‚Ä¢ ‚Ä¢ , ùë£ ùëÜ } which are never shown in the training interactions of ùë¢. (2) Sort the list of ùêø = {e ùëá ùë¢ e ùë£ 0 , ‚Ä¢ ‚Ä¢ ‚Ä¢ , e ùëá ùë¢ e ùë£ ùëÜ } in descending order. (3) Count the number of relevant items |I ùë¢,ùëÅ | occurring in the top ùëÅ terms of ùêø. (4) Calculate Recall@ùëÅ = 1 |U | ùë¢ ‚ààU |I ùë¢,ùëÅ | |I ùë¢ | , where U is the testing set of users and I ùë¢ is the set of testing items for ùë¢.‚Ä¢ NDCG@ùëÅ measures the quality of ranking list at a finer granularity than Recall@ùëÅ . It assigns different weight to different ranks of items. We compute NDCG@ùëÅ as follow:(1) For each user ùë¢ in testing set, we compute the matching score with the rest of items {ùë£ 0 , ‚Ä¢ ‚Ä¢ ‚Ä¢ , ùë£ ùëÜ } which are never shown in the training interactions of ùë¢. (2) Sort the list of ùêø = {e ùëá ùë¢ e ùë£ 0 , ‚Ä¢ ‚Ä¢ ‚Ä¢ , e ùëá ùë¢ e ùë£ ùëÜ } in descending order. (3) Calculate the discounted cumulative gain: ùëõ ‚àà I ùë¢ ) log 2 (ùëõ + 1)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>Algorithm 1: The training process with MixGCF Input: Training set {(ùë¢, ùë£ + )}, Recommender ùëì GNN , Number of negative candidate ùëÄ, Number of aggregation layers ùêø. for ùë° = 1, 2, ‚Ä¢ ‚Ä¢ ‚Ä¢ ,ùëá do Sample a mini-batch of positive pairs {(ùë¢, ùë£ + )}. Initialize loss L = 0. // Negative Sampling via MixGCF. for each (ùë¢, ùë£ + ) pair do Get the aggregated embeddings of each node by ùëì GNN . Get the set of candidate negative embeddings E by uniformly sampling ùëÄ negatives. Get the updated set of negative candidate E ‚Ä≤ by (5). Synthesize a hard negative e ùë£ ‚àí based on E ‚Ä≤ by (6). L = L + ln ùúé (e ùë¢ ‚Ä¢ e ùë£ ‚àí ‚àí e ùë¢ ‚Ä¢ e ùë£ + ). end Update ùúÉ by descending the gradients ‚àá ùúÉ L. end</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 1 :</head><label>1</label><figDesc>Statistics of the datasets.</figDesc><table><row><cell>Dataset</cell><cell cols="2">#Users #Items #Interactions Density</cell></row><row><cell>Alibaba</cell><cell>106, 042 53, 591</cell><cell>907, 407 0.00016</cell></row><row><cell>Yelp2018</cell><cell>31, 668 38, 048</cell><cell>1, 561, 406 0.00130</cell></row><row><cell cols="2">Amazon 192, 403 63, 001</cell><cell>1, 689, 188 0.00014</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2 :</head><label>2</label><figDesc>Overall Performance Comparison. 0.0357 * 0.0713 * 0.0589 * 0.0460 * 0.0216 *</figDesc><table><row><cell></cell><cell cols="2">Alibaba</cell><cell cols="2">Yelp2018</cell><cell cols="2">Amazon</cell></row><row><cell></cell><cell>Recall</cell><cell>NDCG</cell><cell>Recall</cell><cell>NDCG</cell><cell>Recall</cell><cell>NDCG</cell></row><row><cell>LightGCN+RNS</cell><cell>0.0584</cell><cell>0.0275</cell><cell>0.0628</cell><cell>0.0515</cell><cell>0.0398</cell><cell>0.0177</cell></row><row><cell>LightGCN+DNS</cell><cell>0.0737</cell><cell>0.0343</cell><cell>0.0695</cell><cell>0.0571</cell><cell>0.0449</cell><cell>0.0211</cell></row><row><cell>LightGCN+IRGAN</cell><cell>0.0605</cell><cell>0.0280</cell><cell>0.0641</cell><cell>0.0527</cell><cell>0.0412</cell><cell>0.0185</cell></row><row><cell>LightGCN+AdvIR</cell><cell>0.0583</cell><cell>0.0273</cell><cell>0.0624</cell><cell>0.0510</cell><cell>0.0401</cell><cell>0.0185</cell></row><row><cell>LightGCN+MCNS</cell><cell>0.0632</cell><cell>0.0284</cell><cell>0.0658</cell><cell>0.0529</cell><cell>0.0423</cell><cell>0.0192</cell></row><row><cell cols="2">LightGCN+MixGCF 0.0763  NGCF+RNS 0.0426</cell><cell>0.0197</cell><cell>0.0577</cell><cell>0.0469</cell><cell>0.0294</cell><cell>0.0123</cell></row><row><cell>NGCF+DNS</cell><cell>0.0453</cell><cell>0.0207</cell><cell>0.0650</cell><cell>0.0529</cell><cell>0.0312</cell><cell>0.0130</cell></row><row><cell>NGCF+IRGAN</cell><cell>0.0435</cell><cell>0.0200</cell><cell>0.0615</cell><cell>0.0502</cell><cell>0.0283</cell><cell>0.0120</cell></row><row><cell>NGCF+AdvIR</cell><cell>0.0440</cell><cell>0.0203</cell><cell>0.0614</cell><cell>0.0500</cell><cell>0.0318</cell><cell>0.0134</cell></row><row><cell>NGCF+MCNS</cell><cell>0.0430</cell><cell>0.0200</cell><cell>0.0625</cell><cell>0.0501</cell><cell>0.0313</cell><cell>0.0136</cell></row><row><cell cols="2">NGCF+MixGCF 0.0544  PinSage+RNS 0.0196</cell><cell>0.0085</cell><cell>0.0410</cell><cell>0.0328</cell><cell>0.0193</cell><cell>0.0080</cell></row><row><cell>PinSage+DNS</cell><cell>0.0405</cell><cell>0.0183</cell><cell>0.0590</cell><cell>0.0488</cell><cell>0.0217</cell><cell>0.0088</cell></row><row><cell>PinSage+IRGAN</cell><cell>0.0200</cell><cell>0.0090</cell><cell>0.0422</cell><cell>0.0343</cell><cell>0.0248</cell><cell>0.0088</cell></row><row><cell>PinSage+AdvIR</cell><cell>0.0196</cell><cell>0.0090</cell><cell>0.0387</cell><cell>0.0313</cell><cell>0.0243</cell><cell>0.0087</cell></row><row><cell>PinSage+MCNS</cell><cell>0.0212</cell><cell>0.0095</cell><cell>0.0432</cell><cell>0.0349</cell><cell>0.0202</cell><cell>0.0088</cell></row><row><cell>PinSage+MixGCF</cell><cell>0.0489</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note>* * 0.0262 * 0.0688 * 0.0566 * 0.0350 * 0.0154 * * 0.0226 * 0.0632 * 0.0525 * 0.0273 * 0.0124 * ‚Ä¢ NGCF [43]: Inspired by Graph Convolution Network</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 3 :</head><label>3</label><figDesc>Performance of MixGCF without positive mixing.</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 4 :</head><label>4</label><figDesc>Impact of the number of aggregation modules (ùêø).</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 5 :</head><label>5</label><figDesc>Impact of the size of candidate set (ùëÄ).</figDesc><table><row><cell></cell><cell></cell><cell cols="2">Alibaba</cell><cell cols="2">Yelp2018</cell><cell cols="2">Amazon</cell></row><row><cell></cell><cell></cell><cell>Recall</cell><cell>NDCG</cell><cell>Recall</cell><cell>NDCG</cell><cell>Recall</cell><cell>NDCG</cell></row><row><cell></cell><cell>M=8</cell><cell>0.0697</cell><cell>0.0311</cell><cell>0.0664</cell><cell>0.0547</cell><cell>0.0443</cell><cell>0.0203</cell></row><row><cell>LightGCN+MixGCF</cell><cell cols="3">M=16 0.0728 M=32 0.0763  *  0.0357  *  0.0339</cell><cell>0.0684 0.0703</cell><cell cols="3">0.0562 0.0460  *  0.0216  *  0.0579 0.0455 0.0215</cell></row><row><cell></cell><cell cols="2">M=64 0.0744</cell><cell cols="3">0.0355 0.0713  *  0.0589  *</cell><cell>0.0430</cell><cell>0.0206</cell></row><row><cell></cell><cell>M=8</cell><cell>0.0468</cell><cell>0.0201</cell><cell>0.0627</cell><cell>0.0512</cell><cell>0.0350</cell><cell>0.0147</cell></row><row><cell>NGCF+MixGCF</cell><cell cols="2">M=16 0.0518 M=32 0.0532</cell><cell>0.0237 0.0253</cell><cell>0.0658 0.0682</cell><cell>0.0539 0.0560</cell><cell>0.0333 0.0347</cell><cell>0.0144 0.0154</cell></row><row><cell cols="3">M=8 M=16 0.0388 0.0178 M=64 0.0544  PinSage+MixGCF M=32 0.0435</cell><cell>0.0075 0.0173 0.0195</cell><cell>0.0495 0.0546 0.0608</cell><cell>0.0402 0.0448 0.0501</cell><cell>0.0204 0.0207 0.0238</cell><cell>0.0072 0.0084 0.0106</cell></row><row><cell></cell><cell cols="2">M=64 0.0489</cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note>* 0.0262 * 0.0688 * 0.0566 * 0.0350 * 0.0154 * * 0.0226 * 0.0632 * 0.0525 * 0.0273 * 0.0124 *</note></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Acknowledgements. We thank Fanjin Zhang and Da Yin for their helpful comments. This work is funded by National Natural Science Foundation of China (Key Program, No. 61836013), National Science Foundation for Distinguished Young Scholars (No. 61825602).</p></div>
			</div>

			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A APPENDIX</head><p>In the appendix, we present the detailed implementation note of MixGCF, including the running environment and the parameter settings. We then formally introduce ùêæ-pair loss. Finally, we describe the datasets, i.g., Alibaba, Yelp2018 and Amazon, and evaluation metrics. All codes used in this paper are publicly available.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.1 Implementation Note</head><p>Running Environment. The experiments are conducted on a single Linux server with Intel(R) Xeon(R) CPU Gold 6420 @ 2.60GHz, 376G RAM and 10 NVIDIA GeForce RTX 3090TI-24GB. Our method is implemented on PyTorch 1.7.0 and Python 3.7.6. Hyperparameter settings. The following Table lists the parameter settings of each recommender and MixGCF on three datasets. The hyperparameters include the learning rate ùëôùëü , the embedding size ùëë, the number of aggregation modules ùêø, the batch size ùêµ, the coefficient of ùêø2 regularization ùëô 2 , and the size of the candidate set ùëÄ.</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">KBGAN: Adversarial Learning for Knowledge Graph Embeddings</title>
		<author>
			<persName><forename type="first">Liwei</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">William</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wangc</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NAACL-HLT&apos;18</title>
				<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="1470" to="1480" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Unifying Knowledge Graph Learning and Recommendation: Towards a Better Understanding of User Preferences</title>
		<author>
			<persName><forename type="first">Yixin</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiangnan</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zikun</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tat-Seng</forename><surname>Chua</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">WWW</title>
				<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Revisiting Graph based Collaborative Filtering: A Linear Residual Graph Convolutional Network Approach</title>
		<author>
			<persName><forename type="first">Lei</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Le</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Richang</forename><surname>Hong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kun</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Meng</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
				<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">On Sampling Strategies for Neural Network-based Collaborative Filtering</title>
		<author>
			<persName><forename type="first">Ting</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yizhou</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yue</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Liangjie</forename><surname>Hong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">KDD</title>
				<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="767" to="776" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Deep Neural Networks for YouTube Recommendations</title>
		<author>
			<persName><forename type="first">Paul</forename><surname>Covington</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jay</forename><surname>Adams</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Emre</forename><surname>Sargin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">RecSys</title>
				<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="191" to="198" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Simplify and Robustify Negative Sampling for Implicit Collaborative Filtering</title>
		<author>
			<persName><forename type="first">Jingtao</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuhan</forename><surname>Quan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Quanming</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yong</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Depeng</forename><surname>Jin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NeurIPS</title>
				<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">Graph neural networks for social recommendation</title>
		<author>
			<persName><forename type="first">Wenqi</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yao</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qing</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuan</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Eric</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiliang</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dawei</forename><surname>Yin</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="417" to="426" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Graph Random Neural Network for Semi-Supervised Learning on Graphs</title>
		<author>
			<persName><forename type="first">Wenzheng</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jie</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuxiao</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yu</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Huanbo</forename><surname>Luan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qian</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qiang</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Evgeny</forename><surname>Kharlamov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jie</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NeurIPS</title>
				<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">Understanding the difficulty of training deep feedforward neural networks</title>
		<author>
			<persName><forename type="first">Xavier</forename><surname>Glorot</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2010">2010</date>
			<biblScope unit="page" from="249" to="256" />
		</imprint>
	</monogr>
	<note>In AISTATS</note>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">node2vec: Scalable feature learning for networks</title>
		<author>
			<persName><forename type="first">Aditya</forename><surname>Grover</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jure</forename><surname>Leskovec</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">KDD</title>
				<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="855" to="864" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Inductive Representation Learning on Large Graphs</title>
		<author>
			<persName><forename type="first">William</forename><forename type="middle">L</forename><surname>Hamilton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rex</forename><surname>Ying</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jure</forename><surname>Leskovec</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NeurIPS</title>
				<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="1025" to="1035" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Momentum contrast for unsupervised visual representation learning</title>
		<author>
			<persName><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Haoqi</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuxin</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Saining</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
				<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">LightGCN: Simplifying and Powering Graph Convolution Network for Recommendation</title>
		<author>
			<persName><forename type="first">Xiangnan</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kuan</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yan</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yongdong</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Meng</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SIGIR</title>
				<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">Neural Collaborative Filtering</title>
		<author>
			<persName><forename type="first">Xiangnan</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lizi</forename><surname>Liao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hanwang</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Liqiang</forename><surname>Nie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xia</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tat-Seng</forename><surname>Chua</surname></persName>
		</author>
		<idno>WWW. 173-182</idno>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Practical Lessons from Predicting Clicks on Ads at Facebook</title>
		<author>
			<persName><forename type="first">Xinran</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Junfeng</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ou</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tianbing</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bo</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tao</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yanxin</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Antoine</forename><surname>Atallah</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ralf</forename><surname>Herbrich</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ADKDD</title>
				<meeting><address><addrLine>Stuart Bowers, and Joaquin Qui√±onero Candela</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Collaborative Metric Learning</title>
		<author>
			<persName><forename type="first">Cheng-Kang</forename><surname>Hsieh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Longqi</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yin</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tsung-Yi</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Serge</forename><surname>Belongie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Deborah</forename><surname>Estrin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">WWW</title>
				<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Embeddingbased Retrieval in Facebook Search</title>
		<author>
			<persName><forename type="first">Jui-Ting</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ashish</forename><surname>Sharma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shuying</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Li</forename><surname>Xia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Philip</forename><surname>Pronin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Janani</forename><surname>Padmanabhan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Giuseppe</forename><surname>Ottaviano</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Linjun</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">KDD</title>
				<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Hard Negative Mixing for Contrastive Learning</title>
		<author>
			<persName><forename type="first">Yannis</forename><surname>Kalantidis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bulent</forename><surname>Mert</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Noe</forename><surname>Sariyildiz</surname></persName>
		</author>
		<author>
			<persName><surname>Pion</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NeurIPS</title>
				<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note>Philippe Weinzaepfel, and Diane Larlus</note>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">Adam: A Method for Stochastic Optimization</title>
		<author>
			<persName><forename type="first">P</forename><surname>Diederik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jimmy</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName><surname>Ba</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
	<note>In ICLR</note>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">Semi-Supervised Classification with Graph Convolutional Networks</title>
		<author>
			<persName><forename type="first">Thomas</forename><forename type="middle">N</forename><surname>Kipf</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Max</forename><surname>Welling</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
	<note>In ICLR</note>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Matrix Factorization Techniques for Recommender Systems</title>
		<author>
			<persName><forename type="first">Yehuda</forename><surname>Koren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Robert</forename><forename type="middle">M</forename><surname>Bell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chris</forename><surname>Volinsky</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Computer</title>
				<imprint>
			<date type="published" when="2009">2009. 2009</date>
			<biblScope unit="volume">42</biblScope>
			<biblScope unit="page" from="30" to="37" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">On Sampled Metrics for Item Recommendation</title>
		<author>
			<persName><forename type="first">Walid</forename><surname>Krichene</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Steffen</forename><surname>Rendle</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">KDD</title>
				<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="1748" to="1757" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Sorec: social recommendation using probabilistic matrix factorization</title>
		<author>
			<persName><forename type="first">Hao</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Haixuan</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Irwin</forename><surname>Michael R Lyu</surname></persName>
		</author>
		<author>
			<persName><surname>King</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CIKM</title>
				<imprint>
			<date type="published" when="2008">2008</date>
			<biblScope unit="page" from="931" to="940" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Recommender systems with social regularization</title>
		<author>
			<persName><forename type="first">Hao</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dengyong</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chao</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Irwin</forename><surname>Michael R Lyu</surname></persName>
		</author>
		<author>
			<persName><surname>King</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">WSDM</title>
				<imprint>
			<date type="published" when="2011">2011</date>
			<biblScope unit="page" from="287" to="296" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Image-based recommendations on styles and substitutes</title>
		<author>
			<persName><forename type="first">Julian</forename><surname>Mcauley</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christopher</forename><surname>Targett</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qinfeng</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Anton</forename><surname>Van Denhengel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SIGIR</title>
				<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title level="m" type="main">Distributed representations of words and phrases and their compositionality</title>
		<author>
			<persName><forename type="first">Tomas</forename><surname>Mikolov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kai</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Greg</forename><forename type="middle">S</forename><surname>Corrado</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jeff</forename><surname>Dean</surname></persName>
		</author>
		<editor>NeurIPS</editor>
		<imprint>
			<date type="published" when="2013">2013</date>
			<biblScope unit="page" from="3111" to="3119" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">One-class collaborative filtering</title>
		<author>
			<persName><forename type="first">Rong</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yunhong</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bin</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nathan</forename><forename type="middle">N</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rajan</forename><surname>Lukose</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Martin</forename><surname>Scholz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qiang</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICDM</title>
				<imprint>
			<date type="published" when="2008">2008</date>
			<biblScope unit="page" from="502" to="511" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Adversarial Sampling and Training for Semi-Supervised Information Retrieval</title>
		<author>
			<persName><forename type="first">Dae</forename><surname>Hoon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Park</forename></persName>
		</author>
		<author>
			<persName><forename type="first">Yi</forename><surname>Chang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">WWW</title>
				<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="1443" to="1453" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">DeepInf: Social Influence Prediction with Deep Learning</title>
		<author>
			<persName><forename type="first">Jiezhong</forename><surname>Qiu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jian</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hao</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuxiao</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kuansan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jie</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">KDD</title>
				<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="2110" to="2119" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Improving pairwise learning for item recommendation from implicit feedback</title>
		<author>
			<persName><forename type="first">Steffen</forename><surname>Rendle</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christoph</forename><surname>Freudenthaler</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">WSDM</title>
				<imprint>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="273" to="282" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">BPR: Bayesian Personalized Ranking from Implicit Feedback</title>
		<author>
			<persName><forename type="first">Steffen</forename><surname>Rendle</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christoph</forename><surname>Freudenthaler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zeno</forename><surname>Gantner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lars</forename><surname>Schmidt-Thieme</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">UAI</title>
				<imprint>
			<date type="published" when="2009">2009</date>
			<biblScope unit="page" from="452" to="461" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">FaceNet: A Unified Embedding for Face Recognition and Clustering</title>
		<author>
			<persName><forename type="first">Florian</forename><surname>Schroff</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dmitry</forename><surname>Kalenichenko</surname></persName>
		</author>
		<author>
			<persName><forename type="first">James</forename><surname>Philbin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
				<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Transformation invariance in pattern recognitiontangent distance and tangent propagation</title>
		<author>
			<persName><forename type="first">Patrice</forename><forename type="middle">Y</forename><surname>Simard</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yann</forename><forename type="middle">A</forename><surname>Le Cun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">John</forename><forename type="middle">S</forename><surname>Denker</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bernard</forename><surname>Victorri</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Neural networks: tricks of the trade</title>
				<imprint>
			<date type="published" when="1998">1998</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
		<title level="m" type="main">Improved Deep Metric Learning with Multi-class N-pair Loss Objective</title>
		<author>
			<persName><forename type="first">Kihyuk</forename><surname>Sohn</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
	<note>In NeurIPS</note>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Learning to Hash with Graph Neural Networks for Recommender Systems</title>
		<author>
			<persName><forename type="first">Qiaoyu</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ninghao</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xing</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hongxia</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jingren</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xia</forename><surname>Hu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">WWW</title>
				<imprint>
			<date type="published" when="1988">2020. 1988-1998</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<monogr>
		<title level="m" type="main">Line: Large-scale information network embedding</title>
		<author>
			<persName><forename type="first">Jian</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Meng</forename><surname>Qu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mingzhe</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ming</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Junyan</forename></persName>
		</author>
		<author>
			<persName><forename type="first">Qiaozhu</forename><surname>Mei</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="1067" to="1077" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Graph Convolutional Matrix Completion</title>
		<author>
			<persName><forename type="first">Rianne</forename><surname>Van Den</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Thomas</forename><forename type="middle">N</forename><surname>Berg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Max</forename><surname>Kipf</surname></persName>
		</author>
		<author>
			<persName><surname>Welling</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">KDD</title>
				<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">RippleNet: Propagating User Preferences on the Knowledge Graph for Recommender Systems</title>
		<author>
			<persName><forename type="first">Hongwei</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fuzheng</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jialin</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Miao</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wenjie</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xing</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Minyi</forename><surname>Guo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CIKM</title>
				<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="417" to="426" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Knowledge-aware Graph Neural Networks with Label Smoothness Regularization for Recommender Systems</title>
		<author>
			<persName><forename type="first">Hongwei</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fuzheng</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mengdi</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jure</forename><surname>Leskovec</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Miao</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wenjie</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhongyuan</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">KDD</title>
				<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">IRGAN: A Minimax Game for Unifying Generative and Discriminative Information Retrieval Models</title>
		<author>
			<persName><forename type="first">Jun</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lantao</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Weinan</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yu</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yinghui</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Benyou</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Peng</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dell</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SIGIR</title>
				<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="515" to="524" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Neural Memory Streaming Recommender Networks with Adversarial Training</title>
		<author>
			<persName><forename type="first">Qinyong</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hongzhi</forename><surname>Yin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhiting</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Defu</forename><surname>Lian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hao</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zi</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">KDD</title>
				<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="2467" to="2475" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">KGAT:Knowledge Graph Attention Network for Recommendation</title>
		<author>
			<persName><forename type="first">Xiang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiangnan</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yixin</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Meng</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tat-Seng</forename><surname>Chua</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">KDD</title>
				<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="950" to="958" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Neural Graph Collaborative Filtering</title>
		<author>
			<persName><forename type="first">Xiang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiangnan</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Meng</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fuli</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tat-Seng</forename><surname>Chua</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SIGIR</title>
				<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Reinforced Negative Sampling over Knowledge Graph for Recommendation</title>
		<author>
			<persName><forename type="first">Xiang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yaokun</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiangnan</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yixin</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Meng</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tat-Seng</forename><surname>Chua</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">WWW</title>
				<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">CKAN: Collaborative Knowledge-aware Attentive Network for Recommender Systems</title>
		<author>
			<persName><forename type="first">Ze</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Guangyan</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Huobin</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qinghong</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiyang</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SIGIR</title>
				<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="219" to="228" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">A Neural Influence Diffusion Model for Social Recommendation</title>
		<author>
			<persName><forename type="first">Le</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Peijie</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yanjie</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Richang</forename><surname>Hong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiting</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Meng</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SIGIR</title>
				<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Representation Learning on Graphs with Jumping Knowledge Networks</title>
		<author>
			<persName><forename type="first">Keyulu</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chengtao</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yonglong</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tomohiro</forename><surname>Sonobe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ken</forename><surname>Ichi Kawarabayashi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stefanie</forename><surname>Jegelka</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
				<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Understanding Negative Sampling in Graph Representation Learning</title>
		<author>
			<persName><forename type="first">Zhen</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ming</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chang</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hongxia</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jingren</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jie</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">KDD</title>
				<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Graph convolutional neural networks for web-scale recommender systems</title>
		<author>
			<persName><forename type="first">Rex</forename><surname>Ying</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ruining</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kaifeng</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pong</forename><surname>Eksombatchai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">William</forename><forename type="middle">L</forename><surname>Hamilton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jure</forename><surname>Leskovec</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">KDD</title>
				<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="974" to="983" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<monogr>
		<author>
			<persName><forename type="first">Hao</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Huanyu</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jianxin</forename><surname>Wu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2101.04342</idno>
		<title level="m">Mixup Without Hesitation</title>
				<imprint>
			<date type="published" when="2021">2021. 2021</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b50">
	<monogr>
		<title level="m" type="main">mixup: Beyond empirical risk minimization</title>
		<author>
			<persName><forename type="first">Hongyi</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Moustapha</forename><surname>Cisse</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yann</forename><forename type="middle">N</forename><surname>Dauphin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><surname>Lopez-Paz</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note>In ICLR</note>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Optimizing top-n collaborative filtering via dynamic negative item sampling</title>
		<author>
			<persName><forename type="first">Weinan</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tianqi</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jun</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yong</forename><surname>Yu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SIGIR</title>
				<imprint>
			<date type="published" when="2013">2013</date>
			<biblScope unit="page" from="785" to="788" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">Deep interest network for click-through rate prediction</title>
		<author>
			<persName><forename type="first">Guorui</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaoqiang</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chenru</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ying</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Han</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiao</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yanghui</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Junqi</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Han</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kun</forename><surname>Gai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">KDD</title>
				<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="1059" to="1068" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
