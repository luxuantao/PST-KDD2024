<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">3D Surface Reconstruction Using Graph Cuts with Surface Constraints</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author role="corresp">
							<persName><forename type="first">Son</forename><surname>Tran</surname></persName>
							<email>sontran@cs.umd.edu</email>
							<affiliation key="aff0">
								<orgName type="department">Dept. of Computer Science</orgName>
								<orgName type="institution">University of Maryland</orgName>
								<address>
									<postCode>20742</postCode>
									<settlement>College Park</settlement>
									<region>MD</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Larry</forename><surname>Davis</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Dept. of Computer Science</orgName>
								<orgName type="institution">University of Maryland</orgName>
								<address>
									<postCode>20742</postCode>
									<settlement>College Park</settlement>
									<region>MD</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">3D Surface Reconstruction Using Graph Cuts with Surface Constraints</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">6F3A13D8536C8E9F18812498C0889693</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.3" ident="GROBID" when="2023-07-27T08:41+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>We describe a graph cut algorithm to recover the 3D object surface using both silhouette and foreground color information. The graph cut algorithm is used for optimization on a color consistency field. Constraints are added to improve its performance. These constraints are a set of predetermined locations that the true surface of the object is likely to pass through. They are used to preserve protrusions and to pursue concavities respectively in the first and the second phase of the algorithm. We also introduce a method for dealing with silhouette uncertainties arising from background subtraction on real data. We test the approach on synthetic data with different numbers of views <ref type="bibr" target="#b7">(8,</ref> 16, 32,  64)  and on a real image set containing 30 views of a toy squirrel.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>We consider the problem of reconstructing the 3D surface of an object from a set of images taken from calibrated viewpoints. The information exploited includes the object's silhouettes and its foreground color or texture. 3D shape recovery using silhouettes constitutes a major line of research in computer vision, the shape-from-silhouette approach. In methods employing silhouettes only (see e.g. <ref type="bibr" target="#b0">[1]</ref>), voxels in a volume are carved away until their projected images are consistent with the set of silhouettes. The resulting object is the visual hull. In general, the visual hull can be represented in other forms such as bounding edges ( <ref type="bibr" target="#b1">[2]</ref>), and can be reconstructed in a number of different ways. The main drawback of visual hulls is that they are unable to capture concavities on the object surface ( <ref type="bibr" target="#b2">[3]</ref>).</p><p>A 3D surface can also be reconstructed using color or texture consistency between different views. Stereo techniques find the best pixel matching between pairs of views and construct disparity maps which represent (partial) shapes. Combining from multiple stereo maps has been studied, but is quite complicated ( <ref type="bibr" target="#b3">[4]</ref>). Space carving ( <ref type="bibr" target="#b4">[5]</ref>) and recent surface evolution methods (e.g. <ref type="bibr" target="#b5">[6]</ref>, <ref type="bibr" target="#b6">[7]</ref>) use a more general consistency check among multiple views.</p><p>The combination of both silhouettes and foreground color to reconstruct an object's surface has been studied in a number of recent papers ( <ref type="bibr" target="#b6">[7]</ref>, <ref type="bibr" target="#b7">[8]</ref>, <ref type="bibr" target="#b8">[9]</ref>).</p><p>Our work is motivated by <ref type="bibr" target="#b7">[8]</ref> and <ref type="bibr" target="#b9">[10]</ref> where the graph cut algorithm serves as the underlying 3D discrete optimization tool. The near global optimality properties of the graph cut algorithm are discussed in <ref type="bibr" target="#b10">[11]</ref>. As noted in <ref type="bibr" target="#b7">[8]</ref> and in other works however, the graph cut algorithm usually prefers shorter cuts, which leads to protrusive parts of the object surface being cut off. We overcome this limitation with a two-phase procedure. In the first phase (phase I), protrusions are protected during the optimization by forcing the solution to pass close to a set of predetermined surface points called "constraint points". In the second phase (phase II), concavities on the object surface are aggressively pursued. Silhouette uncertainties, which are important in practice but have been ignored in previous research ( <ref type="bibr" target="#b7">[8]</ref>, <ref type="bibr" target="#b8">[9]</ref>, . . . ) are also taken into account.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.1">Related Works</head><p>The application of reliable surface points to constrain the reconstruction of a surface appears in a number of recent papers ([2], <ref type="bibr" target="#b6">[7]</ref>, <ref type="bibr" target="#b8">[9]</ref>, . . . ). Isidoro et al ( <ref type="bibr" target="#b6">[7]</ref>) refine the shape and texture map with an EM-like procedure; the evolution of the shape at each iteration is anchored around a set of locations called frontier points. Cheung et al ( <ref type="bibr" target="#b1">[2]</ref>) use another set of points called color surface points to align multiple visual hulls constructed at different times to obtain a closer approximation to the object's true surface. Usually, these points have no special patterns on the surface. In some cases, however, they might lie on continuous curves such as the rims in <ref type="bibr" target="#b8">[9]</ref>, where each (smooth and closed) rim is a contour generator. The mesh of rims can be used to partition the surface into local patches. Surface estimation is then performed individually for each patch, with some interaction to ensure certain properties such as smoothness.</p><p>The identification of these surface points is typically based on the silhouettes and color/photo consistency. A frontier point in <ref type="bibr" target="#b6">[7]</ref> is the point with lowest texture back-projection error among those on the evolving surface that project onto a single silhouette point. Frontier points are recomputed at each iteration. The rims in <ref type="bibr" target="#b8">[9]</ref> are built with a rim mesh algorithm. In order for the mesh to exist, certain assumptions have to be made, the most limiting one being no self-occlusion. In <ref type="bibr" target="#b1">[2]</ref>, the colored surface points are searched for along bounding edges which collectively represent the surface of the object.</p><p>Surface reconstruction methods that use color or texture such as <ref type="bibr" target="#b1">[2]</ref>, <ref type="bibr" target="#b7">[8]</ref>, <ref type="bibr" target="#b6">[7]</ref>, <ref type="bibr" target="#b8">[9]</ref> and most stereo algorithms involve optimization. The original space carving algorithm ( <ref type="bibr" target="#b4">[5]</ref>) used a simple greedy algorithm. Other examples of local methods include stochastic search ( <ref type="bibr" target="#b6">[7]</ref>) and, recently, surface evolution using level sets or PDEs (e.g. <ref type="bibr" target="#b5">[6]</ref>). Local techniques are often sensitive to initialization and local minimum. Here, we use the 3D graph cut algorithm which is more global in scope ( <ref type="bibr" target="#b10">[11]</ref>). It was applied in <ref type="bibr" target="#b2">[3]</ref> to solve the occupancy problem and in <ref type="bibr" target="#b9">[10]</ref> for 3D image segmentation. The work described in <ref type="bibr" target="#b6">[7]</ref> has similar motivation to ours: developing a constrained graph cut solution to object surface recovery. Their constraints are based on the rim mesh mentioned above. Multiple interconnected sub-graphs are built, with one for each rim mesh face. Our constraint points are not required to form rims and we use only one graph; our formulation is most similar to <ref type="bibr" target="#b7">[8]</ref>, which is the departure point for our research. Section 2 describes the basic steps of the formulation from <ref type="bibr" target="#b7">[8]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Volumetric Graph Cuts</head><p>Following <ref type="bibr" target="#b7">[8]</ref>, we first construct the visual hull V from the set of N image silhouettes, denoted {Sil i }. V is used as the initial approximation to the object shape. A photo consistency field for all voxels v ∈ V is constructed and used as the graph on which a graph cut optimization is performed. Visibility for a voxel v ∈ V , V is(v), is approximated with the visibility of the closest voxel to v on the surface S out of V . The consistency score for v, ρ(v) is the weighted normalized cross correlation (NCC) between the pairs of local image patches that v projects to in the different views:</p><formula xml:id="formula_0">ρ(v) = Ci,Cj ∈V is(v) w(pos(C i , C j ))N CC(p(C i , v), p(C j , v)) (<label>1</label></formula><formula xml:id="formula_1">)</formula><p>where w(pos(C i , C j ) is a weight depending on the relative position of the two camera centers C i and C j (small when the difference between the viewing angles of the i-th and j-th cameras is large and vice versa); p(C i , v) is the local image patch around the image of v in the i-th image I i . If the surface, S out , of the visual hull, V , is not far from the actual surface S * , then with consistency computed this way, voxels that lie on S * would have smallest ρ values (Figure <ref type="figure" target="#fig_0">1</ref>.a). Therefore, finding S * can be formulated as an energy minimization problem, where the energy is defined as</p><formula xml:id="formula_2">E(S) = S ρ(x)dA<label>(2)</label></formula><p>A graph cut algorithm can be used to solve this problem in a manner similar to <ref type="bibr" target="#b11">[12]</ref> and <ref type="bibr" target="#b9">[10]</ref>. Each voxel is a node in the graph, G, with a 6-neighbor system for edges. The weight for the edge between voxel (node) v i and v j is defined as <ref type="figure" target="#fig_0">1</ref>.b), where h is the voxel size. S out and S inthe surface inside V at a distance d from S outform an enclosing volume in which S * is assumed to lie. Similar to <ref type="bibr" target="#b11">[12]</ref> and <ref type="bibr" target="#b8">[9]</ref>, every voxel v ∈ S in (S out ) is connected to the Sink (Source) node through an edge with very high weight. With the graph G constructed this way, the graph cut algorithm is then applied to find S * .</p><formula xml:id="formula_3">w(v i , v j ) = 4/3πh 2 (ρ(v i ) + ρ(v j ))/2 (Figure</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Graph Cut with Surface Point Constraints</head><p>As mentioned in <ref type="bibr" target="#b7">[8]</ref>, the above procedure suffers from the limitation that the graph cut algorithm prefers shorter cuts. This produces inaccurate surfaces at protrusions, which are often cut off ( <ref type="bibr" target="#b7">[8]</ref>). We address this problem by constraining the solution cut to pass through certain surface points. First we show how to identify those points. Next, we show how to enforce the solution cut to pass through or close to them. Finally, methods for dealing with silhouette uncertainty are included.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Constraint on Surface Points</head><p>Assume, to begin with, that the set of silhouettes has absolute locational certainty. Every ray (C i , p j i ) from a camera center C i through a point p j i on the silhouette Sil i has to touch the object surface at at least one point P ([2], <ref type="bibr" target="#b8">[9]</ref>) (Figure <ref type="figure" target="#fig_1">2</ref>.a). In <ref type="bibr" target="#b1">[2]</ref>, the authors search for P along this ray. We, additionally, take into account the discretization of the silhouette and make the search region not a single ray (C i , p j i ) but a surface patch s ⊂ S out where s = {v | v ∈ S out and v projects to p j i through C i }. Since every voxel on S out has to project onto some point on some silhouette {Sil i }, the union of all s is S out . Therefore, S out is completely accounted for when we search for all P 's. In <ref type="bibr" target="#b6">[7]</ref>, the authors also use the projection from object space to silhouettes to find the search regions for their set of constraint points. However, these regions, and therefore the resulting constraint points, lie on an evolving surface and have to be recomputed at each step of their iterative procedure. Here, the determination of P is done only once and is based on S out , the surface of the original visual hull. Let P denotes the set of all such P 's. To identify the location of each P ∈ P within its corresponding search region, we use color or texture information from the image foreground. Ideally, the images of such voxels should have zero consistency score ρ or zero color variance. Practically, they are voxels whose projections have the lowest ρ within a search region. Figure <ref type="figure" target="#fig_1">2</ref>.b shows an example of the constraint points, P, for the synthetic face that is used in the experiments in section 5. Note that their distribution is quite general and they do not obviously form rims. This creates difficulties for approaches that assume exact silhouette information such as <ref type="bibr" target="#b8">[9]</ref> and <ref type="bibr" target="#b12">[13]</ref> . By marking which sub-regions of S out are produced by which camera, P can be constructed in time linear in the number of voxels in S out .</p><p>If the average number of points on a silhouette is n s , then the number of points in P is N.n s . Many of them lie on protrusive parts of the object surface. In general, P provides a large set of constraints for the graph cut optimization.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Graph Cut with Surface Constraint Points</head><p>Given the set of surface constraint voxels, P, we want to construct a cut that passes through every voxel p ∈ P. Unfortunately, it is difficult to introduce such constraints directly into the 3D graph cut algorithm. Instead, we adopt an indirect approach by blocking the solution surface from cutting a continuous region that connects p and S in . Figure <ref type="figure" target="#fig_2">3</ref>.a illustrates the blocking region: it is a curve bl(p) from the surface point p ∈ P to S in . More generally, a blocking region can be represented as a blurred volume around the blocking curves using a Gaussian blurring function. We next describe how to construct bl(p).</p><p>Let D(S) and ∇D(S) denote the 3D distance transform of a surface S and the gradient of the distance transform, respectively. For each p ∈ P, the corresponding curve bl(p) is constructed using ∇D(S out ) and ∇D(S in ) as follows. First, starting from p, we move along ∇D(S out ) for a small distance l. Second, we follow -∇D(S in ) until S in is met. Points are added into bl(p) as we move. To avoid redundancy, if a point is met that has been added to some previously constructed bl(p ), we stop collecting points for bl(p). This procedure is carried out for all points in P. D(S out ) can be considered as an implicit shape representation with the zerolevel set being S out ; so, the normal of S out at a surface point p is the gradient of D(S out ), i.e. ∇D(S out ), evaluated at that point. Therefore, in the first step, we initially move in the direction of the normal of S out at p. Given that p is assumed to be on the true surface, S * , by moving this way, we will reduce the chance of erroneously "crossing" S * . After a small distance l, we could have continued to move along ∇D(S out ). However, we switch to moving along -∇D(S in ) for the following reasons. First, if we have a group of constraint points that are close together, then their respective bl(p)'s built by using ∇D(S out ) will usually meet and collapse into a single curve well before S in is reached. Such a merge is not desirable when the graph cut weight from a voxel v in bl(p) to the Sink node is not set to infinity, but to some other smaller value. (This is necessary for dealing with noise and discretization ambiguities -see below). Second, there are places where the above gradient fields vanish, and we must either abandon constructing the current bl(p) or need several bookkeeping steps such as making small random jumps to take care of this issue. Of the two gradient fields, ∇D(S in ) is more homogenous and this happens less frequently to it. This procedure constructs the set of all blocking curves BL through which the solution cut should not pass. This constraint might be incorporated into the graph cut algorithm by setting the weights of the edges from each voxel in BL to the Sink node to be infinity. However, the set P (and hence BL) often contains false positives, so this strategy can lead to significant errors. Therefore, instead, for every voxel v ∈ BL, we set w(v, Sink) = 4/3πh 2 , where h is the voxel size. This is the maximum weight for the edges between any two neighboring voxels in V . This uniform weight setting works well provided that the silhouette set is accurate, as shown in experiments on synthetic data in section 5.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Incorporating silhouette uncertainties.</head><p>When dealing with real image sequences, errors in background subtraction and from the morphological operations typically employed to find silhouettes introduce artifacts ( <ref type="bibr" target="#b2">[3]</ref>). So, there is always uncertainty in silhouette extraction. We would, of course, like our silhouette to be as accurate as possible. But we still need to measure the local positional uncertainty of the silhouette and incorporate this uncertainty into the surface estimation algorithm. We extract silhouettes in the following way. First a background image, I bgr , is subtracted from the image I, with I = |I -I bgr |. Then, a small threshold θ I = 2σ noise is applied to I to get the largest connected component BW obj , which is assumed to contain the object's true silhouette. Next, along the boundary of BW obj , we find the set P fix -a set of high confidence silhouette points -whereP fix = {p | I &gt; Θ I } and Θ I is a large threshold. Finally, an active contour method is applied to I with points in P fix being part of the contour and fixed. So, we first identify boundary sections with high likelihood of being on the silhouette and recover the rest of the silhouette with an active contour. The associated uncertainties for points on contours are measured with a quadratic function as described below.</p><p>The uncertainties on the location of the silhouette affect the process of finding P. To account for them, we need to determine the probability that a point in P is really on S * . It is estimated with a combination of cues from silhouette and photometric consistency, i.e.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Pr(p ∈ P) ∼ Pr(P hotoConsistency(p), a ∈ Sil</head><formula xml:id="formula_4">i )<label>( 3 )</label></formula><p>where p projects to the point a on the silhouette Sil i through the camera center C i . Assuming that photo consistency and silhouette uncertainty are independent, we have</p><p>Pr(p ∈ P) ∼ Pr(P hotoConsistency(p))Pr(a</p><formula xml:id="formula_5">∈ Sil i ) ( 4 ) ∼ ρ(p)Pr(a ∈ Sil i )<label>( 5 )</label></formula><p>where, similar to <ref type="bibr" target="#b2">[3]</ref>, Pr(a</p><formula xml:id="formula_6">∈ Sil i ) is a truncated linear function of | I(a)| 2 .</formula><p>(Figure <ref type="figure" target="#fig_2">3</ref>.b illustrates uncertainty measure along the contour extracted from a difference image).</p><p>The search region, s ⊂ S out , for a constraint voxel p described in section 3.1 is now extended to a sub-volume around s with a thickness proportionate to Pr(a ∈ Sil i ). Note that the extension is also outwards in addition to inwards. To determine the color consistency value for the searched points that are outside V which haven't been computed so far, we dilate V with a small disk (e.g. a disk of 5 × 5 pixels) and proceed with the ρ computation described in section 2. Instead of applying uniform weight to the edges connecting voxels in BL to the Sink node, we now weight these edges for p ∈ P and for voxels that are in the associated bl(p) using Pr(p ∈ P).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">A Second Phase to Handle Concavities</head><p>As discussed in section 3.1, the set of surface constraint points, P, provides a large set of constraints on surface recovery which tend to best capture protrusive parts of the object's surface. So, the surface reconstructed by the first stage of recovery (phase I) is generally accurate over such areas. This is supported by the experiments described in section 5. On the other hand, it is well known that the silhouette does not contain information about concave regions of the surface ( <ref type="bibr" target="#b2">[3]</ref>). In addition, the graph cut algorithm, which prefers shorter cuts, will not follow a concavity well unless we "aggressively" pursue it.</p><p>We propose the procedure in figure <ref type="figure">4</ref> as a second phase to correct the estimation of the surface over concave regions.</p><p>We first (step 1) divide all of the voxels on the surface S I into three groups. The first group,P surf , has small ρ (or high photo consistency); the second group, P outside , consists of voxels with high ρ ; and the last group consists of the remaining voxels. Percentile(S, θ) returns the ρ value which is the θ-th percentile of the ρ score for S I . The parameters θ 1 and θ 2 determine the size of each group. In general, their appropriate values depend on the properties of the surface under consideration. Although as we observed, the final result is not very sensitive to these parameters. For the experiments in section 5, θ 1 and θ 2 are set to 0.7 and 0.95 respectively.</p><p>Let SI be the surface constructed by the algorithm in phase I.</p><p>Step 1. From SI , extract two sets of points P surf and P outside ,</p><formula xml:id="formula_7">P surf = {v | v ∈ SI and ρ(v) &lt; Percentile(SI , θ1)} (6) P outside = {v | v ∈ SI and ρ(v) &gt; Percentile(SI , θ2)} (7)</formula><p>Step 2. Using the procedure in section 3.2 to find BL inside = ∪v∈P surf bl(v).</p><p>Set the weight w(v, Sink) for all v ∈ BL inside using the previous method.</p><p>Step 3. Get BL outside = ∪v∈P outside bl(v) with the procedure in section 3.2 For all v ∈ BL outside and v ∈ BL inside</p><formula xml:id="formula_8">w(v, Source) = c.Pr(v is outside S * ) = c. ∞ d(v) exp(-p 2 /σ 2 surf )dp (<label>8</label></formula><formula xml:id="formula_9">)</formula><p>where c is a normalizing constant, d(v) is the distance from v to Sout.</p><p>The weights for all remaining voxels are set using photo consistency scores as before.</p><p>Step 4. Perform the graph cut algorithm to extract the final surface, SII .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Fig. 4. The steps of the second phase</head><p>Since all voxels in P surf lie on S I and have high photo consistency (small ρ), we assume that they belong to or are very close to the true surface S * . Therefore, in step 2, we connect them and all the voxels in their associated BL inside to the Sink node. Essentially, we treat P surf in a similar way to the set of constraint points, P, in phase I.</p><p>On the other hand, the voxels in P outside have low photo consistency (high ρ), so in step 3 we connect them to the Source node. By doing so, we effectively assume that these voxels are outside the true surface S * (and hence do not belong to the object's occupancy volume). The reasons we do this are as follows. Any such voxel is unlikely to lie on the actual surface S * , so is either inside or outside of it. If such a voxel were inside the true surface S * , then the surface region on S * that "covers" it would either be protrusive (case 1 -fig. <ref type="figure" target="#fig_3">5</ref>) or concave (case 2 -fig. <ref type="figure" target="#fig_3">5</ref>). If this region were protrusive (case 1), then it would likely have been captured by the constraint points, P, so would have been included in S I by phase I. If that region were concave (case 2), then the phase I graph cut algorithm would have included the region in S I , instead of P outside , because it would have incurred a smaller cutting cost. This is because voxels that lie on that region would have low ρ, while the voxels in P outside have high ρ and form even more concave (or "longer") surface regions. Therefore, voxels in P outside are assumed to be outside of S * (case 3 -fig. <ref type="figure" target="#fig_3">5</ref>), the only remaining possibility.</p><p>Moreover, the region of S * that lies "under" P outside is assumed to be concave. Therefore, to better recover it, we bias the solution cut inwards by treating the blocking curves BL outside differently. Voxels on these curves are assumed to be outside S * with a probability distribution that decreases as the distance of these voxels from S out increases (note that we use S out instead of S I ). We model the probability of the surface location as a Gaussian distribution N (S m , σ 2 surf ), where S m is a "mean surface" midway between S out and S in . The variance σ 2</p><p>surf is set to be (1/4d) 2 for the experiments in section 5, where d is the distance from S in to S out . This leads to approximating the probability that a voxel v is outside of S * with the cumulative distribution of N (S m , σ 2 surf ), and so the weight from voxels in BL outside to the Source node is computed using (8) in step 3.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Experimental Results</head><p>We demonstrate the performance of our approach on both synthetic and real data (640 × 480 images). Volumetric discretization are 256 × 256 × 256 for all experiments. The synthetic experiment is with a textured head (figure <ref type="figure" target="#fig_4">6</ref>.a-c).</p><p>Note that the nose is quite protrusive and the eye areas are concave. For the results in figure <ref type="figure" target="#fig_4">6</ref>, twenty images and the associated calibration information were constructed. Figure <ref type="figure" target="#fig_4">6</ref>.d shows the visual hull V obtained from the silhouettes. Each colored patch of the surface S out is "carved" by some camera. Patches from any single camera may not be connected and so are rims ( <ref type="bibr" target="#b8">[9]</ref>). Moreover, if self-occlusion occurs, some patches may not contain any true surface points at all. Figure <ref type="figure" target="#fig_4">6</ref>.e and 6.f show the result of using the basic algorithm from <ref type="bibr" target="#b7">[8]</ref> described in section 3.1 with different ballooning factors, λ, to overcome the preference of the algorithm to shorter cuts. As can be seen, if λ is too high (0.3), the protrusive parts (the nose) are preserved, but the concave regions (the eyes) suffer. Lowering λ (0.1) helps to recover concave areas but at the price of losing protrusive parts. Figure <ref type="figure" target="#fig_4">6</ref>.g shows the result of phase I when constraint points are used. Protrusive parts are well preserved now. However, the concave regions still are still not accurately recovered: the eye areas are nearly flat. Figure <ref type="figure" target="#fig_4">6</ref>.h compares the results of phase I (the top part) and phase II (the bottom part), where the eye areas are now improved.</p><p>In the second experiment, we measure the reconstruction errors of the synthetic face when different numbers of views are used <ref type="bibr">(8, 16, 32, and 64)</ref>. In generating images, the viewing direction of the camera is always towards the center of the face. For every set of views, the camera is placed in positions that are arbitrary, but distributed roughly even in front of the face. For the basic algorithm, λ is set to 0.15 to get a balance between the recovery of protrusions and concavities. Since the ground truth for the face is given as a cloud of points, G 0 , we use the 3D distance transform to measure the recovery error E. Specifically, for a surface S, E(S, G 0 ) = (D(S, G 0 ) + D(G 0 , S))/(|S| + |G 0 |), where D(S, G 0 ) is the sum of distances from all points in S to G 0 . E(S, G 0 ) is thus the average distance between points in S and G 0 (in voxel units). Figure <ref type="figure" target="#fig_5">7</ref> shows the reconstruction errors. The visual hull V produces quite a large error with 8 views but is noticeably better as the number of views increases. For the basic algorithm, with λ = 0.15, some protrusions are cut off. Note that since the cutting off effects can have unpredictable consequences, the reconstruction error can increase as the number of views increases (although not significantly). Adding more views in this case turns out to be "helping" the nose of the face to be more cut off. As a result, the visual hull may produce better results for larger number of views. Our methods behave consistently and produce better performance. Our result with 8 views, although with no discernible improvement for more than 16 views, is better than the visual hull with 64 views. The error of our method compared to the basic algorithm, is reduced roughly 33%. Note that in term of average error distance, phase II is not much better than phase I. This is because the focus of phase II is only on small (concave) portions left by phase I (θ 2 = 0.95, section 4).</p><p>In the third experiment, 30 real images of a colored plastic squirrel were collected. We imaged the object under natural lighting conditions with a cluttered background, and moved the camera around the object. Due to self-shadowing and the arrangement of the light sources, the object is well lit on one side and poorly lit on the other side (see figures 8.a and 8.b for examples). The color information from the poorly lit side is noisy and tends to saturate to black. These 30 images are divided roughly even for both sides. The object's actual size is about 300 × 150 × 300 mm 3 (width-length-height); this is also the size of the discretized volume used. Camera calibration was done using a publicly available tool box with the principal point's uncertainty from 1.4 -1.7 pixels. Silhouette extraction is performed using the method described in section 3.2. The silhouettes can be 1 to 5 pixels off from the "true" silhouettes. Figure <ref type="figure" target="#fig_6">8</ref>.c show the visual hull constructed from them. Assuming that these silhouettes are exact leads to undesirable consequences. Figure <ref type="figure" target="#fig_6">8</ref>.d shows the result of the basic algorithm. Even when we add the set of constraint points, our algorithm (phase I) still produces bad results: a number of incorrect bumps and dents on the surface. Figure <ref type="figure" target="#fig_6">8</ref>.e, top row, zooms in on some of them (the image are smoothen for better visualization). Adding silhouette uncertainties (bottom row) produce much improved results. To allow for comparison with the basic algorithm, the dilated visual hull discussed at the end of section 3.2 is also used for it.</p><p>For the well lit side of the object, figure <ref type="figure" target="#fig_6">8</ref>.f shows the result of the basic algorithm and figure <ref type="figure" target="#fig_6">8</ref>.g shows the result of our methods (phase I). Figure <ref type="figure" target="#fig_6">8</ref>.h compares the two results on several places: the top row is for the basic algorithm and the bottom row is for ours. The phase I and phase II give nearly the same result. In other words, phase II has little effects on this well-illuminated side.</p><p>For poorly lit side of the object, figure <ref type="figure" target="#fig_6">8</ref>.k shows the result of the basic algorithm, figure <ref type="figure" target="#fig_6">8</ref>.l is for phase I and figure <ref type="figure" target="#fig_6">8</ref>.m is for phase II. Note the difference between the two legs and along the tail. </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 1 .</head><label>1</label><figDesc>Fig. 1. a) a slice of the photo consistency field, yellow line denotes the true surface. b) Nodes and edges in the graph G.</figDesc><graphic coords="3,96.68,316.21,235.91,113.30" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 2 .</head><label>2</label><figDesc>Fig. 2. a) Rays touch V 's surface at p, b) Example of the set of constraint points, P</figDesc><graphic coords="4,54.93,477.48,320.26,98.54" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 3 .</head><label>3</label><figDesc>Fig. 3. a) Blocking regions (curves). b) Locational uncertainties (gray areas) of the contour extracted from a difference image.</figDesc><graphic coords="5,57.38,441.48,320.81,128.93" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Fig. 5 .</head><label>5</label><figDesc>Fig. 5. Possible displacements of SI and S * . The solid curve represents SI with bold segments for P surf and thin segments for P outside . Of these cases, only case 3 is likely.</figDesc><graphic coords="9,41.17,52.44,346.84,93.04" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Fig. 6 .</head><label>6</label><figDesc>Fig.6. Synthetic face reconstruction: a-c) Three of the images collected; d) visual hull V ; e-f) using basic step, λ = .3 and .1; g) using constraint points P after phase I; h) after phase II (bottom) as compared to after phase I (top)</figDesc><graphic coords="9,303.22,466.02,77.62,84.63" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Fig. 7 .</head><label>7</label><figDesc>Fig. 7. Recovery errors for different set of views. For each group, from left to right, values are respectively for visual hull, basic algorithm and our phase I, II results. (A unit along the y-axis corresponds to the size of a voxel).</figDesc><graphic coords="10,50.36,399.73,327.62,155.90" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Fig. 8 .</head><label>8</label><figDesc>Fig. 8. Reconstruction of the squirrel object. a-b) two of the images collected; c) the visual hull V ; d-e) the result of the basic algorithm and our phase I when silhouettes are assumed exact (see text). Well lit area results: f) the basic algorithm; g) our phase I algorithm; h) some detailed comparison between the basic algorithm (top row) and the final result of phase I (bottom row). Poorly lit area results: k) the basic, l) phase I and m) phase II algorithms. Note the differences inside the red circles.</figDesc><graphic coords="12,44.89,404.69,103.59,114.95" type="bitmap" /></figure>
		</body>
		<back>

			<div type="funding">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>This work is supported by the NSF grant IIS-0325715 entitled ITR: New Technology for the Capture, Analysis and Visualization of Human Movement.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Rapid octree construction from image sequences</title>
		<author>
			<persName><forename type="first">R</forename><surname>Szeliski</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">CVGIP: Image Understanding</title>
		<imprint>
			<biblScope unit="volume">57</biblScope>
			<biblScope unit="page" from="23" to="32" />
			<date type="published" when="1993">1993</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Visual hull alignment and refinement across time: A 3d reconstruction algorithm combining shape-from-silhouette with stereo</title>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">K M</forename><surname>Cheung</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Baker</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Kanade</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conf. Computer Vision and Pattern Recognition (CVPR-2003)</title>
		<meeting>IEEE Conf. Computer Vision and Pattern Recognition (CVPR-2003)</meeting>
		<imprint>
			<date type="published" when="2003">2003</date>
			<biblScope unit="page" from="375" to="382" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Exact voxel occupancy with graph cuts</title>
		<author>
			<persName><forename type="first">D</forename><surname>Snow</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Viola</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Zabih</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conf. Computer Vision and Pattern Recognition (CVPR-2000)</title>
		<meeting>IEEE Conf. Computer Vision and Pattern Recognition (CVPR-2000)</meeting>
		<imprint>
			<date type="published" when="2000">2000</date>
			<biblScope unit="page" from="345" to="352" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">A surface reconstruction method using global graph cut optimization</title>
		<author>
			<persName><forename type="first">S</forename><surname>Paris</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Sillion</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Long</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Asian Conf. Computer Vision (ACCV-2004)</title>
		<meeting>Asian Conf. Computer Vision (ACCV-2004)</meeting>
		<imprint>
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">A theory of shape by space carving</title>
		<author>
			<persName><forename type="first">K</forename><surname>Kutulakos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Seitz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Int&apos;l Conf. Computer Vision (ICCV-1999)</title>
		<meeting>IEEE Int&apos;l Conf. Computer Vision (ICCV-1999)</meeting>
		<imprint>
			<date type="published" when="1999">1999</date>
			<biblScope unit="page" from="307" to="314" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Visibility constrained surface evolution</title>
		<author>
			<persName><forename type="first">J</forename><surname>Solem</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Kahl</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Heyden</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conf. Computer Vision and Pattern Recognition (CVPR-2005)</title>
		<meeting>IEEE Conf. Computer Vision and Pattern Recognition (CVPR-2005)</meeting>
		<imprint>
			<date type="published" when="2005">2005</date>
			<biblScope unit="page" from="892" to="900" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Stochastic refinement of the visual hull to satisfy photometric and silhouette consistency constraints</title>
		<author>
			<persName><forename type="first">J</forename><surname>Isidoro</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Sclaroff</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Int&apos;l Conf. Computer Vision (ICCV-2003)</title>
		<meeting>IEEE Int&apos;l Conf. Computer Vision (ICCV-2003)</meeting>
		<imprint>
			<date type="published" when="2003">2003</date>
			<biblScope unit="page" from="1335" to="1342" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Multi-view stereo via volumetric graph-cuts</title>
		<author>
			<persName><forename type="first">G</forename><surname>Vogiatzis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Torr</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Cippola</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conf. Computer Vision and Pattern Recognition (CVPR-2005)</title>
		<meeting>IEEE Conf. Computer Vision and Pattern Recognition (CVPR-2005)</meeting>
		<imprint>
			<date type="published" when="2005">2005</date>
			<biblScope unit="page" from="391" to="399" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Multi-view reconstruction using photo-consistency and exact silhouette constraints: A maximum-flow formulation</title>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">N</forename><surname>Sinha</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Pollefeys</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Int&apos;l Conf. Computer Vision (ICCV-2005)</title>
		<meeting>IEEE Int&apos;l Conf. Computer Vision (ICCV-2005)</meeting>
		<imprint>
			<date type="published" when="2005">2005</date>
			<biblScope unit="volume">I</biblScope>
			<biblScope unit="page" from="349" to="356" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Computing geodesics and minimal surfaces via graph cuts</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Boykov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Kolmogorov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Int&apos;l Conf. Computer Vision (ICCV-2003)</title>
		<meeting>IEEE Int&apos;l Conf. Computer Vision (ICCV-2003)</meeting>
		<imprint>
			<date type="published" when="2003">2003</date>
			<biblScope unit="page" from="26" to="33" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Fast approximate energy minimization via graph cuts</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Boykov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Veksler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Zabih</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Pattern Anal. and Machine Intell</title>
		<imprint>
			<biblScope unit="volume">23</biblScope>
			<biblScope unit="page" from="1222" to="1239" />
			<date type="published" when="2001">2001</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Interactive graph cuts for optimal boundary and region segmentation of objects in n-d images</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Boykov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">P</forename><surname>Jolly</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Int&apos;l Conf. Computer Vision (ICCV-2001)</title>
		<meeting>IEEE Int&apos;l Conf. Computer Vision (ICCV-2001)</meeting>
		<imprint>
			<date type="published" when="2001">2001</date>
			<biblScope unit="page" from="105" to="112" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Silhouette and stereo fusion for 3d object modeling</title>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">H</forename><surname>Esteban</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Schmitt</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. 4th Int&apos;l Conf. on 3D Digital Imaging and Modeling (3DIM</title>
		<meeting>4th Int&apos;l Conf. on 3D Digital Imaging and Modeling (3DIM</meeting>
		<imprint>
			<date type="published" when="2003">2003. 2003</date>
			<biblScope unit="page" from="46" to="53" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
