<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Leveraging Text Data Using Hybrid Transformer-LSTM Based End-to-End ASR in Transfer Learning</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Zhiping</forename><surname>Zeng</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">School of Computer Science and Engineering</orgName>
								<orgName type="institution">Nanyang Technological University</orgName>
								<address>
									<country key="SG">Singapore</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="institution">Huya AI</orgName>
								<address>
									<settlement>Guangzhou</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Van</forename><forename type="middle">Tung</forename><surname>Pham</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">School of Computer Science and Engineering</orgName>
								<orgName type="institution">Nanyang Technological University</orgName>
								<address>
									<country key="SG">Singapore</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Haihua</forename><surname>Xu</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">School of Computer Science and Engineering</orgName>
								<orgName type="institution">Nanyang Technological University</orgName>
								<address>
									<country key="SG">Singapore</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Yerbolat</forename><surname>Khassanov</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">School of Computer Science and Engineering</orgName>
								<orgName type="institution">Nanyang Technological University</orgName>
								<address>
									<country key="SG">Singapore</country>
								</address>
							</affiliation>
							<affiliation key="aff2">
								<orgName type="institution" key="instit1">ISSAI</orgName>
								<orgName type="institution" key="instit2">Nazarbayev University</orgName>
								<address>
									<country key="KZ">Kazakhstan</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Eng</forename><forename type="middle">Siong</forename><surname>Chng</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">School of Computer Science and Engineering</orgName>
								<orgName type="institution">Nanyang Technological University</orgName>
								<address>
									<country key="SG">Singapore</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Chongjia</forename><surname>Ni</surname></persName>
							<affiliation key="aff3">
								<orgName type="department">Machine Intelligence Technology</orgName>
								<address>
									<settlement>Alibaba Group</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Bin</forename><surname>Ma</surname></persName>
							<affiliation key="aff3">
								<orgName type="department">Machine Intelligence Technology</orgName>
								<address>
									<settlement>Alibaba Group</settlement>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Leveraging Text Data Using Hybrid Transformer-LSTM Based End-to-End ASR in Transfer Learning</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.2" ident="GROBID" when="2022-12-25T13:53+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>cross-lingual transfer learning</term>
					<term>transformer</term>
					<term>lstm</term>
					<term>unpaired text</term>
					<term>independent language model</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>In this work, we study leveraging extra text data to improve lowresource end-to-end ASR under cross-lingual transfer learning setting. To this end, we extend our prior work <ref type="bibr" target="#b0">[1]</ref>, and propose a hybrid Transformer-LSTM based architecture. This architecture not only takes advantage of the highly effective encoding capacity of the Transformer network but also benefits from extra text data due to the LSTM-based independent language model network. We conduct experiments on our in-house Malay corpus which contains limited labeled data and a large amount of extra text. Results show that the proposed architecture outperforms the previous LSTM-based architecture [1] by 24.2% relative word error rate (WER) when both are trained using limited labeled data. Starting from this, we obtain further 25.4% relative WER reduction by transfer learning from another resource-rich language. Moreover, we obtain additional 13.6% relative WER reduction by boosting the LSTM decoder of the transferred model with the extra text data. Overall, our best model outperforms the vanilla Transformer ASR by 11.9% relative WER. Last but not least, the proposed hybrid architecture offers much faster inference compared to both LSTM and Transformer architectures.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>End-to-end (E2E) architecture has been a promising strategy for ASR. In this strategy, a single network is employed to directly map acoustic features into a sequence of characters or subwords without the need of a pronunciation dictionary that is required by the conventional hidden Markov model based systems. Furthermore, the components of the E2E network can be jointly trained using a common objective criterion to achieve overall optimization which greatly simplifies the ASR development process. Although the simplicity of E2E ASR architecture is attractive, especially for new languages, it requires a huge amount of labeled training data.</p><p>In this work, we focus on E2E ASR for a low-resource language. Specifically, we assume that the target language possesses a limited amount of labeled data to train E2E systems, while an extra text corpus of the language can be easily collected. Additionally, we assume that we possess a large amount of labeled data from another resource-rich source language. This is a common scenario in real-world applications.</p><p>The extra text is usually employed to train language models (LM) applied during decoding <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b2">3,</ref><ref type="bibr" target="#b3">4]</ref> and re-scoring stages <ref type="bibr" target="#b4">[5]</ref>. Such techniques not only require external language models but also lead to a slow inference. To tackle this problem, <ref type="bibr" target="#b0">[1]</ref> has proposed long short term memory (LSTM)-based encoderdecoder architecture which allows improving the LM capacity of the decoder using the extra text data. However, it utilized the LSTM structure for the encoder which has shown limited modeling capacity as well as slow training. On the other hand, Transformer <ref type="bibr" target="#b5">[6]</ref> has been a promising approach for E2E ASR due to its high modeling capacity and fast training. However, its decoder closely interacts with the encoder output through an encoder-decoder cross-attention. Therefore, it is not straightforward to employ extra text data to improve the decoder.</p><p>In this work, we propose a hybrid Transformer-LSTM architecture which combines the advantages of <ref type="bibr" target="#b0">[1]</ref> and <ref type="bibr" target="#b5">[6]</ref>. It not only has a high encoding capacity of the Transformer but also benefits from the extra text data due to the LSTM-based independent language model decoder. To further benefit from the labeled data from another language, we employ cross-lingual transfer learning, which is a popular approach to address the limited resource problem in ASR <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b7">8,</ref><ref type="bibr" target="#b8">9,</ref><ref type="bibr" target="#b9">10,</ref><ref type="bibr" target="#b10">11]</ref>, on the proposed architecture. Specifically, we first use labeled data of the resource-rich language to train an ASR model and then transfer it to the target language. Lastly, the extra text data is used to boost the decoder of the transferred model.</p><p>The paper is organized as follows. Section 2 describes baseline architectures mentioned in <ref type="bibr" target="#b0">[1]</ref> and <ref type="bibr" target="#b5">[6]</ref>. Then, the proposed techniques are presented in Section 3. Experimental setup and results are presented in Section 4 and 5 respectively. Section 6 concludes our work.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Baseline architectures 2.1. LSTM-based encoder-decoder architecture</head><p>A LSTM-based encoder-decoder architecture <ref type="bibr" target="#b0">[1]</ref>, denoted as A1 in the rest of this paper, consists of a Bidirectional LSTM encoder and a LSTM-based decoder which are shown in Fig. <ref type="figure">1</ref>. Let &lt;X, Y&gt; be a training utterance, where X is a sequence of acoustic features and Y = {y1, y2, ..., y |Y| } is a sequence of output units. The encoder acts as an acoustic model which maps acoustic features X into an intermediate representation h. Then, the decoder, which consists of an embedding, a LSTM and a projection layers, generates one output unit at each decoding step i as follows,</p><formula xml:id="formula_0">si = LST M (si−1, embedding(yi−1)) (1) ci = attention(h, si) (2) P (yi | X, y&lt;i) = sof tmax(proj(si) + proj(ci))<label>(3)</label></formula><p>where ci is the context vector, si−1 and si are output hidden states at time step i − 1 and i respectively, embedding() and proj() are embedding and projection layers respectively. Figure <ref type="figure">1</ref>: LSTM-based encoder-decoder architecture (A1) <ref type="bibr" target="#b0">[1]</ref>, where the decoder acts as an independent language model.</p><p>From Equation (1), the LSTM is only conditioned on the previous decoding hidden state and previous decoding output. In other words, the LSTM acts as an independent language model that can be easily updated with text-only data <ref type="bibr" target="#b0">[1]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.">Transformer encoder-decoder architecture</head><p>Transformer has been proposed in <ref type="bibr" target="#b5">[6]</ref> for sequence-to-sequence modeling in natural language processing tasks, then adopted to the ASR task in <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b12">13]</ref>. The model architecture, denoted as A2, is shown in Fig. <ref type="figure" target="#fig_1">2</ref>. The encoder is shown in the left half of Fig. <ref type="figure" target="#fig_1">2</ref>. It consists of Ne encoder-blocks, each of them has two sub-blocks: self-attention and position-wise feed-forward network (FFN). The self-attention employs multi-head attention (MHA) which is a function of three inputs: query Q, key K and value V . It includes multiple heads which can be processed in parallel. Each head employs a dot-product attention (DotP rodAtt) as follows,</p><formula xml:id="formula_1">DotP rodAtt(Q, K, V ) = sof tmax( QK T d k )V<label>(4)</label></formula><p>where d k is the hidden dimension. Besides, layer normalization <ref type="bibr" target="#b13">[14]</ref> and residual connection <ref type="bibr" target="#b14">[15]</ref> are introduced to each encoder-block for effective training.</p><p>The decoder is shown in the right half of Fig. <ref type="figure" target="#fig_1">2</ref> which consists of N d decoder-blocks. Different from the encoder-block, each decoder-block has one more sub-block, i.e. the crossattention. Its first input comes from the previous sub-block output while another input is from the output of the encoder.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Proposed techniques</head><p>To exploit extra text data while having high modeling capacity, we propose a Transformer-LSTM architecture in Section 3.1.</p><p>Then, we exploit using extra text data to improve the proposed architecture under the cross-lingual transfer learning setting.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">The Transformer-LSTM architecture</head><p>In this section, we first compare approaches presented in Section 2.1 and 2.2. Then, based on the comparison, we propose a novel architecture that takes advantage of both approaches.</p><p>Previous work <ref type="bibr" target="#b15">[16]</ref> showed that the Transformer not only produces better encoding representation but is also faster than the LSTM counterpart in training. First, the Transformer encoder uses dot-product attention (see Equation ( <ref type="formula" target="#formula_1">4</ref>)) which allows each position has access to information from all other positions in the same sequence regardless of their distance. In contrast, although in theory LMST can model long-range dependence, in practice it faces difficulty to capture dependencies of far-distance elements <ref type="bibr" target="#b16">[17]</ref> which limits its modeling capacity for long sequences such as acoustic signal. Second, by relying entirely on feed-forward components, the Transformer model avoids any sequential dependencies, and hence can maximize parallel training. In contrast, training a LSTM-based network is slow due to the recurrence property of the LSTM.</p><p>Despite being highly effective, the Transformer decoder is not easy to be improved using text-only data. Specifically, the decoder includes the cross-attention sub-block which is conditioned on the encoder output. In contrast, the LSTM-based decoder (in Section 2.1) can be easily boosted using the text data. Another issue of the Transformer decoder is slow inference <ref type="bibr" target="#b17">[18]</ref>. Specifically, to generate an output yi, the decoder needs to process all previous decoding units y1:i−1. On the other hand, the LSTM-based decoder has faster inference since it only needs the last output unit yi−1 to generate yi.</p><p>Based on the above comparisons, we propose a hybrid architecture that takes advantage of both Transformer and LSTM architectures. Specifically, our encoder is from Transformer, while the decoder is taken from the LSTM architecture. The benefits of the proposed architecture lie in two aspects. First, it has high modeling capacity, as well as faster training and decoding. Second, the LSTM-based architecture allows us to easily leverage text data to boost the decoder, yielding improved ASR performance. We denote the proposed architecture as A3 in the rest of this paper.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Exploiting extra text data under cross-lingual transfer learning</head><p>To tackle the low-resource training problem, we first perform cross-lingual transfer learning. We start with training E2E models using a source language. We then replace the languagedependent components of the decoder (i.e. the embedding and output projection layers) of the source language by those of the target language. Finally, the models are fine-tuned using labeled data of the target language. Although transfer learning is not our focus, to achieve the best performance, we carefully examine various transfer settings as presented in Section 5.2. More importantly, we aim to boost the transferred model of the proposed architecture using extra text data of the target language. Fig. <ref type="figure" target="#fig_2">3</ref> describes our process.</p><p>From Fig. <ref type="figure" target="#fig_2">3</ref>, the entire process is implemented with two main steps. In the first step, we merge the extra text and the labeled data together to fine-tune the transferred model. This avoids a so-called catastrophic forgetting problem as mentioned in <ref type="bibr" target="#b0">[1]</ref>. Specifically, at each training iteration, we mix a batch of labeled data consisting of B labeled utterances with a batch of text data consisting of Btext utterances to fine-tune the transferred model with the following loss function:</p><formula xml:id="formula_2">L total (θ) = (1 − λ)LASR(θ) + λLLM (θ d )<label>(5)</label></formula><p>where λ denotes an interpolation factor, θ and θ d denote entire E2E parameters and decoder parameters respectively, LASR(θ) and LLM (θ d ) denote the ASR loss and LM loss generated by the labeled data and text data respectively. In the second step, the model is further fine-tuned with the labeled data of the target language. Similar to <ref type="bibr" target="#b0">[1]</ref>, we empirically found that the second step is necessary to improve overall performance.</p><p>Step 1: Fine-tune E2E model</p><p>Step  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Experimental setup 4.1. Data</head><p>We conduct experiments on our in-house corpus of Malay language which consists of limited labeled data plus extra text. We split the labeled data into three sets for training, validation and evaluation. Detailed division is shown in Table <ref type="table">1</ref>. We perform speed perturbation based data augmentation <ref type="bibr" target="#b18">[19]</ref> on the training data. For extra text data, we examine two sets: the first set has over 8 million sentences, denoted as T 1; while the second set is a subset of the first one which consists of 2 million sentences, denoted as T 2.</p><p>For the source language, which is English, we use two subsets of the National Speech Corpus (NSC) <ref type="bibr" target="#b19">[20]</ref> to train source models. The first subset, denoted as S 200h consists of 200 hours, where we also apply speed perturbation based data augmentation. The second subset consists of 1000 hours, denoted as S 1000h .</p><p>Table <ref type="table">1</ref> </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">E2E setting</head><p>ESPnet toolkit <ref type="bibr" target="#b20">[21]</ref> is used to train our E2E architectures. We use 80 mel-scale filterbank coefficients with pitch as input features, and 500 Byte-Pair Encoding (BPE) units are used as output units. For all E2E architectures, the acoustic features are processed by the VGG network <ref type="bibr" target="#b21">[22]</ref>. Detailed setting of architectures can be seen in Table <ref type="table" target="#tab_2">2</ref>. Each BLSTM layer has 320 cells, while each LSTM layer of the decoder has 256 cells.</p><p>In each self-attention and cross-attention sub-blocks, we use 4 heads with hidden dimension of 2048. The FFN consists of two ReLu activation functions and two affine transforms with size of 2048. To allow transfer learning, we use the same settings for both source and target languages. During the fine-tuning process in Section 3.2, we set B labeled = 30 and Btext = 90. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Results and analysis</head><p>The overall ASR performance on the test set after applying proposed techniques (in Section 3) is presented in Table <ref type="table">3</ref>. In following subsections, we will describe and elaborate results for each proposed technique.</p><p>Table <ref type="table">3</ref>: The WER(%) on the test set of different E2E models. S 1000h -Encoder denotes that we use the data S 1000h of the source language to train a source model, then transfer only the encoder of the source model to the target language. T 1 denotes an extra text corpus of the target language that consists of 8 million sentences.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>No. Architecture</head><p>Transfer learning settings Extra text WER(%) This section presents the results of different E2E architectures, i.e. A1, A2 and A3 trained using only the labeled data of the target language. The word error rate (WER) results are reported in the first three rows in Table <ref type="table">3</ref>. We also report the decoding speed of these models in Table <ref type="table" target="#tab_4">4</ref>. As we can see, A3 not only outperforms A1 by 24.2% relative WER but also offers 1.5 times faster decoding. This indicates that employing the Transformer for the encoder is very effective. A2 achieves best WER, but has much slower decoding speed compared to A3. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.">Results of transfer learning</head><p>In this section, we examine the ASR performance of different transfer learning settings. Firstly, we examine the effect of using different amounts of source language data on the target language's performance. Secondly, we analyze the different level of transfer learning: (1) only l (e.g. l = 3, 6, 9) bottom layers of the encoder is transferred; (2) entire encoder is transferred;</p><p>(3) both encoder and decoder (except embedding and projection layers) are transferred. These experiments were conducted only for A2 and A3, since A1 produced worst results. The experiment results are given in Table <ref type="table" target="#tab_5">5</ref>.</p><p>We observed that transferring the entire encoder achieves the best results for both A2 and A3. Additionally, transfer with 1000 hours of source data is noticeably better than that of 200 hours with data augmentation. The best results are summarized in rows 4 and 5 in Table <ref type="table">3</ref>. It can be seen that cross-lingual transfer learning leads to significant improvement for both A2 and A3. For example, the result in row 5 outperforms that of row 3 by 25.4% relative WER (from 13.8% to 10.3%). </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3.">Results of utilizing extra text data</head><p>We first present the ASR performance on development data when using extra text data T 1 and T 2 to fine-tune A3 after cross-lingual transfer learning. The results are shown in Fig. <ref type="figure" target="#fig_3">4</ref>. We observed that using extra text data is very effective and T 1 produces substantial improvement over T 2. We also observed that Step 2 (see Fig. <ref type="figure" target="#fig_2">3</ref>), i.e. to use labeled data to fine-tune the E2E network, is essential. Finally, λ = 0.7 yields the best results in most of the cases.</p><p>We then employ T 1 with λ = 0.7 on the test set and the result is reported in the row 6 of Table <ref type="table">3</ref>. Using extra text data significantly improves A3 by 13.6% relative WER (from 10.3% to 8.9%). With the help of the extra text data, the proposed architecture outperforms the Transformer baseline by 11.9% relative WER (from 10.1% to 8.9%).  T 1 and T 2, is used to fine-tune A3. λ is the interpolation factor in Equation ( <ref type="formula" target="#formula_2">5</ref>).</p><p>Step 1 and Step 2 are explained in Fig. <ref type="figure" target="#fig_2">3</ref>.</p><p>We now investigate the effect of an external language model on the proposed architecture A3. We train a Recurrent Neural Network LM (RNN-LM) as a 1-layer LSTM with 1024 cells using both transcriptions of the training data and the extra text data T 1, then integrate the RNN-LM into inference process of A3 (row 5 and 6 in Table <ref type="table">3</ref>). Results are reported in Table <ref type="table" target="#tab_7">6</ref>. As we can see, after fine-tuning with T 1, A3 still benefits from the external RNN-LM and we observed 34.8% relative WER reduction (from 8.9% to 5.8%). </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Conclusions</head><p>In this paper, we first proposed the Transformer-LSTM based architecture which not only takes advantage of the highly effective encoding capacity of the Transformer, but also benefits from the extra text data due to the LSTM-based independent language model decoder. We then examined exploiting extra text data to boost the LSTM decoder under cross-lingual transfer learning. Experimental results show that, with the help of the extra text data, the proposed architecture significantly outperforms baselines. Additionally, the proposed architecture also offers faster decoding.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: Transformer architecture (A2).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: Boosting the transferred model using extra text data of the target language.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 4 :</head><label>4</label><figDesc>Figure 4: The performance on the dev data after cross-lingual transfer learning when different amount of extra text data, i.e. T 1 and T 2, is used to fine-tune A3. λ is the interpolation factor in Equation (5). Step 1 and Step 2 are explained in Fig. 3.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>:</head><label></label><figDesc>Detailed division of the labeled Malay data.</figDesc><table><row><cell></cell><cell cols="2">Train Dev</cell><cell>Test</cell></row><row><cell>#Speakers</cell><cell>57</cell><cell>6</cell><cell>6</cell></row><row><cell>#Utterances</cell><cell cols="3">8500 1785 1957</cell></row><row><cell>Length (hours)</cell><cell>20.6</cell><cell>4.3</cell><cell>4.1</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2 :</head><label>2</label><figDesc>Setting of different E2E architectures.</figDesc><table><row><cell># Paras encoders</cell><cell>decoders</cell></row><row><cell>A1 77.4M 6 BLSTM</cell><cell>1 LSTM</cell></row><row><cell cols="2">A2 120 M 12 (self-att + FFN) 6 (self-att +</cell></row><row><cell></cell><cell>cross-att + FFN)</cell></row><row><cell cols="2">A3 81.1M 12 (self-att + FFN) 1 LSTM</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 4 :</head><label>4</label><figDesc>Decoding speed of different E2E architectures.</figDesc><table><row><cell>Architecture</cell><cell>Decoding speed (seconds/utt)</cell></row><row><cell>A1</cell><cell>7.5</cell></row><row><cell>A2</cell><cell>31.61</cell></row><row><cell>A3</cell><cell>5.00</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 5 :</head><label>5</label><figDesc>ASR performance of different transfer learning settings.</figDesc><table><row><cell></cell><cell>Source data</cell><cell>Transfer modules</cell><cell>WER(%) Dev Test</cell></row><row><cell></cell><cell></cell><cell>Encoder + Decoder</cell><cell>15.1 11.0</cell></row><row><cell></cell><cell></cell><cell>Encoder</cell><cell>14.9 10.4</cell></row><row><cell>A2</cell><cell>S 200h</cell><cell cols="2">9 bottom encoder layers 16.2 11.6</cell></row><row><cell></cell><cell></cell><cell cols="2">6 bottom encoder layers 16.9 12.3</cell></row><row><cell></cell><cell></cell><cell cols="2">3 bottom encoder layers 17.3 12.6</cell></row><row><cell></cell><cell></cell><cell>Encoder + Decoder</cell><cell>14.2 10.4</cell></row><row><cell></cell><cell></cell><cell>Encoder</cell><cell>13.9 10.1</cell></row><row><cell>A2</cell><cell>S 1000h</cell><cell cols="2">9 bottom encoder layers 15.3 11.1</cell></row><row><cell></cell><cell></cell><cell cols="2">6 bottom encoder layers 16.7 12.3</cell></row><row><cell></cell><cell></cell><cell cols="2">3 bottom encoder layers 17.1 12.7</cell></row><row><cell></cell><cell></cell><cell>Encoder + Decoder</cell><cell>15.5 11.0</cell></row><row><cell></cell><cell></cell><cell>Encoder</cell><cell>15.1 10.8</cell></row><row><cell>A3</cell><cell>S 200h</cell><cell cols="2">9 bottom encoder layers 16.4 12.2</cell></row><row><cell></cell><cell></cell><cell cols="2">6 bottom encoder layers 17.4 12.8</cell></row><row><cell></cell><cell></cell><cell cols="2">3 bottom encoder layers 17.6 13.1</cell></row><row><cell></cell><cell></cell><cell>Encoder + Decoder</cell><cell>14.6 10.7</cell></row><row><cell></cell><cell></cell><cell>Encoder</cell><cell>14.3 10.3</cell></row><row><cell>A3</cell><cell>S 1000h</cell><cell cols="2">9 bottom encoder layers 15.5 11.3</cell></row><row><cell></cell><cell></cell><cell cols="2">6 bottom encoder layers 16.7</cell></row><row><cell></cell><cell></cell><cell cols="2">3 bottom encoder layers 17.2 12.7</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 6 :</head><label>6</label><figDesc>ASR performance of A3 (row 5 and 6 from Table3) on test set with and without RNN-LM.</figDesc><table><row><cell>Row No. (From Table 3)</cell><cell cols="2">External LM WER(%)</cell></row><row><cell>5</cell><cell>-+ RNN-LM</cell><cell>10.3 6.8</cell></row><row><cell>6</cell><cell>-+ RNN-LM</cell><cell>8.9 5.8</cell></row></table></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.">Acknowledgements</head><p>This work is supported by the project of Alibaba-NTU Singapore Joint Research Institute. The computational work for this article was partially on resources of the National Supercomputing Centre, Singapore (https://www.nscc.sg).</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Independent language model architecture for end-toend asr</title>
		<author>
			<persName><forename type="first">V</forename><forename type="middle">T</forename><surname>Pham</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Yerbolat</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">S</forename><surname>Chng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Ni</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of ICASSP</title>
				<meeting>of ICASSP</meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="7054" to="7058" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Towards end-to-end speech recognition with recurrent neural networks</title>
		<author>
			<persName><forename type="first">A</forename><surname>Graves</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Jaitly</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of ICML</title>
				<meeting>of ICML</meeting>
		<imprint>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="1764" to="1772" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">End-to-end speech recognition with word-based RNN language models</title>
		<author>
			<persName><forename type="first">T</forename><surname>Hori</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Watanabe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of HLT</title>
				<meeting>of HLT</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="389" to="396" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">On using monolingual corpora in neural machine translation</title>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">¸</forename><surname>Gülc ¸ehre</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Firat</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Barrault</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Bougares</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Schwenk</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<idno>abs/1503.03535</idno>
	</analytic>
	<monogr>
		<title level="j">CoRR</title>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Listen, attend and spell: A neural network for large vocabulary conversational speech recognition</title>
		<author>
			<persName><forename type="first">W</forename><surname>Chan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Jaitly</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><forename type="middle">V</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Vinyals</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of ICASSP</title>
				<meeting>of ICASSP</meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="4960" to="4964" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Attention is all you need</title>
		<author>
			<persName><forename type="first">A</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">N</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Polosukhin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of NIPS</title>
				<meeting>of NIPS</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="5998" to="6008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Cross-language knowledge transfer using multilingual deep neural network with shared hidden layers</title>
		<author>
			<persName><forename type="first">J</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Gong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of ICASSP</title>
				<meeting>of ICASSP</meeting>
		<imprint>
			<date type="published" when="2013">2013</date>
			<biblScope unit="page" from="7304" to="7308" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Multilingual acoustic models using distributed deep neural networks</title>
		<author>
			<persName><forename type="first">G</forename><surname>Heigold</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Vanhoucke</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Senior</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Nguyen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Ranzato</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Devin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Dean</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of ICASSP</title>
				<meeting>of ICASSP</meeting>
		<imprint>
			<date type="published" when="2013">2013</date>
			<biblScope unit="page" from="8619" to="8623" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Investigating end-to-end speech recognition for mandarin-english code-switching</title>
		<author>
			<persName><forename type="first">C</forename><surname>Shan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Weng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Xie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of ICASSP</title>
				<meeting>of ICASSP</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="6056" to="6060" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">Multilingual training and cross-lingual adaptation on ctc-based acoustic model</title>
		<author>
			<persName><forename type="first">S</forename><surname>Tong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">N</forename><surname>Garner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Bourlard</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017">2017</date>
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">Sequencebased multi-lingual low resource speech recognition</title>
		<author>
			<persName><forename type="first">S</forename><surname>Dalmia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Sanabria</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Metze</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">W</forename><surname>Black</surname></persName>
		</author>
		<idno>abs/1802.07420</idno>
		<imprint>
			<date type="published" when="2018">2018</date>
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Speech-transformer: A norecurrence sequence-to-sequence model for speech recognition</title>
		<author>
			<persName><forename type="first">L</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Xu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of ICASSP</title>
				<meeting>of ICASSP</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="5884" to="5888" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">A comparative study on Transformer vs RNN in speech applications</title>
		<author>
			<persName><forename type="first">S</forename><surname>Karita</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Watanabe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Yoshimura</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Hayashi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Hori</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Inaguma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Someki</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Yalta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Yamamoto</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of ASRU</title>
				<meeting>of ASRU</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="449" to="456" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Layer normalization</title>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">J</forename><surname>Ba</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">R</forename><surname>Kiros</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
		<idno>abs/1607.06450</idno>
	</analytic>
	<monogr>
		<title level="j">CoRR</title>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of CVPR</title>
				<meeting>of CVPR</meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="770" to="778" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">The best of both worlds: Combining recent advances in neural machine translation</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">X</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Firat</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Bapna</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Macherey</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Foster</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Schuster</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Hughes</surname></persName>
		</author>
		<idno>abs/1804.09849</idno>
		<imprint>
			<date type="published" when="2018">2018</date>
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Why self-attention? a targeted evaluation of neural machine translation architectures</title>
		<author>
			<persName><forename type="first">G</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Mller</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Rios</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Sennrich</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of EMNLP</title>
				<meeting>of EMNLP</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="4263" to="4272" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Accelerating neural transformer via an average attention network</title>
		<author>
			<persName><forename type="first">B</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Su</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of ACL</title>
				<meeting>of ACL</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="1789" to="1798" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Audio augmentation for speech recognition</title>
		<author>
			<persName><forename type="first">T</forename><surname>Ko</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Peddinti</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Povey</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Khudanpur</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of INTERSPEECH</title>
				<meeting>of INTERSPEECH</meeting>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="3586" to="3589" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Building the Singapore English national speech corpus</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">X</forename><surname>Koh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of INTERSPEECH</title>
				<meeting>of INTERSPEECH</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="321" to="325" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Espnet: End-to-end speech processing toolkit</title>
		<author>
			<persName><forename type="first">S</forename><surname>Watanabe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Hori</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Karita</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Hayashi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Nishitoba</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Unno</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><forename type="middle">E Y</forename><surname>Soplin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Heymann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Wiesner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of INTER-SPEECH</title>
				<meeting>of INTER-SPEECH</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="2207" to="2211" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Joint CTC-attention based end-to-end speech recognition using multi-task learning</title>
		<author>
			<persName><forename type="first">S</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Hori</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Watanabe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of ICASSP</title>
				<meeting>of ICASSP</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="4835" to="4839" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
