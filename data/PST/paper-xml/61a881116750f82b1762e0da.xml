<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Co-evolution Transformer for Protein Contact Prediction</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">He</forename><surname>Zhang</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Fusong</forename><surname>Ju</surname></persName>
							<email>fusongju@microsoft.com</email>
						</author>
						<author>
							<persName><forename type="first">Jianwei</forename><surname>Zhu</surname></persName>
							<email>jianwzhu@microsoft.com</email>
						</author>
						<author>
							<persName><forename type="first">Liang</forename><surname>He</surname></persName>
							<email>lihe@microsoft.com</email>
						</author>
						<author>
							<persName><forename type="first">Bin</forename><surname>Shao</surname></persName>
							<email>binshao@microsoft.com</email>
						</author>
						<author>
							<persName><forename type="first">Nanning</forename><surname>Zheng</surname></persName>
							<email>nnzheng@mail.xjtu.edu.cn</email>
						</author>
						<author>
							<persName><forename type="first">Yan</forename><surname>Liu</surname></persName>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="institution">Xi&apos;an Jiaotong University</orgName>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff1">
								<orgName type="institution">Microsoft Research Asia</orgName>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff2">
								<orgName type="institution">Microsoft Research Asia</orgName>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff3">
								<orgName type="institution">Microsoft Research Asia</orgName>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff4">
								<orgName type="institution">Microsoft Research Asia</orgName>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff5">
								<orgName type="institution">Xi&apos;an Jiaotong University</orgName>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff6">
								<orgName type="institution">Microsoft Research Asia</orgName>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff7">
								<orgName type="institution">Microsoft Research</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Co-evolution Transformer for Protein Contact Prediction</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.2" ident="GROBID" when="2022-12-25T12:44+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Proteins are the main machinery of life and protein functions are largely determined by their 3D structures. The measurement of the pairwise proximity between amino acids of a protein, known as inter-residue contact map, well characterizes the structural information of a protein. Protein contact prediction (PCP) is an essential building block of many protein structure related applications. The prevalent approach to contact prediction is based on estimating the inter-residue contacts using hand-crafted coevolutionary features derived from multiple sequence alignments (MSAs). To mitigate the information loss caused by hand-crafted features, some recently proposed methods try to learn residue co-evolutions directly from MSAs. These methods generally derive coevolutionary features by aggregating the learned residue representations from individual sequences with equal weights, which is inconsistent with the premise that residue co-evolutions are a reflection of collective covariation patterns of numerous homologous proteins. Moreover, non-homologous residues and gaps commonly exist in MSAs. By aggregating features from all homologs equally, the non-homologous information may cause misestimation of the residue co-evolutions. To overcome these issues, we propose an attention-based architecture, Co-evolution Transformer (CoT), for PCP. CoT jointly considers the information from all homologous sequences in the MSA to better capture global coevolutionary patterns. To mitigate the influence of the nonhomologous information, CoT selectively aggregates the features from different homologs by assigning smaller weights to non-homologous sequences or residue pairs. Extensive experiments on two rigorous benchmark datasets demonstrate the effectiveness of CoT. In particular, CoT achieves a 51.6% top-L long-range precision score for the Free Modeling (FM) domains on the CASP14 benchmark, which outperforms the winner group of CASP14 contact prediction challenge by 9.8%.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Most in vivo biological processes are carried out by proteins, whose functions are mainly determined by their 3D structures <ref type="bibr" target="#b0">[1]</ref>. Structural information is crucial for understanding the functions of a protein. The inter-residue contact map which measures the pairwise proximity between all amino acid pairs well characterizes the structural information of a protein. Protein contact prediction (PCP) is an important building block of many protein structure related applications, such as protein structure prediction <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b2">3]</ref>, protein complex assembly <ref type="bibr" target="#b3">[4]</ref>, and protein design <ref type="bibr" target="#b4">[5]</ref>. It is so important that it is one of the challenges of Critical Assessment of protein Structure Prediction (CASP), which is the "world championship" of the computational structural biology field.</p><p>The prevalent PCP approaches were built atop the co-evolution principle that the spatially proximate residues tend to co-evolve to maintain the functions of a protein <ref type="bibr" target="#b5">[6]</ref><ref type="bibr" target="#b6">[7]</ref><ref type="bibr" target="#b7">[8]</ref>. The existing PCP methods generally infer pairwise coevolutionary patterns from MSAs. Among them, direct coupling analysis (DCA) <ref type="bibr" target="#b8">[9]</ref> techniques are widely used to obtain coevolutionary features by fitting Potts models or calculating precision matrix <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b10">11]</ref>. Subsequently, a variety of deep-learning based methods were proposed to leverage the DCA-based features to estimate inter-residue contacts <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b11">[12]</ref><ref type="bibr" target="#b12">[13]</ref><ref type="bibr" target="#b13">[14]</ref><ref type="bibr" target="#b14">[15]</ref>. However, DCA techniques only consider single-residue and pairwise statistics of the sequences, ignoring high-order interactions among the residues within a sequence.</p><p>To mitigate the information loss caused by hand-crafted features (e.g., DCA-based features), some recently proposed methods try to learn residue co-evolutions directly from MSAs <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b15">16,</ref><ref type="bibr" target="#b16">17]</ref>. For example, the SOTA of them, CopulaNet <ref type="bibr" target="#b1">[2]</ref> learns residue representations from individual sequences and then aggregates these features with equal weights to derive residue co-evolutions. This causes two issues: 1) Inferring residue representations from individual sequences independently is inconsistent with the premise that residue co-evolutions are a reflection of collective covariation patterns of numerous homologs <ref type="bibr" target="#b17">[18]</ref>; 2) Assigning equal weights to the features coming from different homologs ignores the fact that, the quality of homologs varies a lot because of the existence of non-homologous residues and gaps <ref type="bibr" target="#b18">[19,</ref><ref type="bibr" target="#b19">20]</ref>.</p><p>In this paper, we propose an attention-based architecture, Co-evolution Transformer (CoT), to address or mitigate the issues discussed above. The core component of CoT is the co-evolution attention (CoA) module. Different from the previous methods that focus on extracting information from individual sequences, CoA is capable of incorporating the residue co-evolution patterns derived from all homologous sequences into an attention function to learn residue representations. Moreover, the CoA learns to automatically weight the residue representations learned from different homologs, and then selectively aggregates the features to construct the co-evolution attention map. This design mitigates the influence of non-homologous information.</p><p>CoT dramatically outperforms the baseline methods on two rigorous benchmarks CASP14 <ref type="bibr" target="#b20">[21]</ref> and CAMEO (Continuous Automated Model EvaluatiOn) <ref type="bibr" target="#b21">[22]</ref>. In particular, CoT achieves a 51.6% top-L long-range precision score for the Free Modeling (FM) domains on the CASP14 benchmark, which outperforms the winner group of CASP14 contact prediction challenge by 9.8%. Our code will be released at https://github.com/microsoft/ProteinFolding/tree/main/ coevolution_transformer.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Work</head><p>Protein contact prediction is a binary classification task for amino acid residue pairs. A residue pair is called a contact if their distance is less than or equal to a distance threshold, typically 8Å, i.e., 8 × 10 −10 m.</p><p>As widely acknowledged, co-evolution information is closely correlated to the contacts. In order to extract the co-evolution patterns for residue pairs, multiple sequence alignments (MSAs) are generated from raw protein sequences. For a target protein sequence, a generated alignment consists of multiple sequences with each corresponding position being an aligned residue or a gap (annotated by a dash), and these aligned sequences as a whole are called a multiple sequence alignment. Many protein databases as well as various search schemes have been proposed to generate MSAs efficiently <ref type="bibr" target="#b18">[19,</ref><ref type="bibr" target="#b19">20]</ref>. There are roughly three categories of methods for predicting contacts from MSAs, namely, unsupervised methods, supervised methods, and pre-training based methods.</p><p>Unsupervised Methods To quantify the strength of the direct relation between the residue pairs of a protein sequence while excluding effects from other residues, many statistical modeling methods based on direct coupling analysis (DCA) have been proposed to fit Potts models <ref type="bibr" target="#b9">[10]</ref> or precision matrix <ref type="bibr" target="#b10">[11]</ref> to MSAs, e.g., mean-field DCA <ref type="bibr" target="#b8">[9]</ref>, sparse inverse covariance <ref type="bibr" target="#b10">[11]</ref> and pseudo-likelihood maximization <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b22">23,</ref><ref type="bibr" target="#b23">24]</ref>, to name a few. These methods further exploit dedicated scores based on DCA for contact prediction. However, they only consider single-residue and pairwise statistics of the sequences, thus fail to capture high-order interactions among the multiple residues within a sequence. Supervised Methods By taking DCA-derived scores as features, deep neural networks based supervised methods significantly outperform the unsupervised methods <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b11">[12]</ref><ref type="bibr" target="#b12">[13]</ref><ref type="bibr" target="#b13">[14]</ref><ref type="bibr" target="#b14">[15]</ref>. However, the information lost by the DCA-based features from the sequences is still not recoverable. To mitigate this issue, several models are proposed to learn residue co-evolution information directly from the sequences in the MSAs <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b15">16,</ref><ref type="bibr" target="#b16">17,</ref><ref type="bibr" target="#b24">25]</ref>. Among them, CopulaNet <ref type="bibr" target="#b1">[2]</ref>, the SOTA of the CASP13 benchmark, derives coevolutionary features differentially by aggregating the learned residue representations from the sequences. However, although they are capable of modeling the high-order interactions among the multiple residues within single sequence, the global information carried by the MSAs is ignored because they still model the protein sequences independently. AlphaFold2 <ref type="bibr" target="#b24">[25]</ref> is claimed to be modeling the full MSAs, achieving an amazing performance on the CASP14 structure prediction task. Although its performance on the structure prediction task is remarkable, they did not participate in the CASP14 PCP task meanwhile no further details of their methods are publicly available.</p><p>Pre-training Based Methods Following the pre-train and fine-tune paradigm, pre-trained language models are adapted to representation learning for single protein sequences from the unlabeled data <ref type="bibr" target="#b25">[26]</ref><ref type="bibr" target="#b26">[27]</ref><ref type="bibr" target="#b27">[28]</ref><ref type="bibr" target="#b28">[29]</ref><ref type="bibr" target="#b29">[30]</ref><ref type="bibr" target="#b30">[31]</ref>. Many of them take contact prediction as an important downstream task to validate their performance. While these methods show another solution to this task, they are still at an early stage thus cannot achieve comparable performance to the SOTA approaches currently. To further improve the performance, a pre-trained language model named MSA Transformer is proposed to learn a better MSA representation directly <ref type="bibr" target="#b28">[29]</ref>. MSA Transformer did solid work on learning coevolution information from unlabeled MSAs. However, non-homologous subsequences are inevitably introduced during the learning process.</p><p>To exploit the co-evolution information from the full MSAs effectively, our proposed CoT model is built upon co-evolution attention, a novel attention mechanism dedicated to incorporate the coevolution information directly from MSAs in a supervised way.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Co-evolution Transformer</head><p>Co-evolution Transformer (CoT) is constructed by stacking several repeated CoT layers. Each CoT layer is composed of two attention modules, i.e., a co-evolution attention (CoA) module and a self-attention module, as shown in Figure <ref type="figure" target="#fig_1">1</ref>.</p><p>Given a prepared MSA, the stacked CoT layers are used to learn the residue representations. Residue co-evolutions are derived from the representations of the final layer and further employed to estimate the inter-residue contacts.</p><p>In this section, we first start with a brief introduction to the vanilla Transformer Encoder, followed by the detailed descriptions of each CoT component. To better illustrate the co-evolution attention mechanism, the proposed co-evolution attention mechanism and the self-attention mechanism are discussed in the end.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Vanilla Transformer Encoder</head><p>Transformer is a network architecture built on the attention mechanism for machine translation. The Transformer encoder is widely adopted for machine learning tasks due to its excellent feature extraction capability when modeling long-distance interactions in sequences <ref type="bibr" target="#b31">[32]</ref><ref type="bibr" target="#b32">[33]</ref><ref type="bibr" target="#b33">[34]</ref>. Each Transformer encoder layer consists of two modules, i.e., a multi-head self-attention (MHSA) module and a position-wise fully connected feed-forward (FFN) module. To connect these two modules, residual  connections <ref type="bibr" target="#b34">[35]</ref> and layer normalization (LAYERNORM) <ref type="bibr" target="#b35">[36]</ref> are applied as below:</p><formula xml:id="formula_0">x = LAYERNORM(x + MHSA(x)),<label>(1) x</label></formula><formula xml:id="formula_1">= LAYERNORM(x + FFN(x)),<label>(2)</label></formula><p>The multi-head self-attention module is also adopted by the CoT layer as a component to learn the residue representations of different sequences in the MSA, however, iterating through the columns instead of rows/sequences.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Co-evolution Attention Module</head><p>For a target protein sequence and its MSA, the goal of the Co-evolution Attention Module is to leverage the whole MSA to derive pairwise inter-residue interaction features, namely residue co-evolutions. Intuitively, the residue co-evolutions are analogous to the covariance matrix in mathematics, depicting the correlations between any two residue positions (two columns in the MSA).</p><p>Then, these features are used as attention maps to guide representation learning of each sequence in the MSA. To achieve this goal, a CoA module employs two consequent submodules, i.e. co-evolution aggregation and co-evolution enhancement. The co-evolution aggregation submodule is designed to generate the coevolutionary features by aggregating pairwise interactions from all homologs in the MSA, while the co-evolution enhancement submodule further enhances the coevolutionary features and derives the final co-evolution attention. The overview of the CoA module is illustrated in Figure <ref type="figure" target="#fig_2">2</ref>.</p><p>Given the target protein sequence (r 1 , r 2 . . . r L ), where L is the length of the sequence, the corresponding representation of its MSA is denoted as X ∈ R K×L×d model , where K is the number of homologous sequences and d model is the hidden dimension of the residue representation. A detailed representation schema of the MSA is described in Appendix.</p><p>For the k-th sequence in the MSA, the overall procedure of the CoA module in each layer can be summarized as follows:  </p><formula xml:id="formula_2">X k = LAYERNORM(X k + COATTN(X)),<label>(3)</label></formula><formula xml:id="formula_3">X k = LAYERNORM(X k + FFN(X k ))<label>(4)</label></formula><formula xml:id="formula_4">S k ij , k = 1 • • • K.</formula><p>The weights are used to selectively aggregate the K features. Finally, the aggregated feature is fed to a convolutional module to obtain the co-evolution attention map. For better visualization, we omit the feature channels and the details about how co-evolution attention attend to residue embeddings.</p><p>where k ∈ [1 . . . K] and COATTN is defined as:</p><formula xml:id="formula_5">COATTN(X) = CONCAT(head 1 , . . . , head H ),<label>(5)</label></formula><formula xml:id="formula_6">head h = ATTN h (X, A) X k h W h ,<label>(6)</label></formula><p>where h ∈ [1 . . . H] is the head index, A ∈ R L×L×dco is the co-evolution feature map generated with d co as its feature dimension, ATTN h (X, A) ∈ R L×L is the h-th co-evolution attention head, X k h ∈ R L×dv is the h-th segment of X k , and W h ∈ R dv×dv are learnable weights (d v = d model /H). Note that the co-evolution attention ATTN h (X, A) is shared by all the K homologous sequences within the same head.</p><p>Submodule 1: Co-evolution Aggregation To depict inter-residue interactions, the co-evolution aggregation submodule exploits outer product on the residue representations. A selective pooling operation is then applied to the aggregated outer products from all homologous sequences, the disturbance of non-homologous information can be greatly reduced by this weighting mechanism.</p><p>Two tensors P, Q ∈ R K×L×d model , are generated from X by separate linear projections. For the residue pair r i , r j in the k-th sequence of the MSA, the pair co-evolution features A ij are calculated by:</p><formula xml:id="formula_7">A ij = PROJ K k=1 S k ij (P k i ⊗ P k j ) ,<label>(7)</label></formula><p>and</p><formula xml:id="formula_8">S k ij = 1 Z exp(Q k i ⊗ Q k j ), (<label>8</label></formula><formula xml:id="formula_9">)</formula><p>where ⊗ is the outer product operator, is the element-wise multiplication operator, PROJ is a flatten operation followed by a linear projection, converting the aggregated pair co-evolution features to A ∈ R L×L×dco , and Z stands for the normalization factor. The weight for the pair representation is denoted as S k ij , where S k ij ∈ R d model ×d model whose elements fall in the range of [0, 1]. The weights are normalized over the k sequences, thus we have k S k ij = 1.</p><p>Submodule 2: Co-evolution Enhancement To model the high-order interactions among multiple residues, the co-evolution map A is concatenated with A from the previous CoT layer, the values are fed into a convolutional module (CONV) to generate an enhanced co-evolution map by:</p><formula xml:id="formula_10">A = CONV([A ; A]).<label>(9)</label></formula><p>In practice, we adopt a ResNet <ref type="bibr" target="#b34">[35]</ref> as the convolution module. Note that we simply assign A an all-zero tensor for the first layer.</p><p>The h-th co-evolution attention head is projected from A by</p><formula xml:id="formula_11">ATTN h (X, A) = SOFTMAX(A M h ),<label>(10)</label></formula><p>where M h ∈ R dco×1 are learnable weights.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Why Co-evolution Attention</head><p>A straightforward way to model the co-evolution is by applying self-attention to individual sequences in the MSA. In this case, for a residue pair r i , r j in the k-th homolog, self-attention measures their compatibility (attention weight, A k ij ) by:</p><formula xml:id="formula_12">A k ij ∝ SOFTMAX{(U X k i ) (V X k j )},<label>(11)</label></formula><p>where U, V are learnable weights.</p><p>In comparison, the attention weight of COA, calculated by the co-evolution aggregation module (AGGREGATE) and the co-evolution enhancement module (ENHANCE), can be summarized as:</p><formula xml:id="formula_13">A ij = AGGREGATE(X i ⊗ X j ),<label>(12)</label></formula><formula xml:id="formula_14">A ij = ENHANCE(A ij , NEIGHBOR(i, j)),<label>(13)</label></formula><formula xml:id="formula_15">A k ij ∝ SOFTMAX{A ij },<label>(14)</label></formula><p>Compare with self-attention, co-evolution attention is more expressive at three aspects: 1) COA leverages the global information from all the homologs instead of single homolog to derive the attention, which fits better to the residue co-evolution insight; 2) COA handles the non-homologous information naturally by the selective pooling operation during aggregation, providing a solution to a widely existed but inevitable dilemma for MSAs; 3) COA enhances the co-evolution signal by propagating information from the neighbors, making it easier to capture the high-order interactions among multiple residues.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experimental Evaluation</head><p>We have conducted extensive experiments and analyses to evaluate the effectiveness of CoT.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Experiment Setup</head><p>Benchmark Two standard benchmarks are used for the model evaluation in the conducted experiments, i.e., CASP14 and CAMEO. 1) CASP14 is the latest and most important benchmark for protein contact prediction <ref type="bibr" target="#b20">[21]</ref>. This benchmark includes three kinds of protein domains, i.e., FM (22 domains), FM/TBM (14 domains), and TBM (50 domains), where a domain is a protein sequence prepared by the CASP organizers. The first protein among them was released on May. 18, 2020. 2) CAMEO is another benchmark to evaluate weekly-updated protein structure submissions continuously <ref type="bibr" target="#b21">[22]</ref>. The proteins are classified into three categories: easy, medium, and hard. Among them, 176 hard targets, released in the last year (from 2020-04-17 to 2021-04-10), are selected for model evaluation.</p><p>Dataset All models are trained on 96, 167 protein structures (chains) collected from PDB <ref type="bibr" target="#b36">[37]</ref> (before Apr. Evaluation Following the procedure of trRosetta <ref type="bibr" target="#b14">[15]</ref>, the contact prediction task is converted into a multi-class classification task. The inter-residue distance range is divided into 37 bins, i.e., (0 Å, 2.5 Å], (2.5 Å, 3.0 Å], • • • , (20.0 Å, +∞), while the models are trained with the bin labels. For contact, the summed probability value of the bins with distance less than 8 Å are used as the final prediction.</p><p>Metrics For the evaluation criterion, the prevalent metrics are employed, which are Precision@L, Precision@L/2, and Precision@L/5 of long-range residue contacts, where Precision@n stands for the precision score for the top-n pairs of the highest probability in the predicted contact map. Here, L refers to the length of protein sequence and long-range means there are at least 23 other residues between these two residues in the sequence.</p><p>Implementations Given an MSA, 256 sequences are randomly sampled and cropped by length 200.</p><p>The CoT model is equipped with 6 CoT layers with hidden size as 128 and the attention head number as 8. All models are trained with Adam optimizer <ref type="bibr" target="#b37">[38]</ref> via a cross-entropy loss for 100k iterations. The learning rate, the weight decay, and the batch size are set to 10 −4 , 0.01, and 16 respectively. The hyperparameters of the CoT model is selected according to the performance on the validation set, and a detailed comparison of different hyperparameter settings are summarized in Appendix. The total training cost of the CoT model is about 30 hours on 4 Tesla V100 GPU cards.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Evaluation Results</head><p>For the sake of fairness, CoT is compared with three SOTA methods for contact prediction, including RaptorX <ref type="bibr" target="#b13">[14]</ref>, trRosetta <ref type="bibr" target="#b14">[15]</ref> and CopulaNet <ref type="bibr" target="#b1">[2]</ref>, on the identical MSAs by searching BFD30 using HHblits with default parameters. As shown in Table <ref type="table">1</ref>, CoT outperforms the baselines, while surpassing CopulaNet, the best of the SOTAs, by 9.7%, 4.5%, 10.1% and 10.1% for Precision@L scores on four kinds of targets, respectively. MSA Transformer <ref type="bibr" target="#b28">[29]</ref> is not included in the comparison due to the different task settings (pretrained vs. supervised) and the unavailability of its supervised finetuning codebase. Nevertheless, we rerun the CoT model on the CASP13-FM dataset to compare with it, obtaining a 65.0% Precision@L score, which is better than 57.1% reported in MSA Transformer. A detailed discussion is described in Appendix.</p><p>Methods used by the groups participated in the CASP14 challenge are considered to the best for the benchmark due to their deep optimization towards MSA generation and model ensemble strategy.</p><p>To further evaluate the performance of the proposed method, the top-3 winner groups/methods on   As shown in Table <ref type="table" target="#tab_1">2</ref>, CoT † outperforms the best method. On the hardest domains (CASP14-FM), CoT † even increases 9.8%, 12.5%, 13.3% scores for all the three metrics, compared with Gr. 368, the best group on this benchmark.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Comparison with Self-attention</head><p>To compare co-evolution attention with self-attention, a CoT model variant where the COA module is replaced by the self-attention module is implemented (denoted as CoT-SA). Both CoT and CoT-SA are evaluated on the CASP14 and CAMEO benchmarks. As shown in Table <ref type="table">1</ref>, the CoT outperforms CoT-SA by 6.4%, 7.5%, 7.7%, and 6.8% Precision@L scores on four kinds of targets, indicating that CoA is much better than self-attention for this task. To further understand the co-evolution attention, the two attention matrices for both models are visualized. As shown in Figure <ref type="figure" target="#fig_4">3</ref> (a) and (c), the co-evolution attention patterns are much closer to the ground-truth contact map than that of self-attention, illustrating that CoA is more effective in extracting contact patterns.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Ablation Study</head><p>To evaluate the contribution of each model component, we set up ablative configurations for the co-evolution aggregation submodule, the enhancement submodule, and the self-attention module.</p><p>Experiments with various ablative configurations are conducted as listed in Table <ref type="table" target="#tab_2">3</ref>. For the coevolution aggregation submodule, an average pooling is implemented as an alternative of selective pooling, where the features of different sequences are aggregated equally. The result shows that selective pooling obviously plays a critical role in the model, as the Precision@L score of the models with selective pooling on FM domains increased from 42.7% to 48.2% compared with that with average pooling.  <ref type="figure">-------------------------------------------I</ref>   <ref type="figure">-------------------------------------------E</ref>   <ref type="figure">-----------------------------------------------------------------------------------------L</ref>   <ref type="figure">------------------------------------Y</ref>   <ref type="figure">-----------------------------------</ref> <ref type="figure">---------------------------------------------L</ref>   <ref type="figure">---------------------------------------------I</ref>   <ref type="figure">---------------------------------------------L</ref>   <ref type="figure">-----------------------------------I</ref>   <ref type="figure">---------------------------------------------I</ref>   <ref type="figure">---------------------------------------------I</ref>   <ref type="figure">---------------------------------------------I</ref>  <ref type="figure">---------------------------------------------L</ref>   <ref type="figure">---------------------------------------L</ref>  <ref type="figure">--------------------------------------------</ref>  Figure <ref type="figure" target="#fig_17">4</ref> (a) is an MSA for the CASP14 target T1061-D1. Since the model is not sensitive to MSA sequence order, we rearrange the MSA for better visualization. Then we visualize the sequence weights CoT learned from the MSA for two selected residue pairs r 44 , r 55 and r 105 , r 127 . Figure <ref type="figure" target="#fig_17">4</ref> (b) and (c) show that selective pooling assigns larger weights to more homologous sequences as expected. Meanwhile, the difference between two sequence weight distributions of the two pairs demonstrates that the model is able to learn pair-specific weights.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>L T D Y F R D V T Y N G I L Y R S G K V K S I S S H K Q N R Q L S I G S L S F T I T G T A E D E V L K L V Q N G V S F L D R G I T I H Q A I I N E E G N I L P V D P D T D G P L L F --------------------------S E T T V A R A T N M S L T L L G E E N E E L S G A T L S N P L F T N R E V F V H K I F F D A E V E K --------G A I Q V -------------------------------------------------------Y A R Y I N R E V F V Y K V H I N P E G T I L ------S A P Y L V --------------------------------------------D S P E V E S V L S T Y A R Y I N R D V F V Y K A H I N V D G A I I ------G E P Y L L --------------------------------------------D S P E V E S V L S T Y A R Y I N R D V F V Y K V H I N P D G A V I ------G G A Y L L ------------------------------------------------------T Y N G Y I N R D V T I Y R A H I D P N G V V I ------G A P Y L I ----------------------------------------------------------Y H N R D V F I Y K A F L D P S ---I -------T P I L V --------------------------------------------A N P E V E G V L L R Y S R Y I N R E V F V Y K A H I N P D G S F I ------G D P Y L L --------------------------------------------T P D E V S G L I Q T Y A G Y I N R E V F I Y K A H L D P N G A I I ------G A P Y L L ----------------------------------------------------------F F Q G A G I P E G V S I E I T P D S P ------V G P I L L --------------------------------------------V S D E L Q G P L N T L K S Y A N R D V F V Y K A F L D P D G T L I -----D S S A V L I ---------------------------------------------S D E L K G P L -T L K S Y H N R D V F I Y K A F L N P D Y T -I -----I G A P T L V --------------------------------------------I S D E I S G P L N T L K S Y A N K D V F V Y K V F L D P D G T V L -----D N S G V L I -</head><formula xml:id="formula_17">- V T D G S R D I V F N D Q T Y V A N K L T K V G A V S E T I Q A K A S S M T L N V S S A -</formula><formula xml:id="formula_18">T D A G Y N V T W D D R E Y V A N K L L K V G S V T E S T R I K V D G M S I D L D A T -</formula><formula xml:id="formula_19">T D A S I D L Y F N D T K Y R A N R V L Q V P S I T E Y S E V R A T S Q N L Q L D G N -</formula><p>The co-evolution enhancement is a convolutional module to learn correlations among multiple residues, and the overall performance reduces significantly on FM domains from 48.2% to 41.6% when removing this module, as compared in Table <ref type="table" target="#tab_2">3</ref>. These results illustrate that the high-order residue interaction is very important for contact prediction, as the interaction might reflect some structure patterns (e.g, structure motifs) <ref type="bibr" target="#b13">[14,</ref><ref type="bibr" target="#b38">39]</ref>. Moreover, as illustrated in Figure <ref type="figure" target="#fig_4">3</ref>, by applying co-evolution enhancement, long-range residue interactions can be better extracted.</p><p>By ablating the self-attention module, the overall model performance drops slightly, indicating that the CoA module is the main contributor to achieve the performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5">Model Analysis</head><p>The Effect of MSA Quality The quality of predicted contacts is highly related to M ef f <ref type="bibr" target="#b39">[40]</ref>, which is the number of the non-redundant sequence homologs in the MSA (sequences of more than 70% identity is considered redundant). For the CASP14 FM domains, the correlation coefficients between the logarithm of M ef f and the Precision@L scores is 0.63, demonstrating that the performance of contact prediction is strongly correlated to MSA quality. CoT cannot predict the contact maps of some proteins very well (e.g., the Precision@L score of T1093-D1 is only 20.6%). For most of these proteins, the M ef f is as small as less than 20, indicating that low-quality MSAs are still a bottleneck for contact prediction. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusion</head><p>We propose an attention-based architecture (CoT) to learn residue coevolutions from multiple sequence alignments (MSAs). As the core component of CoT, co-evolution attention (CoA) leverages the full information of an MSA to learn residue representations by treating coevolutionary patterns as attention. Moreover, it employs a selective pooling operation to mitigate the influence of nonhomologous information. The experimental results on two rigorous benchmarks demonstrate the effectiveness of CoT.</p><p>On the other hand, the experimental results also reveal a failure case of CoT, i.e., CoT cannot accurately predict the contacts of proteins with low-depth MSAs. This is an issue shared by other existing approaches as well. to address the issue caused by low-quality MSAs remains an open problem. We believe pretrained protein models may be a potential solution to it <ref type="bibr" target="#b25">[26]</ref><ref type="bibr" target="#b26">[27]</ref><ref type="bibr" target="#b27">[28]</ref><ref type="bibr" target="#b28">[29]</ref><ref type="bibr" target="#b29">[30]</ref>.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: An Overview of Co-evolution Transformer. Given an input MSA, several CoT layers are stacked to learn residue representations, which are then used to derive residue co-evolutions to estimate inter-residue contacts. Each CoT layer consists of a co-evolution Attention (CoA) module and a self-attention module.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: Schema of Co-evolution Attention. Given a residue pair r i , r j , we first learn K covariance features and a set of pair-specific sequence weights S kij , k = 1 • • • K.The weights are used to selectively aggregate the K features. Finally, the aggregated feature is fed to a convolutional module to obtain the co-evolution attention map. For better visualization, we omit the feature channels and the details about how co-evolution attention attend to residue embeddings.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: Comparison of the extracted attention maps for 4q2z_H. (a) CoT-SA model; (b) CoT model w/o co-evolution enhancement; (c) CoT model; (d) Ground-truth contact map. The red circle covers typical long-range contacts.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head></head><label></label><figDesc>N E E Y K G A T M T N P S F V N R E V F V Y K A F I N P E G G ---------D P V L I -</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head></head><label></label><figDesc>S D E I T A P L Q A S P S F I N R T V L I E K V F I D P E G T P W ------G A A I T I -</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head></head><label></label><figDesc>T D A G H N I S F D D Q V Y I A D K I L N V S N Y S E T V E A R A S G M T L D I A A E A S D E L K G P I -</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head></head><label></label><figDesc>T D G S Y D I E F D D Q T Y I A N K V I N V G S I Q E T I E A R A T A I N L Q I A G A -</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_11"><head></head><label></label><figDesc>T D A S Y D I S F D D Q K Y R A N T L L S V S N V T E T I E A R A S N F N I K L A A T -</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_12"><head></head><label></label><figDesc>T D S Q Y D I A Y N D Q N Y I A N K L T K L G T I K E S I L A K A S T I T M T L D S A S S E E I N T L I V -</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_13"><head></head><label></label><figDesc>T D G A F D I V W D D Q T Y N A N K L G K I G S I T E T T E A K A S S T N I T L D T A -</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_14"><head></head><label></label><figDesc>T D A G Y D I S F D D Q N Y V A N R V T S V G S F T D T I D L K I A N V S L K L D A T -</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_15"><head></head><label></label><figDesc>T D A A Y D I L F N D Q I Y R A N K L S K V G S V S E S S T I K I S S M N I D L N A S -</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_16"><head></head><label></label><figDesc>T D G A V N I D Y N D Q T Y I A D K I A K I G S Y S E T I E A K A T G M T L S L S A E V S D E I Q -</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_17"><head>Figure 4 :</head><label>4</label><figDesc>Figure 4: Learned sequence weights for T1061-D1. (a) Part of MSA for T1061-D1 (residue 40-130), where the purple region represents aligned residues and '-' stands for a gap. (b) The sequence weight distribution for the pair r 44 , r 55 . (c) The sequence weight distribution for the pair r 105 , r 127 .</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 :</head><label>2</label><figDesc>Comparison on CASP14. Gr. 368, Gr. 488, and Gr. 010 are the results of the top-3 groups in the CASP14 challenge. CoT † refers to the results of CoT with MSA selection.</figDesc><table><row><cell cols="7">Table 1: Comparison on CASP14 and CAMEO (Precision@L )</cell></row><row><cell></cell><cell></cell><cell></cell><cell cols="2">CASP14</cell><cell></cell><cell>CAMEO</cell></row><row><cell cols="2">Methods</cell><cell cols="5">FM (22) FM/TBM (14) TBM (50) Hard (176)</cell></row><row><cell cols="2">RaptorX [14]</cell><cell>33.9</cell><cell>58.1</cell><cell>63.1</cell><cell></cell><cell>53.2</cell></row><row><cell cols="2">trRosetta [15]</cell><cell>31.3</cell><cell>57.6</cell><cell>61.1</cell><cell></cell><cell>50.1</cell></row><row><cell cols="2">CopulaNet [2]</cell><cell>38.5</cell><cell>62.2</cell><cell>65.5</cell><cell></cell><cell>56.5</cell></row><row><cell cols="2">CoT-SA (ours)</cell><cell>41.8</cell><cell>59.2</cell><cell>67.9</cell><cell></cell><cell>59.8</cell></row><row><cell cols="2">CoT (ours)</cell><cell>48.2</cell><cell>66.7</cell><cell>75.6</cell><cell></cell><cell>66.6</cell></row><row><cell></cell><cell></cell><cell>FM (22)</cell><cell cols="2">FM/TBM (14)</cell><cell></cell><cell>TBM (50)</cell></row><row><cell>Method</cell><cell>L</cell><cell>L/2 L/5</cell><cell>L</cell><cell>L/2 L/5</cell><cell>L</cell><cell>L/2 L/5</cell></row><row><cell>Gr. 368</cell><cell cols="6">41.8 55.7 66.6 64.5 78.6 87.4 73.1 87.1 94.5</cell></row><row><cell>Gr. 488</cell><cell cols="6">40.4 52.9 65.0 63.6 78.8 88.5 72.0 86.9 93.7</cell></row><row><cell>Gr. 010</cell><cell cols="6">39.6 53.4 63.8 61.5 77.0 86.8 66.1 80.9 89.5</cell></row><row><cell cols="7">CoT  † (ours) 51.6 68.2 79.9 66.8 82.2 90.5 77.9 91.0 96.1</cell></row></table><note>1, 2020), which are split into the train and validation sets (95, 667 and 500 proteins, respectively). For each protein sequence, we generate its MSA by searching UniRef30 (version 2020-02), UniRef90 (version 2020-02), BFD30 (version 2019-03), MGnify90 (version 2019-05) with HHblits (version 3.3.0) and HMMER (version 3.3.2). The details of the MSA generation procedure are described in Appendix.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 3 :</head><label>3</label><figDesc>Ablations for CoA on CASP14 (Precision@L). AGGRE., ENHAN. and SA refer to the co-evolution aggregation submodule, the co-evolution enhancement submodule, and the self-attention module, respectively. CASP14 are compared with CoT. Different groups use different data sources to build their MSAs. To eliminate the variance of different data sources, the most confident prediction (the prediction with the highest probability score) of CoT on 12 MSAs from different databases with different search settings, denoted as CoT † , is selected as the final prediction.</figDesc><table><row><cell>CASP14</cell><cell>CAMEO</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head></head><label></label><figDesc>The Effect of Training Data Most methods generate the training data from the same data sources, i.e., public protein sequence databases (UniProt, Metagenome database) and structure databases (PDB). However, different methods customize the data processing pipelines (e.g., MSA generation strategies), causing slightly different training data for the models. To study the effect of training data, we train another model CoT * on a smaller training dataset but of comparable size to that used by CopulaNet<ref type="bibr" target="#b1">[2]</ref>, i.e., PDB30. The performance of CoT * is slightly lower than CoT, e.g., 46.3% vs. 48.2% for Precision@L on the CASP14 FM domains and 66.1% vs. 66.6% on the CAMEO hard targets. But it still outperforms CopulaNet a lot, for example, 46.3% vs. 38.2% for Precision@L on CASP14 FM domains. These results demonstrate that the performance gain of CoT is not mainly achieved by the larger size of the training data.</figDesc><table /></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgements</head><p>We thank all the anonymous reviewers for their valuable comments. The work was supported in part with National Key R&amp;D Program of China Grant 2017YFA0700800.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">Proteins: structure and function</title>
		<author>
			<persName><forename type="first">David</forename><surname>Whitford</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2013">2013</date>
			<publisher>John Wiley &amp; Sons</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">CopulaNet: Learning residue co-evolution directly from multiple sequence alignment for protein structure prediction</title>
		<author>
			<persName><forename type="first">Fusong</forename><surname>Ju</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jianwei</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bin</forename><surname>Shao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lupeng</forename><surname>Kong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tie-Yan</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wei-Mou</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dongbo</forename><surname>Bu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nature Communications</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1" to="9" />
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Improved protein structure prediction using potentials from deep learning</title>
		<author>
			<persName><forename type="first">Richard</forename><surname>Andrew W Senior</surname></persName>
		</author>
		<author>
			<persName><forename type="first">John</forename><surname>Evans</surname></persName>
		</author>
		<author>
			<persName><forename type="first">James</forename><surname>Jumper</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Laurent</forename><surname>Kirkpatrick</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tim</forename><surname>Sifre</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chongli</forename><surname>Green</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Augustin</forename><surname>Qin</surname></persName>
		</author>
		<author>
			<persName><surname>Žídek</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alex</forename><surname>Alexander Wr Nelson</surname></persName>
		</author>
		<author>
			<persName><surname>Bridgland</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nature</title>
		<imprint>
			<biblScope unit="volume">577</biblScope>
			<biblScope unit="issue">7792</biblScope>
			<biblScope unit="page" from="706" to="710" />
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Accurate prediction of inter-protein residue-residue contacts for homo-oligomeric protein complexes</title>
		<author>
			<persName><forename type="first">Yumeng</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sheng-You</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Briefings in Bioinformatics</title>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">De novo protein design by deep network hallucination</title>
		<author>
			<persName><forename type="first">Ivan</forename><surname>Anishchenko</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tamuka</forename><surname>Martin Chidyausiku</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sergey</forename><surname>Ovchinnikov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Samuel</forename><forename type="middle">J</forename><surname>Pellock</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><surname>Baker</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">bioRxiv</title>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Three-dimensional structures of membrane proteins from genomic sequencing</title>
		<author>
			<persName><forename type="first">Thomas</forename><forename type="middle">A</forename><surname>Hopf</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lucy</forename><forename type="middle">J</forename><surname>Colwell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Robert</forename><surname>Sheridan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Burkhard</forename><surname>Rost</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chris</forename><surname>Sander</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Debora</forename><forename type="middle">S</forename><surname>Marks</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Cell</title>
		<imprint>
			<biblScope unit="volume">149</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="1607" to="1621" />
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Protein structure prediction from sequence variation</title>
		<author>
			<persName><forename type="first">Debora</forename><forename type="middle">S</forename><surname>Marks</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Thomas</forename><forename type="middle">A</forename><surname>Hopf</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chris</forename><surname>Sander</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nature biotechnology</title>
		<imprint>
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="1072" to="1080" />
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Protein contact prediction using metagenome sequence data and residual neural networks</title>
		<author>
			<persName><forename type="first">Qi</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhenling</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ivan</forename><surname>Anishchenko</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qian</forename><surname>Cong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><surname>Baker</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jianyi</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Bioinformatics</title>
		<imprint>
			<biblScope unit="volume">36</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="41" to="48" />
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Direct-coupling analysis of residue coevolution captures native contacts across many protein families</title>
		<author>
			<persName><forename type="first">Faruck</forename><surname>Morcos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrea</forename><surname>Pagnani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bryan</forename><surname>Lunt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Arianna</forename><surname>Bertolino</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Debora</forename><forename type="middle">S</forename><surname>Marks</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chris</forename><surname>Sander</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Riccardo</forename><surname>Zecchina</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>José</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Terence</forename><surname>Onuchic</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Martin</forename><surname>Hwa</surname></persName>
		</author>
		<author>
			<persName><surname>Weigt</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Proceedings of the National Academy of Sciences</title>
		<imprint>
			<biblScope unit="volume">108</biblScope>
			<biblScope unit="issue">49</biblScope>
			<biblScope unit="page" from="E1293" to="E1301" />
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Improved contact prediction in proteins: using pseudolikelihoods to infer potts models</title>
		<author>
			<persName><forename type="first">Magnus</forename><surname>Ekeberg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Cecilia</forename><surname>Lövkvist</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yueheng</forename><surname>Lan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Martin</forename><surname>Weigt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Erik</forename><surname>Aurell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Physical Review E</title>
		<imprint>
			<biblScope unit="volume">87</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page">12707</biblScope>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">PSICOV: precise structural contact prediction using sparse inverse covariance estimation on large multiple sequence alignments</title>
		<author>
			<persName><forename type="first">Daniel</forename><forename type="middle">Wa</forename><surname>David T Jones</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Domenico</forename><surname>Buchan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Massimiliano</forename><surname>Cozzetto</surname></persName>
		</author>
		<author>
			<persName><surname>Pontil</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Bioinformatics</title>
		<imprint>
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="184" to="190" />
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">High precision in protein contact prediction using fully convolutional neural networks and minimal sequence features</title>
		<author>
			<persName><forename type="first">T</forename><surname>David</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shaun</forename><forename type="middle">M</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName><surname>Kandathil</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Bioinformatics</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="issue">19</biblScope>
			<biblScope unit="page" from="3308" to="3315" />
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Accurate de novo prediction of protein contact map by ultra-deep learning model</title>
		<author>
			<persName><forename type="first">Sheng</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Siqi</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhen</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Renyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jinbo</forename><surname>Xu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">PLoS computational biology</title>
		<imprint>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page">e1005324</biblScope>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Distance-based protein folding powered by deep learning</title>
		<author>
			<persName><forename type="first">Jinbo</forename><surname>Xu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the National</title>
				<meeting>the National</meeting>
		<imprint>
			<publisher>Academy of Sciences</publisher>
			<date type="published" when="2019">2019</date>
			<biblScope unit="volume">116</biblScope>
			<biblScope unit="page" from="16856" to="16865" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Improved protein structure prediction using predicted interresidue orientations</title>
		<author>
			<persName><forename type="first">Jianyi</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ivan</forename><surname>Anishchenko</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hahnbeom</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhenling</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sergey</forename><surname>Ovchinnikov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><surname>Baker</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Proceedings of the National Academy of Sciences</title>
		<imprint>
			<biblScope unit="volume">117</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="1496" to="1503" />
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Deep learning-based prediction of protein structure using learned representations of multiple sequence alignments</title>
		<author>
			<persName><forename type="first">Shaun</forename><forename type="middle">M</forename><surname>Kandathil</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Joe</forename><forename type="middle">G</forename><surname>Greener</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andy</forename><forename type="middle">M</forename><surname>Lau</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><forename type="middle">T</forename><surname>Jones</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">bioRxiv</title>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">rawMSA: End-to-end deep learning using raw multiple sequence alignments</title>
		<author>
			<persName><forename type="first">Claudio</forename><surname>Mirabello</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Björn</forename><surname>Wallner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">PloS one</title>
		<imprint>
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page">e0220182</biblScope>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Deep generative models of variation capture the effects of mutations</title>
		<author>
			<persName><forename type="first">John</forename><forename type="middle">B</forename><surname>Adam J Riesselman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Debora</forename><forename type="middle">S</forename><surname>Ingraham</surname></persName>
		</author>
		<author>
			<persName><surname>Marks</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nature methods</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page" from="816" to="822" />
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">HHblits: lightningfast iterative protein sequence searching by hmm-hmm alignment</title>
		<author>
			<persName><forename type="first">Michael</forename><surname>Remmert</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andreas</forename><surname>Biegert</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andreas</forename><surname>Hauser</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Johannes</forename><surname>Söding</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nature methods</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="173" to="175" />
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">A comprehensive benchmark study of multiple sequence alignment methods: current challenges and future perspectives</title>
		<author>
			<persName><forename type="first">Julie</forename><forename type="middle">D</forename><surname>Thompson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Benjamin</forename><surname>Linard</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Odile</forename><surname>Lecompte</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Olivier</forename><surname>Poch</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">PloS one</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page">e18093</biblScope>
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Critical assessment of methods of protein structure prediction (CASP)-Round XIII</title>
		<author>
			<persName><forename type="first">Andriy</forename><surname>Kryshtafovych</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Torsten</forename><surname>Schwede</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Maya</forename><surname>Topf</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Krzysztof</forename><surname>Fidelis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">John</forename><surname>Moult</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Proteins: Structure, Function, and Bioinformatics</title>
		<imprint>
			<biblScope unit="volume">87</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page" from="1011" to="1020" />
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Continuous Automated Model EvaluatiOn (CAMEO) complementing the critical assessment of structure prediction in casp12</title>
		<author>
			<persName><forename type="first">Jürgen</forename><surname>Haas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alessandro</forename><surname>Barbato</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dario</forename><surname>Behringer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gabriel</forename><surname>Studer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Steven</forename><surname>Roth</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Martino</forename><surname>Bertoni</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Khaled</forename><surname>Mostaguir</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rafal</forename><surname>Gumienny</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Torsten</forename><surname>Schwede</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Proteins: Structure, Function, and Bioinformatics</title>
		<imprint>
			<biblScope unit="volume">86</biblScope>
			<biblScope unit="page" from="387" to="398" />
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Learning generative models for protein fold families</title>
		<author>
			<persName><forename type="first">Sivaraman</forename><surname>Balakrishnan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hetunandan</forename><surname>Kamisetty</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jaime</forename><forename type="middle">G</forename><surname>Carbonell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Su-In</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christopher</forename><surname>James</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Langmead</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Proteins: Structure, Function, and Bioinformatics</title>
		<imprint>
			<biblScope unit="volume">79</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="1061" to="1078" />
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">CCMpred-fast and precise prediction of protein residue-residue contacts from correlated mutations</title>
		<author>
			<persName><forename type="first">Stefan</forename><surname>Seemayer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Markus</forename><surname>Gruber</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Johannes</forename><surname>Söding</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Bioinformatics</title>
		<imprint>
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="issue">21</biblScope>
			<biblScope unit="page" from="3128" to="3130" />
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">High accuracy protein structure prediction using deep learning</title>
		<author>
			<persName><forename type="first">John</forename><surname>Jumper</surname></persName>
		</author>
		<author>
			<persName><surname>Evans</surname></persName>
		</author>
		<author>
			<persName><surname>Pritzel</surname></persName>
		</author>
		<author>
			<persName><surname>Green</surname></persName>
		</author>
		<author>
			<persName><surname>Figurnov</surname></persName>
		</author>
		<author>
			<persName><surname>Tunyasuvunakool</surname></persName>
		</author>
		<author>
			<persName><surname>Ronneberger</surname></persName>
		</author>
		<author>
			<persName><surname>Bates</surname></persName>
		</author>
		<author>
			<persName><surname>Zidek</surname></persName>
		</author>
		<author>
			<persName><surname>Bridgland</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Fourteenth Critical Assessment of Techniques for Protein Structure Prediction</title>
				<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="page">24</biblScope>
		</imprint>
	</monogr>
	<note>Abstract Book</note>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title level="m" type="main">ProtTrans: Towards cracking the language of life&apos;s code through self-supervised deep learning and high performance computing</title>
		<author>
			<persName><forename type="first">Ahmed</forename><surname>Elnaggar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><surname>Heinzinger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christian</forename><surname>Dallago</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ghalia</forename><surname>Rihawi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yu</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Llion</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tom</forename><surname>Gibbs</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tamas</forename><surname>Feher</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christoph</forename><surname>Angerer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Debsindhu</forename><surname>Bhowmik</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2007.06225</idno>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Evaluating protein transfer learning with tape</title>
		<author>
			<persName><forename type="first">Roshan</forename><surname>Rao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nicholas</forename><surname>Bhattacharya</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Neil</forename><surname>Thomas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yan</forename><surname>Duan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xi</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">John</forename><surname>Canny</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pieter</forename><surname>Abbeel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yun</forename><forename type="middle">S</forename><surname>Song</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
				<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="page">9689</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Transformer protein language models are unsupervised structure learners</title>
		<author>
			<persName><forename type="first">Roshan</forename><surname>Rao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sergey</forename><surname>Ovchinnikov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Joshua</forename><surname>Meier</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alexander</forename><surname>Rives</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tom</forename><surname>Sercu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">bioRxiv</title>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title/>
		<author>
			<persName><forename type="first">Roshan</forename><surname>Rao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jason</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Robert</forename><surname>Verkuil</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Joshua</forename><surname>Meier</surname></persName>
		</author>
		<author>
			<persName><forename type="first">John</forename><forename type="middle">F</forename><surname>Canny</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pieter</forename><surname>Abbeel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="s">Tom Sercu, and Alexander Rives. MSA transformer. bioRxiv</title>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Biological structure and function emerge from scaling unsupervised learning to 250 million protein sequences</title>
		<author>
			<persName><forename type="first">Alexander</forename><surname>Rives</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Joshua</forename><surname>Meier</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tom</forename><surname>Sercu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Siddharth</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zeming</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jason</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Demi</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Myle</forename><surname>Ott</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lawrence</forename><surname>Zitnick</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jerry</forename><surname>Ma</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Proceedings of the National Academy of Sciences</title>
		<imprint>
			<biblScope unit="volume">118</biblScope>
			<biblScope unit="issue">15</biblScope>
			<biblScope unit="page">2021</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
		<title level="m" type="main">Profile prediction: An alignment-based pre-training task for protein sequence models</title>
		<author>
			<persName><forename type="first">Pascal</forename><surname>Sturmfels</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jesse</forename><surname>Vig</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ali</forename><surname>Madani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nazneen</forename><surname>Fatema</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rajani</forename></persName>
		</author>
		<idno type="arXiv">arXiv:2012.00195</idno>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
		<title level="m" type="main">Attention-based models for speech recognition</title>
		<author>
			<persName><forename type="first">Jan</forename><surname>Chorowski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dzmitry</forename><surname>Bahdanau</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dmitriy</forename><surname>Serdyuk</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kyunghyun</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1506.07503</idno>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Video action transformer network</title>
		<author>
			<persName><forename type="first">Rohit</forename><surname>Girdhar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Joao</forename><surname>Carreira</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Carl</forename><surname>Doersch</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrew</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
				<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="244" to="253" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
		<title level="m" type="main">A survey on visual transformer</title>
		<author>
			<persName><forename type="first">Kai</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yunhe</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hanting</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xinghao</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jianyuan</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhenhua</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yehui</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">An</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chunjing</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yixing</forename><surname>Xu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2012.12556</idno>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shaoqing</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
				<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="770" to="778" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<monogr>
		<title/>
		<author>
			<persName><forename type="first">Jimmy</forename><surname>Lei Ba</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jamie</forename><surname>Ryan Kiros</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Geoffrey</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1607.06450</idno>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
	<note type="report_type">Layer normalization. arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">RCSB Protein Data Bank: biological macromolecular structures enabling research and education in fundamental biology</title>
		<author>
			<persName><forename type="first">Helen</forename><forename type="middle">M</forename><surname>Stephen K Burley</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Charmi</forename><surname>Berman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chunxiao</forename><surname>Bhikadiya</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Li</forename><surname>Bi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Luigi</forename><forename type="middle">Di</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Cole</forename><surname>Costanzo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ken</forename><surname>Christie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jose</forename><forename type="middle">M</forename><surname>Dalenberg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shuchismita</forename><surname>Duarte</surname></persName>
		</author>
		<author>
			<persName><surname>Dutta</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">biomedicine, biotechnology and energy. Nucleic acids research</title>
		<imprint>
			<biblScope unit="volume">47</biblScope>
			<biblScope unit="issue">D1</biblScope>
			<biblScope unit="page" from="D464" to="D474" />
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<monogr>
		<author>
			<persName><forename type="first">P</forename><surname>Diederik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jimmy</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName><surname>Ba</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1412.6980</idno>
		<title level="m">Adam: A method for stochastic optimization</title>
				<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">ISSEC: inferring contacts among protein secondary structure elements using deep object detection</title>
		<author>
			<persName><forename type="first">Qi</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jianwei</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fusong</forename><surname>Ju</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lupeng</forename><surname>Kong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shiwei</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wei-Mou</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dongbo</forename><surname>Bu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">BMC bioinformatics</title>
		<imprint>
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1" to="13" />
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Predicting protein contact map using evolutionary and physical constraints by integer programming</title>
		<author>
			<persName><forename type="first">Zhiyong</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jinbo</forename><surname>Xu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Bioinformatics</title>
		<imprint>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="issue">13</biblScope>
			<biblScope unit="page" from="266" to="273" />
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
