<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Multi-View Clustering</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Steffen</forename><surname>Bickel</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science Unter</orgName>
								<orgName type="institution">Humboldt-Universität zu Berlin</orgName>
								<address>
									<addrLine>Linden 6</addrLine>
									<postCode>10099</postCode>
									<settlement>Berlin</settlement>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Tobias</forename><surname>Scheffer</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science Unter</orgName>
								<orgName type="institution">Humboldt-Universität zu Berlin</orgName>
								<address>
									<addrLine>Linden 6</addrLine>
									<postCode>10099</postCode>
									<settlement>Berlin</settlement>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Multi-View Clustering</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">934AFFF8FF225647CA640786656728DF</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.3" ident="GROBID" when="2023-07-28T04:24+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>We consider clustering problems in which the available attributes can be split into two independent subsets, such that either subset suffices for learning. Example applications of this multi-view setting include clustering of web pages which have an intrinsic view (the pages themselves) and an extrinsic view (e.g., anchor texts of inbound hyperlinks); multi-view learning has so far been studied in the context of classification. We develop and study partitioning and agglomerative, hierarchical multi-view clustering algorithms for text data. We find empirically that the multiview versions of k-Means and EM greatly improve on their single-view counterparts. By contrast, we obtain negative results for agglomerative hierarchical multi-view clustering. Our analysis explains this surprising phenomenon.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>In some interesting application domains, instances are represented by attributes that can naturally be split into two subsets, either of which suffices for learning. A prominent example are web pages, which can be classified based on their content as well as based on the anchor texts of inbound hyperlinks; other examples include collections of research papers. If few labeled examples and, in addition, unlabeled data are available, then the co-training algorithm <ref type="bibr" target="#b3">[4]</ref> and other multi-view classification algorithms <ref type="bibr" target="#b14">[15,</ref><ref type="bibr" target="#b4">5]</ref> improve the classification accuracy often substantially.</p><p>Multi-view algorithms train two independent hypotheses which bootstrap by providing each other with labels for the unlabeled data. The training algorithms tend to maximize the agreement between the two independent hypotheses. Dasgupta et al. <ref type="bibr" target="#b6">[7]</ref> have shown that the disagreement between two independent hypotheses is an upper bound on the error rate of one hypothesis; this observation explains at least some of the often remarkable success of multiview learning. It also gives rise to the question whether the multi-view approach can be used to improve clustering algorithms.</p><p>Partitioning methods -such as k-Means, k-Medoids, and EM -and hierarchical, agglomerative methods <ref type="bibr" target="#b10">[11]</ref> are among the clustering approaches most frequently used in data mining. We study multi-view versions of these families of algorithms for document clustering.</p><p>The rest of this paper is organized as follows. We review related work in Section 2, the problem setting and evaluation issues in Section 3. We discuss partitioning multiview clustering algorithms in Section 4 and hierarchical algorithms in Section 5. Section 6 concludes.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Work</head><p>Research on multi-view learning in the semi-supervised setting has been introduced by two papers, Yarowsky <ref type="bibr" target="#b17">[18]</ref> and Blum and Mitchell <ref type="bibr" target="#b3">[4]</ref>. Yarowsky describes an algorithm for word sense disambiguation. It uses a classifier based on the local context of a word (view one) and a second classifier using the senses of other occurrences of that word in the same document (view two), where both classifiers iteratively bootstrap each other.</p><p>Blum and Mitchell introduce the term co-training as a general term for bootstrapping procedures in which two hypotheses are trained on distinct views. They describe a cotraining algorithm which augments the training set of two classifiers with the n p positive and n n negative highest confidence examples from the unlabeled data in each iteration for each view. The two classifiers work on different views and a new training example is exclusively based on the decision of one classifier.</p><p>Blum and Mitchell require a conditional independence assumption of the views and give an intuitive explanation on why their algorithm works, in terms of maximizing agreement on unlabeled data. They also state that the Yarowsky algorithm falls under the co-training setting. The co-EM algorithm <ref type="bibr" target="#b15">[16,</ref><ref type="bibr" target="#b9">10,</ref><ref type="bibr" target="#b4">5]</ref> is a multi-view version of the Expectation Maximization algorithm for semi-supervised learning.</p><p>Proceedings of the IEEE International Conference on Data Mining, 2004. <ref type="bibr" target="#b5">[6]</ref> suggest a modification of the cotraining algorithm which explicitly optimizes an objective function that measures the degree of agreement between the rules in different views. They also describe an extension to the AdaBoost algorithm that boosts this objective function.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Collins and Singer</head><p>Dasgupta et al. <ref type="bibr" target="#b6">[7]</ref> give PAC bounds for the generalization error of co-training in terms of the agreement rate of hypotheses in two independent views. This also justifies the Collins and Singer approach of directly optimizing the agreement rate of classifiers over the different views.</p><p>Clustering algorithms can be divided into two categories <ref type="bibr" target="#b2">[3]</ref>: generative (or model-based) approaches and discriminative (or similarity-based) approaches.</p><p>Model-based approaches attempt to learn generative models from the documents, with each model representing one cluster. Usually generative clustering approaches are based on the Expectation Maximization (EM) <ref type="bibr" target="#b7">[8]</ref> algorithm. The EM algorithm is an iterative statistical technique for maximum likelihood estimation in settings with incomplete data. Given a model of data generation, and data with some missing values, EM will locally maximize the likelihood of the model parameters and give estimates for the missing values.</p><p>Similarity-based clustering approaches optimize an objective function that involve the pairwise document similarities, aiming at maximizing the average similarities within clusters and minimize the average similarities between clusters. Most of the similarity based clustering algorithms follow the hierarchical agglomerative approach <ref type="bibr" target="#b10">[11]</ref>, where a dendrogram is build up by iteratively merging closest examples/clusters.</p><p>Related clustering algorithms that work in a multi-view setting include reinforcement clustering <ref type="bibr" target="#b16">[17]</ref> and a multiview version of DBSCAN <ref type="bibr" target="#b11">[12]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Problem Setting and Evaluation</head><p>We consider the problem that data is generated by a mixture model. Without knowing the true parameters of the mixture model, we want to estimate parameters of mixture components and thereby cluster the data into subsets so that with high probability two examples that are generated by the same mixture component get assigned to the same cluster, and examples generated by different components get assigned to different clusters. We consider the special case of a multi-view setting, where the available attributes V of examples are split into disjoint sets V (1) and V (2) . An instance x is decomposed and viewed as (x (1) , x (2) ), where x (1) and x (2) are vectors over the attributes V (1) and V (2) , respectively. These views have to satisfy the conditional independence assumption. Definition 1 Views V (1) and V (2) are conditionally independent given a mixture component y, when ∀x (1) ∈ V (1) , x (2) ∈ V (2) : p(x (1) , x (2) |y) = p(x (1) |y)p(x (2) |y).</p><p>To measure the quality of a clustering, we use the average entropy over all clusters (Equation <ref type="formula">1</ref>). It is based on the impurity of a cluster given the true mixture components of the data. p ij is the proportion of the mixture component j in cluster i. m i is the size of cluster i, k is the number of clusters, and m the total number of examples.</p><formula xml:id="formula_0">E = k i=1 m i - j p ij log(p ij ) m (1)</formula><p>In order to evaluate the clustering algorithms presented in the next sections we will use several data sets. One popular data set for evaluating multi-view classifiers is the We-bKB data set <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b14">15]</ref>.</p><p>Based on its content (V (1) ) as well as on the anchor texts of inbound links (V (2) ) a web page can be classified into six different types of university web pages (course, department, faculty, project, staff, student). We select all pages from the data set for which links with anchor text exist. This results in a data set with 2316 examples distributed over six classes having the two-views property. We generate tfidf-vectors without stemming.</p><p>For the WebKB data set the conditional independence assumption might be slightly violated. To construct an artificial data set that has the conditional independence property, we adapt an experimental setting of <ref type="bibr" target="#b15">[16]</ref>. We use 10 of the 20 classes of the well known 20 newsgroups data set. After building tfidf vectors, for each of the five classes, we generate examples by concatenating vectors x (1) from one group with randomly drawn vectors x (2)  from a second group to construct multi-view examples (x (1) , x (2) ). This procedure generates views which are perfectly independent (peers are selected randomly). The resulting classes are based on the following five pairs of the original 20 newsgroup classes: (comp.graphics, rec.autos), (rec.motorcycles, sci.med), (sci.space, misc.forsale), (rec.sport.hockey, soc.religion.christian), (comp.sys.ibm.pc.hardware, comp.os.ms-windows.misc).</p><p>We randomly select 200 examples for each of the 10 newsgroups, which results in 1000 concatenated examples uniformly distributed over the five classes.</p><p>In order to find out how our algorithms perform when there is no natural feature split in the data, we use document data sets and randomly split the available attributes into two subsets and average the performance over 10 distinct attribute splits. We choose six data sets that come with the CLUTO clustering toolkit: re0 (Reuters-21578), fbis (TREC-5), la1 (Los Angeles Times), hitech (San Jose Mercury), tr11 (TREC) and wap (WebACE project). For a detailed description of the data sets see <ref type="bibr" target="#b18">[19]</ref>.</p><p>For all experiments with partitioning clustering algorithms the diagrams are based on averaging over ten clustering runs to compensate for the randomized initialization. Error bars indicate standard errors.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Multi-View EM Clustering</head><p>In this section we want to analyze whether we can extend EM based cluster algorithms, so that they incorporate the multi-view setting with independent views. Different EM applications differ in specific models. We focus on models that are suitable for document clustering. Gaussian models could be used for multi-view EM as well, but are not applicable for document clustering. We firstly describe the general EM algorithm extended for two views, then we describe two instances of this algorithm and present and analyze empirical results.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">General Multi-View EM Algorithm</head><p>In the field of semi-supervised learning, co-EM based methods Positive results on the co-EM algorithm for the problem of semi-supervised learning <ref type="bibr" target="#b15">[16,</ref><ref type="bibr" target="#b4">5]</ref> lead to the question whether co-EM can improve on EM for unsupervised learning setting as well. The co-EM algorithm is shown in Table <ref type="table">1</ref>. In each iteration i, each view v finds the model parameters Θ (v) i which maximize the likelihood given the expected values for the hidden variables of the other view. In turns M, E steps in view one and M, E steps in view two are executed. The single expectation and maximization steps are equivalent to the the E and M steps of the original EM algorithm <ref type="bibr" target="#b7">[8]</ref>.</p><p>The algorithm is not guaranteed to converge. Our experiments show that the algorithm often does not converge. As displayed in Table <ref type="table">1</ref>, we do not run the algorithm until convergence but until a special stopping criterion is met.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Mixture of Multinomials EM Algorithm</head><p>We now instantiate the general multi-view EM definition of Table <ref type="table">1</ref> by a multi-view version of mixture-ofmultinomials EM. The mixture-of-multinomials model for document clustering is based on the idea that generating a document of length n from mixture component j can be modeled as a process in which n words are drawn at random from the dictionary. There is an individual probability for each word in the dictionary and words are drawn with replacement; hence, the number of occurrences of a specific word in the document is governed by a multinomial distribution. Since there is a distinct distribution of words in each mixture component, the resulting distribution which governs the document collection is a mixture of multinomials. Like all other tractable models, this model assumes in-</p><formula xml:id="formula_1">Table 1. Multi-View EM. Input: Unlabeled data D = {(x (1) 1 , x (2) 1 ), . . . , (x (1) n , x (2) n )}. 1. Initialize Θ (2) 0 , T , t = 0.</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">E step view 2: compute expectation for hidden variables</head><p>given the model parameters Θ</p><p>(2) 0</p><p>3. Do until stopping criterion is met:</p><formula xml:id="formula_2">(a) For v = 1 . . . 2: i. t = t + 1 ii. M step view v: Find model parameters Θ (v) t</formula><p>that maximize the likelihood for the data given the expected values for the hidden variables of view v of iteration t -1 iii. E step view v: compute expectation for hidden variables given the model parameters</p><formula xml:id="formula_3">Θ (v) t (b) End For v.</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">return combined Θ = Θ</head><p>(1)</p><formula xml:id="formula_4">t-1 ∪ Θ (2) t</formula><p>dependence of the word occurrences given the mixture components -any model that does not make this assumption has to deal with a number of covariances which is quadratic in the number of dictionary entries.</p><p>For the estimation of mixture-of-multinomials model parameters we use an expectation maximization approach. We adopt the definition of EM for mixture-of-multinomials from <ref type="bibr" target="#b19">[20]</ref>. The expectation step is shown in Equations 2 (likelihood) and 3 (posterior). The maximization step is shown in Equations 4 (word probabilities) and 5 (prior), where n</p><formula xml:id="formula_5">(v)</formula><p>il is the number of word w l 's occurrences in document</p><formula xml:id="formula_6">x (v) i in view v. Θ (v) denotes the combined set of parameters θ (v) j and α (v) ij . P (x (v) i |θ (v) j ) = l P (v) j (w l ) n (v) il (2) P (j|x (v) i , Θ (v) ) = α (v) ij P (x (v) i |θ (v) j ) j α (v) ij P (x (v) i |θ (v) j )<label>(3)</label></formula><formula xml:id="formula_7">P (v) j (w l ) = 1 + i P (j|x (v) i , Θ (v) )n (v) il l (1 + i P (j|x (v) i , Θ (v) )n (v) il )<label>(4)</label></formula><formula xml:id="formula_8">α (v) j = 1 m i P (j|x (v) i , Θ (v) )<label>(5)</label></formula><p>According to Table <ref type="table">1</ref>, running the mixture-of-multinomials EM as multi-view EM means running M-step and E-step in the respective view and interchanging the posteriors P (j|x</p><formula xml:id="formula_9">(v) i , Θ (v)</formula><p>). After each iteration we compute the loglikelihood of the data (Equation <ref type="formula">6</ref>) for each view. We terminate the optimization process, if the log-likelihood of the data did not reach a new maximum for a fixed number of iterations in each view.</p><formula xml:id="formula_10">log P (X (v) |Θ (v) ) = m i=1 log   k j=1 α (v) ij P (x (v) i |θ (v) j )   (6)</formula><p>The final assignment of examples to partitions π j , j = 1, . . . , k after termination is shown in Equation <ref type="formula">7</ref>. We assign an example to the cluster that has the largest averaged posterior over both views.</p><formula xml:id="formula_11">π j = {x i ∈ X : j = argmax j (P (j |x (1) i , Θ (1) ) + P (j |x (2) i , Θ (2) ))} (7)</formula><p>In our experiments we often encountered empty clusters for the mixture-of-multinomials EM. To prevent prior estimations of zero, we set the prior to a constant value α</p><formula xml:id="formula_12">(v) j = 1 k .</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Multi-View Spherical k-Means</head><p>A drawback of mixture-of-multinomials, described in the preceding section, is that documents with equal composition of words but with different word counts yield different posteriors P (j|x</p><formula xml:id="formula_13">(v) i , Θ (v)</formula><p>). We can overcome this problem by normalizing each document vector to unit length. A clustering algorithm that deals with this type of normalized document vectors is spherical k-Means <ref type="bibr" target="#b8">[9]</ref>, which is the regular k-Means algorithm with cosine similarity as distance (similarity) measure.</p><p>In order to describe the multi-view version of spherical k-Means, we simply need to describe the single expectation and maximization steps, the sequence of those steps in the multi-view setting again follows Table <ref type="table">1</ref>. The parameter Θ v consists of the concept vectors c (v) j ; j = 1, . . . , k; v = 1, 2; that have unit length c (v) j = 1. k is the desired number of clusters. All example vectors also have unit length x (v) i = 1. We start with randomly initialized concept vectors c</p><p>(2) j , j = 1, . . . , k. An expectation step assigns the documents that are closest to its concept vector c (v) j to the corresponding partition π (v) j (Equation <ref type="formula">8</ref>).</p><formula xml:id="formula_14">π (v) j = {x (v) i ∈ X (v) : x (v) i , c (v) j &gt; x (v) i , c (v) , = j} (8)</formula><p>A maximization step computes new concept vectors (model parameters) according to Equation <ref type="formula" target="#formula_15">9</ref>.</p><formula xml:id="formula_15">c (v) j = x (v) ∈π (v) j x (v) x (v) ∈π (v) j x (v)<label>(9)</label></formula><p>According to Table <ref type="table">1</ref>, after a maximization and an expectation step in one view, the partitions π (v) j get interchanged for a maximization and an expectation step in the other view, and so on.</p><p>After each iteration we compute the objective function for each view (Equation <ref type="formula">10</ref>). We terminate the optimization process, if the objective function did not reach a new minimum for a fixed number of iterations in each view.</p><formula xml:id="formula_16">k j=1 x (v) ∈π (v) j</formula><p>x (v) , c (v) j <ref type="bibr" target="#b9">(10)</ref> After termination, the corresponding cluster partitions π</p><p>(1) j and π</p><p>(2) j do not necessarily contain exactly the same examples. In order to obtain a combined clustering result we want to assign each example to one distinct cluster that is determined through the closest concept vector. In order to do this we compute a consensus mean for each cluster and view. Only those examples are included that both views agree on (Equation <ref type="formula">11</ref>).</p><formula xml:id="formula_17">m (v) j = x (1) i ∈π (1) j ∧x (2) i ∈π (2) j x (v) i x (1) i ∈π (1) j ∧x (2) i ∈π (2) j x (v) i (11)</formula><p>We assign each example to the final cluster that provides the most similar consensus vector, determined by averaging over the arcus cosine values in both views (Equation <ref type="formula" target="#formula_18">12</ref>).</p><formula xml:id="formula_18">π j = {x i ∈ X :<label>(12) arccos( m (1) j , x (1) i</label></formula><p>) + arccos( m</p><formula xml:id="formula_19">(2) j , x<label>(2) i</label></formula><p>) &lt; arccos( m (1) , x</p><p>(1) i</p><p>) + arccos( m (2) , x</p><p>(2) i</p><p>), j = }</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.">Empirical Results</head><p>The comparison of multi-view mixture-of-multinomials EM and spherical k-Means with their single-view counterparts for the WebKB data set is shown in Figure <ref type="figure" target="#fig_0">1</ref>. The number of clusters is set to k = 6. Figure <ref type="figure">2</ref> displays the same setting, but with different number of desired clusters k. We notice a tremendous improvement of cluster quality with the multi-view algorithms.</p><p>Figure <ref type="figure">3</ref> displays results for the artificial data set built from the 20 newsgroup data set, as described in Section 3. On this data set the multi-view algorithms improve the entropy even more than for the WebKB data set. The total independence property of the artificial data set seems to support the success of multi-view EM.</p><p>Figure <ref type="figure" target="#fig_3">4</ref> shows the results for the six document data sets without natural multi-view property, where we randomly split the available attribute sets into two subsets. In ten of twelve cases the multi-view outperform the single-view algorithms significantly.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5.">Analysis</head><p>We now want to investigate why the multi-view algorithms obtain such dramatic improvemtents in terms of cluster entropy over their single-view counterparts.</p><p>Dasgupta et al. <ref type="bibr" target="#b6">[7]</ref> and Abney <ref type="bibr" target="#b0">[1]</ref> have made the important observation that the disagreement between two independent hypotheses is an upper bound on the error risk of either hypothesis. Let us briefly sketch why this is indeed always the case. Consider a clustering problem with two mixture components; let x be an instance with true mixture component y, and let π (1) (x) and π (2) (x) be two independent clustering hypotheses which assign x to a cluster. Let furthermore both hypotheses π (1) and π (2) have a risk of assigning an instance to a wrong mixture component of at most 50% (otherwise the clustering hypothesis has only re-  In Equation <ref type="formula">13</ref>we distinguish between the two possible cases of disagreement (either hypothesis may be wrong), utilizing the independence assumption. In Equation <ref type="formula">14</ref>we exploit the assumed error rate of at most 50%: both hypotheses are less likely to be wrong than just one hypothesis. Exploiting the independence assumption again takes us to Equation <ref type="formula" target="#formula_20">15</ref>.</p><formula xml:id="formula_20">P (π (1) (x) = π (2) (x)) = P (π (1) (x) = y, π (2) (x) = ȳ) +P (π (1) (x) = ȳ, π (2) (x) = y) (13) ≥ max i P (π (i) (x) = ȳ, π ( ī) (x) = ȳ) +P (π (i) (x) = ȳ, π ( ī) (x) = y) (14) = max i P (π (i) (x) = y) (<label>15</label></formula><formula xml:id="formula_21">)</formula><p>In unsupervised learning, the risk of assigning instances to wrong mixture components cannot be minimized immediately. However, the above argument says that by minimizing the disagreement between two independent hypotheses, we can minimize an upper bound on the probability of an assignment of an instance to a wrong mixture component.</p><p>In order to find out whether the multi-view EM algorithm does in fact maximize the agreement between the views, we determine the agreement rate of the mixture-ofmultinomials multi-view EM as shown in Equation <ref type="formula" target="#formula_22">16</ref>. It is the number of examples the views agree on the assignment to components, divided by the total number of examples m.</p><formula xml:id="formula_22"># argmax j P (j|x (1) i , Θ (1) ) = argmax j P (j |x (1) i , Θ (2) ) m (<label>16</label></formula><formula xml:id="formula_23">)</formula><p>For each iteration step the entropy and the corresponding agreement rate are shown in Figure <ref type="figure" target="#fig_4">5</ref>. With increasing entropy the agreement of the views increases as well. This means our algorithm optimizes an objective function where the agreement rate is part of the optimization criterion. Additionally we want to analyze the relationship between the results of multi-view and single-view EM regarding the single-view objective function. We run the mixture-of-multinomials multi-view EM until termination, concatenate the resulting word probability vectors P (v) j (w l ) and use the resulting vector as initialization for a singleview clustering run in the concatenated space. Figure <ref type="figure" target="#fig_6">6</ref> shows the log-likelihood and entropy of the multi-view run, the following single-view run and the single-view run with regular random initialization. We calculate the loglikelihood of the multi-view algorithm by computing the regular single-view log-likelihood but replacing P (x i |θ j ) with P (x</p><formula xml:id="formula_24">(1) i |θ j )P (x<label>(2)</label></formula><p>i |θ j ) (Equation <ref type="formula">17</ref>).</p><formula xml:id="formula_25">log(P (X|Θ)) = n i=1 log k j=1 αijP (x (1) i |θj)P (x (2) i |θj) (17)</formula><p>The single-view algorithm yields a higher likelihoodwhich is not surprising because only the single-view algorithm directly optimizes the likelihood. Surprisingly, however, we observe an even greater log-likelihood at the end of the single-view run initialized with the multi-view result. This means we find better local optima by running co-EM and transferring the resulting model-parameter into the single-view setting compared to running single-view EM with random initialization. We notice that the single-view clustering which follows multi-view clustering does not affect the entropy -hence, running a single-view algorithm after a multi-view algorithm is not beneficial.  In Section 4.1 we mentioned that the multi-view EM algorithm is not guaranteed to converge. We consider the following simple example. We assume that we are clustering with the multi-view k-Means algorithm with k = 2. There is one specific example whose attribute vector in view one equals the concept vector of component one and in view two equals the concept vector of component two.</p><p>By running the multi-view k-Means algorithm this example will be in turns assigned to component one and component two. The algorithm will not converge because the assignment of the specific example alternates.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Multi-View Agglomerative Clustering</head><p>Agglomerative clustering algorithms are based on iteratively merging nearest clusters. A natural extension of this procedure for the multi-view setting is splitting up the iterative merging procedure so that one iteration executes one merging step in one view and the next iteration step in the other view and so on. We want to find out if this approach has advantages compared to clustering with a single view. We will now describe this algorithm, present empirical results and analyze its behavior.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.">Algorithm</head><p>The general idea of our agglomerative multi-view clustering is inspired by the co-training algorithm <ref type="bibr" target="#b3">[4]</ref>. Cotraining greedily augments the training set with the n p positive and n n negative highest confidence examples from the unlabeled data in each iteration for each view. Each new training example is exclusively based on the decision of one classifier in its view.</p><p>Agglomerative clustering is based on a distance measure between clusters. In the multi-view setting we have two attribute sets V (1) and V (2) and two distance measures d (1) (C i , C j ) and d (2) (C i , C j ). Agglomerative clustering starts with each example having its own cluster. Then iteratively merging the closest clusters builds up a dendrogram. The multi-view agglomerative clustering is similar but merges in turns the closest clusters in view one and view two. All merging operations work on a combined dendrogram for both views, this results in one final dendrogram. The algorithm is shown in Table <ref type="table" target="#tab_0">2</ref>.</p><p>By following this procedure, we assume that with low dependence between the views we get a better quality clustering than on the concatenated views. If a cluster pair has a low distance in one view but a medium distance in the other view, then our algorithm would probably merge this pair in an earlier iteration than the clustering on concatenated views would do. The clustering dendrogram gets built up on high confidence decisions made in the separate views and one view might benefit from </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.">Empirical Results</head><p>We compare the cluster quality of multi-view agglomerative clustering with the regular single-view clustering on the concatenated views. Figure <ref type="figure" target="#fig_7">7</ref> shows the entropy values for different values of k (number of clusters) by starting from the root node and expanding the first k -1 clusters in reversed order as they were merged.</p><p>We notice that the multi-view agglomerative clustering does not achieve lower entropy values compared to the corresponding single-view clustering. In some cases, especially for the WebKB data set with average-linkage, the entropy of single-view clustering is much lower.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3.">Analysis</head><p>The question is, why does agglomerative multi-view clustering deteriorate cluster quality in most cases, even if our views are perfectly independent, as with the artificial data set?</p><p>If we assume that our data is actually generated by a set of mixture components, the the goal in agglomerative clus-tering is to avoid merges of instances that belong to different mixture components. We want to analyze the risk for those cross-component merges for agglomerative multi-view and single-view clustering.</p><p>We use cosine similarity in our agglomerative clustering, so we are clustering in a space of directional data. We assume a von Mises-Fisher (vMF) distribution for the real distribution of our mixture components ( <ref type="bibr" target="#b1">[2]</ref>). The von Mises-Fisher distribution, for directional data, is the analog to the Gaussian distribution for Euclidean data in the sense that it is the unique distribution of L 2 -normalized data that maximizes the entropy given the first and second moments of the distribution ( <ref type="bibr" target="#b13">[14]</ref>).</p><p>According to this assumption, the vMF probability density of one component in one view is shown in Equation <ref type="formula" target="#formula_26">18</ref>. µ (v) is the mean vector, κ (v) the variance of the distribution and c(κ (v) ) a normalization term.</p><formula xml:id="formula_26">f (x (v) |µ (v) , κ (v) ) = c(κ (v) )e cos(x (v) µ (v) ) κ (v) = c(κ (v) )e µ (v) ,x (v) κ (v) µ (v) x (v)<label>(18)</label></formula><p>Before we proceed, we show that the cosine similarity between two example vectors x and y in the concatenated space can be written as the average over the cosine similarities of the subspace vectors (Equation <ref type="formula" target="#formula_28">20</ref>), if we assume that x (1) = x (2) and y (1) = y (2) .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>cos(x y) =</head><p>x (1)  x (2)   x (1) 2 + x (2) 2 y (1) y (2)   y (1) 2 + y (2) 2</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>=</head><p>x (1) , y (1) + x (2) y (2)  2 x (1) • y (1) )</p><p>= cos(x (1) y (1) ) + cos(x (2) y (2) ) 2</p><p>If the two views are independent we can write the probability density of the concatenated views as a product of the single densities as shown in Equation <ref type="formula" target="#formula_29">21</ref>. f (x|µ, κ) = f (x (1) |µ (1) , κ (1) )f (x (2) |µ (2) , κ (2) )</p><p>= c(κ (1) )e µ (1) ,x (1) κ (1) µ (1) • x (1) c(κ (2) )e µ (2) ,x (2)</p><formula xml:id="formula_30">κ (2) µ (2) • x (2)</formula><p>For reasons of a simplified presentation we assume that κ (1) = κ (2) , x (1) = x (2) and µ (1) = µ (2) . With this assumption we get Equation 22 and applying Equation 19 leads to Equation <ref type="formula" target="#formula_32">23</ref>. </p><p>= c 2 (κ (1) )e cos(x µ)</p><formula xml:id="formula_32">1 2 κ (1)<label>(23)</label></formula><p>We see that the resulting distribution is again vMF distributed with variance κ = 1 2 κ (1) . If we consider the case with only two mixture components, the distribution densities in the concatenated space have a smaller overlap, because the the variance is halved compared to the separate views. We might assume that the similarity of the means of the components in the concatenated views are doubled (as the variance is halved), but this is not the case according to Equation <ref type="formula" target="#formula_28">20</ref>. The similarity of the means in the concatenated space is just the averaged similarity over the mean similarities in the separate views. If distributions have a smaller overlap, then the probability for cross-component merges is smaller.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Conclusion</head><p>We presented the problem setting of clustering in a multi-view environment and described two algorithm types that work in this setting in terms of incorporating the conditional independence property of the views.</p><p>The EM-based multi-view algorithms significantly outperform the single-view counterparts for several data sets. Even when no natural feature split is available, and we randomly split the available features into two subsets, we gain significantly better results than the single-view variants in almost all cases.</p><p>In our analysis we discovered that the multi-view EM algorithm optimizes agreement between the views. Because the disagreement is an upper bound on the error rate of one view, the good performance of multi-view EM can be explained through this property.</p><p>The agglomerative multi-view algorithm yields equal or worse results than the single-view version in most cases. We identified that the reason for this behavior is that the mixture components have a smaller overlap when the views are concatenated. This means in the single-view setting the probability for cross-component merges is lower, which directly improves cluster quality.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 .</head><label>1</label><figDesc>Figure 1. Single and multi-view mixture-ofmultinomials EM (left) and spherical k-Means (right) for the WebKB data set.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 .Figure 3 .</head><label>23</label><figDesc>Figure 2. Single and multi-view mixture-ofmultinomials EM (left) and spherical k-Means (right) for the WebKB data set and different k.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 4 .</head><label>4</label><figDesc>Figure 4. Single and multi-view clustering for six document data sets with random feature splits.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 5 .</head><label>5</label><figDesc>Figure 5. Entropy and agreement rate.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 6 .</head><label>6</label><figDesc>Figure 6. Log-likelihoods (left) and entropy (right) of multi-view, continued single-view and regular single-view EM (left).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure 7 .</head><label>7</label><figDesc>Figure 7. Single and multi-view agglomerative clustering for WebKB (left) and artificial data set (right).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>f</head><label></label><figDesc>(x|µ, κ) = c 2 (κ (1) )e µ (1) ,x (1) + µ (2) ,x (1) κ (1) µ (1) • x (1)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 2 .</head><label>2</label><figDesc>Multi-view agglomerative clustering. )}, distance measures d 1 (Ci, Cj) and d 2 (Ci, Cj).1. Initialize Ci = xi, i = 1 . . . n.2. For t = 1 . . . n: For v = 1 . . . 2: confidence decision made in the other view. As distance measure d(C i , C j ), we use cosine similarity with d min (C i , C j ) (single-linkage), d max (C i , C j ) (completelinkage) and d avg (C i , C j ) (average-linkage) according to<ref type="bibr" target="#b12">[13]</ref>.</figDesc><table><row><cell>Input: Unlabeled data D = {(x</cell><cell>(1) 1 , x (2) 1 ), . . . , (x (1) n , x</cell><cell>(2)</cell></row><row><cell cols="3">(a) Find pair of closest clusters (C i , C j ) = argmin d v (Ci, Cj), for i, j = 1 . . . (n -t + 1).</cell></row><row><cell>(C i ,C j )</cell><cell></cell><cell></cell></row><row><cell>(b) Merge C i and C j .</cell><cell></cell><cell></cell></row><row><cell>3. Return dendrogram.</cell><cell></cell><cell></cell></row></table><note><p>n high</p></note></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgment</head><p>This work has been supported by the German Science Foundation DFG under grant SCHE540/10-1.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<author>
			<persName><forename type="first">S</forename><surname>Abney</surname></persName>
		</author>
		<author>
			<persName><surname>Bootstrapping</surname></persName>
		</author>
		<title level="m">Proc. of the 40th Annual Meeting of the Association for Comp. Linguistics</title>
		<meeting>of the 40th Annual Meeting of the Association for Comp. Linguistics</meeting>
		<imprint>
			<date type="published" when="2002">2002</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">A comparative study of generative models for document clustering</title>
		<author>
			<persName><forename type="first">A</forename><surname>Banerjee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Dhillon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Ghosh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Sra</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of The Ninth ACM SIGKDD Conference on Knowledge Discovery and Data Mining</title>
		<meeting>The Ninth ACM SIGKDD Conference on Knowledge Discovery and Data Mining</meeting>
		<imprint>
			<date type="published" when="2003">2003</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">Survey of clustering data mining techniques</title>
		<author>
			<persName><forename type="first">P</forename><surname>Berkhin</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2002">2002</date>
		</imprint>
	</monogr>
	<note>Unpublished manuscript, available from accrue.com</note>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Combining labeled and unlabeled data with co-training</title>
		<author>
			<persName><forename type="first">A</forename><surname>Blum</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Mitchell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Conference on Computational Learning Theory</title>
		<meeting>the Conference on Computational Learning Theory</meeting>
		<imprint>
			<date type="published" when="1998">1998</date>
			<biblScope unit="page" from="92" to="100" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Co-EM support vector learning</title>
		<author>
			<persName><forename type="first">U</forename><surname>Brefeld</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Scheffer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of the Int. Conf. on Machine Learning</title>
		<meeting>of the Int. Conf. on Machine Learning</meeting>
		<imprint>
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Unsupervised models for named entity classification</title>
		<author>
			<persName><forename type="first">M</forename><surname>Collins</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Singer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP</title>
		<imprint>
			<date type="published" when="1999">1999</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">PAC generalization bounds for co-training</title>
		<author>
			<persName><forename type="first">S</forename><surname>Dasgupta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Littman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Mcallester</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of Neural Information Processing Systems (NIPS)</title>
		<meeting>Neural Information Processing Systems (NIPS)</meeting>
		<imprint>
			<date type="published" when="2001">2001</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Maximum likelihood from incomplete data via the EM algorithm</title>
		<author>
			<persName><forename type="first">A</forename><surname>Dempster</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Laird</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Rubin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of the Royal Statistical Society, Series B</title>
		<imprint>
			<biblScope unit="volume">39</biblScope>
			<date type="published" when="1977">1977</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Concept decompositions for large sparse text data using clustering</title>
		<author>
			<persName><forename type="first">I</forename><forename type="middle">S</forename><surname>Dhillon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">S</forename><surname>Modha</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Machine Learning</title>
		<imprint>
			<biblScope unit="page" from="143" to="175" />
			<date type="published" when="2001">2001</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Combining labeled and unlabeled data for multiclass text categorization</title>
		<author>
			<persName><forename type="first">R</forename><surname>Ghani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Conference on Machine Learning</title>
		<meeting>the International Conference on Machine Learning</meeting>
		<imprint>
			<date type="published" when="2002">2002</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Hierarchical agglomerative clustering methods for automatic document classification</title>
		<author>
			<persName><forename type="first">A</forename><surname>Griffiths</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Robinson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Willett</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Doc</title>
		<imprint>
			<biblScope unit="volume">40</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="175" to="205" />
			<date type="published" when="1984">1984</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Clustering multi-represented objects with noise</title>
		<author>
			<persName><forename type="first">K</forename><surname>Kailing</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Kriegel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Pryakhin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Schubert</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of the Pacific-Asia Conf. on Knowl. Disc. and Data Mining</title>
		<meeting>of the Pacific-Asia Conf. on Knowl. Disc. and Data Mining</meeting>
		<imprint>
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">A general theory of classificatory sorting strategies. i. hierarchical systems</title>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">N</forename><surname>Lance</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">T</forename><surname>Williams</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computer Journal</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="page" from="373" to="380" />
			<date type="published" when="1966">1966</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Statistics of directional data</title>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">V</forename><surname>Mardia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of the Royal Statistical Society, Series B</title>
		<imprint>
			<biblScope unit="volume">37</biblScope>
			<biblScope unit="page" from="349" to="393" />
			<date type="published" when="1975">1975</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Active + semisupervised learning = robust multi-view learning</title>
		<author>
			<persName><forename type="first">I</forename><surname>Muslea</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Kloblock</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Minton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Conference on Machine Learning</title>
		<meeting>the International Conference on Machine Learning</meeting>
		<imprint>
			<date type="published" when="2002">2002</date>
			<biblScope unit="page" from="435" to="442" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Analyzing the effectiveness and applicability of co-training</title>
		<author>
			<persName><forename type="first">K</forename><surname>Nigam</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Ghani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of Information and Knowledge Management</title>
		<meeting>Information and Knowledge Management</meeting>
		<imprint>
			<date type="published" when="2000">2000</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Recom: Reinforcement clustering of multi-type interrelated data objects</title>
		<author>
			<persName><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Tao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Ma</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the ACM SIGIR Conference on Information Retrieval</title>
		<meeting>the ACM SIGIR Conference on Information Retrieval</meeting>
		<imprint>
			<date type="published" when="2003">2003</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Unsupervised word sense disambiguation rivaling supervised methods</title>
		<author>
			<persName><forename type="first">D</forename><surname>Yarowsky</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of the 33rd Annual Meeting of the Association for Comp. Linguistics</title>
		<meeting>of the 33rd Annual Meeting of the Association for Comp. Linguistics</meeting>
		<imprint>
			<date type="published" when="1995">1995</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">Criterion functions for document clustering: Experiments and analysis</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Karypis</surname></persName>
		</author>
		<idno>TR 01-40</idno>
		<imprint>
			<date type="published" when="2001">2001. 2001</date>
			<pubPlace>Minneapolis, MN</pubPlace>
		</imprint>
		<respStmt>
			<orgName>Department of Computer Science, University of Minnesota</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">Technical Report</note>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Generative model-based clustering of directional data</title>
		<author>
			<persName><forename type="first">S</forename><surname>Zhong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Ghosh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SDM Workshop on Clustering High-Dimensional Data and Its Applications</title>
		<imprint>
			<date type="published" when="2003">2003</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
