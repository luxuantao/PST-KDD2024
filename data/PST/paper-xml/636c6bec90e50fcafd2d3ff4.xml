<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">EFFICIENTLY SCALING TRANSFORMER INFERENCE</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date type="published" when="2022-11-09">9 Nov 2022</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Reiner</forename><surname>Pope</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Sholto</forename><surname>Douglas</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Aakanksha</forename><surname>Chowdhery</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Jacob</forename><surname>Devlin</surname></persName>
						</author>
						<author>
							<persName><forename type="first">James</forename><surname>Bradbury</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Anselm</forename><surname>Levskaya</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Jonathan</forename><surname>Heek</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Kefan</forename><surname>Xiao</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Shivani</forename><surname>Agrawal</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Jeff</forename><surname>Dean</surname></persName>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="department">Efficiently Scaling Transformer Inference</orgName>
								<address>
									<addrLine>2 4 8 16 32 64 128 256</addrLine>
									<postCode>512</postCode>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff1">
								<orgName type="department">Latency per Generated Token (Milliseconds)</orgName>
								<address>
									<addrLine>125 0.25 0.5 1 2 4 8 16</addrLine>
									<postCode>0625 0, 32 64 128</postCode>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">EFFICIENTLY SCALING TRANSFORMER INFERENCE</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2022-11-09">9 Nov 2022</date>
						</imprint>
					</monogr>
					<idno type="arXiv">arXiv:2211.05102v1[cs.LG]</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-01-03T08:28+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>We study the problem of efficient generative inference for Transformer models, in one of its most challenging settings: large deep models, with tight latency targets and long sequence lengths. Better understanding of the engineering tradeoffs for inference for large Transformer-based models is important as use cases of these models are growing rapidly throughout application areas. We develop a simple analytical model for inference efficiency to select the best multi-dimensional partitioning techniques optimized for TPU v4 slices based on the application requirements. We combine these with a suite of low-level optimizations to achieve a new Pareto frontier on the latency and model FLOPS utilization (MFU) tradeoffs on 500B+ parameter models that outperforms the FasterTransformer suite of benchmarks. We further show that with appropriate partitioning, the lower memory requirements of multiquery attention (i.e. multiple query heads share single key/value head) enables scaling up to 32? larger context lengths. Finally, we achieve a low-batch-size latency of 29ms per token during generation (using int8 weight quantization) and a 76% MFU during large-batch-size processing of input tokens, while supporting a long 2048-token context length on the PaLM 540B parameter model.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">INTRODUCTION</head><p>Scaling Transformer-based models to 100B+ <ref type="bibr" target="#b5">(Brown et al., 2020;</ref><ref type="bibr" target="#b20">Kaplan et al., 2020;</ref><ref type="bibr">Rae et al., 2021;</ref><ref type="bibr" target="#b18">Hoffmann et al., 2022)</ref> and later 500B+ parameters <ref type="bibr" target="#b9">(Chowdhery et al., 2022;</ref><ref type="bibr" target="#b36">Smith et al., 2022)</ref> has led to state of the art results on natural language processing benchmarks. The practical utility of these large language models (LLMs) in a variety of applications makes them compelling for widespread use. While the sequence parallelism of the Transformer architecture enables highly parallel training, efficient deployment of these models is challenging in practice because generative inference proceeds one token at a time and the computation for each token sequentially depends on the previously generated tokens. Thus, models that support efficient training at scales of thousands of chips require careful attention to parallel layout and memory optimizations to unlock the scalability needed for efficient, low-latency inference. This paper focuses on a simple set of engineering principles that enable serving large-scale Transformer-based models efficiently in a variety of challenging production settings.</p><p>We consider the requirements of downstream applications for LLMs. Some applications, including interactive workloads like chatbots, involve tight latency constraints <ref type="bibr" target="#b39">(Thoppilan et al., 2022)</ref>. Others, including offline inference for 1 Google.</p><p>Correspondence to: Sholto Douglas &lt;sholto@google.com&gt;, Aakanksha Chowdhery &lt;chowdhery@google.com&gt;. scoring or distillation, emphasize high throughput and low cost per token at any latency.</p><p>We discuss briefly what makes generative inference of LLMs challenging. First, large models have a large memory footprint both due to the trained model parameters as well as the transient state needed during decoding. The model parameters generally do not fit in the memory of a single accelerator chip. The attention key and value tensors of each layer, which we refer to as the KV cache, must also be stored in memory for the duration of decoding. Second, tight latency targets become especially challenging for generative inference given the much lower parallelizability of Transformer generation relative to training. The large memory footprint gives rise to a large amount of memory traffic to load the parameters and KV cache from high-bandwidth memory (HBM) into the compute cores for each step, and hence a large total memory bandwidth required to meet a given latency target. Finally, inference cost from the attention mechanism scales quadratically with input sequence length <ref type="bibr" target="#b37">(Sukhbaatar et al., 2019;</ref><ref type="bibr" target="#b8">Choromanski et al., 2020;</ref><ref type="bibr" target="#b11">Dao et al., 2022)</ref>.</p><p>We found two keys to optimize LLMs for inference efficiency. First, we found it useful to build a powerful and abstract partitioning framework to enable reaching the limits of model parallel scaling given the limited parallelizability of Transformer inference. Within this framework, we analytically solve for the best partitioning strategy for a given model size with specific application requirements. This enables the user to intuitively understand the tradeoffs and  <ref type="table" target="#tab_8">2</ref> and<ref type="table" target="#tab_5">3</ref> show details on a few specific scenarios from the Pareto frontier where the applications have low-latency or high-throughput requirements.</p><p>select the best multi-axis tensor partitioning strategy, batch size and chip configuration for their application, in contrast to a black-box exhaustive search over partitioning strategies <ref type="bibr" target="#b47">(Zheng et al., 2022;</ref><ref type="bibr" target="#b44">Xu et al., 2021)</ref>. To fully realize the performance in practice, we use additional fine-grained control over cross-chip collective operations and low-level scheduling optimizations. Second, we apply memory optimizations and take full advantage of PaLM's multiquery attention to reduce unnecessary tensor overheads and maximize the batch size that fits on a given number of chips, enabling higher throughput.</p><p>The primary goal of this paper is to provide a set of engineering principles for how best to partition a model in order to scale Transformer inference. In other words, how is the performance of different partitioning strategies affected by changes in model size, sequence length, and number of hardware chips? How does the optimal partitioning strategy change when trading off between latency and throughput? What is the intuitive and mathematical reasoning behind these effects? As we show in later sections, the right tradeoffs and strategies change as model size, sequence length, and application requirements for latency and throughput targets change, so having a framework that enables easy expression of different strategies and choices is important.</p><p>In Section 2, we describe the specific metrics and tradeoffs we use to compare different partitioning strategies. In Section 3.1, we provide an overview of partitioning principles for large language models. In the remainder of Section 3, we describe a number of specific partitioning strategies, with an empirical validation on the PaLM family of large language models in Section 4.</p><p>For a state-of-the-art 540B parameter dense model running on 64 TPU v4 chips, we achieve a low-batch-size latency of 29ms per token during generation (with int8 weight quantization) and a 76% MFU during large-batch-size processing of input tokens while supporting a large context length of 2048 tokens. Figure <ref type="figure" target="#fig_0">1</ref>(left) shows our performance for generating text using the PaLM models. For an interactive application such as a chatbot running on PaLM 540B with int8 weights, our implementation on 64 TPU v4 chips can process 64 tokens of text from a user, consult a cached conversation history of 1920 tokens, and generate a 64-token response in a total of 1.9 seconds. For an offline throughputoriented application, our implementation can process 1984 tokens of input and generate 64 tokens of output, for huge numbers of examples, with an overall FLOPS efficiency of 73%. Table <ref type="table" target="#tab_4">2</ref> shows more details on a few specific scenarios.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">INFERENCE COST TRADEOFFS</head><p>Scaling up model sizes can unlock new capabilities and applications but has fundamental tradeoffs in terms of inference cost. We measure the inference cost in terms of the following metrics: latency, throughput, and model FLOPS utilization. The latency is the total time for an inference and can be broken down into the time to process the input tokens present at the start of the inference (which we call "prefill") and the time to autoregressively generate output tokens (which we term "decode"). The decode latency can also be measured "per step", i.e. divided by the number of tokens in each sequence. The throughput of prefill or decode is the number of tokens processed or generated per second. The model FLOPS utilization (MFU) is the ratio of the observed throughput to the theoretical maximum throughput if the benchmarked hardware setup were operating at peak FLOPS with no memory or communication overhead.</p><p>Larger models do not fit on a single accelerator chip and need to be partitioned across many accelerator chips to fit in memory. This also enables us to divide the memory and compute costs described below over all the chips, but comes at the cost of introducing chip-to-chip communication.</p><p>Memory costs. We store tensors such as weights and the KV cache in on-device high-bandwidth memory (HBM). While there are other tensors that pass through the HBM, their memory footprint is much smaller, so we focus on just these two largest groups of tensors. These tensors need to be transferred from HBM to the compute cores of the chip once per forward pass (prefill or decode step) of the model. This takes a certain amount of time, which we call the "memory time." At small batch sizes and sequence lengths, the time to load weights dominates. At larger batch sizes and sequence lengths (e.g. 2048+ tokens with batch size 512+), the time to load the KV cache dominates.</p><p>Compute costs. An N -parameter decoder-only model requires 2N matmul FLOPs in the forward pass per token seen because each matmul performs one multiplication and one addition per pair of input token and parameter values in the forward pass <ref type="bibr" target="#b20">(Kaplan et al., 2020)</ref>. If all chips were running at peak FLOPS, these matmuls would take a certain amount of time, which we call the "compute time." The matmuls in the attention mechanism typically add a much smaller number of FLOPs per token for large models and can often be excluded. Even though the computational cost of attention is relatively small, it can still account for a significant fraction of memory capacity and bandwidth costs, since (unlike the weights) the KV cache is unique for each sequence in the batch.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Expected tradeoffs and challenges</head><p>Both the weight loading part of the memory time and the non-attention compute time are proportional to the model size and inversely proportional to the number of chips. However, for a given partitioning layout, the time needed for chip-to-chip communication decreases less quickly (or not at all) with the number of chips used, so it becomes an increasingly important bottleneck as the chip count grows. We consider some scenarios where these tradeoffs become especially challenging.</p><p>If an application requires the lowest possible latency, we need to apply more chips and partition the model in as many ways as we profitably can. Lower latency can often be achieved with smaller batch sizes, but smaller batch sizes also result in worse MFU, resulting in a higher total cost (in terms of chip-seconds or dollars) per token.</p><p>If an application requires generating text with long attention contexts, it substantially increases the inference time. For a 500B+ model with multihead attention, the attention KV cache grows large: for batch size 512 and context length 2048, the KV cache totals 3TB, which is 3 times the size of the model's parameters. The on-chip memory needs to load this KV cache from off-chip memory once for every token generated during which the computational core of the chip is essentially idle.</p><p>If an applications requires offline inference and latency is not a concern, the primary goal is to maximize per-chip throughput (i.e., minimize total cost per token). It is most efficient to increase the batch size because larger batches typically result in better MFU, but certain partitioning strategies that are not efficient for small batch sizes become efficient as the batch size grows larger.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Inference Setup</head><p>We briefly introduce the inference setup and notation. We consider a Transformer model with n params parameters laid out for inference on n chips chips. The model has model (or embed) dimension d model (or E), feedforward intermediate dimension d ff (or F ), and n heads (or H) heads.</p><p>Each example in a batch of B sequences has L input tokens of input text, and generates L gen tokens of output text. Since the input tokens are all present at the start of the inference, we can run the model over all B ? L input many tokens in parallel, in a single forwards pass over all the tokens. We call this step prefill. The output tokens are generated autoregressively, with a sequential loop of L gen steps. Each step consists of a single forwards pass through the model, after which we sample one new token for each of the B examples in the batch. This loop is known as generation or decode.</p><p>Since prefill can run in parallel over L input , but decode must run sequentially over L gen , the two phases have different performance characteristics and we analyze them separately.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">PARTITIONING FOR INFERENCE EFFICIENCY</head><p>We must partition large models over many chips in order to fit weight and activation tensors in memory and fit compute and memory time within latency requirements. Model partitioning introduces communication between chips, and different partitioning strategies for a given model involve different patterns and amounts of communication. In this section, we detail several high-level strategies for partitioning a large Transformer language model for cost-effective and latency-effective inference.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Partitioning notation and communication collectives</head><p>We describe the partitioning layouts in this section based on a TPU v4 system with 3D torus topology X ? Y ? Z.</p><p>Following <ref type="bibr" target="#b44">(Xu et al., 2021)</ref>, we use subscripts to specify the tensor dimension that is partitioned. For example, notation BLE xyz means that the last dimension E of a tensor of logical shape BLE is split into X ? Y ? Z partitions, where x, y and z refer to the physical TPU v4 axes, and the per-chip tensor is of shape</p><formula xml:id="formula_0">[B, L, E/(X ? Y ? Z)].</formula><p>Here B, E and F refers to batch, model embed and MLP feedforward dimension. We use L to refer to the sequence length and explicitly specify prefill or generation phase.</p><p>If a tensor is replicated over an axis x, that axis is omitted from the notation. We also use a suffix "partialsum-x" to indicate that a given tensor has been contracted (summed) locally on each chip (over some axis not represented in the shape), but still needs to be summed across the chips in the TPU x axis (creating a tensor replicated over x) before the result is meaningful.</p><p>We use several communication collectives originating from MPI <ref type="bibr" target="#b10">(Clarke et al., 1994)</ref>. The all-reduce(x) primitive sums a partialsum tensor such as BLE yz (partialsum-x) across sets of chips in the x axis of the torus and broadcasts the sum back to all the involved chips, returning output of shape BLE yz . For the reasons outlined in <ref type="bibr" target="#b29">Rajbhandari et al. (2020)</ref>, we typically split all-reduce into two phases: a reduction phase and a broadcast phase. The reduction phase is called reduce-scatter(x), and it sums BLE yz (partialsum-x) tensors across sets of chips in the x axis but produces an output that's sharded rather than replicated over the chips in that axis, in a layout such as B x LE yz or BLE xyz . The broadcast phase is called all-gather(x), and it broadcasts and concatenates the tensor BLE xyz to all chips in the x axis, producing an output X times larger than its input, replicated over the x axis: BT E yz . The all-to-all collective shifts sharding from one tensor dimension to another, e.g. BLH x Q ? B x LHQ by using direct communication between every (source, destination) pair. Figure A.1 illustrates these primitives.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Partitioning the feedforward layer</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.1">Feedforward layer, 1D weight-stationary layout</head><p>Overview. When a model doesn't fit on a single chip, the simplest partitioning strategy is 1D weight-stationary, where each E ? F weight matrix is partitioned (or sharded) among n chips along the E or F axis. Each weight shard is multiplied by the appropriate activation shard on each chip, and the results are aggregated between the chips with an allgather and/or reduce-scatter. Additionally, when computing two consecutive matrix multiplications (as in a Transformer MLP block), there is a "trick" <ref type="bibr" target="#b35">(Shoeybi et al., 2019)</ref> to avoid any cross-chip communication between the matmuls: if the first matmul is partitioned by the output axis, the resulting activation shard on each chip will be the exact one needed to compute the second matmul partitioned by the input axis.</p><p>As we parallelize the computation across more chips, the memory latency and compute latency does decrease, often near-linearly. However, the communication latency remains roughly constant independent of the number of chips used, since the entire activation matrix is aggregated across chips for every pair of matrix multiplications. As the number of chips grows larger, communication becomes a bottleneck.</p><p>Details. We consider as a baseline the layout where the weights and activations of the feedforward layer are partitioned over n chips along the d ff dimension, as in Megatron <ref type="bibr" target="#b35">(Shoeybi et al., 2019)</ref>. Figure <ref type="figure" target="#fig_1">2</ref>(a) shows the partitioning layout for this case. On the TPU v4's 3D torus topology the partition layout for weights is EF xyz and F xyz E, i.e. they are partitioned in to X ? Y ? Z = n chips partitions with X, Y , and Z partitions across physical TPU axes. The weights are kept stationary in each chip, and the activations are transferred between chips to match the weight layout, requiring one all-gather and one reduce-scatter.</p><p>In this 1D weight-stationary partitioning strategy, each chip gets inputs and outputs of shape BLE in the reduce-scatter and all-gather respectively. We derive the the communication cost of these operations in Appendix A.1. The resulting communication time is</p><formula xml:id="formula_1">T comm = 2BLE network bandwidth .</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.2">Feedforward layer, 2D weight-stationary layout</head><p>Overview. For a larger number of chips, a more economical strategy involves partitioning each E ? F weight matrix along both the E and F axes, such that each shard is roughly square. For example, if E = 1024, F = 4096, and n chips = 64, then we would shard 4-ways among E and 16-ways among F , so that each of the 64 chips stores a 256-by-256 chunk of the weight matrix, and activations are transferred between chips. This is called 2D weightstationary. The total compute cost is the same as 1D weightstationary, but communication is much more efficient: when multiplying an activation matrix through a set of consecutive weight matrices, we can alternate which of the two axes we perform the activation aggregation on between each multiplication. With the correct partitioning, each chip will always have the necessary activation shard to multiply with its weight shard, without ever having a fully replicated copy of the activation tensor. Since each axis is partitioned on O( ? n chips ), the communication time scales as O( 1 ? nchips ) rather than remaining constant. This means that even if the 2D layout is communication-limited at a certain chip count and batch size, we can continue to reduce latency by adding more chips, because communication time continues to reduce. However, while the 1D weight-stationary "trick" requires Whereas the 1D weight-stationary layout runs its all-gather and reduce-scatter with unsharded shape BLE per chip, this 2D weight-stationary layout partitions d model so that the communication volume for d ff partitioning is reduced from BLE to BLE X . This comes at the cost of introducing a second pair of reduce-scatter and all-gather operations, whose cost must be balanced with the existing communication.</p><formula xml:id="formula_2">BLE xyz EFxyz W in einsum BLE gelu FxyzE Wout BLFxyz BLE (partialsum-xyz) BLExyz + einsum Z Y BLExyz E x F yz W in einsum BLE x gelu F yz E x W out BLFxyz BLE x (partialsum-yz) BLExyz + einsum 2D weight- stationary 1D weight- stationary B xy LEz E x Fyz W in einsum B xy LE B xy LF z gelu FyzEx W out BxyLE (partialsum-z) einsum Weight- gathered BLFyz EF z F z E B xy LE z + (a) (b) (c)</formula><p>The partitioning layout for weights is E x F yz , i.e. they are partitioned along the d model dimension into X partitions and along the d ff dimension into Y ? Z partitions, where X ? Y ? Z = n chips . The partitioning layout for the input activations is the same as the previous section. Note that we again keep the partitioned weights stationary on their chips, but because of their 2D layout, the activation communication includes two all-gathers and reduce-scatters.</p><p>We derive the optimal values of X, Y and Z to minimize total communication time in Appendix A. </p><formula xml:id="formula_3">T comm = 8BLE ? n chips ? network bandwidth .</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.3">Feedforward layer, weight-gathered layout</head><p>Overview. In the previously described weight-stationary strategies, each chip stores one shard of each weight matrix in memory, and that chip is responsible for multiplying it's "stationary" weight shard with each corresponding activation shard. The output of each per-chip matrix multiplication must then be aggregated between chips to be used as input to the subsequent operations.</p><p>However, as the batch size (and sequence length) grows larger, the size of the output activations may become significantly larger than the size of the weights. When this happens, it can become more economical to keep the activations stationary on each chip, and instead transfer the weights between chips. For very large batch sizes, it is best to keep the activations fully stationary between sequential matrix multiplications, requiring that we fully transfer the weights between all chips. We call this approach XYZweight-gathered. For moderate batch sizes, it is beneficial to use a "hybrid" approach where both weights and activations are partially transferred along different axes. We refer to these approaches as X-weight-gathered and XY-weightgathered.</p><p>Details. Figure <ref type="figure" target="#fig_1">2</ref>(c) shows the XY-weight-gathered layout. A key aspect of the specific layout we choose is that weights start in the same E x F yz layout as in 2D weightstationary, so that we can use the same weight layout for weight-gathered (duing prefill) and weight-stationary (during decoding). Just before the einsums, the weight tensors are all-gathered over the X and Y axes, with communication volume EF Z . This is additional communication relative to weight-stationary layout, but in return we reduce the communication on activations: one reduce-scatter/all-gather pair for activations is skipped, and the communication volume on the other pair drops from BLE X to BLE XY . By changing the relative sizes of the X, Y , and Z axes, we can trade off weight communication against activation communication, and thereby minimize the total communication volume. But we choose to share the weights between weightstationary and weight-gathered layouts, which means we are required to match the choices of X, Y and Z made for the weight-stationary layout. What we do instead is pick between a few variants of the weight-gathered layout. The variant shown in Figure <ref type="figure" target="#fig_1">2</ref> Figure <ref type="figure" target="#fig_4">3</ref> shows how the communication-optimal configuration switches between these layouts as batch size growswhile the 2D weight-stationary strategy minimizes communication at low tokens per batch, different weight-gathered layouts are optimal at larger number of tokens per batch. This highlights the importance of choosing different infer- ence configurations depending on application goals.</p><p>We now show the asymptotic scaling of weight-gathered layouts. Let N be the number of chips that weights are all-gathered over:</p><formula xml:id="formula_4">N = X in X-weight-gathered, N = XY in XY -weight-gathered, N = XY Z in XY Z-weight- gathered.</formula><p>Total communication is minimized by the choice</p><formula xml:id="formula_5">N = BLnchips F</formula><p>which we derive in Appendix A.2.2. The total communication time is</p><formula xml:id="formula_6">T comm = 4E ? BLF ? n chips ? network bandwidth</formula><p>Note that BL corresponds to the total batch size in tokens. The communication time for the weight-stationary layout is linear in BL, while the communication time for the weightgathered layout is linear in ? BL. Therefore, the weightgathered layout becomes cheaper when the batch size and prefill sequence length are sufficiently large.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Partitioning the attention layer</head><p>Multihead attention can be parallelized in essentially the same ways as a feedforward layer, with n heads replacing d ff . But inference with multihead attention incurs significant memory capacity and bandwidth costs to store and load the KV cache, and these costs can dominate the rest of the inference at large batches or long context lengths.</p><p>An alternative approach, called multiquery attention <ref type="bibr" target="#b33">(Shazeer, 2019;</ref><ref type="bibr" target="#b9">Chowdhery et al., 2022)</ref>, still emits n heads for the query tensor, but only a single head for the key and value tensors. This key and value head is shared across the n heads query heads. This reduces the size of the KV cache tensors by a factor of n heads and hence the</p><formula xml:id="formula_7">Q K / V 1 heads batch heads batch time</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Multi-head attention</head><p>Multi-head attention can be sharded across heads without replication</p><p>Multi-query attention Instead by sharding over batch, only a slice of K is needed for einsum, reducing memory access cost.</p><p>Multi-head attention, sharded over heads Multi-query attention, sharded over heads Multi-query attention, sharded over batch</p><formula xml:id="formula_8">Q K / V Q K / V heads time batch batch batch time Q K / V Q K / V heads time batch Q K / V Q K / V heads batch time (a) (b) (c)</formula><p>Figure <ref type="figure" target="#fig_6">4</ref>: Multiquery attention has lower memory cost to load the KV cache when sharded over batch.</p><p>memory time spent loading them. But it also removes an axis otherwise used for parallelism, so the KV cache and related computations need to be partitioned differently.</p><p>Partitioning strategy. The key design consideration is to minimize the memory time of repeatedly loading the KV cache that dominates the inference cost. The partitioning layout of projection matrices that have a n heads dimension (W Q and W O in multiquery attention, and those two plus W K and W V in multihead attention) should match the layout used in the feedforward layer. Multi-head attention (sharded over heads)</p><formula xml:id="formula_9">activations after W Q /W K /W V projection BLHxyzQ + BLLHxyz BLHxyzQ Q K masks reduce-scatter(x) all-gather(x) V W O projection Z X Y BLHxyzQ K cache BLHxyzQ V cache BLHyzQ (partialsum-x) einsum softmax BxyzLHQ B xyz LHQ BLH yz Q einsum</formula><p>Multi-query attention (sharded over batch) During autoregressive generation, there is only one token per example of Q, K, and V tensors, whereas the KV cache has many (perhaps 2048) tokens. Since the KV cache is orders of magnitude larger than the Q, K, and V tensors, it is very profitable to spend the all-to-all communication time on the small tensors to save the memory time on the large tensors.</p><formula xml:id="formula_10">activations after W Q /W K /W V projection B xyz LQ + B xyz LLH Q K masks reduce-scatter(x) all-gather(x) V W O projection Z X Y B xyz LQ K cache B xyz LQ V cache</formula><p>During prefill, it is typically not profitable to shard attention over batch. The Q tensor has many (perhaps 2048) tokens, all of which are queried against the same K and V tensors.</p><p>The memory load of the K and V tensors is amortized over all tokens in the Q tensor, and so this memory load is typically not a bottleneck during prefill. Therefore for prefill we use the sharded-over-heads layout.</p><p>With the proposed partitioning layout, multiquery attention enables using larger batch sizes and sequence lengths, thereby increasing throughput in addition to the latency reduction from reduced memory time. As shown in Section 4.2, the savings are an order of magnitude compared to multihead attention.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Parallel attention/feedforward layers</head><p>We discuss the inference latency gains from the "parallel" formulation of each Transformer block <ref type="bibr">(Wang and Komatsuzaki, 2021)</ref> as used in PaLM <ref type="bibr" target="#b9">(Chowdhery et al., 2022)</ref> instead of the standard "serialized" formulation, where the feedforward layer and attention layer are computed in parallel from the layernormed input and summed to get the output.</p><p>The benefits from the parallel formulation are as follows.</p><p>First, there is only one layernorm per layer instead of two, which reduces latency at small batch sizes. Second, the input matrices of the feedforward layer can be fused with the query projection matrix W Q of the attention layer, the key/value projection matrices W K and W V can be fused in the attention layer, and the output matrix of the feedforward layer can be fused with the output projection matrix W O of the attention layer. This fusion results in higher FLOPS utilization because larger matrix-multiplications run more efficiently on accelerators. More importantly, it also eliminates one of the two all-reduce operations in each Transformer layer needed for d ff /n heads parallelism, cutting communication time over this axis in half.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.5">Low-level optimizations</head><p>We use the Looped CollectiveEinsum technique from <ref type="bibr">(Wang et al., 2023)</ref> to run communication concurrently with computation. This allows us to partially or fully hide the communication time of most of the reduce-scatter and all-gather operations in Figures <ref type="figure" target="#fig_1">2</ref> and<ref type="figure" target="#fig_7">5</ref>. For all reduce-scatter operations in Figures <ref type="figure" target="#fig_1">2</ref> and<ref type="figure" target="#fig_7">5</ref>, we had a choice of whether to reduce-scatter into a batch or sequence dimension (B or L) or into the hidden dimension (E or F ). We chose the latter, because it exposes more effective opportunities for Looped CollectiveEinsum, whereas <ref type="bibr" target="#b22">Korthikanti et al. (2022)</ref> chose the former, to avoid communication in layernorm.</p><p>The CollectiveEinsum loops are the overwhelming majority of the inference latency, so we invested considerable effort to maximize their performance. First, we used the underlying "async CollectivePermute" APIs of <ref type="bibr">Wang et al. (2023)</ref> to develop a suite of variants of the CollectiveEinsum concept, to optimize for different scenarios: latency versus throughput, different numbers of torus axes, fusing with different input/output collectives. Second, we explicitly match up communication collectives with the matrix multiplies that they should be fused with, to maximize the potential for overlap. Through such optimizations, we achieved about 1.4 times better performance than the simpler compilerpartitioned-and-scheduled implementation that we started with. Some of the weight-gathered layouts would exhaust memory without these optimizations.</p><p>We also included the following low-level optimizations: better in-memory layout of tensors to minimize padding and copying during matrix multiplies, faster top-k/top-p implementations for decode sampling, faster log-base-2 implementations of Softmax and Swish, and support for incremental processing of sequences during prefill (Faster-Transformer).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.6">Quantization</head><p>We use the AQT library <ref type="bibr" target="#b25">(Lew et al., 2022)</ref>  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">CASE STUDY FOR PALM MODELS</head><p>Methodology We now conduct an empirical study of our techniques on the PaLM family of models <ref type="bibr" target="#b9">(Chowdhery et al., 2022)</ref>, which we select since the model architecture incorporates the techniques of multiquery attention and parallel attention and feedforward layers.</p><p>Our inference framework is based on JAX <ref type="bibr" target="#b3">(Bradbury et al., 2018)</ref> and XLA (XLA, 2019), and our original high-level implementation was based on T5X (t5x, 2021). We use up to 256 TPU v4 chips (Google, 2022) for our benchmarks. Each TPU v4 chip can run bfloat16 matrix arithmetic at 275 TFLOPS, has 32 GiB of High Bandwidth Memory (HBM) at 1200 GB/s of bandwidth, and has 270 GB/s of interconnect bandwidth in a 3D torus topology (TPUv4).</p><p>For the PaLM 540B model we padded the number of attention heads up from 48 to 64 in order to partition more effectively on 64+ chips. This adds 18B parameters to the model, which comes at a 3% MFU cost, which was more than recovered by being able to partition more effectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Partitioning feedforward layer</head><p>We evaluate the relative performance of our feedforward layer partitioning strategies. First we evaluate performance of decoding. We use batch size 512 to balance latency and MFU. Figure <ref type="figure">6</ref> shows the performance of 1D and 2D weight-stationary layouts as we increase the chip count. Both layouts start to become communication-limited, but the 2D layout performs better because of its asymptotically better scaling with chip count.</p><p>Next we consider the prefill phase. We consider batch sizes from 2048 tokens (1 example, 2048 tokens) to 1 million tokens (512 examples, 2048 tokens per example). Figure <ref type="figure">7</ref> shows that the optimal partitioning layout switches from the 2D weight-stationary layouts to the weight-gathered layouts as the batch size increases. The weight-gathered layouts are inefficient at low batch sizes, but eventually they become the most efficient at high batch sizes, achieving 76% MFU when the communication overhead is almost negligible. Such large batch sizes would fail from memory exhaustion without multiquery attention, as shown in Section 4.2. This highlights the importance of flexibility in configuring the inference system with different choices depending on the application setting and goals.</p><p>These results give us our basic strategy for selecting partitioning layout: during the prefill phase, we select from weight-stationary and weight-gathered layouts based on the current number of tokens in the batch. During the generate phase, we select the 2D weight-stationary layout because the batch size in tokens is always small.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Partitioning Attention layer</head><p>We now evaluate the partitioning layout for multiquery attention proposed in Section 3.3. We consider PaLM with multiquery attention in both the baseline layout that partitions by attention heads and the optimized layout that partitions by batch. We also create a modified variant of PaLM 540B which uses multihead attention instead of multiquery. At large batch sizes and context lengths, the KV cache can become very large, putting us at the risk of running out of memory. Table <ref type="table" target="#tab_2">1</ref> shows that the optimized multiquery layout can fit up to 32-64 times longer context lengths than the multihead and baseline multiquery variant.</p><p>During prefill, multiquery and multihead attention incur similar inference latencies because we compute many attention queries in parallel and the attention computation becomes compute-limited on the attention matrix multiplies. During generation, Figure <ref type="figure">8</ref> shows that the optimized multiquery layout improves speed. The speed improvement is small when the context length is short because almost all of the time is spent on the feedforward layer. As the context length grows longer, the time to load the KV cache in the attention layer becomes a much larger portion of overall inference time. Multiquery attention scales up to sequence lengths of 8192-32,768 tokens (batch sizes 512 and 128 respectively) with attention taking only 8-31% of total runtime.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Parallel attention/feedforward layers</head><p>We consider a variant of PaLM 540B with the parallel formulation of Transformer block replaced by serial attention/feed-forward layers. During generation, we use 2D weightstationary layout, 64 chips, and batch size 512. The serial formulation incurs 14% higher inference latency per step than the parallel version because of the increased communication time for activations. In the prefill phase, this difference shrinks because the weight-gathered layouts incur less activation communication.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">End-to-end results on PaLM</head><p>We find the Pareto frontier between efficiency and latency as we scale the model size for the PaLM family of models: 8B, 62B and 540B, with weights in either bfloat16 or int8. We use a context length 2048 and sweep over the batch size and chip count.</p><p>To meaningfully compare throughput across multiple model sizes with different chip count and batch sizes, we report the cost of an inference in terms of chip-seconds per token calculated as cost (chip-seconds per token) = n chips ? time BL .</p><p>This is directly proportional to operational cost and inversely proportional to MFU.</p><p>Figure <ref type="figure" target="#fig_0">1</ref> <ref type="bibr">(left)</ref> shows the relationship between model size, latency, and cost in the generate phase, at the Pareto frontier of optimal batch size, chip count, and partitioning strategy. The lowest cost is achieved at batch sizes larger than about 512, where the cost is proportional to the number of parameters. As we decrease the batch size, we improve the latency but incur higher cost per token. The minimum latency for generation is 3 times lower than the batch-512 latency.</p><p>We observe that int8 weight quantization achieves the minimum latency in Figure <ref type="figure" target="#fig_0">1</ref> (left): for example, we achieve 28.5ms/token with int8 weights at batch size 64 on PaLM 540B, while we achieve 36.9ms/token with bfloat16 weights. At low latency targets the cost is improved just over a factor of 2, because low-batch-size cost is dominated by weight loading time. At large batch size, cost is more neutral between int8 and bfloat16, because large-batch cost is dominated by the compute time and the matmuls still use bfloat16 arithmetic. We believe that quantization of activations to int8 could enable a further cost improvement.    Tables <ref type="table" target="#tab_8">2</ref> and<ref type="table" target="#tab_5">3</ref> show some key configurations from the Pareto frontiers of Figure <ref type="figure" target="#fig_0">1</ref>, on PaLM 540B and PaLM 62B.</p><p>In the low-latency scenarios we combine batch-1 prefill with batch 32-to-64 decode: batch size 1 achieves best latency in the prefill phase, but for the generate phase we can increase the batch size up to 64 with negligible latency impact, and doing so is dramatically better for generate MFU. This mixture of batch sizes is possible in practice either by generating multiple samples from the same input text, or by pipelining a batch-1 prefill server into a batch-64 decoding server.</p><p>In the high-throughput scenarios of Tables <ref type="table" target="#tab_8">2</ref> and<ref type="table" target="#tab_5">3</ref>, we use larger batch sizes and we switch partitioned layouts between prefill and decode. We use bfloat16 weights for high-throughput scenario, because the weight-loading time is unimportant at large batch sizes, and because our software is missing some optimizations for large-batch int8 mode.</p><p>Comparing 62B (Table <ref type="table" target="#tab_5">3</ref>) vs. 540B models (Table <ref type="table" target="#tab_4">2</ref>), we find that we use more chips for the 540B model, but similar batch sizes and the same partitioned layouts. High- </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">FASTERTRANSFORMER BENCHMARKS</head><p>We now compare with the FasterTransformer benchmarks (FasterTransformer) across a wide range of batch sizes and configurations of prefill and generate. There are multiple differences between our benchmark setup and the FasterTransformer benchmark. In particular, we use different types of chips and chip counts -FasterTransformer uses 16-32 NVIDIA A100s with 80GiB HBM, while we use 64 Google TPU v4 chips with 32GiB HBM. Therefore, we report throughput numbers in terms of MFU, which normalizes for both chip count and chip FLOPS.</p><p>Figure <ref type="figure" target="#fig_9">9</ref> shows the performance of our implementation relative to three FasterTransformer configurations. We benchmark the Megatron 530B model <ref type="bibr" target="#b36">(Smith et al., 2022)</ref>  FasterTransformer reports results with 8-, 16-, and 32-way tensor parallelism. Their 32-way tensor parallelism achieves a maximum of 33% MFU across all reported benchmarks, compared to 46% MFU in their 16-way tensor parallel configuration. This likely indicates a communication bottleneck of scaling tensor parallelism beyond this point. In contrast, our implementation is able to scale up to 64-way tensor parallelism while still achieving 44% MFU, suggesting superior scalability of our 2D weight-stationary partitioning strategy on TPU v4's larger high-speed interconnect domains.</p><p>We provide results on all the configurations used in the FasterTransformer baseline in Appendix D. We also note that our benchmarks throughout the paper attempt to include more challenging inference scenarios, such as context lengths in the range 1024-4096, and report the inference latency for the generate phase and the prefill phase separately (since they have different characteristics).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">RELATED WORK</head><p>Parallelism approaches. Prior works propose several approaches for efficient partitioning to train large models efficiently, for e.g., NeMo Megatron <ref type="bibr" target="#b22">(Korthikanti et al., 2022)</ref>, GSPMD <ref type="bibr" target="#b44">(Xu et al., 2021)</ref> and Alpa <ref type="bibr" target="#b47">(Zheng et al., 2022)</ref>. FasterTransformer establishes a benchmark suite for multi-GPU multi-node inference for a range of different model sizes, including Megatron-Turing NLG 530B. ML inference efficiency. Several approaches <ref type="bibr" target="#b17">(Gupta and Agrawal, 2020)</ref> to improve the inference efficiency of Transformer models focus on model architecture improvements, for example efficient attention layers <ref type="bibr" target="#b30">(Roy et al., 2020;</ref><ref type="bibr" target="#b8">Choromanski et al., 2020;</ref><ref type="bibr" target="#b21">Kitaev et al., 2020;</ref><ref type="bibr" target="#b37">Sukhbaatar et al., 2019;</ref><ref type="bibr" target="#b7">Child et al., 2019)</ref>, distillation <ref type="bibr" target="#b31">(Sanh et al., 2019;</ref><ref type="bibr" target="#b38">Sun et al., 2020)</ref>, and model compression techniques, such as pruning <ref type="bibr">(Li et al., 2020b;</ref><ref type="bibr" target="#b4">Brix et al., 2020;</ref><ref type="bibr" target="#b48">Zhou et al., 2021;</ref><ref type="bibr">Li et al., 2020a;</ref><ref type="bibr" target="#b41">Wang et al., 2020)</ref>, or quantization <ref type="bibr" target="#b12">(Dettmers et al., 2022;</ref><ref type="bibr" target="#b1">Abdolrashidi et al., 2021;</ref><ref type="bibr" target="#b45">Zafrir et al., 2019;</ref><ref type="bibr" target="#b46">Zhang et al., 2018)</ref>. This paper reuses the prior work on model quantization to add to the inference speedups, and the techniques we describe could also be coupled with other model compression methods.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">CONCLUSIONS</head><p>Large Transformer-based models are unlocking new capabilities and applications in several domains, but we need significant advances to democratize their access as we scale up the model size. This paper investigates the scaling properties of Transformer inference workloads and proposes practical partitioning approaches to meet challenging application requirements such as tight latency targets (on the order of seconds for 500B+ parameter models). We show that the best latencies are achieved by going far beyond the traditional paradigm of single-server inference, and scaling inference up to 64+ chips. Longer context lengths incur higher memory costs, but multiquery attention with appropriate partitioning reduces this cost and makes long-context inference practical. The proposed partitioning strategies generalize to many topologies, including single-and multinode NVLink networks in GPU systems.</p><p>Although we achieve our goal of pushing the boundaries of scale for inference workloads, we observe that FLOP count and communication volume can fundamentally limit inference performance of dense Transformer models. Sparsity techniques, such as task-based mixture of expert architectures <ref type="bibr" target="#b15">(Fedus et al., 2022;</ref><ref type="bibr" target="#b23">Kudugunta et al., 2021;</ref><ref type="bibr" target="#b24">Lepikhin et al., 2020;</ref><ref type="bibr" target="#b34">Shazeer et al., 2017)</ref>, and adaptive computation techniques that allocate different amounts of compute per input and generation timestep <ref type="bibr" target="#b19">(Jaszczur et al., 2021;</ref><ref type="bibr" target="#b32">Schuster et al., 2022)</ref>, promise to reduce FLOPs per token of Transformer models. We are hopeful that such techniques that reduce FLOPs per token, as well as techniques that compress chip-to-chip communication, will enable further gains in both cost and latency.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A PARTITIONING STRATEGIES: DERIVING COMMUNICATION COSTS</head><p>A.1 Cost of all-gather/reduce-scatter <ref type="figure">Figure A</ref>.1 shows typical collective operations we use in partitioning strategies and their communication patterns across three devices. For an all-gather over K partitions, where each chip produces an output of size D, the communication pattern requires chunks of size D K to be transferred over (K -1) interconnect links in the process of getting copied to (K -1) chips. The resulting communication time for the all-gather is</p><formula xml:id="formula_11">T comm(all-gather) = D (network bandwidth) K -1 K .</formula><p>This is a general cost model that holds true for most realworld network topologies <ref type="bibr" target="#b6">(Chan et al., 2007)</ref>, not just the TPU's torus topology.</p><p>The communication time for a reduce-scatter T comm(reduce-scatter) is the same, except that D is the size of the (larger) input buffer rather than the (smaller) output buffer. Thus, the total communication time for an all-reduce is T comm(all-reduce) = 2 ? T comm(all-gather) .</p><p>In most formulas, we will disregard the (K -1)/K term, approximating it as 1 under the assumption K 1, in order to simplify the algebra. This yields a simple approximation: reduce-scatter time is proportional to the size of the per-chip input, and all-gather time is proportional to the size of the per-chip output.  A key aspect of the specific layout we choose is that weights start in the same E x F yz layout as in 2D weightstationary, so that we can instantly switch between weightgathered layout and weight-stationary layout. Just before the einsums, the weight tensors are all-gathered over the X and Y axes, with communication volume EF/Z. By changing the relative sizes of the X, Y , and Z axes, we can trade off weight communication against activation communication, and thereby minimize the total communication volume. We now show the asymptotic scaling of weightgathered layouts. Let N be the number of chips that weights are all-gathered over: N = X in X-weight-gathered, N = XY in XY -weight-gathered, and N = XY Z in XY Z-weight-gathered.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.2 Details for communication time calculations</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Weight communication is:</head><p>T comm(weights) = 2EF ? N n chips ? network bandwidth .</p><p>Activation communication is:</p><formula xml:id="formula_12">T comm(acts) = 2BLE N ? network bandwidth .</formula><p>Total communication is minimized by the choice N = BSn chips /F , which yields total communication time</p><formula xml:id="formula_13">T comm = 4E ? BLF ? n chips ? network bandwidth</formula><p>Figure <ref type="figure" target="#fig_4">3</ref> shows how the communication-optimal configuration switches between these layouts as batch size grows. While the 2D weight-stationary strategy minimizes communication at low tokens per batch, different weight-gathered layouts are optimal at larger number of tokens per batch. </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: Cost vs. latency for PaLM models. We use a context length of 2048. Points in each line represent the Pareto frontier of efficiency versus latency. Chip count is C, batch size is B. Left: latency per token for generating 64 tokens, assuming the context has already been processed. Right: time to process 2048 input tokens; excludes the time to generate any output tokens. Tables2 and 3show details on a few specific scenarios from the Pareto frontier where the applications have low-latency or high-throughput requirements.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: Partitioning layouts for feedforward layer.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head></head><label></label><figDesc>2.1. Assuming d ff = 4 ? d model , we achieve the minimum communication time with X = 0.5 ? ? n chips and Y Z = 2 ? ? n chips . The resulting total communication time is:</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head></head><label></label><figDesc>(c) uses all-gather(xy) for the weights and B xy LE z partitioning of batch for the activations. Our other variants use all-gather(x) or all-gather(xyz) for weights, and correspondingly use B x LE yz or B xyz LE partitioning of the activations. Figure A.2 shows the three weight-gathered layouts.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: Communication volume as a function of batch size, for a feedforward layer. As batch size (in tokens) grows, it is better to switch to a layout that all-gathers the weights over increasingly more chips to minimize the communication volume. Communication volumes estimated for X = Y = Z = 4, d model = 16384, and d ff = 65536.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 4</head><label>4</label><figDesc>Figure4(a) shows a typical partitioning layout for multihead attention, matching the 2D weight stationary feedforward layout. Here the Q, K, and V activations are partitioned over the n heads dimension into n chips partitions when n heads is a multiple of n chips . For n chips greater than n heads , the attention heads are partially replicated. The most similar partitioning layout for multiquery attention (shown in Figure4(b)) treats the KV cache the same as in multihead attention. Even though the key and value tensors are shared across all heads, they must be replicated on each chip and the memory cost savings of multiquery attention are lost.We instead propose a partitioning strategy for the multiquery attention where the Q, K, and V matrices are partitioned over the batch B dimension into n chips partitions. Figure4(c)shows that this reduces the memory cost of loading the KV cache per chip by a factor of n chips , thereby reducing the memory time by the same factor. The proposed partitioning strategy incurs additional communication cost of resharding the input activation tensors using an all-to-all collective as shown in Figure5(b) in comparison to the multiquery attention sharding strategy shown in Figure5(a) where the Q, K,</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure 5 :</head><label>5</label><figDesc>Figure 5: Comparison of partitioning layouts for attention layer: multihead attention sharded over heads versus multiquery attention sharded over batch.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Figure 1 (</head><label>1</label><figDesc>Figure 1 (right) shows the relationship between model size, latency, and cost in the prefill phase. The tradeoff between batch size and latency is less severe in the prefill phase than the generate phase and even batch size 1 runs with fairly low cost. Further, the cost of batch-512 prefill is 2 times lower than batch-512 generate because of the increased MFU of the weight-gathered layouts we use during prefill. More details on the relationship between model size and MFU are presented in Figure C.1 and Section C in the Appendix.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>Figure 9 :</head><label>9</label><figDesc>Figure 9: Model FLOPS utilization (MFU) versus total latency for running a 60-input-token, 20-output-token inference, at a range of batch sizes.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head></head><label></label><figDesc>Figure 2(b) shows the partitioning layout. The partitioning layout for weights is E x F yz , i.e. they are partitioned along the d model dimension into X partitions and along the d ff dimension into Y ?Z partitions, where X ?Y ?Z = n chips . We now show how to size the X, Y and Z axes of the torus to minimize total communication time in 2D weightstationary layout. The communication time is:T comm = 2BL network bandwidth E X + F Y Z</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_11"><head>Figure</head><label></label><figDesc>Figure A.2 shows the different weight-gathered layouts, while Figure 2(c) shows one instance of XY weight-gathered layout.A key aspect of the specific layout we choose is that weights start in the same E x F yz layout as in 2D weightstationary, so that we can instantly switch between weightgathered layout and weight-stationary layout. Just before the einsums, the weight tensors are all-gathered over the X and Y axes, with communication volume EF/Z.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0"><head></head><label></label><figDesc></figDesc><graphic url="image-1.png" coords="16,76.99,67.07,442.90,143.99" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 1 :</head><label>1</label><figDesc>Maximum context length supported for different attention variants of PaLM 540B on 64 chips. We reserve 30% of the total memory for KV cache. Optimized multiquery attention enables up to 32x larger context lengths.</figDesc><table><row><cell cols="6">Multiquery vs. Multihead Attention (8 layers)</cell></row><row><cell>Latency per Step (milliseconds)</cell><cell>0 20 40 60</cell><cell>128</cell><cell cols="3">512 Context Sequence Length 2048 Multiquery (Optimized Layout) Multiquery (Inefficient Layout) Multihead</cell><cell>8192</cell></row><row><cell cols="6">Figure 8: Latency per generated token vs. sequence length,</cell></row><row><cell cols="6">for an 8-layer version of PaLM 540B on 64 chips with batch</cell></row><row><cell cols="6">size 256. The dotted line represents that on the full 118-</cell></row><row><cell cols="6">layer model and context lengths longer than 512, the KV</cell></row><row><cell cols="6">cache will not fit in memory when using multihead attention</cell></row><row><cell cols="6">or the baseline multiquery partitioning.</cell></row><row><cell cols="4">Model variant</cell><cell>dhead</cell><cell>Max context length</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>batch=128 batch=512</cell></row><row><cell cols="4">Multihead</cell><cell>128</cell><cell>1320</cell><cell>330</cell></row><row><cell cols="4">Baseline multiquery</cell><cell>256</cell><cell>660</cell><cell>165</cell></row><row><cell cols="4">Optimized multiquery</cell><cell>256</cell><cell>43,000</cell><cell>10,700</cell></row><row><cell cols="4">multihead variant.</cell><cell></cell></row></table><note><p>To keep parameter count in the attention layer constant, we shrink d head from 256 in the multiquery variant to 128 in the</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 2 :</head><label>2</label><figDesc>Example configurations for PaLM 540B, in the same</figDesc><table><row><cell cols="5">setting as Figure 1. Prefill latency is for processing 2048 tokens;</cell></row><row><cell cols="5">decode latency is for generating 64 tokens. Feedforward network</cell></row><row><cell cols="5">(FFN) layouts are Weight Stationary 2D (WS 2D, Section 3.2.2)</cell></row><row><cell cols="5">and Weight Gathered XYZ (WG XYZ, Section 3.2.3). Attention</cell></row><row><cell cols="3">layouts are from Section 3.3.</cell><cell></cell><cell></cell></row><row><cell></cell><cell cols="2">Low-latency</cell><cell cols="2">High-throughput</cell></row><row><cell></cell><cell>Prefill</cell><cell cols="2">Decode Prefill</cell><cell>Decode</cell></row><row><cell>Chips</cell><cell>16</cell><cell>16</cell><cell>32</cell><cell>8</cell></row><row><cell>Batch</cell><cell>1</cell><cell>32</cell><cell>512</cell><cell>512</cell></row><row><cell>FFN</cell><cell cols="4">WS 2D WS 2D WG XYZ WS 2D</cell></row><row><cell>Attention</cell><cell>Head</cell><cell>Batch</cell><cell>Batch</cell><cell>Batch</cell></row><row><cell>sharding</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Weights</cell><cell>int8</cell><cell>int8</cell><cell>bfloat16</cell><cell>bfloat16</cell></row><row><cell>format</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>MFU</cell><cell>36%</cell><cell>8%</cell><cell>73%</cell><cell>37%</cell></row><row><cell>Latency</cell><cell>0.16s</cell><cell>0.73s</cell><cell>20.2s</cell><cell>5.1s</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 3 :</head><label>3</label><figDesc>Example configurations for PaLM 62B, in the same</figDesc><table><row><cell>setting as Figure 1. Prefill latency is for processing 2048 tokens;</cell></row><row><cell>decode latency is for generating 64 tokens. Feedforward network</cell></row><row><cell>(FFN) layouts are Weight Stationary 2D (WS 2D, Section 3.2.2)</cell></row><row><cell>and Weight Gathered XYZ (WG XYZ, Section 3.2.3). Attention</cell></row><row><cell>layouts are from Section 3.3.</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>Table D .</head><label>D</label><figDesc>4: Results for the 128-input-token, 8-output-token benchmark. All times in milliseconds. The bold and underline annotations are not per row, but instead show the Pareto frontier of time vs. MFU. See Section D for full explanation.</figDesc><table><row><cell></cell><cell cols="5">FasterTransformer MT-NLG 530B total</cell><cell></cell><cell></cell><cell cols="6">Ours (530B/540B on 64 TPU v4 with 2D partitioning)</cell><cell></cell></row><row><cell></cell><cell cols="2">TP16</cell><cell>TP32</cell><cell></cell><cell cols="2">PP3/TP8</cell><cell cols="2">PaLM prefill</cell><cell cols="2">PaLM generate</cell><cell cols="2">PaLM total</cell><cell cols="2">MT-NLG total</cell></row><row><cell>batch</cell><cell cols="2">time MFU</cell><cell cols="2">time MFU</cell><cell cols="2">time MFU</cell><cell cols="2">time MFU</cell><cell>time</cell><cell>MFU</cell><cell cols="2">time MFU</cell><cell cols="2">time MFU</cell></row><row><cell>1</cell><cell>585</cell><cell>5%</cell><cell>451</cell><cell>3%</cell><cell>866</cell><cell>2%</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell></cell><cell></cell></row><row><cell>2</cell><cell>667</cell><cell>9%</cell><cell>508</cell><cell>6%</cell><cell>932</cell><cell>4%</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell></cell><cell></cell></row><row><cell>4</cell><cell>765</cell><cell>15%</cell><cell>606</cell><cell cols="2">10% 1097</cell><cell>7%</cell><cell>81</cell><cell>39%</cell><cell>258</cell><cell>1%</cell><cell>343</cell><cell>10%</cell><cell>338</cell><cell>10%</cell></row><row><cell>8</cell><cell>990</cell><cell>23%</cell><cell>766</cell><cell cols="2">15% 1434</cell><cell>11%</cell><cell>149</cell><cell>42%</cell><cell>234</cell><cell>2%</cell><cell>403</cell><cell>17%</cell><cell>384</cell><cell>16%</cell></row><row><cell>16</cell><cell>1377</cell><cell>34%</cell><cell>1074</cell><cell cols="2">22% 2104</cell><cell>15%</cell><cell>287</cell><cell>44%</cell><cell>253</cell><cell>3%</cell><cell>586</cell><cell>23%</cell><cell>540</cell><cell>23%</cell></row><row><cell>32</cell><cell>2251</cell><cell>41%</cell><cell>1741</cell><cell cols="2">27% 2623</cell><cell>23%</cell><cell>536</cell><cell>47%</cell><cell>263</cell><cell>6%</cell><cell cols="2">796 34%</cell><cell>799</cell><cell>33%</cell></row><row><cell>64</cell><cell>4002</cell><cell>46%</cell><cell>3114</cell><cell cols="2">30% 3578</cell><cell>34%</cell><cell>1056</cell><cell>48%</cell><cell>317</cell><cell>10%</cell><cell cols="2">1329 40%</cell><cell>1372</cell><cell>39%</cell></row><row><cell cols="2">128 OOM</cell><cell>-</cell><cell>5784</cell><cell cols="2">32% 5512</cell><cell>45%</cell><cell>2202</cell><cell>46%</cell><cell>381</cell><cell>17%</cell><cell cols="2">2343 46%</cell><cell>2583</cell><cell>45%</cell></row><row><cell cols="2">256 OOM</cell><cell cols="2">-11232</cell><cell cols="2">33% 9614</cell><cell>51%</cell><cell>4479</cell><cell>45%</cell><cell>431</cell><cell>29%</cell><cell>4710</cell><cell>45%</cell><cell>4911</cell><cell>45%</cell></row><row><cell>512</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>8913</cell><cell>45%</cell><cell>734</cell><cell>34%</cell><cell>9673</cell><cell>44%</cell><cell>9647</cell><cell>43%</cell></row><row><cell>1024</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell cols="2">-17766</cell><cell cols="2">45% 1370</cell><cell cols="2">37% 19723</cell><cell cols="2">43% 19136</cell><cell>43%</cell></row></table></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div><head n="8">ACKNOWLEDGMENTS</head><p>Our work builds on top of the work of many, many teams at <rs type="institution">Google</rs>. We'd especially like to recognize the PaLM team, T5X team, the <rs type="institution">Pathways infrastructure team</rs>, the <rs type="institution">JAX team</rs>, the Flaxformer team, the XLA team, and the <rs type="institution">AQT team</rs>. We are grateful to <rs type="person">Blake Hechtman</rs>, <rs type="person">Marcello Maggioni</rs>, <rs type="person">Zongwei Zhou</rs>, and <rs type="person">Shibo Wang</rs> for XLA support and performance optimizations. We would like to thank our colleagues for valuable inputs and discussion on the project -<rs type="person">Jacob Austin</rs>, <rs type="person">Yuanzhong Xu</rs>, <rs type="person">Lukasz Lew</rs>, <rs type="person">Sharan Narang</rs>, <rs type="person">Adam Roberts</rs>, <rs type="person">Noah Fiedel</rs>, and <rs type="person">Mike Gunter</rs>. We also thank <rs type="person">Hyeontaek Lim</rs>, <rs type="person">James Laudon</rs>, <rs type="person">George Necula</rs>, <rs type="person">Martin Abadi</rs> and <rs type="person">Chandu Thekkath</rs> for their review and feedback in improving the presentation of this work, and <rs type="person">Erica Moreira</rs> for the support of compute resources.</p></div>
			</div>			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B MINIMUM PREFILL LATENCY</head><p>We report here the minimum latency required for prefill. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C MFU VS LATENCY TRADEOFF</head><p>We report here the relationship between model size, latency, and MFU. Figure <ref type="figure">C</ref>.1 shows the Pareto frontier of MFU vs. latency as we sweep the batch size and the number of chips same as Figure <ref type="figure">1</ref>. The MFU for decode is typically much lower than for prefill. In the prefill phase, the "jumps" in MFU show the transition point from weight stationary 2D layout to XYZ weight gathered layout.</p><p>In most cases, the larger models achieve higher MFUs than the smaller models, because larger matrix multiplies are more efficient. However, at long-latency decodes, PaLM 62B achieves higher MFU than PaLM 540B, because the former uses 8-way model parallelism and the latter uses 64way model parallelism. We may be able to further optimize PaLM 540B by reducing the model parallelism in the highthroughput (latency-tolerant) regime.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D FULL COMPARISON TO FASTERTRANSFORMER</head><p>In this section, we report the latency and MFU of our implementations of both the PaLM 540B model and the Megatron-Turing NLG 530B model run on 64 TPU v4 chips, in comparison to FasterTransformer baselines. We first note the model architecture differences in Table <ref type="table">D</ref>.1.</p><p>Then, we report the the full set of comparisons for the three configurations in the FasterTransformer benchmarks: 20 input tokens and 8 output tokens in Visually, this corresponds to being "up and to the left" in Figure <ref type="figure">9</ref>.</p><p>We do not report batch sizes below 4 because our partitioning strategy partitions multiquery attention over batch and achieves no speedup for a batch size smaller than 4 (the minimum size of a TPU v4 torus axis).   </p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<ptr target="https://github.com/google-research/t5x" />
		<title level="m">T5x</title>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Pareto-optimal quantized resnet is mostly 4-bit</title>
		<author>
			<persName><forename type="first">Amirali</forename><surname>Abdolrashidi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lisa</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shivani</forename><surname>Agrawal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jonathan</forename><surname>Malmaud</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Oleg</forename><surname>Rybakov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chas</forename><surname>Leichner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lukasz</forename><surname>Lew</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="3091" to="3099" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">Deepspeed inference: Enabling efficient inference of transformer models at unprecedented scale</title>
		<author>
			<persName><forename type="first">Reza</forename><surname>Yazdani Aminabadi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Samyam</forename><surname>Rajbhandari</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Minjia</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ammar</forename><surname>Ahmad Awan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Cheng</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Du</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Elton</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jeff</forename><surname>Rasley</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shaden</forename><surname>Smith</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Olatunji</forename><surname>Ruwase</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuxiong</forename><surname>He</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2207.00032</idno>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">JAX: Composable transformations of Python+NumPy programs</title>
		<author>
			<persName><forename type="first">James</forename><surname>Bradbury</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Roy</forename><surname>Frostig</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Peter</forename><surname>Hawkins</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Matthew</forename><forename type="middle">James</forename><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chris</forename><surname>Leary</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dougal</forename><surname>Maclaurin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">George</forename><surname>Necula</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Adam</forename><surname>Paszke</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jake</forename><surname>Vanderplas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Skye</forename><surname>Wanderman-Milne</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qiao</forename><surname>Zhang</surname></persName>
		</author>
		<ptr target="http://github.com/google/jax" />
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">Successfully applying the stabilized lottery ticket hypothesis to the transformer architecture</title>
		<author>
			<persName><forename type="first">Christopher</forename><surname>Brix</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Parnia</forename><surname>Bahar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hermann</forename><surname>Ney</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2005.03454</idno>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Language models are few-shot learners</title>
		<author>
			<persName><forename type="first">Tom</forename><surname>Brown</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Benjamin</forename><surname>Mann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nick</forename><surname>Ryder</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Melanie</forename><surname>Subbiah</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jared</forename><forename type="middle">D</forename><surname>Kaplan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Prafulla</forename><surname>Dhariwal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Arvind</forename><surname>Neelakantan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pranav</forename><surname>Shyam</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Girish</forename><surname>Sastry</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Amanda</forename><surname>Askell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sandhini</forename><surname>Agarwal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ariel</forename><surname>Herbert-Voss</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gretchen</forename><surname>Krueger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tom</forename><surname>Henighan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rewon</forename><surname>Child</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aditya</forename><surname>Ramesh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Daniel</forename><surname>Ziegler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jeffrey</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Clemens</forename><surname>Winter</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chris</forename><surname>Hesse</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mark</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Eric</forename><surname>Sigler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mateusz</forename><surname>Litwin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Scott</forename><surname>Gray</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Benjamin</forename><surname>Chess</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jack</forename><surname>Clark</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christopher</forename><surname>Berner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sam</forename><surname>Mccandlish</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alec</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dario</forename><surname>Amodei</surname></persName>
		</author>
		<ptr target="https://proceedings" />
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page" from="1877" to="1901" />
		</imprint>
	</monogr>
	<note>neur ips.cc/paper/2020/file/1457c0d6bfcb4 967418bfb8ac142f64a-Paper.pdf</note>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Collective communication: theory, practice, and experience</title>
		<author>
			<persName><forename type="first">Ernie</forename><surname>Chan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Marcel</forename><surname>Heimlich</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Avi</forename><surname>Purkayastha</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Robert</forename><surname>Van De Geijn</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Concurrency and Computation: Practice and Experience</title>
		<imprint>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="issue">13</biblScope>
			<biblScope unit="page" from="1749" to="1783" />
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">Generating long sequences with sparse transformers</title>
		<author>
			<persName><forename type="first">Rewon</forename><surname>Child</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Scott</forename><surname>Gray</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alec</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1904.10509</idno>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">Rethinking attention with performers</title>
		<author>
			<persName><forename type="first">Krzysztof</forename><surname>Choromanski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Valerii</forename><surname>Likhosherstov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><surname>Dohan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xingyou</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andreea</forename><surname>Gane</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tamas</forename><surname>Sarlos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Peter</forename><surname>Hawkins</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jared</forename><surname>Davis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Afroz</forename><surname>Mohiuddin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lukasz</forename><surname>Kaiser</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2009.14794</idno>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<author>
			<persName><forename type="first">Aakanksha</forename><surname>Chowdhery</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sharan</forename><surname>Narang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jacob</forename><surname>Devlin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Maarten</forename><surname>Bosma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gaurav</forename><surname>Mishra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Adam</forename><surname>Roberts</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Paul</forename><surname>Barham</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Charles</forename><surname>Sutton Hyung Won</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sebastian</forename><surname>Chung</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Parker</forename><surname>Gehrmann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kensen</forename><surname>Schuh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sasha</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Joshua</forename><surname>Tsvyashchenko</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Abhishek</forename><surname>Maynez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Parker</forename><surname>Rao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yi</forename><surname>Barnes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Noam</forename><surname>Tay</surname></persName>
		</author>
		<author>
			<persName><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Emily</forename><surname>Vinodkumar Prabhakaran</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nan</forename><surname>Reif</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ben</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Reiner</forename><surname>Hutchinson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">James</forename><surname>Pope</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jacob</forename><surname>Bradbury</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><surname>Austin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Guy</forename><surname>Isard</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pengcheng</forename><surname>Gur-Ari</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Toju</forename><surname>Yin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Anselm</forename><surname>Duke</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sanjay</forename><surname>Levskaya</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sunipa</forename><surname>Ghemawat</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Henryk</forename><surname>Dev</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xavier</forename><surname>Michalewski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vedant</forename><surname>Garcia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kevin</forename><surname>Misra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Liam</forename><surname>Robinson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Denny</forename><surname>Fedus</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Daphne</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><surname>Ippolito</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hyeontaek</forename><surname>Luan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Barret</forename><surname>Lim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alexander</forename><surname>Zoph</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ryan</forename><surname>Spiridonov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><surname>Sepassi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shivani</forename><surname>Dohan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mark</forename><surname>Agrawal</surname></persName>
		</author>
		<author>
			<persName><surname>Omernick</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2204.02311</idno>
		<ptr target="https://arxiv.org/abs/2204.02311" />
		<title level="m">PaLM: Scaling language modeling with Pathways</title>
		<editor>
			<persName><forename type="first">Andrew</forename><forename type="middle">M</forename><surname>Dai</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Thanumalayan</forename><surname>Sankaranarayana Pillai</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Marie</forename><surname>Pellat</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Aitor</forename><surname>Lewkowycz</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Erica</forename><surname>Moreira</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Rewon</forename><surname>Child</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Oleksandr</forename><surname>Polozov</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Katherine</forename><surname>Lee</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Zongwei</forename><surname>Zhou</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Xuezhi</forename><surname>Wang</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Brennan</forename><surname>Saeta</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Mark</forename><surname>Diaz</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Orhan</forename><surname>Firat</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Michele</forename><surname>Catasta</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Jason</forename><surname>Wei</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Kathy</forename><surname>Meier-Hellstern</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Douglas</forename><surname>Eck</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Jeff</forename><surname>Dean</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Slav</forename><surname>Petrov</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Noah</forename><surname>Fiedel</surname></persName>
		</editor>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">The mpi message passing interface standard</title>
		<author>
			<persName><forename type="first">Lyndon</forename><surname>Clarke</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ian</forename><surname>Glendinning</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rolf</forename><surname>Hempel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Programming environments for massively parallel distributed systems</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="1994">1994</date>
			<biblScope unit="page" from="213" to="218" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">Flashattention: Fast and memoryefficient exact attention with io-awareness</title>
		<author>
			<persName><forename type="first">Tri</forename><surname>Dao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Daniel</forename><forename type="middle">Y</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stefano</forename><surname>Ermon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Atri</forename><surname>Rudra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christopher</forename><surname>R?</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2205.14135</idno>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<author>
			<persName><forename type="first">Tim</forename><surname>Dettmers</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mike</forename><surname>Lewis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Younes</forename><surname>Belkada</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Luke</forename><surname>Zettlemoyer</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2208.07339</idno>
	</analytic>
	<monogr>
		<title level="m">-bit matrix multiplication for transformers at scale</title>
		<imprint>
			<date type="published" when="2022">2022</date>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page">8</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">Effective transformer</title>
		<author>
			<persName><surname>Effectivetransformer</surname></persName>
		</author>
		<ptr target="https://github.com/bytedance/effectivetransformer" />
		<imprint>
			<date type="published" when="2022-10">October-2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<author>
			<persName><surname>Fastertransformer</surname></persName>
		</author>
		<author>
			<persName><surname>Fastertransformer</surname></persName>
		</author>
		<ptr target="https://github.com/NVIDIA/FasterTransformer/blob/main/docs/gptguide.md" />
		<title level="m">Gpt guide</title>
		<imprint>
			<date type="published" when="2022-10">October-2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">A review of sparse expert models in deep learning</title>
		<author>
			<persName><forename type="first">William</forename><surname>Fedus</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jeff</forename><surname>Dean</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Barret</forename><surname>Zoph</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2209.01667</idno>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title/>
		<author>
			<persName><forename type="first">Google</forename><surname>Cloud</surname></persName>
		</author>
		<ptr target="https://cloud.google.com/tpu" />
		<imprint>
			<date type="published" when="2022-10">2022. October-2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">Compression of deep learning models for text: A survey</title>
		<author>
			<persName><forename type="first">Manish</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Puneet</forename><surname>Agrawal</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2008.05221</idno>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<author>
			<persName><forename type="first">Jordan</forename><surname>Hoffmann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sebastian</forename><surname>Borgeaud</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Arthur</forename><surname>Mensch</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Elena</forename><surname>Buchatskaya</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Trevor</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Eliza</forename><surname>Rutherford</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Diego</forename><surname>De Las</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lisa</forename><forename type="middle">Anne</forename><surname>Casas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Johannes</forename><surname>Hendricks</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aidan</forename><surname>Welbl</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tom</forename><surname>Clark</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Eric</forename><surname>Hennigan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Katie</forename><surname>Noland</surname></persName>
		</author>
		<author>
			<persName><forename type="first">George</forename><surname>Millican</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bogdan</forename><surname>Van Den Driessche</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aurelia</forename><surname>Damoc</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Simon</forename><surname>Guy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Karen</forename><surname>Osindero</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Erich</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jack</forename><forename type="middle">W</forename><surname>Elsen</surname></persName>
		</author>
		<author>
			<persName><surname>Rae</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2203.15556</idno>
		<title level="m">Oriol Vinyals, and Laurent Sifre. Training compute-optimal large language models</title>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Sparse is enough in scaling transformers</title>
		<author>
			<persName><forename type="first">Sebastian</forename><surname>Jaszczur</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aakanksha</forename><surname>Chowdhery</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Afroz</forename><surname>Mohiuddin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lukasz</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wojciech</forename><surname>Gajewski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Henryk</forename><surname>Michalewski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jonni</forename><surname>Kanerva</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in Neural Information Processing Systems</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="page" from="9895" to="9907" />
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<author>
			<persName><forename type="first">Jared</forename><surname>Kaplan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sam</forename><surname>Mccandlish</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tom</forename><surname>Henighan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tom</forename><forename type="middle">B</forename><surname>Brown</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Benjamin</forename><surname>Chess</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rewon</forename><surname>Child</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Scott</forename><surname>Gray</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alec</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jeffrey</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dario</forename><surname>Amodei</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2001.08361</idno>
		<title level="m">Scaling laws for neural language models</title>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">Reformer: The efficient transformer</title>
		<author>
			<persName><forename type="first">Nikita</forename><surname>Kitaev</surname></persName>
		</author>
		<author>
			<persName><forename type="first">?ukasz</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Anselm</forename><surname>Levskaya</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2001.04451</idno>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">Reducing activation recomputation in large transformer models</title>
		<author>
			<persName><forename type="first">Vijay</forename><surname>Korthikanti</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jared</forename><surname>Casper</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sangkug</forename><surname>Lym</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lawrence</forename><surname>Mcafee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><surname>Andersch</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mohammad</forename><surname>Shoeybi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bryan</forename><surname>Catanzaro</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2205.05198</idno>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">Beyond distillation: Task-level mixture-of-experts for efficient inference</title>
		<author>
			<persName><forename type="first">Sneha</forename><surname>Kudugunta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yanping</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ankur</forename><surname>Bapna</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Maxim</forename><surname>Krikun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dmitry</forename><surname>Lepikhin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Minh-Thang</forename><surname>Luong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Orhan</forename><surname>Firat</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2110.03742</idno>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">GShard: Scaling giant models with conditional computation and automatic sharding</title>
		<author>
			<persName><forename type="first">Dmitry</forename><surname>Lepikhin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hyoukjoong</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuanzhong</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dehao</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Orhan</forename><surname>Firat</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yanping</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Maxim</forename><surname>Krikun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Noam</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhifeng</forename><surname>Chen</surname></persName>
		</author>
		<ptr target="https://openreview.net/forum?id=qrwe7XHTmYb" />
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<author>
			<persName><forename type="first">Lukasz</forename><surname>Lew</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vlad</forename><surname>Feinberg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shivani</forename><surname>Agrawal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jihwan</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jonathan</forename><surname>Malmaud</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lisa</forename><surname>Wang</surname></persName>
		</author>
		<ptr target="http://github.com/google/aqt" />
		<title level="m">Pouya Dormiani, and Reiner Pope. Aqt: Accurate quantized training)</title>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<title level="m" type="main">Efficient transformer-based large scale language representations using hardware-friendly block structured pruning</title>
		<author>
			<persName><forename type="first">Bingbing</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhenglun</forename><surname>Kong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tianyun</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ji</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhengang</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Caiwen</forename><surname>Ding</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2009.08065</idno>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Train big, then compress: Rethinking model size for efficient training and inference of transformers</title>
		<author>
			<persName><forename type="first">Zhuohan</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Eric</forename><surname>Wallace</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sheng</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kevin</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kurt</forename><surname>Keutzer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dan</forename><surname>Klein</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Joey</forename><surname>Gonzalez</surname></persName>
		</author>
		<idno>PMLR</idno>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="5958" to="5968" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<title/>
		<author>
			<persName><forename type="first">Jack</forename><forename type="middle">W</forename><surname>Rae</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sebastian</forename><surname>Borgeaud</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Trevor</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Katie</forename><surname>Millican</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jordan</forename><surname>Hoffmann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">Francis</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName><forename type="first">John</forename><surname>Aslanides</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sarah</forename><surname>Henderson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Roman</forename><surname>Ring</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Susannah</forename><surname>Young</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Eliza</forename><surname>Rutherford</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tom</forename><surname>Hennigan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jacob</forename><surname>Menick</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Albin</forename><surname>Cassirer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Richard</forename><surname>Powell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">George</forename><surname>Van Den Driessche</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lisa</forename><forename type="middle">Anne</forename><surname>Hendricks</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Maribeth</forename><surname>Rauh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Po-Sen</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Amelia</forename><surname>Glaese</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Johannes</forename><surname>Welbl</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sumanth</forename><surname>Dathathri</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Saffron</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jonathan</forename><surname>Uesato</surname></persName>
		</author>
		<author>
			<persName><forename type="first">John</forename><surname>Mellor</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Irina</forename><surname>Higgins</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Antonia</forename><surname>Creswell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nat</forename><surname>Mcaleese</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Amy</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Erich</forename><surname>Elsen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Siddhant</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Elena</forename><surname>Jayakumar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><surname>Buchatskaya</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Esme</forename><surname>Budden</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Karen</forename><surname>Sutherland</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michela</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Laurent</forename><surname>Paganini</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lena</forename><surname>Sifre</surname></persName>
		</author>
		<author>
			<persName><surname>Martens</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lorraine</forename><surname>Xiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Adhiguna</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aida</forename><surname>Kuncoro</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Elena</forename><surname>Nematzadeh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Domenic</forename><surname>Gribovskaya</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Angeliki</forename><surname>Donato</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Arthur</forename><surname>Lazaridou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jean-Baptiste</forename><surname>Mensch</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Maria</forename><surname>Lespiau</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nikolai</forename><surname>Tsimpoukelli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Doug</forename><surname>Grigorev</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Thibault</forename><surname>Fritz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mantas</forename><surname>Sottiaux</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Toby</forename><surname>Pajarskas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhitao</forename><surname>Pohlen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Daniel</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Cyprien</forename><surname>Toyama</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yujia</forename><surname>De Masson D'autume</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tayfun</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vladimir</forename><surname>Terzi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Igor</forename><surname>Mikulik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aidan</forename><surname>Babuschkin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Diego</forename><surname>Clark</surname></persName>
		</author>
		<author>
			<persName><surname>De Las</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aurelia</forename><surname>Casas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chris</forename><surname>Guy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">James</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Matthew</forename><surname>Bradbury</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Blake</forename><forename type="middle">A</forename><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Laura</forename><surname>Hechtman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Iason</forename><surname>Weidinger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">William</forename><forename type="middle">S</forename><surname>Gabriel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Edward</forename><surname>Isaac</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Simon</forename><surname>Lockhart</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Laura</forename><surname>Osindero</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chris</forename><surname>Rimell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Oriol</forename><surname>Dyer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kareem</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jeff</forename><surname>Ayoub</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lorrayne</forename><surname>Stanway</surname></persName>
		</author>
		<author>
			<persName><surname>Bennett</surname></persName>
		</author>
		<imprint/>
	</monogr>
	<note>Demis Hassabis, Koray Kavukcuoglu, and Geoffrey Irving. Scaling language models: Methods, analysis &amp; insights from training Gopher. CoRR, abs/2112.11446, 2021</note>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Zero: Memory optimizations toward training trillion parameter models</title>
		<author>
			<persName><forename type="first">Samyam</forename><surname>Rajbhandari</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jeff</forename><surname>Rasley</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Olatunji</forename><surname>Ruwase</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuxiong</forename><surname>He</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SC20: International Conference for High Performance Computing, Networking, Storage and Analysis</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="1" to="16" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
		<title level="m" type="main">Efficient content-based sparse attention with routing transformers</title>
		<author>
			<persName><forename type="first">Aurko</forename><surname>Roy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mohammad</forename><surname>Saffar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ashish</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><surname>Grangier</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2003.05997</idno>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
		<title level="m" type="main">Distilbert, a distilled version of bert: smaller, faster, cheaper and lighter</title>
		<author>
			<persName><forename type="first">Victor</forename><surname>Sanh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lysandre</forename><surname>Debut</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Julien</forename><surname>Chaumond</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Thomas</forename><surname>Wolf</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1910.01108</idno>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
		<author>
			<persName><forename type="first">Tal</forename><surname>Schuster</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Adam</forename><surname>Fisch</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jai</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mostafa</forename><surname>Dehghani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dara</forename><surname>Bahri</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yi</forename><surname>Vinh Q Tran</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Donald</forename><surname>Tay</surname></persName>
		</author>
		<author>
			<persName><surname>Metzler</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2207.07061</idno>
		<title level="m">Confident adaptive language modeling</title>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
		<title level="m" type="main">Fast transformer decoding: One write-head is all you need</title>
		<author>
			<persName><forename type="first">Noam</forename><surname>Shazeer</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1911.02150</idno>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Outrageously large neural networks: The sparsely-gated mixture-of-experts layer</title>
		<author>
			<persName><forename type="first">Noam</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Azalia</forename><surname>Mirhoseini</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Krzysztof</forename><surname>Maziarz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andy</forename><surname>Davis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Quoc</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Geoffrey</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jeff</forename><surname>Dean</surname></persName>
		</author>
		<ptr target="http://dblp.uni-trier.de/db/conf/iclr/iclr2017.html#ShazeerMMDLHD17" />
	</analytic>
	<monogr>
		<title level="m">ICLR (Poster). OpenReview.net</title>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<monogr>
		<title level="m" type="main">Megatron-LM: Training multi-billion parameter language models using model parallelism</title>
		<author>
			<persName><forename type="first">Mohammad</forename><surname>Shoeybi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mostofa</forename><surname>Patwary</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Raul</forename><surname>Puri</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Patrick</forename><surname>Legresley</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jared</forename><surname>Casper</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bryan</forename><surname>Catanzaro</surname></persName>
		</author>
		<idno>CoRR, abs/1909.08053</idno>
		<ptr target="http://arxiv.org/abs/1909.08053" />
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<monogr>
		<title level="m" type="main">Using deepspeed and megatron to train megatronturing nlg 530b, a large-scale generative language model</title>
		<author>
			<persName><forename type="first">Shaden</forename><surname>Smith</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mostofa</forename><surname>Patwary</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Brandon</forename><surname>Norick</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Patrick</forename><surname>Legresley</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Samyam</forename><surname>Rajbhandari</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jared</forename><surname>Casper</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhun</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shrimai</forename><surname>Prabhumoye</surname></persName>
		</author>
		<author>
			<persName><forename type="first">George</forename><surname>Zerveas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vijay</forename><surname>Korthikanti</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2201.11990</idno>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b37">
	<monogr>
		<title level="m" type="main">Adaptive attention span in transformers</title>
		<author>
			<persName><forename type="first">Sainbayar</forename><surname>Sukhbaatar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Edouard</forename><surname>Grave</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Piotr</forename><surname>Bojanowski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Armand</forename><surname>Joulin</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1905.07799</idno>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Mobilebert: a compact taskagnostic bert for resource-limited devices</title>
		<author>
			<persName><forename type="first">Zhiqing</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hongkun</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaodan</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Renjie</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yiming</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Denny</forename><surname>Zhou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 58th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="2158" to="2170" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<monogr>
		<author>
			<persName><forename type="first">Romal</forename><surname>Thoppilan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Daniel</forename><forename type="middle">De</forename><surname>Freitas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jamie</forename><surname>Hall</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Noam</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Apoorv</forename><surname>Kulshreshtha</surname></persName>
		</author>
		<author>
			<persName><surname>Heng-Tze</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alicia</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Taylor</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Leslie</forename><surname>Bos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yu</forename><surname>Baker</surname></persName>
		</author>
		<author>
			<persName><surname>Du</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2201.08239</idno>
		<ptr target="https://arxiv.org/pdf/2201.08239" />
		<title level="m">Language models for dialog applications</title>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<ptr target="https://github.com/kingoflolz/mesh-transformer-jax" />
	</analytic>
	<monogr>
		<title level="m">Google Cloud unveils world&apos;s largest publicly available ML hub with Cloud TPU v4, 90% carbon-free energy</title>
		<editor>
			<persName><forename type="first">Ben</forename><surname>Wang</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Aran</forename><surname>Komatsuzaki</surname></persName>
		</editor>
		<editor>
			<persName><surname>Gpt-J-6b</surname></persName>
		</editor>
		<imprint>
			<date type="published" when="2021-05">May 2021</date>
		</imprint>
	</monogr>
	<note>: A 6 Billion Parameter Autoregressive Language Model</note>
</biblStruct>

<biblStruct xml:id="b41">
	<monogr>
		<title level="m" type="main">Hat: Hardware-aware transformers for efficient natural language processing</title>
		<author>
			<persName><forename type="first">Hanrui</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhanghao</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhijian</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Han</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ligeng</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chuang</forename><surname>Gan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Song</forename><surname>Han</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2005.14187</idno>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Overlap communication with dependent computation via decomposition in large deep learning models</title>
		<author>
			<persName><forename type="first">Shibo</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jinliang</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Amit</forename><surname>Sabne</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andy</forename><surname>Davis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Berkin</forename><surname>Ilbeyi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Blake</forename><surname>Hechtman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dehao</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Karthik</forename><surname>Srinivasa Murthy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Marcello</forename><surname>Maggioni</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qiao</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sameer</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tongfei</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuanzhong</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zongwei</forename><surname>Zhou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 28th International Conference on Architectural Support for Programming Languages and Operating Systems (ASPLOS)</title>
		<meeting>the 28th International Conference on Architectural Support for Programming Languages and Operating Systems (ASPLOS)</meeting>
		<imprint>
			<biblScope unit="page">2023</biblScope>
		</imprint>
	</monogr>
	<note>In To appear in the</note>
</biblStruct>

<biblStruct xml:id="b43">
	<monogr>
		<author>
			<persName><surname>Xla</surname></persName>
		</author>
		<author>
			<persName><surname>Xla</surname></persName>
		</author>
		<ptr target="https://www.tensorflow.org/xla" />
		<title level="m">Optimizing compiler for TensorFlow</title>
		<imprint>
			<date type="published" when="2019-09">2019. September-2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<monogr>
		<title level="m" type="main">GSPMD: general and scalable parallelization for ml computation graphs</title>
		<author>
			<persName><forename type="first">Yuanzhong</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hyoukjoong</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dehao</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Blake</forename><surname>Hechtman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yanping</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rahul</forename><surname>Joshi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Maxim</forename><surname>Krikun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dmitry</forename><surname>Lepikhin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andy</forename><surname>Ly</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Marcello</forename><surname>Maggioni</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ruoming</forename><surname>Pang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Noam</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shibo</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tao</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yonghui</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhifeng</forename><surname>Chen</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2105.04663</idno>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Q8bert: Quantized 8bit bert</title>
		<author>
			<persName><forename type="first">Ofir</forename><surname>Zafrir</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Guy</forename><surname>Boudoukh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Peter</forename><surname>Izsak</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Moshe</forename><surname>Wasserblat</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2019 Fifth Workshop on Energy Efficient Machine Learning and Cognitive Computing-NeurIPS Edition (EMC2-NIPS)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="36" to="39" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Lq-nets: Learned quantization for highly accurate and compact deep neural networks</title>
		<author>
			<persName><forename type="first">Dongqing</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiaolong</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dongqiangzi</forename><surname>Ye</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gang</forename><surname>Hua</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European conference on computer vision (ECCV)</title>
		<meeting>the European conference on computer vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="365" to="382" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<monogr>
		<title level="m" type="main">Alpa: Automating inter-and intra-operator parallelism for distributed deep learning</title>
		<author>
			<persName><forename type="first">Lianmin</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhuohan</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hao</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yonghao</forename><surname>Zhuang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhifeng</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yanping</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yida</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuanzhong</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Danyang</forename><surname>Zhuo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Joseph</forename><forename type="middle">E</forename><surname>Gonzalez</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2201.12023</idno>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Learning n:m fine-grained structured sparse neural networks from scratch</title>
		<author>
			<persName><forename type="first">Aojun</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yukun</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Junnan</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jianbo</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhijie</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kun</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wenxiu</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hongsheng</forename><surname>Li</surname></persName>
		</author>
		<ptr target="https://openreview.net/forum?id=K9bw7vqps" />
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
