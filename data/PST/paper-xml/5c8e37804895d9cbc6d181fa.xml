<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Short-term Load Forecasting with Deep Residual Networks</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Kunjin</forename><surname>Chen</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Kunlong</forename><surname>Chen</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Qin</forename><surname>Wang</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Ziyu</forename><surname>He</surname></persName>
						</author>
						<author>
							<persName><roleName>Member, IEEE</roleName><forename type="first">Jun</forename><surname>Hu</surname></persName>
						</author>
						<author>
							<persName><roleName>Fellow, IEEE</roleName><forename type="first">Jinliang</forename><surname>He</surname></persName>
						</author>
						<author>
							<persName><forename type="first">K</forename><forename type="middle">L</forename><surname>Chen</surname></persName>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="department">Department of Electrical Engineering</orgName>
								<orgName type="laboratory">State Key Lab of Power Systems</orgName>
								<orgName type="institution">Tsinghua University</orgName>
								<address>
									<postCode>100084</postCode>
									<settlement>Beijing</settlement>
									<country>P. R. of China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff1">
								<orgName type="department">Department of Electrical Engineering</orgName>
								<orgName type="institution">Beijing Jiaotong University</orgName>
								<address>
									<postCode>100044</postCode>
									<settlement>Beijing</settlement>
									<country>P. R. of China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff2">
								<orgName type="department">Department of Information Technology and Electrical Engineering</orgName>
								<orgName type="institution">ETH Zürich</orgName>
								<address>
									<postCode>8092</postCode>
									<settlement>Zürich</settlement>
									<country key="CH">Switzerland</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff3">
								<orgName type="department">Department of Industrial and Systems Engineering</orgName>
								<orgName type="institution">University of Southern California</orgName>
								<address>
									<postCode>90007</postCode>
									<settlement>Los Angeles</settlement>
									<region>CA</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Short-term Load Forecasting with Deep Residual Networks</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">603FA0290D7DC247C8B81A490BED4F7C</idno>
					<idno type="DOI">10.1109/TSG.2018.2844307</idno>
					<note type="submission">This article has been accepted for publication in a future issue of this journal, but has not been fully edited. Content may change prior to final publication. Citation information: DOI 10.1109/TSG.2018.2844307, IEEE Transactions on Smart Grid 1</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.3" ident="GROBID" when="2023-07-28T13:18+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Short-term load forecasting</term>
					<term>deep learning</term>
					<term>deep residual network</term>
					<term>probabilistic load forecasting</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>We present in this paper a model for forecasting short-term electric load based on deep residual networks. The proposed model is able to integrate domain knowledge and researchers' understanding of the task by virtue of different neural network building blocks. Specifically, a modified deep residual network is formulated to improve the forecast results. Further, a two-stage ensemble strategy is used to enhance the generalization capability of the proposed model. We also apply the proposed model to probabilistic load forecasting using Monte Carlo dropout. Three public datasets are used to prove the effectiveness of the proposed model. Multiple test cases and comparison with existing models show that the proposed model is able to provide accurate load forecasting results and has high generalization capability.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>over-parameterized and the results they had to offer were not convincing enough <ref type="bibr" target="#b10">[11]</ref>. In addition to the fact that the size of neural networks would grow rapidly with the increase in the numbers of input variables, hidden nodes or hidden layers, other criticisms mainly focus on the "overfitting" issue of neural networks <ref type="bibr" target="#b0">[1]</ref>. Nevertheless, different types and variants of neural networks have been proposed and applied to STLF, such as radial basis function (RBF) neural networks <ref type="bibr" target="#b11">[12]</ref>, wavelet neural networks <ref type="bibr" target="#b12">[13,</ref><ref type="bibr" target="#b13">14]</ref>, extreme learning machines (ELM) <ref type="bibr" target="#b14">[15]</ref>, to name a few.</p><p>Recent developments in neural networks, especially deep neural networks, have had great impacts in the fields including computer vision, natural language processing, and speech recognition <ref type="bibr" target="#b15">[16]</ref>. Instead of sticking with fixed shallow structures of neural networks with hand-designed features as inputs, researchers are now able to integrate their understandings of different tasks into the network structures. Different building blocks including convolutional neural networks (CNN) <ref type="bibr" target="#b16">[17]</ref>, and long short-term memory (LSTM) <ref type="bibr" target="#b17">[18]</ref> have allowed deep neural networks to be highly flexible and effective. Various techniques have also been proposed so that neural networks with many layers can be trained effectively without the vanishing of gradients or severe overfitting. Applying deep neural networks to short-term load forecasting is a relatively new topic. Researchers have been using restricted Boltzmann machines (RBM) and feed-forward neural networks with multiple layers in forecasting of demand side loads and natural gas loads <ref type="bibr" target="#b18">[19,</ref><ref type="bibr" target="#b19">20]</ref>. However, these models are increasingly hard to train as the number of layers increases, thus the number of hidden layers are often considerably small (e.g., 2 to 5 layers), which limits the performance of the models.</p><p>In this work, we aim at extending existing structures of ANN for STLF by adopting state-of-the-art deep neural network structures and implementation techniques. Instead of stacking multiple hidden layers between the input and the output, we learn from the residual network structure proposed in <ref type="bibr" target="#b20">[21]</ref> and propose a novel end-to-end neural network model capable of forecasting loads of next 24 hours. An ensemble strategy to combine multiple individual networks is also proposed. Further, we extend the model to probabilistic load forecasting by adopting Monte Carlo (MC) dropout (for a comprehensive review of probabilistic electric load forecasting, the reader is referred to <ref type="bibr" target="#b21">[22,</ref><ref type="bibr" target="#b22">23]</ref>). The contributions of this work are two-folds. First, a fully end-to-end model based on deep residual networks for STLF is proposed. The proposed model does not involve external feature extraction or feature selection algorithms, and only raw data of loads, temperature and information that is readily available are used as inputs. The results show that the forecasting performance can be greatly enhanced by improving the structure of the neural networks and adopting the ensemble strategy. As complicated feature engineering techniques and additional information (e.g., humidity, wind speed, cloud cover, etc.) are not involved, we provide a good benchmark that can be easily compared with. In addition, the building blocks of the proposed model can also be adapted to existing neural-network-based STLF models. Combining the building blocks with existing feature extraction and feature selection techniques is straight-forward and may lead to further improvement in accuracy. Additional data can also be easily incorporated. Second, a new formulation of probabilistic STLF for an ensemble of neural networks is proposed. By using MC dropout, we can directly obtain the probability forecasting results using the models trained for the task of point forecasting.</p><p>The remainder of the paper is organized as follows. In section II, we formulate the proposed model based on deep residual networks. The ensemble strategy, the MC dropout method, as well as the implementation details are also provided. In section III, the results of STLF by the proposed model are presented. We also discuss the performance of the proposed model and compare it with existing methods. Section IV concludes this paper and proposes future works. The source code for the STLF model proposed in this paper is available at https://github.com/yalickj/load-forecasting-resnet.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>II. SHORT-TERM LOAD FORECASTING BASED ON DEEP RESIDUAL NETWORKS</head><p>In this paper, we propose a day-ahead load forecasting model based on deep residual networks. We first formulate the low-level basic structure where the inputs of the model are processed by several fully connected layers to produce preliminary forecasts of 24 hours. The preliminary forecasts are then passed through a deep residual network. After presenting the structure of the deep residual network, some modifications are made to further enhance its learning capability. An ensemble strategy is designed to enhance the generalization capability of the proposed model. The formulation of MC dropout for probabilistic forecasting is also provided.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Model Input and the Basic Structure for Load Forecasting of One Hour</head><p>We use the model with the basic structure to give preliminary forecasts of the 24 hours of the next day. Specifically, the inputs used to forecast the load for the hth hour of the next day, L h , are listed in Table <ref type="table" target="#tab_1">I</ref>. The values for loads and temperatures are normalized by dividing the maximum value of the training dataset. The selected inputs allow us to capture both short-term closeness and long-term trends in the load and temperature time series <ref type="bibr" target="#b23">[24]</ref>. More specifically, we expect that L month that are not available at the time of forecasting, which also helps associate the forecasts of the whole day. Note that the sizes of the above-mentioned inputs can be adjusted flexibly. In addition, one-hot codes for season 1 , weekday/weekend distinction, and holiday/nonholiday 2 distinction are added to help the model capture the periodic and unordinary temporal characteristics of the load time series.</p><p>The structure of the neural network model for load forecasting of one hour is illustrated in Fig. <ref type="figure" target="#fig_1">1</ref> , we forward pass it through two fully-connected layers, the second layer of which is denoted as F C 1 . S and W are concatenated to produce two fully-connected layers, one used as part of the input of F C 1 , the other used as part of the input of F C 2 . H is also connected to F C 2 . In order to produce the output L h , we concatenate F C 1 , F C 2 , and T h , and connect them with a fully-connected layer, which is then connected to L h with another fully connected layer. All fullyconnected layers but the output layer use scaled exponential linear units (SELU) as the activation function.</p><p>The adoption of the ReLU has greatly improved the performance of deep neural networks <ref type="bibr" target="#b24">[25]</ref>. Specifically, ReLU has the form  ReLU(y i ) = max(0, y i )</p><p>where y i is the linear activation of the i-th node of a layer.</p><p>A problem with ReLU is that if a unit can not be activated by any input in the dataset, the gradient-based optimization algorithm is unable to update the weights of the unit, so that the unit will never be activated again. In addition, the network will become very hard to train if a large proportion of the hidden units produce constant 0 gradients <ref type="bibr" target="#b25">[26]</ref>. This problem can be solved by adding a slope to the negative half axis of ReLU. With a simple modification to the formulation of ReLU on the negative half axis, we get PReLU <ref type="bibr" target="#b26">[27]</ref>. The activations of a layer with PReLU as the activation function is obtained by</p><formula xml:id="formula_1">PReLU(y i ) = { y i if y i &gt; 0 β i y i if y i ≤ 0 (2)</formula><p>where β i is the coefficient controlling the slope of β i y i when y i ≤ 0. A further modification to ReLU that induces selfnormalizing properties is provided in <ref type="bibr" target="#b27">[28]</ref>, where the activation function of SELU is given by</p><formula xml:id="formula_2">SELU(y i ) = λ { y i if y i &gt; 0 αe yi -α if y i ≤ 0<label>(3)</label></formula><p>where λ and α are two tunable parameters. It is shown in <ref type="bibr" target="#b27">[28]</ref> that if we have λ ≈ 1.0577 and α ≈ 1.6733, the outputs of the layers in a fully-connected neural network would approach the standard normal distribution when the inputs follow the standard normal distribution. This helps the networks to prevent the problems of vanishing and exploding gradients.</p><p>As previously mentioned, in order to associate the forecasts of the 24 hours of the next day, the corresponding values within</p><formula xml:id="formula_3">L hour h are replaced by {L 1 , • • • , L h-1 } for h &gt; 1.</formula><p>Instead of simply copying the values, we maintain the neural network connections underneath them. Thus, the gradients of subsequent hours can be propagated backward through time. This would help the model adjust the forecast value of each hour given the inputs and forecast values of the rest of the hours.</p><p>We then concatenate {L 1 , • • • , L 24 } as L, which directly becomes the output of the model with the basic structure. Next, we proceed to formulate the deep residual network and add it on top of L. The output of the deep residual network is denoted as ŷ and has the same size of L.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. The Deep Residual Network Structure for Day-ahead Load Forecasting</head><p>In <ref type="bibr" target="#b20">[21]</ref>, an innovative way of constructing deep neural networks for image recognition is proposed. In this paper, the residual block in Fig. <ref type="figure" target="#fig_2">2</ref> is used to build the deep neural network structure. In the residual block, instead of learning a mapping from x to H(x), a mapping from x to F(x, Θ) is learned, where Θ is a set of weights (and biases) associated with the residual block. Thus, the overall representation of the residual block becomes</p><formula xml:id="formula_4">H(x) = F(x, Θ) + x (4)</formula><p>A deep residual network can be easily constructed by stacking a number of residual blocks. We illustrate in Fig. <ref type="figure" target="#fig_3">3</ref> the structure of the deep residual network (ResNet) used for the proposed model. More specifically, if K residual blocks are stacked, the forward propagation of such a structure can be represented by</p><formula xml:id="formula_5">x K = x 0 + K ∑ i=1 F(x i-1 , Θ i-1 )<label>(5)</label></formula><p>where x 0 is the input of the residual network, x K the output of the residual network, and Θ i = {θ i,l | 1≤l≤L } the set of weights associated with the ith residual block, L being the number of layers within the block. The back propagation of the overall loss of the neural network to x 0 can then be calculated as</p><formula xml:id="formula_6">∂L ∂x 0 = ∂L ∂x K (1 + ∂ ∂x 0 K ∑ i=1 F(x i-1 , Θ i-1 ))<label>(6)</label></formula><p>where L is the overall loss of the neural network. The "1" in the equation indicates that the gradients at the output of the network can be directly back-propagated to the input of the network, so that the vanishing of gradients (which is often observed when the gradients at the output have to go through many layers before reaching the input) in the network is much less likely to occur <ref type="bibr" target="#b28">[29]</ref>. As a matter of fact, this equation can also be applied to any pair (x i , x j ) (0 ≤ i &lt; j ≤ K), where x i and x j are the output of the ith residual block (or the input of the network when i = 0), and the jth residual block, respectively.</p><p>In addition to the stacked residual blocks, extra shortcut connections can be added into the deep residual network, as is introduced in <ref type="bibr" target="#b29">[30]</ref>. Concretely, two levels of extra shortcut connections are added to the network. The lower level shortcut connection bypasses several adjacent residual blocks, while the higher level shortcut connection is made between the input and output. If more than one shortcut connection reaches a residual block or the output of the network, the values from the connections are averaged. Note that after  adding the extra shortcut connections, the formulations of the forward-propagation of responses and the back-propagation of gradients are slightly different, but the characteristics of the network that we care about remain unchanged.</p><p>We can further improve the learning ability of ResNet by modifying its structure. Inspired by the convolutional network structures proposed in <ref type="bibr" target="#b30">[31,</ref><ref type="bibr" target="#b31">32]</ref>, we propose the modified deep residual network (ResNetPlus), whose structure is shown in Fig. <ref type="figure" target="#fig_4">4</ref>. First, we add a series of side residual blocks to the model (the residual blocks on the right). Unlike the implementation in <ref type="bibr" target="#b31">[32]</ref>, the input of the side residual blocks is the output of the first residual block on the main path (except for the first side residual block, whose input is the input of the network). The output of each main residual block is averaged with the output of the side residual block in the same layer (indicated by the blue dots on the right). Similar to the densely connected network in <ref type="bibr" target="#b30">[31]</ref>, the outputs of those blue dots are connected to all main residual blocks in subsequent layers. Starting from the second layer, the input of each main residual block is obtained by averaging all connections from the blue dots on the right together with the connection from the input of the network (indicated by the blue dots on the main path). It is expected that the additional side residual blocks and the dense shortcut connections can improve the representation capability and the efficiency of error back-propagation of the network. Later in this paper, we will compare the performance of the basic structure, the basic structure connected with ResNet, and the basic structure connected with ResNetPlus.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. The Ensemble Strategy of Multiple Models</head><p>It is widely acknowledged in the field of machine learning that an ensemble of multiple models has higher generalization capability <ref type="bibr" target="#b15">[16]</ref> than individual models. In <ref type="bibr" target="#b32">[33]</ref>, analysis of neural network ensembles for STLF of office buildings is provided by the authors. Results show that an ensemble of neural networks reduces the variance of performances. A demonstration of the ensemble strategy used in this paper is shown in Fig. <ref type="figure">5</ref>. More specifically, the ensemble strategy consists of two stages.</p><p>The first stage of the strategy takes several snapshots during the training of a single model. In <ref type="bibr" target="#b33">[34]</ref>, the authors show that setting cyclic learning rate schedules for stochastic gradient descent (SGD) optimizer greatly improves the performance of existing deep neural network models. In this paper, as we use Adam (abbreviated from adaptive moment estimation <ref type="bibr" target="#b34">[35]</ref>) as the optimizer, the learning rates for each iteration are decided adaptively. Thus, no learning rate schedules are set by ourselves. This scheme is similar to the NoCycle snapshot ensemble method discussed in <ref type="bibr" target="#b33">[34]</ref>, that is, we take several snapshots of the same model during its training process 1949-3053 (c) 2018 IEEE. Personal use is permitted, but republication/redistribution requires IEEE permission. See http://www.ieee.org/publications_standards/publications/rights/index.html for more information. (e.g., the 4 snapshots along the training process of the model with initial parameters W</p><p>(1) 0 ). As is indicated in Fig. <ref type="figure">5</ref>, the snapshots are taken after an appropriate number of epochs, so that the loss of each snapshot is of similar level.</p><p>We can further ensemble a number of models that are trained independently. This is done by simply re-initializing the parameters of the model (e.g., W</p><p>(1) 0 to W (5) 0 are 5 sets of initial parameters sampled from the same distribution used for initializing the model), which is one of the standard practices of obtaining good ensemble models <ref type="bibr" target="#b35">[36]</ref>. The numbers of snapshots and re-trained models are hyper-parameters, which means they can be tuned using the validation dataset. After we obtain the all the snapshot models, we average the outputs of the models and produce the final forecast.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D. Probabilistic Forecasting Based on Monte Carlo Dropout</head><p>If we look at the deep residual network (either ResNet or ResNetPlus) as an ensemble of relatively shallow networks, the increased width and number of connections in the network can provide more shallow networks to form the ensemble model <ref type="bibr" target="#b31">[32]</ref>. It is expected that the relatively shallow networks themselves can partially capture the nature of the load forecasting task, and multiple shallow networks with the same input can give varied outputs. This indicates that the proposed model have the potential to be used for probabilistic load forecasting.</p><p>Probabilistic forecasting of time series can be fulfilled by capturing the uncertainty within the models <ref type="bibr" target="#b36">[37]</ref>. From a Bayesian probability theory point of view, the predictive probability of a Bayesian neural network can be obtained with</p><formula xml:id="formula_7">p(y * |x * ) = ∫ W p(y * |f W (x * ))p(W|X, Y ) dW<label>(7)</label></formula><p>where X and Y are the observations we use to train f W (•), a neural network with parameters W. The intractable posterior distribution p(W|X, Y ) is often approximated by various inference methods <ref type="bibr" target="#b36">[37]</ref>. In this paper, we use MC dropout <ref type="bibr" target="#b37">[38]</ref> to obtain the probabilistic forecasting uncertainty, which is easy and computationally efficient to implement. Specifically, dropout refers to the technique of randomly dropping out hidden units in a neural network during the training of the network <ref type="bibr" target="#b38">[39]</ref>, and a parameter p is used to control the probability that any hidden neuron is dropped out. If we apply dropout stochastically for M times at test time and collect the outputs of the network, we can approximate the first term of the forecasting uncertainty, which is</p><formula xml:id="formula_8">Var(y * |x * ) = Var [E(y * |W, x * )] + E [Var(y * |W, x * )] = Var(f W (x * )) + σ 2 ≈ 1 M M ∑ m=1 (ŷ * (m) -ȳ * ) 2 + σ 2<label>(8)</label></formula><p>where ŷ * (m) is the mth output we obtain, ȳ * is the mean of all M outputs, and E denotes the expectation operator. The second term, σ 2 , measures the inherent noise for the data generating process. According to <ref type="bibr" target="#b36">[37]</ref>, σ 2 can be estimated using an independent validation dataset. We denote the validation dataset with</p><formula xml:id="formula_9">X ′ = {x ′ 1 , • • • , x ′ V }, Y ′ = {y ′ 1 , • • • , y ′ V }, and estimate σ 2 by σ 2 = β V V ∑ v=1 (y ′ v -f Ŵ (x ′ v )) 2<label>(9)</label></formula><p>where f Ŵ (•) is the model trained on the training dataset and β is a parameter to be estimated also using the validation dataset. We need to extend the above estimation procedure to an ensemble of models. Concretely, for an ensemble of K neural network models of the same structure, we estimate the first term of ( <ref type="formula" target="#formula_8">8</ref>) with a single model of the same structure trained with dropout. The parameter β in ( <ref type="formula" target="#formula_9">9</ref>) is also estimated by the model. More specifically, we find the β that provides the best 90% and 95% interval forecasts on the validation dataset. σ 2 is estimated by replacing f Ŵ (•) in ( <ref type="formula" target="#formula_9">9</ref>) by the ensemble model, f * (•). Note that the estimation of σ 2 is specific to each hour of the day.</p><p>After obtaining the forecasting uncertainty for each forecast, we can calculate the α-level interval with the point forecast, f * (x * ), and its corresponding quantiles to obtain probabilistic forecasting results.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E. Model Design and Implementation Details</head><p>The proposed model consists of the neural network structure for load forecasting of one hour (referred to as the basic structure), the deep residual network (referred to as ResNet) for improving the forecasts of 24 hours, and the modified deep residual network (referred to as ResNetPlus). The configurations of the models are elaborated as follows.</p><p>1) The model with the basic structure: The graphic representation of the model with the basic structure is shown in 2) The deep residual network (ResNet): ResNet is added to the neural network with the basic structure. Each residual block has a hidden layer with 20 hidden nodes and SELU as the activation function. The size of the outputs of the blocks is 24, which is the same as that of the inputs. A total of 30 residual blocks are stacked, forming a 60-layer deep residual network. The second level of shortcut connections is made every 5 residual blocks. The shortcut path of the highest level connects the input and the output of the network.</p><p>3) The modified deep residual network (ResNetPlus): The structure of ResNetPlus follows the structure shown in Fig. <ref type="figure" target="#fig_4">4</ref>. The hyper-parameters inside the residual blocks are the same as ResNet.</p><p>In order to properly train the models, the loss of the model, L, is formulated as the sum of two terms:</p><formula xml:id="formula_10">L = L E + L R (<label>10</label></formula><formula xml:id="formula_11">)</formula><p>where L E measures the error of the forecasts, and L R is an out-of-range penalty term used to accelerate the training process. Specifically, L E is defined as</p><formula xml:id="formula_12">L E = 1 N H N ∑ i=1 H ∑ h=1 ŷ(i,h) -y (i,h) y (i,h)<label>(11)</label></formula><p>where ŷ(i,h) and y (i,h) are the output of the model and the actual normalized load for the hth hour of the ith day, respectively, N the number of data samples, and H the number of hourly loads within a day (i.e., H = 24 in this case). This error measure, widely known as the mean absolute percentage error (MAPE), is also used to evaluate the forecast results of the models. The second term, L R , is calculated as</p><formula xml:id="formula_13">L R = 1 2N N ∑ i=1 max(0, max h ŷ(i,h) -max h y (i,h) ) + max(0, min h y (i,h) -min h ŷ(i,h) )<label>(12)</label></formula><p>This term penalizes the model when the forecast daily load curves are out of the range of the actual load curves, thus accelerating the beginning stage of the training process. When a model is able to produce forecasts with relatively high accuracy, this term serves to emphasize the cost for overestimating the peaks and the valleys of the load curves. All the models are trained using the Adam optimizer with default parameters as suggested in <ref type="bibr" target="#b34">[35]</ref>. The models are implemented using Keras 2.0.2 with Tensorflow 1.0.1 as backend in the Python 3.5 environment <ref type="bibr" target="#b39">[40,</ref><ref type="bibr" target="#b40">41]</ref>. A laptop with Intel R ⃝ Core TM i7-5500U CPUs is used to train the models. Training the ResNetPlus model with data of three years for 700 epochs takes approximately 1.5 hours. When 5 individual models are trained, the total training time is less than 8 hours.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>III. RESULTS AND DISCUSSION</head><p>In this section, we use the North-American Utility dataset <ref type="foot" target="#foot_0">3</ref>and the ISO-NE dataset <ref type="foot" target="#foot_1">4</ref> to verify the effectiveness of the proposed model. As we use actual temperature as the input, we further modify the temperature values to evaluate the performance of the proposed model. Results of probabilistic 2.04 -ESN <ref type="bibr" target="#b44">[45]</ref> 2.37 2.53 SSA-SVR <ref type="bibr" target="#b0">[1]</ref> 1.99 2.03 WT-ELM-MABC <ref type="bibr" target="#b45">[46]</ref> 1.87 1.95 CLPSO-MA-SVR <ref type="bibr" target="#b46">[47]</ref> 1.80 1.85 WT-ELM-LM <ref type="bibr" target="#b47">[48]</ref> 1 forecasting on the North-American Utility dataset and the GEFCom2014 dataset <ref type="bibr" target="#b41">[42]</ref> are also provided.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Performance of the Proposed model on the North-American Utility Dataset</head><p>The first test case uses the North-American Utility dataset. This dataset contains load and temperature data at one-hour resolution for a north-American utility. The dataset covers the time range between January 1st, 1985 and October 12th, 1992. The data of the two-year period prior to October 12th, 1992 is used as the test set, and the data prior to the test set is used for training the model. More specifically, two starting dates, namely, January 1st, 1986, and January 1st, 1988, are used for the training sets. As the latter starting date is used in experiments in the literature, we tune the hyper-parameters using the last 10% of the training set with this starting date <ref type="foot" target="#foot_2">5</ref> . The model trained with the training set containing 2 years of extra data has the same hyper-parameters.</p><p>Before reporting the performance of the ensemble model obtained by combining multiple individual models, we first look at the performance of the three models mentioned in section II. The test losses of the three models are shown in Fig. <ref type="figure" target="#fig_7">6</ref> (the models are trained with the training set starting with January 1st, 1988). In order to yield credible results, we train each model 5 times and average the losses to obtain the solid lines in the figure. The coloured areas indicate the range between one standard deviation above and below the average losses. It is observed in the figure that ResNet is able to improve the performance of the model, and further reduction in loss can be achieved when ResNetPlus is implemented. Note that the results to be reported in this paper are all obtained with the ensemble model. For simplicity, the ensemble model with the basic structure connected with ResNetPlus is referred to as "the ResNetPlus model" hereinafter.</p><p>We compare the results of the proposed ResNetPlus model with existing models proposed in <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b42">[43]</ref><ref type="bibr" target="#b43">[44]</ref><ref type="bibr" target="#b44">[45]</ref><ref type="bibr" target="#b45">[46]</ref><ref type="bibr" target="#b46">[47]</ref><ref type="bibr" target="#b47">[48]</ref>, as is shown in Table <ref type="table" target="#tab_2">II</ref>. In order to estimate the performance of the models when forecast temperature is used, we also add a Gaussian noise with mean 0 o F, and standard deviation 1 o F to the temperature input and report the MAPE in this case. It is seen in the table that the proposed model outperforms existing models which highly depend on external feature extraction, feature selection, or hyper-parameter optimization techniques. The proposed model also has a lower increase of MAPE when modified temperature is applied. In addition, the test loss can be further reduced when more data is added to the training set.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Performance of the Proposed Model on the ISO-NE Dataset</head><p>The second task of the paper is to examine the generalization capability of the proposed model. To this end, we use the majority of the hyper-parameters of ResNetPlus tuned with the North-American Utility dataset to train load forecasting models for the ISO-NE dataset (The time range of the dataset is between March 2003 and December 2014). Here, the ResNetPlus structure has 10 layers on the main path.</p><p>The first test case is to predict the daily loads of the year 2006 in the ISO-NE dataset. For the proposed ResNetPlus model, the training period is from June 2003 to December 2005 <ref type="foot" target="#foot_3">6</ref> (we reduce the size of L month h and T month h to 3 so that more training samples can be used, and the rest of the hyper-parameters are unchanged). In comparison, the similar day-based wavelet neural network (SIWNN) model in <ref type="bibr" target="#b12">[13]</ref> is trained with data from 2003 to 2005, while the models proposed in <ref type="bibr" target="#b48">[49]</ref> and <ref type="bibr" target="#b45">[46]</ref>     <ref type="table" target="#tab_6">IV</ref>, we report the performance of the proposed model and compare it with models mentioned in <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b48">49,</ref><ref type="bibr" target="#b49">50]</ref>. Results show that the proposed ResNetPlus model outperforms existing models with respect to the overall MAPE for the two years, and an improvement of 8.9% is achieved for the year 2011. Note that all the existing models are specifically tuned on the ISO-NE dataset for the period from 2004 to 2009, while the design of the proposed ResNetPlus model is directly implemented without any tuning.</p><p>As we use actual temperature values for the input of the proposed model (except for the "modified temperature" case of North-American Utility dataset), the results we have obtained previously provide us with an estimated upper bound of the performance of the model. Thus, we need to further analyze how the proposed model would perform when forecast temperature data is used, and whether the ensemble model is more robust to noise in forecast weather. We follow the way of modifying temperature values introduced in <ref type="bibr" target="#b42">[43]</ref>, and consider three cases of temperature modification:   the standard deviation of case 1 to 3 o F. For all three cases, we repeat the trials 5 times and calculate the means and standard deviations of increased MAPE compared with the case where actual temperature data is used.</p><p>The results of increased test MAPEs for the year 2006 with modified temperature values are shown in Fig. <ref type="figure" target="#fig_8">7</ref>. We compare the performance of the proposed ResNetPlus model (which is an ensemble of 15 single snapshot models) with a single snapshot model trained with 700 epochs. As can be seen in the figure, the ensemble strategy greatly reduces the increase of MAPE, especially for case 1, where the increase of MAPE is 0.0168%. As the reported smallest increase of MAPE for case 1 in <ref type="bibr" target="#b0">[1]</ref> is 0.04%, it is reasonable to conclude that the proposed model is robust against the uncertainty of temperature for case 1 (as we use a different dataset here, the results are not directly comparable). Is is also observed that the ensemble strategy is able to reduce the standard deviation of multiple trials. This also indicates the higher generalization capability of the proposed model with the ensemble strategy.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Probabilistic Forecasting for the Ensemble Model</head><p>We first use the North-American Utility dataset to demonstrate the probabilistic STLF by MC dropout. The last year of the dataset is used as the test set and the previous year is used for validation. Dropout with p = 0.1 is added to the previously implemented ensemble model<ref type="foot" target="#foot_6">7</ref> except for the input layer and the output layer (dropout with p ranging from 0.05 and 0.2 produce similar results, similar to the results reported in <ref type="bibr" target="#b37">[38]</ref>). The first term in <ref type="bibr" target="#b7">(8)</ref> and is estimated by a single model trained with 500 epochs (with M = 100 for ( <ref type="formula" target="#formula_8">8</ref>) and p = 0.1), and the estimated value of β is 0.79. The empirical coverages produced by the proposed model with respect to different z-scores are listed in Table <ref type="table" target="#tab_7">V</ref>, and an illustration of the 95% prediction intervals for two weeks in 1992 is provided in Fig. <ref type="figure">8</ref>. The results show that the proposed model with MC dropout is able to give satisfactory empirical coverages for different intervals.</p><p>In order to quantify the performance of the probabilistic STLF by MC dropout, we adopt the pinball loss and Winkler score mentioned in <ref type="bibr" target="#b22">[23]</ref> and use them to assess the proposed method in terms of coverage rate and interval width. Specifically, the pinball loss is averaged over all quantiles and hours in the prediction range, and the Winkler scores are averaged over all the hours of the year in the test set. We implement the ResNetPlus model<ref type="foot" target="#foot_7">8</ref> on the GEFCom2014 dataset and compare the results with those reported in <ref type="bibr" target="#b22">[23,</ref><ref type="bibr" target="#b50">51]</ref>. Following the setting in <ref type="bibr" target="#b22">[23]</ref>, the load and temperature data from 2006 to 2009 is used to train the proposed model, the data of the year 2010 is used for validation, and the test results are obtained using data of the year 2011. The temperature values used for the input of the model are calculated as the mean of the temperature values of all 25 weather stations in the dataset.</p><p>In Table <ref type="table" target="#tab_8">VI</ref>, we present the values of pinball loss and Winkler scores for the proposed model and the models in <ref type="bibr" target="#b22">[23,</ref><ref type="bibr" target="#b50">51]</ref> for the year of 2011 in the GEFCom2014 dataset. The Lasso method in <ref type="bibr" target="#b50">[51]</ref> serves as a benchmark for methods that build regression models on the input data, and the quantile regression averaging (QRA) method in <ref type="bibr" target="#b22">[23]</ref> builds quantile regression models on sister point forecasts (the row of Ind stands for the performance of a single model). It can be seen in Table <ref type="table" target="#tab_8">VI</ref> that the proposed ResNetPlus model is able to provide improved probabilistic forecasting results compared with existing methods in terms of the pinball loss and two Winkler scores. As we obtain the probabilistic forecasting results by sampling the trained neural networks with MC Fig. <ref type="figure">8</ref>. Actual load and 95% prediction intervals for a winter week (left) and a summer week (right) of 1992 for the North-American Utility dataset. The two weeks start with February 3rd, 1992, and July 6th, 1992, respectively. dropout, we can conclude that the proposed model is good at capturing the uncertainty of the task of STLF.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>IV. CONCLUSION AND FUTURE WORK</head><p>We have proposed an STLF model based on deep residual networks in this paper. The low-level neural network with the basic structure, the ResNetPlus structure, and the twostage ensemble strategy enable the proposed model to have high accuracy as well as satisfactory generalization capability. Two widely acknowledged public datasets are used to verify the effectiveness of the proposed model with various test cases. Comparisons with existing models have shown that the proposed model is superior in both forecasting accuracy and robustness to temperature variation. We have also shown that the proposed model can be directly used for probabilistic forecasting when MC dropout is adopted.</p><p>A number of paths for further work are attractive. As we have only scratched the surface of state-of-the-art of deep neural networks, we may apply more building blocks of deep neural networks (e.g., CNN or LSTM) into the model to enhance its performance. In addition, we will further investigate the implementation of deep neural works for probabilistic STLF and make further comparisons with existing methods.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head></head><label></label><figDesc>h , L week h , T month h and T week h can help the model identify long-term trends in the time series (the days of the same day-of-week index as the next day are selected as they are more likely to have similar load characteristics [13]), while L day h and T day h are able to provide short-term closeness and characteristics. The input L hour h feeds the loads of the most recent 24 hours to the model. Forecast loads are used to replace the values in L hour h</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 1 .</head><label>1</label><figDesc>Fig. 1. The structure of the neural network model for load forecasting of one hour.</figDesc><graphic coords="3,51.19,55.61,246.66,136.82" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 2 .</head><label>2</label><figDesc>Fig. 2. The building block of the deep residual network. SELU is used as the activation function between two linear layers.</figDesc><graphic coords="4,129.14,56.22,90.84,130.73" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Fig. 3 .</head><label>3</label><figDesc>Fig. 3. An illustration of the deep residual network (ResNet) structure. More shortcut connections are made in addition to the ones within the blocks. In this figure, every three residual blocks has one shortcut connection and another shortcut connection is made from the input to the output. Each round node averages all of its inputs.</figDesc><graphic coords="4,131.97,231.65,85.30,257.93" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Fig. 4 .</head><label>4</label><figDesc>Fig. 4. An illustration of the modified deep residual network (ResNetPlus) structure. The blue dots in the figure average their inputs, and the outputs are connected to subsequent residual blocks.</figDesc><graphic coords="4,338.29,56.50,198.23,204.99" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head></head><label></label><figDesc>Fig. 5. A demonstration of the ensemble strategy used in this paper. The snapshot models are taken where the slope of validation loss is considerably small.</figDesc><graphic coords="5,54.02,55.74,241.13,143.01" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Fig. 1 .</head><label>1</label><figDesc>Each fully-connected layer for [L day h nodes, while the fully-connected layers for [S, W ] have 5 hidden nodes. F C 1 , F C 2 , and the fully-connected layer before L h have 10 hidden nodes. All but the output layer use SELU as the activation function.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Fig. 6 .</head><label>6</label><figDesc>Fig. 6. Test losses of the neural network with the basic structure (Basic), the model with the deep residual network (Basic + ResNet), and the model with the modified deep residual network (Basic + ResNetPlus). Each model is trained 5 times with shuffled weight initialization. The solid lines are the average losses, and the standard deviation above and below the average losses are indicated by coloured areas.</figDesc><graphic coords="7,75.28,56.08,198.40,158.49" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Fig. 7 .</head><label>7</label><figDesc>Fig. 7. The comparison of the proposed model with the ensemble strategy and the proposed model without ensemble when different cases of modified temperature are applied. The model without ensemble is a single ResNetPlus model trained with 700 epochs.</figDesc><graphic coords="8,75.28,56.09,198.40,158.41" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>deviation 1 o</head><label>1</label><figDesc>F to the original temperature values before normalization. • Case 2: add Gaussian noise with mean 0 o F, and change the standard deviation of case 1 to 2 o F. • Case 3: add Gaussian noise with mean 0 o F, and change</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>TABLE I INPUTS</head><label>I</label><figDesc>FOR THE LOAD FORECAST OF THE hTH HOUR OF THE NEXT DAY</figDesc><table><row><cell>Input</cell><cell cols="2">Size Description of the Inputs</cell></row><row><cell>L month h</cell><cell>6</cell><cell>Loads of the hth hour of the days that are 4, 8, 12, 16, 20, and 24 weeks prior to the next day</cell></row><row><cell>L week h</cell><cell>4</cell><cell>Loads of the hth hour of the days that are 1, 2, 3, and 4 weeks prior to the next day</cell></row><row><cell>L day h</cell><cell>7</cell><cell>Loads of the hth hour of every day of the week prior to the next day</cell></row><row><cell>L hour h</cell><cell>24</cell><cell>Loads of the most recent 24 hours prior to the hth hour of the next day</cell></row><row><cell>T month h T week h T day h</cell><cell>6 4 7</cell><cell>Temperature values of the same hours as L month h Temperature values of the same hours as L week h Temperature values of the same hours as L day h</cell></row><row><cell>T h</cell><cell>1</cell><cell>The actual temperature of the hth hour of the next day</cell></row><row><cell>S</cell><cell>4</cell><cell>One-hot code for season</cell></row><row><cell>W</cell><cell>2</cell><cell>One-hot code for weekday/weekend distinction</cell></row><row><cell>H</cell><cell>2</cell><cell>One-hot code for holiday/non-holiday distinction</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>TABLE II COMPARISON</head><label>II</label><figDesc>OF THE PROPOSED RESNETPLUS MODEL WITH EXISTING MODELS ON THE NORTH-AMERICAN UTILITY DATASET WITH RESPECTTO MAPE (%)</figDesc><table><row><cell>Model</cell><cell>Actual temperature</cell><cell>Modified temperature</cell></row><row><cell>WT-NN [43]</cell><cell>2.64</cell><cell>2.84</cell></row><row><cell>WT-NN [44]</cell><cell></cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head></head><label></label><figDesc>use data from March 2003 to December 2005 (both models use past loads up to 200 hours prior to the hour to be predicted). The results of MAPEs with respect to each month are listed in Table III. The MAPEs for the 12 months in 2006 are not explicitly reported in [49]. It is seen in the table that the proposed ResNetPlus model</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>TABLE III MAPES</head><label>III</label><figDesc>(%) OF THE PROPOSED RESNETPLUS MODEL FOR THE ISO-NE DATASET IN 2006 AND A COMPARISON WITH EXISTING MODELS</figDesc><table><row><cell></cell><cell>SIWNN</cell><cell>WT-ELM-</cell><cell>WT-ELM-</cell><cell>Proposed</cell></row><row><cell></cell><cell>[13]</cell><cell>PLSR</cell><cell>MABC</cell><cell>model</cell></row><row><cell></cell><cell></cell><cell>[49]</cell><cell>[46]</cell><cell></cell></row><row><cell>Jan</cell><cell>1.60</cell><cell>-</cell><cell>1.52</cell><cell>1.619</cell></row><row><cell>Feb</cell><cell>1.43</cell><cell>-</cell><cell>1.28</cell><cell>1.308</cell></row><row><cell>Mar</cell><cell>1.47</cell><cell>-</cell><cell>1.37</cell><cell>1.172</cell></row><row><cell>Apr</cell><cell>1.26</cell><cell>-</cell><cell>1.05</cell><cell>1.340</cell></row><row><cell>May</cell><cell>1.61</cell><cell>-</cell><cell>1.23</cell><cell>1.322</cell></row><row><cell>Jun</cell><cell>1.79</cell><cell>-</cell><cell>1.54</cell><cell>1.411</cell></row><row><cell>Jul</cell><cell>2.70</cell><cell>-</cell><cell>2.07</cell><cell>1.962</cell></row><row><cell>Aug</cell><cell>2.62</cell><cell>-</cell><cell>2.06</cell><cell>1.549</cell></row><row><cell>Sep</cell><cell>1.48</cell><cell>-</cell><cell>1.41</cell><cell>1.401</cell></row><row><cell>Oct</cell><cell>1.38</cell><cell>-</cell><cell>1.23</cell><cell>1.293</cell></row><row><cell>Nov</cell><cell>1.39</cell><cell>-</cell><cell>1.33</cell><cell>1.507</cell></row><row><cell>Dec</cell><cell>1.75</cell><cell>-</cell><cell>1.65</cell><cell>1.465</cell></row><row><cell cols="2">Average 1.75</cell><cell>1.489</cell><cell>1.48</cell><cell>1.447</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>TABLE IV COMPARISON</head><label>IV</label><figDesc>OF THE PROPOSED RESNETPLUS MODEL WITH EXISTING MODELS ON THE ISO-NE DATASET FOR 2010 AND 2011</figDesc><table><row><cell>Model</cell><cell>2010</cell><cell>2011</cell></row><row><cell>RBFN-ErrCorr original [50]</cell><cell>1.80</cell><cell>2.02</cell></row><row><cell>RBFN-ErrCorr modified [12]</cell><cell>1.75</cell><cell>1.98</cell></row><row><cell>WT-ELM-PLSR [49]</cell><cell>1.50</cell><cell>1.80</cell></row><row><cell>Proposed model</cell><cell>1.50</cell><cell>1.64</cell></row><row><cell cols="3">has the lowest overall MAPE for the year 2006. For some</cell></row><row><cell cols="3">months, however, the WT-ELM-MABC model proposed in</cell></row><row><cell cols="3">[46] produces better results. Nevertheless, as most of the</cell></row><row><cell cols="3">hyper-parameters are not tuned on the ISO-NE dataset, we</cell></row><row><cell cols="3">can conclude that the proposed model has good generalization</cell></row><row><cell cols="2">capability across different datasets.</cell><cell></cell></row><row><cell cols="3">We further test the generalization capability of the proposed</cell></row><row><cell cols="3">ResNetPlus model on data of the years 2010 and 2011. The</cell></row><row><cell cols="3">same model for the year 2006 is used for this test case,</cell></row><row><cell cols="3">and historical data from 2004 to 2009 is used to train the</cell></row><row><cell>model. In Table</cell><cell></cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>TABLE V EMPIRICAL</head><label>V</label><figDesc>COVERAGES OF THE PROPOSED MODEL WITH MC DROPOUT</figDesc><table><row><cell>z-score</cell><cell>Expected Coverage</cell><cell>Empirical Coverage</cell></row><row><cell>1.000</cell><cell>68.27%</cell><cell>71.14%</cell></row><row><cell>1.280</cell><cell>≈ 80.00%</cell><cell>81.72%</cell></row><row><cell>1.645</cell><cell>≈ 90.00%</cell><cell>90.02%</cell></row><row><cell>1.960</cell><cell>≈ 95.00%</cell><cell>94.15%</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>TABLE VI COMPARISON</head><label>VI</label><figDesc>OF PROBABILISTIC FORECASTING PERFORMANCE MEASURES FOR THE YEAR 2011 IN THE GEFCOM2014 DATASET</figDesc><table><row><cell>Model</cell><cell>Pinball</cell><cell>Winkler</cell><cell>Winkler</cell></row><row><cell></cell><cell></cell><cell>(50%)</cell><cell>(90%)</cell></row><row><cell>Lasso [51]</cell><cell>7.44</cell><cell>-</cell><cell>-</cell></row><row><cell>Ind [23]</cell><cell>3.22</cell><cell>26.35</cell><cell>56.38</cell></row><row><cell>QRA [23]</cell><cell>2.85</cell><cell>25.04</cell><cell>55.85</cell></row><row><cell>Proposed model</cell><cell>2.52</cell><cell>22.41</cell><cell>42.63</cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3" xml:id="foot_0"><p>Available at https://class.ee.washington.edu/555/el-sharkawi.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4" xml:id="foot_1"><p>Available at https://www.iso-ne.com/isoexpress/web/reports/load-anddemand.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="5" xml:id="foot_2"><p>For this dataset, 4 snapshots are taken between 1200 to 1350 epochs for 8 individual models. For the basic structure, all layers except the input and the output layers are shared for the 24 hours (sharing weights for 24 hours is only implemented in this test case). The ResNetPlus model has 30 layers on the main path.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="6" xml:id="foot_3"><p>The training dataset is used to determine how the snapshots are taken for the ensemble model for the ISO-NE dataset. For each implementation, 5 individual models are trained, and the snapshots are taken at 600, 650, and</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="700" xml:id="foot_4"><p>epochs.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" xml:id="foot_5"><p>• Case 1: add Gaussian noise with mean 0 o F, and standard</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="7" xml:id="foot_6"><p>the model implemented here uses ResNet instead of ResNetPlus, and the information of season, weekday/weekend distinction, and holiday/non-holiday distinction is not used. In addition, the activation function used for the residual blocks is ReLU.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="8" xml:id="foot_7"><p>Five individual models are trained with a dropout rate of 0.1 and 6 snapshots are taken from 100 epochs to 350 epochs. M is set to 100 for MC dropout and the first term in (7) is estimated by a single model trained with 100 epochs. The estimated value of β is 0.77.</p></note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">A strategy for short-term load forecasting by support vector regression machines</title>
		<author>
			<persName><forename type="first">E</forename><surname>Ceperic</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Ceperic</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Baric</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Power Syst</title>
		<imprint>
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="4356" to="4364" />
			<date type="published" when="2013-11">Nov. 2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Short-term load forecasting for the holidays using fuzzy linear regression method</title>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">B</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><forename type="middle">S</forename><surname>Baek</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">H</forename><surname>Hong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Jang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Power Syst</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="96" to="101" />
			<date type="published" when="2005-02">Feb. 2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Nonparametric regression based short-term load forecasting</title>
		<author>
			<persName><forename type="first">W</forename><surname>Charytoniuk</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">S</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">V</forename><surname>Olinda</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Power Syst</title>
		<imprint>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="725" to="730" />
			<date type="published" when="1998-08">Aug. 1998</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Electric load forecasting based on locally weighted support vector regression</title>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">E</forename><surname>Elattar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Goulermas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><forename type="middle">H</forename><surname>Wu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Syst., Man, Cybern., Part C</title>
		<imprint>
			<biblScope unit="volume">40</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="438" to="447" />
			<date type="published" when="2010-02">Feb. 2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Short-term electricity demand forecasting using double seasonal exponential smoothing</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">W</forename><surname>Taylor</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of the Operational Research Society</title>
		<imprint>
			<biblScope unit="volume">54</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="799" to="805" />
			<date type="published" when="2003-07">Jul. 2003</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Short-term transmission-loss forecast for the slovenian transmission power system based on a fuzzy-logic decision approach</title>
		<author>
			<persName><forename type="first">M</forename><surname>Rejc</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Pantos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Power Syst</title>
		<imprint>
			<biblScope unit="volume">26</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="1511" to="1521" />
			<date type="published" when="2011-01">Jan. 2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Load forecasting</title>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">A</forename><surname>Feinberg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Genethliou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="s">in Applied Mathematics for Restructured Electric Power Systems</title>
		<editor>
			<persName><forename type="first">J</forename><forename type="middle">H</forename><surname>Chow</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">F</forename><forename type="middle">F</forename><surname>Wu</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">J</forename><surname>Momoh</surname></persName>
		</editor>
		<imprint>
			<biblScope unit="page" from="269" to="285" />
			<date type="published" when="2005">2005</date>
			<publisher>Springer</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">A comparison of univariate methods for forecasting electricity demand up to a day ahead</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">W</forename><surname>Taylor</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">M D</forename><surname>Menezes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">E</forename><surname>Mcsharry</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Forecasting</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1" to="16" />
			<date type="published" when="2006-03">Jan./Mar. 2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Electric load forecasting methods: Tools for decision making</title>
		<author>
			<persName><forename type="first">H</forename><surname>Hahn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Meyer-Nieberg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Pickl</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">European Journal of Operational Research</title>
		<imprint>
			<biblScope unit="volume">199</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="902" to="907" />
			<date type="published" when="2009-12">Dec. 2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Review of smart meter data analytics: Applications, methodologies, and challenges</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Hong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Kang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Smart Grid</title>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Neural networks for short-term load forecasting: A review and evaluation</title>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">S</forename><surname>Hippert</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">E</forename><surname>Pedreira</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">C</forename><surname>Souza</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Power Syst</title>
		<imprint>
			<biblScope unit="volume">16</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="44" to="55" />
			<date type="published" when="2001-02">Feb. 2001</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">A novel RBF training algorithm for short-term electric load forecasting and comparative studies</title>
		<author>
			<persName><forename type="first">C</forename><surname>Cecati</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Kolbusz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Różycki</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Siano</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">M</forename><surname>Wilamowski</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Ind. Electron</title>
		<imprint>
			<biblScope unit="volume">62</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page" from="6519" to="6529" />
			<date type="published" when="2015-04">Apr. 2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Short-term load forecasting: similar day-based wavelet neural networks</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Power Syst</title>
		<imprint>
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="322" to="330" />
			<date type="published" when="2010-01">Jan. 2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Short-term load forecasting: Multi-level wavelet neural networks with holiday corrections</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">B</forename><surname>Luh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Bomgardner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">H</forename><surname>Beerel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Power &amp; Energy Society General Meeting</title>
		<imprint>
			<date type="published" when="2009">2009. 2009</date>
			<biblScope unit="page" from="1" to="7" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Short-term load forecasting of australian national electricity market by an ensemble model of extreme learning machine</title>
		<author>
			<persName><forename type="first">R</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><forename type="middle">Y</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Meng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">P</forename><surname>Wong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IET Generation, Transmission &amp; Distribution</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="391" to="397" />
			<date type="published" when="2013-06">Jun. 2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title/>
		<author>
			<persName><forename type="first">I</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Courville</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Deep</forename><surname>Learning</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016">2016</date>
			<publisher>MIT Press</publisher>
			<pubPlace>Cambridge, MA, USA</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Imagenet classification with deep convolutional neural networks</title>
		<author>
			<persName><forename type="first">A</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2012">2012</date>
			<biblScope unit="page" from="1097" to="1105" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Long short-term memory</title>
		<author>
			<persName><forename type="first">S</forename><surname>Hochreiter</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Schmidhuber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural Computation</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="1735" to="1780" />
			<date type="published" when="1997-11">Nov. 1997</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Deep neural network based demand side short term load forecasting</title>
		<author>
			<persName><forename type="first">S</forename><surname>Ryu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Noh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Kim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Energies</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page">3</biblScope>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Deep neural network regression for short-term load forecasting of natural gas</title>
		<author>
			<persName><forename type="first">G</forename><surname>Merkel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">J</forename><surname>Povinelli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">H</forename><surname>Brown</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">37th Annual International Symposium on Forecasting</title>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="770" to="778" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Probabilistic electric load forecasting: A tutorial review</title>
		<author>
			<persName><forename type="first">T</forename><surname>Hong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Fan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Forecasting</title>
		<imprint>
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="914" to="938" />
			<date type="published" when="2016-09">Jul./Sep. 2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Probabilistic load forecasting via quantile regression averaging on sister forecasts</title>
		<author>
			<persName><forename type="first">B</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Nowotarski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Hong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Weron</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Smart Grid</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="730" to="737" />
			<date type="published" when="2017-06">Jun. 2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">DNN-based prediction model for spatio-temporal data</title>
		<author>
			<persName><forename type="first">J</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Yi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 24th ACM SIGSPATIAL International Conference on Advances in Geographic Information Systems</title>
		<meeting>the 24th ACM SIGSPATIAL International Conference on Advances in Geographic Information Systems</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page">92</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Improving deep neural networks for LVCSR using rectified linear units and dropout</title>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">E</forename><surname>Dahl</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">N</forename><surname>Sainath</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Acoustics, Speech and Signal Processing (ICASSP)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2013">2013. 2013</date>
			<biblScope unit="page" from="8609" to="8613" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Rectifier nonlinearities improve neural network acoustic models</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">L</forename><surname>Maas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">Y</forename><surname>Hannun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">Y</forename><surname>Ng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Conference on Machine Learning</title>
		<meeting>the International Conference on Machine Learning</meeting>
		<imprint>
			<date type="published" when="2013">2013</date>
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Delving deep into rectifiers: Surpassing human-level performance on imagenet classification</title>
		<author>
			<persName><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="1026" to="1034" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Selfnormalizing neural networks</title>
		<author>
			<persName><forename type="first">G</forename><surname>Klambauer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Unterthiner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Mayr</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Hochreiter</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="972" to="981" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Identity mappings in deep residual networks</title>
		<author>
			<persName><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="630" to="645" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Residual networks of residual networks: Multilevel residual networks</title>
		<author>
			<persName><forename type="first">K</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Liu</surname></persName>
		</author>
		<idno type="DOI">10.1109/TCSVT.2017.2654543</idno>
	</analytic>
	<monogr>
		<title level="m">IEEE Transactions on Circuits and Systems for Video Technology</title>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
	<note>to be published</note>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Densely connected convolutional networks</title>
		<author>
			<persName><forename type="first">G</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">Q</forename><surname>Weinberger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Van Der Maaten</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
		<title level="m" type="main">On the connection of deep fusion to ensembling</title>
		<author>
			<persName><forename type="first">L</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Tu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Zeng</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1611.07718</idno>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Short-term load forecasting with neural network ensembles: A comparative study [application notes]</title>
		<author>
			<persName><forename type="first">M</forename><surname>De Felice</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Yao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Comput. Intell. Mag</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="47" to="56" />
			<date type="published" when="2011-07">Jul. 2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
		<author>
			<persName><forename type="first">G</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Pleiss</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">E</forename><surname>Hopcroft</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">Q</forename><surname>Weinberger</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1704.00109</idno>
		<title level="m">Snapshot ensembles: Train 1, get M for free</title>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b34">
	<monogr>
		<title level="m" type="main">Adam: A method for stochastic optimization</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">P</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Ba</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1412.6980</idno>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b35">
	<monogr>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">R</forename><surname>Webb</surname></persName>
		</author>
		<title level="m">Statistical Pattern Recognition</title>
		<meeting><address><addrLine>Chiechester, UK</addrLine></address></meeting>
		<imprint>
			<publisher>John Wiley &amp; Sons</publisher>
			<date type="published" when="2003">2003</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<monogr>
		<title level="m" type="main">Deep and confident prediction for time series at uber</title>
		<author>
			<persName><forename type="first">L</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Laptev</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1709.01907</idno>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Dropout as a bayesian approximation: Representing model uncertainty in deep learning</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Gal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Ghahramani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 33rd International Conference on Machine Learning</title>
		<meeting>the 33rd International Conference on Machine Learning</meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="1050" to="1059" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Dropout: a simple way to prevent neural networks from overfitting</title>
		<author>
			<persName><forename type="first">N</forename><surname>Srivastava</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Salakhutdinov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1929" to="1958" />
			<date type="published" when="2014-01">Jan. 2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<monogr>
		<title level="m" type="main">Keras</title>
		<author>
			<persName><forename type="first">F</forename><surname>Chollet</surname></persName>
		</author>
		<ptr target="https://github.com/fchollet/keras" />
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Tensorflow: A system for large-scale machine learning</title>
		<author>
			<persName><forename type="first">M</forename><surname>Abadi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 12th USENIX Symposium on Operating Systems Design and Implementation</title>
		<meeting>the 12th USENIX Symposium on Operating Systems Design and Implementation</meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="volume">16</biblScope>
			<biblScope unit="page" from="265" to="283" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Probabilistic energy forecasting: Global energy forecasting competition 2014 and beyond</title>
		<author>
			<persName><forename type="first">T</forename><surname>Hong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Pinson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Zareipour</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Troccoli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">J</forename><surname>Hyndman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Forecasting</title>
		<imprint>
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="896" to="913" />
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Feature extraction via multiresolution analysis for short-term load forecasting</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">R</forename><surname>Reis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">A</forename><surname>Da</surname></persName>
		</author>
		<author>
			<persName><surname>Silva</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Power Syst</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="189" to="198" />
			<date type="published" when="2005-01">Jan. 2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Short-term load forecasting of power systems by combination of wavelet transform and neuro-evolutionary algorithm</title>
		<author>
			<persName><forename type="first">N</forename><surname>Amjady</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Keynia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Energy</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="46" to="57" />
			<date type="published" when="2009-01">Jan. 2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Application of echo state networks in short-term electric load forecasting</title>
		<author>
			<persName><forename type="first">A</forename><surname>Deihimi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Showkati</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Energy</title>
		<imprint>
			<biblScope unit="volume">39</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="327" to="340" />
			<date type="published" when="2012-03">Mar. 2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Short-term load forecasting by wavelet transform and evolutionary extreme learning machine</title>
		<author>
			<persName><forename type="first">S</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Goel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="s">Electric Power Systems Research</title>
		<imprint>
			<biblScope unit="volume">122</biblScope>
			<biblScope unit="page">96103</biblScope>
			<date type="published" when="2015-05">May 2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Comprehensive learning particle swarm optimization based memetic algorithm for model selection in shortterm load forecasting using support vector regression</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Bao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Xiong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Applied Soft Computing</title>
		<imprint>
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="page" from="15" to="25" />
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">A novel wavelet-based ensemble method for short-term load forecasting with hybrid neural networks and feature selection</title>
		<author>
			<persName><forename type="first">S</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Goel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Power Systems</title>
		<imprint>
			<biblScope unit="volume">31</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="1788" to="1798" />
			<date type="published" when="2016-06">Jun. 2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">An ensemble approach for short-term load forecasting by extreme learning machine</title>
		<author>
			<persName><forename type="first">S</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Goel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Applied Energy</title>
		<imprint>
			<biblScope unit="volume">170</biblScope>
			<biblScope unit="page" from="22" to="29" />
			<date type="published" when="2016-05">May 2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">An incremental design of radial basis function networks</title>
		<author>
			<persName><forename type="first">H</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">D</forename><surname>Reiner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Bartczak</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">M</forename><surname>Wilamowski</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Neural Networks and Learning Systems</title>
		<imprint>
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page" from="1793" to="1803" />
			<date type="published" when="2014-02">Feb. 2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Currently, he is a Ph.D. student with the Department of Electrical Engineering. His research interests include applications of machine learning and data science in power systems</title>
		<author>
			<persName><forename type="first">F</forename><surname>Ziel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Kunlong Chen received the B.Sc. degree in electrical engineering from Beijing Jiaotong University, Beijing, China and the engineering degree from Cen-traleSupélec</title>
		<meeting><address><addrLine>Beijing, China; Paris, France</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2015">2016. 2015</date>
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="page" from="1029" to="1037" />
		</imprint>
	</monogr>
	<note>He is currently pursuing the M.Sc. degree with the Department of Electrical Engineering in Beijing Jiaotong University. His research interests include applications of statistical learning techniques in the field of electrical engineering</note>
</biblStruct>

<biblStruct xml:id="b51">
	<monogr>
		<title level="m" type="main">Currently, he is a Master student at ETH Zürich. His reserach interests include computer vision and deep learning applications. Ziyu He recieved his B.Sc. degree from Zhejiang University in 2015 and his M. S. degree from Columbia University in 2017. He is currently a Ph.D. student in the department of industrial and systems engineering at University of Southern California. His research interests are optimization, machine learning and their applications in energy</title>
		<imprint>
			<date type="published" when="2015">2015</date>
			<publisher>Qin Wang received his B.Sc</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<monogr>
		<title level="m" type="main">Currently, he is an associate professor in the same department. His research fields include overvoltage analysis in power system, sensors and big data, dielectric materials and surge arrester technology. Jinliang He (M&apos;02-SM&apos;02-F&apos;08) received the B.Sc. degree from Wuhan University of Hydraulic and Electrical Engineering</title>
		<author>
			<persName><forename type="first">Jun</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">;</forename><forename type="middle">B</forename><surname>Sc</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Sc</surname></persName>
		</author>
		<author>
			<persName><forename type="middle">D</forename><surname>Ph</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1988">July 1998, July 2000, July 2008. 1988. 1991 and 1994. 2014 to 2015. 2001</date>
			<pubPlace>China; Wuhan, China; Beijing, China; Palo Alto, CA, USA</pubPlace>
		</imprint>
		<respStmt>
			<orgName>Department of Electrical Engineering, Tsinghua University in Beijing ; M.Sc. degree from Chongqing University, Chongqing, China, and the Ph.D. degree from Tsinghua University ; Department of Electrical Engineering, Tsinghua University ; Department of Electrical Engineering, Stanford University ; Professor with Tsinghua University. He is currently the Chair with High Voltage Research Institute, Tsinghua University</orgName>
		</respStmt>
	</monogr>
	<note>He has authored five books and 400 technical papers. His research interests include overvoltages and EMC in power systems and electronic systems, lightning protection, grounding technology, power apparatus, and dielectric material</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
