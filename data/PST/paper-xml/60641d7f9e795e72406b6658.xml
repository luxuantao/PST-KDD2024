<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">The Surprising Performance of Simple Baselines for Misinformation Detection</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date type="published" when="2021-04-14">14 Apr 2021</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Kellin</forename><surname>Pelrine</surname></persName>
							<email>kellin.pelrine@mila.quebec</email>
						</author>
						<author>
							<persName><forename type="first">Jacob</forename><surname>Danovitch</surname></persName>
							<email>jacob.danovitch@mila.quebec</email>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">SOCS</orgName>
								<orgName type="institution" key="instit2">McGill University Mila -Quebec AI Institute</orgName>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff1">
								<orgName type="institution" key="instit1">SOCS</orgName>
								<orgName type="institution" key="instit2">McGill University Mila -Quebec AI Institute Reihaneh Rabbany</orgName>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff2">
								<orgName type="institution" key="instit1">SOCS</orgName>
								<orgName type="institution" key="instit2">McGill University Mila -Quebec AI Institute</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">The Surprising Performance of Simple Baselines for Misinformation Detection</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2021-04-14">14 Apr 2021</date>
						</imprint>
					</monogr>
					<idno type="DOI">10.1145/3442381.3450111</idno>
					<idno type="arXiv">arXiv:2104.06952v1[cs.CL]</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.2" ident="GROBID" when="2022-12-25T13:46+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>misinformation</term>
					<term>social media</term>
					<term>natural language processing</term>
					<term>datasets</term>
					<term>COVID-19</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>As social media becomes increasingly prominent in our day to day lives, it is increasingly important to detect informative content and prevent the spread of disinformation and unverified rumours. While many sophisticated and successful models have been proposed in the literature, they are often compared with older NLP baselines such as SVMs, CNNs, and LSTMs. In this paper, we examine the performance of a broad set of modern transformer-based language models and show that with basic fine-tuning, these models are competitive with and can even significantly outperform recently proposed state-of-the-art methods. We present our framework as a baseline for creating and evaluating new methods for misinformation detection. We further study a comprehensive set of benchmark datasets, and discuss potential data leakage and the need for careful design of the experiments and understanding of datasets to account for confounding variables. As an extreme case example, we show that classifying only based on the first three digits of tweet ids, which contain information on the date, gives state-ofthe-art performance on a commonly used benchmark dataset for fake news detection -Twitter16. We provide a simple tool to detect this problem and suggest steps to mitigate it in future datasets.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">INTRODUCTION</head><p>Social media is filled with both treasure troves and landmines of information. There are far too many interactions to mine them by hand. But failing to understand them can have dire consequences.</p><p>Misinformation can challenge fair elections <ref type="bibr" target="#b53">[54]</ref> and cost billions of dollars <ref type="bibr" target="#b72">[73]</ref>. Misinformation spread in the COVID-19 "infodemic" <ref type="bibr" target="#b17">[18,</ref><ref type="bibr" target="#b68">69]</ref> can cost lives. At the same time, successful mining of useful information can enable contact tracing and other measures to save lives <ref type="bibr" target="#b45">[46,</ref><ref type="bibr" target="#b69">70]</ref>, as well as other applications like explainable fact checking <ref type="bibr" target="#b19">[20]</ref>. Motivated by the profound impact, there has been substantial research on detecting fake news, misinformation, and related topics (such as bot and troll detection) in the past few years <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b9">10,</ref><ref type="bibr" target="#b58">59,</ref><ref type="bibr" target="#b62">63,</ref><ref type="bibr" target="#b79">80]</ref>. A majority of these works are based on text/content classification <ref type="bibr" target="#b76">[77,</ref><ref type="bibr" target="#b80">81,</ref><ref type="bibr" target="#b99">100,</ref><ref type="bibr" target="#b100">101]</ref>. As real-world events such as the spread of COVID-19 misinformation show <ref type="bibr" target="#b17">[18,</ref><ref type="bibr" target="#b68">69]</ref>, we are still far from addressing misinformation and more work is needed.</p><p>In this paper, we examine the use of modern pre-trained language models (LMs) for classifying content. Although language models are ubiquitous baselines in this domain, the ones used are often older models such as SVMs, CNNs, LSTMs, etc. <ref type="bibr" target="#b14">[15,</ref><ref type="bibr" target="#b22">23,</ref><ref type="bibr" target="#b27">28,</ref><ref type="bibr" target="#b77">78]</ref>, instead of transformer-based models that have been dominant in NLP recently, such as the exemplar BERT model <ref type="bibr" target="#b23">[24,</ref><ref type="bibr" target="#b97">98]</ref>. Here, we report the performance of these recently proposed language models on common benchmark datasets for misinformation detection. More specifically, we consider small, medium and large models and show that most can achieve comparable or even better performance than state-of-the-art (SOTA) methods. Even the smallest 4 million parameter BERT-Tiny <ref type="bibr" target="#b87">[88]</ref> can approach or beat state-of-the-art in some cases. These SOTA methods are usually much more complex and in many cases incorporate more information beyond the content, e.g. the reply thread. Although some of these more sophisticated methods are also designed to incorporate language models, it is not straightforward how to change their language module and they may or may not synergize well with the latest language models.</p><p>We further study commonly used benchmark datasets and report a significant issue with the Twitter15 and Twitter16 datasets <ref type="bibr" target="#b46">[47,</ref><ref type="bibr" target="#b50">51,</ref><ref type="bibr" target="#b51">52]</ref>, which can produce unrealistically high performance just by inferring the tweet date. This issue is also present to a lesser degree in FakeNewsNet <ref type="bibr" target="#b78">[79]</ref>. We propose a simple test to identify it in other datasets and suggest how to mitigate it in data collection.</p><p>We also provide the exact splits to use our results as benchmarks, as well as other suggestions on proper evaluation techniques, and discuss potential future directions of work in this domain.</p><p>To summarize, the main contributions of this paper are threefold: • We show that recent pretrained LMs are competitive with and sometimes even substantially better than SOTA models in common benchmarks for misinformation detection. • We discuss experiment design considerations, effect of different data splits, and further demonstrate potential data leakage issues on commonly used benchmark datasets.</p><p>• We package our framework as a simple yet strong baseline for relevant tasks, and provide the tweet IDs to exactly reconstruct our splits, facilitating use of our reported results for benchmarks.</p><p>Our framework is available on GitHub.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">BACKGROUND AND RELATED WORK</head><p>There are two key literatures related to this work: (mis-)information classification on the application domain side, and natural language processing models on the methodological side. We discuss first the former and then the latter.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Detecting Misinformation</head><p>We can broadly divide misinformation detection algorithms into methods that analyze content and (social) context. Here, we focus on the methods that are content-based. For more complete review of the related works, please refer to the recent surveys <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b9">10,</ref><ref type="bibr" target="#b58">59,</ref><ref type="bibr" target="#b76">77,</ref><ref type="bibr" target="#b79">80,</ref><ref type="bibr" target="#b100">101]</ref>. The content has been analyzed previously on multiple levels. At the word level, methods have used features including word counts, derived measures like TF-IDF, small combinations of words like bigrams, and word embeddings <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b10">11,</ref><ref type="bibr" target="#b30">31,</ref><ref type="bibr" target="#b74">75]</ref>. At the sentence level, methods rely on features like syntax and complexity <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b10">11,</ref><ref type="bibr" target="#b30">31,</ref><ref type="bibr" target="#b74">75]</ref>. Finally, there are features derived from the text as a whole, including general representations and more specialized ones such as topic or sentiment analysis <ref type="bibr" target="#b14">[15,</ref><ref type="bibr" target="#b20">21,</ref><ref type="bibr" target="#b34">35,</ref><ref type="bibr" target="#b35">36]</ref>.</p><p>More specifically, we consider the following state-of-the-art algorithms as a representative set of the current methods:</p><p>• Cheng et al. <ref type="bibr" target="#b14">[15]</ref> use a multitask (rumor detection, tracking, veracity classification, and stance classification) LSTM-based VAE <ref type="bibr" target="#b39">[40]</ref>, with the text as input. • Han et al. <ref type="bibr" target="#b27">[28]</ref> use a GNN <ref type="bibr" target="#b75">[76]</ref> to encode the propagation network and features of the users in it (not including text). • Huang et al. <ref type="bibr" target="#b29">[30]</ref> build a heterogeneous tweet-word-user graph, linking the representations with an attention mechanism. • Lu and Li <ref type="bibr" target="#b49">[50]</ref> use a co-attention mechanism to combine text and propagation information. • Shu et al. <ref type="bibr" target="#b77">[78]</ref> use attention mechanisms to combine wordand sentence-level features from articles, encoded with a GRU, with a comment (tweet) encoder. • Wang et al. <ref type="bibr" target="#b91">[92]</ref> link text and object detections in images with entities and a knowledge graph constructed from them. • Wu et al. <ref type="bibr" target="#b95">[96]</ref> uses a combination of a decision tree model and co-attention to evaluate credibility of tweets using text, propagation, and user information. • Wu et al. <ref type="bibr" target="#b96">[97]</ref> build a propagation graph from tweets and replies. They embed each using Doc2Vec <ref type="bibr" target="#b44">[45]</ref>, and use a GRU (Gated Recurrent Unit) <ref type="bibr" target="#b16">[17]</ref> and attention mechanism to pass information and pool the graph.</p><p>The most related work to ours is <ref type="bibr" target="#b31">[32]</ref>. They highlight "crosssource" failure of existing misinformation detection methods, in terms of training on one dataset and testing on another, and also propose a new model designed to solve it. Our results support and expand on theirs by identifying "cross-topic" and "cross-domain" effects even within the same datasets, depending on how they are split, as well as differences between datasets. We take a different direction for modeling: rather than focusing on cross-source tasks and developing a specialized model, we look at a spectrum of tasks and ask how general language models perform in this domain.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Detecting Informative Content</head><p>Instead of detecting which tweets are spreading misinformation, there are also many contexts in which we want detect tweets that provide useful information. Recently, there has been a big focus in the research community on COVID-19 <ref type="bibr" target="#b7">[8]</ref>. Mining information about disaster events is not new <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b12">13,</ref><ref type="bibr" target="#b33">34,</ref><ref type="bibr" target="#b37">38,</ref><ref type="bibr" target="#b42">43,</ref><ref type="bibr" target="#b83">[84]</ref><ref type="bibr" target="#b84">[85]</ref><ref type="bibr" target="#b85">[86]</ref>. But COVID-19 is unprecedented in its global scale and impact, and requires new methods and understanding.</p><p>Although the focus of our experiments is on misinformation, we include a dataset for detecting informative COVID-19 tweetsdiscussed in more detail in section 3.2 -to highlight the connections between these two areas and how language models can model both as content classification.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Pre-Trained Language Models</head><p>The invention of word embeddings was a fundamental breakthrough <ref type="bibr" target="#b54">[55,</ref><ref type="bibr" target="#b55">56]</ref> leading towards today's language models. By predicting words, one can go beyond one-hot and similar encodings and convert words into vectors that encode their meaning and are more useful for downstream tasks.</p><p>The first embedding models such as Word2Vec <ref type="bibr" target="#b54">[55,</ref><ref type="bibr" target="#b55">56]</ref>, GloVe <ref type="bibr" target="#b65">[66]</ref>, and FastText <ref type="bibr" target="#b36">[37]</ref> generate fixed vectors for each input word or token. This has the advantage of simplicity, but can struggle to capture words whose meanings vary depending on the context, and in turn struggle with downstream tasks that require this level of language understanding. Therefore, better methods to incorporate context became a key research topic. A followup direction was to use convolutional neural networks, ubiquitous in computer vision, on text <ref type="bibr" target="#b38">[39]</ref>. These could effectively capture local context, but still missed incorporating longer range context. Similarly, RNNs, which could capture local sequences, had problems with vanishing gradients and could not capture longer range information <ref type="bibr" target="#b56">[57]</ref>.</p><p>LSTMs <ref type="bibr" target="#b28">[29]</ref>, which like RNNs operate on sequences but allow longer range dependencies, helped solve this issue. There are state of the art models for misinformation detection, including the ones discussed in the domain section above, which use various versions or improvements on LSTMs, such as the GRU by Cho et al. <ref type="bibr" target="#b16">[17]</ref>. These also led to contextualized embeddings such as ELMo <ref type="bibr" target="#b67">[68]</ref>.</p><p>Another major breakthrough came with attention <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b98">99]</ref> and then, based on attention, transformers <ref type="bibr" target="#b88">[89]</ref>. These facilitate far more parallelization than LSTMs <ref type="bibr" target="#b56">[57]</ref>, which means models based on them can be trained on much larger and more general corpora. A key early model using these is BERT <ref type="bibr" target="#b23">[24]</ref> (discussed further in the following section), and there has subsequently been numerous different models proposed based on transformers <ref type="bibr" target="#b97">[98]</ref>. These vary from small tweaks to existing models such as pretraining on domainspecific data <ref type="bibr" target="#b59">[60,</ref><ref type="bibr" target="#b61">62]</ref>, to significantly different models such as GPT (and then GPT-2 and GPT-3) <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b70">71,</ref><ref type="bibr" target="#b71">72]</ref>. Virtually all of these models can provide contextualized word/sentence/document embeddings for downstream tasks. These models are easily accessible through packages such as <ref type="bibr" target="#b92">[93]</ref>, and with hundreds of versions to choose from, as Xia et al. <ref type="bibr" target="#b97">[98]</ref> suggests the hardest question may be which one to use.</p><p>Although there are many excellent options, a large number of recent papers in the misinformation domain still benchmark their methods against older models <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b14">15,</ref><ref type="bibr" target="#b19">20,</ref><ref type="bibr" target="#b22">23,</ref><ref type="bibr" target="#b27">28,</ref><ref type="bibr" target="#b77">78,</ref><ref type="bibr" target="#b94">95]</ref>. While there are still contexts in which these models do well and there is nothing wrong with making these comparisons, our results show that this can sometimes result in weak benchmarks when the more modern language models are omitted. Among other results, we give suggestions on how to incorporate these new models into evaluation in this domain. Please note that although training these newest models from scratch can require a prohibitive amount of computational resources, pretrained weights are readily available and are commonly used instead.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">METHODOLOGY</head><p>Our baselines simply consist of a set of language models and benchmark datasets. Here we first explain the language models we consider in our baseline framework in Section 3.1, then we describe the benchmark datasets in Section 3.2. For the Funnel Transformer and ALBERT, we employ a mean pooling strategy over the final layer of output. For ELMo we use an LSTM pooler. For all others, we use the [CLS] token embedding from the final layer of output. For all models, we allow all parameters of the language model to be finetuned, use a single fully-connected layer on the pooled embeddings for classification, train using cross-entropy loss, and use AdamW <ref type="bibr" target="#b48">[49]</ref> with a slanted triangular learning rate scheduler for optimization.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Language Models</head><p>We evaluate a variety of language models. Where available, we use the implementations and pre-trained weights provided by the Huggingface Transformers library <ref type="bibr" target="#b92">[93]</ref>, and train using PyTorch <ref type="bibr" target="#b63">[64]</ref> and AllenNLP <ref type="bibr" target="#b24">[25]</ref>. We include the following language models (with the exact version<ref type="foot" target="#foot_0">1</ref> in parentheses):</p><p>BERT (bert-base-uncased) <ref type="bibr" target="#b23">[24]</ref>. Bidirectional Encoder Representations from Transformers, or BERT for short, is a large pre-trained bidirectional transformer. BERT was pre-trained using two objectives. The first, masked language modelling, required BERT to predict a masked token from the input. The second, next sentence prediction, required predicting whether two sentences appeared consecutively in the training corpus. To complete the latter task, BERT prepends the input text with a special [CLS] token, and inserts a special [SEP] token between the first and second sentence as well as at the end of the second sentence. The [CLS] token is commonly used as a document-level representation for classification tasks.</p><p>BERT-Tiny (bert_uncased_L-2_H-128_A-2) <ref type="bibr" target="#b87">[88]</ref>. BERT-Tiny is the smallest of several pre-trained language models that follow the same pre-training procedure as BERT, as well as a knowledge distillation fine-tuning procedure. These smaller models achieve strong performance on downstream tasks with significantly fewer parameters.</p><p>RoBERTa (roberta-large) <ref type="bibr" target="#b47">[48]</ref>. RoBERTa follows a similar architecture to BERT but removes the next-sentence prediction pre-training objective while making the masking procedure dynamic by regenerating the mask for each example every time and leverage larger batch sizes and increased training iterations to improve performance. They find that BERT underfits its training data.</p><p>ALBERT (albert-large-v2) <ref type="bibr" target="#b43">[44]</ref>. ALBERT adds an additional pre-training objective while incorporating two methods that reduce the number of parameters in the model, factorizing the embedding layer and tying weights across hidden layers.</p><p>BERTweet (bertweet-base) <ref type="bibr" target="#b61">[62]</ref>. BERTweet follows an identical training procedure to RoBERTa, and is pre-trained on 850 million tweets.</p><p>COVID-Twitter-BERT (covid-twitter-bert-v2) <ref type="bibr" target="#b59">[60]</ref>. COVID-Twitter-BERT (CT-BERT) follows an identical training procedure to BERT, and the latest version is pre-trained on 1.2 billion training examples generated from 97 million tweets.</p><p>DeCLUTR (declutr-base) <ref type="bibr" target="#b25">[26]</ref>. DeCLUTR is a transformer-based language model that proposes a contrastive, self-supervised method for learning general purpose sentence embeddings. The model is trained with a masked language modelling objective as well as with contrastive loss using both easy and hard negatives.</p><p>Funnel Transformer (xlarge-base) <ref type="bibr" target="#b21">[22]</ref>. The Funnel Transformer improves the efficiency of bidirectional transformer models by applying a pooling operation after each layer, akin to convolutional neural networks, to reduce the length of the input.</p><p>ELMo (Original<ref type="foot" target="#foot_1">2</ref> ) <ref type="bibr" target="#b67">[68]</ref>. ELMo is one of the first large-scale pre-trained language models. It is a character-based model and is the only model in this list that does not optimize for masked language modelling. Instead, it uses bidirectional LSTMs <ref type="bibr" target="#b28">[29]</ref> to perform autoregressive language modelling in both the forward and backward directions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Benchmark Datasets</head><p>We consider the comprehensive set of benchmark datasets available in the literature for misinformation (information) detection. In the following we briefly explain each dataset.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.1">PHEME.</head><p>The PHEME dataset <ref type="bibr" target="#b40">[41,</ref><ref type="bibr" target="#b101">102]</ref> contains 6425 tweets about 9 newsworthy events. The events are unrelated. The tweets were collected as the stories developed, with a journalist annotating them as rumour vs. non-rumour. They grouped them by story, and then marked whether the stories were true or false once it was confirmed, or as unverified if they could not be certain during the collection period. There are also other labels such as stance, which we do not consider here.</p><p>We split this dataset five different ways to compare with different state-of-the-art results that use the corresponding settings.</p><p>PHEME9 T/F which matches Wu and Rao <ref type="bibr" target="#b93">[94]</ref>. First, we take only the tweets marked as true or false and do a 70-10-20 train-dev-test split.</p><p>PHEME5 R/NR which matches Wang et al. <ref type="bibr" target="#b91">[92]</ref>. Next we take the 5 largest events (comprising 90% of the dataset) and use the rumour and non-rumour labels. We again split <ref type="bibr">70-10-20.</ref> This setup matches both <ref type="bibr" target="#b91">[92]</ref> (70-30 split with no dev set) and <ref type="bibr" target="#b15">[16]</ref> (which notes 10% withheld for validation). PHEME5 3-way which matches Cheng et al. <ref type="bibr" target="#b15">[16]</ref>. Here, we take the 5 largest events and tweets labeled true, false, and unverified, splitting 70-10-20.</p><p>PHEME9 4-way which matches Wu et al. <ref type="bibr" target="#b96">[97]</ref>. For this, we subsample 800, 400, 600, 500 tweets with at least 3 replies, that are non-rumor, false, true, and unverified, respectively. They are split 80-10-10.</p><p>PHEME5 Lc which matches Cheng et al. <ref type="bibr" target="#b15">[16]</ref>. Finally, we also examine an event-based split. Starting with the 5 largest events, we train on four and test on the largest (tweets related to Charlie Hebdo terror attack), with the 3-way true, false, and unverified labels.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>3.2.2</head><p>FakeNewsNet. The FakeNewsNet dataset <ref type="bibr" target="#b78">[79,</ref><ref type="bibr" target="#b80">81,</ref><ref type="bibr" target="#b81">82]</ref> contains articles fact-checked by PolitiFact<ref type="foot" target="#foot_2">3</ref> or GossipCop <ref type="foot" target="#foot_3">4</ref> , and related tweets. The labels are "real" and "fake." We retrieve 467 thousand tweets in the PolitiFact dataset, and 1.25 million in GossipCop. We split this dataset by first splitting the articles, then assigning tweets to each split based on the article they are associated with. We divide it 75-10-15, corresponding to the splits in <ref type="bibr" target="#b27">[28,</ref><ref type="bibr" target="#b76">77,</ref><ref type="bibr" target="#b82">83]</ref>. We remove two events from Politifact that are labeled both real and fake, politifact14920 and politifact14940. Work on this dataset such as <ref type="bibr" target="#b27">[28,</ref><ref type="bibr" target="#b76">77]</ref> has focused on classifying the articles, and using the tweets and related user information as supplemental information to improve performance. Because our pipeline was set up for tweet data rather than articles, we apply a simple way of classifying the articles through the tweets, by classifying the tweets and converting the predictions to an article prediction based on majority vote of the corresponding tweets' labels. Note that the works we compare with use the tweets, so this is not using an additional type of data that they have excluded. Following <ref type="bibr" target="#b27">[28,</ref><ref type="bibr" target="#b76">77]</ref>, we evaluate only on articles with at least 3 corresponding tweets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>3.2.3</head><p>Twitter15 and Twitter16. These datasets <ref type="bibr" target="#b46">[47,</ref><ref type="bibr" target="#b50">51,</ref><ref type="bibr" target="#b51">52]</ref> contain tweets related to stories from fact-checking websites and random tweets. There are 1490 and 818 tweets respectively, labeled true, false, unverified, or non-rumor. We reserve 10% for the dev set, then split the remainder 75:25, matching <ref type="bibr" target="#b29">[30]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>3.2.4</head><p>Twitter15 T/F and Twitter16 T/F. These are Twitter15 and Twitter16 with the true and false examples only. This gives 742 and 412 examples respectively. We split 70-10-20, giving the same training set size as <ref type="bibr" target="#b49">[50]</ref>, which split 70-30 with no dev set.</p><p>3.2.5 WNUT-2020. This dataset was created for a shared task in WNUT-2020 on classifying tweets as "informative" or "uninformative", in providing information about "recovered, suspected, confirmed and death cases as well as location or travel history of the cases" <ref type="bibr" target="#b62">[63]</ref>. Many of the models submitted in the competition were based on pre-trained transformer-based language models <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b11">12,</ref><ref type="bibr" target="#b32">33,</ref><ref type="bibr" target="#b32">33,</ref><ref type="bibr" target="#b52">53,</ref><ref type="bibr" target="#b60">61,</ref><ref type="bibr" target="#b60">61,</ref><ref type="bibr" target="#b66">67,</ref><ref type="bibr" target="#b86">87,</ref><ref type="bibr" target="#b90">91]</ref>, including the first place model which was an ensemble of CT-BERT and RoBERTa <ref type="bibr" target="#b41">[42]</ref>, and the tied first place model which was a carefully tuned version of CT-BERT <ref type="bibr" target="#b57">[58]</ref>. A 70-10-20 split was provided by the task organizers.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>3.2.6</head><p>CoAID. The CoAID dataset <ref type="bibr" target="#b18">[19]</ref> is a misinformation dataset related to COVID-19. This dataset is new and consists of tweets, articles, and claims. The authors provide baselines on the articles. To the best of our knowledge, as of this writing, no existing work has performed classification on the tweets.</p><p>We conducted experiments here aimed to establish a baseline for future works. However, we found that most splits result in virtually all models obtaining near-perfect (above 95) F1 score. We examined in particular the "news" tweets. Many tweets about the same news item have similar or even duplicated text, so we tried splitting similar to FakeNewsNet, first splitting the news articles and then assigning tweets to each set based on their corresponding article. We also tried subsampling a small train set to make the problem harder. This did result in one set of results with lower F1, but we found it was unstable and did not replicate consistently when we redid the splitting and training process. Thus, we do not report results on this dataset here. However, it contains a significant amount of useful data on an important problem, so we encourage future work to determine challenging splits and help others take full advantage of this resource.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Implementation Details</head><p>Each model is trained on an RTX8000 GPU using mixed precision with a batch size of 32. and learning rate of 1e-5. We do not perform any hyperparameter tuning. We train for 50 epochs on all datasets except Politifact and Gossipcop. There we train for 2 epochs, because they are much larger than the rest.</p><p>We run each model to completion exactly 5 times, and report mean and standard deviation, with two exceptions. First, datasets with a fixed split (PHEME5 Lc and WNUT-2020), which are run once. Second, a bug caused a small number of GossipCop results with Funnel to be lost, and due to time constraints we were unable to fully repeat the experiments. Thus we do not report results for that model and dataset.</p><p>Complete implementation details, hyperparameter configurations, and tweet IDs for each split of each dataset can be found on GitHub. <ref type="foot" target="#foot_4">5</ref></p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Experimental Results</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4.1">Performance Evaluation.</head><p>We roughly categorize the algorithms by size. The first set are "large" models with around 400 million parameters. The second set are "medium" ones with around 100 million. The third set are "small" ones under 20 million. Order in the tables is alphabetical within category. The exact parameter counts are shown in table <ref type="table" target="#tab_0">1</ref>.</p><p>Results are shown in tables 2 and 3. The first row presents the algorithm that achieves, to the best of our knowledge, state-of-theart (SOTA) performance on the given dataset and split.</p><p>We discuss these results in section 4. competitive performance can be achieved by using only the first 2 or 3 digits of the tweet ID. Due to the way unique IDs are generated for tweets, these digits reveal information about the time the tweet was posted. <ref type="foot" target="#foot_5">6</ref> We train a random forest on these digits, setting a high depth (25, such that increasing it further does not change validation performance) and otherwise using the default scikit-learn <ref type="bibr" target="#b64">[65]</ref> parameters.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4.2">Potential Data</head><p>The class labels are balanced, so a random baseline will have 25% F1. This is roughly what one would intuitively expect from a model using only IDs as features, because the time, particularly at this approximate scale, should have little correlation if any with whether a tweet is true or false, and would not be useful on its own for classification in applied settings. Unfortunately, as shown in table 6, this is not the case. The IDs perform moderately well on Twitter15, and approach state-of-the-art on Twitter16. The nonrumor class on Twitter15 is also trivial to detect from the date.</p><p>Of course, classification using tweet IDs alone is uninteresting in isolation. However, this paints a broader picture of potential confounding variables within these datasets (and others collected in the same manner). Tweet IDs are not the only things that evolve over time; the content posted to Twitter itself changes drastically over time, as language evolves and topics enter and exit public discourse. For example, consider two particularly time-sensitive words: "Clinton" and "Trump", the surnames of the 2016 US presidential candidates. The presence of these words, like the tweet IDs, is revealing about the time at which a tweet was posted (and therefore the potential label distribution), as public discourse largely focused on their campaigns throughout 2016. In table 6, we see that the presence of those words (uncased) alone allow us to rule out "true, " and in the case of Twitter15 also "false. " This is obviously very unrealistic, and can cause a classifier to rely too much on particular keywords and fail to generalize to unseen data in applied settings.</p><p>After discovering this problem with Twitter15 and Twitter16, we next examined the other datasets. Results are shown in table 4. Here Top Model refers to the best model reported in this paper, either the SOTA model (indicated by a citation) or one of our language models. PHEME9 All is comprised of all PHEME tweets with the The issue is not as pronounced on these datasets as on Twitter15 and Twitter16, but is still present, especially in PolitiFact and Gos-sipCop. CoAID and PHEME are not too bad; classifying the IDs gives better than random performance, but the performance is still bad. WNUT-2020 is split particularly well in time, as classifying the IDs is no better than random.</p><p>To examine the data from another perspective, we can look at words by label in Twitter16. In figure 1, we show an example visualization of the label "false" versus the rest. We can see that "steve" is extremely correlated with a label of "false. " This is due to a false tweet about Steve Jobs being adopted that is exactly duplicated 12 times, and nearly duplicated an additional 5. In the real world, it is unlikely that "Steve" guarantees a false label, but in this data it does. We can also see that there seem to be some false tweets about mass shootings -again, unlikely to be so strongly discriminative in the real world.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">DISCUSSION</head><p>These datasets fall on a spectrum of required generalization. On one end, there are "in-topic" datasets such as WNUT-2020 and the four PHEME splits not including PHEME5 Lc. These correspond to a real-world problem where one wants to detect misinformation or other content on a known topic. For example, a current task is detecting 5G COVID-19 conspiracy tweets <ref type="bibr" target="#b5">[6]</ref>. On the other end, there are "cross-domain" datasets, such as PHEME5 Lc, where although the task is shared, the content of the test examples is otherwise unrelated to the train examples. A real-world application of this is detecting misinformation about a new event. In the middle, there are datasets whose examples share related domains, but have somewhat different topics, such as PolitiFact.</p><p>The language models examined here excel at "in-topic" classification, often beating state-of-the-art methods by large margins in the results here. On the other hand, they perform poorly at cross-domain tasks. For example, they produce very mediocre performance on PHEME Lc, likely due to overfitting to the topics of the training set and failing to generalize to a different topic. Stateof-the-art models in this setting, such as <ref type="bibr" target="#b15">[16]</ref>, use other information beyond content to avoid this failure. Meanwhile, in between the two extreme types of datasets, we see more variable results. For example, LMs beat state-of-the-art on GossipCop but not PolitiFact.</p><p>Real-world problems can fall anywhere along the entire spectrum, and even the best results on these datasets -whether ours or others' -still fail on a significant fraction of detections. So better methods are needed on all counts. The results here suggest that understanding where one's task falls on this spectrum is very helpful. When it is further towards the "in-topic" side, recent language models may be very effective, at minimum providing strong benchmarks, and potentially a foundation on which to build more sophisticated models.</p><p>Comparing new domain-specific models with older language models may complicate interpretability of the results. The obvious, definitive solution is to always compare with the latest language models. However, due to time and computation constraints, this may not always be feasible -for example, even the largest models we report here are not the largest models available. There are three alternatives. One is to match splits and rely on other papers that run the latest LMs. This is may not be viable for new data, but we encourage work on standard datasets to provide solid benchmarks, and matching splits with those benchmarks whenever possible to facilitate comparison. As noted in the introduction, we provide IDs for our exact splits so that our results can be used in this way. <ref type="foot" target="#foot_6">7</ref>Another alternative is to build models which can incorporate the latest language model, and provide evidence that it improves results compared to language models alone (and does so better in some  way than existing work). These may be particularly valuable, as they are to some degree future-proofed, or at least future-adaptive, since they can be updated with newer language models. Finally, researchers can explore different directions that are orthogonal to content, and give evidence that they provide information that cannot be extracted from the content by language models. There is already a great deal of research that can fall along these lines, such as <ref type="bibr" target="#b13">[14,</ref><ref type="bibr" target="#b31">32,</ref><ref type="bibr" target="#b73">74]</ref>. But it is not always clear to what degree other features are orthogonal to content, and explicit analysis in future work could be useful.</p><p>Our results on classifying tweet IDs show that depending on the collection strategy, the tweet date may be surprisingly informative for the labels. This means the distribution of data may not be a good match for real-world tasks. A simple random forest on the first few digits of the tweet ID can be used to detect if this issue is present in the data. We suggest this test be applied to future datasets to ensure a reasonable level of temporal randomization, or to find and report that it is not random and thus to facilitate designing algorithms and interpreting results in light of that. Similar techniques can be applied to data from other social networks.</p><p>It is also possible to time-randomize data after the primary collection. In this domain, the fake examples are typically the ones that are hard to find, as in most contexts the majority of tweets are real <ref type="bibr" target="#b26">[27]</ref>. We can time-randomize without retrieving new fake tweets if we can retrieve additional real tweets (and tweets from the other classes, when working with more than real/fake) from approximately the same time as the fake ones. Then we can replace the original real tweets, yielding both fake and real tweets that are hard to distinguish using time. This may be facilitated by the new Twitter Academic API, <ref type="foot" target="#foot_7">8</ref> which gives more historical access than the normal API. Since Twitter16 is commonly used <ref type="bibr" target="#b29">[30,</ref><ref type="bibr" target="#b34">35,</ref><ref type="bibr" target="#b89">90]</ref>, has severe time-determinacy, and does not have too many tweets to deal with, future work to improve this dataset in particular could be worthwhile, though the keyword issues with fake tweets might be hard to fix.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">CONCLUSION</head><p>Overall, our findings here highlight the need to combine both development of better algorithms and data science. To solve critical real-world problems like misinformation, we need better understandings of how different models and datasets compare and interact. Otherwise we risk creating sophisticated models that are beaten by brute force approaches -applying the biggest LM one can run -or models that work well on standard datasets but poorly in practice, e.g. by learning to predict the date or keywords like "Steve. " The SOTA models we compare with in this work have more to offer than pure performance (for example, explainability), and there are other tasks like early detection that we have not examined here. But we suggest future work can benefit from adjusting and further considering how we frame these problems. This will help not only build higher metrics but also real-world solutions.</p><p>Besides considerations for improving current lines of research, this paper suggests two other promising directions for future work:</p><p>• Benchmarking standard misinformation datasets and publishing exact splits. We see, for example on PHEME, that there are many ways researchers have split the data, and benchmarks are lacking. This leads to difficulty for the research community in comparing results. Better benchmarks and the ability to compare exact splits would help mitigate this issue. • New, thoroughly benchmarked and validated datasets. Although Twitter15 and Twitter16 have flaws highlighted in this paper, they also have excellent propagation information, which has likely motivated many of the approaches that leverage that to use those datasets. Improved datasets should lead to improved real-world results.</p><p>Finally, we hope to integrate these models with work on other features and data types, and produce thoroughly evaluated models that can combine the best of both recent language models and misinformation detection domain algorithms.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: False vs. Other Classes</figDesc><graphic url="image-1.png" coords="7,53.80,203.49,504.41,283.03" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 :</head><label>1</label><figDesc>Leakage. During our analysis, we observed a trend in the Twitter15 and Twitter16 dataset. It appears that time is a highly discriminative factor in separating the classes. This creates the possibility of data leakage by means of several potential confounding variables. To illustrate, we demonstrate that Number of Parameters</figDesc><table><row><cell cols="9">CT-BERT Funnel RoBERTa BERT BERTweet DeCLUTR ELMo ALBERT BERT-Tiny</cell></row><row><cell>340M</cell><cell>468M</cell><cell>355M</cell><cell>110M</cell><cell>135M</cell><cell>125M</cell><cell>94M</cell><cell>17M</cell><cell>4M</cell></row><row><cell></cell><cell></cell><cell cols="5">Table 2: PHEME Results (Macro F1 score)</cell><cell></cell><cell></cell></row><row><cell cols="9">PHEME9 T/F PHEME5 R/NR PHEME5 3-way PHEME9 4-way PHEME5 Lc Average Rank</cell></row><row><cell>SOTA</cell><cell>82.5 [96]</cell><cell cols="2">87.6 [16, 92]</cell><cell>66.7 [16]</cell><cell cols="2">75.3 [97]</cell><cell>51.3 [16]</cell><cell>5.6</cell></row><row><cell>CT-BERT</cell><cell>92.0 ± 0.9</cell><cell cols="2">89.0 ± 0.8</cell><cell>84.6 ± 1.5</cell><cell cols="2">79.0 ± 2.6</cell><cell>27.9</cell><cell>3.4</cell></row><row><cell>Funnel</cell><cell>86.7 ± 3.2</cell><cell cols="2">87.3 ± 0.6</cell><cell>79.4 ± 3.7</cell><cell cols="2">71.4 ± 3.3</cell><cell>28.7</cell><cell>6.4</cell></row><row><cell>RoBERTa</cell><cell>93.2 ± 0.9</cell><cell cols="2">89.4 ± 0.3</cell><cell>87.7 ± 1.9</cell><cell cols="2">82.5 ± 3.3</cell><cell>29.0</cell><cell>2.0</cell></row><row><cell>BERT</cell><cell>89.9 ± 1.1</cell><cell cols="2">87.2 ± 0.4</cell><cell>81.2 ± 1.4</cell><cell cols="2">76.8 ± 2.7</cell><cell>24.2</cell><cell>5.8</cell></row><row><cell>BERTweet</cell><cell>89.8 ± 0.6</cell><cell cols="2">87.3 ± 0.6</cell><cell>81.8 ± 0.9</cell><cell cols="2">76.6 ± 4.1</cell><cell>29.0</cell><cell>5.0</cell></row><row><cell>DeCLUTR</cell><cell>90.2 ± 0.8</cell><cell cols="2">88.3 ± 0.4</cell><cell>83.7 ± 2.1</cell><cell cols="2">77.8 ± 3.5</cell><cell>30.2</cell><cell>3.2</cell></row><row><cell>ELMo</cell><cell>81.7 ± 2.4</cell><cell cols="2">84.2 ± 0.8</cell><cell>65.8 ± 1.8</cell><cell cols="2">64.3 ± 4.0</cell><cell>30.3</cell><cell>9.4</cell></row><row><cell>ALBERT</cell><cell>85.3 ± 2.9</cell><cell cols="2">84.2 ± 2.7</cell><cell>71.1 ± 2.2</cell><cell cols="2">65.7 ± 3.1</cell><cell>29.4</cell><cell>7.2</cell></row><row><cell>BERT-Tiny</cell><cell>81.6 ± 2.0</cell><cell cols="2">84.7 ± 0.8</cell><cell>67.3 ± 2.0</cell><cell cols="2">61.0 ± 2.5</cell><cell>36.5</cell><cell>7.6</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 3 :</head><label>3</label><figDesc>Other Dataset Results (Macro F1 score) Although in most cases our untuned hyperparameters work well, it appears they are not appropriate for DeCLUTR on FakeNewsNet.</figDesc><table><row><cell></cell><cell>PolitiFact</cell><cell cols="7">GossipCop Twitter15 Twitter16 Twitter15 T/F Twitter16 T/F WNUT-2020 Average Rank</cell></row><row><cell>SOTA</cell><cell>92.8 [77]</cell><cell>85.0 [28]</cell><cell>91.0 [30]</cell><cell>92.4 [30]</cell><cell>82.5 [50]</cell><cell>75.9 [50]</cell><cell>91.0 [43, 58]</cell><cell>4.4</cell></row><row><cell>CT-BERT</cell><cell>86.0 ± 3.2</cell><cell>90.6 ± 0.2</cell><cell cols="2">83.5 ± 2.8 83.9 ± 0.9</cell><cell>93.8 ± 1.6</cell><cell>94.0 ± 3.5</cell><cell>90.6</cell><cell>2.8</cell></row><row><cell>Funnel</cell><cell>86.4 ± 3.2</cell><cell>-a</cell><cell cols="2">66.9 ± 3.0 69.6 ± 2.9</cell><cell>83.2 ± 3.8</cell><cell>90.8 ± 2.2</cell><cell>88.5</cell><cell>-</cell></row><row><cell>RoBERTa</cell><cell>86.7 ± 1.2</cell><cell cols="3">92.8 ± 0.5 81.8 ± 1.5 84.8 ± 1.9</cell><cell>94.4 ± 0.8</cell><cell>95.7 ± 2.8</cell><cell>90.5</cell><cell>2.3</cell></row><row><cell>BERT</cell><cell>81.8 ± 3.0</cell><cell>89.8 ± 0.4</cell><cell cols="2">77.5 ± 3.3 78.2 ± 4.1</cell><cell>89.7 ± 1.6</cell><cell>91.6 ± 4.5</cell><cell>88.5</cell><cell>5.3</cell></row><row><cell>BERTweet</cell><cell>88.5 ± 1.2</cell><cell>92.6 ± 0.6</cell><cell cols="2">76.7 ± 2.9 77.7 ± 2.7</cell><cell>86.7 ± 1.8</cell><cell>92.0 ± 3.7</cell><cell>88.8</cell><cell>4.4</cell></row><row><cell cols="5">DeCLUTR 36.6 ± 1.4 b 43.3 ± 0.4 b 80.4 ± 2.6 80.5 ± 1.7</cell><cell>91.7 ± 1.5</cell><cell>94.5 ± 2.5</cell><cell>89.1</cell><cell>5.1</cell></row><row><cell>ELMo</cell><cell>83.1 ± 1.6</cell><cell>92.0 ± 0.5</cell><cell cols="2">53.7 ± 2.7 55.5 ± 4.9</cell><cell>74.4 ± 3.5</cell><cell>83.3 ± 5.0</cell><cell>82.4</cell><cell>8.0</cell></row><row><cell>ALBERT</cell><cell>80.1 ± 2.9</cell><cell>88.2 ± 0.9</cell><cell cols="2">63.4 ± 4.0 68.0 ± 3.5</cell><cell>83.3 ± 1.8</cell><cell>88.9 ± 4.3</cell><cell>86.8</cell><cell>7.7</cell></row><row><cell>BERT-tiny</cell><cell>85.3 ± 2.8</cell><cell>86.5 ± 0.6</cell><cell cols="2">54.6 ± 3.4 48.8 ± 3.9</cell><cell>77.8 ± 4.8</cell><cell>77.8 ± 4.9</cell><cell>79.9</cell><cell>8.9</cell></row></table><note>a Due to a bug, runs for this model and dataset were lost. See section 3.3 for details. b</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 4 :</head><label>4</label><figDesc>Evaluating tweet ID classification (Macro F1 score)</figDesc><table><row><cell cols="6">Twitter15 False True Unverified Non-rumor Macro Avg.</cell></row><row><cell cols="2">SOTA [30] 92.9</cell><cell>90.5</cell><cell>85.4</cell><cell>95.3</cell><cell>91.0</cell></row><row><cell>2-digit RF</cell><cell>62.4</cell><cell>65.6</cell><cell>61.1</cell><cell>99.4</cell><cell>72.1</cell></row><row><cell>3-digit RF</cell><cell>73.0</cell><cell>69.5</cell><cell>79.7</cell><cell>98.2</cell><cell>80.1</cell></row><row><cell cols="6">Twitter16 False True Unverified Non-rumor Macro Avg.</cell></row><row><cell cols="2">SOTA [30] 91.3</cell><cell>94.7</cell><cell>89.9</cell><cell>93.5</cell><cell>92.4</cell></row><row><cell>2-digit RF</cell><cell>83.5</cell><cell>87.6</cell><cell>82.1</cell><cell>90.7</cell><cell>86.0</cell></row><row><cell>3-digit RF</cell><cell>90.7</cell><cell>95.3</cell><cell>84.4</cell><cell>92.9</cell><cell>90.8</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 5 :</head><label>5</label><figDesc>Label counts of tweets containing "Clinton" and "Trump" We did not compare results on this split, so we do not list a top model entry in that column. Random refers to a stratified random classifier, i.e. outputting a class randomly with probability according to the training label distribution.</figDesc><table><row><cell cols="3">Twitter15 Clinton Trump</cell><cell cols="3">Twitter16 Clinton Trump</cell></row><row><cell>True</cell><cell>0</cell><cell>0</cell><cell>True</cell><cell>0</cell><cell>0</cell></row><row><cell>False</cell><cell>0</cell><cell>0</cell><cell>False</cell><cell>17</cell><cell>18</cell></row><row><cell>Unverified</cell><cell>22</cell><cell>30</cell><cell>Unverified</cell><cell>17</cell><cell>39</cell></row><row><cell>Non-rumor</cell><cell>6</cell><cell>14</cell><cell>Non-rumor</cell><cell>8</cell><cell>6</cell></row><row><cell>full 4-way labels.</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 6 :</head><label>6</label><figDesc>Evaluating tweet ID classification (F1 score)</figDesc><table><row><cell></cell><cell cols="5">PolitiFact GossipCop PHEME9 All CoAID WNUT-2020</cell></row><row><cell cols="2">Top Model 92.8 [77]</cell><cell>91.0</cell><cell>-</cell><cell>82.2</cell><cell>91.6</cell></row><row><cell>Random</cell><cell>50.1</cell><cell>49.9</cell><cell>26.8</cell><cell>35.1</cell><cell>51.4</cell></row><row><cell>2-digit RF</cell><cell>77.3</cell><cell>65.8</cell><cell>25.6</cell><cell>49.5</cell><cell>34.6</cell></row><row><cell>3-digit RF</cell><cell>76.2</cell><cell>66.2</cell><cell>43.5</cell><cell>46.2</cell><cell>51.4</cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0">As available at https://huggingface.co/models</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_1">Available at https://allennlp.org/elmo</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3" xml:id="foot_2">https://www.politifact.com/</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4" xml:id="foot_3">https://www.gossipcop.com/</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="5" xml:id="foot_4">https://github.com/ComplexData-MILA/misinfo-baselines</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="6" xml:id="foot_5">https://github.com/client9/snowflake2time</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="7" xml:id="foot_6">https://github.com/ComplexData-MILA/misinfo-baselines</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="8" xml:id="foot_7">https://developer.twitter.com/en/solutions/academic-research</note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Classification approaches to identify informative tweets</title>
		<author>
			<persName><forename type="first">Piush</forename><surname>Aggarwal</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Student Research Workshop Associated with RANLP 2019</title>
				<meeting>the Student Research Workshop Associated with RANLP 2019</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="7" to="15" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Sentiment aware fake news detection on online social networks</title>
		<author>
			<persName><forename type="first">Oluwaseun</forename><surname>Ajao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Deepayan</forename><surname>Bhowmik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shahrzad</forename><surname>Zargari</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICASSP 2019-2019 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)</title>
				<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="2507" to="2511" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">CIA_NITT at WNUT-2020 Task 2: Classification of COVID-19 Tweets Using Pre-trained Language Models</title>
		<author>
			<persName><forename type="first">Yandrapati</forename><surname>Prakash</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Babu</forename></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Eswari</surname></persName>
		</author>
		<idno>ArXiv abs/2009.05782</idno>
		<imprint>
			<date type="published" when="2020">2020. 2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">Neural machine translation by jointly learning to align and translate</title>
		<author>
			<persName><forename type="first">Dzmitry</forename><surname>Bahdanau</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kyunghyun</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1409.0473</idno>
		<imprint>
			<date type="published" when="2014">2014. 2014</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Semi-supervised learning and graph neural networks for fake news detection</title>
		<author>
			<persName><forename type="first">Adrien</forename><surname>Benamira</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Benjamin</forename><surname>Devillers</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Etienne</forename><surname>Lesot</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Ayush</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Manal</forename><surname>Ray</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fragkiskos</forename><forename type="middle">D</forename><surname>Saadi</surname></persName>
		</author>
		<author>
			<persName><surname>Malliaros</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2019 IEEE/ACM International Conference on Advances in Social Networks Analysis and Mining (ASONAM)</title>
				<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="568" to="569" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<ptr target="https://multimediaeval.github.io/editions/2020/tasks/fakenews/" />
		<title level="m">FakeNews: Corona virus and 5G conspiracy</title>
				<imprint>
			<date type="published" when="2020-10-20">2020. 2020-10-20</date>
		</imprint>
	</monogr>
	<note>MediaEval Multimedia Benchmark</note>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">A survey on fake news and rumour detection techniques</title>
		<author>
			<persName><forename type="first">Alessandro</forename><surname>Bondielli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Francesco</forename><surname>Marcelloni</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Information Sciences</title>
		<imprint>
			<biblScope unit="volume">497</biblScope>
			<biblScope unit="page" from="38" to="55" />
			<date type="published" when="2019">2019. 2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Scientists are drowning in COVID-19 papers. Can new tools keep them afloat</title>
		<author>
			<persName><forename type="first">Jeffrey</forename><surname>Brainard</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Science</title>
		<imprint>
			<date type="published" when="2020">2020. 2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">Language models are few-shot learners</title>
		<author>
			<persName><forename type="first">Benjamin</forename><surname>Tom B Brown</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nick</forename><surname>Mann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Melanie</forename><surname>Ryder</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jared</forename><surname>Subbiah</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Prafulla</forename><surname>Kaplan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Arvind</forename><surname>Dhariwal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pranav</forename><surname>Neelakantan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Girish</forename><surname>Shyam</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Amanda</forename><surname>Sastry</surname></persName>
		</author>
		<author>
			<persName><surname>Askell</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2005.14165</idno>
		<imprint>
			<date type="published" when="2020">2020. 2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">Automatic Rumor Detection on Microblogs: A Survey</title>
		<author>
			<persName><forename type="first">Juan</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Junbo</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xirong</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhiwei</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Han</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jintao</forename><surname>Li</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1807.03505</idno>
		<ptr target="http://arxiv.org/abs/1807.03505" />
		<imprint>
			<date type="published" when="2018">2018. 2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Information credibility on twitter</title>
		<author>
			<persName><forename type="first">Carlos</forename><surname>Castillo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Marcelo</forename><surname>Mendoza</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Barbara</forename><surname>Poblete</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 20th international conference on World wide web</title>
				<meeting>the 20th international conference on World wide web</meeting>
		<imprint>
			<date type="published" when="2011">2011</date>
			<biblScope unit="page" from="675" to="684" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">NEU at WNUT-2020 Task 2: Data Augmentation To Tell BERT That Death Is Not Necessarily Informative</title>
		<author>
			<persName><forename type="first">Kumud</forename><surname>Chauhan</surname></persName>
		</author>
		<idno>ArXiv abs/2009.08590</idno>
		<imprint>
			<date type="published" when="2020">2020. 2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Uncovering sentiment and retweet patterns of disaster-related tweets from a spatiotemporal perspective-A case study of Hurricane Harvey</title>
		<author>
			<persName><forename type="first">Sijing</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jin</forename><surname>Mao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gang</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chao</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yujie</forename><surname>Cao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Telematics and Informatics</title>
		<imprint>
			<biblScope unit="volume">47</biblScope>
			<biblScope unit="page">101326</biblScope>
			<date type="published" when="2020">2020. 2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Proactive Discovery of Fake News Domains from Real-Time Social Media Feeds</title>
		<author>
			<persName><forename type="first">Zhouhan</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Juliana</forename><surname>Freire</surname></persName>
		</author>
		<idno type="DOI">10.1145/3366424.3385772</idno>
		<ptr target="https://doi.org/10.1145/3366424.3385772" />
	</analytic>
	<monogr>
		<title level="m">Companion of The 2020 Web Conference 2020</title>
				<editor>
			<persName><forename type="first">Amal</forename><forename type="middle">El</forename></persName>
		</editor>
		<editor>
			<persName><forename type="first">Fallah</forename><surname>Seghrouchni</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Gita</forename><surname>Sukthankar</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Tie-Yan</forename><surname>Liu</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Maarten</forename><surname>Van Steen</surname></persName>
		</editor>
		<meeting><address><addrLine>Taipei, Taiwan</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2020-04-20">2020. April 20-24, 2020</date>
			<biblScope unit="page" from="584" to="592" />
		</imprint>
	</monogr>
	<note>ACM / IW3C2</note>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">VRoC: Variational Autoencoder-aided Multi-task Rumor Classifier Based on Text</title>
		<author>
			<persName><forename type="first">Mingxi</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shahin</forename><surname>Nazarian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Paul</forename><surname>Bogdan</surname></persName>
		</author>
		<idno type="DOI">10.1145/3366423.3380054</idno>
		<ptr target="https://doi.org/10.1145/3366423.3380054" />
	</analytic>
	<monogr>
		<title level="m">WWW &apos;20: The Web Conference</title>
				<editor>
			<persName><forename type="first">Yennun</forename><surname>Huang</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Irwin</forename><surname>King</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Tie-Yan</forename><surname>Liu</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Maarten</forename><surname>Van Steen</surname></persName>
		</editor>
		<meeting><address><addrLine>Taipei, Taiwan</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2020-04-20">2020. 2020. April 20-24, 2020</date>
			<biblScope unit="page" from="2892" to="2898" />
		</imprint>
	</monogr>
	<note>ACM / IW3C2</note>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">VRoC: Variational Autoencoder-aided Multi-task Rumor Classifier Based on Text</title>
		<author>
			<persName><forename type="first">Mingxi</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shahin</forename><surname>Nazarian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Paul</forename><surname>Bogdan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of The Web Conference</title>
				<meeting>The Web Conference</meeting>
		<imprint>
			<date type="published" when="2020">2020. 2020</date>
			<biblScope unit="page" from="2892" to="2898" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">Learning phrase representations using RNN encoder-decoder for statistical machine translation</title>
		<author>
			<persName><forename type="first">Kyunghyun</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bart</forename><surname>Van Merriënboer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Caglar</forename><surname>Gulcehre</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dzmitry</forename><surname>Bahdanau</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fethi</forename><surname>Bougares</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Holger</forename><surname>Schwenk</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1406.1078</idno>
		<imprint>
			<date type="published" when="2014">2014. 2014</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<author>
			<persName><forename type="first">Matteo</forename><surname>Cinelli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Walter</forename><surname>Quattrociocchi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alessandro</forename><surname>Galeazzi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Carlo</forename><forename type="middle">Michele</forename><surname>Valensise</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Emanuele</forename><surname>Brugnoli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ana</forename><forename type="middle">Lucia</forename><surname>Schmidt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Paola</forename><surname>Zola</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fabiana</forename><surname>Zollo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Antonio</forename><surname>Scala</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2003.05004</idno>
		<title level="m">The covid-19 social media infodemic</title>
				<imprint>
			<date type="published" when="2020">2020. 2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<author>
			<persName><forename type="first">Limeng</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dongwon</forename><surname>Lee</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2006.00885[cs.SI]</idno>
		<title level="m">CoAID: COVID-19 Healthcare Misinformation Dataset</title>
				<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">DETERRENT: Knowledge Guided Graph Attention Network for Detecting Healthcare Misinformation</title>
		<author>
			<persName><forename type="first">Limeng</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Haeseung</forename><surname>Seo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Maryam</forename><surname>Tabar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fenglong</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Suhang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dongwon</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 26th ACM SIGKDD International Conference on Knowledge Discovery &amp; Data Mining</title>
				<meeting>the 26th ACM SIGKDD International Conference on Knowledge Discovery &amp; Data Mining</meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="492" to="502" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">SAME: sentiment-aware multi-modal embedding for detecting fake news</title>
		<author>
			<persName><forename type="first">Limeng</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Suhang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dongwon</forename><surname>Lee</surname></persName>
		</author>
		<idno type="DOI">10.1145/3341161.3342894</idno>
		<ptr target="https://doi.org/10.1145/3341161.3342894" />
	</analytic>
	<monogr>
		<title level="m">ASONAM &apos;19: International Conference on Advances in Social Networks Analysis and Mining</title>
				<editor>
			<persName><forename type="first">Francesca</forename><surname>Spezzano</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Wei</forename><surname>Chen</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Xiaokui</forename><surname>Xiao</surname></persName>
		</editor>
		<meeting><address><addrLine>Vancouver, British Columbia, Canada</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2019-08-30">2019. 27-30 August, 2019</date>
			<biblScope unit="page" from="41" to="48" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">Funnel-Transformer: Filtering out Sequential Redundancy for Efficient Language Processing</title>
		<author>
			<persName><forename type="first">Zihang</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Guokun</forename><surname>Lai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yiming</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Quoc</surname></persName>
		</author>
		<author>
			<persName><surname>Le</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2006.03236[cs.LG]</idno>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<author>
			<persName><forename type="first">Ronald</forename><surname>Denaux</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jose</forename><surname>Manuel Gomez-Perez</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2008.12742</idno>
		<title level="m">Linked Credibility Reviews for Explainable Misinformation Detection</title>
				<imprint>
			<date type="published" when="2020">2020. 2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding</title>
		<author>
			<persName><forename type="first">Jacob</forename><surname>Devlin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ming-Wei</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kenton</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kristina</forename><surname>Toutanova</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1810.04805</idno>
		<ptr target="http://arxiv.org/abs/1810.04805" />
		<imprint>
			<date type="published" when="2018">2018. 2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<author>
			<persName><forename type="first">Matt</forename><surname>Gardner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Joel</forename><surname>Grus</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mark</forename><surname>Neumann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Oyvind</forename><surname>Tafjord</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pradeep</forename><surname>Dasigi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nelson</forename><forename type="middle">F</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Matthew</forename><surname>Peters</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><surname>Schmitz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Luke</forename><forename type="middle">S</forename><surname>Zettlemoyer</surname></persName>
		</author>
		<idno type="arXiv">arXiv:arXiv:1803.07640</idno>
		<title level="m">AllenNLP: A Deep Semantic Natural Language Processing Platform</title>
				<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title level="m" type="main">DeCLUTR: Deep Contrastive Learning for Unsupervised Textual Representations</title>
		<author>
			<persName><forename type="first">Osvald</forename><surname>John M Giorgi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gary</forename><forename type="middle">D</forename><surname>Nitski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bo</forename><surname>Bader</surname></persName>
		</author>
		<author>
			<persName><surname>Wang</surname></persName>
		</author>
		<idno>ArXiv abs/2006.03659</idno>
		<imprint>
			<date type="published" when="2020">2020. 2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Fake news on Twitter during the 2016 U.S. presidential election</title>
		<author>
			<persName><forename type="first">Nir</forename><surname>Grinberg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kenneth</forename><surname>Joseph</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lisa</forename><surname>Friedland</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Briony</forename><surname>Swire-Thompson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><surname>Lazer</surname></persName>
		</author>
		<idno type="DOI">10.1126/science.aau2706</idno>
		<ptr target="https://science.sciencemag.org/content/363/6425/374.full.pdf" />
	</analytic>
	<monogr>
		<title level="j">Science</title>
		<imprint>
			<biblScope unit="volume">363</biblScope>
			<biblScope unit="page" from="374" to="378" />
			<date type="published" when="2019">2019. 2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<author>
			<persName><forename type="first">Yi</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shanika</forename><surname>Karunasekera</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christopher</forename><surname>Leckie</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2007.03316</idno>
		<title level="m">Graph Neural Networks with Continual Learning for Fake News Detection from Social Media</title>
				<imprint>
			<date type="published" when="2020">2020. 2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Long short-term memory</title>
		<author>
			<persName><forename type="first">Sepp</forename><surname>Hochreiter</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jürgen</forename><surname>Schmidhuber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural computation</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="page" from="1735" to="1780" />
			<date type="published" when="1997">1997. 1997</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<title level="m" type="main">Heterogeneous Graph Attention Networks for Early Detection of Rumors on Twitter</title>
		<author>
			<persName><forename type="first">Qi</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Junshuai</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jia</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bin</forename><surname>Wang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2006.05866</idno>
		<imprint>
			<date type="published" when="2020">2020. 2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Conquering Cross-source Failure for News Credibility: Learning Generalizable Representations beyond Content Embedding</title>
		<author>
			<persName><forename type="first">Yen-Hao</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ting-Wei</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ssu-Rui</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fernando</forename></persName>
		</author>
		<author>
			<persName><forename type="first">Henrique</forename><forename type="middle">Calderon</forename><surname>Alvarado</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yi-Shin</forename><surname>Chen</surname></persName>
		</author>
		<idno type="DOI">10.1145/3366423.3380158</idno>
		<ptr target="https://doi.org/10.1145/3366423.3380158" />
	</analytic>
	<monogr>
		<title level="m">WWW &apos;20: The Web Conference</title>
				<editor>
			<persName><forename type="first">Irwin</forename><surname>Huang</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Tie-Yan</forename><surname>King</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Maarten</forename><surname>Liu</surname></persName>
		</editor>
		<editor>
			<persName><surname>Van Steen</surname></persName>
		</editor>
		<meeting><address><addrLine>Taipei, Taiwan</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2020-04-20">2020. 2020. April 20-24, 2020</date>
			<biblScope unit="page" from="774" to="784" />
		</imprint>
	</monogr>
	<note>ACM / IW3C2</note>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Conquering Cross-source Failure for News Credibility: Learning Generalizable Representations beyond Content Embedding</title>
		<author>
			<persName><forename type="first">Yen-Hao</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ting-Wei</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ssu-Rui</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fernando</forename></persName>
		</author>
		<author>
			<persName><forename type="first">Henrique</forename><forename type="middle">Calderon</forename><surname>Alvarado</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yi-Shin</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of The Web Conference</title>
				<meeting>The Web Conference</meeting>
		<imprint>
			<date type="published" when="2020">2020. 2020</date>
			<biblScope unit="page" from="774" to="784" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
		<title level="m" type="main">BANANA at WNUT-2020 Task 2: Identifying COVID-19 Information on Twitter by Combining Deep Learning and Transfer Learning Models</title>
		<author>
			<persName><forename type="first">Tin</forename><surname>Van Huynh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Nguyen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Son</forename><forename type="middle">T</forename><surname>Luu</surname></persName>
		</author>
		<idno>ArXiv abs/2009.02671</idno>
		<imprint>
			<date type="published" when="2020">2020. 2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Twitter as a Lifeline: Human-annotated Twitter Corpora for NLP of Crisis-related Messages</title>
		<author>
			<persName><forename type="first">Muhammad</forename><surname>Imran</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Prasenjit</forename><surname>Mitra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Carlos</forename><surname>Castillo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Tenth International Conference on Language Resources and Evaluation (LREC 2016)</title>
				<meeting>the Tenth International Conference on Language Resources and Evaluation (LREC 2016)<address><addrLine>Portoroz, Slovenia; Paris, France</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="23" to="28" />
		</imprint>
	</monogr>
	<note>European Language Resources Association (ELRA)</note>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">RumorSleuth: joint detection of rumor veracity and user stance</title>
		<author>
			<persName><forename type="first">Mohammad Raihanul</forename><surname>Islam</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sathappan</forename><surname>Muthiah</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Naren</forename><surname>Ramakrishnan</surname></persName>
		</author>
		<idno type="DOI">10.1145/3341161.3342916</idno>
		<ptr target="https://doi.org/10.1145/3341161.3342916" />
	</analytic>
	<monogr>
		<title level="m">ASONAM &apos;19: International Conference on Advances in Social Networks Analysis and Mining</title>
				<editor>
			<persName><forename type="first">Francesca</forename><surname>Spezzano</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Wei</forename><surname>Chen</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Xiaokui</forename><surname>Xiao</surname></persName>
		</editor>
		<meeting><address><addrLine>Vancouver, British Columbia, Canada</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2019-08-30">2019. 27-30 August, 2019</date>
			<biblScope unit="page" from="131" to="136" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Assessment of tweet credibility with LDA features</title>
		<author>
			<persName><forename type="first">Jun</forename><surname>Ito</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jing</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hiroyuki</forename><surname>Toda</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yoshimasa</forename><surname>Koike</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Satoshi</forename><surname>Oyama</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 24th International Conference on World Wide Web</title>
				<meeting>the 24th International Conference on World Wide Web</meeting>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="953" to="958" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<monogr>
		<title level="m" type="main">Bag of tricks for efficient text classification</title>
		<author>
			<persName><forename type="first">Armand</forename><surname>Joulin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Edouard</forename><surname>Grave</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Piotr</forename><surname>Bojanowski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tomas</forename><surname>Mikolov</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1607.01759</idno>
		<imprint>
			<date type="published" when="2016">2016. 2016</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Determining disaster severity through social media analysis: Testing the methodology with South East Queensland Flood tweets</title>
		<author>
			<persName><forename type="first">Nayomi</forename><surname>Kankanamge</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tan</forename><surname>Yigitcanlar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ashantha</forename><surname>Goonetilleke</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Md</forename><surname>Kamruzzaman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International journal of disaster risk reduction</title>
		<imprint>
			<biblScope unit="volume">42</biblScope>
			<biblScope unit="page">101360</biblScope>
			<date type="published" when="2020">2020. 2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<monogr>
		<author>
			<persName><forename type="first">Yoon</forename><surname>Kim</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1408.5882</idno>
		<title level="m">Convolutional neural networks for sentence classification</title>
				<imprint>
			<date type="published" when="2014">2014. 2014</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b39">
	<monogr>
		<author>
			<persName><forename type="first">P</forename><surname>Diederik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Max</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName><surname>Welling</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1312.6114</idno>
		<title level="m">Auto-encoding variational bayes</title>
				<imprint>
			<date type="published" when="2013">2013. 2013</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b40">
	<monogr>
		<title level="m" type="main">All-in-one: Multitask learning for rumour verification</title>
		<author>
			<persName><forename type="first">Elena</forename><surname>Kochkina</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Maria</forename><surname>Liakata</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Arkaitz</forename><surname>Zubiaga</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1806.03713</idno>
		<imprint>
			<date type="published" when="2018">2018. 2018</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b41">
	<monogr>
		<author>
			<persName><forename type="first">Priyanshu</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aadarsh</forename><surname>Singh</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2010.04335</idno>
		<title level="m">NutCracker at WNUT-2020 Task 2: Robustly Identifying Informative COVID-19 Tweets using Ensembling and Adversarial Training</title>
				<imprint>
			<date type="published" when="2020">2020. 2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Tweettracker: An analysis tool for humanitarian and disaster relief</title>
		<author>
			<persName><forename type="first">Shamanth</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Geoffrey</forename><surname>Barbier</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mohammad</forename><surname>Ali Abbasi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Huan</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ICwSM</title>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="page" from="78" to="82" />
			<date type="published" when="2011">2011. 2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<monogr>
		<title level="m" type="main">ALBERT: A Lite BERT for Self-supervised Learning of Language Representations</title>
		<author>
			<persName><forename type="first">Zhenzhong</forename><surname>Lan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mingda</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sebastian</forename><surname>Goodman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kevin</forename><surname>Gimpel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Piyush</forename><surname>Sharma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Radu</forename><surname>Soricut</surname></persName>
		</author>
		<idno>ArXiv abs/1909.11942</idno>
		<imprint>
			<date type="published" when="2020">2020. 2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Distributed representations of sentences and documents</title>
		<author>
			<persName><forename type="first">Quoc</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tomas</forename><surname>Mikolov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International conference on machine learning</title>
				<imprint>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="1188" to="1196" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Characterizing the Propagation of Situational Information in Social Media During COVID-19 Epidemic: A Case Study on Weibo</title>
		<author>
			<persName><forename type="first">L</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Duan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">K</forename><surname>Tsoi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Computational Social Systems</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page" from="556" to="562" />
			<date type="published" when="2020">2020. 2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Real-time rumor debunking on twitter</title>
		<author>
			<persName><forename type="first">Xiaomo</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Armineh</forename><surname>Nourbakhsh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Quanzhi</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rui</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sameena</forename><surname>Shah</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 24th ACM International on Conference on Information and Knowledge Management</title>
				<meeting>the 24th ACM International on Conference on Information and Knowledge Management</meeting>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="1867" to="1870" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<monogr>
		<author>
			<persName><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Myle</forename><surname>Ott</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Naman</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jingfei</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mandar</forename><surname>Joshi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Danqi</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Omer</forename><surname>Levy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Lewis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Luke</forename><surname>Zettlemoyer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Veselin</forename><surname>Stoyanov</surname></persName>
		</author>
		<idno>ArXiv abs/1907.11692</idno>
		<title level="m">RoBERTa: A Robustly Optimized BERT Pretraining Approach</title>
				<imprint>
			<date type="published" when="2019">2019. 2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<monogr>
		<title/>
		<author>
			<persName><forename type="first">Ilya</forename><surname>Loshchilov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Frank</forename><surname>Hutter</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1711.05101[cs.LG]</idno>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
	<note type="report_type">Decoupled Weight Decay Regularization</note>
</biblStruct>

<biblStruct xml:id="b49">
	<monogr>
		<title level="m" type="main">GCAN: Graph-aware co-attention networks for explainable fake news detection on social media</title>
		<author>
			<persName><forename type="first">Yi-Ju</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Cheng-Te</forename><surname>Li</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2004.11648</idno>
		<imprint>
			<date type="published" when="2020">2020. 2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b50">
	<monogr>
		<title level="m" type="main">Detecting rumors from microblogs with recurrent neural networks</title>
		<author>
			<persName><forename type="first">Jing</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wei</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Prasenjit</forename><surname>Mitra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sejeong</forename><surname>Kwon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bernard</forename><forename type="middle">J</forename><surname>Jansen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kam-Fai</forename><surname>Wong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Meeyoung</forename><surname>Cha</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016">2016. 2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<monogr>
		<title level="m" type="main">Detect rumors in microblog posts using propagation structure via kernel learning</title>
		<author>
			<persName><forename type="first">Jing</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wei</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kam-Fai</forename><surname>Wong</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017">2017</date>
			<publisher>Association for Computational Linguistics</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<monogr>
		<title level="m" type="main">EdinburghNLP at WNUT-2020 Task 2: Leveraging Transformers with Generalized Augmentation for Identifying Informativeness in COVID-19 Tweets</title>
		<author>
			<persName><forename type="first">Nickil</forename><surname>Maveli</surname></persName>
		</author>
		<idno>ArXiv abs/2009.06375</idno>
		<imprint>
			<date type="published" when="2020">2020. 2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">Fake news, rumor, information pollution in social media and web: A contemporary survey of stateof-the-arts, challenges and opportunities</title>
		<author>
			<persName><forename type="first">Priyanka</forename><surname>Meel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dinesh</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vishwakarma</forename></persName>
		</author>
		<idno type="DOI">10.1016/j.eswa.2019.112986</idno>
		<ptr target="https://doi.org/10.1016/j.eswa.2019.112986" />
	</analytic>
	<monogr>
		<title level="j">Expert Systems with Applications</title>
		<imprint>
			<biblScope unit="volume">153</biblScope>
			<biblScope unit="page">112986</biblScope>
			<date type="published" when="2020">2020. 2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<monogr>
		<author>
			<persName><forename type="first">Tomas</forename><surname>Mikolov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kai</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Greg</forename><surname>Corrado</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jeffrey</forename><surname>Dean</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1301.3781</idno>
		<title level="m">Efficient estimation of word representations in vector space</title>
				<imprint>
			<date type="published" when="2013">2013. 2013</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b55">
	<analytic>
		<title level="a" type="main">Distributed representations of words and phrases and their compositionality</title>
		<author>
			<persName><forename type="first">Tomas</forename><surname>Mikolov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kai</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Greg</forename><forename type="middle">S</forename><surname>Corrado</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jeff</forename><surname>Dean</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
				<imprint>
			<date type="published" when="2013">2013</date>
			<biblScope unit="page" from="3111" to="3119" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<monogr>
		<author>
			<persName><forename type="first">Shervin</forename><surname>Minaee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nal</forename><surname>Kalchbrenner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Erik</forename><surname>Cambria</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Narjes</forename><surname>Nikzad</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Meysam</forename><surname>Chenaghlu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jianfeng</forename><surname>Gao</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2004.03705</idno>
		<title level="m">Deep learning based text classification: A comprehensive review</title>
				<imprint>
			<date type="published" when="2020">2020. 2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b57">
	<analytic>
		<title level="a" type="main">NLP North at WNUT-2020 Task 2: Pre-training versus Ensembling for Detection of Informative COVID-19 English Tweets</title>
		<author>
			<persName><forename type="first">Anders</forename><surname>Giovanni</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Møller</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Sixth Workshop on Noisy User-generated Text</title>
				<meeting>the Sixth Workshop on Noisy User-generated Text</meeting>
		<imprint>
			<date type="published" when="2020">2020. 2020</date>
			<biblScope unit="page" from="331" to="336" />
		</imprint>
	</monogr>
	<note>Rob Van Der Goot, and Barbara Plank</note>
</biblStruct>

<biblStruct xml:id="b58">
	<monogr>
		<title level="m" type="main">Fake News, Conspiracies and Myth Debunking in Social Media-A Literature Survey Across Disciplines. Conspiracies and Myth Debunking in Social Media-A Literature Survey Across Disciplines</title>
		<author>
			<persName><forename type="first">Valeryia</forename><surname>Mosinzova</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Benjamin</forename><surname>Fabian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tatiana</forename><surname>Ermakova</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Annika</forename><surname>Baumann</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019-02-03">2019. February 3, 2019. 2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b59">
	<monogr>
		<author>
			<persName><forename type="first">Martin</forename><surname>Müller</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Marcel</forename><surname>Salathé</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Per</forename><forename type="middle">E</forename><surname>Kummervold</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2005.07503</idno>
		<title level="m">COVID-Twitter-BERT: A Natural Language Processing Model to Analyse COVID-19 Content on Twitter</title>
				<imprint>
			<date type="published" when="2020">2020. 2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b60">
	<monogr>
		<title level="m" type="main">TATL at W-NUT 2020 Task 2: A Transformer-based Baseline System for Identification of Informative COVID-19 English Tweets</title>
		<author>
			<persName><forename type="first">A</forename><surname>Nguyen</surname></persName>
		</author>
		<idno>ArXiv abs/2008.12854</idno>
		<imprint>
			<date type="published" when="2020">2020. 2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b61">
	<analytic>
		<title level="a" type="main">BERTweet: A pretrained language model for English Tweets</title>
		<author>
			<persName><forename type="first">Thanh</forename><surname>Dat Quoc Nguyen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Anh</forename><forename type="middle">Tuan</forename><surname>Vu</surname></persName>
		</author>
		<author>
			<persName><surname>Nguyen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing: System Demonstrations</title>
				<meeting>the 2020 Conference on Empirical Methods in Natural Language Processing: System Demonstrations</meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b62">
	<analytic>
		<title level="a" type="main">WNUT-2020 Task 2: Identification of Informative COVID-19 English Tweets</title>
		<author>
			<persName><forename type="first">Thanh</forename><surname>Dat Quoc Nguyen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Afshin</forename><surname>Vu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mai</forename><forename type="middle">Hoang</forename><surname>Rahimi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Linh</forename><forename type="middle">The</forename><surname>Dao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Long</forename><surname>Nguyen</surname></persName>
		</author>
		<author>
			<persName><surname>Doan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 6th Workshop on Noisy User-generated Text</title>
				<meeting>the 6th Workshop on Noisy User-generated Text</meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b63">
	<monogr>
		<title/>
		<author>
			<persName><forename type="first">S</forename><surname>Adam Paszke</surname></persName>
		</author>
		<author>
			<persName><surname>Gross</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Soumith Chintala</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Chanan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zachary</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zeming</forename><surname>Devito</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alban</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Desmaison</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Antiga</surname></persName>
		</author>
		<author>
			<persName><surname>Lerer</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
	<note>PyTorch</note>
</biblStruct>

<biblStruct xml:id="b64">
	<analytic>
		<author>
			<persName><forename type="first">F</forename><surname>Pedregosa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Varoquaux</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Gramfort</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Michel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Thirion</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Grisel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Blondel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Prettenhofer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Weiss</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Dubourg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Vanderplas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Passos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Cournapeau</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Brucher</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Perrot</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Duchesnay</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Scikit-learn: Machine Learning in Python</title>
				<imprint>
			<date type="published" when="2011">2011. 2011</date>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="page" from="2825" to="2830" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b65">
	<analytic>
		<title level="a" type="main">GloVe: Global Vectors for Word Representation</title>
		<author>
			<persName><forename type="first">Jeffrey</forename><surname>Pennington</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christopher</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
		<ptr target="http://www.aclweb.org/anthology/D14-1162" />
	</analytic>
	<monogr>
		<title level="m">Empirical Methods in Natural Language Processing (EMNLP)</title>
				<imprint>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="1532" to="1543" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b66">
	<monogr>
		<title level="m" type="main">CXP949 at WNUT-2020 Task 2: Extracting Informative COVID-19 Tweets -RoBERTa Ensembles and The Continued Relevance of Handcrafted Features</title>
		<author>
			<persName><forename type="first">Calum</forename><surname>Perrio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Harish</forename><surname>Tayyar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Madabushi</forename></persName>
		</author>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b67">
	<analytic>
		<title level="a" type="main">Deep contextualized word representations</title>
		<author>
			<persName><forename type="first">Matthew</forename><forename type="middle">E</forename><surname>Peters</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mark</forename><surname>Neumann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mohit</forename><surname>Iyyer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Matt</forename><surname>Gardner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christopher</forename><surname>Clark</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kenton</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Luke</forename><surname>Zettlemoyer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of NAACL</title>
				<meeting>of NAACL</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b68">
	<analytic>
		<title level="a" type="main">COVID-19 infodemic: More retweets for science-based information on coronavirus than for false information</title>
		<author>
			<persName><forename type="first">Cristina</forename><forename type="middle">M</forename><surname>Pulido</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Beatriz</forename><surname>Villarejo-Carballido</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gisela</forename><surname>Redondo-Sama</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aitor</forename><surname>Gómez</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Sociology</title>
		<imprint>
			<biblScope unit="page">0268580920914755</biblScope>
			<date type="published" when="2020">2020. 2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b69">
	<analytic>
		<title level="a" type="main">Prediction of Number of Cases of 2019 Novel Coronavirus (COVID-19) Using Social Media Search Index</title>
		<author>
			<persName><forename type="first">Lei</forename><surname>Qin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qiang</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yidan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ke-Fei</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mingchih</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ben-Chang</forename><surname>Shia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Szu-Yuan</forename><surname>Wu</surname></persName>
		</author>
		<idno type="DOI">10.3390/ijerph17072365</idno>
		<ptr target="https://doi.org/10.3390/ijerph17072365" />
	</analytic>
	<monogr>
		<title level="j">International Journal of Environmental Research and Public Health</title>
		<imprint>
			<biblScope unit="volume">17</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page">2365</biblScope>
			<date type="published" when="2020-03">2020. Mar 2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b70">
	<monogr>
		<title level="m" type="main">Improving language understanding by generative pre-training</title>
		<author>
			<persName><forename type="first">Alec</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Karthik</forename><surname>Narasimhan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tim</forename><surname>Salimans</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b71">
	<monogr>
		<title level="m" type="main">Language Models are Unsupervised Multitask Learners</title>
		<author>
			<persName><forename type="first">Alec</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jeff</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rewon</forename><surname>Child</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><surname>Luan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dario</forename><surname>Amodei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019">2019. 2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b72">
	<monogr>
		<title level="m" type="main">Can &apos;fake news&apos; impact the stock market? by Forbes</title>
		<author>
			<persName><forename type="first">Kenneth</forename><surname>Rapoza</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017">2017. 2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b73">
	<analytic>
		<title level="a" type="main">A Kernel of Truth: Determining Rumor Veracity on Twitter by Diffusion Pattern Alone</title>
		<author>
			<persName><forename type="first">Nir</forename><surname>Rosenfeld</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aron</forename><surname>Szanto</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><forename type="middle">C</forename><surname>Parkes</surname></persName>
		</author>
		<idno type="DOI">10.1145/3366423.3380180</idno>
		<ptr target="https://doi.org/10.1145/3366423.3380180" />
	</analytic>
	<monogr>
		<title level="m">WWW &apos;20: The Web Conference</title>
				<editor>
			<persName><forename type="first">Irwin</forename><surname>Huang</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Tie-Yan</forename><surname>King</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Maarten</forename><surname>Liu</surname></persName>
		</editor>
		<editor>
			<persName><surname>Van Steen</surname></persName>
		</editor>
		<meeting><address><addrLine>Taipei, Taiwan</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2020-04-20">2020. 2020. April 20-24, 2020</date>
			<biblScope unit="page" from="1018" to="1028" />
		</imprint>
	</monogr>
	<note>ACM / IW3C2</note>
</biblStruct>

<biblStruct xml:id="b74">
	<analytic>
		<title level="a" type="main">Fake news or truth? using satirical cues to detect potentially misleading news</title>
		<author>
			<persName><forename type="first">Niall</forename><surname>Victoria L Rubin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yimin</forename><surname>Conroy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sarah</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><surname>Cornwell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the second workshop on computational approaches to deception detection</title>
				<meeting>the second workshop on computational approaches to deception detection</meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="7" to="17" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b75">
	<analytic>
		<title level="a" type="main">The graph neural network model</title>
		<author>
			<persName><forename type="first">Franco</forename><surname>Scarselli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Marco</forename><surname>Gori</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chung</forename><surname>Ah</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Markus</forename><surname>Tsoi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gabriele</forename><surname>Hagenbuchner</surname></persName>
		</author>
		<author>
			<persName><surname>Monfardini</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Neural Networks</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="page" from="61" to="80" />
			<date type="published" when="2008">2008. 2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b76">
	<analytic>
		<title level="a" type="main">defend: Explainable fake news detection</title>
		<author>
			<persName><forename type="first">Kai</forename><surname>Shu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Limeng</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Suhang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dongwon</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Huan</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 25th ACM SIGKDD International Conference on Knowledge Discovery &amp; Data Mining</title>
				<meeting>the 25th ACM SIGKDD International Conference on Knowledge Discovery &amp; Data Mining</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="395" to="405" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b77">
	<analytic>
		<title level="a" type="main">dEFEND: Explainable Fake News Detection</title>
		<author>
			<persName><forename type="first">Kai</forename><surname>Shu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Limeng</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Suhang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dongwon</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Huan</forename><surname>Liu</surname></persName>
		</author>
		<idno type="DOI">10.1145/3292500.3330935</idno>
		<ptr target="https://doi.org/10.1145/3292500.3330935" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 25th ACM SIGKDD International Conference on Knowledge Discovery &amp; Data Mining, KDD 2019</title>
				<editor>
			<persName><forename type="first">Ankur</forename><surname>Teredesai</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Vipin</forename><surname>Kumar</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Ying</forename><surname>Li</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Rómer</forename><surname>Rosales</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Evimaria</forename><surname>Terzi</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">George</forename><surname>Karypis</surname></persName>
		</editor>
		<meeting>the 25th ACM SIGKDD International Conference on Knowledge Discovery &amp; Data Mining, KDD 2019<address><addrLine>Anchorage, AK, USA</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2019-08-04">2019. August 4-8, 2019</date>
			<biblScope unit="page" from="395" to="405" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b78">
	<monogr>
		<title level="m" type="main">FakeNewsNet: A Data Repository with News Content, Social Context and Dynamic Information for Studying Fake News on Social Media</title>
		<author>
			<persName><forename type="first">Kai</forename><surname>Shu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Deepak</forename><surname>Mahudeswaran</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Suhang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dongwon</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Huan</forename><surname>Liu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1809.01286</idno>
		<imprint>
			<date type="published" when="2018">2018. 2018</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b79">
	<analytic>
		<title/>
		<author>
			<persName><forename type="first">Kai</forename><surname>Shu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Amy</forename><surname>Sliva</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Suhang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiliang</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Huan</forename><surname>Liu</surname></persName>
		</author>
		<idno type="DOI">10.1145/3137597.3137600</idno>
		<ptr target="https://doi.org/10.1145/3137597.3137600" />
	</analytic>
	<monogr>
		<title level="j">Fake News Detection on Social Media: A Data Mining Perspective. SIGKDD Explorations</title>
		<imprint>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="page" from="22" to="36" />
			<date type="published" when="2017">2017. 2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b80">
	<analytic>
		<title level="a" type="main">Fake news detection on social media: A data mining perspective</title>
		<author>
			<persName><forename type="first">Kai</forename><surname>Shu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Amy</forename><surname>Sliva</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Suhang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiliang</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Huan</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM SIGKDD Explorations Newsletter</title>
		<imprint>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="page" from="22" to="36" />
			<date type="published" when="2017">2017. 2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b81">
	<monogr>
		<title/>
		<author>
			<persName><forename type="first">Kai</forename><surname>Shu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Suhang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Huan</forename><surname>Liu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1712.07709</idno>
		<imprint>
			<date type="published" when="2017">2017. 2017</date>
		</imprint>
	</monogr>
	<note type="report_type">Exploiting Tri-Relationship for Fake News Detection. arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b82">
	<analytic>
		<title level="a" type="main">Ahmed Hassan Awadallah, Scott Ruston, and Huan Liu</title>
		<author>
			<persName><forename type="first">Kai</forename><surname>Shu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Guoqing</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yichuan</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Subhabrata</forename><surname>Mukherjee</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2004.01732</idno>
	</analytic>
	<monogr>
		<title level="m">Leveraging Multi-Source Weak Social Supervision for Early Detection of Fake News</title>
				<imprint>
			<date type="published" when="2020">2020. 2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b83">
	<analytic>
		<title level="a" type="main">Event classification and location prediction from tweets during disasters</title>
		<author>
			<persName><forename type="first">Jyoti</forename><forename type="middle">Prakash</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Yogesh</surname></persName>
		</author>
		<author>
			<persName><surname>Dwivedi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Abhinav</forename><surname>Nripendra P Rana</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kawaljeet</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kapoor</forename><surname>Kaur</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Annals of Operations Research</title>
		<imprint>
			<biblScope unit="volume">283</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="737" to="757" />
			<date type="published" when="2019">2019. 2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b84">
	<analytic>
		<title level="a" type="main">Communicating on Twitter during a disaster: An analysis of tweets during Typhoon Haiyan in the Philippines</title>
		<author>
			<persName><forename type="first">Bruno</forename><surname>Takahashi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Edson</forename><forename type="middle">C</forename><surname>Tandoc</surname><genName>Jr</genName></persName>
		</author>
		<author>
			<persName><forename type="first">Christine</forename><surname>Carmichael</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computers in human behavior</title>
		<imprint>
			<biblScope unit="volume">50</biblScope>
			<biblScope unit="page" from="392" to="398" />
			<date type="published" when="2015">2015. 2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b85">
	<analytic>
		<title level="a" type="main">On identifying disaster-related tweets: Matching-based or learning-based?</title>
		<author>
			<persName><forename type="first">Hien</forename><surname>To</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sumeet</forename><surname>Agrawal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Seon</forename><surname>Ho Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Cyrus</forename><surname>Shahabi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2017 IEEE Third International Conference on Multimedia Big Data (BigMM)</title>
				<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="330" to="337" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b86">
	<monogr>
		<author>
			<persName><forename type="first">Khiem</forename><surname>Vinh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tran</forename></persName>
		</author>
		<author>
			<persName><forename type="first">Hao</forename><surname>Phu Phan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kiet</forename><surname>Van Nguyen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ngan</forename><surname>Luu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">-Thuy</forename><surname>Nguyen</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2009.02935</idno>
		<title level="m">UIT-HSE at WNUT-2020 Task 2: Exploiting CT-BERT for Identifying COVID-19 Information on the Twitter Social Network</title>
				<imprint>
			<date type="published" when="2020">2020. 2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b87">
	<monogr>
		<author>
			<persName><forename type="first">Iulia</forename><surname>Turc</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ming-Wei</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kenton</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kristina</forename><surname>Toutanova</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1908.08962v2</idno>
		<title level="m">Well-Read Students Learn Better: On the Importance of Pre-training Compact Models</title>
				<imprint>
			<date type="published" when="2019">2019. 2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b88">
	<analytic>
		<title level="a" type="main">Attention is all you need</title>
		<author>
			<persName><forename type="first">Ashish</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Noam</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Niki</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jakob</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Llion</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aidan</forename><forename type="middle">N</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Łukasz</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Illia</forename><surname>Polosukhin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
				<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="5998" to="6008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b89">
	<analytic>
		<title level="a" type="main">Rumor detection in social networks via deep contextual modeling</title>
		<author>
			<persName><forename type="first">Ben</forename><surname>Amir Pouran</surname></persName>
		</author>
		<author>
			<persName><forename type="first">My</forename><forename type="middle">T</forename><surname>Veyseh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Thien</forename><surname>Thai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dejing</forename><surname>Huu Nguyen</surname></persName>
		</author>
		<author>
			<persName><surname>Dou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2019 IEEE/ACM International Conference on Advances in Social Networks Analysis and Mining</title>
				<meeting>the 2019 IEEE/ACM International Conference on Advances in Social Networks Analysis and Mining</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="113" to="120" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b90">
	<analytic>
		<title level="a" type="main">Phonemer at</title>
		<author>
			<persName><forename type="first">Anshul</forename><surname>Wadhawan</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2010.00294</idno>
	</analytic>
	<monogr>
		<title level="m">WNUT-2020 Task 2: Sequence Classification Using COVID Twitter BERT and Bagging Ensemble Technique based on Plurality Voting</title>
				<imprint>
			<date type="published" when="2020">2020. 2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b91">
	<analytic>
		<title level="a" type="main">Fake News Detection via Knowledge-driven Multimodal Graph Convolutional Networks</title>
		<author>
			<persName><forename type="first">Youze</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shengsheng</forename><surname>Qian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jun</forename><surname>Hu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2020 International Conference on Multimedia Retrieval</title>
				<meeting>the 2020 International Conference on Multimedia Retrieval</meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="540" to="547" />
		</imprint>
	</monogr>
	<note>Quan Fang, and Changsheng Xu</note>
</biblStruct>

<biblStruct xml:id="b92">
	<monogr>
		<title level="m" type="main">HuggingFace&apos;s Transformers: State-of-the-art Natural Language Processing</title>
		<author>
			<persName><forename type="first">Thomas</forename><surname>Wolf</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lysandre</forename><surname>Debut</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Victor</forename><surname>Sanh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Julien</forename><surname>Chaumond</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Clement</forename><surname>Delangue</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Anthony</forename><surname>Moi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pierric</forename><surname>Cistac</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tim</forename><surname>Rault</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R'emi</forename><surname>Louf</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Morgan</forename><surname>Funtowicz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jamie</forename><surname>Brew</surname></persName>
		</author>
		<idno>ArXiv abs/1910.03771</idno>
		<imprint>
			<date type="published" when="2019">2019. 2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b93">
	<monogr>
		<author>
			<persName><forename type="first">Lianwei</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuan</forename><surname>Rao</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2004.10009</idno>
		<title level="m">Adaptive Interaction Fusion Networks for Fake News Detection</title>
				<imprint>
			<date type="published" when="2020">2020. 2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b94">
	<monogr>
		<title level="m" type="main">Different absorption from the same sharing: Sifted multi-task learning for fake news detection</title>
		<author>
			<persName><forename type="first">Lianwei</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuan</forename><surname>Rao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Haolin</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ambreen</forename><surname>Nazir</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ling</forename><surname>Sun</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1909.01720</idno>
		<imprint>
			<date type="published" when="2019">2019. 2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b95">
	<monogr>
		<title level="m" type="main">DTCA: Decision tree-based co-attention networks for explainable claim verification</title>
		<author>
			<persName><forename type="first">Lianwei</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuan</forename><surname>Rao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yongqiang</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hao</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ambreen</forename><surname>Nazir</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2004.13455</idno>
		<imprint>
			<date type="published" when="2020">2020. 2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b96">
	<analytic>
		<title level="a" type="main">Rumor Detection Based On Propagation Graph Neural Network With Attention Mechanism</title>
		<author>
			<persName><forename type="first">Zhiyuan</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dechang</forename><surname>Pi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Junfu</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Meng</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jianjun</forename><surname>Cao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Expert Systems with Applications</title>
		<imprint>
			<biblScope unit="page">113595</biblScope>
			<date type="published" when="2020">2020. 2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b97">
	<monogr>
		<author>
			<persName><forename type="first">Patrick</forename><surname>Xia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shijie</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Benjamin</forename><surname>Van Durme</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2010.00854</idno>
		<title level="m">Which* BERT? A Survey Organizing Contextualized Encoders</title>
				<imprint>
			<date type="published" when="2020">2020. 2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b98">
	<analytic>
		<title level="a" type="main">Hierarchical attention networks for document classification</title>
		<author>
			<persName><forename type="first">Zichao</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Diyi</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chris</forename><surname>Dyer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaodong</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alex</forename><surname>Smola</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Eduard</forename><surname>Hovy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2016 conference of the North American chapter of the association for computational linguistics: human language technologies</title>
				<meeting>the 2016 conference of the North American chapter of the association for computational linguistics: human language technologies</meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="1480" to="1489" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b99">
	<monogr>
		<title level="m" type="main">Fake news: A survey of research, detection methods, and opportunities</title>
		<author>
			<persName><forename type="first">Xinyi</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Reza</forename><surname>Zafarani</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1812.00315</idno>
		<imprint>
			<date type="published" when="2018">2018. 2018</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b100">
	<analytic>
		<title level="a" type="main">Fake news: Fundamental theories, detection strategies and challenges</title>
		<author>
			<persName><forename type="first">Xinyi</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Reza</forename><surname>Zafarani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kai</forename><surname>Shu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Huan</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Twelfth ACM International Conference on Web Search and Data Mining</title>
				<meeting>the Twelfth ACM International Conference on Web Search and Data Mining</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="836" to="837" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b101">
	<analytic>
		<title level="a" type="main">Analysing how people orient to and spread rumours in social media by looking at conversational threads</title>
		<author>
			<persName><forename type="first">Arkaitz</forename><surname>Zubiaga</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Maria</forename><surname>Liakata</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rob</forename><surname>Procter</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Geraldine</forename><forename type="middle">Wong</forename></persName>
		</author>
		<author>
			<persName><forename type="first">Sak</forename><surname>Hoi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Peter</forename><surname>Tolmie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">PloS one</title>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="page">e0150989</biblScope>
			<date type="published" when="2016">2016. 2016</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
