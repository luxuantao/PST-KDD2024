<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">dCat: Dynamic Cache Management for Efficient, Performance-sensitive Infrastructure-as-a-Service</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Cong</forename><surname>Xu</surname></persName>
							<email>xucong@us.ibm.com</email>
						</author>
						<author>
							<persName><forename type="first">Karthick</forename><surname>Rajamani</surname></persName>
							<email>karthick@us.ibm.com</email>
						</author>
						<author>
							<persName><forename type="first">Alexandre</forename><surname>Ferreira</surname></persName>
							<email>apferrei@us.ibm.com</email>
						</author>
						<author>
							<persName><forename type="first">Wesley</forename><surname>Felter</surname></persName>
							<email>wesley@felter.org</email>
						</author>
						<author>
							<persName><forename type="first">Juan</forename><surname>Rubio</surname></persName>
							<email>rubioj@us.ibm.com</email>
						</author>
						<author>
							<persName><forename type="first">Yang</forename><forename type="middle">2018</forename><surname>Li</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Juan</forename><surname>Ru- Bio</surname></persName>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="institution">IBM Research Austin</orgName>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff1">
								<orgName type="institution">IBM Research Austin</orgName>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff2">
								<orgName type="institution">IBM Research Austin</orgName>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff3">
								<orgName type="institution">IBM Research Austin</orgName>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff4">
								<orgName type="institution">IBM Research Austin</orgName>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff5">
								<orgName type="institution">Carnegie Mellon University</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">dCat: Dynamic Cache Management for Efficient, Performance-sensitive Infrastructure-as-a-Service</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="DOI">10.1145/3190508.3190555</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-01-03T09:31+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>In the modern multi-tenant cloud, resource sharing increases utilization but causes performance interference between tenants. More generally, performance isolation is also relevant in any multi-workload scenario involving shared resources. Last level cache (LLC) on processors is shared by all CPU cores in x86, thus the cloud tenants inevitably suffer from the cache flush by their noisy neighbors running on the same socket. Intel Cache Allocation Technology (CAT) provides a mechanism to assign cache ways to cores to enable cache isolation, but its static configuration can result in underutilized cache when a workload cannot benefit from its allocated cache capacity, and/or lead to sub-optimal performance for workloads that do not have enough assigned capacity to fit their working set.</p><p>In this work, we propose a new dynamic cache management technology (dCat) to provide strong cache isolation with better performance. For each workload, we target a consistent, minimum performance bound irrespective of others on the socket and dependent only on its rightful share of the LLC capacity. In addition, when there is spare capacity on the socket, or when some workloads are not obtaining beneficial performance from their cache allocation, dCat dynamically reallocates cache space to cache-intensive workloads. We have implemented dCat in Linux on top of CAT to dynamically adjust cache mappings. dCat requires no modifications to applications so that it can be applied to all cloud workloads. Based on our evaluation, we see an average of 25% improvement over shared cache and 15.7% over static CAT for selected, memory intensive, SPEC CPU2006 workloads. For typical cloud workloads, with Redis we see 57.6% improvement (over shared LLC) and 26.6% improvement (over static partition) and with ElasticSearch we see 11.9% improvement over both.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">INTRODUCTION</head><p>Public clouds have grown significantly in the last few years and the Infrastructure-as-a-Service (IaaS) segment's adoption is expected to grow even more rapidly in the next few years <ref type="bibr" target="#b3">[6]</ref>. The adoption of virtual servers enabled with virtualization technologies has allowed companies such as Amazon and Google with vast internal needs for computing to offer that same physical infrastructure they use to the public as IaaS. Sharing of resources among different workloads and tenants allows better utilization of the physical infrastructure, lowering the cost of computing for all. However, multiple workloads on same server can lead to performance interference due to competition for the shared resources. Performance isolation is important for application availability (e.g. protect against Denial-of-Service attacks) as well as for responsiveness and predictability (e.g. for capacity planning). Researchers have pointed out the importance of resource isolation in order to have performance isolation <ref type="bibr" target="#b9">[12,</ref><ref type="bibr" target="#b10">13]</ref> for both responsiveness of latency-critical applications as well as to reduce the dominating tail-latency impact for applications built with microservices model. Public clouds today offer dedicated instances (virtual server that has a physical host all to itself) as a means to get consistent performance. However, this reduces overall utilization of infrastructure and drives up the cost of computing. The key to address this dichotomy is leveraging the ability to provide performance isolation within a server when needed while not giving up on the ability to share/re-distribute physical resources among multiple workloads.</p><p>Providing dedicated resources to a workload within a server not shared with other workloads on that same server provides a good start for balancing isolation and efficiency needs. That is possible for certain resources such as processor cores, memory capacity, and I/O bandwidth where the hardware, operating systems and hypervisors have mature mechanisms to partition these resources. However, other shared resources such as CPU cache are currently not controlled intelligently. These resources are fairly precious relative to the cores and inherently designed to be shared by all workloads running on a processor socket to maximize their utilization.</p><p>While today IaaS's focus is on just availability of contracted resources in the form of a virtual server, we envision a next-generation, performance-sensitive IaaS that would ensure consistent performance for any workload using those contracted resources with resource isolation mechanisms and drive up utilization of resources by also enabling resource sharing without impacting performance consistency. To do this, we cannot just statically partition the resources among different workloads (that would lead to fragmentation and under-utilization) but need to do so dynamically in a performancesensitive manner.</p><p>In this work, we examine how we can do this effectively for the processor's shared last-level cache (LLC). Intel x86 processors have an LLC that is set-associative, inclusive and shared by all the cores on the processor socket. Recognizing the need for cache isolation, Intel introduced a Cache Allocation Technology (CAT) <ref type="bibr" target="#b17">[20]</ref> that allows runtime partitioning of the L3 cache among different cores. CAT works by restricting which ways a core can evict cache lines from, in practice limiting the size and associativity of cache each core is able to use. <ref type="foot" target="#foot_0">1</ref> However, we observe that CAT causes suboptimal performance for some memory-intensive applications when used naively. CAT is a low-level, hardware mechanism (and drivers) which enables cache partitioning. It needs to be supplemented with a controller managing it to dynamically change the size of cache partitions based on runtime behavior. <ref type="foot" target="#foot_1">2</ref> Since public clouds generally treat workloads as black boxes, they cannot estimate the cache needs of workloads in advance. Many cloud users are also not sophisticated enough to know how much cache capacity their workloads need either. If a workload is assigned a cache allocation smaller than its working set, it will suffer expensive cache misses. Too-large cache allocations leave the cache poorly utilized, preventing other workloads from getting as much cache capacity as they could benefit from.</p><p>We propose a dynamic cache allocation technology (dCat) built on top of CAT. dCat first targets a guaranteed minimum performance level for a workload, irrespective of "noisy neighbor" workloads running on the same socket -this performance is what can be attained by using CAT for cache based isolation. This guaranteed minimum level termed baseline offers a consistent baseline performance level that can be leveraged for planning purposes. In addition, it enables higher performance for workloads that can use more cache capacity when others are not getting much benefit from their assigned LLC. dCat optimizes the cache assignment at running time and always allocates cache to those workloads that really need it, while still guarantees that all workloads at least get the same performance that they would have under static cache partitioning. We implemented dCat as a Linux daemon which is transparent to the workloads (and VMs, containers) running on the x86 platform. It can be applied to all existing cloud environments without any modification to both the OS kernel and the upper-level applications.</p><p>To summarize, our contributions in this paper are:</p><p>? We establish the notion of a performance-sensitive IaaS framework where a consistent baseline performance level can be guaranteed based on contracted resource allocation for a workload while allowing dynamic resource adjustments to improve physical resource utilization and cloud efficiency.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">CHALLENGES</head><p>The LLC in Intel x86 processors is inclusive<ref type="foot" target="#foot_2">3</ref> , set-associative and shared among all cores in a processor socket. We look at some of the main challenges in managing this cache for simultaneously providing multi-tenant performance isolation and increasing utilization in this section.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">The Impact of Cache Interference</head><p>We use two simple, well-understood, internally developed memory access benchmarks for this exercise: MLR and MLOAD. MLR is a stream of random read accesses to an array, while MLOAD is a stream of sequential read accesses to an array. For each of the workloads, the size of its array determines the size of its working set.</p><p>Figure <ref type="figure" target="#fig_0">1</ref> shows the performance of MLR (latency, lower is better) with 6MB working set and 16MB working set. We run four scenarios for each working set: without noisy neighbors, with noisy neighbor workloads which are two instances of MLOAD (60MB working set), and both with CAT used to assign 13.5MB of dedicated cache (6 LLC cache-ways out of a Xeon-E5 processor with a 20-cache-way 45MB LLC) to MLR and with fully shared cache (i.e. without CAT). The working sets are sized so that performance is largely dependent on how well the workload can utilize the LLC. For the MLR-6MB, we can see that its performance will be significantly hurt by the noisy neighbors if we do not provide any dedicated cache (compare Shared cache w/noisy with Shared cache w/o noisy); however, if we provide the 13.5MB dedicated cache, its performance is protected and isolated from the neighbors (compare CAT w/noisy with Shared cache w/o noisy). This demonstrates that CAT can provide performance isolation if the dedicated cache for a workload is large enough to hold its working set. However, if the dedicated cache is not large enough, CAT will fail to provide performance isolation. For example, though 13.5MB dedicated LLC is provided, the MLR-16MB is severely interfered by its noisy neighbors (compare CAT w/noisy with Shared cache w/o noisy), because in this case, the MLR-16MB has to frequently load data from memory instead of only from LLC. Therefore, we can conclude that whether CAT can provide performance isolation between workloads is highly dependent on the size of the dedicated cache it statically allocates to each workload run by different tenants. However, determining the size for a dedicated cache for each workload is not a trivial task. In order to determine the appropriate size, we need to first predict the working set of a workload. However, in the majority of cases, cloud providers have very limited beforehand knowledge about tenants' workloads (or their working set size and how they change over time). In addition, even if we can accurately predict the working set of a workload, it is also hard to determine an appropriate cache size to hold its working set because of cache conflict misses <ref type="bibr" target="#b7">[10]</ref>. Conflict miss is a specific cache miss that may happen when multiple cache lines compete for the associative ways in a single set of the cache. This conflict miss exists even the huge page (2MB on Linux x86) is enabled for workloads. This is exemplified in Figure <ref type="figure" target="#fig_1">2</ref>, where we show average access latencies of MLR on two Broadwell systems (8-core, 12-way 12MB LLC Xeon-D and 18-core 20-way 45MB LLC Xeon-E5). CAT is used to restrict the cache allocation for the blue bars (4KB regular page) and orange bars (2MB huge page). The yellow bars represent performance for full cache capacity (4KB regular page). CAT reduces capacity by limiting the number of ways that a core can access. Enough capacity is provided even under the reduced cache size to not have capacity misses (2MB workload working set with 2-way 2MB dedicated LLC for Xeon-D, and 4.5MB workload working set with 2-way 4.5MB dedicated LLC for Xeon-E5). However, as can be seen, the performance is still significantly lower with the reduced cache size. This is because the reduced capacity also comes with reduced associativity which results in higher conflict misses leading to higher average access latency. With the virtual to physical address mappings, a contiguous virtual address space larger than a page size (4KB) does not necessarily occupy a contiguous physical address space. Mapping to cache lines is determined by the physical address and whether a memory location finds place within restricted cache ways depends on both the mapping (potentially conflicting for cache lines) as well as total space in those ways, and not just the latter. When associativity is reduced (by limiting cache ways), the impact of conflict from non-contiguous address mapping is seen more. Huge page can guarantee more contiguous physical address after translation, this is why the 2MB working set workload using huge page achieves similar performance to the full cache on Xeon-D. However, the workload having larger working set than one huge page (e.g. 4.5MB working set on Xeon-E5) still suffer the conflict miss. Figure <ref type="figure" target="#fig_3">3</ref> shows the corresponding histogram of the number of cache lines that are mapped to each cache set. We observe that, with 4KB page, even when 2 cache ways of LLC are allocated to equal the working set size, conflict misses can still occur as in the case of Xeon-D around 32.5% of sets have 3 or more cache lines mapped to (about 29% of sets for Xeon-E5); and therefore, these cache lines will conflict/compete for the 2 allocated cache ways. In Xeon-D hugepage case, 2MB working set only needs 1 huge page to fit so that it just can be perfectly cached by the allocated LLC leading to zero conflict miss. Unfortunately, in Xeon-E5 hugepage case, 4.5MB working set needs 3 pages but the cache can only fully hold 2. After address translation, we still see about 11.2% of sets have 3 cache lines mapped to thus resulting in significant conflict misses. These conflict misses make it harder to predict the cache needs for a workload even if we know its working set size.  CAT provides the means to create cache usage isolation between cores (and so between different workloads running on different cores), but by doing so it limits cache capacity as well as associativity available to a specific workload to a static value that is hard to appropriately determine beforehand. Consequently, an adaptive approach dCat is proposed to manage the cache in this work. dCat operates based on runtime behavior and allocates additional cache to those workloads that could benefit from them, while still guaranteeing that all workloads get at least their baseline performance level.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Problems of Alternative Solutions</head><p>We have demonstrated above the static LLC allocation provided by Intel CAT leaves significant system performance on the table. Next, we examine several alternative solutions and argue their shortcomings when used in real systems.</p><p>OS-level page coloring Lin's study <ref type="bibr" target="#b26">[29]</ref> aims to offer cache partitioning by using OS-level page coloring which directly assigns LLC among threads in the real system. All application threads are classified into one of four classes based on their performance degradation using only 1MB LLC compared to the baseline configuration using 4MB. Although this color-based classification scheme may be applicable to workload creation, it is unable to be easily used for dynamic, on-the-fly classification of workloads behavior. In particular, measuring the performance degradation requires running two copies of a program on two cores simultaneously, each with its own dedicated LLC. Due to its limitation, it has not been accepted by current Linux kernel.</p><p>Fine-grained cache partitioning on chip Some researchers have noticed the performance interference caused by shared LLC and tried to provide a series of chip-level cache allocation mechanisms according to the workloads behaviors <ref type="bibr" target="#b19">[22,</ref><ref type="bibr" target="#b20">23,</ref><ref type="bibr" target="#b24">27,</ref><ref type="bibr" target="#b33">36,</ref><ref type="bibr" target="#b34">37,</ref><ref type="bibr" target="#b38">41,</ref><ref type="bibr" target="#b39">42]</ref>. These approaches are explored in simulation as opposed to our proposal which is implemented in real systems. All these prior proposals require additional hardware support for the cache partitioning capability. On the contrary, our proposal dCat is a software mechanism leveraging the currently available hardware knobs (performance counters and Intel CAT) to perform performance driven cache partitioning with performance isolation. It does not require any hardware modification for the current hardware platform, making it easier to be deployed. Besides, these prior proposals can not be directly applied to our goals of maintaining performance isolation first and then maximizing system throughput. The key differences in our work compared with prior proposals start with our goal of performance isolation in multi-tenant/-workload situations. A baseline performance is guaranteed for each workload. Cache utilization and resulting performance improvement are structured on top of this guarantee. Prior works mostly focus on different dynamic cache partitioning approaches in multiple workloads context for improving overall system miss-rate/performance (not performance isolation).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">DESIGN</head><p>Our premise is that a dynamic cache allocation mechanism for the shared LLC can improve the performance of memory intensive workloads as well as guarantee the performance isolation among cloud workloads. dCat achieves this by dynamically adjusting the cache allocation to different workloads according to their behaviors. If some workloads do not use LLC or are unable to benefit from current LLC allocation, dCat reduces their cache size and reassign it to those who take advantage of a larger cache size to get better performance. The cache is allocated in terms of number of ways, which is the minimum partition unit of Intel Xeon processors. In dCat, one of the objectives is never to impact the performance of the workloads (when compared to the expected performance while using the reserved cache size). To realize this expectation, dCat performs five steps: Get Baseline, Collect Statistics, Detect Phase Change, Categorize Workloads and Allocate Cache which are shown in Figure <ref type="figure" target="#fig_4">4</ref>.</p><p>As used in the present specification, the term "workload" refers to an application run by a tenant. It can also apply to a virtual machine (VM) or a container owned by a tenant in public or private cloud. If the "workload" uses more than one CPU core, we conduct the following five steps based on the overall performance of all involved cores. This means if one of the applications run by a tenant in a VM is cache-sensitive, this VM is considered to benefit from increased cache.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Get Baseline</head><p>The first step in dCat is to determine the baseline performance for each workload. Baseline performance is defined as the performance of the workload when using the predefined cache partition (e.g. the cache partition paid by tenants). Since one workload can behave very differently in terms of cache needs over time, phase detection is essential. Baseline performance is only defined for a specific workload phase, so it has to be determined again at every phase change. In general terms, dCat tries to determine if a workload would benefit, or be indifferent to suffering if more or less cache is made available to it. The workload classification, that is specific for each phase and current cache size, is done by either inferring behavior over measured metrics or by comparing performance when different cache sizes are used.</p><p>Baseline performance for dCat is identified as the measured IPC for this workload phase when running with the statically predefined cache size (number of ways reserved). IPC is generally accepted as a good indicator of relative workload performance <ref type="bibr" target="#b23">[26]</ref> and it is adversely affected by increase of cache misses.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Collect Statistics</head><p>The cache workload behavior has to be determined so dCat can allocate cache effectively. Unfortunately, in the cloud environment, each workload runs as a black box preventing dCat to have any preexisting information about it. To mitigate this issue, we periodically collect the following information of each workload which is used by the phase change detection, workload categorization, and cache allocation steps: assigned to a workload. ? IPC: The IPC of a workload which is the ratio of the retired instructions to the unhalted cycles of all core assigned to a workload.</p><p>We also define some corresponding thresholds to help us determine the workload characteristic in memory access: L1 cache reference threshold (l1_re f _thr), LLC reference threshold (llc_re f _thr), LLC cache miss threshold (llc_miss_rate_thr), etc. All these thresholds are configurable depending on the needs of users.</p><p>dCat collects metrics per core. For those workloads using multiple CPU cores, dCat measures the performance of all used cores and calculate the average to evaluate the workloads. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Detect Phase Change</head><p>Since IPC is used to determine workload performance and IPC also varies per workload phase, any workload phase change invalidates the performance comparison by changing the baseline IPC. dCat uses a simplified phase detection algorithm based on measuring the number of memory accesses (i.e. LOAD and STORE) per instruction (estimated by l1_re f /ret_ins) and assuming that sizable changes (10% used as the threshold in our prototype) on this value represent a phase change. We observe that the value of memory accesses per instruction is only related to the workload itself (e.g.internal logic) and is independent of the memory subsystem configuration (e.g. cache allocation). This is verified by a simple experiment in which we run the MLR and MLOAD with different working set size and varied cache-ways from 1 to 8. The results can be found in Figure <ref type="figure" target="#fig_5">5</ref>. This is a very simple phase change detection but it meets our need of being reasonably independent of IPC and works well in our tests. There are other workload phase change detection methods <ref type="bibr" target="#b5">[8,</ref><ref type="bibr" target="#b6">9,</ref><ref type="bibr" target="#b35">38]</ref> that can be used and they are pluggable into our work.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Categorize Workloads</head><p>With this baseline performance level determined and the collected statistics, the workloads can be categorized into the following classes based on cache utilization. That is, it can be determined 1) which cache allocations are being underutilized or over utilized, 2) which workloads could benefit from an increased amount of cache, and/or 3) which workloads would not be harmed by a reduced cache size.</p><p>? Reclaim: It is a state when a phase change is detected and the workload has to return to the baseline state.  The relationship among these categories and their transition are displayed in Figure <ref type="figure" target="#fig_6">6</ref>. The start state for any workload is Keeper. If this workload is idle or has low utilization of LLC (i.e.llc_re f ? llc_re f _thr), it is assumed that it has more cache than it needs and label it to Donor and only maintains the minimum cache size (one way for Intel x86 4 ). If it has high utilization of LLC but the LLC miss rate is small, it is still a Donor but the cache size can only be reduced by one way in each interval until the LLC miss rate becomes non-trivial (hence labeled as Keeper). If this workload has significant LLC references and high LLC cache misses (i.e.llc_re f &gt; llc_re f _thr and llc_miss_rate &gt; llc_miss_rate_thr), it may benefit from having a larger cache and is marked to Unknown. An Unknown workload that shows performance gains when getting a larger cache size is categorized as Receiver otherwise it is still marked as Unknown. If all the available cache size is used or the cache size passes a threshold (e.g., 3 times the predefined cache size) and the state is still Unknown the workload is marked Streaming. Streaming never reuses data in cache, so it is a special Donor and only need to maintain the minimum cache size. In the Receiver state, if the workload's LLC miss rate after increasing the cache size is small or there is no further performance improvement (i.e.llc_miss_rate &lt; llc_miss_rate_thr or ipc_imp &lt; ipc_imp_thr), dCat stops increasing the cache size and set the state to Keeper. At any time, Reclaim is applied immediately once there is a phase change detected. Since we guarantee the baseline performance for all workloads, Reclaim has the highest priority. If there is no available cache in the resource pool to recover the baseline cache size of Reclaim workloads, dCat has to reclaims cache from those whose current cache size is larger than their baseline size.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.5">Allocate Cache Using CAT</head><p>After categorizing the workloads, the available cache needs to be distributed. In order to describe our allocation method clearly, let us use a concrete example to illustrate. Assume one tenant rents a VM (e.g. AWS M4.large instance) from a cloud provider to run some workloads. The cache allocation over time of this VM is shown in Figure <ref type="figure" target="#fig_8">7</ref>(a). After getting the initial computing resource, there is no workload running at the very beginning leaving LLC underutilized. This VM thus is categorized as Donor and only has the minimal cache size allocated (one way). The rest of the cache are assigned to the resource pool. At time t1, the user starts running a memoryintensive workload (e.g. MLR) resulting in a phase change, and dCat immediately assigns the reserved cache size. In this particular case, the reserved cache can not fit the working set thus the LLC cache miss rate is above the threshold. If there is available cache in the resource pool, the workload cache size is increased (one way) for each round until t2 when the LLC miss rate is smaller than the threshold and the IPC is not improved any more. At t3, the workload stops running, so it becomes a Donor again due to the low number of LLC accesses. While, if this was a streaming workload (as in Figure <ref type="figure" target="#fig_8">7</ref>(b)), dCat stops increasing the cache size at t2 when the cache size reaches the Streaming threshold but without getting performance improvement (IPC does not change). It becomes a Donor after t2.</p><p>We notice that because dCat adds more cache to workloads lacking cache in each round, it takes some time to get the preferred state in which workloads suffers minor cache misses. This process is inevitable for the first running of each workload because dCat has no idea about the workloads' needs in advance. However, since it starts with the reserved LLC (when cache-sensitive workloads start running) it does not reduce performance from what is the expected baseline. dCat maintains a table, performance table, that contains normalized IPC (normalized to baseline) collected per workload phase and cache size. This table is used to determine if a phase change has occurred, detect a sizable change on the number of memory accesses per instruction, and also to optimize the workload classification and cache allocation. This table is not necessarily complete, having all the entries filled, because only the reachable entries and/or the ones that impacts decisions are needed. For example, Table <ref type="table" target="#tab_3">1</ref> shows partial table of a workload. It indicates that assigning 6 ways is enough to get the highest performance. When the same phase of this workload is seen again dCat will reuse the collected performance data for this phase skipping the process of discovery. Take the case shown in Figure <ref type="figure" target="#fig_8">7</ref>(a) for instance, at t4 the same phase of workload is seen again, dCat directly gives it the best number of ways instead of going to the baseline first then adding one way per round. This can reduce the time of getting enough cache to fit the working set thus further improving the overall performance. The contents of one table (mappings from number of ways to normalized IPC) are only meaningful to a specific phase, so they have to be updated once the phase changes.</p><p>This table is also essential in determining how to distribute the cache. If multiple workloads benefit from larger caches, the decision of how much and to whom to give additional cache can lead to different results. dCat supports two different allocation policies depending on which objective to achieve: 1) maximize the total performance or 2) maximize the number of workloads that achieve better performance. Maximizing total performance can lead to a single workload holding all the additional cache available (i.e. maximize the overall performance), in the other case every workload benefits (i.e.maximize the fairness) even though the best total performance may not be achieved.</p><p>To maximize the fairness among workloads, the table contents are ignored during the cache allocation. dCat evenly distributes the available cache size regardless of the magnitude of IPC improvement. For example, 2 cache-sensitive workloads run in a host while 4 cache ways are still available in the resource pool after both workloads get the baseline cache size assigned. dCat under max fairness mode will assign additional 2 cache ways to each regardless of their performance improvement contributed by the larger cache. Nevertheless, dCat differentiates the allocating process for Unknown workloads and Receiver workloads. Our goal is to figure out if the Unknown workloads are streaming or not sooner. So dCat gives Unknown workloads higher priority than Receiver workloads during cache allocation. Once Unknown workloads are recognized to Streaming, their cache-ways are transferred to those Receiver workloads or put into the resource pool.</p><p>To maximize the overall performance, dCat refers the performance table if table entries are valid. dCat traverses the performance tables of all workloads to find the combination whose sum of the normalized IPC of all workloads are the largest. That is for n workloads and m ways, we need find the solution from the performance table having Max(? n i=1 norm_IPC i ) while ? n i=1 ways i ? m. This usually happens when dCat needs to re-allocate the cache size. For example, there are two workloads (e.g. A and B) running and both can benefit from a larger cache. Their baseline cache size is 2 ways. The CPU socket has 10 cache ways totally. At the beginning, the performance table is empty, dCat uses even distribution to assign ways, and the normalized IPC for each assignment is stored. When there are no ways available in the resource pool, each workload gets 5 ways. Their performance table contents (number of ways : normalized IPC) are (1 : N/A), (2 : 1), (3 : 1.05), (4: 1.08), (5 : 1.12) for A and (1 : N/A), (2 : 1), (3 : 1.1), (4 : 1.2), (5 : 1.25) for B. At a certain time, another workload C starts running and reclaims 2 ways. Since there is no way available in the pool, dCat has to collect 2 ways from A and B, then assign them to workload C to assure its baseline performance considering Reclaim has the highest priority. Since C's performance table is empty initially, it retains the baseline size and generates normalized IPC 1. To get the largest overall performance for A, B and C, we just need to find a combination generating the largest sum of normalized IPC for A and B. After traversing A's and B's performance table, we realize giving A 3 ways and B 5 ways is the best choice, because this combination generates the largest total normalized IPC (i.e. 2.3 for A plus B, and 3.3 for A, B and C).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">IMPLEMENTATION</head><p>We implemented a prototype of dCat for Intel Xeon (Xeon-D and Xeon-E5 v4) platform. dCat is a C program and runs as a daemon in the host OS (Linux 4.4 is used for our prototype). dCat daemon periodically performs the five steps demonstrated by Figure <ref type="figure" target="#fig_4">4</ref>. It is transparent to the VMs, guest OS and all user-level applications.</p><p>Workload Profiling To determine the phase change and categorize the workloads, we have to record and analyze a workload's behavioral characteristics. This is done by profiling CPUs where the workload is running, that is, collecting CPU metrics like LLC cache miss and L1 cache reference. We use a Linux kernel module named msr to read a series of performance events from processor counters <ref type="foot" target="#foot_4">5</ref> . Some important performance events used by dCat are listed in Table <ref type="table" target="#tab_4">2</ref>. The IPC is the ratio of retired instructions to unhalted cycles. dCat determines a workload phase by memory accesses (equal to LOAD + STORE) per instruction. However, msr is unable to count the issued LOAD and STORE instructions. Since x86 CPU generally checks the fastest, L1 cache first before consuming a data, we use L1 references value to estimate the memory accesses number. One workload phase, accordingly, is defined by l1_re f /ret_ins. Cache Allocation dCat leverages the Intel Platform Quality of Service (pqos) library <ref type="bibr" target="#b18">[21]</ref> (which provides the interface to CAT) to dynamically set the mapping between the cache size and CPU cores. This library supports up to 16 Class of Service (COS). Each COS represents a subset of LLC. dCat dynamically defines the COSs according to the cache requirement of each workload and applies them to the corresponding cores where workloads are running. Since existing Intel commodity processors only allow the way level partitioning, dCat assigns cache in the unit of cache-way. To guarantee the cache isolation, we do not allow the COS overlap among cores, that is the cores running one workload (or a set of workloads owned by the same tenant) only have one COS.</p><p>We periodically collect the CPU/cache information and change the cache allocation for different workloads owned by different tenants. The period time is a configurable parameter (e.g. 1s). It can be changed by the administrators according to their needs. Note that if the interval is too large, dCat can not adjust the cache allocation on demand. If the value is too small resulting in very frequent change of cache assignment, it would introduce non-trivial overhead. Besides, we use IPC as the feedback of cache adjustment, the benefit of extra cache may not be reflected within the very short interval.</p><p>Since LLC can only be assigned to CPU cores, dCat needs the information that where the workloads (VMs or containers) are running. In bare-metal, it can be collected from OS which maintains the footprints of all applications. In public or private cloud, containers and VMs are widely used to consolidate the physical servers. We can get this information from the hypervisor which schedules vCPUs on physical cores. To be simplified, we assume there is no CPU over provisioning in our prototype, that is no CPU sharing among vCPUs. So, each VM/container has dedicated CPU resource. This section presents our evaluation of dCat using both microbenchmarks and some real world applications. Evaluation Setup Our experimental system consists of a server with Intel Xeon E5-2697 v4 processor with 18 cores at 2.3 GHz and a 20-way 45 MB LLC. The capacity of each cache way is 2.25 MB. The host server runs KVM as the hypervisor and Linux 4.4 as the OS for both the guest VM and the host. To avoid the impact of CPU frequency scaling, all CPUs' frequency governors are set to performance. For Redis, PostgreSQL and Elasticsearch, the server is connected to a client using a 10 Gbps network.</p><p>To simulate the multi-tenant cloud environment, all benchmarks and background workloads are running in VMs representing independent tenants. All VMs in our experiments are assigned 2 vCPUs and 4GB RAM each. We pin each vCPU to separate physical threads in the host, giving every VM dedicated CPU resources. KVM vhost-net is enabled to boost the virtual network performance. All performance results e.g.data access latency, running time, bandwidth etc. shown in this section are obtained from the application side.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Microbenchmark Results</head><p>Parameters Sensitivity dCat have a series of parameters to decide if the corresponding operations occur or not. The most import two are cache miss parameter (llc_miss_rate_thr) and IPC improvement parameter (ipc_imp_thr). The former one decides how sensitive of dCat to cache miss. Once the cache miss rate of a VM is larger than this threshold, dCat assumes one or more cache sensitive workloads are running and try to assign more cache to fit their working set. The later one affects how dCat categorizes workloads (i.e. if Receiver or not). If one's IPC improvement is larger than ipc_imp_thr after getting additional cache, it becomes Receiver and may continue getting more cache if it is available.</p><p>Figure <ref type="figure" target="#fig_9">8</ref> displays the relationship between cache allocation, data access latency and the value of cache miss threshold when a MLR with 8MB working set run in the target VM. VM's baseline cache size is set to 2 ways. The results are collected after running MLR for 30s when the cache-ways assignment does not change (i.e.getting the preferred cache ways). This experiment indicates smaller cache miss parameter predicts the cache requirement more accurately and brings better performance. Nevertheless, small value leads to higher cache demand and the cache pool drains sooner. Cloud providers can set this threshold to their desired values according to their hardware models and customers' needs. We choose 3% in our following experiments.</p><p>Figure <ref type="figure" target="#fig_10">9</ref> presents the cache allocation when the target VM retains the role of Receiver. The workload and experimental setup are same to the cache miss threshold experiment above. We vary the IPC improvement threshold from 3% to 40%. The 9 cache ways for 3% threshold means the VM achieves more than 3% IPC improvement for every additional cache way assigned until having 9 cache ways. Like cache miss threshold, smaller IPC improvement represents higher sensitivity and accuracy, but it also means higher pressure on the cache resource. We set this threshold to 5% in our following experiments. Cache Allocation To verify that dCat can dynamically adjust the number of ways according to the workload's behavior, we repeat  some similar experiments shown in Section 2 that run a workload with a larger working set than the allocated LLC size. In Section 2 we showed that static cache partitioning via CAT works poorly for this scenario due to lacking cache capacity and associativity. Here, 6 VMs each with a baseline of 6.75MB LLC (3 ways) are running in the host machine. We run MLR in a VM (i.e. target VM) and track the cache allocation of this VM under the management of dCat. The working set of MLR is varied from 4MB to 16MB. We run lookbusy [2] which consumes CPU but does not have significant cache access in the rest 5 VMs. This experiment shows that dCat can dynamically assign the correct amount of cache to the VM hosting MLR (shown in Figure <ref type="figure" target="#fig_11">10</ref>). In this case, lookbusy does not benefit from increased cache so that each lookbusy VM only holds 1 cache way (classified as Donor) and all rest cache ways on the socket will be gradually assigned to the MLR VM until its performance in terms of IPC stops increasing.</p><p>The overhead of dCat is measured. We observed that the CPU utilization of dCat is always below 1% when the above experiment is running, which means the CPU overhead introduced by dCat is negligible.</p><p>We also measure the data access latency when running the MLR experiments above. Figure <ref type="figure" target="#fig_0">11</ref> shows the normalized latency over the full cache case in which MLR occupies all cache-ways of the CPU socket which is far enough to hold its whole working set. We can see that dCat's normalized latency is just slightly higher than the full cache scenario and much better than the static partition with CAT.</p><p>What Figure <ref type="figure" target="#fig_11">10</ref> shows is the cache allocation and normalized IPC (to baseline) for workloads' first running. Since no performance history can be referred to, the cache-way is incremented by one each round (except the reclaim). If the period of each round is one  While, when the same workloads stop then run again, this process can be accelerated with the help of performance table introduced in Section 3.5. Figure <ref type="figure" target="#fig_1">12</ref> shows this scenario. We run the MLR-8MB in the VM. The first running ends at 14s. At 19s the MLR-8MB starts running again and directly gets 8 cache-ways. The second running ends at 26s. The results shown in this figure matches our expectation displayed by Figure <ref type="figure" target="#fig_8">7</ref>(a) in Section 3.5 very well.</p><p>Figure <ref type="figure" target="#fig_3">13</ref> shows the cache allocation and normalized IPC of running MLOAD (with 60MB working set) instead of MLR in the target VM. Since 60MB is much larger than the available cache on the socket, MLOAD-60MB generates cyclic data access pattern <ref type="bibr" target="#b32">[35]</ref> so that can not benefit from additional ways. Its IPC does not change despite increasing cache size. When its cache allocation reaches the streaming threshold (e.g. 3 times of the baseline allocation), it is classified as streaming and its allocation is changed to only 1 way. The results closely match our expectation displayed by Figure <ref type="figure" target="#fig_8">7</ref>(b) in Section 3.5. The cache taken away from streaming workloads are put into a free pool and can be assigned to other workloads that would benefit from more cache. If static cache partitioning was used, this workload would always consume its initial allocation (3 cache-ways) even if is not useful, wasting 2 ways. If the cache was shared in an uncontrolled way, the high memory pressure from MLOAD-60MB would slow down other workloads by evicting their data. Multiple memory-intensive workloads (or VMs) may run on the same host machine so we evaluate the efficiency of dCat for two simultaneous memory intensive VMs. First, we run 2 MLR (one with 8MB working set, the other with 12MB) in two VMs while 4 other VMs run lookbusy. Let us recall that in Section 3 we describe two cache allocation policies supported by dCat. One is the even distribution (better fairness), the other one is allocating based on performance table (maximized overall performance). With the fair policy, each VM get the same cache allocation if there are ways available in the free pool. We demonstrate the performance-maximizing policy in Figure <ref type="figure" target="#fig_4">14</ref>. During the first 14 seconds the two policies behave the same because the performance table is empty and there are enough ways available in the free pool. The two VMs get equal cache size each step. When each one gets 8 ways, there is no more   We also try the scenario of running one MLR with 8 MB working set and one MLOAD with 60 MB working set (noisy neighbor). We add one more VM in this experiment than above; the other 5 VMs run lookbusy. The results are shown in Figure <ref type="figure" target="#fig_14">15</ref> and Figure <ref type="figure" target="#fig_15">16</ref>. Figure <ref type="figure" target="#fig_14">15</ref> shows the cache allocation and normalized IPC over time. They both reclaim the baseline cache (3 ways) first then get one additional each round until 6s when each one obtains 7 cache ways. At 7s, there is only one cache-way available (the 5 background VMs use the rest 5 cache-ways). Since Unknown (current categorization of MLOAD-60MB) has higher priority than Receiver (current categorization of MLR-8MB), MLOAD-60MB gets the last cache-way. Then MLOAD-60MB releases 7 cache ways at 8s because it has not IPC improvement when all available cache are consumed (classified as Donor), leading to more cache ways available in the free pool. MLR-8MB stops receiving more cache way after getting one of the released cache ways by MLOAD-60MB. MLR-8MB's final cache allocation is 8 ways which is its preferred state. From Figure <ref type="figure" target="#fig_15">16</ref> we can see that when the MLR's performance is improved by around 175% via dCat, the performance of MLOAD does not get hurt. It means we fully utilize the unused cache ways in the system to improve the performance of those workloads which indeed need the cache.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Benchmark/Application Results</head><p>This section shows the results of some more complex benchmarks and real world applications with dCat.</p><p>First, we use the SPEC2006 <ref type="bibr" target="#b16">[19]</ref> benchmark suite to measure the performance of dCat. In this experiment, 5 VMs with a baseline of 4 cache-ways (9MB total) each are running in the system. We run the selected (single-threaded) SPEC2006 benchmarks in one VM and run 2 MLOAD (60 MB working set) in two separate VMs as the noisy neighbors. The rest two VMs run lookbusy, acting as polite neighbors. The SPEC2006 benchmark outputs the running time of each workload, the results in Figure <ref type="figure" target="#fig_8">17</ref> are the reciprocal value of the running time and are normalized to the results with shared cache. Since the different SPEC2006 benchmarks have different working set size and different sensitivity to the cache <ref type="bibr" target="#b21">[24]</ref>, cache isolation and allocation have different effects for the subtests. Some workloads get better performance with dCat but some do not, but at least achieve the same performance compared with static partition and shared cache. For most SPEC benmarks, static partitioning via CAT achieves the same or better performance compared with the shared cache scenario because it prevents warm data from being evicted by noisy neighbors (MLOAD-60MB in this case). However, when the working set size is larger than the reserved cache partition, the resulting performance is far from optimal. dCat provides better performance than CAT due to the extra cache collected from other VMs which do not benefit from larger cache. For different workloads, the benefit from dCat is different. Take omnetpp and astar for example-the ratio of core working set size (CWSS) to working set size (WSS) of these two workloads is high <ref type="bibr" target="#b13">[16]</ref>. It means they have high reuse of the data in the working set, so the larger cache capacity granted by dCat gives more benefit (up to 83% compared with static cache and up to 129% over the shared cache). The geo-mean performance of 20 chosen benchmarks is improved by 25% over the shared cache and by 15.7% over static partitioning. The assigned ways by dCat for each benchmark (or different data set) are shown in Table <ref type="table">3</ref>. Each benchmark runs hundreds of seconds so the way assignment changes during the run. The values listed in the third column are the ceiling number during the run. The cache allocation changes are caused by either the workload characteristic or the phase changes.</p><p>Next, we show the results of three cloud applications. The experimental setup is similar to the SPEC2006 experiments. We run the tested application in one out of five VMs. Two VMs run MLOAD-60MB as the memory-intensive noisy neighbors. The other two VMs run lookbusy.</p><p>We first evaluate the performance of the Redis <ref type="bibr" target="#b41">[44]</ref> in-memory key/value store. Since Redis keeps all datae in memory, cache is critical to performance. We run the Redis server in the target VM and run the client side benchmark Memtier <ref type="bibr" target="#b1">[3]</ref> in another host within the same LAN. 1 million records (128 bytes each) are inserted to Redis server first as the sample data. Then Memtier generates many concurrent Get requests (using 8 threads and pipeline depth of 30) the performance isolation among tenants, the VMs/containers have to own dedicated CPU cores/threads. ? Intel Xeon processors currently support up to 16 COS, thus the isolated VMs/containers per socket can not exceed 16. Meanwhile, the VM/container number should be also smaller than the number of ways of cache associativity. ? Intel does not have an instruction to clear a specific cacheway. To explicitly clear a cache-way after changing the cache allocation, one user-level cache flush application (e.g. reading an array sequentially) is necessary.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">RELATED WORK</head><p>We have introduced some alternative solutions for LLC partitioning in Section 2. In this section, we discuss other related work in the same research area.</p><p>Performation isolation in memory hiearchy Subramanian et al. <ref type="bibr" target="#b36">[39]</ref> developed an Application Slowdown Model that accurately estimates the slowdown caused by cache and memory interference, also using hardware extensions. <ref type="bibr" target="#b28">[31]</ref> presents a characterization methodology that enables the accurate prediction of the performance degradation that results from contention for shared resources in the memory subsystem. However, they did not propose to mitigate the performance interference caused by the shared LLC which dCat targets. CPI2 <ref type="bibr" target="#b42">[45]</ref> is a technique for semi-black-box performance isolation that compares the IPC of multiple running copies of a workload and looks for outliers to find performance interference, then throttles coscheduled workloads to reduce interference. Similarly, Quasar <ref type="bibr" target="#b11">[14]</ref> is a cluster scheduler that profiles workloads and uses that information to predict allocation sizes and placements for future runs that will maximize performance and minimize interference. Instead of hardware or kernel controls, it relies on workload parameters such as memory and number of threads to control resource usage. Both of these techniques rely on profiling multiple copies of each workload, while dCat is designed to be effective in environments where such knowledge is not available.</p><p>Heracles <ref type="bibr" target="#b27">[30]</ref> is one of the first systems to use Intel CAT and it uses two levels of closed-loop control to isolate latency-critical workloads from interference caused by background best-effort jobs. It can dramatically increase the utilization of a cluster by safely running much more best-effort work without violating service level objectives. Heracles is not a general performance isolation system, though, since it assumes only two workload classes and it requires the latency-critical workload to provide performance metrics. It mainly targets some specific workloads in google's data centers. In a public cloud each server can host more than two workloads and the workloads may be black boxes that provide no performance feedback. dCat does not rely on any feedback from user-level applications, and it supports multiple workloads from different tenants.</p><p>Ginseng <ref type="bibr" target="#b12">[15]</ref> proposes auction-based dynamic cache allocation using Intel CAT for black-box workloads in a public cloud. Instead of optimizing metrics like IPC, throughput, or latency, Ginseng attempts to maximize the profit produced by the cache. Using auctions allows each workload to have independent and arbitrary utility functions, but it also shifts the burden of workload performance analysis to the cloud customer and creates bin-packing problems since cache demand may not be proportional to other resources on the machine.</p><p>Page coloring <ref type="bibr" target="#b25">[28,</ref><ref type="bibr" target="#b40">43]</ref> offers a different cache partitioning approach which is orthogonal to dCat. It allocates different cache sets to different workloads by controlling the physical address assignment for those workloads' pages. Unfortunately, when LLC allocation changes, this approach has to re-assign physical address to some pages and thus induces the overhead of copying those pages between different memory locations. Due to its complexity and non-trivial overhead, Linux does not support this method in kernel so far.</p><p>Performation isolation in I/O Performance interference and isolation in I/O subsystem has been studied extensively. SecondNet <ref type="bibr" target="#b15">[18]</ref> and Oktopus <ref type="bibr" target="#b4">[7]</ref> statically allocate the network bandwidth to guarantee the bandwidth reservation but may lead to the underutilized network (not work-conserving). Elasticswitch <ref type="bibr" target="#b31">[34]</ref>, EyeQ <ref type="bibr" target="#b22">[25]</ref> and SliverLine <ref type="bibr" target="#b30">[33]</ref> all provide the network isolation with the workconserving model. However, all of them above did not consider the CPU interference caused by shared LLC which is targeted by this work.</p><p>In order to provide storage isolation among VMs, mClock [17] modifies hypervisor's I/O scheduler to differentiate VMs' I/O limits and reservations. Argon <ref type="bibr" target="#b37">[40]</ref> uses cache management and time-sliced disk scheduling to guarantee the performance isolation in a shared storage server. Stout <ref type="bibr" target="#b29">[32]</ref> utilizes batch processing to minimize storage request latency, but does not consider the fairness among tenants.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8">CONCLUSION</head><p>The shared last level cache (LLC) in Intel's latest x86 architecture provides an effective design for maximizing the utilization of cache in a processor socket. However, when multiple workloads run on a processor, the interference in the cache can lead to poor/inconsistent performance. This can be detrimental to operations in a public-cloud environment or any where predictable performance is needed. Intel's CAT technology gives the means to provide cache isolation by limiting the number of ways mapped to a particular core at any given instant. Static application of it is enough to provide consistent performance, but it can cause under-utilization of cache and poorer performance per workload than a shared cache when the static partitioning does not quite match the individual needs for the workloads.</p><p>In this work we design, implement and evaluate a dynamic cache allocation scheme, dCat. dCat leverages CAT for setting cache allocations (partitions) to multiple concurrently executing workloads on a socket and combines it with continuous assessment and dynamic re-sizing of allocations based on the real-time needs of the workloads. This allows dCat to get both a consistent minimum bound on performance for each workload irrespective of its neighbors sharing the socket as well as improve workload performance when the neighbors are not making beneficial use of their allocations. We explain the issues and analyze the fundamental workings of dCat with micro-benchmarks and evaluate them with SPEC CPU2006, Redis, PostgreSQL and ElasticSearch as workloads. dCat gets consistently the best performance for a workload compared to fully shared cache or static application of CAT while preserving the performance isolation possible with CAT.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: Impact of cache interference for MLR</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: Impact of CAT-limited cache size</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head></head><label></label><figDesc>Xeon-D Hugepage Xeon-E5 Xeon-E5 Hugepage</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: Cache set conflicts on Intel Broadwell Processors.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 4 :</head><label>4</label><figDesc>Figure 4: Flow chart of dCat.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 5 :</head><label>5</label><figDesc>Figure 5: The phase change detector of dCat.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 6 :</head><label>6</label><figDesc>Figure 6: State Transition Diagram of dCat.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Figure 7 :</head><label>7</label><figDesc>Figure 7: Example of cache allocation with dCat.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>Figure 8 :</head><label>8</label><figDesc>Figure 8: Impact of cache miss threshold</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head>Figure 9 :</head><label>9</label><figDesc>Figure 9: Impact of IPC improvement threshold</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_11"><head>Figure 10 :</head><label>10</label><figDesc>Figure 10: Cache-way allocation and normalized IPC (to baseline) for MLR</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_12"><head>Figure 11 :Figure 12 :</head><label>1112</label><figDesc>Figure 11: Normalized (to full cache) data access latency for MLR</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_13"><head>Figure 13 :Figure 14 :</head><label>1314</label><figDesc>Figure 13: Cache-way allocation and normalized IPC (to baseline) for MLOAD</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_14"><head>Figure 15 :</head><label>15</label><figDesc>Figure 15: Cache-way allocation and normalized IPC (to baseline) for MLR and MLOAD</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_15"><head>Figure 16 :</head><label>16</label><figDesc>Figure 16: Normalized (to full cache) data access latency with dCat for MLR and MLOAD</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>?</head><label></label><figDesc>We explore the cache needs of different workloads with diverse working set sizes and data access patterns in the multitenant cloud environment.? We develop dCat in real systems, which provides dynamic cache allocation to different workloads according to their cache needs.</figDesc><table /><note><p>? Our evaluations with the prototype of dCat show that it not only effectively guarantees a minimum performance to workloads irrespective of noisy neighbor presence, but also leads to little overhead and improves performance by up to 57.6% over an unmanaged shared cache and by up to 26.6% over a statically-partitioned cache across a range of different workloads.</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head></head><label></label><figDesc>of LOAD/STORE instructions issued by this workload. ? LLC references (llc_re f ): The total LLC references of all cores assigned to a workload. Low llc_re f means this workload does not require lots of LLC thus can not benefit from it. ? LLC cache misses (llc_miss): The total LLC cache misses suffered by a workload. We calculate the LLC cache miss rate (llc_miss_rate) by llc_miss/llc_re f . ? Retired instructions (ret_ins): Instructions retired by all cores</figDesc><table /><note><p>? L1 cache references (l1_re f ): The total L1 references of all cores assigned to a workload. In Intel commodity processor, all data accesses go to the L1 cache first. Thus we use it to estimate the amount</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>?</head><label></label><figDesc>Receiver: It is a receiver if it benefits from more cache but suffers from less cache. ? Donor: It is classified as donor if the workload does not suffer if cache is reduced and also does not benefit from more cache. ? Keeper: It is a keeper if it would suffer if less cache is available but does not benefit from more. ? Streaming: Streaming is a special classification for workloadsthat even though they have a lot of cache misses, there is no reuse of data in the cache. ? Unknown: The unknown classification is used when no determination can be made using the current information. In dCat, this indicates that this workload at this size can be a Receiver, Donor or Streaming but a determination requires a comparison between different cache sizes. Some workloads with cyclic access pattern<ref type="bibr" target="#b32">[35]</ref> have this behavior.</figDesc><table><row><cell>low cache references or no cache misses</cell></row><row><cell>always no</cell></row><row><cell>performance</cell></row><row><cell>improvement</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 1 :</head><label>1</label><figDesc>Performance table for a workload phase.</figDesc><table><row><cell>Cache-ways</cell><cell>Normalized IPC</cell><cell>Mark</cell></row><row><cell>1</cell><cell>N/A</cell><cell></cell></row><row><cell>2</cell><cell>0.9</cell><cell></cell></row><row><cell>3</cell><cell>1.0</cell><cell>baseline</cell></row><row><cell>4</cell><cell>1.15</cell><cell></cell></row><row><cell>5</cell><cell>1.25</cell><cell></cell></row><row><cell>6</cell><cell>1.3</cell><cell>preferred</cell></row><row><cell>7</cell><cell>1.3</cell><cell></cell></row><row><cell>8</cell><cell>1.3</cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 2 :</head><label>2</label><figDesc>Performance events used by dCat.</figDesc><table><row><cell></cell><cell>Event Num.</cell><cell>Umask Value</cell></row><row><cell>LLC Misses</cell><cell>2EH</cell><cell>41H</cell></row><row><cell>LLC References</cell><cell>2EH</cell><cell>4FH</cell></row><row><cell>L1 Cache Misses</cell><cell>D1H</cell><cell>08H</cell></row><row><cell>L1 Cache Hits</cell><cell>D1H</cell><cell>01H</cell></row><row><cell>Retired Instructions</cell><cell>309</cell><cell></cell></row><row><cell>Unhalted Cycles</cell><cell>30A</cell><cell></cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0"><p>In this paper we do not consider cache isolation between hyperthreads of the same core, since those threads have unavoidable interference caused by shared execution resources.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_1"><p>Intel cache monitoring technology (CMT) cannot substitute such controller, as CMT has no means to determine the optimal cache partitioning.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3" xml:id="foot_2"><p>The inclusivity implies that data kicked out of the LLC because of capacity or conflict miss is also dropped from the closer caches (L1, L2).</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4" xml:id="foot_3"><p>Intel x86 does not allow to allocate 0 way.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="5" xml:id="foot_4"><p>Intel Cache Monitor Technology (CMT) misses some metrics such as L1 reference, phase change detection etc. that we need for dCat. CMT only reports statistics but cannot integrate with CAT to dynamically allocate cache.</p></note>
		</body>
		<back>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>PostgreSQL [4] is a widely used open source object-relational database system. It caches table data, indexes, and query execution plans in a LRU based memory buffer. A larger cache is helpful for reading data faster from the database. We run the PostgreSQL server in the target VM and run pgbench <ref type="bibr" target="#b2">[5]</ref> benchmark issuing select queries from another host in the same LAN. 10 million tuples are inserted into PostgreSQL database initially. Table <ref type="table">5</ref> displays the measured TPS (transactions per second) including connections establishing and average latency per select transaction. With the assigned additional cache-ways, dCat achieves 10.7% lower latency than static partition and performs around 5.7% better than the shared cache. We also tried the multiple database instances scenario in which 3 PostgreSQL instances run in 3 separate VMs (the adversary workloads are still MLOAD-60MB and lookbusy), we observed the similar improvement with dCat. Elasticsearch <ref type="bibr" target="#b0">[1]</ref> is a widely-used search and analytics engine. We use the YCSB <ref type="bibr" target="#b8">[11]</ref> cloud benchmark suite to measure the performance of Elasticsearch. We use workload C which reads from a database of 100K records, each 1 KB. The results are shown in Table <ref type="table">6</ref>; dCat improves average latency by 10% and 99th percentile latency by 11.6% over both static partitioning and shared cache.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">DISCUSSION</head><p>Since dCat is a software solution based on Intel CAT, it inevitably inherits some limitations/restrictions:</p><p>? The cache allocation knob resides on each CPU core (or thread if hyper-threading is enabled). In order to guarantee</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title/>
		<author>
			<persName><surname>Elasticearch</surname></persName>
		</author>
		<ptr target="https://https://www.elastic.co/" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title/>
		<author>
			<persName><surname>Memtier</surname></persName>
		</author>
		<ptr target="https://github.com/RedisLabs/memtier_benchmark" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<ptr target="https://www.postgresql.org/docs/devel/static/pgbench.html" />
		<title level="m">The PostgreSQL Benchmark</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">Gartner says worldwide public cloud services market to grow 18 percent in 2017</title>
		<ptr target="https://www.gartner.com/newsroom/id/3616417" />
		<imprint>
			<date type="published" when="2017-02">Feb 2017</date>
			<publisher>Gartner Press Release</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Towards predictable datacenter networks</title>
		<author>
			<persName><forename type="first">H</forename><surname>Ballani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Costa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Karagiannis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Rowstron</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACM</title>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2011">2011</date>
			<biblScope unit="volume">41</biblScope>
			<biblScope unit="page" from="242" to="253" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Core-level activity prediction for multicore power management</title>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">L</forename><surname>Bircher</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">K</forename><surname>John</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Journal on Emerging and Selected Topics in Circuits and Systems</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="218" to="227" />
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">A user friendly phase detection methodology for hpc systems&apos; analysis</title>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">L T</forename><surname>Chetsa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Lefevre</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J.-M</forename><surname>Pierson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Stolf</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Da Costa</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Green Computing and Communications (GreenCom), 2013 IEEE and Internet of Things (iThings/CPSCom</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2013">2013</date>
			<biblScope unit="page" from="118" to="125" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Hardware identification of cache conflict misses</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">D</forename><surname>Collins</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">M</forename><surname>Tullsen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 32nd annual ACM/IEEE international symposium on Microarchitecture</title>
		<meeting>the 32nd annual ACM/IEEE international symposium on Microarchitecture</meeting>
		<imprint>
			<publisher>IEEE Computer Society</publisher>
			<date type="published" when="1999">1999</date>
			<biblScope unit="page" from="126" to="135" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Benchmarking cloud serving systems with ycsb</title>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">F</forename><surname>Cooper</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Silberstein</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Tam</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Ramakrishnan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Sears</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 1st ACM symposium on Cloud computing</title>
		<meeting>the 1st ACM symposium on Cloud computing</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2010">2010</date>
			<biblScope unit="page" from="143" to="154" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">The tail at scale</title>
		<author>
			<persName><forename type="first">J</forename><surname>Dean</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">A</forename><surname>Barroso</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Communications of the ACM</title>
		<imprint>
			<biblScope unit="volume">56</biblScope>
			<biblScope unit="page" from="74" to="80" />
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Quantifying interference for datacenter applications</title>
		<author>
			<persName><forename type="first">C</forename><surname>Delimitrou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Kozyrakis</surname></persName>
		</author>
		<author>
			<persName><surname>Ibench</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Workload Characterization (IISWC), 2013 IEEE International Symposium on</title>
		<imprint>
			<date type="published" when="2013-09">Sept 2013</date>
			<biblScope unit="page" from="23" to="33" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Quasar: Resource-efficient and qosaware cluster management</title>
		<author>
			<persName><forename type="first">C</forename><surname>Delimitrou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Kozyrakis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 19th International Conference on Architectural Support for Programming Languages and Operating Systems</title>
		<meeting>the 19th International Conference on Architectural Support for Programming Languages and Operating Systems<address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2014">2014</date>
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="page" from="127" to="144" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Marketdriven llc allocation</title>
		<author>
			<persName><forename type="first">L</forename><surname>Funaro</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><forename type="middle">A</forename><surname>Ben-Yehuda</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Schuster</surname></persName>
		</author>
		<author>
			<persName><surname>Ginseng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2016 USENIX Annual Technical Conference (USENIX ATC</title>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Cpu2006 working set size</title>
		<author>
			<persName><forename type="first">D</forename><surname>Gove</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM SIGARCH Computer Architecture News</title>
		<imprint>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="page" from="90" to="96" />
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">mclock: handling throughput variability for hypervisor io scheduling</title>
		<author>
			<persName><forename type="first">A</forename><surname>Gulati</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Merchant</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">J</forename><surname>Varman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 9th USENIX conference on Operating systems design and implementation</title>
		<meeting>the 9th USENIX conference on Operating systems design and implementation</meeting>
		<imprint>
			<publisher>USENIX Association</publisher>
			<date type="published" when="2010">2010</date>
			<biblScope unit="page" from="437" to="450" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Secondnet: a data center network virtualization architecture with bandwidth guarantees</title>
		<author>
			<persName><forename type="first">C</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Kong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 6th International Conference on Emerging Networking EXperiments and Technologies</title>
		<meeting>the 6th International Conference on Emerging Networking EXperiments and Technologies</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2010">2010</date>
			<biblScope unit="page">15</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Spec cpu2006 benchmark descriptions</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">L</forename><surname>Henning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM SIGARCH Computer Architecture News</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="1" to="17" />
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Cache qos: From concept to reality in the intel? xeon? processor e5-2600 v3 product family</title>
		<author>
			<persName><forename type="first">A</forename><surname>Herdrich</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Verplanke</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Autee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Illikkal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Gianos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Sing-Hal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Iyer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2016 IEEE International Symposium on High Performance Computer Architecture (HPCA</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="657" to="668" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Increasing platform determinism with platform quality of service for the data plane development kit</title>
		<author>
			<persName><surname>Intel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Intel White Paper</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Cqos: A framework for enabling qos in shared caches of cmp platforms</title>
		<author>
			<persName><forename type="first">R</forename><surname>Iyer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 18th Annual International Conference on Supercomputing</title>
		<meeting>the 18th Annual International Conference on Supercomputing<address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2004">2004</date>
			<biblScope unit="page" from="257" to="266" />
		</imprint>
	</monogr>
	<note>ICS &apos;04, ACM</note>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Qos policies and architecture for cache/memory in cmp platforms</title>
		<author>
			<persName><forename type="first">R</forename><surname>Iyer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Illikkal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Makineni</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Newell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Solihin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Hsu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Reinhardt</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2007 ACM SIGMETRICS International Conference on Measurement and Modeling of Computer Systems</title>
		<meeting>the 2007 ACM SIGMETRICS International Conference on Measurement and Modeling of Computer Systems</meeting>
		<imprint>
			<date type="published" when="2007">2007</date>
			<biblScope unit="page" from="25" to="36" />
		</imprint>
	</monogr>
	<note>SIGMETRICS &apos;07, ACM</note>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">Memory characterization of workloads using instrumentation-driven simulation-a pin-based memory characterization of the spec cpu2000 and spec cpu2006 benchmark suites</title>
		<author>
			<persName><forename type="first">A</forename><surname>Jaleel</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2007">2007</date>
			<publisher>Intel Corporation, VSSAD</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">Eyeq: Practical network performance isolation for the multi-tenant cloud</title>
		<author>
			<persName><forename type="first">V</forename><surname>Jeyakumar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Alizadeh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Mazieres</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Prabhakar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Azure</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Measuring interference between live datacenter applications</title>
		<author>
			<persName><forename type="first">M</forename><surname>Kambadur</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Moseley</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Hank</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">A</forename><surname>Kim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Conference on High Performance Computing, Networking, Storage and Analysis</title>
		<meeting>the International Conference on High Performance Computing, Networking, Storage and Analysis</meeting>
		<imprint>
			<date type="published" when="2012">2012</date>
			<biblScope unit="volume">51</biblScope>
			<biblScope unit="page">12</biblScope>
		</imprint>
	</monogr>
	<note>SC &apos;12</note>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Ubik: Efficient cache sharing with strict qos for latency-critical workloads</title>
		<author>
			<persName><forename type="first">H</forename><surname>Kasture</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Sanchez</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 19th International Conference on Architectural Support for Programming Languages and Operating Systems</title>
		<meeting>the 19th International Conference on Architectural Support for Programming Languages and Operating Systems</meeting>
		<imprint>
			<date type="published" when="2014">2014</date>
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="page" from="729" to="742" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Os-controlled cache predictability for real-time systems</title>
		<author>
			<persName><forename type="first">J</forename><surname>Liedtke</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Hartig</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Hohmuth</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Real-Time Technology and Applications Symposium, 1997. Proceedings., Third IEEE</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="1997">1997</date>
			<biblScope unit="page" from="213" to="224" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Gaining insights into multicore cache partitioning: Bridging the gap between simulation and real systems</title>
		<author>
			<persName><forename type="first">J</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Sadayappan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">High Performance Computer Architecture</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2008">2008. 2008. 2008</date>
			<biblScope unit="page" from="367" to="378" />
		</imprint>
	</monogr>
	<note>IEEE 14th International Symposium on</note>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Improving resource efficiency at scale</title>
		<author>
			<persName><forename type="first">D</forename><surname>Lo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Govindaraju</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Ranganathan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Kozyrakis</surname></persName>
		</author>
		<author>
			<persName><surname>Heracles</surname></persName>
		</author>
		<idno>ISCA 15</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 42Nd Annual International Symposium on Computer Architecture</title>
		<meeting>the 42Nd Annual International Symposium on Computer Architecture<address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="450" to="462" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Bubbleup: Increasing utilization in modern warehouse scale computers via sensible colocations</title>
		<author>
			<persName><forename type="first">J</forename><surname>Mars</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Hundt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Skadron</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">L</forename><surname>Soffa</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 44th annual IEEE/ACM International Symposium on Microarchitecture</title>
		<meeting>the 44th annual IEEE/ACM International Symposium on Microarchitecture</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2011">2011</date>
			<biblScope unit="page" from="248" to="259" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">An adaptive interface to scalable cloud storage</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">C</forename><surname>Mccullough</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Dunagan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Wolman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">C</forename><surname>Snoeren</surname></persName>
		</author>
		<author>
			<persName><surname>Stout</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of the USENIX Annual Technical Conference</title>
		<meeting>of the USENIX Annual Technical Conference</meeting>
		<imprint>
			<date type="published" when="2010">2010</date>
			<biblScope unit="page" from="47" to="60" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
		<title level="m" type="main">Silverline: Data and network isolation for cloud services</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Mundada</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Ramachandran</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Feamster</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Elasticswitch: Practical work-conserving bandwidth guarantees for cloud computing</title>
		<author>
			<persName><forename type="first">L</forename><surname>Popa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Yalagandula</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Banerjee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">C</forename><surname>Mogul</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Turner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">R</forename><surname>Santos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM SIGCOMM Computer Communication</title>
		<imprint>
			<biblScope unit="page" from="351" to="362" />
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Adaptive insertion policies for high performance caching</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">K</forename><surname>Qureshi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Jaleel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><forename type="middle">N</forename><surname>Patt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">C</forename><surname>Steely</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Emer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In ACM SIGARCH Computer Architecture News</title>
		<imprint>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="page" from="381" to="391" />
			<date type="published" when="2007">2007</date>
			<publisher>ACM</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Utility-based cache partitioning: A lowoverhead, high-performance, runtime mechanism to partition shared caches</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">K</forename><surname>Qureshi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><forename type="middle">N</forename><surname>Patt</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">39th Annual IEEE/ACM International Symposium on Microarchitecture</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2006">2006. 2006</date>
			<biblScope unit="page" from="423" to="432" />
		</imprint>
	</monogr>
	<note type="report_type">MICRO-39</note>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Vantage: Scalable and efficient fine-grain cache partitioning</title>
		<author>
			<persName><forename type="first">D</forename><surname>Sanchez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Kozyrakis</surname></persName>
		</author>
		<idno>ISCA 11</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 38th Annual International Symposium on Computer Architecture</title>
		<meeting>the 38th Annual International Symposium on Computer Architecture</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2011">2011</date>
			<biblScope unit="page" from="57" to="68" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Phase tracking and prediction</title>
		<author>
			<persName><forename type="first">T</forename><surname>Sherwood</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Sair</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Calder</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In ACM SIGARCH Computer Architecture News</title>
		<imprint>
			<biblScope unit="volume">31</biblScope>
			<biblScope unit="page" from="336" to="349" />
			<date type="published" when="2003">2003</date>
			<publisher>ACM</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">The application slowdown model: Quantifying and controlling the impact of inter-application interference at shared caches and main memory</title>
		<author>
			<persName><forename type="first">L</forename><surname>Subramanian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Seshadri</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Ghosh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Khan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Mutlu</surname></persName>
		</author>
		<idno>MICRO-48</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 48th International Symposium on Microarchitecture</title>
		<meeting>the 48th International Symposium on Microarchitecture</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="62" to="75" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Argon: Performance insulation for shared storage servers</title>
		<author>
			<persName><forename type="first">M</forename><surname>Wachs</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Abd-El-Malek</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Thereska</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">R</forename><surname>Ganger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">FAST</title>
		<imprint>
			<date type="published" when="2007">2007</date>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page" from="5" to="5" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Dynamic classification of program memory behaviors in cmps</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Loh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2nd Workshop on Chip Multiprocessor Memory Systems and Interconnects</title>
		<meeting>the 2nd Workshop on Chip Multiprocessor Memory Systems and Interconnects</meeting>
		<imprint>
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Pipp: promotion/insertion pseudo-partitioning of multi-core shared caches</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">H</forename><surname>Loh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In ACM SIGARCH Computer Architecture News</title>
		<imprint>
			<biblScope unit="page" from="174" to="183" />
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Coloris: a dynamic cache partitioning system using page coloring</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Ye</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>West</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 23rd international conference on Parallel architectures and compilation</title>
		<meeting>the 23rd international conference on Parallel architectures and compilation</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="381" to="392" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Redis: Lightweight key/value store that goes the extra mile</title>
		<author>
			<persName><forename type="first">J</forename><surname>Zawodny</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Linux Magazine</title>
		<imprint>
			<biblScope unit="volume">79</biblScope>
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Cpi2: Cpu performance isolation for shared compute clusters</title>
		<author>
			<persName><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Tune</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Hagmann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Jnagal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Gokhale</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Wilkes</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 8th ACM European Conference on Computer Systems</title>
		<meeting>the 8th ACM European Conference on Computer Systems<address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2013">2013</date>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="page" from="379" to="391" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
