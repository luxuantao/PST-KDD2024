<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Face spoofing detection from single images using texture and local shape analysis</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">J</forename><surname>Ma ¨a ¨tta</surname></persName>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">Center for Machine Vision Research</orgName>
								<orgName type="department" key="dep2">Department of Computer Science and Engineering</orgName>
								<orgName type="institution">University of Oulu</orgName>
								<address>
									<postBox>P.O. Box 4500</postBox>
									<postCode>FI-90014</postCode>
									<settlement>Oulu</settlement>
									<country key="FI">Finland</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">¨a</forename><surname>Hadid</surname></persName>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">Center for Machine Vision Research</orgName>
								<orgName type="department" key="dep2">Department of Computer Science and Engineering</orgName>
								<orgName type="institution">University of Oulu</orgName>
								<address>
									<postBox>P.O. Box 4500</postBox>
									<postCode>FI-90014</postCode>
									<settlement>Oulu</settlement>
									<country key="FI">Finland</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">M</forename><surname>Pietika</surname></persName>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">Center for Machine Vision Research</orgName>
								<orgName type="department" key="dep2">Department of Computer Science and Engineering</orgName>
								<orgName type="institution">University of Oulu</orgName>
								<address>
									<postBox>P.O. Box 4500</postBox>
									<postCode>FI-90014</postCode>
									<settlement>Oulu</settlement>
									<country key="FI">Finland</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Face spoofing detection from single images using texture and local shape analysis</title>
					</analytic>
					<monogr>
						<idno type="ISSN">2047-4938</idno>
					</monogr>
					<idno type="MD5">B1C9A5026D65FA204A5FD056608BF6E4</idno>
					<idno type="DOI">10.1049/iet-bmt.2011.0009</idno>
					<note type="submission">Received on 16th December 2011 Revised on 9th February 2012</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.3" ident="GROBID" when="2023-07-28T11:29+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Current face biometric systems are vulnerable to spoofing attacks. A spoofing attack occurs when a person tries to masquerade as someone else by falsifying data and thereby gaining illegitimate access. Inspired by image quality assessment, characterisation of printing artefacts and differences in light reflection, the authors propose to approach the problem of spoofing detection from texture analysis point of view. Indeed, face prints usually contain printing quality defects that can be well detected using texture and local shape features. Hence, the authors present a novel approach based on analysing facial image for detecting whether there is a live person in front of the camera or a face print. The proposed approach analyses the texture and gradient structures of the facial images using a set of low-level feature descriptors, fast linear classification scheme and score level fusion. Compared to many previous works, the authors proposed approach is robust and does not require user-cooperation. In addition, the texture features that are used for spoofing detection can also be used for face recognition. This provides a unique feature space for coupling spoofing detection and face recognition. Extensive experimental analysis on three publicly available databases showed excellent results compared to existing works.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Despite the great deal of progress during the recent years <ref type="bibr" target="#b0">[1]</ref>, 2D face biometrics (that is identifying individuals based on their 2D facial biometric characteristic) is still a major area of research. Wide range of viewpoints, occlusions, aging of subjects and complex outdoor lighting are challenges in face recognition. Although there is a significant number of works addressing these issues, the vulnerabilities of face biometric systems to spoofing attacks are mostly overlooked. For instance, the Windows XP and Vista laptops of Lenovo, Asus and Toshiba come with built-in webcams and embedded biometric systems that authenticate users by scanning their faces. However, in 2009, the Security and Vulnerability Research Team of the University of Hanoi (Vietnam) has demonstrated at Black Hat 2009 conference, the world's premier technical security conference, how to easily spoof and bypass these systems (Lenovo's Veriface III, Asus' SmartLogon V1.0.0005, and Toshiba's Face Recognition 2.0.2.32 -each set to its highest security level) using fake facial images of the legitimate user and thus gaining access to the laptops. This vulnerability is now listed in the National Vulnerability Database of the National Institute of Standards and Technology (NIST) in the US. This single example demonstrates the vulnerabilities in current face biometric systems, which suggest an urgent need for addressing spoofing attacks to enhance the security and robustness of face biometric systems, and to bring the technology into practical use.</p><p>A spoofing attack occurs when a person tries to masquerade as someone else by falsifying data and thereby gaining illegitimate access and advantages. For instance, one can spoof a face recognition system by presenting a photograph, a video, a mask or a 3D model of a targeted person in front of the camera. Although one can also use make-up or plastic surgery as other means of spoofing, photographs are probably the most common sources of spoofing attacks because one can easily download and capture facial images. As illustrated in Fig. <ref type="figure" target="#fig_0">1</ref>, face images captured from printed photos can look very similar to face images captured from real faces.</p><p>Lately, the vulnerabilities to spoofing attacks have received more attention and a good example of this was the recently organised IJCB 2011 Competition on Counter Measures to 2D Facial Spoofing Attacks <ref type="bibr" target="#b1">[2]</ref>. Furthermore, the number of publications in the field is growing steadily and some publicly available databases <ref type="bibr">[3 -6]</ref> have been released. Still, the field of non-intrusive anti-spoofing methods is rather immature, since there exist no consensus on the best spoofing detection practices and techniques and not that many standard databases to develop and test the algorithms for objective comparison <ref type="bibr" target="#b5">[6]</ref>.</p><p>Typical countermeasure against spoofing is liveness detection that aims at detecting physiological signs of life such as eye blinking, facial expression changes, mouth movements etc. Another existing countermeasure to spoofing attacks consists of combining face recognition with other biometric modalities such as gait and speech. Indeed, multi-modal systems are intrinsically more difficult to spoof than uni-modal systems. Some other attempts to counter face spoofing are based on structure from motion to calculate the depth information.</p><p>Inspired by image quality assessment, characterisation of printing artefacts and by differences in light reflection, we propose to approach the problem of spoofing detection from texture analysis point of view. The differences in surface geometry between human faces and face prints cause distinctive specular reflections and shades as a human face is a complex non-rigid 3D object whereas a photograph can be seen as a planar rigid object. Furthermore printing artefacts and different surface properties, for example, pigments, may cause degradation in image quality which can be assessed using texture and local shape analysis. Hence, we present a novel approach based on analysing facial image textures and gradient structures for detecting whether there is a live person or a face print in front of the camera. Compared to many previous works, our proposed approach is robust and does not require user-cooperation. In addition, the texture features that are used for spoofing detection can also be used for face recognition. This provides a unique feature space for coupling spoofing detection and face recognition.</p><p>In this work, we extend our spoofing detection approach using local binary pattern (LBP)-based micro-texture analysis <ref type="bibr" target="#b6">[7]</ref> by introducing two low-level features, Gabor wavelet features <ref type="bibr" target="#b7">[8]</ref> and histogram of oriented gradients (HOG) [9], to the face description which now consists of three enhanced feature vectors. The proposed method adopts complementary properties of two powerful texture descriptors, since LBP encodes the micro-texture patterns and Gabor filters more macroscopic information. In addition, HOG-based local shape description provides additional information to the face description. A homogeneous kernel map <ref type="bibr" target="#b9">[10]</ref> is applied on each resulting feature vector transforming the data into compact linear representation and reproducing an accurate approximation of the desired kernel function. This representation enables then to use fast linear support vector machine (SVM) <ref type="bibr" target="#b10">[11]</ref> classifiers. The final decision, whether there is a live person in front of the camera or not, is based on the score level fusion of the individual SVM outputs. Extensive experiments on three publicly available database (NUAA Photograph Imposter Database <ref type="bibr" target="#b3">[4]</ref>, Yale Recaptured Database <ref type="bibr" target="#b4">[5]</ref> and Print-Attack Database <ref type="bibr" target="#b5">[6]</ref>) containing several real and fake faces showed excellent results compared to many previous works.</p><p>The rest of the paper is organised as follows. Section 2 discusses related works on face spoofing attacks and countermeasures. Our proposed approach, using a set of low-level feature descriptors and linear classification scheme, is then described in Section 3 and evaluated in Section 4, where extensive experiments are conducted. The results are thoroughly analysed and also compared to many previous works. A conclusion is drawn in Section 5.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related work</head><p>Without anti-spoofing measures most of the state-of-the-art facial biometric systems are basically vulnerable to attacks, since they try to maximise the discrimination between identities, instead of determining whether the presented trait originates from a real live client. Even a simple photograph of the enrolled person's face, displayed as a hard-copy or on a screen, will fool the system. Short surveys of previous attempts against spoofing attacks can be found in <ref type="bibr" target="#b2">[3,</ref><ref type="bibr">12]</ref>. Typical countermeasure against spoofing is liveness detection that aims at detecting physiological signs of life such as eye blinking, facial expression changes, mouth movements and so on. For instance, Pan et al. <ref type="bibr" target="#b2">[3]</ref> exploited the observation that humans blink once every 2 -4 s and proposed an eye blink-based anti-spoofing method. It uses conditional random field framework to model and detect eye blinking. Another commonly used countermeasure is motion analysis since it can be assumed that the movement of planar objects, for example, video displays and photographs, differs significantly from real human faces which are complex 3D objects. Kollreider et al. <ref type="bibr" target="#b12">[13]</ref> presented an optical-flow-based method to capture and track the subtle movements of different facial parts, assuming that facial parts in real faces move differently than on photographs. In another work <ref type="bibr" target="#b13">[14]</ref>, Bao et al. also used optical flow for motion estimation for detecting attacks produced with planar media such as prints or screens. Experiments on a private database showed a 6% false-alarm against about 14% false-acceptance.</p><p>Another category of anti-spoofing methods are based on the analysis of skin properties such as skin texture and skin reflectance. For instance, Li et al. <ref type="bibr" target="#b14">[15]</ref> described a method for detecting print-attack face spoofing. The method is based on the analysis of 2D Fourier spectra, assuming that photographs are usually smaller in size and they would contain fewer high-frequency components compared to real faces. Such an approach may work well for down-sampled photos but is likely to fail for higher-quality images. The database used in the experiments is unfortunately not publicly available.</p><p>In a recent work, Tan et al. <ref type="bibr" target="#b3">[4]</ref> considered the Lambertian reflectance to discriminate between the 2D images of face prints and 3D live faces. The method extracts latent reflectance features using a variational retinex-based method and difference-of-Gaussians-based approach. The features are then used for classification. The authors reported promising results on a database composed of real accesses and attacks to 15 subjects using both photo-quality and laser-quality prints. The database, the NUAA Photograph Imposter Database, is made publically available. This provides a valuable resource for fairly comparing the results of different methods. Hence, our current work also considers this database.</p><p>Other countermeasures against face spoofing attacks include multi-modal analysis and multi-spectral methods. A system combining face recognition with other biometric modalities such as gait and speech is indeed intrinsically more difficult to spoof than uni-modal systems. Multispectral images can also be used for analysing the reflectance of object surfaces and thus discriminating live faces from fake ones <ref type="bibr">[16 -18]</ref>.</p><p>In recently organised IJCB 2011 Competition on Counter Measures to 2D Facial Spoofing Attacks <ref type="bibr" target="#b1">[2]</ref>, a common trend was to use multiple anti-spoofing measures combining motion, liveness and texture and the participants were able to achieve impressive results. However, all best-performing algorithms used also some kind of texture analysis. The main problem of motion analysis and liveness detectionbased anti-spoofing measures is that the verification process takes some time or moreover, the user needs to be very cooperative. In addition, these techniques are vulnerable to video replay attacks. The main advantages of spoofing detection schemes based on properties of a single image is that they treat video playback attacks as if they were photo attacks, since only a single frame is considered <ref type="bibr" target="#b18">[19]</ref>.</p><p>Bai et al. <ref type="bibr" target="#b18">[19]</ref> used micro-textures extracted from the specularity component of a recaptured image and a linear SVM classifier to detect spoofing attacks. The major drawback of this method is that it requires high-resolution input images in order to discriminate the fine micro-texture of the used spoofing medium. Another interesting approach was introduced in [20] by Gao et al. who used a set of physical features to discriminate the recaptured images from real ones.</p><p>It appears that most of the existing methods for spoofing detection are either very complex (and hence not very practical for real-world face biometric systems requiring fast processing) or using non-conventional imaging systems (e.g. multi-spectral imaging) and devices (e.g. thermal cameras). Therefore we propose an approach based on highly discriminative texture and local shape features, using conventional webcam-quality images and requiring no user cooperation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Spoofing detection using texture and local shape analysis</head><p>Face images captured from printed photos may visually look very similar to the images captured from live faces (see Fig. <ref type="figure" target="#fig_0">1</ref>). Consequently, all these images would be largely overlapping in the original input space. Therefore a suitable feature space is needed for separating the two classes (live against fake face images). The main issue is how to derive such a feature space. Our method aims at learning the fine differences between the images of real face and those of face prints, and then designing a feature space which emphasises those differences.</p><p>A close look at the differences between real faces and face prints reveals that human faces and prints reflect light in different ways, because a human face is a complex nonrigid 3D object whereas a photograph can be seen as a planar rigid object. The surface properties of real faces and prints, for example, pigments, are also different. These two distinctive properties may cause characteristic specular reflections and shades. In addition, face prints often contain printing artefacts, such as jitter and banding <ref type="bibr" target="#b20">[21]</ref>, that can be detected with texture and local shape analysis on uniform or smooth areas. Furthermore, spoof attacks when executed with face prints tend to engender some overall image blur because of, for example, a low-resolution printing device or rapid motion caused by simulated photoattacks performed like in <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b3">4]</ref>. Example images of possible cues used for face print spoofing detection are presented in Fig. <ref type="figure">2</ref>.</p><p>Inspired by the observations above, and particularly by image quality assessment and characterisation of printing artefacts, we derive a facial representation (or a feature space) that is able to capture typical characteristics of real and fake face images. Hence, the key idea of our approach is emphasising the texture and gradient structure differences in the feature space. We extend our spoofing detection approach using LBP-based micro-texture analysis <ref type="bibr" target="#b6">[7]</ref> by introducing two complementary low-level features to the face description, Gabor wavelets and HOG. The block diagram of our anti-spoofing approach can be seen in Fig. <ref type="figure" target="#fig_1">3</ref>. The proposed method adopts two powerful texture features, LBPs and Gabor wavelets, for describing not only the micro-textures but also more macroscopic information. In addition, local shape description is introduced using HOG. Each low-level descriptor produces its own face representation on which homogeneous kernel map is Fig. <ref type="figure">2</ref> Examples of face print properties that could be used for spoofing detection, for example, overall image blur, low contrast, characteristic specular reflections and printing artefacts applied to transform the data into compact linear representation. Each vector in its own transformed feature space is then fed to a linear SVM classifier and score level fusion of the individual SVM outputs determines whether there is a live person or a fake image in front of the camera. We describe below our enhanced spoofing detection method.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Low-level feature extraction</head><p>The texture feature descriptors considered in this paper include LBPs and Gabor wavelets. In addition, shape information is combined to the face representation using HOG. A summary of these descriptors is presented as follows.</p><p>The LBP texture analysis operator, introduced by Ojala et al. <ref type="bibr" target="#b21">[22]</ref>, is defined as a grey-scale invariant texture measure, derived from a general definition of texture in a local neighbourhood. It is a powerful means of texture description and among its properties in real-world applications are its discriminative power, computational simplicity and tolerance against monotonic grey-scale changes. We chose uniform LBP patterns in our experiments, because of the more compact feature histogram (see <ref type="bibr" target="#b21">[22]</ref> for details).</p><p>In addition to the LBP-based texture analysis, we use also Gabor wavelet features to enhance the texture representation of the facial image. The basic idea is to extract features at multiple scales and orientation using a Gabor wavelet decomposition. For classification purposes, a feature vector is constructed using the mean and standard deviation of the magnitude of the transform coefficients at different scales and orientations <ref type="bibr" target="#b7">[8]</ref>.</p><p>The local shape characteristics are introduced to the face representation using HOG which captures the edge or gradient structures of the facial image. HOG representation is invariant to local geometric transformations if translations or rotations are much smaller than the local spatial or orientation bin size <ref type="bibr">[9]</ref>.</p><p>The proposed face description consists of three enhanced feature histograms which encode the texture information and gradient structures of the facial images. First, the face is detected, cropped and normalised into an M × M pixel image. In order to preserve spatial information, the facial images are spatially partitioned into several local regions, each of which corresponds to a local patch of the face image. Then the descriptors are extracted from each block and the resulting feature vectors are concatenated into an enhanced feature vector.</p><p>Our investigations have shown, however, that texture and gradient structure details, that are needed for discriminating a real human face from fake ones, can best be detected when LBP, Gabor wavelet and HOG face representations are computed separately using different block divisions for each feature. Our LBP description <ref type="bibr" target="#b6">[7]</ref> computes LBP 8,1 u2 features from 3 × 3 overlapping regions (with an overlapping size of 14 pixels) to capture the spatial information and enhances the holistic description by including two global LBP histograms computed over the whole face image using LBP 8,2 u2 and LBP 16,2 u2 operators. Gabor filter description is determined using 4 × 4 equally spaced non-overlapping regions from which 40 Gabor wavelets of five different scales and eight orientations are extracted. Eight orientations are considered when computing the HOG features and the blocks overlap half of their area but the used block size depends on the size and geometric normalisation of the input face images.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Classification</head><p>In <ref type="bibr" target="#b9">[10]</ref> Vedaldi and Zisserman presented a technique to accelerate kernel evaluations using a homogeneous kernel map which enables the use of additive kernels, such as x 2 and intersection, in large-scale problems. The desired kernel can be approximated to a very good level by linear ones using suitable explicit feature map which transforms the data into a compact linear representation. Very fast linear SVMs can be then used on this representation instead of non-linear SVMs. The approximations stand on a solid theoretical ground and it has been demonstrated that the performance is indistinguishable from the original nonlinear SVMs, thus reducing the train and test times notably <ref type="bibr" target="#b9">[10]</ref>.</p><p>In our experiments, we applied homogeneous kernel map on each face description to obtain their corresponding linear approximation of a x 2 kernel. Each resulting representation is then fed to an SVM classifier and conventional Z-score normalisation technique and weighted score level fusion are used for combining the outputs of the individual SVMs to determine whether the input image corresponds to a live face or not.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experimental analysis</head><p>In this section, we evaluate the proposed approach on three different databases which consist of single images and videos. First, we perform experiments on the NUAA Photograph Imposter Database and compare the results between LBP-based face representation <ref type="bibr" target="#b6">[7]</ref> and the proposed approach using also Gabor wavelets and HOG. In addition, we validate the use of homogeneous kernel map by comparing it to non-linear SVM used in <ref type="bibr" target="#b6">[7]</ref>. Then, we compare the proposed method to previously published results using Yale Recaptured Database and Print-Attack Database. Linear SVM implementation of LIBLINEAR <ref type="bibr" target="#b22">[23]</ref> and a three-dimensional approximated feature map computed with VLFeat <ref type="bibr" target="#b23">[24]</ref> are used in all experiments.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Evaluation of the extended method</head><p>We considered the publicly available NUAA Photograph Imposter Database <ref type="bibr" target="#b3">[4]</ref> for comparing the enhanced face representation to our previous work presented in <ref type="bibr" target="#b6">[7]</ref> and to validate the use of homogeneous kernel map and linear SVM instead of original non-linear SVM. The data set consists of images of both real client accesses and highquality photo attacks which were recorded using conventional webcams at 20 fps with resolution of 640 × 480 pixels. The face images of live humans and their photographs were collected in three sessions at intervals of about 2 weeks. In addition, during each session, the environmental and illumination conditions are changing. Examples of images from the database can be seen in Fig. <ref type="figure" target="#fig_0">1</ref>.</p><p>The database is divided into separate training and test purposes. The training set contains altogether 1743 face images of nine real clients (889 and 854 from the first and the second sessions, respectively) and 1748 imposter images of the same nine clients (855 and 893 images from the first and the second sessions, respectively). The test set is constructed from 3362 client samples and 5761 imposter images taken during the third session. Only three clients who took part in the first two sessions attended the third session. Furthermore, six new clients and their photographs are introduced in the test set to further increase the level of difficulty. In order to achieve fair comparison, we consider the provided greyscale face images which have been geometrically normalised into images of 64 × 64 pixels.</p><p>We started by investigating whether linear kernel approximation has indistinguishable performance from the full kernel. The experiments were performed using the LBP feature representation presented in <ref type="bibr" target="#b6">[7]</ref> and only the classification scheme differs. The results were surprising since the kernel map approximation was able to produce exactly the same results in terms of equal error rate (EER) as the truly non-linear SVM. However, from the receiver operating characteristic (ROC) curves, which can be seen in Fig. <ref type="figure">4</ref>, we can notice some minor differences at lower false acceptance rates (FAR) which is a crucial operating point in high security applications. Otherwise, the ROC curves are almost the same, thus the kernel approximation is working also according to our experiments.</p><p>We performed also experiments to find out whether the combination of different features (LBP, Gabor wavelets and HOG) can improve the spoofing detection performance. Block size of 21 × 21 pixels was used for extracting HOG features and the weight of each individual matcher is inversely proportional to the corresponding EERs <ref type="bibr" target="#b24">[25]</ref>. First, we wanted to see whether the LBP, Gabor wavelets and HOG descriptions are able to provide complementary information. The results of these experiments are shown in Fig. <ref type="figure" target="#fig_3">5</ref> and Table <ref type="table" target="#tab_0">1</ref>. From the ROC curves and EER values, it can be seen that the combination of all three features leads to best results. Table <ref type="table" target="#tab_1">2</ref> presents a performance comparison between our proposed approach and the best results in <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b4">5,</ref><ref type="bibr" target="#b6">7]</ref> using the same protocol. The results are compared using EER and area under curve (AUC). The comparative results clearly assess the superiority of our approach (improvement in EER from 2.8 to 1.1% and in AUC from 0.995 to 0.999). The ROC curves of Fig. <ref type="figure">4</ref> indicate that the proposed approach is able to improve the performance even at a low FAR.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Experiments using liquid crystal display (LCD) spoofs</head><p>Yale Recaptured Database, proposed by Peixoto et al. <ref type="bibr" target="#b4">[5]</ref>, is composed of 640 real frontal faces from Yale Face Database B <ref type="bibr" target="#b25">[26]</ref> (10 test subjects with 64 different illumination conditions) and 1920 LCD spoofs displaying the images from the Yale Face Database B on three LCD monitors, an LG Flatron L196WTQ Wide 19 ′ , a CTL 171Lx 17 ′ TFT and a DELL Inspiron 1545 notebook. The recaptured images were taken with a Kodak C813 8.2 megapixels and a Samsung Omnia i900, with 5 megapixels. All images were cropped into greyscale images of 64 × 64 pixels. Examples of images from the database can be seen in Fig. <ref type="figure" target="#fig_2">6</ref>.</p><p>Block size of 8 × 8 pixels was used for extracting HOG features and equal weights were assigned to the feature representations in matcher weighting as the bias of each classifier was not computed. All experiments have been performed using ten-fold cross-validation like in <ref type="bibr" target="#b4">[5]</ref> a performance comparison can be seen in Table <ref type="table">3</ref>. The LBP representation <ref type="bibr" target="#b6">[7]</ref> and the proposed approach outperform the best results reported in <ref type="bibr" target="#b4">[5]</ref>, the proposed approach yielding to perfect separation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Evaluation on the Print-Attack Database</head><p>Print-Attack Database <ref type="bibr" target="#b5">[6]</ref> was originally introduced within IJCB 2011 Competition on Counter Measures to 2D Facial Spoofing Attacks <ref type="bibr" target="#b1">[2]</ref>. The data set consists of 200 real client access and 200 print-attack (50 clients) videos which were captured in controlled and uncontrolled lighting conditions (with homogeneous and more complex background, respectively). The videos were recorded using a built-in webcam of a Macbook laptop at 25fps with resolution of 640 × 480 pixels. The print-attacks were generated by taking high-resolution photographs of each client under the same conditions as in their authentication sessions and the captured images were printed in colour on A4-sized paper. The spoofing attack attempts were performed with fixed or hand-held prints.</p><p>The database is divided into three sets, training, development and test data. The EER of development set is used for tuning the threshold which is applied for Fig. <ref type="figure">4</ref> Performance (ROC curves) of the proposed approach, the linear LBP face representation using homogeneous kernel map and the non-linear LBP face representation used in <ref type="bibr" target="#b6">[7]</ref> discriminating the test samples. Table <ref type="table" target="#tab_2">4</ref> summarises how the data set is split into the three separate sets. Clients have been randomly divided for each subset so that the identities do not overlap between the subsets, thus making the problem harder.</p><p>The Print-Attack Database differs from the previous two data sets as the provided data consist of videos of the whole authentication scene instead of segmented and normalised face images. Therefore OpenCV library implementation of the Viola -Jones algorithm <ref type="bibr" target="#b26">[27]</ref> was used for face detection and the eye locations were retrieved using 2D Cascaded AdaBoost <ref type="bibr" target="#b27">[28]</ref>. The face images were geometrically normalised according to the detected eye coordinates and cropped into greyscale images of 80 × 80 pixels. Examples of the geometrically normalised face images can be seen in Fig. <ref type="figure" target="#fig_4">7</ref>. The spoofing detection is still performed from single face images because we want to use the same static image analysis-based spoofing detection scheme in all experiments, though the format of the provided data changes. However, the overlapping size of LBP blocks is now 17 pixels and the block size for extracting HOG features is 10 × 10 pixels. Here, we utilise the corresponding EER of each classifier on development set as weights like in <ref type="bibr" target="#b28">[29]</ref> to allow each classifier more influence.</p><p>Table <ref type="table" target="#tab_3">5</ref> shows a performance comparison between our proposed approach and the teams who participated in the IJCB 2011 Competition on Counter Measures to 2D Facial Spoofing Attacks <ref type="bibr" target="#b1">[2]</ref>. Almost all methods seem to work extremely well on the data set including our approach which is able to obtain perfect results on the test set and only two videos of the development set were incorrectly classified. It is worth mentioning that the proposed antispoofing measure is very robust although only a single image of the face area is considered. The other algorithms are either very complex, since multiple cues, for example, motion and texture analysis or even scene information, are fused together, or not too generic because the problemspecific solution works probably only on similar printing artefacts which are present in the provided data set.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusions</head><p>Current face biometric systems are very vulnerable to spoofing attacks and photographs are probably the most common sources of spoofing attacks. Inspired by image quality assessment, characterisation of printing artefacts and by differences in light reflection, we proposed an approach for spoofing detection based on learning texture features and gradient structures from single images that discriminate live face images from fake ones. Our proposed face description used three enhanced feature histograms which encode the texture and gradient structures of the facial images. A homogeneous kernel map was applied on each resulting feature vector transforming the data into compact linear representation and reproducing an accurate approximation of the desired kernel function. This representation enables then to use fast linear SVM classifiers whose outputs are combined using matcher    weighting to give a final decision. Extensive experiments on three publicly available databases containing several real and fake faces showed excellent results. Compared to many previous works, our proposed approach is robust, computationally fast and does not require user cooperation.</p><p>In addition, the texture features that are used for spoofing detection can also be used for face recognition. This provides a unique feature space for coupling spoofing detection and face recognition. The current publicly available databases have been an important kick-off for finding out best practices for Table <ref type="table">3</ref> Performance comparison between the proposed approach and the best results in <ref type="bibr" target="#b4">[5]</ref> and only LBP <ref type="bibr" target="#b6">[7]</ref> on the Yale Recaptured Database <ref type="bibr">[</ref>  non-intrusive spoofing detection, since common data sets and protocols are provided for objective comparison. However, the excellent results suggest also that more complex databases with various types of high-quality spoofing attacks and proper protocol are needed for future development, since the current publicly available databases have their limitations, thus are not generalising the problem well enough. We believe that our approach can also be extended to detect spoofing attacks using masks or 3D models of the face because skin has a very particular texture with, for example, pores whereas fake faces have seldom such a level of detail.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 1</head><label>1</label><figDesc>Fig. 1 Example of images captured from real faces (upper row) and from printed photos (lower row)</figDesc><graphic coords="2,121.89,56.69,354.23,175.69" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 3</head><label>3</label><figDesc>Fig. 3 Block diagram of the proposed approach</figDesc><graphic coords="4,90.71,56.69,414.27,133.46" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 6</head><label>6</label><figDesc>Fig. 6 Example images from Yale Recaptured Database, real faces (upper row) and printed photos (lower row)</figDesc><graphic coords="6,51.02,608.60,493.91,152.16" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Fig. 5</head><label>5</label><figDesc>Fig. 5 Performance (ROC curves) of the proposed approach and the different feature combinations</figDesc><graphic coords="6,45.64,56.69,237.60,134.65" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Fig. 7</head><label>7</label><figDesc>Fig. 7 Examples of the geometrically normalised face images which were segmented from the videos provided in Print-Attack Database, real faces (upper row) and printed photos (lower row)</figDesc><graphic coords="7,51.02,426.90,493.91,155.74" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1</head><label>1</label><figDesc>Performance comparison between the proposed approach and the different feature combinations</figDesc><table><row><cell>Method</cell><cell>AUC</cell><cell>EER, %</cell></row><row><cell>LBP + Gabor</cell><cell>0.998</cell><cell>2.0</cell></row><row><cell>LBP + HOG</cell><cell>0.999</cell><cell>1.5</cell></row><row><cell>Gabor + HOG</cell><cell>0.996</cell><cell>2.4</cell></row><row><cell>proposed approach</cell><cell>0.999</cell><cell>1.1</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2</head><label>2</label><figDesc>Performance comparison between the proposed</figDesc><table><row><cell cols="3">approach and the best results in [4], [5] and [7] on the NUAA</cell></row><row><cell>Photograph Imposter Database [4]</cell><cell></cell><cell></cell></row><row><cell>Method</cell><cell>AUC</cell><cell>EER, %</cell></row><row><cell>Tan et al. [4]</cell><cell>0.94</cell><cell>-</cell></row><row><cell>Peixoto et al. [5]</cell><cell>0.966</cell><cell>8.2</cell></row><row><cell>LBP only [7]</cell><cell>0.995</cell><cell>2.8</cell></row><row><cell>proposed approach</cell><cell>0.999</cell><cell>1.1</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 4</head><label>4</label><figDesc>Decomposition of the Print-Attack Database</figDesc><table><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>5]</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>Min</cell><cell>Mean</cell><cell>Max</cell><cell>STD</cell></row><row><cell></cell><cell cols="2">Peixoto et al. [5]</cell><cell></cell><cell></cell></row><row><cell></cell><cell cols="2">accuracy, %</cell><cell></cell><cell>89.0</cell><cell>91.7</cell><cell>93.8</cell><cell>1.4</cell></row><row><cell></cell><cell cols="2">true positive rate, %</cell><cell></cell><cell>83.3</cell><cell>85.8</cell><cell>87.7</cell><cell>1.4</cell></row><row><cell></cell><cell cols="2">false positive rate, %</cell><cell></cell><cell>0.0</cell><cell>2.5</cell><cell>5.4</cell><cell>1.5</cell></row><row><cell></cell><cell cols="2">LBP only [7]</cell><cell></cell><cell></cell></row><row><cell></cell><cell cols="2">accuracy, %</cell><cell></cell><cell>98.4</cell><cell>99.6</cell><cell>100</cell><cell>0.5</cell></row><row><cell></cell><cell cols="2">true positive rate, %</cell><cell></cell><cell>98.4</cell><cell>99.2</cell><cell>100</cell><cell>0.8</cell></row><row><cell></cell><cell cols="2">false positive rate, %</cell><cell></cell><cell>0.0</cell><cell>0.2</cell><cell>1.6</cell><cell>0.5</cell></row><row><cell></cell><cell cols="2">Proposed approach</cell><cell></cell><cell></cell></row><row><cell></cell><cell cols="2">accuracy, %</cell><cell></cell><cell>100</cell><cell>100</cell><cell>100</cell><cell>0.0</cell></row><row><cell></cell><cell cols="2">positive rate, %</cell><cell></cell><cell>100</cell><cell>100</cell><cell>100</cell><cell>0.0</cell></row><row><cell></cell><cell cols="2">false positive rate, %</cell><cell></cell><cell>0.0</cell><cell>0.0</cell><cell>0.0</cell><cell>0.0</cell></row><row><cell>Type</cell><cell>Train</cell><cell>Development</cell><cell>Test</cell><cell>Total</cell></row><row><cell>real</cell><cell>60</cell><cell>60</cell><cell>80</cell><cell>200</cell></row><row><cell>attack</cell><cell>30 + 30</cell><cell>30 + 30</cell><cell>40 + 40</cell><cell>100 + 100</cell></row><row><cell>total</cell><cell>120</cell><cell>120</cell><cell>160</cell><cell>400</cell></row></table><note><p>Numbers indicate how many videos are included in each subset (the sums indicate the amount of hand-based and fixed-support attacks)</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 5</head><label>5</label><figDesc>Performance comparison between the proposed approach and the teams who participated in the IJCB 2011 Competition on Counter Measures to 2D Facial Spoofing Attacks<ref type="bibr" target="#b1">[2]</ref> </figDesc><table><row><cell>Method</cell><cell cols="2">Development</cell><cell></cell><cell>Test</cell><cell></cell></row><row><cell></cell><cell>FAR</cell><cell>FRR</cell><cell>FAR</cell><cell>FRR</cell><cell>HTER</cell></row><row><cell>AMILAB</cell><cell>0.00</cell><cell>0.00</cell><cell>0.00</cell><cell>1.25</cell><cell>0.63</cell></row><row><cell>CASIA</cell><cell>1.67</cell><cell>1.67</cell><cell>0.00</cell><cell>0.00</cell><cell>0.00</cell></row><row><cell>IDIAP</cell><cell>0.00</cell><cell>0.00</cell><cell>0.00</cell><cell>0.00</cell><cell>0.00</cell></row><row><cell>SIANI</cell><cell>1.67</cell><cell>1.67</cell><cell>0.00</cell><cell>21.25</cell><cell>10.63</cell></row><row><cell>UNICAMP</cell><cell>1.67</cell><cell>1.67</cell><cell>1.25</cell><cell>0.00</cell><cell>0.00</cell></row><row><cell>UOULU</cell><cell>0.00</cell><cell>0.00</cell><cell>0.00</cell><cell>0.00</cell><cell>0.00</cell></row><row><cell>proposed approach</cell><cell>1.67</cell><cell>1.67</cell><cell>0.00</cell><cell>0.00</cell><cell>0.00</cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" xml:id="foot_0"><p>doi: 10.1049/iet-bmt.2011.0009 &amp; The Institution of Engineering and Technology 2012 www.ietdl.org</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" xml:id="foot_1"><p>&amp; The Institution of Engineering and Technology 2012 doi: 10.1049/iet-bmt.2011.0009 www.ietdl.org</p></note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Acknowledgments</head><p>This work has been performed within the TABULA RASA project 7th Framework Research Programme of the European Union (EU), grant agreement number 257289. The financial support of the Academy of Finland is also gratefully acknowledged.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">Handbook of face recognition</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">K</forename><surname>Jain</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">Z</forename><surname>Li</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2011">2011</date>
			<publisher>Springer-Verlag</publisher>
			<pubPlace>New York, Secaucus, NJ</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Competition on counter measures to 2-d facial spoofing attacks</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">M</forename><surname>Chakka</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Anjos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Marcel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IAPR IEEE Int. Joint Conf. on Biometrics (IJCB)</title>
		<meeting>IAPR IEEE Int. Joint Conf. on Biometrics (IJCB)<address><addrLine>Washington, DC, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">Liveness detection for face recognition</title>
		<author>
			<persName><forename type="first">G</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Sun</surname></persName>
		</author>
		<idno>Ch. 9</idno>
		<editor>Delac, K., Grgic, M., Bartlett, M.S.</editor>
		<imprint>
			<date type="published" when="2008">2008</date>
			<publisher>TECH</publisher>
		</imprint>
	</monogr>
	<note>Recent advances in face recognition</note>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Face liveness detection from a single image with sparse low rank bilinear discriminative model</title>
		<author>
			<persName><forename type="first">X</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Jiang</surname></persName>
		</author>
		<ptr target="http://portal.acm.org/citation.cfm?id=1888212.1888251" />
	</analytic>
	<monogr>
		<title level="m">Proc. 11th European Conf. on Computer vision: Part VI. ECCV&apos;10</title>
		<meeting>11th European Conf. on Computer vision: Part VI. ECCV&apos;10</meeting>
		<imprint>
			<date type="published" when="2010">2010</date>
			<biblScope unit="page" from="504" to="517" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Face liveness detection under bad illumination conditions</title>
		<author>
			<persName><forename type="first">B</forename><surname>Peixoto</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Michelassi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Rocha</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEE Int. Conf. on Image Processing</title>
		<imprint>
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Counter-measures to photo attacks in face recognition: a public database and a baseline</title>
		<author>
			<persName><forename type="first">A</forename><surname>Anjos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Marcel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IAPR IEEE Int. Joint Conf. on Biometrics (IJCB)</title>
		<meeting>IAPR IEEE Int. Joint Conf. on Biometrics (IJCB)<address><addrLine>Washington, DC, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Face spoofing detection from single images using micro-texture analysis</title>
		<author>
			<persName><forename type="first">¨</forename><surname>Ma ¨a ¨tta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Hadid</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Pietika ¨inen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IAPR IEEE Int. Joint Conf. on Biometrics (IJCB)</title>
		<meeting>IAPR IEEE Int. Joint Conf. on Biometrics (IJCB)<address><addrLine>Washington, DC, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Texture features for browsing and retrieval of image Data</title>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">S</forename><surname>Manjunath</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">Y</forename><surname>Ma</surname></persName>
		</author>
		<idno type="DOI">10.1109/34.531803</idno>
		<ptr target="http://dx.doi.org/10.1109/34.531803" />
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Pattern Anal. Mach. Intell</title>
		<imprint>
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="page" from="837" to="842" />
			<date type="published" when="1996">1996</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Histograms of oriented gradients for human detection</title>
		<author>
			<persName><forename type="first">N</forename><surname>Dalal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Triggs</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Int. Conf. on Computer Vision &amp; Pattern Recognition</title>
		<imprint>
			<date type="published" when="2005">2005</date>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="886" to="893" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Efficient additive kernels via explicit feature maps</title>
		<author>
			<persName><forename type="first">A</forename><surname>Vedaldi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conf. on Computer Vision and Pattern Recognition</title>
		<meeting>IEEE Conf. on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">Statistical learning theory</title>
		<author>
			<persName><forename type="first">V</forename><forename type="middle">N</forename><surname>Vapnik</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1998">1998</date>
			<publisher>Wiley-Interscience</publisher>
			<biblScope unit="page">12</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Spoof detection schemes</title>
		<author>
			<persName><forename type="first">K</forename><surname>Nixon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Aimale</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Rowe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Handbook of biometrics</title>
		<imprint>
			<biblScope unit="page" from="403" to="423" />
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Non-intrusive liveness detection by face images</title>
		<author>
			<persName><forename type="first">K</forename><surname>Kollreider</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Fronthaler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Bigun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Image Vis. Comput</title>
		<imprint>
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="page" from="233" to="244" />
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">A liveness detection method for face recognition based on optical flow field</title>
		<author>
			<persName><forename type="first">W</forename><surname>Bao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Jiang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">on Image Analysis and Signal Processing</title>
		<imprint>
			<date type="published" when="2009">2009. 2009</date>
			<biblScope unit="page" from="233" to="236" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">Live face detection based on the analysis of Fourier spectra</title>
		<author>
			<persName><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">K</forename><surname>Jain</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2004">2004</date>
			<biblScope unit="page" from="296" to="303" />
		</imprint>
	</monogr>
	<note>Biometric technology for human identification</note>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Face liveness detection by learning multispectral reflectance distributions</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Yi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Lei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">Z</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Int. Conf. on Face and Gesture</title>
		<imprint>
			<biblScope unit="page" from="436" to="441" />
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">The Imaging Issue in an automatic face/ disguise detection system</title>
		<author>
			<persName><forename type="first">I</forename><surname>Pavlidis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Symosek</surname></persName>
		</author>
		<ptr target="http://dl.acm.org/citation.cfm?id=518905.795366" />
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Workshop on Computer Vision Beyond the Visible Spectrum: Methods and Applications (CVBVS 2000)</title>
		<meeting>IEEE Workshop on Computer Vision Beyond the Visible Spectrum: Methods and Applications (CVBVS 2000)</meeting>
		<imprint>
			<date type="published" when="2000">2000</date>
			<biblScope unit="page">15</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">TIR/VIS correlation for liveness detection in face recognition</title>
		<author>
			<persName><forename type="first">L</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Wu</surname></persName>
		</author>
		<ptr target="http://dl.acm.org/citation.cfm?id=2044575.2044590" />
	</analytic>
	<monogr>
		<title level="m">Proc. 14th Int. Conf. on Computer Analysis of Images and Patterns -Volume Part II. CAIP&apos;11</title>
		<meeting>14th Int. Conf. on Computer Analysis of Images and Patterns -Volume Part II. CAIP&apos;11</meeting>
		<imprint>
			<date type="published" when="2011">2011</date>
			<biblScope unit="page" from="114" to="121" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Is physics-based liveness detection truly possible with a single image?</title>
		<author>
			<persName><forename type="first">J</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">T</forename><surname>Ng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><forename type="middle">Q</forename><surname>Shi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Int. Symp. on Circuits and Systems (ISCAS)</title>
		<imprint>
			<date type="published" when="2010">2010</date>
			<biblScope unit="page" from="3425" to="3428" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Single-view recaptured image detection based on physics-based features</title>
		<author>
			<persName><forename type="first">X</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">T</forename><surname>Ng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Qiu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">F</forename><surname>Chang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Int. Conf. on Multimedia &amp; Expo (ICME)</title>
		<imprint>
			<date type="published" when="2010">2010</date>
			<biblScope unit="page" from="1469" to="1474" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Characterization of electrophotographic print artifacts: banding, jitter, and ghosting</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">H</forename><surname>Eid</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">N</forename><surname>Ahmed</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">E</forename><surname>Cooper</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">E</forename><surname>Rippetoe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Image Process</title>
		<imprint>
			<biblScope unit="page" from="1313" to="1326" />
			<date type="published" when="2011">2011, 20</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Multiresolution gray-scale and rotation invariant texture classification with local binary patterns</title>
		<author>
			<persName><forename type="first">T</forename><surname>Ojala</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Pietika ¨inen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">¨</forename><surname>Ma ¨enpa ¨a</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename></persName>
		</author>
		<ptr target="http://portal.acm.org/citation.cfm?id=628329.628808" />
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Pattern Anal. Mach. Intell</title>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="page" from="971" to="987" />
			<date type="published" when="2002">2002</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">LIBLINEAR: a library for large linear classification</title>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">E</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">W</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">J</forename><surname>Hsieh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><forename type="middle">R</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">J</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Mach. Learn. Res</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="page" from="1871" to="1874" />
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">VLFeat: an open and portable library of computer vision algorithms</title>
		<author>
			<persName><forename type="first">A</forename><surname>Vedaldi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Fulkerson</surname></persName>
		</author>
		<ptr target="http://www.vlfeat.org/" />
		<imprint>
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Large scale evaluation of multimodal biometric authentication using state-of-the-art systems</title>
		<author>
			<persName><forename type="first">R</forename><surname>Snelick</surname></persName>
		</author>
		<author>
			<persName><forename type="first">U</forename><surname>Uludag</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Mink</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Indovina</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Jain</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Pattern Anal. Mach. Intell</title>
		<imprint>
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="page" from="450" to="455" />
			<date type="published" when="2005">2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">From few to many: illumination cone models for face recognition under variable lighting and pose</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">S</forename><surname>Georghiades</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">N</forename><surname>Belhumeur</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">J</forename><surname>Kriegman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Pattern Anal. Mach. Intell</title>
		<imprint>
			<biblScope unit="volume">23</biblScope>
			<biblScope unit="page" from="643" to="660" />
			<date type="published" when="2001">2001</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Rapid object detection using a boosted cascade of simple features</title>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">A</forename><surname>Viola</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">J</forename><surname>Jones</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conf. on Computer Vision and Pattern Recognition</title>
		<meeting>IEEE Conf. on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2001">2001</date>
			<biblScope unit="page" from="511" to="518" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">2D cascaded adaboost for eye localization</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Niu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Shan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Gao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. 18th Int. Conf. on Pattern Recognition</title>
		<meeting>18th Int. Conf. on Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Combining face and iris biometrics for identity verification</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">K</forename><surname>Jain</surname></persName>
		</author>
		<ptr target="http://dl.acm.org/citation.cfm?id=1762222.1762327" />
	</analytic>
	<monogr>
		<title level="m">Proc. fourth Int. Conf. on Audio-and Video-Based Biometric Person Authentication. AVBPA&apos;03</title>
		<meeting>fourth Int. Conf. on Audio-and Video-Based Biometric Person Authentication. AVBPA&apos;03</meeting>
		<imprint>
			<date type="published" when="2003">2003</date>
			<biblScope unit="page" from="805" to="813" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
