<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">SpeakerBeam: Speaker aware neural network for target speaker extraction in speech mixtures</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<title level="a" type="main">SpeakerBeam: Speaker aware neural network for target speaker extraction in speech mixtures</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">0D623D0E552931CF3B48E992EF9D5A01</idno>
					<idno type="DOI">10.1109/JSTSP.2019.2922820</idno>
					<note type="submission">This article has been accepted for publication in a future issue of this journal, but has not been fully edited. Content may change prior to final publication. Citation information: DOI 10.1109/JSTSP.2019.2922820, IEEE Journal of Selected Topics in Signal Processing JOURNAL OF SELECTED TOPICS IN SIGNAL PROCESSING 1 This article has been accepted for publication in a future issue of this journal, but has not been fully edited. Content may change prior to final publication. Citation information: DOI 10.1109/JSTSP.2019.2922820, IEEE Journal of Selected Topics in Signal Processing This article has been accepted for publication in a future issue of this journal, but has not been fully edited. Content may change prior to final publication. Citation information: DOI 10.1109/JSTSP.2019.2922820, IEEE Journal of Selected Topics in Signal Processing JOURNAL OF SELECTED TOPICS IN SIGNAL PROCESSING 12</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.3" ident="GROBID" when="2023-07-28T05:03+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>speaker extraction</term>
					<term>speaker-aware neural network</term>
					<term>multi-speaker speech recognition</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>The processing of speech corrupted by interfering overlapping speakers is one of the challenging problems as regards today's automatic speech recognition systems. Recently, approaches based on deep learning have made great progress towards solving this problem. Most of these approaches tackle the problem as speech separation, i.e. they blindly recover all the speakers from the mixture. In some scenarios, such as smart personal devices, we may however be interested in recovering one target speaker from a mixture. In this paper, we introduce SpeakerBeam, a method for extracting a target speaker from the mixture based on an adaptation utterance spoken by the target speaker. Formulating the problem as speaker extraction avoids certain issues such as label permutation and the need to determine the number of speakers in the mixture. With SpeakerBeam, we jointly learn to extract a representation from the adaptation utterance characterizing the target speaker and to use this representation to extract the speaker. We explore several ways to do this, mostly inspired by speaker adaptation in acoustic models for automatic speech recognition. We evaluate the performance on the widely used WSJ0-2mix and WSJ0-3mix datasets, and these datasets modified with more noise or more realistic overlapping patterns. We further analyze the learned behavior by exploring the speaker representations and assessing the effect of the length of the adaptation data. The results show the benefit of including speaker information in the processing and the effectiveness of the proposed method.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>I. INTRODUCTION</head><p>Automatic speech recognition systems are now becoming widely deployed in real applications, which increases the need for robustness in adverse conditions. One particularly challenging problem, commonly occurring in spontaneous conversations and human-machine communication, is speech corrupted by interfering speakers. This type of interference has shown to be very difficult to reduce and greatly deteriorates the quality of speech transcriptions. Most of the research dealing with overlapping speech has focused on speech separation <ref type="bibr" target="#b0">[1]</ref>, <ref type="bibr" target="#b1">[2]</ref>, where all the source signals are recovered from the observed mixture signal. This problem has been studied in the past using methods, such as Computational auditory scene analysis [3], <ref type="bibr" target="#b3">[4]</ref>, Non-negative matrix factorization <ref type="bibr" target="#b4">[5]</ref>, <ref type="bibr" target="#b5">[6]</ref> and Factorial Hidden Markov models <ref type="bibr" target="#b6">[7]</ref>, <ref type="bibr" target="#b7">[8]</ref>, and was greatly advanced recently thanks to deep learning based approaches K. Zmolikova, L. Burget and J. Cernocky are with Brno University of Technology, Speech@FIT, Czech Republic.</p><p>M. Delcroix, K. Kinoshita, T. Ochiai, T. Nakatani are with NTT Communications Science Laboratories, NTT Corporation, Kyoto, Japan. <ref type="bibr" target="#b1">[2]</ref>, <ref type="bibr" target="#b8">[9]</ref>, <ref type="bibr" target="#b9">[10]</ref>, <ref type="bibr" target="#b10">[11]</ref>. However, in some practical situations, such as smart personal devices, we may be interested in recovering a single target speaker while reducing noise and the effect of interfering speakers <ref type="bibr" target="#b11">[12]</ref>, <ref type="bibr" target="#b12">[13]</ref>, <ref type="bibr" target="#b13">[14]</ref>, <ref type="bibr" target="#b14">[15]</ref>. We call this problem target speaker extraction. In contrast to speech separation, extracting the target speaker avoids problems such as label permutation, dependence on the number of speakers and the speaker-tracing problem (see Section II-A for further discussion).</p><p>Most previous studies aiming to extract the target speaker <ref type="bibr" target="#b15">[16]</ref>, <ref type="bibr" target="#b16">[17]</ref>, <ref type="bibr" target="#b17">[18]</ref> realized their aim by training a neural network on the target speaker data only, thus creating a model specifically designed to extract this particular speaker. The models are trained either in a speaker-pair-dependent mode, where both the target speaker and the interferer are observed in the training data, or in a target-dependent mode where the model can generalize to unseen interfering speakers. Both of these modes rely on the assumption of having substantial amount of data from the target speaker and do not allow the extraction of a speaker that was unseen during the training.</p><p>In this work, we follow the idea of target speaker extraction using a neural network, but rather than using a specialized model for a particular target speaker, we train a speaker independent model and inform it about the target speaker using additional speaker information. The network can use this information to focus on the target speaker, considering all the others as interference. We call this approach SpeakerBeam. The neural network in SpeakerBeam can be trained on a variety of speakers and employed to extract speakers unseen during the training. The additional speaker information determining the target speaker is obtained from an adaptation utterance spoken by the target speaker. In practice, this adaptation utterance could be obtained, for example, from part of a conversation without any overlap or pre-recorded by the target user on his/her personal device.</p><p>We explore different approaches for utilizing the information from adaptation utterances to cause the neural network to extract the target speaker. Most of these approaches are inspired by speaker adaptation of acoustic models. There are two main problems to be solved: a) how to use the speaker information to modify the behavior of the neural network, and b) how to extract the speaker information from the adaptation utterance. For the first problem, we look into three different methods: input bias adaptation <ref type="bibr" target="#b18">[19]</ref>, <ref type="bibr" target="#b19">[20]</ref>, <ref type="bibr" target="#b20">[21]</ref>, factorized layer <ref type="bibr" target="#b21">[22]</ref> and scaled activations <ref type="bibr" target="#b22">[23]</ref>. To extract the speaker information from the adaptation utterance, we can either use speaker representations that have been widely used for speaker identification tasks, such as i-vectors <ref type="bibr" target="#b23">[24]</ref> or jointly learn the speaker representation using sequence summarization <ref type="bibr" target="#b20">[21]</ref> or its modification with a simple attention mechanism.</p><p>In this paper, we first explain how this work relates to recent speech separation and extraction methods and our previous work (Section II). Then, we describe the proposed SpeakerBeam method and its variants (Section III). Section IV describes the integration of the method with multichannel processing and an automatic speech recognizer. Sections V and VI outline the datasets used and the experimental setup. Finally, the results are reported in Section VII and further analysis is provided in Section VIII.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>II. RELATIONSHIP TO PREVIOUS WORK</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Related speech separation work</head><p>Most recent work on the neural network processing of overlapped speech tackles the problem from a speech separation perspective, i.e. recovering all the sources from the given mixture. Compared with the separation of speech and non-speech signals (e.g. speech-noise or speech-music mixtures), where the individual sources have inherently different characteristics, speech-speech separation gives rise to several problems that require more specialized approaches. To introduce these problems, let us consider a simple approach, where the neural network processes a mixture signal and produces all the source signals as individual outputs. This approach suffers from the following problems:</p><p>1) dependence on the number of speakers -the architecture of the neural network inherently limits the number of speakers in the mixture, that can be processed. 2) label-permutation problem -the correspondence between outputs of the network and the speakers is arbitrary, therefore there are multiple possible correct outputs of the network where the speaker order varies. This makes it difficult to define the targets for the network and to compute the error function during training. 3) speaker-tracing problem -when processing a mixture with a network frame-by-frame or block-by-block, the order of the speakers on the output may change arbitrarily and proper alignment across the frames or blocks needs to be ensured. The two main approaches that address neural network based speech separation are Deep clustering (DC) and Permutation Invariant Training (PIT). In DC <ref type="bibr" target="#b9">[10]</ref>, <ref type="bibr" target="#b24">[25]</ref> and its variants <ref type="bibr" target="#b25">[26]</ref>, a neural network is used to compute embeddings for all timefrequency bins. These embeddings can be then clustered into group time-frequency bins corresponding to the same speaker. This solves the label-permutation problem as the estimated embeddings are ignorant as regards the order of the sources. The architecture of such a network is also independent of the number of speakers, although this number must be determined during the clustering step.</p><p>In PIT <ref type="bibr" target="#b10">[11]</ref>, <ref type="bibr" target="#b26">[27]</ref>, the neural network outputs estimations of all source signals. The main idea is to solve the label permutation problem by finding the permutation of the estimated sources on the output of the network that best matches the desired targets. Kolbaek et al. <ref type="bibr" target="#b26">[27]</ref> have also shown that the same network can be used to process mixtures with different numbers of speakers as long as we can define a maximum, which can be a reasonable assumption in many scenarios. The objective of PIT is more closely related to the actual separation task than in DC and can be more easily combined with the joint training of e.g. an ASR system.</p><p>For the speaker tracing problem, both the DC and PIT methods rely on the ability of a recurrent architecture to keep its outputs consistent over time. In DC, the network should keep the embeddings for the same speaker in the same part of the embedding space, and in PIT, it should keep assigning the same speaker to the same output of the network. This has proven to work well in cases where the mixture is short and fully overlapped, but can cause problems for longer recordings or more complicated overlapping patterns, which naturally occur in real conversations.</p><p>The proposed SpeakerBeam method does not suffer from problems 1) and 2) as the neural network predicts the speech of the target speaker only. Additionally, it also solves the speaker tracing problem as the explicit speaker information enables the neural network to follow the same speaker over different frames or processing segments.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Relationship with our previous work</head><p>We gradually built and refined the SpeakerBeam approach over several studies <ref type="bibr" target="#b27">[28]</ref>, <ref type="bibr" target="#b28">[29]</ref>, <ref type="bibr" target="#b29">[30]</ref>, <ref type="bibr" target="#b30">[31]</ref>. In this section, we clarify the relationship between this work and our previous research.</p><p>In <ref type="bibr" target="#b27">[28]</ref>, we first introduced the speaker-aware extraction scheme as part of a multi-channel system and experimented with different speaker-dependent neural network architectures. The work in <ref type="bibr" target="#b27">[28]</ref> focused mainly on a closed-speaker-set case and evaluated a factorized layer scheme (see Section III) as the most suitable method. We later extended this method with sequence summarization in <ref type="bibr" target="#b28">[29]</ref> to improve the performance in an open-speaker set scenario. Therein, we also evaluated SpeakerBeam as the front-end of an automatic speech recognition system. In <ref type="bibr" target="#b29">[30]</ref>, the automatic speech recognition performance was further improved by exploring the joint training of the SpeakerBeam front-end with an ASR system. While these studies <ref type="bibr" target="#b27">[28]</ref>, <ref type="bibr" target="#b28">[29]</ref>, <ref type="bibr" target="#b29">[30]</ref> focused on a multichannel case, in <ref type="bibr" target="#b30">[31]</ref>, we investigated the ASR performance in a singlechannel setting.</p><p>This paper builds upon the previous ones, summarizes the findings, and brings new modifications, evaluation and analysis. In particular, we provide a thorough evaluation of the single-channel scenario and different variants of SpeakerBeam on standard WSJ0-2mix and WSJ0-3mix datasets. We also create new WSJ0-2mix-long and WSJ0-2mix-noisy datasets to explore the effect of more natural overlapping patterns and a higher amount of noise on the results. Furthermore, we experiment with a combination of SpeakerBeam and DC, leading to improved performance. We finally provide an analysis of the learned embeddings and behavior with different lengths of adaptation utterance. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Related speaker extraction work</head><p>After our proposal of SpeakerBeam in <ref type="bibr" target="#b27">[28]</ref>, several other studies followed the idea of extracting a target speaker using an adaptation utterance <ref type="bibr" target="#b14">[15]</ref>, <ref type="bibr" target="#b31">[32]</ref>, <ref type="bibr" target="#b32">[33]</ref>. The authors of <ref type="bibr" target="#b14">[15]</ref> built upon deep attractor networks <ref type="bibr" target="#b25">[26]</ref> and suggested using the adaptation utterance to map the time-frequency points of the mixture into a canonical embedding space, where the embeddings corresponding to the target speaker are pulled together. The results show effectiveness even for very short adaptation utterances, however, the approach remains to be tested on a publicly available dataset or under more challenging conditions.</p><p>The work reported in <ref type="bibr" target="#b31">[32]</ref> realized target speech extraction by combining speech separation and speaker identification. The authors proposed making use of embeddings in deep attractor networks to identify the target speaker in the extracted signals. This approach cannot exploit auxiliary information about the target speaker to improve the separation process. Moreover, it requires an additional module for speaker selection that may introduce speaker identification errors.</p><p>The method introduced in <ref type="bibr" target="#b32">[33]</ref> proposes concatenating a dvector <ref type="bibr" target="#b33">[34]</ref> extracted from the adaptation utterance with one of the layers of the neural network to achieve the target speaker extraction. A similar way of using the speaker representation did not work well in our experiments (see 'input-bias' method in Section VII-A). This may possibly be due to the difference in the experimental settings, i.e. in <ref type="bibr" target="#b32">[33]</ref>, the target speaker was notably dominant over the interference (10.1 dB signalto-distortion ratio), while in our experiments, the target and interference are equally strong on average (0.2 dB SDR).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>III. PROPOSED SPEAKERBEAM METHOD</head><p>In this section, we formally define the problem of speaker extraction, introduce the notation we use and describe the proposed SpeakerBeam method. Figure <ref type="figure" target="#fig_0">1</ref> shows the overall scheme of the mixing model and the extraction.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Problem definition</head><p>The problem of speaker extraction is to isolate the speech of a target speaker from an observed mixture of multiple overlapping speakers and optionally an additional noise. We assume a mixing model:</p><formula xml:id="formula_0">y (m) [n] = s (m) 0 [n] + I-1 i=1 s (m) i [n] + v (m) [n],<label>(1)</label></formula><p>where n is the discrete time index, I is the number of speakers in the mixture, s</p><p>[n] for i = 0, . . . , I -1 is the speech signal of the ith speaker as captured by microphone m with i = 0 being the target speaker, v (m) [n] is the additional noise and y (m) [n] is the observed mixture.</p><p>In this work, we perform the extraction in the short-time Fourier transform (STFT) domain, where we can model the mixing process as</p><formula xml:id="formula_2">Y (m) [t, f ] = S (m) 0 [t, f ] + I-1 i=1 S (m) i [t, f ] + V (m) [t, f ]. (2)</formula><p>Here, [t, f ] are the indexes corresponding to the time frame and the frequency bin and Y , S i , V are the STFT-domain counterparts of y, s i , v, respectively. We will use the notation Y, S i , V for the T ×F matrices comprising all time-frequency points</p><formula xml:id="formula_3">Y [t, f ], S i [t, f ], V [t, f ],</formula><p>respectively, with T being the number of time frames and F the number of frequency bins in the STFT representation of given signal. In the remainder of this section, we will focus on a single channel case (in this case, index (m) can be omitted). A multi-channel extension of SpeakerBeam will be addressed in Section IV.</p><p>Our method extracts the target speaker from the mixture, using additional information about the target speaker in the form of an adaptation utterance. This utterance will be denoted a[n] in the time domain, A[t, f ] in the STFT domain and A for a T a × F matrix comprising all time-frequency points, where T a is the number of frames in the adaptation utterance. The adaptation utterance a[n] contains speech from the same speaker as s 0 [n], however it is always a different utterance from</p><formula xml:id="formula_4">s 0 [n].</formula><p>The extraction is performed by means of a neural network that takes the mixture as an input, the adaptation utterance as</p><formula xml:id="formula_5">λ (bias) concat L 0 (X 0 ; ψ 0 ) σ 0 X 0 ≡ |Y| L 1 (X 1 ; ψ 1 ) σ 1 X 1 L 2 (X 2 ; ψ 2 ) σ 2 X 2 M (a) Input bias adaptation. L 0 (X 0 ; ψ 0 ) σ 0 X 0 ≡ |Y| L 1 (X 1 ; ψ (1) 1 ) σ 1 X 1 L 2 (X 2 ; ψ 2 ) σ 2 X 2 M L 1 (X 1 ; ψ (0) 1 ) L 1 (X 1 ; ψ (2) 1 ) • • • λ (f act) (b) Factorized layer. L 0 (X 0 ; ψ 0 ) σ 0 X 0 ≡ |Y| L 1 (X 1 ; ψ 1 ) • σ 1 X 1 L 2 (X 2 ; ψ 2 ) σ 2 X 2 M λ (act) (c) Scaled activations.</formula><p>Fig. <ref type="figure">2</ref>. Three different methods of informing the neural network about the target speaker. The red box represents the speaker information λ derived from the adaptation utterance. • denotes vector-scalar multiplication, • denotes element-wise vector-vector multiplication.</p><p>auxiliary information and provides a mask that can be used to obtain an estimate of the target speech:</p><formula xml:id="formula_6">M = g(|Y|, |A|),<label>(3)</label></formula><formula xml:id="formula_7">∧ S 0 = M Y,<label>(4)</label></formula><p>where g is the transformation carried out by the mask estimation neural network, |•| denotes the magnitude of a given STFT signal, M is the estimated mask, denotes element-wise multiplication and ∧ S 0 is the estimated target-speaker STFT signal. In general, it would be possible to process directly the complex spectrum of the signals, but in this work, we limit ourselves to using the magnitudes only as in most of the related studies <ref type="bibr" target="#b9">[10]</ref>, <ref type="bibr" target="#b25">[26]</ref>, <ref type="bibr" target="#b10">[11]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Informing the network</head><p>Modifying the behavior of a neural network using additional speaker information is a task that has been heavily explored for speaker adaptation in acoustic models. The methods applied in our work are thus inspired by previous findings in this field. We explore three different ways of informing the networkinput bias adaptation, a factorized layer and scaled activations, as depicted in Figure <ref type="figure">2</ref>. Please note, that the figure depicts a rough schematic view of the network, and a more precise description of the layers and the architecture will be given in the System configuration sub-section in Section VI. All three methods make use of the speaker information λ (red box in Figure <ref type="figure">2</ref>). In III-C, we will specify how λ is obtained from the adaptation utterance.</p><p>1) Input bias adaptation: The most straightforward technique, commonly used in acoustic modeling, is to append the speaker information to the features on the input of the neural network <ref type="bibr" target="#b18">[19]</ref>, <ref type="bibr" target="#b19">[20]</ref>, <ref type="bibr" target="#b20">[21]</ref>. This effectively performs the adaptation of the biases in the first layer of the network <ref type="bibr" target="#b21">[22]</ref>. We can express the neural network processing as</p><formula xml:id="formula_8">X 1 = σ 0 (L 0 ([X 0 , λ (bias) ]; ψ 0 )),<label>(5)</label></formula><formula xml:id="formula_9">X k+1 = σ k (L k (X k ; ψ k )) for k ≥ 1,<label>(6)</label></formula><p>where X k is the input to the kth layer, L k (X k , ψ k ) is the transformation computed by the kth layer parameterized by ψ k , and σ k is an activation function. For example, with fully connected layers, ψ = {W, b} and L(X, ψ) = Wx + b, where W is a weight matrix and b is a bias vector.</p><p>2) Factorized layer: Previous literature has shown, that a more powerful adaptation than simply adapting the input bias can be achieved by modifying all the parameters in one of the layers of the network. In a method introduced in <ref type="bibr" target="#b34">[35]</ref>, one of the layers of the network is factorized into multiple sublayers, which are then combined using weights derived from the speaker information. Following the previous notation and denoting the index of the factorized layer q and the number of sub-layers as J, the network processing is defined as</p><formula xml:id="formula_10">X k+1 =    σ k (L k (X k ; ψ k )) for k = q, σ k ( J-1 j=0 λ (fact) j L k (X k ; ψ (j) k )) for k = q.<label>(7)</label></formula><p>The network thus learns common basis for all the speakers, which then can be combined with different weights λ (fact) to make the network extract different speakers. The size of vector λ (fact) is determined by the number of factorized sub-layers J, which is chosen as a hyper-parameter.</p><p>3) Scaled activations: An alternative speaker adaptation method is introduced in <ref type="bibr" target="#b22">[23]</ref>, <ref type="bibr" target="#b35">[36]</ref>, where the output of each unit in one of the layers of the network is scaled by weights derived from the speaker information. This method is similar to the factorized layer approach, however it is computationally simpler. In this case, the processing performed by the neural network is:</p><formula xml:id="formula_11">X k+1 =    σ k (L k (X k ; ψ k )) for k = q, σ k (λ (act) L k (X k ; ψ k )) for k = q. (<label>8</label></formula><formula xml:id="formula_12">)</formula><p>Here the size of vector λ (act) is determined by the size of the adaptive layer, rather than the number of factorized sublayers as in the previous approach. Note that although the method introduced here follows the same idea as in <ref type="bibr" target="#b22">[23]</ref>, <ref type="bibr" target="#b35">[36]</ref>,</p><p>it differs slightly in how the scaling weights are obtained and where exactly they are applied.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Obtaining the speaker information</head><p>In this section, we describe methods for extracting speaker information λ from an adaptation utterance, which is then used to inform the network, as described in the previous section. We explore three different methods -i-vector based extraction, a sequence summarizing network and its extension using simple attention.</p><p>1) I-vectors: A common way to represent speaker-related information in speech data is the i-vector, which has been used extensively for e.g. speaker recognition <ref type="bibr" target="#b23">[24]</ref>, speaker adaptation <ref type="bibr" target="#b36">[37]</ref>, <ref type="bibr" target="#b18">[19]</ref> or speaker diarization <ref type="bibr" target="#b37">[38]</ref>. I-vectors are fixed-length low-dimensional representations of speech segments of variable length. For more information on i-vector extraction, we refer the reader to <ref type="bibr" target="#b23">[24]</ref>, <ref type="bibr" target="#b38">[39]</ref>. In our work, we extract the i-vector from the adaptation utterance and postprocess it with an auxiliary network to obtain the vector λ used in one of the three schemes presented in the previous section.</p><p>2) Sequence summarizing network: Although i-vector extraction is designed to preserve speaker variability, it uses a separate step, which is not optimized for the target speaker extraction task we are addressing. Therefore, some information important for speaker extraction may be lost. The second method we propose applies the adaptation utterance directly to the input of the auxiliary network. To convert from framewise features to an utterance-wise vector, we employ average pooling after the last layer in the auxiliary network. This way, the extraction of speaker information from the adaptation utterance may be learned jointly with the speaker extraction:</p><formula xml:id="formula_13">λt = z(|A|),<label>(9)</label></formula><formula xml:id="formula_14">λ = 1 T a λt , (<label>10</label></formula><formula xml:id="formula_15">)</formula><p>where z is the transformation performed by the auxiliary neural network, λt is the frame-wise vector extracted by the auxiliary network for frame t, which is then averaged over the T frames in the adaptation utterance to obtain the final λ.</p><p>3) Sequence summarizing network with attention: The average pooling at the end of the auxiliary network weighs all frames equally. This may be detrimental when some of the frames are silence or for example, corrupted by noise. To make the scheme more flexible, we extend it with a simple attention mechanism. Here, the output of the auxiliary network is extended with one value, āt . This predicted value for each frame is then used, after a softmax operation, to weigh the contribution of the individual frames to the averaging operation:</p><formula xml:id="formula_16">( λt , āt ) = z(|A|),<label>(11)</label></formula><formula xml:id="formula_17">a = softmax(ā),<label>(12)</label></formula><formula xml:id="formula_18">λ = a t λt ,<label>(13)</label></formula><p>where ā = [ā 1 , . . . , āTa ] denotes the attention energies (before the softmax), and a = [a 1 , . . . , a Ta ] is the final attention vector, after the softmax normalization.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D. Training objective</head><p>The neural network in SpeakerBeam estimates a T-F mask corresponding to the target speech. Different choices for the objective function for training the mask estimation networks have been previously explored in the literature. Here, we follow the findings in <ref type="bibr" target="#b39">[40]</ref>, which show that a good choice for the objective function is the mean square error between the magnitude of the STFT of the desired speech and the magnitude of the STFT of the observation, masked by the estimated mask. In addition, we also weigh the different timefrequency points using the phase differences between the clean and observed signals as suggested in <ref type="bibr" target="#b26">[27]</ref>. This leads to an objective function with the form:</p><formula xml:id="formula_19">J spkbeam = ||M |Y|-|S 0 | max(0, cos(θ y -θ s0 ))|| 2 , (<label>14</label></formula><formula xml:id="formula_20">)</formula><p>where θ y and θ s0 are the T × F matrices of the phases of observed speech and target speaker speech, respectively.</p><p>We also explore the multi-task training of SpeakerBeam together with the Deep clustering method, in a similar fashion to that employed for Chimera networks <ref type="bibr" target="#b40">[41]</ref>, where the DC objective serves as a regularizer in a singing voice separation task. In this case, the neural network has two output layers, one predicting a mask for SpeakerBeam and the other predicting embeddings for Deep clustering</p><formula xml:id="formula_21">(M, E) = g(|Y|, |A|), (<label>15</label></formula><formula xml:id="formula_22">)</formula><p>where E is the matrix of the embeddings. The objective function of the training is then computed as the average of the SpeakerBeam and Deep clustering objective functions</p><formula xml:id="formula_23">J spkbeam+dc = αJ spkbeam + βJ dc ,<label>(16)</label></formula><p>where α, β are interpolation weights. In this paper, we set α and β so that both objectives are in approximately the same range (α = 0.5, β = 0.5e -5 ). For details on the computation of the objective function for deep clustering J dc from the estimated embeddings E, please refer to <ref type="bibr" target="#b9">[10]</ref>, <ref type="bibr" target="#b24">[25]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>IV. INTEGRATION WITH BEAMFORMING AND ASR</head><p>In this section, we describe how to integrate the Speaker-Beam method with beamforming and an ASR-level objective criterion.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Multi-channel extraction</head><p>Following the single-channel procedure, the neural network estimates a mask corresponding to the target speech in the mixture. The mask is estimated for each channel separately and the overall mask is obtained as a median across all the channels. The resulting mask is then used to accumulate statistics about the target signal and compute statistically optimal beamforming filters. Finally, to obtain the estimate of the target speech, the filters are applied to the multi-channel signal. The procedure for estimating the statistics of the target signal can be described as</p><formula xml:id="formula_24">M (m) = g(|Y (m) |, |A|),<label>(17)</label></formula><formula xml:id="formula_25">M = median(M (m) ),<label>(18)</label></formula><formula xml:id="formula_26">Φ SS [f ] = t M [t, f ]y[t, f ]y H [t, f ] t M [t, f ] ,<label>(19)</label></formula><formula xml:id="formula_27">Φ N N [f ] = t (1 -M [t, f ])y[t, f ]y H [t, f ] t (1 -M [t, f ]) ,<label>(20)</label></formula><p>where M (m) is the estimated mask for channel m and Φ SS , Φ N N are the spatial co-variance matrices (SCM) corresponding to the target speech and interference, respectively.</p><formula xml:id="formula_28">y[t, f ] = [Y (1) [t, f ], . . . , Y (M ) [t, f ]</formula><p>] is a vector comprising the observed signal at time-frequency point [t, f ] for all microphones. Different beamforming filters, such as the Generalized Eigenvalue beamformer (GEV) <ref type="bibr" target="#b41">[42]</ref> or Minimum variance distortionless response (MVDR) beamformer <ref type="bibr" target="#b42">[43]</ref>, can then be computed using the estimated Φ SS , Φ N N . In this work, we use the GEV beamformer defined as</p><formula xml:id="formula_29">h GEV [f ] = arg max h h H [f ]Φ SS [f ]h[f ] h H [f ]Φ N N [f ]h[f ] , (<label>21</label></formula><formula xml:id="formula_30">)</formula><p>where h is a beamforming filter. The computed beamforming filters then can be used to estimate the target signal as</p><formula xml:id="formula_31">∧ S[t, f ] = h H [f ]y[t, f ].<label>(22)</label></formula><p>This procedure for neural network mask-based beamforming was proposed for speech denoising <ref type="bibr" target="#b43">[44]</ref>, <ref type="bibr" target="#b44">[45]</ref> and has been shown to be very effective. Estimating the mask by using the neural network from each channel separately ensures independence from microphone array configuration, and averaging across time and channels when computing the statistics provides robustness against small errors made by the network. Additionally, speech produced by the linear filtering process is better suited for processing by automatic speech recognition systems than signals produced by masking as in Eq. (4).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Joint training with ASR</head><p>For a case where SpeakerBeam is used in a chain with beamforming and the ASR acoustic model, we also explore the option of training it jointly with the acoustic model, using the cross-entropy between the estimated tied-state distribution and the true tied-state labels, J asr . To train the SpeakerBeam network, this objective is then back-propagated through the acoustic model, feature extraction and the beamforming process:</p><formula xml:id="formula_32">∂J asr ∂ψ = ∂J asr ∂ ∧ s fbank ∂ ∧ s fbank ∂ ∧ S ∂ ∧ S ∂M ∂M ∂ψ ,<label>(23)</label></formula><p>where ∧ s fbank are the features extracted from the estimated signal, ∧ S is the STFT of the estimated signal, M are the estimated masks and ψ is the vector of the parameters of the SpeakerBeam neural network. Most of the gradients can be computed using backpropagation through standard neural network blocks. For gradient ∂ ∧ S/∂M, we need to backpropagate through a GEV beamformer, in particular through complex eigenvalue decomposition. This step was thoroughly covered in <ref type="bibr" target="#b45">[46]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>V. DATASETS</head><p>For our evaluation, we chose the dataset introduced in <ref type="bibr" target="#b9">[10]</ref>, which has been used in many previous studies <ref type="bibr" target="#b9">[10]</ref>, <ref type="bibr" target="#b24">[25]</ref>, <ref type="bibr" target="#b25">[26]</ref>, <ref type="bibr" target="#b26">[27]</ref>. It consists of simulated mixtures based on utterances taken from the Wall Street Journal (WSJ0) corpus <ref type="bibr" target="#b46">[47]</ref>. For different experiments, we report results for four different versions of the dataset, namely WSJ0-2mix <ref type="bibr" target="#b9">[10]</ref> for singlechannel 2-speaker experiments, WSJ0-3mix <ref type="bibr" target="#b9">[10]</ref> for singlechannel 3-speaker experiments, WSJ0-2mix-MC <ref type="bibr" target="#b47">[48]</ref> for multichannel experiments and our own modification of WSJ0-2mix, WSJ0-2mix-long, which consists of single-channel 2-speaker mixtures with a longer duration and more complicated overlapping pattern and WSJ0-2mix-noisy, where we mixed additional noise into the mixtures. In the following, we describe these sets in detail. With all datasets, the adaptation utterances are randomly chosen. In evaluation set, for each mixture and each speaker in the mixture, we randomly choose one utterance from the same speaker, different than the utterance in the mixture, to be the adaptation utterance. For training set, for each mixture and each speaker, we randomly choose 100 adaptation utterances which we iterate through over the training epochs (the same adaptation utterance may be repeated). The choice for both evaluation and training is fixed for all experiments.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. WSJ0-2mix and WSJ0-3mix</head><p>The WSJ0-2mix <ref type="bibr" target="#b9">[10]</ref> contains mixtures of two speakers at signal-to-noise ratios between 0 dB and 5 dB. It consists of a training set, a cross validation set and an evaluation set of 30, 10 and 5 hours, respectively. For training and cross-validation sets, the mixed utterances were randomly selected from the si_tr_s, while for evaluation set, the utterances were taken from si_dt_05 and si_et_05 parts of WSJ0. In total, the training set contains 20000 mixtures from 101 speakers, the cross-validation set contains 5000 mixtures from the same 101 speakers and the evaluation set contains 3000 utterances from 18 speakers (unseen in the training). The WSJ0-3mix <ref type="bibr" target="#b24">[25]</ref> contains three-speaker mixtures analogous to WSJ0-2mix in terms of the amounts of data, number of speakers and WSJ0 sets from which the utterances are selected. All data are used at an 8 kHz sampling rate for consistency with previous studies. In experiments evaluating only signal-based measures, we use "min" versions of the datasets, where the mixture is cut to the length of the shortest utterance (for consistency with previous work). However, to be able to evaluate ASR accuracy in Sections VIII.D, VIII.E, VIII.F, we use the "max" version, where the shorter utterance in the mixture is padded with zeros.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. WSJ0-2mix-long</head><p>In addition to the original WSJ0-2mix, we created a dataset that aims to model more realistic overlapping conditions, similar to those occurring in natural conversations. The mixing 1932-4553 (c) 2019 IEEE. Personal use is permitted, but republication/redistribution requires IEEE permission. See http://www.ieee.org/publications_standards/publications/rights/index.html for more information. process followed the procedure used to create WSJ0-2mix, but for each of the speakers in the mixture, we selected 3 random utterances and placed them in sequence with random pauses in between (sampled uniformly in the 0-10 seconds range). This resulted in a dataset of mixtures with an average length of 45 seconds and an average overlap of 20%. Figure <ref type="figure" target="#fig_2">3</ref> shows a schematic comparison of the types of mixtures in WSJ0-2mix and WSJ0-2mix-long.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. WSJ0-2mix-MC</head><p>The WSJ0-2mix-MC <ref type="bibr" target="#b47">[48]</ref> dataset is a spatialized version of WSJ0-2mix. It is created by convolving the data with room impulse responses generated with the image method <ref type="bibr" target="#b48">[49]</ref>, <ref type="bibr" target="#b49">[50]</ref> to simulate an 8-channel microphone array. The room characteristics, speaker locations microphone array geometry are randomly generated -microphone array sizes range from 15 to 25 cm, T60 is drawn from 0.2-0.6 seconds. The average distance of a speaker from the array is 1.3 m with a 0.4 m standard deviation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D. WSJ0-2mix-noisy</head><p>The WSJ0-2mix-noisy dataset is equivalent to WSJ0-2mix, but with additional noises added to the mixtures. The noises were randomly selected from the CHiME-1 <ref type="bibr" target="#b50">[51]</ref> and CHiME-3 <ref type="bibr" target="#b51">[52]</ref> corpora. The CHiME-1 noises were recorded in a living room, and thus contain noises from typical domestic environments and often children's speech. The CHiME-3 noises are from four environments -buses, streets, cafes and pedestrian areas. We split the noises into training and test subsets and mixed them into the mixtures at SNRs of 20 dB to 0 dB (with respect to the mixture signal).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>VI. SYSTEM CONFIGURATION A. Speaker extraction neural network settings</head><p>In the experiments, we used two different neural network configurations. The first and smaller configuration, is used to compare the different techniques of informing the neural network about the speaker in the SpeakerBeam scheme. For all the following experiments, we used the larger configuration. The small configuration consisted of one BLSTM and three fully connected layers. All the layers used ReLU activations and batch normalization, except for the output layer with logistic sigmoid activation. The numbers of neurons in the layers were 300-1024-1024-257. The larger configuration consisted of 3 BLSTM layers, each followed by a linear projection layer and one linear output layer. The BLSTM layers had 512 units per direction and their output of dimensionality 1024  <ref type="formula" target="#formula_11">8</ref>), ( <ref type="formula" target="#formula_16">11</ref>)-( <ref type="formula" target="#formula_18">13</ref>).</p><p>(512 forward + 512 backward) was then transformed by the projection layer back to dimension 512. Each projection layer was followed by tanh nonlinearity. The larger configuration is depicted in Figure <ref type="figure">4</ref>. For both the factorized layer and scaled activations methods, the second layer was used as speaker adaptive layer. With the factorized layer, it was split into 30 sub-layers. For the input-bias method, the dimension of the appended speaker vector (extracted by the auxiliary network) was 100. The networks were trained with an Adam optimizer with a learning rate 1e -4. With the larger configuration, we did not use dropout, or batch normalization (in contrast with the smaller configuration where batch normalization was used). The network parameters were initialized using the Glorot initialization <ref type="bibr" target="#b52">[53]</ref>. The neural networks used for comparison with DC and PIT had the same architecture apart from the last layer, which was I × 257 for PIT (predicting masks for all the speakers) and D×257 for DC, where D = 30 is the embedding size.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Speaker information extraction settings</head><p>For i-vectors, we used a Kaldi i-vector extractor <ref type="bibr" target="#b53">[54]</ref>, trained on clean data. The Universal Background model we used consisted of 2048 Gaussians, and the i-vectors were 100dimensional. The i-vectors were computed per utterance.</p><p>As the auxiliary network, we used a network with 2 fully connected layers with 200 units per layer and ReLU activations. The output layer had linear activation. Its size was determined by the method used (see Figure <ref type="figure">2</ref>). The auxiliary network was trained jointly with the main network.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Beamforming settings</head><p>The beamforming was undertaken in the STFT domain with 20 ms windows and a 10 ms shift. We used a GEV beamformer as specified in IV-A. We regularized the noise spatial covariance matrix by adding 1e -3 to its diagonal to stabilize its inversion. The output signal was post-processed with a singlechannel post-filter <ref type="bibr" target="#b41">[42]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D. ASR settings</head><p>The input acoustic features were 40-dimensional log Mel filterbanks with a context window extension of 11 frames. The features were mean-normalized per utterance. For the acoustic model, we used a simple DNN with 5 fully connected hidden layers of 2048 units each and ReLU activation functions. For training, we used HMM tied-state alignments obtained from single-channel clean data using a GMM-HMM system.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>VII. EXPERIMENTS</head><p>This section provides an experimental evaluation of our approach. We compare the different methods used to inform the neural network about the target speaker and compare the performance with DC and PIT. We also explore the effectivewith mixtures containing three speakers. Then, we explore the performance with noisy and multichannel data. All the experiments are evaluated using the signal-to-distortion ratio (SDR) (as defined by <ref type="bibr" target="#b54">[55]</ref> and computed using <ref type="bibr" target="#b55">[56]</ref>) or the frequency-weighted signal-to-noise ratio (fw-SNR) computed using tools provided with the REVERB challenge <ref type="bibr" target="#b56">[57]</ref>. For automatic speech recognition experiments, we provide word error rates (WER).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Methods for informing the network</head><p>We compared different methods for informing the network about the target speaker and for extracting speaker information from the adaptation utterance as described in Sections III-B, III-C. These experiments were performed on the WSJ0-2mix dataset and used the smaller architecture of the network, as some methods do not scale well to a larger architecture. Table <ref type="table" target="#tab_0">I</ref> shows the results of the experiments.</p><p>We can observe that the input bias adaptation (input-bias) method performs rather poorly. In this case, the neural network does not learn to make proper use of the additional input features and keeps extracting all speakers present in the mixture. Although adapting the bias is a very successful approach to ASR acoustic models adaptation, for our task, it is arguably insufficiently powerful. We confirmed that the poor results are not a consequence of the smaller architecture of the network by repeating the input-bias + seqsum + att experiment with the larger architecture. This lead to -1.7 dB SDR degradation and -0.9 dB fw-SNR degradation.</p><p>The factorized layer (fact-layer) and scaled activations (scaled-act) both yield notably better extraction. The factorized layer approach tends to be slightly better, however, this is at the cost of increased computation and memory demands due to the many sub-layers. Therefore, for experiments described in the following sections, we used the scaled activations method, which constitutes a compromise between performance and computational cost.</p><p>Comparing the different methods of extracting the speaker information, we find that all three methods (ivec, seqsum, seqsum-att) lead to similar results. Training the speaker representation jointly with the network performs slightly better. Although the attention does not significantly improve the performance, we observed that the learned attention weights properly detect the non-silent parts of the adaptation utterance, which could be helpful when the adaptation utterances contain larger amounts of silence or noise. We therefore retained the attention mechanism for the following experiments.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Comparison with DC and PIT</head><p>To better evaluate the ability of SpeakerBeam to extract a target speaker, we compare its performance with Deep clustering and Permutation invariant training. For these experiments, we use the larger architecture, which is similar to settings used in previously published work on DC <ref type="bibr" target="#b9">[10]</ref>, <ref type="bibr" target="#b24">[25]</ref> and PIT <ref type="bibr" target="#b26">[27]</ref>. Note that with PIT and DC, the outputs are assigned to individual speakers in an oracle way, i.e. we choose an assignment that minimizes the error. With SpeakerBeam, we extract each of the speakers by providing the network with the speaker information, and the assignment is thus decided within the method. For a fairer comparison, we could consider coupling DC and PIT with a speaker identification module, possibly introducing additional errors. However, we adhere to using the oracle assignment to inspect the upper bound of such an extraction.</p><p>The first set of experiments compares performance for the 2mix dataset, which is commonly used to evaluate speech separation methods. Table <ref type="table" target="#tab_2">II</ref> shows that the results for this dataset are comparable, with Deep clustering performing slightly worse than the other two methods. Previously published work on DC <ref type="bibr" target="#b57">[58]</ref> achieved an SDR improvement of 9.4 dB with a similar network architecture. The main differences between <ref type="bibr" target="#b57">[58]</ref> and our setup are the optimization schedule and dropout regularization. Tuning these training settings could thus lead to improved accuracy.</p><p>The last experiment in the first part of Table <ref type="table" target="#tab_2">II</ref> shows that we can combine Deep clustering and SpeakerBeam. For this experiment, the SpeakerBeam architecture was extended by an additional output layer with a Deep clustering objective. This additional loss can serve to better train the network, while during evaluation, this output is discarded. The results show that the combination indeed helps with training, and the accuracy surpasses both SpeakerBeam and DC when used individually.</p><p>The second part of Table <ref type="table" target="#tab_2">II</ref> shows the performance with the WSJ0-2mix-long dataset with longer, less overlapped mixtures. For these mixtures, we used networks trained for the WSJ0-2mix and refined them using random 10-second excerpts from the WSJ0-2mix-long training data. The network could thus learn to process segments with no or a partial overlap. The results show that for these data, SpeakerBeam performs better. The degradation of DC and PIT compared with SpeakerBeam originates from the errors in tracing the speaker correctly over time; in some mixtures, the speakers on the output are switched in the middle of the utterance as shown in the example in Figure <ref type="figure" target="#fig_4">5</ref>. The outputs of DC and PIT would require further processing for tracing the speakers over the utterance, whereas SpeakerBeam does this jointly with the extraction. We can speculate that such behavior would appear more frequently with even longer mixtures or more speakers. Combining the DC and SpeakerBeam objectives during training again leads to a performance gain.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Three speaker experiments</head><p>Table <ref type="table" target="#tab_3">III</ref> shows the results of the extraction when applied to mixtures with 3 speakers. Since the neural network in SpeakerBeam is independent of the number of speakers in the mixture, we can train the same network for both 2-speaker and 3-speaker data.  The use of all the data for training leads to good performance with both 2 and 3 speaker mixtures. We performed the same set of experiments with DC and PIT. For DC, we used the oracle number of speakers during the clustering step. For PIT, we used a network with 3 outputs. For 2-speaker mixtures, during the training, we considered one of the outputs to be silent channel, and during testing, we kept only two outputs with the most energy. This follows the procedure described in <ref type="bibr" target="#b26">[27]</ref>. The results of both PIT and DC show similar trend as with SpeakerBeam, with even slightly worse generalization from network trained on 2 speakers to 3-speaker mixtures, especially with PIT.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D. Automatic speech recognition experiments</head><p>Table <ref type="table" target="#tab_0">IV</ref> shows the results we obtained for the automatic speech recognition of the extracted speech in the WSJ0-2mix dataset. Note that in these experiments, to allow for ASR evaluation, we used the max version of the dataset, where the length of the mixture corresponds to the length of the longer of the two utterances. By contrast, in the previous experiments the mixtures were cut to the length of the shorter utterance. The single speaker and mixtures results show the lower and upper bounds of the error. In all the experiments, the speech recognition system is trained on matched training data (singlespeaker, mixture or processed with SpeakerBeam, PIT or DC). We can see that SpeakerBeam significantly reduces the error compared with the original mixtures and can thus work as a front-end for an ASR system. Additionally, we also processed single speaker data with SpeakerBeam, to see how much the processing degrades the speech when there is no overlap. The result shows degradation from 12.2 % to 15.3 % WER. In this case, SpeakerBeam was not trained on single-speaker data, such training could possibly reduce the performance gap.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E. Multi-channel experiments</head><p>The experimental results in Table <ref type="table">V</ref> show the ASR performance with the multi-channel dataset WSJ0-2mix-MC. Note that these results cannot be directly compared with Table <ref type="table" target="#tab_0">IV</ref> as WSJ0-2mix-MC contains much more reverberation. Speaker-Beam is used here in combination with a GEV beamformer as described in Section IV-A. The use of the beamformer, which employs the SpeakerBeam output, improves the accuracy of the ASR system to 22.5% WER.</p><p>In addition, training the front-end jointly with the ASR using the cross-entropy objective further improves the results. In this case, the SpeakerBeam network is initialized with the network trained with the mask objective (Eq. ( <ref type="formula" target="#formula_19">14</ref>)) and the acoustic model with the network trained on the data enhanced by SpeakerBeam. Both networks are then jointly fine-tuned with the final ASR objective. The masks extracted with the front-end tend to be sparser when trained for the ASR objective which may be more convenient for further processing with beamforming.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>F. Noisy data</head><p>Tables VI,VII show the results of experiments on WSJ0-2mix-noisy. For all the experiments (PIT, DC, SpkBeam), the networks are trained on a training set, where each mixture contains additional noise with a randomly selected SNR as described in Section V-D. For testing, we created several copies of the test-set with various levels of noise ranging from 20 to 0 dB. We can see that even with quite high levels of unstationary noises, SpeakerBeam still succeeds in extracting the target speaker and improves both the signal-level measure and the ASR performance. For more results on noisy and reverberant mixtures, reader can also refer to our study in <ref type="bibr" target="#b58">[59]</ref> or our demo video <ref type="bibr" target="#b59">[60]</ref>. Note that although the presented experiments use noises recorded in real environments, the mixtures consist of fully overlapped speech, thus may not well reflect the nature of real conversations. The application of speech extraction methods in real conditions is an important issue which we plan to investigate in future work.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>VIII. ANALYSIS OF LEARNED BEHAVIOR</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Learned speaker embeddings</head><p>The auxiliary network in the SpeakerBeam architecture should convey information about the speaker from the adaptation utterance to the main network performing the speaker extraction. However, the auxiliary network is never trained with a direct speaker-related objective, only with the final objective of the speaker extraction. In this section, we explore how the learned vectors for the output of the auxiliary network capture the speaker information. Figure <ref type="figure">6</ref> shows the embeddings obtained from the adaptation utterances in test data, projected into two dimensions by means of t-SNE <ref type="bibr" target="#b60">[61]</ref>. We can see that the vectors form 18 clusters corresponding to the 18 speakers in the test data. Note that there is no overlap between the speakers in the training and evaluation sets. The auxiliary network thus seems to generalize well to unseen speakers. The same conclusions can also be drawn from Figure <ref type="figure">7</ref>, which shows the pair-wise Euclidean distances of the embeddings. Apart from the distinct speaker clusters, we can also see two main categories of speakers corresponding to males and females; the gender represents important variability in the embedding space.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Analysis of performance per speaker</head><p>The speaker characteristics are arguably a big factor in the performance of speaker extraction. In this Section, we inspect more closely the accuracy of the method for different speakers. First, we examine whether the accuracy varies greatly for different target speakers in the dataset. We used the WSJ0-2mix dataset and the larger neural network architecture for this analysis (corresponds to the 9.7 dB improvement in Table <ref type="table" target="#tab_2">II</ref>). Figure <ref type="figure" target="#fig_7">8</ref> shows that the mean SDR improvement does not vary very significantly for different target speakers, with a minimum of 8.0 dB mean SDR improvement for speaker '423' and a maximum of 11.2 dB mean SDR improvement for speaker '442'. A greater variation can be observed in the results, if we consider the impact of the combination of two speakers in the mixture. In Figure <ref type="figure" target="#fig_11">9</ref>, we show the mean SDR improvements for different combinations of target and interfering speakers for SpeakerBeam, PIT and DC. Again, we can see two main groups of speakers corresponding to gender. Mixtures of samegender speakers tend to be much more difficult to separate. Overall, the mean SDR improvement on same-gender mixtures is 7.2 dB, while for different-gender mixtures, it is 11.9 dB (For PIT, the SDR improvements are 6.3 dB and 11.8 dB and for DC, 5.9 dB and 10.9 dB for same-gender and differentgender mixtures respectively). We can see a few speaker pairs where the method is unable to differentiate between the speakers sufficiently well and the improvements are close to zero. By comparison with Figure <ref type="figure">7</ref>, these correspond to cases where the extracted speaker embeddings are very similar. We believe that the ability to differentiate between these speakers would improve by training SpeakerBeam with a larger speaker variability in the training set (the WSJ0-2mix dataset we used comprises 101 training speakers).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Impact the adaptation utterance length</head><p>In all of our experiments, the average length of an adaptation utterance was about 6 seconds. However, for some applications, it might be more convenient to use shorter utterances. In Figure <ref type="figure" target="#fig_12">10</ref>, we thus further analyze the impact the length of the adaptation utterance has on the accuracy of the separation. For this analysis, we used the WSJ0-2mix dataset and assigned each test utterance an adaptation utterance of longer than 8.5 seconds. All the adaptation utterances were then cut to different lengths of 0.5 to 8 seconds and used as an input to the auxiliary network. During the cutting, we also removed the initial 0.5 seconds of the utterances to avoid an initial silence. The plot shows the average SDR improvements achieved using these shortened adaptation data. For an adaptation utterance of longer than 2.5 seconds, the performance saturates. Already at 1 second, the accuracy of the extraction is fairly close to that of the longer utterances. With less speech, the performance deteriorates, however even with 0.5 seconds of adaptation data, SpeakerBeam manages to improve the SDR compared with the mixtures. Note that these tendencies may be highly dependent on the training data.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>IX. CONCLUSION</head><p>In this paper, we introduced the SpeakerBeam method for extracting a target speaker from a mixture of multiple overlapping speakers based on informing the neural network about the target speaker using additional speaker information. We compared different methods for informing the neural network. The results show that the scaled-activations and factorizedlayer methods are more suitable than simply appending the speaker information to the input. We compared the method to Deep Clustering and Permutation invariant training, where we observed comparable performance for short, fully overlapped mixtures and the advantage of SpeakerBeam for longer mixtures with more complicated overlapping patterns. This is due to the ability of SpeakerBeam to better track the speaker over time. Furthermore, the method can be also combined       with Deep Clustering for further gains. In addition to using our method with single-channel 2-speaker mixtures, we also showed its ability to handle 3-speaker mixtures and the possibility of extending the method to multi-channel processing and joint training with an automatic speech recognition system. In future work, we plan to explore the effect of using larger datasets, especially with higher numbers of speakers, to further improve learned speaker representations and extraction accuracy. Another possible direction involves combining SpeakerBeam with existing speaker diarization approaches and testing its performance on speaker diarization tasks.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 1 .</head><label>1</label><figDesc>Fig. 1. Overall scheme of single-channel extraction for an example with one interfering speaker and noise.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head></head><label></label><figDesc>1932-4553 (c) 2019 IEEE. Personal use is permitted, but republication/redistribution requires IEEE permission. See http://www.ieee.org/publications_standards/publications/rights/index.html for more information.This article has been accepted for publication in a future issue of this journal, but has not been fully edited. Content may change prior to final publication. Citation information: DOI 10.1109/JSTSP.2019.2922820, IEEE Journal of Selected Topics in Signal Processing</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 3 .</head><label>3</label><figDesc>Fig. 3. Type of mixtures in datasets WSJ0-2mix and WSJ0-2mix-long. The first row corresponds to speech from speaker A, the second row to speech from speaker B.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>1 Fig. 4 .</head><label>14</label><figDesc>Fig.<ref type="bibr" target="#b3">4</ref>. Final configuration of the neural network for SpeakerBeam with the scaled activations method and sequence summarization with attention. For more details, see Equations (8), (11)-<ref type="bibr" target="#b12">(13)</ref>.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Fig. 5 .</head><label>5</label><figDesc>Fig. 5. Example of a mixture from the WSJ0-2mix-long dataset as processed by SpeakerBeam and Deep Clustering methods.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Fig. 6 .Fig. 7 .</head><label>67</label><figDesc>Fig. 6. t-SNE of derived speaker representations. The clusters correspond to speakers in the test data.</figDesc><graphic coords="11,82.19,282.71,176.91,176.91" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Fig. 8 .</head><label>8</label><figDesc>Fig. 8. SDR improvements for different speakers in the WSJ0-2mix testing set. The numbers at the top of the figure are the mean SDR improvements for each speaker. The violin plot shows the distribution shape, maximum, minimum and mean SDR improvement over the utterances from the target speaker.</figDesc><graphic coords="12,247.63,265.05,111.96,111.96" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_11"><head>Fig. 9 .</head><label>9</label><figDesc>Fig. 9. Mean SDR improvements for different target-interfering speaker combinations in the WSJ0-2mix testing set. Speakers are sorted by gender. Speakers 051 to 447 are male, speakers 050 to 445 are female.</figDesc><graphic coords="12,74.50,265.05,111.96,111.96" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_12"><head>Fig. 10 .</head><label>10</label><figDesc>Fig. 10. Impact of the adaptation utterance length on SDR improvement.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>TABLE I COMPARISON</head><label>I</label><figDesc>OF DIFFERENT METHODS FOR INFORMING THE NETWORK ABOUT THE SPEAKER AND EXTRACTING THE SPEAKER INFORMATION.RESULTS SHOW SDR AND FW-SNR IMPROVEMENTS FOR THE 2MIXDATASET. IBM STANDS FOR IDEAL BINARY (ORACLE) MASK.</figDesc><table><row><cell>method</cell><cell>speaker representation</cell><cell>2mix ∆SDR [dB]</cell><cell>2mix ∆fw-SNR [dB]</cell></row><row><cell>input-bias</cell><cell>i-vec</cell><cell>-3.8</cell><cell>-1.4</cell></row><row><cell></cell><cell>seqsum</cell><cell>-2.2</cell><cell>-0.8</cell></row><row><cell></cell><cell>seqsum+att</cell><cell>-2.2</cell><cell>-0.8</cell></row><row><cell>fact-layer</cell><cell>i-vec</cell><cell>5.7</cell><cell>3.5</cell></row><row><cell></cell><cell>seqsum</cell><cell>6.1</cell><cell>3.7</cell></row><row><cell></cell><cell>seqsum+att</cell><cell>6.2</cell><cell>3.7</cell></row><row><cell>scaled-act</cell><cell>i-vec</cell><cell>5.2</cell><cell>2.8</cell></row><row><cell></cell><cell>seqsum</cell><cell>5.6</cell><cell>3.5</cell></row><row><cell></cell><cell>seqsum+att</cell><cell>5.7</cell><cell>3.5</cell></row><row><cell>IBM</cell><cell>-</cell><cell>12.8</cell><cell>7.3</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head></head><label></label><figDesc>Table III compares the performance for</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>TABLE II COMPARISON</head><label>II</label><figDesc>OF THE SDR IMPROVEMENTS [DB] WITH WSJ0-2MIX AND WSJ0-2MIX-LONG DATASETS FOR SPEAKERBEAM, DC AND PIT. FOR DC AND PIT, WE USE ORACLE PERMUTATIONS OF THE SEPARATED SOURCES</figDesc><table><row><cell cols="2">FOR EVALUATION.</cell><cell></cell></row><row><cell></cell><cell>2mix</cell><cell>2mix-long</cell></row><row><cell></cell><cell>∆SDR[dB]</cell><cell>∆SDR[dB]</cell></row><row><cell>SpeakerBeam</cell><cell>9.7</cell><cell>14.1</cell></row><row><cell>PIT</cell><cell>9.2</cell><cell>11.0</cell></row><row><cell>DC</cell><cell>8.7</cell><cell>9.8</cell></row><row><cell>PIT + DC</cell><cell>9.9</cell><cell>11.8</cell></row><row><cell>SpeakerBeam + DC</cell><cell>10.9</cell><cell>15.8</cell></row><row><cell>IBM</cell><cell>12.8</cell><cell>17.1</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>TABLE III RESULTS</head><label>III</label><figDesc>OF PERFORMING EXTRACTION ON MIXTURES OF TWO AND THREE SPEAKERS, USING WSJ0-2MIX AND WSJ0-3MIX DATASETS. THE RESULTS ARE IN TERMS OF SDR IMPROVEMENTS [DB].</figDesc><table><row><cell></cell><cell>Training</cell><cell>2mix</cell><cell>3mix</cell></row><row><cell></cell><cell>data</cell><cell>∆SDR[dB]</cell><cell>∆SDR[dB]</cell></row><row><cell>SpeakerBeam</cell><cell>2mix</cell><cell>9.7</cell><cell>3.9</cell></row><row><cell></cell><cell>3mix</cell><cell>7.9</cell><cell>7.4</cell></row><row><cell></cell><cell>2mix + 3mix</cell><cell>9.7</cell><cell>7.7</cell></row><row><cell>DC</cell><cell>2mix</cell><cell>8.7</cell><cell>1.9</cell></row><row><cell></cell><cell>3mix</cell><cell>7.2</cell><cell>6.2</cell></row><row><cell></cell><cell>2mix + 3mix</cell><cell>8.9</cell><cell>6.3</cell></row><row><cell>PIT</cell><cell>2mix</cell><cell>9.0</cell><cell>0.1</cell></row><row><cell></cell><cell>3mix</cell><cell>6.5</cell><cell>6.9</cell></row><row><cell></cell><cell>2mix + 3mix</cell><cell>9.1</cell><cell>7.0</cell></row><row><cell cols="4">both 2-speaker and 3-speaker mixtures with different training</cell></row><row><cell cols="4">sets. The results show that a network trained only on 2-</cell></row><row><cell cols="4">speaker mixtures does not generalize very well to 3-speaker</cell></row><row><cell cols="4">mixtures. If we train only on 3-speaker mixtures, the network</cell></row><row><cell cols="4">can extract speakers from both 2-and 3-speaker mixtures with</cell></row><row><cell cols="4">a reasonable level of performance. For the 2-speaker mixtures,</cell></row><row><cell cols="4">there is still a gap in accuracy compared with matched training.</cell></row></table></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>ACKNOWLEDGMENT</head><p>The BUT authors were partially supported by the Technology Agency of the Czech Republic project No. TJ01000208 "NOSICI", Czech National Science Foundation (GACR) project "NEUREM3" No. 19-26934X, and the National Programme of Sustainability (NPU II) project "IT4Innovations excellence in science -LQ1602". Katerina Zmolikova is a Brno Phd Talent scholarship holder.</p></div>
			</div>

			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0" />			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">Blind speech separation</title>
		<author>
			<persName><forename type="first">S</forename><surname>Makino</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T.-W</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Sawada</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2007">2007</date>
			<publisher>Springer</publisher>
			<biblScope unit="volume">615</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Past review, current progress, and challenges ahead on the cocktail party problem</title>
		<author>
			<persName><forename type="first">C</forename><surname>Y.-M. Qian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Weng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>-K. Chang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><surname>Yu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Frontiers of Information Technology &amp; Electronic Engineering</title>
		<imprint>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="40" to="63" />
			<date type="published" when="2018-01">Jan 2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Computational auditory scene analysis</title>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">J</forename><surname>Brown</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Cooke</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computer Speech &amp; Language</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="297" to="336" />
			<date type="published" when="1994">1994</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">Prediction-driven computational auditory scene analysis</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">P W</forename><surname>Ellis</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1996">1996</date>
		</imprint>
		<respStmt>
			<orgName>Massachusetts Institute of Technology</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">Ph.D. dissertation</note>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Algorithms for non-negative matrix factorization</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">D</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">S</forename><surname>Seung</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2001">2001</date>
			<biblScope unit="page" from="556" to="562" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Monaural sound source separation by nonnegative matrix factorization with temporal continuity and sparseness criteria</title>
		<author>
			<persName><forename type="first">T</forename><surname>Virtanen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE/ACM Transactions on Audio, Speech, and Language Processing</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="1066" to="1074" />
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Superhuman multi-talker speech recognition: A graphical modeling approach</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">R</forename><surname>Hershey</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">J</forename><surname>Rennie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">A</forename><surname>Olsen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">T</forename><surname>Kristjansson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computer Speech &amp; Language</title>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="45" to="66" />
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Single-channel multitalker speech recognition</title>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">J</forename><surname>Rennie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">R</forename><surname>Hershey</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">A</forename><surname>Olsen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Signal Processing Magazine</title>
		<imprint>
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="66" to="80" />
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Supervised speech separation based on deep learning: An overview</title>
		<author>
			<persName><forename type="first">D</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE/ACM Transactions on Audio, Speech, and Language Processing</title>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Deep clustering: Discriminative embeddings for segmentation and separation</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">R</forename><surname>Hershey</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">Le</forename><surname>Roux</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Watanabe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Acoustics, Speech and Signal Processing</title>
		<imprint>
			<date type="published" when="2016">2016. 2016</date>
			<biblScope unit="page" from="31" to="35" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Permutation invariant training of deep models for speaker-independent multi-talker speech separation</title>
		<author>
			<persName><forename type="first">D</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Kolbaek</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z.-H</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Jensen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Acoustics, Speech and Signal Processing</title>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">Anchored speech detection</title>
		<author>
			<persName><forename type="first">R</forename><surname>Maas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">H K</forename><surname>Parthasarathi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>King</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Hoffmeister</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016">2016</date>
			<publisher>INTERSPEECH</publisher>
			<biblScope unit="page" from="2963" to="2967" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">Robust speech recognition via anchor word representations</title>
		<author>
			<persName><forename type="first">B</forename><surname>King</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I.-F</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Vaizman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Maas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">H</forename><surname>Parthasarathi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Hoffmeister</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017">2017</date>
			<publisher>INTERSPEECH</publisher>
			<biblScope unit="page" from="2471" to="2475" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">Device-directed utterance detection</title>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">H</forename><surname>Mallidi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Maas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Goehner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Rastrow</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Matsoukas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Hoffmeister</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1808.02504</idno>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">Deep extractor network for target speaker recovery from single channel speech mixtures</title>
		<author>
			<persName><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Qian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Yu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1807.08974</idno>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Speech separation of a target speaker based on deep neural networks</title>
		<author>
			<persName><forename type="first">J</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Tu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C.-H</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Signal Processing (ICSP), 2014 12th International Conference on</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="473" to="477" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">A deep ensemble learning method for monaural speech separation</title>
		<author>
			<persName><forename type="first">X.-L</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE/ACM Transactions on Audio, Speech, and Language Processing</title>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="967" to="977" />
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">A regression approach to singlechannel speech separation via high-resolution deep neural networks</title>
		<author>
			<persName><forename type="first">J</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Tu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L.-R</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C.-H</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE/ACM Transactions on Audio, Speech, and Language Processing</title>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="1424" to="1437" />
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Speaker adaptation of neural network acoustic models using i-vectors</title>
		<author>
			<persName><forename type="first">G</forename><surname>Saon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Soltau</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Nahamoo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Picheny</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2013 IEEE Workshop on Automatic Speech Recognition and Understanding</title>
		<imprint>
			<date type="published" when="2013">2013</date>
			<biblScope unit="page" from="55" to="59" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Improving DNN speaker independence with i-vector inputs</title>
		<author>
			<persName><forename type="first">A</forename><surname>Senior</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Lopez-Moreno</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Acoustics, Speech and Signal Processing (ICASSP)</title>
		<imprint>
			<date type="published" when="2014">2014. 2014</date>
			<biblScope unit="page" from="225" to="229" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Sequence summarizing neural network for speaker adaptation</title>
		<author>
			<persName><forename type="first">K</forename><surname>Vesely</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Watanabe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Zmolikova</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Karafiat</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Burget</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">H</forename><surname>Cernocky</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Acoustics, Speech and Signal Processing</title>
		<imprint>
			<date type="published" when="2016">2016. 2016</date>
			<biblScope unit="page" from="5315" to="5319" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Context adaptive neural network based acoustic models for rapid adaptation</title>
		<author>
			<persName><forename type="first">M</forename><surname>Delcroix</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Kinoshita</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Ogawa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Huemmer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Nakatani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE/ACM Transactions on Audio, Speech, and Language Processing</title>
		<imprint>
			<biblScope unit="volume">26</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="895" to="908" />
			<date type="published" when="2018-05">May 2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Learning hidden unit contributions for unsupervised speaker adaptation of neural network acoustic models</title>
		<author>
			<persName><forename type="first">P</forename><surname>Swietojanski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Renals</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Spoken Language Technology Workshop (SLT)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2014">2014. 2014</date>
			<biblScope unit="page" from="171" to="176" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Frontend factor analysis for speaker verification</title>
		<author>
			<persName><forename type="first">N</forename><surname>Dehak</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">J</forename><surname>Kenny</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Dehak</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Dumouchel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Ouellet</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE/ACM Transactions on Audio, Speech, and Language Processing</title>
		<imprint>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="788" to="798" />
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title level="m" type="main">Singlechannel multi-speaker separation using deep clustering</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Isik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Le Roux</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Watanabe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">R</forename><surname>Hershey</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="545" to="549" />
		</imprint>
	</monogr>
	<note>Interspeech</note>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Deep attractor network for single-microphone speaker separation</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Mesgarani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Acoustics, Speech and Signal Processing</title>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Multitalker speech separation with utterancelevel permutation invariant training of deep recurrent neural networks</title>
		<author>
			<persName><forename type="first">M</forename><surname>Kolbaek</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z.-H</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Jensen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Kolbaek</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z.-H</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Jensen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE/ACM Transactions on Audio, Speech, and Language Processing</title>
		<imprint>
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page" from="1901" to="1913" />
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<title level="m" type="main">Speaker-aware neural network based beamformer for speaker extraction in speech mixtures</title>
		<author>
			<persName><forename type="first">K</forename><surname>Zmolikova</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Delcroix</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Kinoshita</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Higuchi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Ogawa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Nakatani</surname></persName>
		</author>
		<imprint>
			<date>Aug</date>
			<publisher>INTERSPEECH</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Learning speaker representation for neural network based multichannel speaker extraction</title>
		<author>
			<persName><forename type="first">K</forename><surname>Žmolíková</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Delcroix</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Kinoshita</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Higuchi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Ogawa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Nakatani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Automatic Speech Recognition and Understanding Workshop</title>
		<imprint>
			<date type="published" when="2017">2017. 2017</date>
			<biblScope unit="page" from="8" to="15" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<title level="m" type="main">Optimization of speaker-aware multichannel speech extraction with ASR criterion</title>
		<author>
			<persName><forename type="first">K</forename><surname>Zmolikova</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Delcroix</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Kinoshita</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Higuchi</surname></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Single channel target speaker extraction and recognition with speaker beam</title>
		<author>
			<persName><forename type="first">M</forename><surname>Delcroix</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Zmolikova</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Kinoshita</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Ogawa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Nakatani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2018 IEEE International Conference on Acoustics, Speech and Signal Processing</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="5554" to="5558" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Deep attractor networks for speaker re-identification and blind source separation</title>
		<author>
			<persName><forename type="first">L</forename><surname>Drude</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Neumann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Haeb-Umbach</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Acoustics, Speech and Signal Processing (ICASSP)</title>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
		<title level="m" type="main">Voicefilter: Targeted voice separation by speaker-conditioned spectrogram masking</title>
		<author>
			<persName><forename type="first">Q</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Muckenhirn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Wilson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Sridhar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Hershey</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">A</forename><surname>Saurous</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">J</forename><surname>Weiss</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><forename type="middle">L</forename><surname>Moreno</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1810.04826</idno>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Generalized end-to-end loss for speaker verification</title>
		<author>
			<persName><forename type="first">L</forename><surname>Wan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Papir</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><forename type="middle">L</forename><surname>Moreno</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Acoustics, Speech and Signal Processing (ICASSP), 2018 IEEE International Conference</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="4879" to="4883" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Context adaptive deep neural networks for fast acoustic model adaptation</title>
		<author>
			<persName><forename type="first">M</forename><surname>Delcroix</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Kinoshita</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Hori</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Nakatani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Acoustics, Speech and Signal Processing (ICASSP)</title>
		<imprint>
			<date type="published" when="2015">2015. 2015</date>
			<biblScope unit="page" from="4535" to="4539" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<monogr>
		<title level="m" type="main">Subspace LHUC for fast adaptation of deep neural network acoustic models</title>
		<author>
			<persName><forename type="first">L</forename><surname>Samarakoon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">C</forename><surname>Sim</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016">2016</date>
			<publisher>INTERSPEECH</publisher>
			<biblScope unit="page" from="1593" to="1597" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">ivector-based discriminative adaptation for automatic speech recognition</title>
		<author>
			<persName><forename type="first">M</forename><surname>Karafiát</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Burget</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Matějka</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Glembek</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Černocký</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Automatic Speech Recognition and Understanding Workshop (ASRU)</title>
		<imprint>
			<date type="published" when="2011">2011. 2011</date>
			<biblScope unit="page" from="152" to="157" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Diarization of telephone conversations using factor analysis</title>
		<author>
			<persName><forename type="first">P</forename><surname>Kenny</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Reynolds</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Castaldo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Journal of Selected Topics in Signal Processing</title>
		<imprint>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page">1059</biblScope>
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Simplification and optimization of i-vector extraction</title>
		<author>
			<persName><forename type="first">O</forename><surname>Glembek</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Burget</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Matějka</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Karafiát</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Kenny</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Acoustics, Speech and Signal Processing (ICASSP)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2011">2011. 2011</date>
			<biblScope unit="page" from="4516" to="4519" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">On training targets for supervised speech separation</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Narayanan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE/ACM Transactions on Audio, Speech and Language Processing (TASLP)</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page" from="1849" to="1858" />
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Deep clustering and conventional networks for music separation: Stronger together</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">R</forename><surname>Hershey</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">Le</forename><surname>Roux</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Mesgarani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Acoustics, Speech and Signal Processing</title>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="61" to="65" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Blind acoustic beamforming based on generalized eigenvalue decomposition</title>
		<author>
			<persName><forename type="first">E</forename><surname>Warsitz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Haeb-Umbach</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE/ACM Transactions on Audio, Speech, and Language Processing</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="1529" to="1539" />
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Microphone array processing for distant speech recognition: From close-talking microphones to farfield sensors</title>
		<author>
			<persName><forename type="first">K</forename><surname>Kumatani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Mcdonough</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Raj</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Signal Processing Magazine</title>
		<imprint>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="127" to="140" />
			<date type="published" when="2012-11">Nov 2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Neural network based spectral mask estimation for acoustic beamforming</title>
		<author>
			<persName><forename type="first">J</forename><surname>Heymann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Drude</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Haeb-Umbach</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Acoustics, Speech and Signal Processing</title>
		<imprint>
			<date type="published" when="2016">2016. 2016</date>
			<biblScope unit="page" from="196" to="200" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<monogr>
		<title level="m" type="main">Improved MVDR beamforming using single-channel mask prediction networks</title>
		<author>
			<persName><forename type="first">H</forename><surname>Erdogan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">R</forename><surname>Hershey</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Watanabe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Mandel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">Le</forename><surname>Roux</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
	<note>Interspeech</note>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Optimizing neural-network supported acoustic beamforming by algorithmic differentiation</title>
		<author>
			<persName><forename type="first">C</forename><surname>Böddeker</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Hanebrink</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Drude</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Heymann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Haeb-Umbach</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Acoustics, Speech and Signal Processing (ICASSP)</title>
		<imprint>
			<date type="published" when="2017-03">March 2017</date>
			<biblScope unit="page" from="171" to="175" />
		</imprint>
	</monogr>
	<note>2017 IEEE International Conference on</note>
</biblStruct>

<biblStruct xml:id="b46">
	<monogr>
		<title level="m" type="main">CSR-I (WSJ0) Complete LDC93S6A</title>
		<author>
			<persName><forename type="first">J</forename><surname>Garofolo</surname></persName>
		</author>
		<ptr target="https://catalog.ldc.upenn" />
		<imprint>
			<date type="published" when="1993">1993</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<monogr>
		<title level="m" type="main">Multi-channel deep clustering : Discriminative spectral and spatial embeddings for speakerindependent speech separation</title>
		<author>
			<persName><forename type="first">Z.-Q</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">L</forename><surname>Roux</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">R</forename><surname>Hershey</surname></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Image method for efficiently simulating small-room acoustics</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">B</forename><surname>Allen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">A</forename><surname>Berkley</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The Journal of the Acoustical Society of America</title>
		<imprint>
			<biblScope unit="volume">65</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="943" to="950" />
			<date type="published" when="1979">1979</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Room impulse response generator</title>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">A P</forename><surname>Habets</surname></persName>
		</author>
		<ptr target="http://home.tiscali.nl/ehabets/rirgenerator/rirgenerator.pdf" />
	</analytic>
	<monogr>
		<title level="j">Technische Universiteit Eindhoven, Tech. Rep</title>
		<imprint>
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">The PAS-CAL CHiME speech separation and recognition challenge</title>
		<author>
			<persName><forename type="first">J</forename><surname>Barker</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Vincent</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Christensen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Green</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computer Speech and Language</title>
		<imprint>
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="621" to="633" />
			<date type="published" when="2013-05">May 2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">The third CHiME speech separation and recognition challenge: Dataset, task and baselines</title>
		<author>
			<persName><forename type="first">J</forename><surname>Barker</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Marxer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Vincent</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Watanabe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2015 IEEE Workshop on Automatic Speech Recognition and Understanding</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="504" to="511" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">Understanding the difficulty of training deep feedforward neural networks</title>
		<author>
			<persName><forename type="first">X</forename><surname>Glorot</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the thirteenth international conference on artificial intelligence and statistics</title>
		<meeting>the thirteenth international conference on artificial intelligence and statistics</meeting>
		<imprint>
			<date type="published" when="2010">2010</date>
			<biblScope unit="page" from="249" to="256" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">The kaldi speech recognition toolkit</title>
		<author>
			<persName><forename type="first">D</forename><surname>Povey</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Ghoshal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Boulianne</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Burget</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Glembek</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Goel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Hannemann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Motlicek</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Qian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Schwarz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2011 IEEE Workshop on Automatic Speech Recognition and Understanding</title>
		<imprint>
			<publisher>IEEE Signal Processing Society</publisher>
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
	<note>EPFL-CONF-192584</note>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">Performance measurement in blind audio source separation</title>
		<author>
			<persName><forename type="first">E</forename><surname>Vincent</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Gribonval</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Févotte</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE/ACM Transactions on Audio, Speech, and Language Processing</title>
		<imprint>
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="1462" to="1469" />
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<analytic>
		<title level="a" type="main">mir eval: A transparent implementation of common MIR metrics</title>
		<author>
			<persName><forename type="first">C</forename><surname>Raffel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Mcfee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">J</forename><surname>Humphrey</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Salamon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Nieto</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">P</forename><surname>Ellis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">C</forename><surname>Raffel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 15th International Society for Music Information Retrieval Conference, ISMIR. Citeseer</title>
		<meeting>the 15th International Society for Music Information Retrieval Conference, ISMIR. Citeseer</meeting>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<analytic>
		<title level="a" type="main">The REVERB challenge: A common evaluation framework for dereverberation and recognition of reverberant speech</title>
		<author>
			<persName><forename type="first">K</forename><surname>Kinoshita</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Delcroix</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Yoshioka</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Nakatani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Habets</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Haeb-Umbach</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Leutnant</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Sehr</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Kellermann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Maas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Gannot</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Raj</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Applications of Signal Processing to Audio and Acoustics (WASPAA)</title>
		<imprint>
			<date type="published" when="2013">2013. 2013</date>
			<biblScope unit="page" from="1" to="4" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<analytic>
		<title level="a" type="main">Single-channel multi-speaker separation using deep clustering</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Isik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">L</forename><surname>Roux</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Watanabe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">R</forename><surname>Hershey</surname></persName>
		</author>
		<idno>abs/1607.02173</idno>
		<ptr target="http://arxiv.org/abs/1607.02173" />
	</analytic>
	<monogr>
		<title level="j">CoRR</title>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b58">
	<analytic>
		<title level="a" type="main">Compact network for speakerbeam target speaker extraction</title>
		<author>
			<persName><forename type="first">M</forename><surname>Delcroix</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Zmolikova</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Ochiai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Kinoshita</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Araki</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Nakatani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Acoustics, Speech and Signal Processing (ICASSP)</title>
		<imprint>
			<date type="published" when="2019">2019. 2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b59">
	<monogr>
		<title level="m" type="main">SpeakerBeam</title>
		<ptr target="https://youtu.be/BM0DXWgGY5A" />
		<imprint>
			<pubPlace>English</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b60">
	<analytic>
		<title level="a" type="main">Visualizing data using t-SNE</title>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">V D</forename><surname>Maaten</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of machine learning research</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="page" from="2579" to="2605" />
			<date type="published" when="2008-11">Nov. 2008</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
