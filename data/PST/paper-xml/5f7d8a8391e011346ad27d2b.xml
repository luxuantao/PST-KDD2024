<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">SeqMix: Augmenting Active Sequence Labeling via Sequence Mixup</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date type="published" when="2020-10-05">5 Oct 2020</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Rongzhi</forename><surname>Zhang</surname></persName>
							<email>rongzhi.zhang@gatech.edu</email>
						</author>
						<author>
							<persName><forename type="first">Georgia</forename><surname>Tech</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Yue</forename><surname>Yu</surname></persName>
							<email>yueyu@gatech.edu</email>
						</author>
						<author>
							<persName><forename type="first">Chao</forename><surname>Zhang</surname></persName>
							<email>chaozhang@gatech.edu</email>
						</author>
						<title level="a" type="main">SeqMix: Augmenting Active Sequence Labeling via Sequence Mixup</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2020-10-05">5 Oct 2020</date>
						</imprint>
					</monogr>
					<idno type="arXiv">arXiv:2010.02322v1[cs.CL]</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.2" ident="GROBID" when="2022-12-25T13:47+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Active learning is an important technique for low-resource sequence labeling tasks. However, current active sequence labeling methods use the queried samples alone in each iteration, which is an inefficient way of leveraging human annotations. We propose a simple but effective data augmentation method to improve label efficiency of active sequence labeling. Our method, SeqMix, simply augments the queried samples by generating extra labeled sequences in each iteration. The key difficulty is to generate plausible sequences along with token-level labels. In SeqMix, we address this challenge by performing mixup for both sequences and token-level labels of the queried samples. Furthermore, we design a discriminator during sequence mixup, which judges whether the generated sequences are plausible or not. Our experiments on Named Entity Recognition and Event Detection tasks show that SeqMix can improve the standard active sequence labeling method by 2.27%-3.75% in terms of F 1 scores. The code and data for SeqMix can be found at https://github. com/rz-zhang/SeqMix.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Many NLP tasks can be formulated as sequence labeling problems, such as part-of-speech (POS) tagging <ref type="bibr" target="#b50">(Zheng et al., 2013)</ref>, named entity recognition (NER) <ref type="bibr" target="#b22">(Lample et al., 2016)</ref>, and event extraction <ref type="bibr" target="#b44">(Yang et al., 2019)</ref>. Recently, neural sequential models <ref type="bibr" target="#b22">(Lample et al., 2016;</ref><ref type="bibr" target="#b0">Akbik et al., 2018;</ref><ref type="bibr" target="#b39">Vaswani et al., 2017)</ref> have shown strong performance for various sequence labeling task. However, these deep neural models are label hungrythey require large amounts of annotated sequences to achieve strong performance. Obtaining large amounts of annotated data can be too expensive for practical sequence labeling tasks, due to tokenlevel annotation efforts.</p><p>Active learning is an important technique for sequence labeling in low-resource settings. Active sequence labeling is an iterative process. In each iteration, a fixed number of unlabeled sequences are selected by a query policy for annotation and then model updating, in hope of maximally improving model performance. For example, <ref type="bibr" target="#b38">Tomanek et al. (2007)</ref>; <ref type="bibr" target="#b34">Shen et al. (2017)</ref> select query samples based on data uncertainties; <ref type="bibr" target="#b15">Hazra et al. (2019)</ref> compute model-aware similarity to eliminate redundant examples and improve the diversity of query samples; and <ref type="bibr" target="#b11">Fang et al. (2017)</ref>; <ref type="bibr" target="#b25">Liu et al. (2018)</ref> use reinforcement learning to learn query policies. However, existing methods for active sequence labeling all use the queried samples alone in each iteration. We argue that the queried samples provide limited data diversity, and using them alone for model updating is inefficient in terms of leveraging human annotation efforts.</p><p>We study the problem of enhancing active sequence labeling via data augmentation. We aim to generate augmented labeled sequences for the queried samples in each iteration, thereby introducing more data diversity and improve model generalization. However, data augmentation for active sequence labeling is challenging, because we need to generate sentences and token-level labels jointly. Prevailing generative models <ref type="bibr" target="#b49">(Zhang et al., 2016;</ref><ref type="bibr" target="#b2">Bowman et al., 2016)</ref> are inapplicable because they can only generate word sequences without labels. It is also infeasible to apply heuristic data augmentation methods such as context-based words substitution <ref type="bibr" target="#b20">(Kobayashi, 2018)</ref>, synonym replacement, random insertion, swap, and deletion <ref type="bibr" target="#b41">(Wei and Zou, 2019)</ref>, paraphrasing <ref type="bibr" target="#b5">(Cho et al., 2019)</ref> or back translation <ref type="bibr" target="#b43">(Xie et al., 2019)</ref>, because label composition is complex for sequence labeling. Directly using these techniques to manipulate tokens may inject incorrectly labeled sequences into training data and harm model performance.</p><p>We propose SeqMix, a data augmentation method for generating sub-sequences along with their labels based on mixup <ref type="bibr" target="#b46">(Zhang et al., 2018)</ref>. Under the active sequence labeling framework, Se-qMix is capable of generating plausible pseudo labeled sequences for the queried samples in each iteration. This is enabled by two key techniques in SeqMix: (1) First, in each iteration, it searches for pairs of eligible sequences and mixes them both in the feature space and the label space. (2) Second, it has a discriminator to judge if the generated sequence is plausible or not. The discriminator is designed to compute the perplexity scores for all the generated candidate sequences and select the low-perplexity sequences as plausible ones.</p><p>We show that SeqMix consistently outperforms standard active sequence labeling baselines under different data usage percentiles with experiments on Named Entity Recognition and Event Detection tasks. On average, it achieves 2.95%, 2.27%, 3.75% F 1 improvements on the CoNLL-2003, ACE05 and WebPage datasets. The advantage of SeqMix is especially prominent in low-resource scenarios, achieving 12.06%, 8.86%, 16.49% F 1 improvements to the original active learning approach on the above three datasets. Our results also verify the proposed mixup strategies and the discriminator are vital to the performance of SeqMix.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Preliminaries</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Problem Definition</head><p>Many NLP problems can be formulated as sequence labeling problems. Given an input sequence, the task is to annotate it with token-level labels. The labels often consist of a position prefix provided by a labeling schema and a type indicator provided by the specific task. For example, in the named entity recognition task, we can adopt the BIO (Beginning, Inside, Outside) tagging scheme <ref type="bibr" target="#b26">(Màrquez et al., 2005)</ref> to assign labels for each token: the first token of an entity mention with type X is labeled as B-X, the tokens inside that mention are labeled as I-X and the non-entity tokens are labeled as O.</p><p>Consider a large unlabeled corpus U, traditional active learning starts from a small annotated seed set L, and utilizes a query function ψ(U, K, γ(•)) to obtain K most informative unlabeled samples X = {x 1 , . . . , x K } along with their labels Y = {y 1 , • • • , y K }, where γ(•) is the query policy. Then, we remove X from the unlabeled data U and repeat the above procedure until the satisfactory performance achieved or the annotation capacity reached.</p><p>In SeqMix, we aim to further exploit the annotated set X , Y to generate augmented data X * , Y * . Then the labeled dataset is expanded as L = L ∪ X , Y ∪ X * , Y * . Formally, we define our task as: (1) construct a generator φ(•) to implement sequence and label generation based on the actively sampled data X and its label Y, (2) set a discriminator d(•) to yield the filtered generation, then (3) augment the labeled set as</p><formula xml:id="formula_0">L = L ∪ X , Y ∪ d(φ(X , Y)).</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Active Learning for Sequence Labeling</head><p>Active sequence labeling selects K most informative instances ψ (•, K, γ(•)) in each iteration, with the hope of maximally improving model performance with a fixed labeled budget. With the input sequence x of length T , we denote the model output as f (•|x; θ). Our method is generic to any query policies γ(•). Below, we introduce several representative policies.</p><p>Least Confidence (LC) <ref type="bibr" target="#b7">Culotta and McCallum (2005)</ref> measure the uncertainty of sequence models by the most likely predicted sequence. For a CRF model <ref type="bibr" target="#b21">(Lafferty et al., 2001)</ref>, we calculate γ with the predicted sequential label y * as</p><formula xml:id="formula_1">γ LC (x) = 1 − max y * (P (y * |x; θ) ,<label>(1)</label></formula><p>where y * is the Viterbi parse. For BERT <ref type="bibr" target="#b9">(Devlin et al., 2019)</ref> with a token classification head, we adopt a variant of the least confidence measure:</p><formula xml:id="formula_2">γ LC' (x) = T t=1 (1 − max yt P (y t |x; θ)),<label>(2)</label></formula><p>where P (y t |x; θ) = softmax(f (y t |x; θ)).</p><p>Normalized Token Entropy (NTE) Another uncertainty measure for the query policy is normalized entropy <ref type="bibr" target="#b32">(Settles and Craven, 2008)</ref>, defined as:</p><formula xml:id="formula_3">γ TE (x) = − 1 T T t=1 M m=1 P m (y t |x, θ) log P m (y t |x, θ),<label>(3) where</label></formula><formula xml:id="formula_4">P m (y t |x, θ) = [softmax(f (y t |x; θ))] m .</formula><p>Disagreement Sampling Query-by-committee (QBC) <ref type="bibr" target="#b33">(Seung et al., 1992)</ref>, is another approach for specifying the policy, where the unlabeled data can be sampled by the disagreement of the base models. The disagreement can be defined in several ways, here we take the vote entropy proposed by <ref type="bibr" target="#b8">(Dagan and Engelson, 1995)</ref>. Given a committee consist of C models, the vote entropy for input x is:</p><formula xml:id="formula_5">γ VE (x) = − 1 T T t=1 M m=1 V m (y t ) C log V m (y t ) C ,<label>(4</label></formula><p>) where V m (y t ) is the number of models that predict the t-th token x t as the label m.</p><p>3 The SeqMix Method</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Overview</head><p>Given a corpus for sequence labeling, we assume the dataset contains a small labeled set L and a large unlabeled set U initially. We start from augmenting the seed set L with SeqMix. First, we adopt a pairing function ζ(•) to find paired samples by traversing L. Next, we generate mixed-labeled sequences via latent space linear interpolation with one of the approaches mentioned in Section 3.2. To ensure the semantic quality of the generated sequences, we use a discriminator After that, the iterative active learning procedure begins. In each iteration, we actively select instances from U with a query policy γ(•) (Section 2.2) to obtain the top K samples X = ψ(U, K, γ(•)). The newly selected samples will be labeled with Y, and the batch of samples X , Y will be used for SeqMix. Again, we generate</p><formula xml:id="formula_6">L * = SeqMix( X , Y , α, ζ(•), d(•)</formula><p>) and expand the training set as L = L ∪ L * . Then we train the model θ on the newly augmented set L. The iterative active learning procedure terminates when a fixed number of iterations are reached. We summarize the above procedure in Algorithm 1.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Sequence Mixup in the Embedding Space</head><p>Mixup <ref type="bibr" target="#b46">(Zhang et al., 2018)</ref> is a data augmentation method that implements linear interpolation in the input space. Given two input samples x i , x j along </p><formula xml:id="formula_7">L * = SeqMix(L, α, ζ(•), d(•)) L = L ∪ L * // model initialization θ = train (θ, L)</formula><p>// active learning iterations with augmentation for round in active learning rounds do</p><formula xml:id="formula_8">X = ψ(U, K, γ(•)) U = U − X Annotate X to get X , Y L * = SeqMix( X , Y , α, ζ(•), d(•)) L = L ∪ X , Y ∪ L * θ = train (θ, L) end Output:</formula><p>The sequence model trained with active data augmentation: θ with the labels y i , y j , the mixing process is:</p><formula xml:id="formula_9">x = λx i + (1 − λ)x j , (5) ỹ = λy i + (1 − λ)y j ,<label>(6)</label></formula><p>where λ ∼ Beta(α, α) is the mixing coefficient.</p><p>Through linear combinations on the input level of paired examples and their labels, Mixup regularizes the model to present linear behavior among the training data.</p><p>Mixup is not directly applicable to generate interpolated samples for text data, because the input space is discrete. To overcome this, SeqMix performs token-level interpolation in the embedding space and selects a token closest to the interpolated embedding. Specifically, SeqMix constructs a table of tokens W and their corresponding contextual embeddings E 1 . Given two sequences</p><formula xml:id="formula_10">x i = {w 1 i , • • • , w T i } and x j = {w 1 j , • • • , w T j } with their embedding representations e x i = {e 1 i , • • • , e T i } and e x j = {e 1 j , • • • , e T j }</formula><p>, the t-th mixed token is the token whose embedding e t is closest to the mixed embedding:</p><formula xml:id="formula_11">e t = arg min e∈E e − (λe t i + (1 − λ)e t j ) 2 . (7)</formula><p>1 The construction of {W, E} are discussed in Appendix.</p><p>To get the corresponding w t , we can query the table {W, E} using e t . The label generation is straightforward. For two label sequences</p><formula xml:id="formula_12">y i = {y 1 i , • • • , y T i } and y j = {y 1 j , • • • , y T j }</formula><p>, we get the t-th mixed label as:</p><formula xml:id="formula_13">y t = λy t i + (1 − λ)y t j ,<label>(8)</label></formula><p>where y t i and y t j are one-hot encoded labels. Along with the above sequence mixup procedures, we also introduce a pairing strategy that selects sequences for mixup. The reason is that, in many sequence labeling tasks, the labels of interest are scarce. For example, in the NER and event detection tasks, the "O" label is dominant in the corpus, which do not refer to any entities or events of interest. We thus define the labels of interest as valid labels, e.g., the non-"O" labels in NER and event detection, and design a sequence pairing function to select more informative parent sequences for mixup. Specifically, the sequence pairing function ζ(•) is designed according to valid label density. For a sequence, its valid label density is defined as η = n s , where n is the number of valid labels and s is the length of the sub-sequence. We set a threshold η 0 for ζ(•), and the sequence will be considered as an eligible candidate for mixup only when η ≥ η 0 .</p><p>Based on the above token-level mixup procedure and the sequence pairing function, we propose three different strategies for generating interpolated labeled sequences. These strategies are shown in Figure <ref type="figure" target="#fig_1">1</ref> and described below:</p><p>Whole-sequence mixup As the name suggests, whole-sequence mixup (Figure <ref type="figure" target="#fig_1">1</ref>(a)) performs sequence mixing at the whole-sequence level. Given two sequences x i , y i , x j , y j ∈ L, they must share the same length without counting padding words. Besides, the paring function ζ(•) requires that both the two sequences satisfy η ≥ η 0 . Then we perform mixup at all token positions, by employing Equation <ref type="formula">7</ref>to generate mixed tokens and Equation 8 to generate mixed labels (note that the mixed labels are soft labels).</p><p>Sub-sequence mixup One drawback of the whole-sequence mixup is that it indiscriminately mixes over all tokens, which may include incompatible subsequences and generate implausible sequences. To tackle this, we consider sub-sequence mixup (Figure <ref type="figure" target="#fig_1">1</ref> </p><formula xml:id="formula_14">x i , y i , x j , y j , (i = j) in L do if ζ( x i , y i , x j , y j ) then λ ∼ Beta(α, α) // mixup the target sub-sequences for t = 1, • • • , T do</formula><p>Calculate e t by Eq. ( <ref type="formula">7</ref>); Get corresponding token w t for e t ; Calculate y t by Eq. ( <ref type="formula" target="#formula_13">8</ref>).</p><formula xml:id="formula_15">end xsub = {w 1 , • • • , w T } ỹsub = {y 1 , • • • , y T } // replace the original sequences for k in {i, j} do xk = x k − x ksub + xsub ỹk = y k − y ksub + ỹsub if d( xk ) then L * = L * ∪ xk , ỹk end if |L * | ≥ N then break end end end end</formula><p>Output: Generated sequences and labels L * paired sub-sequences. Denote the sub-sequences of x i , y i , x j , y j as</p><formula xml:id="formula_16">X isub = x 1 isub , . . . , x s isub , X jsub = x 1 jsub , . . . , x s jsub . If ∃ x isub ∈ X isub , x jsub ∈ X jsub , such that their η ≥ η 0 , we have ζ( x i , y i , x j , y j ) = True.</formula><p>Then the subsequences x isub and x jsub are mixed as Figure <ref type="figure" target="#fig_1">1(b)</ref>. The mixed sub-sequence and labels will replace the original parts of the parents samples, and the other parts of the parent samples remain unchanged. In this way, sub-sequence mixup is expected to keep the syntax structure of the original sequence, while providing data diversity.</p><p>Label-constrained sub-sequence mixup can be considered as a special case of sub-sequence mixup, where the constraints inherit sub-sequence mixup, and further require that the sub-sequence labels are consistent. As Figure <ref type="figure" target="#fig_1">1</ref>(c) shows, after mixing such paired samples, the generation will just update the tokens of the sub-sequences while keeping the labels the same as before. Hence, this Figure <ref type="figure" target="#fig_1">1</ref>: Illustration of the three variants of SeqMix. We use s = 5, η 0 = 3 5 for whole-sequence mixup and s = 3, η 0 =<ref type="foot" target="#foot_0">2</ref> 3 for sub-sequence mixup and label-constrained sub-sequence mixup. The solid red frames indicate paired sequences or sub-sequences, and the red dotted frames indicate generated sequence or sub-sequence. In the original sequences, the parts not included in the solid red frames will be unchanged in the generated sequences. For the mixup in the embedding space, we take the embedding in E which is closest to the raw mixed embedding as the generated embedding. For the mixup in the label space, the mixed label can be used as the pseudo label.</p><p>version is called label-constrained sub-sequence mixup.</p><p>Comparing the three variants, label-constrained sub-sequence mixup gives the most restrictions to pairing parent samples, sub-sequence mixup sets the sub-sequence-level pattern, while wholesequence mixup just requires η ≥ η 0 for the sequences with the same length.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Scoring and Selecting Plausible Sequences</head><p>During sequence mixup, the mixing coefficient λ determines the strength of interpolation. When λ approximates 0 or 1, the generated sequence will be similar to one of the parent sequences, while the λ around 0.5 produces relatively diverse generation. However, generating diverse sequences means lowquality sequences can be generated, which can provide noisy contextual information and hurt model performance.</p><p>To maintain the quality of mixed sequences, we set a discriminator to score the perplexity of the sequences. The final generated sequences will consist of only the sequences that pass the sequence quality screening. For screening, we utilize a language model GPT-2 <ref type="bibr" target="#b28">(Radford et al., 2019)</ref> to score sequence x by computing its perplexity:</p><formula xml:id="formula_17">Perplexity(x) = 2 − 1 T T i=1 log p(w i ) , (<label>9</label></formula><formula xml:id="formula_18">)</formula><p>where T is the number of tokens before padding, w i is the i-th token of sequence x. Based on the perplexity and a score range [s 1 , s 2 ], the discriminator can give judgment for sequence x:</p><formula xml:id="formula_19">d(x) = 1 {s 1 ≤ Perplexity (x) ≤ s 2 } . (10)</formula><p>The lower the perplexity score, the more natural the sequence. However, the discriminator should also consider the regularization effectiveness and the generation capacity. Hence, a blind low perplexity setting is undesirable. The overall sequence mixup and selection procedure is illustrated in Algorithm 2.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experiments</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Experiment Setup</head><p>Datasets. We conduct experiments on three sequence labeling datasets for the named entity recognition (NER) and event detection tasks.</p><p>(1) CoNLL-03 <ref type="bibr" target="#b37">(Tjong Kim Sang and De Meulder, 2003)</ref> is a corpus for NER task. It provides four named entity types: persons, locations, organizations, and miscellaneous. 2  (2) ACE05 is a corpus for event detection. It provides 8 event types and 33 subtypes. We study the event trigger detection problem, which aims to identify trigger tokens in a sentence.</p><p>(3) Webpage <ref type="bibr" target="#b29">(Ratinov and Roth, 2009</ref>) is a NER corpus with 20 webpages related to computer science conference and academic websites. It inherits the entity types from CoNLL-03. Data Split. To investigate low-resource sequence labeling, we randomly take 700 labeled sentences from the original CoNLL-03 dataset as the training set. For ACE05 and WebPage dataset, the annotation is sparse, so we conduct experiments on their original dataset without further slicing.</p><p>We set 6 data usage percentiles for the training set in each corpus. The sequence model is initialed on a small seed set, then it performs five iterates of active learning. For the query policy, we use random sampling and the three active learning policies mentioned in Section 2.2. The machine learning performance is evaluated by F 1 score for each data usage percentile. Parameters. We use BERT-base-cased for the NER task as the underlying model, and BERT-basemultilingual-cased for the event trigger detection task. We set the max length as 128 to pad the varying-length sequences. The learning rate of the underlying model is 5e-5, and the batch size is 32. We trained them for 10 Epochs at each data usage percentile. For the parameters of SeqMix, we set the α = 8 to sample λ from Beta(α, α). We use the sub-sequence window length s = {5, 5, 4}, the valid label density η 0 = {0.6, 0.2, 0.5} for CoNLL-03, ACE05 and Webpage, respectively. The augment rate is set as 0.2, and the discriminator score range is set as (0, 500). We also perform a detailed parameter study in Section 4.4.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Results</head><p>The main results are presented in Figure <ref type="figure" target="#fig_3">2</ref>, where we use NTE sampling as the default active learning policy. From the result, it is clear that our method achieves the best performance consistently at each data usage percentile for all three datasets. The best SeqMix method (sub-sequence mixup with NTE sampling) outperforms the strongest active learning baselines by 2.95% on CoNLL-03, 2.27% on ACE05 and 3.75% on WebPage in terms of F 1 score on average. Moreover, the augmentation advantage is especially prominent for the seed set initialization stage where we only have a very limited number of labeled data. Through the augmentation, we improve the model performance from 68.65% to 80.71%, where the seed set is 200 labeled sequences and the augmentation provides extra 40 data points for CoNLL-03. The improvement is also significant on ACE05 (40.65% to 49.51%), and WebPage (55.18% to 71.67%), which indicates that our SeqMix can largely resolve the label scarcity issue in low-resource scenarios.</p><p>We also perform statistical significance tests for the above results. We use Wilcoxon Signed Rank Test <ref type="bibr" target="#b42">(Wilcoxon, 1992)</ref>, a non-parametric alternative to the paired t-test. This significance test fits our task as F-score is generally assumed to be not normally distributed <ref type="bibr" target="#b10">(Dror et al., 2018)</ref>, and nonparametric significance tests should be used in such a case. The results show that sub-sequence mixup and label-constrained sub-sequence mixup can provide a statistical significance (the confidence level α = 0.05 and the number of data points N = 6) for all the comparisons with active learning baselines on used datasets. The whole-sequence mixup passes the statistical significance test with α = 0.1 and N = 6 on CoNLL-03 and WebPage, but fails on ACE05. Among all the three SeqMix variants, subsequence mixup gives the overall best performance (label-constrained sub-sequence mixup achieves very close performance with sub-sequence mixup on ACE05 dataset), but whole-sequence mixup does not yield a consistent improvement to the original active learning method. This is because the whole-sequence mixup may generate semantically poor new sequences. Instead, the sub-sequencelevel process reserves the original context information between the sub-sequence and the other parts of the whole sequence. Meanwhile, the updated sub-sequences inherit the original local informativeness, and introduce linguistic diversity to enhance the model's generalization ability.</p><p>To justify that SeqMix can provide improvement to the active learning framework with various query policies, we employ different query policies with SeqMix augmentation under the same experiment setting as Figure <ref type="figure" target="#fig_3">2</ref>(a). From Figure <ref type="figure" target="#fig_4">3</ref>, we find that there is a consistent performance improvement when employing SeqMix with different query policies. As SeqMix achieves {2.46%, 2.85%, 2.94%} performance gain for random sampling, LC sampling and NTE sampling respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Effect of Discriminator</head><p>To verify the effectiveness of the discriminator, we conduct the ablation study on a subset of CoNLL-    03 with 700 labeled sequences. We use subsequence mixup with NTE sampling as the backbone and change the perplexity score range of the discriminator. We start from the seed set with 200 labeled data, then actively query 100 data in each learning round and repeat 5 rounds in total.</p><p>The result in Table <ref type="table" target="#tab_1">1</ref> demonstrates the discriminator provides a stable improvement for the last four data usage percentiles, and the discriminator with score range (0, 500) can boost the model by 1.07% F 1 score, averaged by all the data usage percentiles. The comparison between 3 different score thresholds demonstrates the lower the perplexity, the better the generation quality. As a result, the final F 1 score becomes higher with the better generated tokens. Actually, we can further narrow down the score range to get more performance improvement in return, but the too strict constraints will slow down the generation in practice and reduce the number of generated samples.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Parameter Study</head><p>In this subsection, we study the effect of several key parameters.</p><p>Augment rate r. We vary the augment rate r =</p><formula xml:id="formula_20">|L * | |ψ(U ,K,γ(•))| in {0.</formula><p>2, 0.4, 0.6, 0.8, 1.0} and keep the number of initial data usage same to investigate the effect of augment rate for data augmentation. Table <ref type="table" target="#tab_3">2</ref> shows that r ≤ 0.6 can provide better F 1 improvement. The model with r = 0.2 surpasses the model with r = 1.0 by 0.73%, evaluated by the average F 1 score for all the data usage percentiles. This result indicates that the model appreciates moderate augmentation more. However, the performance variance based on the augment rate is not prominent compared to the improvement provided by SeqMix to the active learning framework.</p><p>Valid tag density η 0 . We search the valid tag density η 0 as Section 3.2 defined by varying the sub-sequence window length s and the required number of valid tag n within the window. The results in Figure <ref type="figure" target="#fig_5">4</ref>(a) illustrate the combination (s = 5, n = 3) outperforms other settings. When s is too small, the window usually truncates the continuous clause, thus cutting off the local syntax or semantic information. When s is too large, sub-sequence mixup tends to behave like wholesequence SeqMix, where the too long sub-sequence generation can hardly maintain the rationality of syntax and semantics as before. The high η 0 with long window length may result in an insufficient amount of eligible parent sequences. Actually, even with a moderate augment rate α = 0.2, the combination (s = 6, n = 5) has been unable to provide enough generation.</p><p>Mixing parameter α. We show the performance with different α in Figure <ref type="figure" target="#fig_5">4</ref>(b). The parameter α decides the distribution λ ∼ Beta(α, α), and the coefficient λ directly involved the mixing of tokens and labels. Among the values {0.5, 1, 2, 4, 8, 16}, we observed α = 8 presents the best performance.</p><p>It outperforms the second-best parameter setting 0.49% by average. From the perspective of Beta distribution, larger α will make the sampled λ more concentrated around 0.5, which assigns more balance weights to the parent samples to be mixed. In this way, the interpolation produces encoded token with further distance to both the parent samples, thus introduces a more diverse generation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5">Case Study</head><p>Figure <ref type="figure" target="#fig_7">5</ref> presents a generation example via subsequence mixup. For the convenience of presentation, we set the length of sub-sequence s = 3 and the valid label density threshold η 0 = 2 3 . The two input sequences got paired for their eligible sub-sequences "COLORADO 10 St" and "Slovenia , Kwasniewski". The subsequences are mixed by λ = 0.39 in this case, which is sampled from Beta(α, α). Then the generated sub-sequence "Ohio ( novelist" replaces the original parts in the two input sequences. Among the generated tokens, "Ohio" inherits the label B-ORG from "COLORADO" and the label B-LOC from "Slovenia", and the distribution Beta(α, α) assigns the two labels with weights λ = 0.39 and (1 − λ) = 0.61. The open parenthesis is produced by the mixing of a digit and a punctuation mark, and keeps the label O shared by its parents. Similarly, the token "novelist" generated by "St" and "Kwasniewski" gets a mixed label from B-ORG and B-PER.</p><p>The discriminator then evaluates the two generated sequences. The generated sequence i is not reasonable enough intuitively, and its perplexity score 877 exceeds the threshold, so it is not added into the training set. The generated sequence j retains the original syntax and semantic structure much better. Although the open parenthesis seems strange, it plays a role as the comma in the original sequence to separate two clauses. This generation behaves closely to a normal sequence and earns 332 perplexity score, which permits its incorporation into the training set.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Related Work</head><p>Active Sequence Labeling Sequence labeling has been studied extensively for different NLP problems. Different neural architectures has been proposed <ref type="bibr" target="#b17">(Huang et al., 2015;</ref><ref type="bibr" target="#b22">Lample et al., 2016;</ref><ref type="bibr" target="#b27">Peters et al., 2018;</ref><ref type="bibr" target="#b0">Akbik et al., 2018)</ref> in recent years, which have achieved state-of-the-art performance in a number of sequence labeling tasks. However, these neural models usually require exhaustive human efforts for generating labels for each token, and may not perform well in lowresource settings. To improve the performance of low-resource sequence labeling, several approaches have been applied including using semi-supervised methods <ref type="bibr" target="#b6">(Clark et al., 2018;</ref><ref type="bibr" target="#b4">Chen et al., 2020b)</ref>, external weak supervision <ref type="bibr" target="#b24">(Lison et al., 2020;</ref><ref type="bibr" target="#b23">Liang et al., 2020;</ref><ref type="bibr" target="#b30">Ren et al., 2020;</ref><ref type="bibr" target="#b47">Zhang et al., 2019;</ref><ref type="bibr" target="#b45">Yu et al., 2020)</ref> and active learning <ref type="bibr" target="#b34">(Shen et al., 2017;</ref><ref type="bibr" target="#b15">Hazra et al., 2019;</ref><ref type="bibr" target="#b25">Liu et al., 2018;</ref><ref type="bibr" target="#b11">Fang et al., 2017;</ref><ref type="bibr" target="#b12">Gao et al., 2019)</ref>. In this study, we mainly focus on active learning approaches which select samples based on the query policy design. So far, various uncertainty-based <ref type="bibr" target="#b31">(Scheffer et al., 2001;</ref><ref type="bibr" target="#b7">Culotta and McCallum, 2005;</ref><ref type="bibr" target="#b19">Kim et al., 2006)</ref> and committee-based approaches <ref type="bibr" target="#b8">(Dagan and Engelson, 1995)</ref>   <ref type="formula">2017</ref>) further improve the aforementioned active learning approaches to improve the sampling diversity as well as the model's generalization ability on low-resource scenarios. These works mainly claim the sample efficiency provided by the active learning approach but do not study data augmentation for active sequence labeling.</p><p>Interpolation-based Regularizations Mixup implements interpolation in the input space to regularize models <ref type="bibr" target="#b46">(Zhang et al., 2018)</ref>. Recently, the Mixup variants <ref type="bibr" target="#b40">(Verma et al., 2019;</ref><ref type="bibr" target="#b36">Summers and Dinneen, 2019;</ref><ref type="bibr" target="#b14">Guo et al., 2019b)</ref> turn to perform interpolation in the hidden space to capture higher-level information. <ref type="bibr" target="#b13">Guo et al. (2019a)</ref>; <ref type="bibr" target="#b3">Chen et al. (2020a)</ref> apply hidden-space Mixup for text classification. These works, however, have not explored how to perform mixup for sequences with token-level labels, nor do they consider the quality of the mixed-up samples.</p><p>Text Augmentation Our work is also related to text data augmentation. <ref type="bibr" target="#b48">Zhang et al. (2015)</ref>; <ref type="bibr" target="#b41">Wei and Zou (2019)</ref> utilize heuristic approaches including synonym replancement, random insertion, swap and deletion for text augmentation, <ref type="bibr" target="#b18">Kafle et al. (2017)</ref>; <ref type="bibr" target="#b35">Silfverberg et al. (2017)</ref> employ heuristic rules based on specific task, <ref type="bibr" target="#b16">Hu et al. (2017)</ref> propose to augment text data in an encoder-decoder manner. Very recently, <ref type="bibr" target="#b1">(Anaby-Tavor et al., 2020;</ref><ref type="bibr" target="#b20">Kobayashi, 2018)</ref> harness the power of pre-trained language models and augmenting the text data based on contextual patterns. Although these methods can augment the training set and improve the performance of text classification model, they fail to generate sequences and labels simultaneously, thus cannot be adapted to our problem where tokenlevel labels are required during training. Instead, in our study, we propose a new framework SeqMix for data augmentation to facilitate sequence labeling task. Our method can generate token-level labels and preserve the semantic information in the augmented sentences. Moreover, it can be naturally combined with existing active learning approaches and further promote the performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Conclusion</head><p>We proposed a simple data augmentation method SeqMix to enhance active sequence labeling. By performing sequence mixup in the latent space, Se-qMix improves data diversity during active learning, while being able to generate plausible augmented sequences. This method is generic to different active learning policies and various sequence labeling tasks. Our experiments demonstrate that SeqMix can improve active learning baselines consistently for NER and event detection tasks; and its benefits are especially prominent in low-data regimes. For future research, it is interesting to enhance SeqMix with language models during the mixup process, and harness external knowledge for further improving diversity and plausibility.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head></head><label></label><figDesc>d(•) to measure the perplexity of them and filter low-quality sequences out. Then we generate the extra labeled sequences L * = SeqMix(L, α, ζ(•), d(•)) and get the augmented training set L = L ∪ L * . The sequence labeling model θ is initialized on this augmented training set L.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Algorithm 1</head><label>1</label><figDesc>The procedure of active sequence labeling augmentation via SeqMix Input: Labeled seed set L; Unlabeled set U; Query function ψ(•, K, γ(•)); The sequence labeling model θ; Beta distribution parameter α; Pairing function ζ(•); Discriminator function d(•). // seed set augmentation</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head></head><label></label><figDesc>(b)) to mix sub-sequences of the parent sequences. It scans the original samples with a window of fixed-length s to look for Algorithm 2 The generation procedure of SeqMix Input: Labeled set L = X , Y ; Beta distribution parameter α; Pairing function ζ(•); Discriminator function d(•); Number of expected generation N . for</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: The F 1 score of test set in terms of data usage on CoNLL-03, ACE05 and WebPage.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: The improvements to various active learning approaches provided by SeqMix.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 4 :</head><label>4</label><figDesc>Figure 4: Parameter Search for SeqMix</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head></head><label></label><figDesc>have been proposed for improving the sample efficiency. More recently, Shen et al. (2017); Hazra et al. (2019); Liu et al. (2018); Fang et al. (</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure 5 :</head><label>5</label><figDesc>Figure 5: A generation case of sub-sequence mixup.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 1 :</head><label>1</label><figDesc>The F 1 (%) of sub-sequence mixup with NTE sampling in different discriminator score range, evaluated on CoNLL-03 with 700 data.</figDesc><table><row><cell>Data Usage 200 300 400 500 600 700</cell></row><row><cell>(0, +∞) 81.15 82.32 82.74 83.66 83.79 85.05</cell></row><row><cell>(0, 2000) 80.20 82.24 83.21 83.67 83.90 85.11</cell></row><row><cell>(0, 1000) 80.13 81.86 83.58 84.22 84.81 85.16</cell></row><row><cell>(0, 500) 80.71 82.82 84.05 85.28 86.04 86.24</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 2 :</head><label>2</label><figDesc>The F 1 score with variant augment rate r. The value in the parentheses is the difference with the average F 1 for corresponding data usage. The last column presents the average F 1 difference for each learning rate r.</figDesc><table><row><cell>0.85</cell><cell></cell><cell></cell></row><row><cell>0.2 0.65 0.70 0.75 0.80 F 1 score</cell><cell>0.4</cell><cell>0.6 Data Usage 0.8 Random Least Confidence (LC) Normalized Token Entropy (NTE) 1.0 Random + sub-sequence mixup LC + sub-sequence mixup NTE + sub-sequence mixup</cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_0">We take the English version as our target corpus.</note>
		</body>
		<back>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A Information for Dataset</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.1 Dataset Collection</head><p>Here we list the link to datasets used in our experiments.</p><p>• CoNLL-03: https://github.com/ synalp/NER/tree/master/corpus/ CoNLL-2003.</p><p>• ACE05: We are unable to provide the downloadable version due to it is not public. This corpus can be applied through the website of LDC: https://www.ldc.upenn.edu/ collaborations/past-projects/ ace.</p><p>• Webpage: Please refer the link in the paper <ref type="bibr" target="#b29">(Ratinov and Roth, 2009)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.2 Dataset Split</head><p>All the mentioned dataset has been split into train/validate/test set in the released version. We keep consistent with the validation set and the test set in our experiment. For the active learning paradigm, we split the training set as Table <ref type="table">3</ref>.</p><p>The active learners are initialized on the seed set, then they implement 5 active learning rounds.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B Baseline Settings</head><p>For the baselines, we take random sampling and 3 active learning approaches -LC sampling, NTE sampling, and QBC sampling as Section 2.2.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C Implementation Details of SeqMix</head><p>We implement bert-base-cased as the underlying model for the NER task and bert-base-multilingualcased as the underlying model for the event detection task. We use the model from Huggingface Transformer codebase 3 , and the repository 4 to finetune our model for sequence labeling task.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C.1 Number of Parameters</head><p>In our model, we use bert-base-cased and bertbase-multilingual-cased both of them occupy 12layer, 768-hidden, 12-heads with 110M parameters.</p><p>3 https://github.com/huggingface/ transformers 4 https://github.com/kamalkraj/BERT-NER</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C.2 Adapting BERT for sequence labeling task</head><p>To fine-tune on sequence labeling tasks, a dropout layer (p = 0.1) and a linear (token-level) classification layer is built upon the pre-trained model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C.3 SeqMix Details</head><p>In Section 3.2, we construct a table of tokens W and their corresponding contextual embedding E.</p><p>For our underlying BERT model, we use the vocabulary provided by the tokenizer to build up W, and the embedding initialized on the training set as E.</p><p>We also need to construct a special token collection to exclude some generation in the process of sequence mixing. For example, BERT places token [CLS] and [SEP] at the starting position and the ending position for sentence, and pad the inputs with [PAD]. We exclude these disturbing tokens and the parent tokens.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C.4 Parameter Settings</head><p>The key parameters setting in our framework are stated here: (1) The number of active learning round is 5 for all the three datasets, but the size of seed set and the number of samples in each round differs from the dataset. We list the specific numbers as Table <ref type="table">3</ref>. (2) The sub-sequence window length s and the valid label density threshold η 0 vary from the datasets. For CoNLL-03, s = 5, η 0 = 0.6; for ACE05, s = 5, η 0 = 0.2; for Web-Page, s = 4, η 0 = 0.5. (3) We set α = 8 for the Beta distribution. (4) The discriminator score range is set as (0, 500) for all the datasets. (5) For BERT configuration, we choose 5e-5 for learning rate, 128 for padding length, 32 for batch size, 0.1 for dropout rate, 1e-8 for in Adam. At each data usage point, we train the model for 10 Epochs. ( <ref type="formula">6</ref>) We set C = 3 for the QBC query policy.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D Details of Experiments</head><p>We take following criteria to evaluate the sequence labeling task. A named entity is correct only if it is an exact match of the corresponding entity in the data file. An event trigger is correct only if the span and type match with golden labels. Based on the above metric, we evaluate F 1 score in our experiments.  refers to the number of labeled data, excluding the augmentation data. Sub-sequence mixup is trained with (1+α) times data, where the α denotes the augment rate. Note that WebPage is a very limited dataset, there is a big difference between the performance on the validation set and the test set. We average each experiment by 5 times.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D.1 Performance on Development Set</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D.2 Computing Infrastructure</head><p>We implement our system on Ubuntu 18.04.3 LTS system. We run our experiments on an Intel(R) Xeon(R) CPU @ 2.30GHz and NVIDIA Tesla P100-PCIe with 16 GB HBM2 memory. The NVIDIA-SMI version is 418.67 and the CUDA version is 10.1.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D.3 Average Runtime</head><p>For the 5-round active learning with SeqMix augmentation, our program runs about 500 seconds for WebPage dataset, 1700 seconds for the CoNLL slicing dataset, and 3.5 hours for ACE 2005. If the QBC query policy used, all the runtime will be multiplied about 3 times.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D.4 Hyper parameter Search</head><p>For the discriminator score range, we first examine the perplexity score distribution of the CoNLL training set. Then determine an approximate score range (0, 2000) first. We linearly split score ranges below 2000 to conduct parameter study and report the representative ranges in Section 4.3. Given the consideration to the generation speed and the augment rate setting, we finally choose 500 as the upper limit rather than a too narrow score range setting.</p><p>For the mixing coefficient λ, we follow <ref type="bibr" target="#b46">(Zhang et al., 2018)</ref> to sample it from Beta(α, α) and explore α ranging from <ref type="bibr">[0.5, 16]</ref>. We present this parameter study in Section 4.4. The result shows different α did not influence the augmentation performance much.</p><p>For the augment rate and the valid tag density, we also have introduced the parameter study in Section 4.4.</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Contextual string embeddings for sequence labeling</title>
		<author>
			<persName><forename type="first">Alan</forename><surname>Akbik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Duncan</forename><surname>Blythe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Roland</forename><surname>Vollgraf</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 27th International Conference on Computational Linguistics</title>
				<meeting>the 27th International Conference on Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="1638" to="1649" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Do not have enough data? deep learning to the rescue!</title>
		<author>
			<persName><forename type="first">Ateret</forename><surname>Anaby-Tavor</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Boaz</forename><surname>Carmeli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Esther</forename><surname>Goldbraich</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Amir</forename><surname>Kantor</surname></persName>
		</author>
		<author>
			<persName><forename type="first">George</forename><surname>Kour</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Segev</forename><surname>Shlomov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Naama</forename><surname>Tepper</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Naama</forename><surname>Zwerdling</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The Thirty-Fourth AAAI Conference on Artificial Intelligence</title>
				<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="7383" to="7390" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Generating sentences from a continuous space</title>
		<author>
			<persName><forename type="first">R</forename><surname>Samuel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Luke</forename><surname>Bowman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Oriol</forename><surname>Vilnis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrew</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rafal</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Samy</forename><surname>Jozefowicz</surname></persName>
		</author>
		<author>
			<persName><surname>Bengio</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/K16-1002</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of The 20th SIGNLL Conference on Computational Natural Language Learning</title>
				<meeting>The 20th SIGNLL Conference on Computational Natural Language Learning</meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="10" to="21" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Mix-Text: Linguistically-informed interpolation of hidden space for semi-supervised text classification</title>
		<author>
			<persName><forename type="first">Jiaao</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zichao</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Diyi</forename><surname>Yang</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2020.acl-main.194</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics</title>
				<meeting>the 58th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2020">2020a</date>
			<biblScope unit="page" from="2147" to="2157" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">SeqVAT: Virtual adversarial training for semi-supervised sequence labeling</title>
		<author>
			<persName><forename type="first">Luoxin</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Weitong</forename><surname>Ruan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xinyue</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jianhua</forename><surname>Lu</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2020.acl-main.777</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics</title>
				<meeting>the 58th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2020">2020b</date>
			<biblScope unit="page" from="8801" to="8811" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Paraphrase generation for semi-supervised learning in NLU</title>
		<author>
			<persName><forename type="first">Eunah</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName><forename type="first">He</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">William</forename><forename type="middle">M</forename><surname>Campbell</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/W19-2306</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Workshop on Methods for Optimizing and Evaluating Neural Language Generation</title>
				<meeting>the Workshop on Methods for Optimizing and Evaluating Neural Language Generation</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="45" to="54" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Semi-supervised sequence modeling with cross-view training</title>
		<author>
			<persName><forename type="first">Kevin</forename><surname>Clark</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Minh-Thang</forename><surname>Luong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christopher</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Quoc</forename><surname>Le</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/D18-1217</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing</title>
				<meeting>the 2018 Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="1914" to="1925" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Reducing labeling effort for structured prediction tasks</title>
		<author>
			<persName><forename type="first">Aron</forename><surname>Culotta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrew</forename><surname>Mccallum</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 20th AAAI conference on Artificial intelligence</title>
				<meeting>the 20th AAAI conference on Artificial intelligence</meeting>
		<imprint>
			<date type="published" when="2005">2005</date>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page" from="746" to="751" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Committeebased sampling for training probabilistic classifiers</title>
		<author>
			<persName><forename type="first">Ido</forename><surname>Dagan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sean</forename><forename type="middle">P</forename><surname>Engelson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Machine Learning Proceedings</title>
				<imprint>
			<date type="published" when="1995">1995. 1995</date>
			<biblScope unit="page" from="150" to="157" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">BERT: Pre-training of deep bidirectional transformers for language understanding</title>
		<author>
			<persName><forename type="first">Jacob</forename><surname>Devlin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ming-Wei</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kenton</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kristina</forename><surname>Toutanova</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/N19-1423</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2019 Conference of the North American Chapter</title>
		<title level="s">Long and Short Papers</title>
		<meeting>the 2019 Conference of the North American Chapter</meeting>
		<imprint>
			<publisher>Human Language Technologies</publisher>
			<date type="published" when="2019">2019</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="4171" to="4186" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">The hitchhiker&apos;s guide to testing statistical significance in natural language processing</title>
		<author>
			<persName><forename type="first">Rotem</forename><surname>Dror</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gili</forename><surname>Baumer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Segev</forename><surname>Shlomov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics</title>
		<title level="s">and Roi Reichart</title>
		<meeting>the 56th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="1383" to="1392" />
		</imprint>
	</monogr>
	<note>Long Papers</note>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Learning how to active learn: A deep reinforcement learning approach</title>
		<author>
			<persName><forename type="first">Meng</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuan</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Trevor</forename><surname>Cohn</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/D17-1063</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing</title>
				<meeting>the 2017 Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="595" to="605" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Active entity recognition in low resource settings</title>
		<author>
			<persName><forename type="first">Ning</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nikos</forename><surname>Karampatziakis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rahul</forename><surname>Potharaju</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Silviu</forename><surname>Cucerzan</surname></persName>
		</author>
		<idno type="DOI">10.1145/3357384.3358109</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 28th ACM International Conference on Information and Knowledge Management</title>
				<meeting>the 28th ACM International Conference on Information and Knowledge Management</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="2261" to="2264" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">Augmenting data with mixup for sentence classification: An empirical study</title>
		<author>
			<persName><forename type="first">Hongyu</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yongyi</forename><surname>Mao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Richong</forename><surname>Zhang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1905.08941</idno>
		<imprint>
			<date type="published" when="2019">2019a</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Mixup as locally linear out-of-manifold regularization</title>
		<author>
			<persName><forename type="first">Hongyu</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yongyi</forename><surname>Mao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Richong</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the AAAI Conference on Artificial Intelligence</title>
				<meeting>the AAAI Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2019">2019b</date>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page" from="3714" to="3722" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">Active 2 learning: Actively reducing redundancies in active learning methods for sequence tagging</title>
		<author>
			<persName><forename type="first">Rishi</forename><surname>Hazra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shubham</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ambedkar</forename><surname>Dukkipati</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1911.00234</idno>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Toward controlled generation of text</title>
		<author>
			<persName><forename type="first">Zhiting</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zichao</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaodan</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ruslan</forename><surname>Salakhutdinov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Eric</forename><forename type="middle">P</forename><surname>Xing</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 34th International Conference on Machine Learning</title>
				<meeting>the 34th International Conference on Machine Learning</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="volume">70</biblScope>
			<biblScope unit="page" from="1587" to="1596" />
		</imprint>
	</monogr>
	<note>Proceedings of Machine Learning Research</note>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">Bidirectional lstm-crf models for sequence tagging</title>
		<author>
			<persName><forename type="first">Zhiheng</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wei</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kai</forename><surname>Yu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1508.01991</idno>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Data augmentation for visual question answering</title>
		<author>
			<persName><forename type="first">Kushal</forename><surname>Kafle</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mohammed</forename><surname>Yousefhussien</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christopher</forename><surname>Kanan</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/W17-3529</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 10th International Conference on Natural Language Generation</title>
				<meeting>the 10th International Conference on Natural Language Generation</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="198" to="202" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">MMR-based active machine learning for bio named entity recognition</title>
		<author>
			<persName><forename type="first">Seokhwan</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yu</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kyungduk</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jeong-Won</forename><surname>Cha</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gary</forename><surname>Geunbae</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lee</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Human Language Technology Conference of the NAACL</title>
				<meeting>the Human Language Technology Conference of the NAACL</meeting>
		<imprint>
			<date type="published" when="2006">2006</date>
			<biblScope unit="page" from="69" to="72" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Contextual augmentation: Data augmentation by words with paradigmatic relations</title>
		<author>
			<persName><forename type="first">Sosuke</forename><surname>Kobayashi</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/N18-2072</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2018 Conference of the North American Chapter</title>
		<title level="s">Short Papers</title>
		<meeting>the 2018 Conference of the North American Chapter</meeting>
		<imprint>
			<publisher>Human Language Technologies</publisher>
			<date type="published" when="2018">2018</date>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="452" to="457" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Conditional random fields: Probabilistic models for segmenting and labeling sequence data</title>
		<author>
			<persName><forename type="first">Andrew</forename><surname>John D Lafferty</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fernando Cn</forename><surname>Mccallum</surname></persName>
		</author>
		<author>
			<persName><surname>Pereira</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Eighteenth International Conference on Machine Learning</title>
				<meeting>the Eighteenth International Conference on Machine Learning</meeting>
		<imprint>
			<date type="published" when="2001">2001</date>
			<biblScope unit="page" from="282" to="289" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Neural architectures for named entity recognition</title>
		<author>
			<persName><forename type="first">Guillaume</forename><surname>Lample</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Miguel</forename><surname>Ballesteros</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sandeep</forename><surname>Subramanian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kazuya</forename><surname>Kawakami</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chris</forename><surname>Dyer</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/N16-1030</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2016 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</title>
				<meeting>the 2016 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="260" to="270" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Bond: Bert-assisted open-domain named entity recognition with distant supervision</title>
		<author>
			<persName><forename type="first">Chen</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yue</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Haoming</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Siawpeng</forename><surname>Er</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ruijia</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tuo</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chao</forename><surname>Zhang</surname></persName>
		</author>
		<idno type="DOI">10.1145/3394486.3403149</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 26th ACM SIGKDD International Conference on Knowledge Discovery &amp; Data Mining</title>
				<meeting>the 26th ACM SIGKDD International Conference on Knowledge Discovery &amp; Data Mining</meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="1054" to="1064" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Named entity recognition without labelled data: A weak supervision approach</title>
		<author>
			<persName><forename type="first">Pierre</forename><surname>Lison</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jeremy</forename><surname>Barnes</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics</title>
				<meeting>the 58th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="1518" to="1533" />
		</imprint>
	</monogr>
	<note>Aliaksandr Hubin, and Samia Touileb</note>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Learning how to actively learn: A deep imitation learning approach</title>
		<author>
			<persName><forename type="first">Ming</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wray</forename><surname>Buntine</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gholamreza</forename><surname>Haffari</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/P18-1174</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics</title>
		<title level="s">Long Papers</title>
		<meeting>the 56th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="1874" to="1883" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Semantic role labeling as sequential tagging</title>
		<author>
			<persName><forename type="first">Lluís</forename><surname>Màrquez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pere</forename><surname>Comas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Ninth Conference on Computational Natural Language Learning (CoNLL-2005)</title>
				<meeting>the Ninth Conference on Computational Natural Language Learning (CoNLL-2005)</meeting>
		<imprint>
			<date type="published" when="2005">2005</date>
			<biblScope unit="page" from="193" to="196" />
		</imprint>
	</monogr>
	<note>Jesús Giménez, and Neus Català</note>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Deep contextualized word representations</title>
		<author>
			<persName><forename type="first">Matthew</forename><surname>Peters</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mark</forename><surname>Neumann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mohit</forename><surname>Iyyer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Matt</forename><surname>Gardner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christopher</forename><surname>Clark</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kenton</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Luke</forename><surname>Zettlemoyer</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/N18-1202</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2018 Conference of the North American Chapter</title>
		<title level="s">Long Papers</title>
		<meeting>the 2018 Conference of the North American Chapter</meeting>
		<imprint>
			<publisher>Human Language Technologies</publisher>
			<date type="published" when="2018">2018</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="2227" to="2237" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Language models are unsupervised multitask learners</title>
		<author>
			<persName><forename type="first">Alec</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jeffrey</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rewon</forename><surname>Child</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><surname>Luan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dario</forename><surname>Amodei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">OpenAI Blog</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page">9</biblScope>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Design challenges and misconceptions in named entity recognition</title>
		<author>
			<persName><forename type="first">Lev</forename><surname>Ratinov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dan</forename><surname>Roth</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Thirteenth Conference on Computational Natural Language Learning</title>
				<meeting>the Thirteenth Conference on Computational Natural Language Learning</meeting>
		<imprint>
			<date type="published" when="2009">2009. CoNLL-2009</date>
			<biblScope unit="page" from="147" to="155" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Denoising multi-source weak supervision for neural text classification</title>
		<author>
			<persName><forename type="first">Wendi</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yinghao</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hanting</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><surname>Kartchner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Cassie</forename><surname>Mitchell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chao</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Findings of the 2020 Conference on Empirical Methods in Natural Language Processing</title>
				<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Active hidden markov models for information extraction</title>
		<author>
			<persName><forename type="first">Tobias</forename><surname>Scheffer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christian</forename><surname>Decomain</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stefan</forename><surname>Wrobel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Symposium on Intelligent Data Analysis</title>
				<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2001">2001</date>
			<biblScope unit="page" from="309" to="318" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">An analysis of active learning strategies for sequence labeling tasks</title>
		<author>
			<persName><forename type="first">Burr</forename><surname>Settles</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mark</forename><surname>Craven</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2008 Conference on Empirical Methods in Natural Language Processing</title>
				<meeting>the 2008 Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2008">2008</date>
			<biblScope unit="page" from="1070" to="1079" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Query by committee</title>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">S</forename><surname>Seung</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Opper</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Sompolinsky</surname></persName>
		</author>
		<idno type="DOI">10.1145/130385.130417</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Fifth Annual Workshop on Computational Learning Theory</title>
				<meeting>the Fifth Annual Workshop on Computational Learning Theory</meeting>
		<imprint>
			<date type="published" when="1992">1992</date>
			<biblScope unit="page" from="287" to="294" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Deep active learning for named entity recognition</title>
		<author>
			<persName><forename type="first">Yanyao</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hyokun</forename><surname>Yun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zachary</forename><surname>Lipton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yakov</forename><surname>Kronrod</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Animashree</forename><surname>Anandkumar</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/W17-2630</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2nd Workshop on Representation Learning for NLP</title>
				<meeting>the 2nd Workshop on Representation Learning for NLP</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="252" to="256" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Data augmentation for morphological reinflection</title>
		<author>
			<persName><forename type="first">Miikka</forename><surname>Silfverberg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Adam</forename><surname>Wiemerslage</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ling</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lingshuang</forename><surname>Jack</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mao</forename></persName>
		</author>
		<idno type="DOI">10.18653/v1/K17-2010</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the CoNLL SIGMORPHON 2017 Shared Task: Universal Morphological Reinflection</title>
				<meeting>the CoNLL SIGMORPHON 2017 Shared Task: Universal Morphological Reinflection</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="90" to="99" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Improved mixed-example data augmentation</title>
		<author>
			<persName><forename type="first">Cecilia</forename><surname>Summers</surname></persName>
		</author>
		<author>
			<persName><surname>Michael J Dinneen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2019 IEEE Winter Conference on Applications of Computer Vision</title>
				<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="1262" to="1270" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Introduction to the CoNLL-2003 shared task: Language-independent named entity recognition</title>
		<author>
			<persName><forename type="first">Erik</forename><forename type="middle">F</forename><surname>Tjong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kim</forename><surname>Sang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fien</forename><surname>De Meulder</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Seventh Conference on Natural Language Learning at HLT-NAACL 2003</title>
				<meeting>the Seventh Conference on Natural Language Learning at HLT-NAACL 2003</meeting>
		<imprint>
			<date type="published" when="2003">2003</date>
			<biblScope unit="page" from="142" to="147" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">An approach to text corpus construction which cuts annotation costs and maintains reusability of annotated data</title>
		<author>
			<persName><forename type="first">Katrin</forename><surname>Tomanek</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Joachim</forename><surname>Wermter</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Udo</forename><surname>Hahn</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2007 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning</title>
				<meeting>the 2007 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning</meeting>
		<imprint>
			<publisher>EMNLP-CoNLL</publisher>
			<date type="published" when="2007">2007</date>
			<biblScope unit="page" from="486" to="495" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Attention is all you need</title>
		<author>
			<persName><forename type="first">Ashish</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Noam</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Niki</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jakob</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Llion</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aidan</forename><forename type="middle">N</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ł</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Illia</forename><surname>Polosukhin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
				<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="5998" to="6008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Manifold mixup: Better representations by interpolating hidden states</title>
		<author>
			<persName><forename type="first">Vikas</forename><surname>Verma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alex</forename><surname>Lamb</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christopher</forename><surname>Beckham</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Amir</forename><surname>Najafi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ioannis</forename><surname>Mitliagkas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><surname>Lopez-Paz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 36th International Conference on Machine Learning</title>
				<meeting>the 36th International Conference on Machine Learning</meeting>
		<imprint>
			<publisher>PMLR</publisher>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="6438" to="6447" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">EDA: Easy data augmentation techniques for boosting performance on text classification tasks</title>
		<author>
			<persName><forename type="first">Jason</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kai</forename><surname>Zou</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/D19-1670</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)</title>
				<meeting>the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="6382" to="6388" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Individual comparisons by ranking methods</title>
		<author>
			<persName><forename type="first">Frank</forename><surname>Wilcoxon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Breakthroughs in statistics</title>
				<imprint>
			<date type="published" when="1992">1992</date>
			<biblScope unit="page" from="196" to="202" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<monogr>
		<author>
			<persName><forename type="first">Qizhe</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zihang</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Eduard</forename><surname>Hovy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Minh-Thang</forename><surname>Luong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Quoc V</forename><surname>Le</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1904.12848</idno>
		<title level="m">Unsupervised data augmentation for consistency training</title>
				<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Exploring pre-trained language models for event extraction and generation</title>
		<author>
			<persName><forename type="first">Sen</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dawei</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Linbo</forename><surname>Qiao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhigang</forename><surname>Kan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dongsheng</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics</title>
				<meeting>the 57th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="5284" to="5294" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Steam: Selfsupervised taxonomy expansion with mini-paths</title>
		<author>
			<persName><forename type="first">Yue</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yinghao</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiaming</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hao</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jimeng</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chao</forename><surname>Zhang</surname></persName>
		</author>
		<idno type="DOI">10.1145/3394486.3403145</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 26th ACM SIGKDD International Conference on Knowledge Discovery &amp; Data Mining</title>
				<meeting>the 26th ACM SIGKDD International Conference on Knowledge Discovery &amp; Data Mining</meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="1026" to="1035" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">mixup: Beyond empirical risk minimization</title>
		<author>
			<persName><forename type="first">Hongyi</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Moustapha</forename><surname>Cisse</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yann</forename><forename type="middle">N</forename><surname>Dauphin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><surname>Lopez-Paz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
				<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">How to invest my time: Lessons from human-in-the-loop entity extraction</title>
		<author>
			<persName><forename type="first">Shanshan</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lihong</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Eduard</forename><surname>Dragut</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Slobodan</forename><surname>Vucetic</surname></persName>
		</author>
		<idno type="DOI">10.1145/3292500.3330773</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 25th ACM SIGKDD International Conference on Knowledge Discovery &amp; Data Mining</title>
				<meeting>the 25th ACM SIGKDD International Conference on Knowledge Discovery &amp; Data Mining</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="2305" to="2313" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Character-level convolutional networks for text classification</title>
		<author>
			<persName><forename type="first">Xiang</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Junbo</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yann</forename><surname>Lecun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
				<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="649" to="657" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Generating text via adversarial training</title>
		<author>
			<persName><forename type="first">Yizhe</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhe</forename><surname>Gan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lawrence</forename><surname>Carin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS workshop on Adversarial Training</title>
				<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Deep learning for Chinese word segmentation and POS tagging</title>
		<author>
			<persName><forename type="first">Xiaoqing</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hanyang</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tianyu</forename><surname>Xu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing</title>
				<meeting>the 2013 Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2013">2013</date>
			<biblScope unit="page" from="647" to="657" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
