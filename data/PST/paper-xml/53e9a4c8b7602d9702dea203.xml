<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Volume Leases for Consistency in Large-Scale Systems</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Jian</forename><surname>Yin</surname></persName>
							<email>yin@cs.utexas.edu</email>
							<affiliation key="aff0">
								<orgName type="institution">University of Texas at Austin</orgName>
								<address>
									<postCode>78712</postCode>
									<settlement>Austin</settlement>
									<region>TX</region>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><roleName>Member, IEEE</roleName><forename type="first">Lorenzo</forename><surname>Alvisi</surname></persName>
							<email>lorenzo@cs.utexas.edu</email>
							<affiliation key="aff0">
								<orgName type="institution">University of Texas at Austin</orgName>
								<address>
									<postCode>78712</postCode>
									<settlement>Austin</settlement>
									<region>TX</region>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><roleName>Member, IEEE</roleName><forename type="first">Michael</forename><surname>Dahlin</surname></persName>
							<email>dahlin@cs.utexas.edu</email>
							<affiliation key="aff0">
								<orgName type="institution">University of Texas at Austin</orgName>
								<address>
									<postCode>78712</postCode>
									<settlement>Austin</settlement>
									<region>TX</region>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Calvin</forename><surname>Lin</surname></persName>
							<email>lin@cs.utexas.edu</email>
							<affiliation key="aff0">
								<orgName type="institution">University of Texas at Austin</orgName>
								<address>
									<postCode>78712</postCode>
									<settlement>Austin</settlement>
									<region>TX</region>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Volume Leases for Consistency in Large-Scale Systems</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">32C948A026CBEF4681692FF70B7E0D82</idno>
					<note type="submission">received 10 May 1998; revised 25 Sept. 1998.</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.3" ident="GROBID" when="2023-07-28T13:37+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Index TermsÐCache consistency</term>
					<term>lease</term>
					<term>volume</term>
					<term>fault tolerance</term>
					<term>scalable server</term>
					<term>file system</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>AbstractÐThis article introduces volume leases as a mechanism for providing server-driven cache consistency for large-scale, geographically distributed networks. Volume leases retain the good performance, fault tolerance, and server scalability of the semantically weaker client-driven protocols that are now used on the web. Volume leases are a variation of object leases, which were originally designed for distributed file systems. However, whereas traditional object leases amortize overheads over long lease periods, volume leases exploit spatial locality to amortize overheads across multiple objects in a volume. This approach allows systems to maintain good write performance even in the presence of failures. Using trace-driven simulation, we compare three volume lease algorithms against four existing cache consistency algorithms and show that our new algorithms provide strong consistency while maintaining scalability and fault-tolerance. For a trace-based workload of web accesses, we find that volumes can reduce message traffic at servers by 40 percent compared to a standard lease algorithm, and that volumes can considerably reduce the peak load at servers when popular objects are modified.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">INTRODUCTION</head><p>T O fulfill the promise of an environment in which essentially all human knowledge is available from a set of servers distributed across wide area networks, the data infrastructure must evolve from protocols optimized for one applicationÐbrowsersÐto protocols that support a range of more demanding applications. In the future, we expect data-intensive applications to extend beyond human-driven browsers to include program-driven agents, robots, distributed databases, and data miners that will place new demands on the data-distribution infrastructure. These new applications will require aggressive caching for acceptable performance, and they will not be as tolerant of cache inconsistencies as a browser. Unfortunately, current cache consistency protocols do not scale to large systems such as the web because of poor performance, weak consistency guarantees, or poor fault tolerance.</p><p>Cache consistency can be achieved through either clientdriven protocols, in which clients send messages to servers to determine if cached objects are current, or server-driven protocols, in which servers notify clients when data change. In either case, the challenge is to guarantee that a client read always returns the result of the latest completed write. Protocols that achieve this are said to be strongly consistent.</p><p>Client-driven protocols force caches to make a difficult choice. They must either poll the server on each access to cached data or risk supplying incorrect data. The first option, polling on each read, increases both the load on the server and the latency of each cache request; both effects can be significant in large scale systems because servers support many clients and polling latencies can be high. The other option, periodic polling, relaxes consistency semantics and allows caches to supply incorrect data. For example, web browsers account for weak consistency through a human-based error-correction protocol in which users manually press a ªreloadº button when they detect stale data. Weak consistency semantics may be merely annoying to a human, but they can cause parallel and distributed programs to compute incorrect results, and they complicate the use of aggressive caching or replication hierarchies because replication is not transparent to the application.</p><p>Server-driven protocols introduce three challenges of their own. First, strong consistency is difficult to maintain in the face of network or process failures because before modifying an object, a server using these protocols must contact all clients that cache that object. If there are many cached copies, it is likely that at least one client will be unreachable, in which case the server cannot complete the write without violating its consistency guarantees. Second, a server may require a significant amount of memory to track which clients cache which objects. Third, sending cache invalidation messages may entail large bursts of server activity when popular objects are modified.</p><p>In distributed file systems, the problems of server driven protocols were addressed by using leases <ref type="bibr" target="#b7">[8]</ref>, which specify a length of time during which servers notify clients of modifications to cached data. After a lease's timeout expires, a client must renew the lease by sending a message to the server before the client may access the cached object. Leases maintain strong consistency while allowing servers to make progress even if failures occur. If a server cannot contact a client, the server delays writes until the unreachable client's lease expires, at which time it becomes the client's responsibility to contact the server. Furthermore, leases free servers from notifying idle clients before modifying an object; this reduces both the size of the server state and the load sustained by the server when reads and writes are bursty.</p><p>Although leases provide significant benefits for file system workloads, they may be less effective in a wide area network (WAN). To amortize the cost of renewing a lease across multiple reads, a lease should be long enough that in the common case the cache can be accessed without a renewal request. Unfortunately, at least for browser workloads, repeated accesses to an object are often spread over minutes or more. When lease lengths are shorter than the time between reads, leases reduce to client polling. On the other hand, longer lease lengths reduce the three original advantages of leases.</p><p>In this article, we show how volume leases <ref type="bibr" target="#b20">[21]</ref> restore the benefits of leases for WAN workloads. Volume leases combine short leases on groups of files (volumes) with long leases on individual files. Under the volume leases algorithm, a client may access a cached object if it holds valid leases on both the object and the object's volume. This combination provides the fault-tolerance of short leases because when clients become unreachable, a server may modify an object once the short volume lease expires. At the same time, the cost of maintaining the leases is modest because volume leases amortize the cost of lease renewal over a large number of objects.</p><p>We examine three variations of volume leases: volume leases, volume leases with delayed invalidations, and best effort volume leases. In the delayed invalidations algorithm, servers defer sending object invalidation messages to clients whose volume leases have expired. This optimization reduces peaks in server load, and it can reduce overall load by batching invalidation messages and eliminating messages entirely in cases when clients never renew a volume lease. The third variation is motivated by the observation that some workloads do not require strict consistency but do prefer that clients observe fresh data. For example, when an important event occurs, a news service would like to invalidate stale cached copies of their front page quickly, but they may want to begin distributing the new front page immediately rather than wait until they have notified all customers that the old page is invalid. The best effort variation of volume leases uses relaxed consistency to satisfy such applications. We find that this approach can improve performance by allowing servers to utilize longer volume lease timeouts.</p><p>This article evaluates the performance of volume leases using trace-based simulation. We compare the volume algorithms with three traditional consistency algorithms: client polling, server invalidations, and server invalidations with leases. Our simulations demonstrate the benefits of volume leases. For example, volume leases with delayed invalidations can ensure that clients never see stale data and that servers never wait more than 100 sec to perform a write, all while using about the same number of messages as a standard invalidation protocol that can stall server writes indefinitely. Compared to a standard object lease algorithm that also bounds server write delays at 100 sec, this volume algorithm reduces message traffic by 40 percent.</p><p>The rest of this article is organized as follows. Section 2 describes traditional algorithms for providing consistency to cached data, and Section 3 describes our new volume lease algorithms. Section 4 discusses our experimental methodology, and Section 5 presents our experimental results. After discussing related work in Section 6, Section 7 summarizes our conclusions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">TRADITIONAL CONSISTENCY ALGORITHMS</head><p>This section reviews four traditional cache consistency algorithms. The first twoÐPoll Each Read and PollÐrely on client polling. The remaining algorithmsÐCallback and LeaseÐare based on server invalidation. In describing each algorithm we refer to Table <ref type="table">1</ref>, which summarizes key characteristics of each algorithm discussed in this paper, including our three new algorithms. We also refer to Fig. <ref type="figure" target="#fig_0">1</ref>, which defines several parameters of the algorithms.</p><p>In Table <ref type="table">1</ref>, we summarize the cost of maintaining consistency for an object o using each of the algorithms. Columns correspond to key figures of merit: the expected stale time indicates how long a client expects to read stale data after o is modified, assuming random reads, random updates, and random failures. The worst stale time indicates how long o can be cached and stale assuming that: 1. o was loaded immediately before it was modified, and 2. a network failure prevented the server from contacting the client caching o. The read cost shows the expected fraction of cache reads requiring a message to the server. The write cost indicates how many messages the server expects to send to notify clients of a write. The acknowledgment wait delay indicates how long the server will wait to write if it cannot invalidate a cache. The server state column indicates how many clients the server expects to track for each object.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Poll Each Read</head><p>Poll Each Read is the simplest consistency algorithm. Before accessing a cached object, a client asks the object's server if the object is valid. If so, the server responds affirmatively; if not, the server sends the current version.</p><p>This algorithm is equivalent to always having clients read data from the server with the optimization that unchanged data is not resent. Thus, clients never see stale data, and writes by the server always proceed immediately. If a network failure occurs, clients unable to contact a server have no guarantees about the validity of cached objects. To cope with network failures, clients take application-dependent actions, such as signaling an error or returning the cached data along with a warning that it may be stale.</p><p>The primary disadvantage of this algorithm is poor read performance, as all reads are delayed by a roundtrip message between the client and the server. In addition, these messages may impose significant load on the servers <ref type="bibr" target="#b10">[11]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Poll</head><p>Poll is based on Poll Each Read, but it assumes that cached objects remain valid for at least a timeout period of t seconds after a client validates the data. Hence, when t H, Poll is equivalent to Poll Each Read. Choosing the appropriate value of t presents a trade-off: On the one hand, long timeouts improve performance by reducing the number of reads that wait for validation. In particular, if a client accesses data at a rate of reads per second and the timeout is long enough to span several reads, then only I Át of the client's reads will require network messages (see Table <ref type="table">1</ref>). On the other hand, long timeouts increase the likelihood that caches will supply stale data to applications. Gwertzman and Seltzer <ref type="bibr" target="#b9">[10]</ref> show that for web browser workloads, even for a timeout of 10 days, server load is significantly higher than under the Callback algorithm described below. The same study finds that an adaptive timeout scheme works better than static timeouts, but that when the algorithm's parameters are set to make the adaptive timeout algorithm impose the same server load as Callback, about 4 percent of client reads receive stale data.</p><p>If servers can predict with certainty when objects will be modified, then Poll is ideal. In this case, servers can tell clients to use cached copies of objects until the time of the next modification. For this study, we do not assume that servers have such information about the future.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Callback</head><p>In a Callback algorithm <ref type="bibr" target="#b10">[11]</ref>, <ref type="bibr" target="#b15">[16]</ref>, servers keep track of which clients are caching which objects. Before modifying an object, a server notifies the clients with copies of the object and does not proceed with the modification until it has received an acknowledgment from each client. As shown in Table <ref type="table">1</ref>, Callback's read cost is low because a client is guaranteed that a cached object is valid until told otherwise. However, the write cost is high because when an object is modified the server invalidates the cached objects, which may require up to g tot messages. Furthermore, if a client has crashed or if a network partition separates a server from a client, then a write may be delayed indefinitely.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.4">Lease</head><p>To address the limitations of Callback, Gray and Cheriton proposed Lease <ref type="bibr" target="#b7">[8]</ref>. To read an object, a client first acquires a lease for it with an associated timeout t. The client may then read the cached copy until the lease expires. When an object is modified, the object's server invalidates the cached objects of all clients whose leases have not expired. To read the object after the lease expires, a client first contacts the server to renew the lease.</p><p>Lease allows servers to make progress while maintaining strong consistency despite failures. If a client or network failure prevents a server from invalidating a client's cache, the server need only wait until the lease expires before performing the write. By contrast, Callback may force the write to wait indefinitely.</p><p>Leases also improve scalability of writes. Rather than contacting all clients that have ever read an object, a server need only contact recently active clients that hold leases on that object. Leases can thus reduce the amount of state that the server maintains to track clients, as well as the cost of sending invalidation messages <ref type="bibr" target="#b13">[14]</ref>. Servers may also choose to invalidate caches by simply waiting for all outstanding leases to expire rather than by sending messages to a large number of clients; we do not explore this option in this study. Lease presents a trade-off similar to the one offered by Poll. Long leases reduce the cost of reads by amortizing each lease renewal over Á t reads. On the other hand, short leases reduce the delay on writes when failures occur.</p><p>As with polling, a client that is unable to contact a server to renew a lease knows that it holds potentially stale data. The client may then take application-specific actions, such as signaling an error or returning the suspect data along with a warning. However, unlike Poll, Lease never lets clients believe that stale objects are valid.  <ref type="table">1</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">VOLUME LEASES</head><p>Traditional leases provide good performance when the cost of renewing leases is amortized over many reads. Unfortunately, for many WAN workloads, reads of an object may be spread over seconds or minutes, requiring long leases in order to amortize the cost of renewals <ref type="bibr" target="#b9">[10]</ref>. To make leases practical for these workloads, our algorithms use a combination of object leases, which are associated with individual data objects, and volume leases, which are associated with a collection of related objects on the same server. In our scheme, a client reads data from its cache only if both its object and volume leases for that data are valid, and a server can modify data as soon as either lease has expired. By making object leases long and volume leases short, we overcome the limitations of traditional leases: long object leases have low overhead, while short volume leases allow servers to modify data without long delays. Furthermore, if there is spatial locality within a volume, the overhead of renewing short leases on volumes is amortized across many objects. This section first describes the Volume Leases algorithm and then examines a variation called Volume Leases with Delayed Invalidations. At the end of this section, we examine Best Effort Volume Leases to support applications where timely updates are desired, but not required.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">The Basic Algorithm</head><p>Fig. <ref type="figure" target="#fig_1">2</ref>, Fig. <ref type="figure" target="#fig_3">3</ref>, and Fig. <ref type="figure" target="#fig_4">4</ref> show the data structures used by the Volume Leases algorithm, the server side of the algorithm, and the client side of the algorithm, respectively. The basic algorithm is simple:</p><p>. Reading Data. Clients read cached data only if they hold valid object and volume leases on the corresponding objects. Expired leases are renewed by contacting the appropriate servers. When granting a lease for an object o to a client , if o has been modified since the last time held a valid lease on o then the server piggybacks the current data on the lease renewal. . Writing Data. Before modifying an object, a server sends invalidation messages to all clients that hold valid leases on the object. The server delays the write until it receives acknowledgments from all clients, or until the volume or object leases expire. After modifying the object, the server increments the object's version number.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.1">Handling Unreachable Clients</head><p>Client crashes or network partitions can make some clients temporarily unreachable, which may cause problems. Consider the case of an unreachable client whose volume lease has expired but that still holds a valid lease on an object. When the client becomes reachable and attempts to renew its volume lease, the server must invalidate any modified objects for which the client holds a valid object lease. Our algorithm thus maintains at each server an Unreachable set that records the clients that have not acknowledgedÐwithin some timeout periodÐone of the server's invalidation messages.</p><p>After receiving a read request or a lease renewal request from a client in its Unreachable set, a server removes the client from its Unreachable set, renews the client's volume lease, and notifies the client to renew its leases on any currently cached objects belonging to that volume. The client then responds by sending a list of objects along with their version numbers, and the server replies with a message that contains a vector of object identifiers. This message: 1. renews the leases of any objects not modified while the client was unreachable, and 2. invalidates the leases of any objects whose version number changed while the client was unreachable.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.2">Handling Server Failures</head><p>When a server fails we assume that the state used to maintain cache consistency is lost. In LAN systems, servers often reconstruct this state by polling their clients <ref type="bibr" target="#b15">[16]</ref>. This approach is impractical in a WAN, so our protocol allows a server to incrementally construct a valid view of the object lease state, while relying on volume lease expiration to prevent clients from using leases that were granted by a failed server. To recover from a crash, a server first invalidates all volume leases by waiting for them to expire. This invalidation can be done in two ways. A server can save on stable storage the latest expiration time of any volume lease. Then, upon recovery, it reads this timestamp and delays all writes until after this expiration time. Alternatively, the server can save on stable storage the duration of the longest possible volume lease. Upon recovery, the server then delays any writes until this duration has passed.</p><p>Since object lease information is lost when a server crashes, the server effectively invalidates all object leases by treating all clients as if they were in the Unreachable set. It is also possible to store the cache consistency information on stable storage <ref type="bibr" target="#b4">[5]</ref>, <ref type="bibr" target="#b8">[9]</ref>. This approach reduces recovery time at the cost of increased overhead on normal lease renewals. We do not investigate this approach in this paper.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.3">The Cost of Volume Leases</head><p>To analyze Volume Leases, we assume that servers grant leases of length t v on volumes and of length t on objects. Typically, the volume lease is much shorter than the object leases, but when a client accesses multiple objects from the same volume in a short amount of time, the volume lease is likely to be valid for all of these accesses. As the read cost column of Table <ref type="table">1</ref> indicates, the cost of a typical read, measured in messages per read, is</p><formula xml:id="formula_0">I oP o t v I Á t X</formula><p>The first term reflects the fact that the volume lease must be renewed every t v sec but that the renewal is amortized over all objects in the volume, assuming that object o is read o times per second. The second term is the standard cost of renewing an object lease. As the acknowledge wait delay column indicates, if a client or network failure prevents a server from contacting a client, a write to an object must be delayed for mintY t v , i.e., until either lease expires. As the write cost and server state columns indicate, servers track all clients that hold valid object leases and notify them all when objects are modified. Finally, as the stale time columns indicate, Volume Leases never supplies stale data to clients.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.4">Protocol Verification</head><p>To verify the correctness of the consistency algorithm, we implemented a variation of the volume leases algorithm described in Fig. <ref type="figure" target="#fig_3">3</ref> and Fig. <ref type="figure" target="#fig_4">4</ref> using the Teapot system <ref type="bibr" target="#b3">[4]</ref>.</p><p>The Teapot version of the algorithm differs from the one described in the figures in two ways. First, the Teapot version uses a simplified reconnection protocol for Unreachable clients. Rather than restore a client's set of object leases, the Teapot version clears all of the client's object leases when an Unreachable client reconnects. The second difference is that in the Teapot version every network request includes a sequence number that is repeated in the corresponding reply. These sequence numbers allow the protocol to match replies to requests. Teapot allows us to describe the consistency state machines in a convenient syntax and then to generate Murphi <ref type="bibr" target="#b6">[7]</ref> code for mechanical verification. The Murphi system searches the protocol's state space for deadlocks or cases where the system's correcness invariants are violated. Although Murphi's exhaustive search of the state space is an exponential algorithm that only allows us to verify small models of the system, in practice this approach finds many bugs that are difficult to locate by hand and gives us confidence in the correctness of our algorithm <ref type="bibr" target="#b2">[3]</ref>.</p><p>Murphi verifies that the following two invariants hold: 1. when the server writes an object, no client has both a valid object lease and a valid volume lease for that object, and 2. when a client reads an object, it has the current version of the object. The system we verified contains one volume with two objects in it, and it includes one client and one server that communicate over a network. Clients and servers can crash at any time, and the network layer can lose messages at any time but cannot deliver messages out of order; the network layer can also report messages lost when they are, in fact, delivered. We have tested portions of the state space for some larger models, but larger models exhaust our test machine's 1 GByte of memory before the entire state space is examined.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Volume Leases with Delayed Invalidations</head><p>The performance of Volume Leases can be improved by recognizing that once a volume lease expires, a client cannot use object leases from that volume without first contacting the server. Thus, rather than invalidating object leases immediately for clients whose volume leases have expired, the server can send invalidation messages when (and if) the client renews the volume lease. In particular, the Volume Leases with Delayed Invalidations algorithm modifies Volume Leases as follows. If the server modifies an object for which a client holds a valid object lease but an expired volume lease, the server moves the client to a per-volume Inactive set, and the server appends any object invalidations for inactive clients to a per-inactive-client Pending Message list. When an inactive client renews a volume, the server sends all pending messages to that client and waits for the client's acknowledgment before renewing the volume. After a client has been inactive for d seconds, the server moves the client from the Inactive set to the Unreachable set and discards the client's Pending Message list. Thus, d limits the amount of state stored at the server. Small values for d reduce server state but increase the cost of re-establishing volume leases when unreachable clients become reconnected.</p><p>As Table <ref type="table">1</ref> indicates, when a write occurs, the server must contact the g v clients that hold valid volume leases rather than the g o clients that hold valid object leases. Delayed invalidations provide three advantages over Volume Leases. First, server writes can proceed faster because many invalidation messages are delayed or omitted. Second, the server can batch several object invalidation messages to a client into a single network message when the client renews its volume lease, thereby reducing network overhead. Third, if a client does not renew a volume for a long period of time, the server can avoid sending the object invalidation messages by moving the client to the Unreachable set and using the reconnection protocol if the client ever returns.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Best-Effort Volume Leases</head><p>Some applications do not require strong consistency but do want to deliver timely updates to clients. For example, when an important event occurs, a news service would like to invalidate stale copies of their front page quickly rather than wait until all customers know that the old page is invalid. Thus, it is interesting to consider best-effort algorithms. A best effort algorithm should always allow writes to proceed immediately, and it should notify clients of writes when doing so does not delay writes.</p><p>Any of the volume algorithms may be converted to best effort algorithms by sending invalidations in parallel with writes. Table <ref type="table">1</ref> summarizes the characteristics of the best effort version of the Delayed Invalidations algorithm. By sending invalidations in parallel with writes, the algorithm limits the expected stale read time to notifyg v Ðthe time it takes for the server to send the messagesÐwithout delaying writes.</p><p>Note that in the best effort algorithms, volume leases serve a different purpose than in the original volume algorithms: they limit the time during which clients can see stale data. Whereas strong consistency algorithms generally set the volume lease time (t v ) to be the longest period they are willing to delay a write, this is no longer a factor for best effort algorithms. Instead, these algorithms set t v to the longest time they will allow disconnected clients to unknowingly see stale data. Since only the disconnected clients are affected by long t v values, this may allow larger values for t v than before. For example, a news service using strong consistency might not want to block dissemination of a news update for more than a few seconds, but it may be willing to allow a few disconnected clients to see the old news for several minutes. Thus, such a system might use t v IH se under strong consistency, but it might use t v IH minutes under a best effort algorithm. As with the original volume algorithms, combining short volume leases with long object leases allows leases to be short while amortizing renewal costs over many objects.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">METHODOLOGY</head><p>To examine the algorithms' performance, we simulated each algorithm discussed in Table <ref type="table">1</ref> under a workload based on web trace data.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Simulator</head><p>We simulate a set of servers that modify files and provide files to clients, and a set of clients that read files. The simulator accepts timestamped read and modify events from input files and updates the cache state. The simulator records the size and number of messages sent by each server and each client, as well as the size of the cache consistency state maintained at each server.</p><p>We validated the simulator in two ways. First, we obtained Gwertzman and Seltzer's simulator <ref type="bibr" target="#b9">[10]</ref> and one of their traces, and compared our simulator's results to theirs for the algorithms that are common between the two studies. Second, we used our simulator to examine our algorithms under simple synthetic workloads for which we could analytically compute the expected results. In both cases, our simulator's results match the expected results.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.1">Limitations of the Simulator</head><p>Our simulator makes several simplifying assumptions. First, it does not simulate concurrencyÐit completely processes each trace event before processing the next one. This simplification allows us to ignore details such as mutual exclusion on internal data structures, race conditions, and deadlocks. Although this could change the messages that are sent (if, for instance, a file is read at about the same time it is written), we do not believe that simulating these details would significantly affect our performance results.</p><p>Second, we assume infinitely large caches and we do not simulate server disk accesses. Both of these effects reduce potentially significant sources of work that are the same across algorithms. Thus, our results will magnify the differences among the algorithms.</p><p>Finally, we assume that the system maintains cache consistency on entire files rather than on some finer granularity. We chose to examine whole-file consistency because this is currently the most common approach for WAN workloads <ref type="bibr" target="#b0">[1]</ref>. Fine-grained consistency may reduce the amount of data traffic, but it also increases the number of control messages required by the consistency algorithm. Thus, fine-grained cache consistency would likely increase the relative differences among the algorithms.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Workload</head><p>We use a workload based on traces of HTTP accesses at Boston University <ref type="bibr" target="#b5">[6]</ref>. These traces span four months from January 1995 through May 1995 and include all HTTP accesses by Mosaic browsersÐincluding local cache hitsÐfor 33 SPARCstations.</p><p>Although these traces contain detailed information about client reads, they do not indicate when files are modified. We therefore synthesize writes to the objects using a simple model based on two studies of write patterns for web pages. Bestavros <ref type="bibr" target="#b1">[2]</ref> examined traces of the Boston University web server, and Gwertzman and Seltzer <ref type="bibr" target="#b9">[10]</ref> examined the write patterns of three university web servers. Both studies concluded that few files change rapidly, and that globally popular files are less likely to change than other files. For example, Gwertzman and Seltzer's study found that 2 to 23 percent of all files were mutable (each file had a greater than 5 percent chance of changing on any given day) and 0 to 5 percent of the files were very mutable (had greater than 20 percent chance of changing during a 24-hour period).</p><p>Based on these studies, our synthetic write workload divides the files in the trace into four groups. We give the 10 percent most referenced files a low average number of random writes per day (we use a Poisson distribution with an expected number of writes per day of 0.005). We then randomly place the remaining 90 percent of the files into three sets. The first set, which includes 3 percent of all files in the trace, are very mutable and have an expected number of writes per day of 0.2. The second set, 10 percent of all files in the trace, are mutable and have an expected number of writes per day of 0.05. The remaining 77 percent of the files have an expected number of writes per day of 0.02. In Section 5.4, we examine the sensitivity of our results to these parameters.</p><p>We simulate the 1,000 most frequently accessed servers; this subset of the servers accounts for more than 90 percent of all accesses in the trace. Our workload consists of 977,899 reads of 68,665 different files, plus 209,461 artificially generated writes to those files. The files in the workload are grouped into 1,000 volumes corresponding to the 1,000 servers. We leave more sophisticated grouping as future work.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">SIMULATION RESULTS</head><p>This section presents simulation results that compare the volume algorithms with other consistency schemes. In interpreting these results, remember that the trace workload tracks the activities of a relatively small number of clients. In reality, servers would be accessed by many other clients, so the absolute values we report for server and network load are lower than what the servers would actually experience. Instead of focusing on the absolute numbers in these experiments, we focus on the relative performance of the algorithms under this workload.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Server/Network Load</head><p>Fig. <ref type="figure" target="#fig_5">5</ref> shows the performance of the algorithms. The xaxis, which uses a logarithmic scale, gives the object timeout length in seconds (t) used by each algorithm, while the y-axis gives the number of messages sent between the client and servers. For Volume Lease, t refers to the object lease timeout and not the volume lease timeout; we use different curves to show different volume lease timeouts and indicate the volume lease time (t v ) in the second parameter of the label. For the Delay Volume lines, we assume an infinite acknowledgement wait delay (d) as signified by the third parameter; this means that a server never moves idle clients to the unreachable list. The line for Callback is flat because Callback invalidates all cached copies regardless of t. The Lease and basic Volume Lease lines decline until t reaches about 100,000 sec and then rise slightly. This shape comes from two competing influences. As t rises, the number of lease renewals by clients declines, but the number of invalidations sent to clients holding valid leases increases. For this workload, once a client has held an object for 100,000 sec, it is more likely that the server will modify the object than that the client will read it, so leases shorter than this reduce system load. As t increases, Client Poll and Delayed Invalidation send strictly fewer messages. Client Poll never sends invalidation messages, and Delayed Invalidation avoids sending invalidations to clients that are no longer accessing a volume, even if the clients hold valid object leases. Note that for timeouts of 100,000 sec, Client Poll results in clients accessing stale data on about 1 percent of all reads, and for timeout values of 1,000,000 sec, the algorithm results in clients accessing stale copies on about 5 percent of all reads.</p><p>The separation of the Lease(t), Volume(tY t v IH), and Volume(tY t v IHH) lines shows the additional overhead of maintaining volume leases. Shorter volume timeouts increase this overhead. Lease can be thought of as the limiting case of infinite-length volume leases.</p><p>Although Volume Leases imposes a significant overhead compared to Lease for a given value of t, applications that care about fault tolerance can achieve better performance with Volume Leases than without. For example, the triangles in the figure highlight the best performance achievable by a system that does not allow writes to be delayed for more than 10 sec for Lease(t), Volume(tY t v IH), and Delayed Invalidations(tY t v IHY d I). Volume(t IHHY HHHY t v IH) sends 32 percent fewer messages than Lease(t IH), and Delayed Invalidations(t IH U Y t v IHY d I) sends 39 percent fewer messages than Lease(t IH). Similarly, as indicated by the squares in the figure, for applications that can delay writes at most 100 sec, Volume Lease outperforms Lease by 30 percent and Delayed Invalidations outperforms the lease algorithm by 40 percent.</p><p>Although providing strong consistency is more expensive than the Poll algorithm, the cost appears tolerable for many applications. For example, Poll (t IHHY HHH) uses about 15 percent fewer messages than Delayed Invalidations (t IH U Y t v IHHY d I), but it supplies stale data to clients on about 1 percent of all reads. Even in the extreme case of Poll (t IH U ) (in which clients see stale data on over 35 percent of reads), Delayed Invalidations uses less than twice as many messages as the polling algorithm.</p><p>We also examined the network bytes sent by these algorithms and the server CPU load imposed by these algorithms. By both of these metrics, the difference in cost of providing strong consistency compared to Poll was smaller than the difference by the metric of network messages. The relative differences among the lease algorithms was also smaller for these metrics than for the network messages metric for the same reasons.</p><p>A key advantage of Best Effort Volume Leases for applications that permit relaxed consistency is the algorithm may enable longer volume lease timeouts and thus may reduce consistency overhead. Strict consistency algorithms set the volume timeout, t v , to be the longest tolerable write delay, but the best effort algorithms can set t v to be the longest time disconnected clients should be allowed to unknowingly access stale data; this may allow larger values of t v for some services that use Best Effort. Fig. <ref type="figure">6</ref> shows the effect of varying the volume lease timeout on the number of messages sent.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Server State</head><p>Fig. <ref type="figure">7</ref> and Fig. <ref type="figure" target="#fig_7">8</ref> show the amount of server memory required to implement the algorithms. The first shows the requirements at the trace's most heavily loaded server, and the second shows the demand at the trace's tenth most heavily loaded server. The x-axis shows the timeout in seconds using a log scale. The y-axis is given in bytes and represents the average number of bytes of memory used by the server to maintain consistency state. We charge the servers 16 bytes to store an object or volume lease or callback record. For messages queued by the Delay algorithm, we also charge 16 bytes.</p><p>For short timeouts, the lease algorithms use less memory than the callback algorithm because the lease algorithms discard callbacks for inactive clients. Compared to standard leases, Volume Leases increase the amount of state needed at servers, but this increase is small because volume leases are short, so servers generally maintain few active volume leases. If the Delay algorithm never moves clients to the Unreachable set it may store messages destined for inactive clients for a long time and use more memory than the other algorithms. Conversely, if Delay uses a short d parameter so that it can move clients from the Inactive set to the Unreachable set and discard their pending messages and callbacks, Delay can use less state than the other lease or callback algorithms. Note that running Delay with short discard times will increase server load and the number of consistency messages. We have not yet quantified this effect because it will depend on implementation details of the reconnection protocol.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3">Bursts of Load</head><p>Fig. <ref type="figure" target="#fig_8">9</ref> shows a cumulative histogram in which the y value, shown in log scale, counts the number of 1-sec periods in which the load at the server was at least x messages sent or received per second. There are three groups of lines. Client Poll and Object Lease both use short timeouts, so when clients read groups of objects from a server, these algorithms send groups of object renewal messages to the server. Callback and Volume use long object lease periods, so read traffic puts less load on the server, but writes result in bursts of load when popular objects are modified. For this workload, peak loads correspond to bursts of about one message per client. Finally, Delay uses long object leases to reduce bursts of read traffic from clients accessing groups of objects, and it delays sending invalidation messages to reduce bursts of traffic when writes occur. This combination reduces the peak load on the server for this workload.</p><p>For the experiment described in the previous paragraph, Client Poll and Object Lease have periods of higher load than Callback and Volume for two reasons. First, the system shows performance for a modest number of clients. Larger numbers of clients would increase the peak invalidate load for Callback and Volume. For Client Poll and Object Lease, increasing the number of clients would increase peak server load less dramatically because read requests from additional clients would be more spread out in time. The second reason for Callback and Volume's advantage in this experiment is that in the trace clients read data from servers in bursts, but writes to volumes are not bursty in that a write to one object in a volume does not make it more likely that another object from the same volume will soon be modified. Conversely, Fig. <ref type="figure" target="#fig_0">10</ref> shows a ªbursty writeº workload in which when one object is modified, we select k other objects from the same volume to modify at the same time. For this graph, we compute k as a random exponential variable with a mean of 10. This workload significantly increases the bursts of invalidation traffic for Volume and Callback.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.4">Sensitivity</head><p>Our workload utilizes a trace of read events, but it generates write events synthetically. In this subsection, we examine how different assumptions about write frequency affect our results. Fig. <ref type="figure" target="#fig_0">11</ref> shows the performance of the algorithms for representative parameters as we vary the write frequency. Our default workload gives the 10 percent most referenced files a per-day change probability of 0.5 percent, 3 percent of the files a per-day change probability of 20 percent, 10 percent of the files a probability of 5 percent, and 77 percent of the files a per-day change probability of 2 percent. For each point on the graph, we multiply those per-day probabilities by the value indicated by the x-axis. Note that our workload generator converts per-day change probabilities to per-second change probabilities, so per-day probabilities greater than 100 percent are possible.</p><p>We examine the lease algorithms as they might be parameterized in a system that never wishes to delay writes more than 100 sec and compare to a poll algorithm with a 100-sec timeout and a callback algorithm with infinite timeout. These results indicate that the Client Poll(t IHH) and Lease(t IHH) are little affected by changing write rates. This is because the object timeouts are so short that writes are unlikely to cause many invalidations even when their frequency is increased 100-fold. The volume lease algorithms and Callback all cost more as write frequency increases. The cost of Volume(t IY HHHY HHHY t v IHH) and Callback increase more quickly than the cost of Delayed Volume(t IHY HHHY HHHY t v IHHY d I) because the first two algorithms have long object callback periods and thus send invalidation messages to all clients that have done reads between a pair of writes. Delayed Volume rises more slowly because it does not send object invalidations once a volume lease expires.  Our study builds on efforts to assess the cost of strong consistency in wide area networks. Gwertzman and Seltzer <ref type="bibr" target="#b9">[10]</ref> compare cache consistency approaches through simulation and conclude that protocols that provide weak consistency are the most suitable to a Web-like environment. In particular, they find that an adaptive version of Poll(t) exerts a lower server load than an invalidation protocol if the polling algorithm is allowed to return stale data 4 percent of the time. We arrive at different conclusions. In particular, we observe that much of the apparent advantage of weak consistency over strong consistency in terms of network traffic comes from clients reading stale data <ref type="bibr" target="#b13">[14]</ref>. Also, we use volume leases to address many of the challenges to strong consistency.</p><p>We also build on the work of Liu and Cao <ref type="bibr" target="#b13">[14]</ref>, who use a prototype server invalidation system to evaluate the overhead of maintaining consistency at the servers compared to client polling. They also study ways to reduce server state via per-object leases. As with our study, their workload is based on a trace of read requests and synthetically generated write requests. Our work differs primarily in our treatment of fault tolerance issues. In particular, after a server recovers our algorithm uses volume timeouts to ªnotifyº clients that they must contact the server to renew leases; Liu and Cao's algorithm requires the server to send messages to all clients that might be caching objects from the server. Also, our volume leases provide a graceful way to handle network partitions; when a network failure occurs, Liu and Cao's algorithm must periodically retransmit invalidation messages, and it does not guarantee strong consistency in that case.</p><p>Cache consistency protocols have long been studied for distributed file systems <ref type="bibr" target="#b10">[11]</ref>, <ref type="bibr" target="#b15">[16]</ref>, <ref type="bibr" target="#b17">[18]</ref>. Several aspects of Coda's <ref type="bibr" target="#b12">[13]</ref> consistency protocol are reflected in our algorithms. In particular, our notion of a volume is similar to used in Coda <ref type="bibr" target="#b14">[15]</ref>. However, ours differ in two key respects. First, Coda does not associate volumes with leases, and relies instead on other methods to determine when servers and clients become disconnected. The combination of short volume leases and long object leases is one of our main contributions. Second, because Coda was designed for different workloads, its design trade-offs are different. For example, because Coda expects clients to communicate with a small number of servers and it regards disconnection as a common occurrence, Coda aggressively attempts to set up volume callbacks to all servers on each hoard walk (every 10 minutes). In our environment, clients are associated with a larger universe of servers, so we only renew volume leases when a client is actively accessing the server. Also, in our algorithm when an object is modified, the server does not send volume invalidation messages to clients that hold volume leases but not object leases on the object in question. We thus avoid the false sharing problem of which Mummert and Satyanarayanan warn <ref type="bibr" target="#b14">[15]</ref>.</p><p>Our best effort leases algorithm provides similar semantics to and was inspired by Coda's optimistic concurrency protocol <ref type="bibr" target="#b12">[13]</ref>. Bayou <ref type="bibr" target="#b18">[19]</ref> and Rover <ref type="bibr" target="#b11">[12]</ref> also implement optimistic concurrency, but they can detect and react to more general types of conflicts than can Coda.</p><p>Worrell <ref type="bibr" target="#b19">[20]</ref> studied invalidation-based protocols in a hierarchical caching system and concluded that serverdriven consistency was practical for the web. We plan to explore ways to add hierarchy to our algorithms in the future.</p><p>Cache consistency protocols have long been studied for distributed file systems <ref type="bibr" target="#b16">[17]</ref>, <ref type="bibr" target="#b15">[16]</ref>, <ref type="bibr" target="#b17">[18]</ref>. Howard et al. <ref type="bibr" target="#b10">[11]</ref> reached the somewhat counter-intuitive conclusion that server-driven consistency generally imposed less load on the server than client polling even though server-driven algorithms provide stronger guarantees for clients. This is because servers have enough information to know exactly when messages need to be sent.</p><p>Finally, we note that volume leases on the set of all objects provided by a server can be thought of as providing a framework for the ªheartbeatº messages used in many distributed state systems.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">CONCLUSIONS</head><p>We have taken three cache consistency algorithms that have been previously applied to file systems and quantitatively evaluated them in the context of Web workloads. In particular, we compared the timeout-based Client Poll algorithm with the Callback algorithm, in which a server invalidates before each write, and Gray and Cheriton's Lease algorithm. The Lease algorithm presents a trade-off similar to the one offered by Client Poll. On the one hand, long leases reduce the cost of reads by amortizing each lease renewal over many reads. On the other hand, short leases reduce the delay on writes when a failure occurs. To solve this problem, we have introduced the Volume Lease, Volume Lease with Delayed Invalidation, and Best Effort Volume Lease algorithms that allow servers to perform writes with minimal delay, while minimizing the number of messages necessary to maintain consistency. Our simulations confirm the benefits of these algorithm.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 1 .</head><label>1</label><figDesc>Fig. 1. Definition of parameters in Table1.</figDesc><graphic coords="3,31.58,48.17,240.26,115.20" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 2 .</head><label>2</label><figDesc>Fig. 2. Data structures for volume lease algorithm.</figDesc><graphic coords="4,26.19,48.17,516.19,156.76" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head></head><label></label><figDesc>It does this by maintaining a volume epoch number that is incremented with each reboot. Thus, all client requests to renew a volume must also indicate the last epoch number known to the client. If the epoch number is current, then volume lease renewal proceeds normally. If the epoch number is old, then the server treats the client as if the client were in the volume's Unreachable set.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Fig. 3 .</head><label>3</label><figDesc>Fig. 3. The volume lease protocol (server side).</figDesc><graphic coords="5,26.19,48.17,516.19,516.87" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Fig. 4 .</head><label>4</label><figDesc>Fig. 4. The volume lease protocol (client side).</figDesc><graphic coords="6,26.19,48.17,516.19,369.01" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Fig. 5 .</head><label>5</label><figDesc>Fig. 5. The number of messages vs. timeout length.</figDesc><graphic coords="9,109.19,48.17,348.20,226.71" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Fig. 6 .Fig. 7 .</head><label>67</label><figDesc>Fig. 6. Number of messages vs. timeout length for Volume Leases with Delayed Invalidates as volume lease length is varied.</figDesc><graphic coords="10,112.71,48.17,341.14,220.54" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Fig. 8 .</head><label>8</label><figDesc>Fig. 8. State at the 10th most popular server vs. timeout.</figDesc><graphic coords="11,112.99,297.61,340.48,224.39" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Fig. 9 .</head><label>9</label><figDesc>Fig. 9. Periods of heavy server load under default workload for the most heavily loaded server.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>Fig. 10 .Fig. 11 .</head><label>1011</label><figDesc>Fig. 10. Periods of heavy server load under ªbursty writeº workload for the most heavily loaded server.</figDesc><graphic coords="12,112.37,48.17,341.95,223.99" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0"><head></head><label></label><figDesc></figDesc><graphic coords="2,26.19,78.78,516.19,135.61" type="bitmap" /></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" xml:id="foot_0"><p>IEEE TRANSACTIONS ON KNOWLEDGE AND DATA ENGINEERING, VOL. 11, NO. 4, JULY/AUGUST 1999</p></note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>ACKNOWLEDGMENTS</head><p>Some of the work described here appeared in an earlier paper <ref type="bibr" target="#b20">[21]</ref>. We thank James Gwertzman and Margo Seltzer for making their simulator available to us so we could validate our simulator. We thank Carlos Cunha, Azer Bestavros, and Mark Crovella for making the Boston University web traces available to us. This work was funded in part by a National Science Foundation CISE grant (CDA-9624082), gifts from Novell and Sun Microsystems, and DARPA/SPAWAR Grant No. N66001-98-8911. Michael Dahlin and Lorenzo Alvisi were supported by NSF CAREER Awards No. CCR-9733842 and No. CCR-9734185, respectively.</p></div>
			</div>

			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0" />			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title/>
		<author>
			<persName><forename type="first">T</forename><surname>Berners-Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Fielding</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">Frystyk</forename><surname>Nielsen</surname></persName>
		</author>
		<idno>http-v10- spec-00</idno>
		<imprint>
			<date type="published" when="1995-03">Mar. 1995</date>
		</imprint>
		<respStmt>
			<orgName>Internet Engineering Task Force</orgName>
		</respStmt>
	</monogr>
	<note>ªHypertext Transfer ProtocolÐHTTP/1. 0,º Internet Draft draft-ietf</note>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<author>
			<persName><forename type="first">A</forename><surname>Bestavros</surname></persName>
		</author>
		<title level="m">ªSpeculative Data Dissemination and Service to Reduce Server Load, Network Traffic, and Service Time in Distributed Information Systems,º Proc. Int&apos;l Conf. Data Eng</title>
		<imprint>
			<date type="published" when="1996-03">Mar. 1996</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">ªExperience with a Language for Writing Coherence Protocols</title>
		<author>
			<persName><forename type="first">S</forename><surname>Chandra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Dahlin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Richards</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Anderson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Larus</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Usenix Conf. Domain-Specific Languages</title>
		<meeting>Usenix Conf. Domain-Specific Languages</meeting>
		<imprint>
			<date type="published" when="1997-10">Oct. 1997</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<author>
			<persName><forename type="first">S</forename><surname>Chandra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Richards</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Larus</surname></persName>
		</author>
		<title level="m">ªTeapot: Language Support for Writing Memory Coherence Protocols,º Proc. SIGPLAN Conf. on Programming Language Design and Implementation</title>
		<imprint>
			<date type="published" when="1996-05">May 1996</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">ªThe Rio File Cache: Surviving Operating System Crashes</title>
		<author>
			<persName><forename type="first">P</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Ng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Chandra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Aycock</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Rajamani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Lowell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Seventh Int&apos;l Conf. Architectural Support for Programming Languages and Operating Systems (ASPLOS-VIII)</title>
		<meeting>Seventh Int&apos;l Conf. Architectural Support for Programming Languages and Operating Systems (ASPLOS-VIII)</meeting>
		<imprint>
			<date type="published" when="1996-10">Oct. 1996</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title/>
		<author>
			<persName><forename type="first">C</forename><surname>Cunha</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Bestavros</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Crovella</surname></persName>
		</author>
		<author>
			<persName><surname>Traces</surname></persName>
		</author>
		<idno>TR-95-010</idno>
		<imprint>
			<date type="published" when="1995-04">Apr. 1995</date>
		</imprint>
		<respStmt>
			<orgName>Dept. of Computer Science, Boston Univ.</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">Technical Report</note>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<author>
			<persName><forename type="first">D</forename><surname>Dill</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Drexler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Yang</surname></persName>
		</author>
		<title level="m">ªProtocol Verification as a Hardware Design Aid,º Proc. IEEE Int&apos;l Conf. Computer Design: VLSI in Computers and Processors</title>
		<imprint>
			<date type="published" when="1992">1992</date>
			<biblScope unit="page" from="522" to="525" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">ªLeases: An Efficient Fault-Tolerant Mechanism for Distributed File Cache Consistency</title>
		<author>
			<persName><forename type="first">C</forename><surname>Gray</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Cheriton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. 12th ACM Symp. Operating Systems Principles</title>
		<meeting>12th ACM Symp. Operating Systems Principles</meeting>
		<imprint>
			<date type="published" when="1989">1989</date>
			<biblScope unit="page" from="202" to="210" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">ªNotes on Data Base Operating Systems</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">N</forename><surname>Gray</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Operating Systems: An Advanced Course</title>
		<title level="s">Lecture Notes in Computer Science</title>
		<editor>
			<persName><forename type="first">R</forename><surname>Bayer</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">R</forename><forename type="middle">M</forename><surname>Graham</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">G</forename><surname>Seegmueller</surname></persName>
		</editor>
		<imprint>
			<publisher>Springer-Verlag</publisher>
			<date type="published" when="1977">1977</date>
			<biblScope unit="volume">60</biblScope>
			<biblScope unit="page" from="393" to="481" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<author>
			<persName><forename type="first">J</forename><surname>Gwertzman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Seltzer</surname></persName>
		</author>
		<title level="m">ªWorld-Wide Web Cache Consistency,º Proc. 1996 Usenix Technical Conf</title>
		<imprint>
			<date type="published" when="1996-01">Jan. 1996</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">ªScale and Performance in a Distributed File System</title>
		<author>
			<persName><forename type="first">J</forename><surname>Howard</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Kazar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Menees</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Nichols</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Satyanarayanan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Sidebotham</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>West</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Trans. Computer Systems</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="51" to="81" />
			<date type="published" when="1988-02">Feb. 1988</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<author>
			<persName><forename type="first">A</forename><surname>Joseph</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Delespinasse</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Tauber</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Gifford</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Kaashoek</surname></persName>
		</author>
		<author>
			<persName><surname>ªrover</surname></persName>
		</author>
		<title level="m">A Toolkit for Mobile Information Access,º Proc. 15th ACM Symp. Operating Systems Principles</title>
		<imprint>
			<date type="published" when="1995-12">Dec. 1995</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">ªDisconnected Operation in the Coda File System</title>
		<author>
			<persName><forename type="first">J</forename><surname>Kistler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Satyanarayanan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Trans. Computer Systems</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="3" to="25" />
			<date type="published" when="1992-02">Feb. 1992</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">ªMaintaining Strong Cache Consistency in the World-Wide Web</title>
		<author>
			<persName><forename type="first">C</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Cao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. 17th Int&apos;l Conf. Distributed Computing Systems</title>
		<meeting>17th Int&apos;l Conf. Distributed Computing Systems</meeting>
		<imprint>
			<date type="published" when="1997-05">May 1997</date>
			<biblScope unit="page" from="12" to="21" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">ªLarge Granularity Cache Coherence for Intermittent Connectivity</title>
		<author>
			<persName><forename type="first">L</forename><surname>Mummert</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Satyanarayanan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Summer 1994 Usenix Conf</title>
		<meeting>Summer 1994 Usenix Conf</meeting>
		<imprint>
			<date type="published" when="1994-06">June 1994</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">ªCaching in the Sprite Network File System</title>
		<author>
			<persName><forename type="first">M</forename><surname>Nelson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Welch</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Ousterhout</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">º ACM Trans. Computer Systems</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="issue">1</biblScope>
			<date type="published" when="1988-02">Feb. 1988</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">ªDesign and Implementation of the Sun Network Filesystem</title>
		<author>
			<persName><forename type="first">R</forename><surname>Sandberg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Goldberg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Kleiman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Walsh</surname></persName>
		</author>
		<author>
			<persName><surname>Lyon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Summer 1985 Usenix Conf</title>
		<meeting>Summer 1985 Usenix Conf</meeting>
		<imprint>
			<date type="published" when="1985-06">June 1985</date>
			<biblScope unit="page" from="119" to="130" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Experiments with Cache Consistency Protocols</title>
		<author>
			<persName><forename type="first">V</forename><surname>Srinivasan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Mogul</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. 12th ACM Symp. Operating Systems Principles</title>
		<meeting>12th ACM Symp. Operating Systems Principles</meeting>
		<imprint>
			<date type="published" when="1989-12">Dec. 1989</date>
			<biblScope unit="page" from="45" to="57" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">ªManaging Update Conflicts in Bayou, A Weakly Connected Replicated Storage System</title>
		<author>
			<persName><forename type="first">D</forename><surname>Terry</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Theimer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Petersen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Demers</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Spreitzer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Hauser</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. 15th ACM Symp. Operating Systems Principles</title>
		<meeting>15th ACM Symp. Operating Systems Principles</meeting>
		<imprint>
			<date type="published" when="1995-12">Dec. 1995</date>
			<biblScope unit="page" from="172" to="183" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<author>
			<persName><forename type="first">K</forename><surname>Worrell</surname></persName>
		</author>
		<title level="m">ªInvalidation in Large Scale Network Object Caches,º masters thesis</title>
		<meeting><address><addrLine>Boulder</addrLine></address></meeting>
		<imprint>
			<date type="published" when="1994">1994</date>
		</imprint>
		<respStmt>
			<orgName>Univ. of Colorado</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<author>
			<persName><forename type="first">J</forename><surname>Yin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Alvisi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Dahlin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Lin</surname></persName>
		</author>
		<title level="m">ªUsing Leases to Support Server-Driven Consistency in Large-Scale Systems,º Proc. 18th Int&apos;l Conf. Distributed Computing Systems</title>
		<imprint>
			<date type="published" when="1998-05">May 1998</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
