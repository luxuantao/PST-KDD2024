<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">A Collective Learning Framework to Boost GNN Expressiveness</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author role="corresp">
							<persName><forename type="first">Mengyue</forename><surname>Hang</surname></persName>
							<email>&lt;hangm@purdue.edu&gt;.</email>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science</orgName>
								<orgName type="institution">Purdue University</orgName>
								<address>
									<region>West Lafayette</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Jennifer</forename><surname>Neville</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science</orgName>
								<orgName type="institution">Purdue University</orgName>
								<address>
									<region>West Lafayette</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="department">Department of Statistics</orgName>
								<orgName type="institution">Purdue University</orgName>
								<address>
									<settlement>West Lafayette</settlement>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Bruno</forename><surname>Ribeiro</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science</orgName>
								<orgName type="institution">Purdue University</orgName>
								<address>
									<region>West Lafayette</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">A Collective Learning Framework to Boost GNN Expressiveness</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.2" ident="GROBID" when="2022-12-25T13:17+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Graph Neural Networks (GNNs) have recently been used for node and graph classification tasks with great success, but GNNs model dependencies among the attributes of nearby neighboring nodes rather than dependencies among observed node labels. In this work, we consider the task of inductive node classification using GNNs in supervised and semi-supervised settings, with the goal of incorporating label dependencies. Because current GNNs are not universal (i.e., mostexpressive) graph representations, we propose a general collective learning approach to increase the representation power of any existing GNN. Our framework combines ideas from collective classification with self-supervised learning, and uses a Monte Carlo approach to sampling embeddings for inductive learning across graphs. We evaluate performance on five real-world network datasets and demonstrate consistent, significant improvement in node classification accuracy, for a variety of state-of-the-art GNNs.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Graph Neural Networks (GNNs) have recently shown great success at node and graph classification tasks <ref type="bibr" target="#b7">(Kipf &amp; Welling, 2016;</ref><ref type="bibr" target="#b4">Hamilton et al., 2017;</ref><ref type="bibr" target="#b11">Luan et al., 2019)</ref>. GNNs have been applied in both transductive settings (where the test nodes are embedded in the training graph) and inductive settings (where the training and test graphs are disjoint). However, GNN methods focus on modeling dependencies among the observed attributes and the local graph structure around a node. And despite their success, existing GNNs have known shortcomings: they are not universal (most-expressive) graph representations <ref type="bibr" target="#b16">(Morris et al., 2019;</ref><ref type="bibr" target="#b17">Murphy et al., 2019;</ref><ref type="bibr" target="#b31">Xu et al., 2018)</ref>. That is, GNNs are not expressive enough for some node classification tasks.</p><p>At the same time, a large body of work in relational learning has focused on strengthening poorly-expressive (i.e., local) classifiers in relational models (e.g., relational logistic regression, naive Bayes, decision trees <ref type="bibr" target="#b19">(Neville et al., 2003a</ref>)) through collective classification, which have been shown to improve classification performance, particularly in semisupervised learning over partially labeled graphs <ref type="bibr" target="#b30">(Xiang &amp; Neville, 2008;</ref><ref type="bibr" target="#b8">Koller et al., 2007;</ref><ref type="bibr" target="#b22">Pfeiffer III et al., 2015)</ref>.</p><p>In this work, we empirically investigate our hypothesis that, by explicitly incorporating label dependencies among neighboring nodes-akin to how collective classification improves not-so-expressive classifiers-it is possible to devise an add-on training and inference procedure that can improve the expressiveness of existing GNNs, for both semisupervised and supervised node classification tasks. However, dependencies among the node labels in a partially labeled graph are not straightforward to incorporate into GNNs, these require a redesign of how GNNs are trained.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Contributions.</head><p>In this paper, we first show that collective classification is provably unnecessary if one can learn GNNs that are most-expressive. Then, because current GNNs are not most-expressive, we propose an add-on general collective classification framework for semi-supervised learning using GNNs, which our experiments show to consistently improve the node classification accuracy of a variety of stateof-the-art GNNs. Our framework uses a self-supervised learning procedure-via randomized input-masking-that is able to leverage true labels as input attributes. This is combined with a Monte Carlo approach to sampling embeddings, which encode global/joint information as local variability and facilitates inductive learning. Our approach is applicable for learning over partially labeled graphs, whether there are labels available in the test data or not.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Problem formulation</head><p>We consider the problem of inductive semi-supervised node classification, which takes as input a graph G (tr) = (V (tr) , E (tr) , X (tr) , Y (tr)  L ) for training, where V (tr) is a set of n (tr) vertices, E (tr) ⊂ V (tr) × V (tr) is a set of edges with adjacency matrix A (tr) , X (tr) is a n (tr) × p matrix containing node attributes as p-dimensional vectors, and Y (tr)  L is a set of observed labels of a connected set of nodes V (tr)  L ⊂ V (tr) , where V (tr)  L is assumed to be a proper subset of V (tr) , noting arXiv: <ref type="bibr">2003.12169v1 [cs.</ref>LG] 26 Mar 2020 that V (tr) L = ∅ is allowed. Let Y (tr) U be the unknown labels of nodes V (tr)  U = V (tr) \ V (tr) L . The goal is to learn a joint model of Y (tr)  U ∼ P (Y U |G (tr) ) and apply this same model to predict hidden labels Y (te) in another test graph G (te) , i.e., Ŷ (te) U = arg max Y U P (Y U |G (te) ). In this work, we explore two different classification tasks. The first task considers an unlabeled test graph G (te) = (V (te) , E (te) , X (te) ). In this case, the goal is to predict the labels for all nodes Y (te) U = Y (te) . The task of predicting node labels when no labels are available in test data sounds like a fully supervised learning task, rather than a semisupervised task, but it is not. Although it has a similar description to supervised learning node classification tasks (e.g., <ref type="bibr" target="#b4">(Hamilton et al., 2017)</ref>), fully supervised approaches do not fully exploit dependencies between observed labels of instances that are connected in the training data. Specifically, there is a measurable difference -in terms of information content-between having the known training labels Y (tr)   L connected in G (tr) or spread out through the graph. In the latter case, it is unlikely that labeled examples are connected, and thus it will be difficult to learn label dependencies. If the known labels are connected in training, and GNNs could exploit their dependencies through semi-supervised learning, that would further improve their representation power.</p><p>The second task is a more traditional semi-supervised learning task that considers a partially labeled test graph G (te)  partial = (V (te) , E (te) , X (te) , Y (te) L ). In this case, the goal is to predict the hidden labels Y (te)</p><formula xml:id="formula_0">U = Y (te) \ Y (te)</formula><p>L conditioned on the observed labels in the graph. Here, it is clear that exploiting the dependencies among connected labeled instances could improve representation power.</p><p>We denote the above tasks (A) semi-supervised learning with unlabeled test data and (B) semi-supervised learning with partially-labeled test data. In this work we explore variants of both tasks.</p><p>Graph neural networks and semi-supervised learning tasks: Broadly, GNNs can perform semi-supervised learning over partially labeled graphs by propagating feature information throughout the graph. However, in training, existing GNN models ignore the dependency of node labels by assuming conditional independence among node labels and factorizing the joint distribution of labels as</p><formula xml:id="formula_1">P (Y U |X, Y L , A) = Π v∈V L P (y v |X, Y L , A).</formula><p>Hence, when Y L = ∅, the above equation corresponds to unlabeled test data; otherwise, Y L = ∅, the above equation corresponds to partially labeled test data. Specifically, GNN models learn a node representation Z that minimizes the negative log-likelihood loss. The model then makes predictions based on Z, i.e., ∀v ∈ V, where maybe</p><formula xml:id="formula_2">Y L = ∅, Z v = GNN(X, Y L , A; Θ) v , P (y v |X, Y L , A) = σ(WZ v + b) yv ,<label>(1)</label></formula><p>where Z v is the representation for node v, σ(•) is the softmax activation, and Θ, W and b are model parameters. Therefore, current GNNs will infer the label distribution P (y v |X, Y L , A) for each node v ∈ V independently. While here we have described a function that conditions on an arbitrary label set Y L to illustrate GNN's conditional independence assumptions, it is not yet described how to actually incorporate the partial labels into the embedding function. We will address this challenge in Section 3.2.</p><p>Inductive collective classification: Conventional relational machine learning (RML) developed methods to learn joint models from labeled graphs <ref type="bibr" target="#b10">(Lu &amp; Getoor, 2003;</ref><ref type="bibr" target="#b18">Neville &amp; Jensen, 2000</ref>)</p><formula xml:id="formula_3">P (Y U |X, A)</formula><p>To achieve this, many of the methods use pseudolikelihood estimation and consider a Markov assumption -every node v i ∈ V is considered conditionally independent of the rest of the network given its Markov Blanket (MB(v i )). For undirected graphs, this is often simply the set of the immediate neighbors of v i .</p><p>Given the Markov blanket assumption, RML methods typically use a local conditional model (e.g., relational Naive Bayes <ref type="bibr" target="#b20">(Neville et al., 2003b)</ref>, relational logistic regression <ref type="bibr" target="#b24">(Popescul et al., 2002)</ref>) to learn and infer labels within the network. The pseudolikelihood objective considers the nodes in a labeled subgraph G L , where the labels of all neighbors are known:</p><formula xml:id="formula_4">O = v∈V (tr) L log P (y (tr) v |Y (tr) MB(v) , X (tr) , A (tr) )<label>(2)</label></formula><p>The key difference between Equation (2) and the GNNs objective in Equation ( <ref type="formula" target="#formula_2">1</ref>): the RML model is conditioned on the labels Y even when there are no observed labels in the test data, i.e., even when Y (te) L = ∅. When the model is applied to make predictions in an unlabeled graph, joint (i.e., collective) inference methods such as variational inference or Gibbs sampling must be applied in order to use the conditionals from Equation (2). This combines the local conditional probabilities with global inference to estimate the joint distribution over the unlabeled vertices, e.g.,:</p><formula xml:id="formula_5">P (Y U |Y L , X) ≈ Q(Y U ) = Π vi∈V U Q i (y)</formula><p>where each component Q i (y) is iteratively updated.</p><p>Alternatively, a Gibbs sampler iteratively draws a label from the corresponding conditional distributions of the unlabeled vertices:</p><formula xml:id="formula_6">ŷv ∼ P (y v | ŶMB(v) , Y L , X, A), ∀v ∈ V.</formula><p>Note that for conventional RMLs, we assume a fully labeled (sub)network for learning, thus Y MB(vi) only includes known labels, i.e., v j ∈ V L .</p><p>Transductive collective classification: For transductive settings, where the goal is to learn and predict within a partially labeled graph, RML methods have considered semi-supervised formulations <ref type="bibr" target="#b8">(Koller et al., 2007;</ref><ref type="bibr" target="#b30">Xiang &amp; Neville, 2008;</ref><ref type="bibr" target="#b22">Pfeiffer III et al., 2015)</ref> to model the joint probability distribution:</p><formula xml:id="formula_7">P (Y U |Y L , X, A)</formula><p>In this case RML methods use both collective learning and collective inference procedures for semi-supervised learning. For example Expectation Maximization (EM) <ref type="bibr" target="#b30">(Xiang &amp; Neville, 2008;</ref><ref type="bibr" target="#b22">Pfeiffer III et al., 2015)</ref>, iterative updates the parameter estimates by utilizing the expected values of the unlabeled examples to relearn the parameters.</p><p>For instance, the PL-EM algorithm <ref type="bibr" target="#b22">(Pfeiffer III et al., 2015)</ref> optimizes the pseudolikelihood</p><formula xml:id="formula_8">E-Step: evaluate P (Y (tr) U |Y (tr) L , X (tr) , A (tr) , Θ t−1 ) M-Step: learn Θ t : Θ t = arg max Θ Y (tr) U ∈Γ U P (Y (tr) U |Y (tr) L , X (tr) , A (tr) , Θ t−1 )</formula><p>× v∈V (tr)   log P (y (tr) v |Y (tr) MB(v) , X (tr) , A (tr) , Θ)</p><p>Is collective classification able to better represent target label distributions than graph representations like GNNs?</p><p>The answer to the above question is yes and no. Theorem 1 shows that a most-expressive GNNs <ref type="bibr" target="#b17">(Murphy et al., 2019;</ref><ref type="bibr" target="#b12">Maron et al., 2019;</ref><ref type="bibr">Srinivasan &amp; Ribeiro, 2019)</ref> would not be able to benefit from any collective classification method.</p><p>Theorem 1 (Collective classification can be unecessary).</p><p>Consider the task of predicting node labels when no labels are available in test data. Let Γ (v, G) be a most-expressive representation of node v ∈ V in graph G . Then, for any collective inference procedure predicting the class label of v ∈ V , there exists a classifier that takes Γ (v, G) as input and predicts the label of v with equal or higher accuracy.</p><p>The proof of Theorem 1 is in the Appendix. However, since state-of-the-art GNNs are not most-expressive, collective classification could help improve the expressiveness of GNNs. This leads to the key hypothesis of this work Hypothesis 1, which we validate empirically by extensive experimentation in Section 4.</p><p>Hypothesis 1. Since current Graph Neural Networks (e.g. GCN, GraphSAGE) cannot produce most expressive graph representation, collective learning should be able to improve the accuracy of node classification by producing a more expressive graph representation.</p><p>Why? Because current GNNs only propagate node attributes and not observed label information, they are not able to pay attention to the relationship between node attributes, the graph topology, and label dependencies. By including label information as input, the GNNs will be able to incorporate more information, while they try to model the joint label distribution.</p><p>In Section 3, we propose a self-supervised approach to incorporate what we denote as collective learning into GNNs.</p><p>We develop methods for both task A and task B: for test graphs with and without observed labels.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Proposed framework: Collective Learning with GNNs</head><p>In this section, we outline MCC-GNN. It is a general framework to incorporate any GNN, and combines self-supervised learning and Monte Carlo embedding sampling to improve inductive learning on partially labeled graphs. We start by describing an implementation for task A and then add some adjustments to address task B.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Collective learning with unlabeled test data</head><p>Given a partially labeled training graph G (tr) = (V (tr) , E (tr) , X (tr) , Y (tr) L ) and an unlabeled test graph G (te) = (V (te) , E (te) , X (te) ), the goal of the inductive node classification task is to train a joint model on G (tr) to learn P (Y |G (tr) ) and apply it to G (te) . Notation Description G (tr)  training graph</p><formula xml:id="formula_9">V (tr) L , Y<label>(tr)</label></formula><p>L labeled nodes and labels in training graph V (tr)  in , Y (tr)   in nodes with true labeled used as model input V (tr)  out , Y (tr) out nodes with true labels used as target (masked nodes) G (te)  test graph</p><formula xml:id="formula_10">V (te) L , Y<label>(te)</label></formula><p>L labeled nodes and labels in test graph Ŷ t prediction of all vertices at iteration t</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Table 1. Table of Notations</head><p>No test labels: Bootstrapping the predicted input labels.</p><p>Following Hypothesis 1, we propose Monte Carlo Collective Learning GNNs (MCC-GNN), which includes label information as input to GNNs to produce a more expressive representation. Assume we have label predictions Ŷ(t−1) = (y</p><formula xml:id="formula_11">(t−1) v</formula><p>) v∈V at the t-th step of our algorithm. At iteration t, the MCC-GNN model parameters are updated by considering Ŷ (t−1) as part of the input attributes, and the graph representation is an average over the samples of</p><formula xml:id="formula_12">Ŷ (t−1) : Z (t) v = E Ŷ (t−1) GNN(X (tr) , Ŷ (t−1) , A (tr) ; Θ) v . (3)</formula><p>The optimization then becomes</p><formula xml:id="formula_13">Θ t , W t , b t = arg min Θ,W,b − v∈V (tr) L log σ(WZ (t) v + b) y (tr) v ,<label>(4)</label></formula><p>where σ(•) is the softmax activation function, V (tr) L are the nodes with observed labels in the training data, and X (tr)  is again node attribute matrix in the training data. Suppose we use K input samples, then the time/space complexity of the MCC-GNN is K times the time/space complexity of the corresponding GNN model as we have to compute K embeddings at each gradient step.</p><p>Obtaining Ŷ(t−1) . In iteration t, we use the MCC-GNN model parameter Θ t−1 to obtain</p><formula xml:id="formula_14">Z (t−1) v = E Ŷ (t−1) GNN(X (tr) , Ŷ (t−2) , A (tr) ; Θ (t−1) ) v and use the MCC-GNN model parameters W t−1 , b t−1 to obtain ŷ(t−1) v ∼ Categorical(σ(W t−1 Z (t−1) v +b t−1 ) v ), ∀v ∈ V (tr) .</formula><p>Note the recursive nature of our formulation. We bootstrap the probabilities for the base case (used to sample Ŷ (0) ) with another GNN model that uses only node attributes X (tr) as input.</p><p>Estimating Equation (3). Since computing the expectation needed to obtain {Z (t−1) v } v∈V is intractable, we compute an unbiased estimate at each gradient step (rather than at each t). Given probability estimates Ŷ (t−1) over all nodes, we sample multiple label assignments</p><formula xml:id="formula_15">Ŷ (t−1) 1 , • • • , Ŷ (t−1)</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>K</head><p>(one-hot encoded), K &gt; 1, feed them to GNNs and use the average of the generated node embeddings for final classification. Having multiple such input samples helps the model produce more expressive embeddings, as shown by <ref type="bibr" target="#b17">Murphy et al. (2019)</ref>. Note that MCC-GNN can be combined with any existing GNN architecture.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Collective learning with partially labeled test data</head><p>Self-supervised learning and collective classification. In semi-supervised collective classification, algorithms typically only estimate labels for Y U in the E-step. Thus, optimization (in the M-step) only incorporates the impact of collective inference over the unknown portion of the graph.</p><p>In order to learn more robust models, we may also want to consider self-supervised learning, where we mask part of the labeled input, to ensure that collective inference is able to reproduce the correct labeling over labeled portions of the graph.</p><p>We note that a self-supervised learning approach is critical for GNN methods to incorporate label dependencies and move towards collective classification. This is because the input to GNNs is typically the full graph G (tr) . If we included the observed labels directly Y (tr) L in the input, then it would be trivial to learn a model that optimized Equation (1). Instead, we can apply a mask to some of the labels so they do not appear in Y (tr)  L and we can optimize over the masked set. In this way, the model is trained to explore different parts of the graph by applying random masks.</p><p>Self-supervised MCC-GNN. Given two partially labeled graphs G (tr) = (V (tr) , E (tr) , X (tr) , Y (tr)</p><p>L ) and</p><formula xml:id="formula_16">G (te) = (V (te) , E (te) , X (te) , Y (te)</formula><p>L ), the goal is to learn a model that estimates the label distribution conditioning on node attributes X and some true labels Y (tr)  L on the training graph G (tr) , and apply the learned model to the test graph G (te) with true labels Y (te)</p><p>L . We find that given the label prediction Ŷ , the semisupervised collective learning recursion in Equation ( <ref type="formula" target="#formula_13">4</ref>) is ripe for self-supervision. To this end, we apply a mask to some of the observed training labels Y (tr)  L . The unmasked Y (tr)  L labels replace the predicted labels Ŷ (t−1) in Equation (3). And the masked Y (tr)  L labels and their respective nodes now become the target of the optimization in Equation (4).</p><p>The self-supervised mask is randomly sampled from a set of masks M at every gradient step. In this way, the model is trained to explore different label correlations in the graph through these random masks. Node masking procedure. To use true labels as input to the model, we design a self-supervised learning procedure with random label masking. In each stochastic gradient descent step, we randomly sample a binary mask M ∼ Uniform(M) from a set of masks, where M is a |V | × 1 binary (0-1) vector. By applying the mask on the observed labels Y (tr)  L , this set is effectively partitioned into two parts, i.e.Y (tr)  L → Y (tr)  in , Y We now rewrite Equations ( <ref type="formula">3</ref>) and (4) using the sampled mask M . At iteration t, the MCC-GNN model parameters are updated by considering both Y (tr) L M , Ŷ (t−1) as part of the input attributes. Given a mask M , the graph representation is an average over the samples of Ŷ (t−1) , changing Equation (3) to:</p><formula xml:id="formula_17">Z (t) v (M ) = E Ŷ (t−1) GNN(X (tr) , Y (tr) L M, Ŷ (t−1) M , A (tr) ; Θ) v .</formula><p>(5)</p><p>Then, the optimization in Equation ( <ref type="formula" target="#formula_13">4</ref>) becomes</p><formula xml:id="formula_18">Θ t ,W t , b t = arg min Θ,W,b E M − v∈V (tr) L M v log σ(WZ (t) v (M ) + b) y (tr) v ,<label>(6)</label></formula><p>where, again, σ(•) is the softmax activation function, V (tr) L are the nodes with observed labels in the training data, and X (tr) is again node attribute matrix in the training data.</p><p>Stochastic optimization of Equation (6). In order to optimize Equation ( <ref type="formula" target="#formula_18">6</ref>), we first need to compute unbiased estimates for {Z</p><formula xml:id="formula_19">(t−1) v</formula><p>} v∈V in Equation ( <ref type="formula">5</ref>) and an unbiased estimate of the expectation over M in Equation ( <ref type="formula" target="#formula_18">6</ref>).</p><p>First, an unbiased estimate of the expectation over M can lead to a proper Robbins-Monro stochastic optimization procedure <ref type="bibr">(Robbins &amp; Monro, 1951)</ref>, since we can obtain unbiased (low-variance) estimates of the gradient of the loss function by simply sampling a new mask M ∼ Uniform(M) at each gradient step.</p><p>Second, we can compute an unbiased estimate of {Z (t−1) v } v∈V in Equation ( <ref type="formula">5</ref>) at each gradient step, by sampling multiple label assignments</p><formula xml:id="formula_20">Ŷ (t−1) 1 , • • • , Ŷ (t−1)</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>K</head><p>(onehot encoded), K &gt; 1, and feeding them to GNNs and use the average of the generated node embeddings, much like the procedure in Section 3.1.</p><p>To obtain an initial label prediction Ŷ (0) and start the recursion, we use only the true labels as input to MCC-GNN, replacing the Ŷ (t−1) in Equation ( <ref type="formula">5</ref>) with zero matrix. Same random masking procedure is applied. The whole iterative process is visualized in Figure <ref type="figure" target="#fig_0">1</ref> We use five datasets for evaluation. The dataset statistics are shown in Table <ref type="table" target="#tab_0">2</ref>, and the descriptions are as follows:</p><p>• Cora and Pubmed are benchmark datasets for node classification tasks from <ref type="bibr" target="#b27">(Sen et al., 2008)</ref>. They are citation networks with nodes representing publications and edges representing citation relation. Node attributes are bag-ofword features of each document, and the predicted label is the corresponding research field.</p><p>• Facebook <ref type="bibr" target="#b32">(Yang et al., 2017)</ref> is social network of Facebook users from Purdue university, where nodes represent users and edges represent friendship. The features of the nodes are: religious views, gender and whether the users hometown is in Indiana. The predicted labels is political view.</p><p>• Friendster <ref type="bibr" target="#b29">(Teixeira et al., 2019)</ref>  • Protein is a collection of protein graphs from <ref type="bibr" target="#b0">(Borgwardt et al., 2005)</ref>. Each node is labeled with a functional role of the protein, and has a 29 dimensional feature vector. We use 85 graphs with an average size of 150 nodes.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Experiment Setup</head><p>To conduct inductive learning tasks, we have to properly split the graphs into labeled and unlabeled parts. For datasets containing only one graph (Cora, Pubmed, Facebook and Friendster), we randomly sample a connected component to be V (tr) L , and then sample a test set (V T ) from the remainder nodes (V U ). To make partially-labeled test data available, we sample another connected component as V (te)  L with the same size as V (tr) L . The nodes are sampled to guarantee that there is no overlapping between any two sets of V (tr)  L , V (te) L and V T . Here G (tr) and G (te) have the same graph structure but with different labeled nodes.</p><p>For the protein dataset, as we have 85 disjoint graphs, we randomly choose 51 (60%) graphs for training, 17 (20%) graphs for validation and the remaining 17 (20%) graphs for testing. To simulate semi-supervised learning settings, we mask out 50% of true labels on the training graphs. For the tasks with partially-labeled test data, we randomly select 50% of the nodes in the test graphs as labeled nodes, and test on the remaining 50% nodes. We run five trials for all the experiments, and in each trial we randomly split the nodes/graphs as described.</p><p>As our method can be applied to any GNN model structure, we trained three GNNs as examples:</p><p>• GCN <ref type="bibr" target="#b7">(Kipf &amp; Welling, 2016)</ref> which includes two graph convolutional layers. Here we implemented an inductive variant of the original GCN model for our tasks.</p><p>• Supervised GraphSage <ref type="bibr" target="#b4">(Hamilton et al., 2017)</ref> (denoted by GS) with Mean pooling aggregator. We use sample size of 5 for neighbor sampling.</p><p>• Truncated Krylov GCN <ref type="bibr" target="#b11">(Luan et al., 2019)</ref> (denoted by TK), a recent GNN model that leverages multi-scale information in different ways and are scalable in depth. The TK-GCN has stronger expressive power and achieved state-of-the-art performance on node classification tasks. We implemented Snowball architecture which achieved comparable performance with the other truncated Krylov architecture in according to the original paper. In our implementation, we use 10 layers.</p><p>For each of GNNs, we compare its baseline performance (on its own) to the performance we achieve using it for collective learning in MCC-GNN. For a fair comparison, the baseline GNN and MCC-GNN are trained with the same model parameters, e.g. model depth, hidden dimensions, learning rate, early-stopping procedures. For unlabeled test data, we directly compare to the initial prediction obtained by GNN, which is used by MCC-GNN as input attributes.</p><p>For partially labeled test data, the baseline GNN is trained with labeled set V (tr) L . As GNNs cannot take labels as input, Y (te) L is not be used. In addition, we also compare to three relational classifiers, ICA <ref type="bibr" target="#b10">(Lu &amp; Getoor, 2003)</ref>, PL-EM <ref type="bibr" target="#b22">(Pfeiffer III et al., 2015)</ref> and GMNN <ref type="bibr" target="#b25">(Qu et al., 2019)</ref>. The first two models apply collective learning and inference with simple local classifiers Naive Bayes for PL-EM and Logistic regression for ICA. GMNN is the state-of-the-art collective model with GNNs, which uses two GCN models to model label dependency and node attribute dependency respectively. All the three models take true labels in their input, thus we use Y (tr)</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>L</head><p>for training and Y (te)  L for testing. We report the average accuracy score and standard error of five trials for the baseline models, and compute the ab-solute improvement of accuracy of our method over the corresponding base GNN. The only exception is on Friendster dataset. As the labels are highly imbalanced, we report the balanced accuracy score. Numbers in bold represent significant improvement over the baseline GNN based on a paired t-test (p &lt; 0.05), and numbers with * is the best performing method in each column.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Results</head><p>The performance of all the models on node classification task is shown in Table <ref type="table" target="#tab_6">3</ref> (task A with unlabeled test data) and Table <ref type="table" target="#tab_11">4</ref> (task B with partially-labeled test data). Our proposed MCC-GNN is denoted as +CL (for Collective Learning) in the results and our model performance is shown in shaded area.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.1.">UNLABELED TEST DATA</head><p>Comparison with base GNN models Table <ref type="table">Table</ref>  <ref type="table" target="#tab_6">3</ref> shows that our method is able to improve the corresponding noncollective GNN models for all the three model architectures (i.e. GCN, GraphSage and TK-GCN). Although all the models have large variances over multiple trials -which is because different parts of the graphs are being trained and tested on in different trials, our model consistently improves the baseline GNN. The results from a paired t-test comparing the performance of our method and the corresponding non-collective GNN shows that the improvement is almost always significant (marked as bold), with two exceptions on Pubmed and Friendster. Comparing the gains on different datasets, our method achieved smaller gains on Friendster. This is because the Friendster graph is much more sparse than all other graphs (e.g. edge density of Friendster is 1.5e-4 and edge density of Cora is 1.44e-3 <ref type="bibr" target="#b29">(Teixeira et al., 2019)</ref>), which makes it hard for any model to propagate label information and capture the dependency.</p><p>Moreover, comparing the improvement over GCN and TK-GCN, we can observe that in general our method adds more gains to GCN performance, and the difference is more obvious on Cora and Pubmed. For example, with 3% labeled data, our method when combining with GCN has an average of 6.29% improvement over GCN, 2.35% improvement over GraphSage, and 0.96% improvement over TK-GCN. This is in line with our assumption Hypothesis 1 that collective inference can help GNNs produce a more expressive representation. As GCN is provably less expressive than TK-GCN <ref type="bibr" target="#b11">(Luan et al., 2019)</ref>, there is a larger room to increase its expressiveness.</p><p>Comparison with other relational classifiers The two baseline non-GNN relational models, i.e. PL-EM and ICA generally perform worse than GNNs, with the only exception on Protein dataset. This could be because the two non-GNN models generally need a larger portion of labeled PL-EM (Pfeiffer III et al., 2015)   (Lu &amp; Getoor, 2003)  -  (Qu et al., 2019)  -  set to train the weak local classifier, whereas GNNs utilize a neural network architecture as "local classifier", which is better at representation learning by transforming and aggregating node attribute information. However, when the model is trained with a large training set (e.g. with 30% nodes on Protein dataset), modeling the label dependency becomes crucial.</p><p>For GMNN, the collective GNN model, it achieved better performance than its non-collective base model, i.e. GCN, and we can see that our model combining with GCN achieved comparable or slightly better results than GMNN.</p><p>When combing with other more powerful GNNs, our model can easily out-perform it, e.g. on Cora, Pubmed and Facebook datasets, the TK-GCN performs better than GMNN and our method adds extra gains over TK-GCN.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.2.">PARTIALLY LABELED TEST DATA</head><p>Comparison to base GNN models Table <ref type="table" target="#tab_11">Table 4</ref> shows that our method again achieves consistent improvement over the corresponding non-collective GNNs. Comparing with unlabeled test data (task A), the gains are much larger with partially-labeled test data (task B). For example, when combining with GCN on Cora dataset, with unlabeled test data the improvements of our method are 6.29%, 5.20% and 5.18% for various label rates, but with labeled test data, the improvements are 15.69%, 14.02% and 6.31%. This shows the importance of modeling label dependency especially when the some test data labels are observed.</p><p>Comparing the improvement over GCN and TK-GCN, again our model adds more gain to GCN in general, especially on Cora and Pubmed. For example, with 1.52% labels on Pubmed, our method when combining with GCN has an average of 5.62% gains over GCN, 1.49% gains over GraphSage, and 0.54% over truncated Krylov GCN.</p><p>Comparison to other relational classifiers We observed similar patterns as in task A with the unlabeled test data. The non-GNN models PL-EM and ICA perform worse than GNN models, with the only exception on Protein where ICA achieved the best performance, with an average accuracy of 84.39%. At the same time, our method is still able to improve the performance of the corresponding GNNs. The performance gap between GNNs and non-GNNs on Protein dataset shows that there is more room for us to learn from the collective learning and inference methods in order to improve the GNN models, especially for inductive semisupervised tasks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.">Ablation study</head><p>To investigate if adding predicted labels in model input adds extra information with partially-labeled test data described in Section 3.2, we tested the performance of a model variant which only use true labels as input with the same node masking procedure. Figure <ref type="figure" target="#fig_3">2</ref> shows two examples on Cora with GCN Figure <ref type="figure" target="#fig_3">2a</ref> and Pubmed with TK-GCN Figure <ref type="figure" target="#fig_3">2b</ref>, where including predicted labels achieves better performance. We run the model 10 times and calculate the average and standard deviation (shown as shaded area) of classification accuracy at each iteration t as described in Section 3.2. We can see that adding predicted labels starts to improve the performance after the first iteration and achieves consistent gains.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Related work</head><p>On collective learning and neural networks. There has been work on applying deep learning to collective classification. For example, <ref type="bibr" target="#b15">(Moore &amp; Neville, 2017)</ref> proposed to use LSTM-based RNNs for relational classification tasks on graphs. They transform each node and its set of neighbors into an unordered sequence and use an RNN model to predict the class label as the output of that sequence. <ref type="bibr" target="#b23">(Pham et al., 2017)</ref> designed a deep learning model for collective classification in multi-relational domains, which learns local and relational features simultaneously to encodes multirelations between any two instances. The closest work to ours is <ref type="bibr" target="#b14">(Monner &amp; Reggia, 2013)</ref>, which proposed a recurrent collective classification (RCC) framework, a variant of ICA <ref type="bibr" target="#b10">(Lu &amp; Getoor, 2003)</ref>  PL-EM (Pfeiffer III et al., 2015)   (Lu &amp; Getoor, 2003)  -31.17   (Qu et al., 2019)  -   The first and only work applying statistical relational learning to GNNs is Graph Markov Neural Network (GMNN) <ref type="bibr" target="#b25">(Qu et al., 2019)</ref>, which proposed to model joint label distribution with a conditional random field trained with the variational EM algorithm. GMNN is trained by alternating between an E-step and an M-step, and two GCNs are trained for the two steps respectively.</p><p>In parallel to our work, <ref type="bibr" target="#b6">(Jia &amp; Benson, 2020)</ref> considers regression tasks by modeling the joint GNN residual of a target set (y − ŷ) as a multivariate Gaussian, defining the loss function as the marginal likelihood only over labeled nodes ŷL . In contrast, by using the more general foundation of collective classification, our framework can seamlessly model both classification and regression tasks, and include model predictions over the entire graph Ŷ as MCC-GNN's input, thus affecting both the model prediction and the GNN training in both unlabeled and partially-labeled test data tasks.</p><p>On self-supervised learning. Self-supervised learning is closely related to semi-supervised learning. In fact, self-supervised learning can be seen as a self-imposed semi-supervised learning task, where part of the input is masked (or transformed) and must be predicted back by the model <ref type="bibr" target="#b2">(Doersch et al., 2015;</ref><ref type="bibr" target="#b21">Noroozi &amp; Favaro, 2016;</ref><ref type="bibr" target="#b9">Lee et al., 2017;</ref><ref type="bibr" target="#b13">Misra et al., 2016)</ref>. Recently, self-supervised learning has been broadly applied to achieve state-of-the-art accuracy in computer vision <ref type="bibr" target="#b5">(Hénaff et al., 2019;</ref><ref type="bibr" target="#b3">Gidaris et al., 2019)</ref> and natural language processing <ref type="bibr" target="#b1">(Devlin et al., 2018)</ref> supervised learning tasks.</p><p>The recent interest in semi-supervised learning is, possibly, attributed to representation learning being about understanding the data itself, rather than learning a representation that narrowly performs well in the training data of a particular task. The ease that representation learning can be applied in transfer learning tasks, only reinforces the argument that learning to represent is the key to the low generalization error observed in deep learning. Hence, revisiting the value of semi-supervised learning in graph representation learning is extremely valuable to the research community.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Conclusion</head><p>In this work, we answer the question "can collective learning and inference techniques improve the expressiveness of state-of-the-art GNNs in supervised and semi-supervised node classification tasks?" We first show that with the most expressive GNNs there is no need to do collective learning; however, since we do not have the most expressive models, we present a collective learning framework that can be combined with any existing GNN architectures to improve model expressiveness. We considered two inductive semisupervised learning tasks: with and without labeled test data, and showed by extensive empirical study that our collective learning methods significantly improve GNNs performance on both tasks.</p><p>Case (2): In this case, the true label of v is not independent of the predicted labels. By Theorem 1 of <ref type="bibr">Srinivasan &amp; Ribeiro (2019)</ref>, we know that for any random variable H v attached to node v ∈ V , it must be that ∃ϕ a measurable function independent of G s.t.</p><formula xml:id="formula_21">H v a.s. = ϕ (Γ (v, G), v ),</formula><p>where v is an noise source exogenous to G (pure noise), and a.s. implies almost sure equality. Defining</p><formula xml:id="formula_22">H v := Y v , ϕ (Γ (v, G), v ) ⊥ ⊥ Γ (v,G),ϕ Ŷ (t) ,</formula><p>which means Ŷ (t) must either be dependent on v or contain domain knowledge information about the function ϕ that is not in ϕ. Since Ŷ (t) is a vector of random variables fully determined by G and ϕ , it cannot depend on an exogenous variable v , Thus, the predictions must contain domain knowledge of ϕ . Hence, we can directly incorporate this domain knowledge into another classifier ϕ † s.t. Y v ⊥ ⊥ Γ (v,G),ϕ † Ŷ (t) , for instance ϕ † is a function of ϕ . In this case, ϕ † will predict the label of v with equal or higher accuracy than collective classification based on predicted labels Ŷ , which finishes our proof.  Creating more expressive GNN representations by averaging out random features was first proposed by <ref type="bibr" target="#b17">Murphy et al. (2019)</ref>. <ref type="bibr" target="#b17">Murphy et al. (2019)</ref> shows a whole-graph classification application, Circulant Skip Links (CSL) graphs, where such randomized feature averaging is provably (and empirically) more expressive than GNNs. Our Monte Carlo collective learning method can be seen as a type of feature averaging GNN representation though , unlike <ref type="bibr" target="#b17">Murphy et al. (2019)</ref>, the feature sampling is not at random, but rather driven by our own model recursively. Hence, it is fair to ask if our performance gains are simply because random feature averaging is beneficial to GNN representations? Or does collective learning sampling actually improve performance? We need an ablation study.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Additional ablation study</head><p>Therefore, in this section we investigate whether the gains of our method for unlabeled test data are from incorporating feature randomness, or from sampling w.r.t predicted labels (collective learning). To do so, we replace the samples drawn from previous prediction Ŷ as uniformly drawn from the set of class labels at each gradient step in MCC-GNN. The results are shown in Table <ref type="table" target="#tab_13">5</ref>. Clearly, the random features are not able to consistently improve the model performance as our method does (contrast Table <ref type="table" target="#tab_13">5</ref> with Table <ref type="table" target="#tab_6">3</ref>). In summary, collective learning goes beyond the purely randomized approach of <ref type="bibr" target="#b17">Murphy et al. (2019)</ref>, providing much larger, statistically significant, gains.  <ref type="bibr" target="#b7">(Kipf &amp; Welling, 2016;</ref><ref type="bibr" target="#b11">Luan et al., 2019)</ref>. This is illustrated in Figure <ref type="figure" target="#fig_5">3</ref>, where the random training set of the traditional GNN evaluation methods (in e.g., <ref type="bibr" target="#b7">(Kipf &amp; Welling, 2016;</ref><ref type="bibr" target="#b11">Luan et al., 2019)</ref>) is shown on the left, contrasted with our harder task of connected training set shown on the right. This difference in task is the reason why the model performance reported in our paper is not directly comparable with the reported results in previous GNN papers, even though we used the same implementations and hyperparameter search procedures. </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 .</head><label>1</label><figDesc>Figure 1. Self-supervised collective learning with partially labeled test data</figDesc><graphic url="image-1.png" coords="4,309.81,453.06,226.76,216.00" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head></head><label></label><figDesc>(tr)  out , where true labels Y(tr)  in = Y (tr) L M are used as input to MCC-GNN, and Y (tr) out = Y (tr) L M are used as optimization target, where M := 1 − M is the bitwise negated vector of M .</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head></head><label></label><figDesc>(</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 2 .</head><label>2</label><figDesc>Figure2. MCC-GNN performance with and without updating predicted labels on Cora and Pubmed. X-axis refers to iteration number t in Section 3.2 features encoding label information. Unlike our framework, this model stacks multiple recursive steps and is trained end-to-end, also they use a weak local classifier and simple relation features such as average of neighborhood labels instead of GNNs, and no sampling of labels is included. These studies represent different ideas for bringing the power of neural networks to collective classification.</figDesc><graphic url="image-3.png" coords="8,78.08,383.99,188.71,136.80" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Further</head><label></label><figDesc>notes on experimental setup: Training/test split As seen in Section 4.2, to approximate an inductive learning setting, we use a different train/test data split procedure (i.e. connected training set) on Cora and Pubmed networks from the public version (i.e. random training set) used in most of the existing GNN models</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 3 .</head><label>3</label><figDesc>Figure 3. The different data splits between traditional GNN train/test split evaluation (left) and our-more realistic-connected train/random test split evaluation (right)</figDesc><graphic url="image-4.png" coords="12,100.02,308.27,396.85,192.35" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 2 .</head><label>2</label><figDesc>. Dataset statistics</figDesc><table><row><cell>Dataset</cell><cell cols="4"># Nodes # Attributes # Classes # Test</cell></row><row><cell>Cora</cell><cell>2708</cell><cell>1433</cell><cell>7</cell><cell>1000</cell></row><row><cell>Pubmed</cell><cell>19717</cell><cell>500</cell><cell>3</cell><cell>1000</cell></row><row><cell>Friendster</cell><cell>43880</cell><cell>644</cell><cell>5</cell><cell>6251</cell></row><row><cell>Facebook</cell><cell>4556</cell><cell>3</cell><cell>2</cell><cell>1000</cell></row><row><cell>Protein</cell><cell>12679</cell><cell>29</cell><cell>2</cell><cell>2376</cell></row><row><cell cols="2">4. Experiments</cell><cell></cell><cell></cell><cell></cell></row><row><cell>4.1. Datasets</cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 3 .</head><label>3</label><figDesc>Node classification accuracy with unlabeled test data (%). Not to be compared with Table4due to the (required) different train and test data splits. Numbers in bold represent significant improvement in a paired t-test at the p &lt; 0.05 level, and numbers with * is the best performing method in each column.</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head></head><label></label><figDesc>including dynamic relational</figDesc><table><row><cell></cell><cell></cell><cell></cell><cell>Cora</cell><cell></cell><cell></cell><cell>Pubmed</cell><cell></cell><cell>Friendster</cell><cell>Facebook</cell><cell>Protein</cell></row><row><cell># labels</cell><cell></cell><cell>85 (3.21%)</cell><cell>105 (3.88%)</cell><cell>140 (5.17%)</cell><cell>300 (1.52%)</cell><cell>375 (1.90%)</cell><cell>600 (3.04%)</cell><cell>641 (1.47%)</cell><cell>70 (1.54%)</cell><cell>7607 (30%)</cell></row><row><cell>Random</cell><cell></cell><cell>14.28 (0.00)</cell><cell>14.28 (0.00)</cell><cell>14.28 (0.00)</cell><cell>33.33 (0.00)</cell><cell>33.33 (0.00)</cell><cell>33.33 (0.00)</cell><cell>20.00 (0.00)</cell><cell>50.00 (0.00)</cell><cell>50.00 (0.00)</cell></row><row><cell>GCN (Kipf &amp; Welling, 2016)</cell><cell cols="3">-+ CL +15.69 (3.20) +14.02 (3.38) 36.38 (1.35) 48.31 (2.58)</cell><cell>64.02 (1.54) +6.31 (0.89)</cell><cell>54.11 (4.86) +5.62 (1.17)</cell><cell>56.31 (3.10) +5.06 (3.24)</cell><cell>68.13 (1.84) + 4.60 (2.50)</cell><cell>28.44 (0.56) +0.90 (0.32)</cell><cell>63.13 (2.12) +2.95 (0.84)</cell><cell>77.54 (1.09) +0.75 (0.33)</cell></row><row><cell>GS (Hamilton et al., 2017)</cell><cell>-+ CL</cell><cell>48.42 (2.82) +4.52 (0.84)</cell><cell>57.52 (2.15) +3.06 (0.20)</cell><cell>65.04 (0.79) +2.18 (0.21)</cell><cell>58.52 (5.42) +2.42 (0.27)</cell><cell>59.77 (4.68) +1.49 (0.10)</cell><cell>75.01 (4.86) +2.67 (0.56)</cell><cell>28.10 (0.48) +0.73 (0.23)</cell><cell>62.99 (0.88) +2.05 (0.04)</cell><cell>73.01 (2.28) +1.47 (0.63)</cell></row><row><cell>TK (Luan et al., 2019)</cell><cell cols="2">-+ CL +7.18 (1.88)  *  55.68 (2.08)</cell><cell>61.51 (2.45) +3.04 (1.07)  *</cell><cell cols="2">67.95 (0.45) +2.75 (0.47)  *  +1.91 (0.75)  *  63.05 (5.15)</cell><cell>67.95 (0.45) +0.54 (0.44)  *</cell><cell>74.01 (3.58) +3.23 (0.78)  *</cell><cell>29.30 (0.15) +0.45 (0.08)  *</cell><cell>65.80 (1.16) +2.37 (0.80)  *</cell><cell>78.94 (1.50) +1.36(0.94)</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_11"><head>Table 4 .</head><label>4</label><figDesc>Node classification accuracy with partially labeled test data (%). Not to be compared with Table3due to the (required) different train and test data splits. Numbers in bold represent significant improvement in a paired t-test at the p &lt; 0.05 level, and numbers with * represent the best performing method in each column.</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_13"><head>Table 5 .</head><label>5</label><figDesc>Node classification accuracy with unlabeled test data (%) using uniform sampling. Numbers in bold represent significant improvement in a paired t-test at the p &lt; 0.05 level.</figDesc><table /></figure>
		</body>
		<back>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Supplementary Material of collective learning GNN Proof of Theorem 1</head><p>We restate the theorem for completeness.</p><p>Theorem 1 (Collective classification can be unecessary). Consider the task of predicting node labels when no labels are available in test data. Let Γ (v, G) be a most-expressive representation of node v ∈ V in graph G . Then, for any collective inference procedure predicting the class label of v ∈ V , there exists a classifier that takes Γ (v, G) as input and predicts the label of v with equal or higher accuracy.</p><p>Proof. Let Ŷ (v) = ϕ(Γ (v, G)) be a classifier function that takes the most expressive representation Γ (v, G) of node v as input and outputs a predicted class label for v.</p><p>Let Ŷ (t) be the set of predicted labels at iteration t of collective classification and let Y v be the true label of node v ∈ V . Then either (1)</p><p>Case (1): Given the classifier ϕ and the most expressive representation Γ (v, G), the true label of v is independent of the labels predicted with collective classification. In this case, the predicted labels of v's neighbors offer no additional information and, thus, collective classification is unnecessary.</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Protein function prediction via graph kernels</title>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">M</forename><surname>Borgwardt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">S</forename><surname>Ong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Schönauer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Vishwanathan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">J</forename><surname>Smola</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H.-P</forename><surname>Kriegel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Bioinformatics</title>
		<imprint>
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="47" to="56" />
			<date type="published" when="2005">2005</date>
		</imprint>
	</monogr>
	<note>suppl</note>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<author>
			<persName><forename type="first">J</forename><surname>Devlin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M.-W</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Toutanova</surname></persName>
		</author>
		<author>
			<persName><surname>Bert</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1810.04805</idno>
		<title level="m">Pre-training of deep bidirectional transformers for language understanding</title>
				<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Unsupervised visual representation learning by context prediction</title>
		<author>
			<persName><forename type="first">C</forename><surname>Doersch</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">A</forename><surname>Efros</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
				<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="1422" to="1430" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Boosting few-shot visual learning with selfsupervision</title>
		<author>
			<persName><forename type="first">S</forename><surname>Gidaris</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Bursuc</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Komodakis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Pérez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Cord</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
				<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="8059" to="8068" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Inductive representation learning on large graphs</title>
		<author>
			<persName><forename type="first">W</forename><surname>Hamilton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Ying</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Leskovec</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
				<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="1024" to="1034" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">Data-efficient image recognition with contrastive predictive coding</title>
		<author>
			<persName><forename type="first">O</forename><forename type="middle">J</forename><surname>Hénaff</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Razavi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Doersch</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Eslami</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">V</forename><surname>Oord</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1905.09272</idno>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">Outcome correlation in graph neural network regression</title>
		<author>
			<persName><forename type="first">J</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Benson</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">Semi-supervised classification with graph convolutional networks</title>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">N</forename><surname>Kipf</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Welling</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1609.02907</idno>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">Introduction to statistical relational learning</title>
		<author>
			<persName><forename type="first">D</forename><surname>Koller</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Friedman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Džeroski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Sutton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Mccallum</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Pfeffer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Abbeel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M.-F</forename><surname>Wong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Heckerman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Meek</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2007">2007</date>
			<publisher>MIT press</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Unsupervised representation learning by sorting sequences</title>
		<author>
			<persName><forename type="first">H.-Y</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J.-B</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M.-H</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
				<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="667" to="676" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Link-based classification</title>
		<author>
			<persName><forename type="first">Q</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Getoor</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 20th International Conference on Machine Learning (ICML-03)</title>
				<meeting>the 20th International Conference on Machine Learning (ICML-03)</meeting>
		<imprint>
			<date type="published" when="2003">2003</date>
			<biblScope unit="page" from="496" to="503" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">Break the ceiling: Stronger multi-scale deep graph convolutional networks</title>
		<author>
			<persName><forename type="first">S</forename><surname>Luan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X.-W</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Precup</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1906.02174</idno>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<author>
			<persName><forename type="first">H</forename><surname>Maron</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Fetaya</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Segol</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Lipman</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1901.09342</idno>
		<title level="m">On the universality of invariant networks</title>
				<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Shuffle and learn: unsupervised learning using temporal order verification</title>
		<author>
			<persName><forename type="first">I</forename><surname>Misra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">L</forename><surname>Zitnick</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Hebert</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
				<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="527" to="544" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Recurrent neural collective classification</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">D</forename><surname>Monner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">A</forename><surname>Reggia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE transactions on neural networks and learning systems</title>
				<imprint>
			<date type="published" when="2013">2013</date>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="page" from="1932" to="1943" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Deep collective inference</title>
		<author>
			<persName><forename type="first">J</forename><surname>Moore</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Neville</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Thirty-First AAAI Conference on Artificial Intelligence</title>
				<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Weisfeiler and leman go neural: Higher-order graph neural networks</title>
		<author>
			<persName><forename type="first">C</forename><surname>Morris</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Ritzert</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Fey</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">L</forename><surname>Hamilton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">E</forename><surname>Lenssen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Rattan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Grohe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the AAAI Conference on Artificial Intelligence</title>
				<meeting>the AAAI Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page" from="4602" to="4609" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">Relational pooling for graph representations</title>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">L</forename><surname>Murphy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Srinivasan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Rao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Ribeiro</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1903.02541</idno>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Iterative classification in relational data</title>
		<author>
			<persName><forename type="first">J</forename><surname>Neville</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Jensen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. AAAI-2000 workshop on learning statistical models from relational data</title>
				<meeting>AAAI-2000 workshop on learning statistical models from relational data</meeting>
		<imprint>
			<date type="published" when="2000">2000</date>
			<biblScope unit="page" from="13" to="20" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Learning relational probability trees</title>
		<author>
			<persName><forename type="first">J</forename><surname>Neville</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Jensen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Friedland</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Hay</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the ninth ACM SIGKDD international conference on Knowledge discovery and data mining</title>
				<meeting>the ninth ACM SIGKDD international conference on Knowledge discovery and data mining</meeting>
		<imprint>
			<date type="published" when="2003">2003a</date>
			<biblScope unit="page" from="625" to="630" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Simple estimators for relational bayesian classifiers</title>
		<author>
			<persName><forename type="first">J</forename><surname>Neville</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Jensen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Gallagher</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Third IEEE International Conference on Data Mining</title>
				<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2003">2003b</date>
			<biblScope unit="page" from="609" to="612" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Unsupervised learning of visual representations by solving jigsaw puzzles</title>
		<author>
			<persName><forename type="first">M</forename><surname>Noroozi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Favaro</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
				<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="69" to="84" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Overcoming relational learning biases to accurately predict preferences in large scale networks</title>
		<author>
			<persName><forename type="first">Iii</forename><surname>Pfeiffer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">J</forename><surname>Neville</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Bennett</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">N</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 24th International Conference on World Wide Web</title>
				<meeting>the 24th International Conference on World Wide Web</meeting>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="853" to="863" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Column networks for collective classification</title>
		<author>
			<persName><forename type="first">T</forename><surname>Pham</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Tran</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Phung</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Venkatesh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Thirty-First AAAI Conference on Artificial Intelligence</title>
				<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Towards structural logistic regression: Combining relational and statistical learning</title>
		<author>
			<persName><forename type="first">A</forename><surname>Popescul</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">H</forename><surname>Ungar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Lawrence</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">M</forename><surname>Pennock</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Departmental Papers</title>
		<imprint>
			<biblScope unit="page">134</biblScope>
			<date type="published" when="2002">2002</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<author>
			<persName><forename type="first">M</forename><surname>Qu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName><surname>Gmnn</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1905.06214</idno>
		<title level="m">Graph markov neural networks</title>
				<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">A stochastic approximation method</title>
		<author>
			<persName><forename type="first">H</forename><surname>Robbins</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Monro</surname></persName>
		</author>
		<idno type="DOI">10.1214/aoms/1177729586</idno>
		<ptr target="https://doi.org/10.1214/aoms/1177729586" />
	</analytic>
	<monogr>
		<title level="j">Ann. Math. Statist</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page">1951</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Collective classification in network data</title>
		<author>
			<persName><forename type="first">P</forename><surname>Sen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Namata</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Bilgic</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Getoor</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Galligher</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Eliassi-Rad</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">AI magazine</title>
		<imprint>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="93" to="93" />
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<author>
			<persName><forename type="first">B</forename><surname>Srinivasan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Ribeiro</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1910.00452</idno>
		<title level="m">On the equivalence between node embeddings and structural graph representations</title>
				<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<author>
			<persName><forename type="first">L</forename><surname>Teixeira</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Jalaian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Ribeiro</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1905.02296</idno>
		<title level="m">Are graph neural networks miscalibrated?</title>
				<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Pseudolikelihood em for withinnetwork relational learning</title>
		<author>
			<persName><forename type="first">R</forename><surname>Xiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Neville</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2008 Eighth IEEE International Conference on Data Mining</title>
				<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2008">2008</date>
			<biblScope unit="page" from="1103" to="1108" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
		<author>
			<persName><forename type="first">K</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Leskovec</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Jegelka</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1810.00826</idno>
		<title level="m">How powerful are graph neural networks?</title>
				<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
		<title level="m" type="main">Stochastic gradient descent for relational logistic regression via partial network crawls</title>
		<author>
			<persName><forename type="first">J</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Ribeiro</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Neville</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1707.07716</idno>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
