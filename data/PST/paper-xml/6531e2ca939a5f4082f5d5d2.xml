<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">SYNPA: SMT Performance Analysis and Allocation of Threads to Cores in ARM Processors</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date type="published" when="2023-10-19">19 Oct 2023</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Marta</forename><surname>Navarro</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Universitat Polit?cnica de Val?ncia</orgName>
								<address>
									<country key="ES">Spain</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Josu?</forename><surname>Feliu</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Universitat Polit?cnica de Val?ncia</orgName>
								<address>
									<country key="ES">Spain</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Salvador</forename><surname>Petit</surname></persName>
							<email>spetit@disca.upv.es</email>
							<affiliation key="aff0">
								<orgName type="institution">Universitat Polit?cnica de Val?ncia</orgName>
								<address>
									<country key="ES">Spain</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Mar?a</forename><forename type="middle">E</forename><surname>G?mez</surname></persName>
							<email>megomez@disca.upv.es</email>
							<affiliation key="aff0">
								<orgName type="institution">Universitat Polit?cnica de Val?ncia</orgName>
								<address>
									<country key="ES">Spain</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Julio</forename><surname>Sahuquillo</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Universitat Polit?cnica de Val?ncia</orgName>
								<address>
									<country key="ES">Spain</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">SYNPA: SMT Performance Analysis and Allocation of Threads to Cores in ARM Processors</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2023-10-19">19 Oct 2023</date>
						</imprint>
					</monogr>
					<idno type="arXiv">arXiv:2310.12786v1[cs.DC]</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-01-03T09:06+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Simultaneous Multithreading</term>
					<term>Scheduling</term>
					<term>High-Performance Computing</term>
					<term>HPC applications</term>
					<term>Thread-to-Core allocation</term>
					<term>synergistic applications</term>
					<term>intra-core interference</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Simultaneous multithreading processors improve throughput over single-threaded processors thanks to sharing internal core resources among instructions from distinct threads. However, resource sharing introduces inter-thread interference within the core, which has a negative impact on individual application performance and can significantly increase the turnaround time of multi-program workloads. The severity of the interference effects depends on the competing co-runners sharing the core. Thus, it can be mitigated by applying a thread-to-core allocation policy that smartly selects applications to be run in the same core to minimize their interference.</p><p>This paper presents SYNPA, a simple approach that dynamically allocates threads to cores in an SMT processor based on their run-time dynamic behavior. The approach uses a regression model to select synergistic pairs to mitigate intracore interference. The main novelty of SYNPA is that it uses just three variables collected from the performance counters available in current ARM processors at the dispatch stage. Experimental results show that SYNPA outperforms the default Linux scheduler by around 36%, on average, in terms of turnaround time in 8-application workloads combining frontend bound and backend bound benchmarks.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>I. INTRODUCTION</head><p>Simultaneous multithreading (SMT) <ref type="bibr" target="#b0">[1]</ref> is the common multithreading paradigm implemented in recent high-performance server processors. This paradigm allows instructions from multiple threads to be issued in the same cycle, which helps increase the processor throughput. To make the design efficient, the major core resources (e.g., first-level caches and arithmetic operators) are shared among the co-running threads, increasing their utilization. In this way, throughput is improved with minimal area overhead. For instance, Marvell reports an area penalty of only 5% to provide SMT support on the ThunderX3 processor while the performance gain is well over that value <ref type="bibr" target="#b1">[2]</ref>.</p><p>Resource sharing among applications executing in an SMT core causes inter-application interference that impacts the performance of individual applications. As the availability of core resources is key for performance, intra-core resource sharing can increase the execution time of individual applications significantly due to inter-application interference. Performance degradation, however, depends on the resource consumption of the applications co-running on the SMT core. Applications can present a wide diversity of resource demands. For instance, memory-bound applications present a high access rate to the memory hierarchy, while compute-intensive applications stress the arithmetic units. This means that the inter-application interference at the shared resources will strongly depend on the resource demands of the co-running applications.</p><p>As the performance of an application depends on the corunner/s, a plausible approach to boost the processor performance is to rely on an intra-core interference-aware thread allocation policy. This kind of policy selects the applications to be run on each core with the aim of minimizing the intracore interference among them. This goal can be achieved, for instance, by selecting applications whose behaviors complement each other from a resource consumption perspective, e.g., a memory-bound application with a compute-intensive application. This work uses the term synergistic applications to define applications that exhibit complementary behaviors.</p><p>Designing this kind of policy for real processors needs to tackle two main challenges. The first is to determine if it is possible to characterize the synergy of applications with the performance counters available in the target processor. This is challenging as nowadays processors implement hundreds of performance counters working at different stages of the pipeline and measuring distinct magnitudes (e.g., cycles or events). This challenge comprises two major decisions: at which processor stage should performance be analyzed and which performance counters from the ones available in the target processor must be used. The second challenge is how the most synergistic pairs of applications are estimated.</p><p>A few existing approaches have been proposed for Intel <ref type="bibr" target="#b2">[3]</ref> and IBM <ref type="bibr" target="#b3">[4]</ref>, <ref type="bibr" target="#b4">[5]</ref> processors. However, they cannot be applied to ARM processors because the performance counter architecture widely differs across them. Despite ARM processors gaining popularity in the data center segment thanks to their high energy efficiency <ref type="bibr" target="#b5">[6]</ref>, to the best of our knowledge, no existing thread-to-core approach can be applied to increase their throughput.</p><p>In this paper, we propose SYNPA, a thread allocation policy that selects synergistic applications and allocates them to the SMT cores of an ARM processor. SYNPA addresses the aforementioned first challenge by characterizing the performance of applications with a simple three-variable model at the dispatch stage. The second challenge is addressed by devising a linear regression model that predicts the performance of an application when running with another one in an SMT core.</p><p>Experimental results show that SYNPA outperforms the default Linux scheduler in terms of turnaround time by around 36%, on average, in 8-application workloads combining frontend bound with backend bound applications and over 55% in some workloads. SYNPA also improves the Linux scheduler in throughput and fairness by around 2.2% and 25%, respectively, across the same workloads.</p><p>This paper makes the following main contributions:</p><p>? We present a simple model to estimate the performance of applications in SMT execution based on just three performance indicators from the dispatch stage. ? The devised model presents extreme simplicity (only four performance counters), far from existing approaches developed for processors from other vendors <ref type="bibr" target="#b3">[4]</ref>. This translates to a 40% reduction in the overhead to estimate the performance of the possible combinations. ? To the best of our knowledge, this is the first feasible thread allocation approach for an ARM processor with SMT cores.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>II. RELATED WORK</head><p>One of the well-known weaknesses of SMT is that singlethread performance can be widely affected depending on the synergy of the applications executed simultaneously.</p><p>Significant research works developed heuristics to address these weaknesses. Some approaches <ref type="bibr" target="#b6">[7]</ref>, <ref type="bibr" target="#b7">[8]</ref> focus on generic processors using simulators. Snavely and Tullsen <ref type="bibr" target="#b6">[7]</ref> try to address this problem by periodically running a subset of the possible combinations to sample their performance. Cazorla et al. <ref type="bibr" target="#b7">[8]</ref> show that system throughput relies on the scheduling policy regardless of the instruction fetch policy. Other approaches <ref type="bibr" target="#b8">[9]</ref>- <ref type="bibr" target="#b12">[13]</ref> concentrate on commercial processors. <ref type="bibr" target="#b8">[9]</ref> uses offline profiling to improve processor throughput in an Intel Pentium-4 Xeon slightly. Feliu et al. <ref type="bibr" target="#b9">[10]</ref> take scheduling decisions to balance the L1 bandwidth utilization among the cores in an Intel Xeon E5645. Also, using performance counters at runtime but focusing on multithread workloads, Vega et al. <ref type="bibr" target="#b10">[11]</ref> present a heuristic to determine when threads should be consolidated in the SMT cores of the IBM POWER7 processor. Radojkovic et al. <ref type="bibr" target="#b11">[12]</ref> propose a statistical inference method to estimate the task assignment in an UltraSPARC T2 processor. Navarro et al. <ref type="bibr" target="#b12">[13]</ref> propose a heuristic, based on the <ref type="bibr" target="#b2">[3]</ref> method, to identify symbiotic pairs.</p><p>Other works establish linear regression models that use performance counters to estimate the performance of each combination of applications. Two early attempts <ref type="bibr" target="#b13">[14]</ref>, <ref type="bibr" target="#b14">[15]</ref> focused on a simulated generic and simplified processor. Moseley et al. <ref type="bibr" target="#b13">[14]</ref> employ linear modeling and recursive partitioning to estimate the speedup when executing two applications simultaneously. However, no specific performance counter is mentioned. Eyerman and Eeckhout <ref type="bibr" target="#b14">[15]</ref> propose an analytical model that predicts the slowdown suffered by each application when co-scheduled with other applications. The proposed model is based on custom CPI stacks at the dispatch stage <ref type="bibr" target="#b15">[16]</ref>, <ref type="bibr" target="#b16">[17]</ref>; unfortunately, this approach is not practical as performance counters in existing processors can not collect the required events. Finally, we discuss other works closer to ours using linear regression models built with performance counters available in real processors. In SMiTE <ref type="bibr" target="#b17">[18]</ref>, authors propose a regression model that combines sensitivity and contentiousness to estimate performance interference in an Intel Sandy Bridge processor. This work uses a single equation per core and 24 performance counters (12 for application) related to memory events like TLB and caches. Unfortunately, the prediction model performs poorly as the performance monitoring unit is only able to measure events for the entire core. Thus, there is no information for each individual thread running on the core. Feliu et al. <ref type="bibr" target="#b3">[4]</ref> leverage the CPI accounting mechanisms of the IBM POWER8 and adapt the interference model to schedule the optimal combination of applications in the SMT cores. These mechanisms, however, are IBM specific and cannot be applied to processors from other vendors.</p><p>In addition to being the only approach that can work on performance counters of ARM processors, SYNPA is also simpler than the previous approaches proposed for Intel <ref type="bibr" target="#b2">[3]</ref> and IBM <ref type="bibr" target="#b3">[4]</ref>, <ref type="bibr" target="#b4">[5]</ref>. SYNPA requires only three equations and four performance counters to estimate how synergistic is the SMT execution of an application with a particular co-runner. Conversely, <ref type="bibr" target="#b3">[4]</ref>, <ref type="bibr" target="#b4">[5]</ref> propose a model with five equations (with the same number of floating-point multiplications and similar complexity) and six performance counters. Given that the number of possible combinations quickly explodes with the number of cores and applications, the reduction in the number of equations translates into a 40% lower time (overhead) required to estimate the performance of all possible pairs of applications. Similarly, <ref type="bibr" target="#b2">[3]</ref> requires nine metrics that involve 15 performance counters to characterize the performance of an application. In summary, little work has concentrated on regression models addressing the constraints imposed by real processors, where identifying the performance counters from the several hundred available in each specific platform to face the problem, if possible, imposes a real challenge. Moreover, despite the fact that ARM processors have recently emerged in the server market, to the best of our knowledge, this is the first work focusing on these processors.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>III. CHARACTERIZING PERFORMANCE IN ARM PROCESSORS</head><p>This section discusses first general issues about measuring performance at a processor pipeline stage to devise a performance model. After that, we introduce the way we found to characterize the performance of ARM processors.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. General Issues and Architectural Constraints</head><p>Instructions flow from outside the processor and enter into the pipeline at the fetch stage, traverse different stages, and leave the pipeline when they retire at the commit stage. Inside the pipeline, instructions flow in input (program) order at the starting stages (i.e., fetch, decode, and dispatch). After that, instructions are issued to the functional units for execution. At this point, the pipeline splits into multiple pipelines (e.g., integer ALU, floating-point unit, or load unit). Instructions are issued out of program order once they are ready, so they execute and write back their results in an out-of-order manner. After the instructions complete their execution, they are kept in the reorder buffer (ROB) until the commit stage, where they are guaranteed to leave the pipeline in program order.</p><p>Performance can be characterized at multiple points in the pipeline, provided that all executed instructions traverse that point. Figure <ref type="figure" target="#fig_0">1</ref> illustrates this fact. In this example, the processor performance could be monitored at points P1, P2, and P4 as the input flow crosses all these points. These points could refer to the dispatch, issue, and commit stages. The general pipeline is divided into three small pipes, representing the functional units that join back to the general pipeline before instructions leave it. Notice that the overall processor performance could not be measured in one of the small pipelines (e.g., point P3) as they only observe a fraction of the overall flow.</p><p>Previous research has evaluated performance at different points of the pipeline, showing that no point is necessarily better than another to evaluate it <ref type="bibr" target="#b18">[19]</ref>. In any case, when working on a real processor, the point (if any) to monitor the performance is constrained to the available performance counters. For instance, IBM POWER8 allows monitoring the performance at the commit stage <ref type="bibr" target="#b3">[4]</ref>, while Intel processors do it at the issue stage <ref type="bibr" target="#b2">[3]</ref>. Nevertheless, the methods proposed in these works cannot be applied to ARM processors due to the widely different performance counters. Therefore, a new measurement method needs to be devised.</p><p>In order to accurately quantify the application performance, we need a handful of performance counters that measure the application performance at a point in the pipeline traversed by all the instructions. Studying if this is possible and, in such a case, identifying which performance counters to use is challenging since most performance counters are not exclusive to each other. In fact, some of the collected events can overlap each other, which translates into their sum being below or above 100% cycles. As a consequence, in practice, selecting the set of performance counters that will be used can enormously differ depending on the deployed performance counter architecture. Therefore, to develop an accurate performance model, available performance events and their measurement capabilities must be studied stage by stage. Under the above premises, we looked into the available performance counters of ARM processors <ref type="bibr" target="#b19">[20]</ref> to devise a new approach. Unlike in other architectures, we found that hardware events at the dispatch stage in ARM processors are suited to assess performance. More precisely, we found that frontend and backend stalls can be discerned at this stage with the available counters. Moreover, compared to other processor stages, the sum of cycles measured at this stage gets closer to 100% of the total execution cycles. Therefore, based on these performance counters, we devised the following approach to characterize performance for the ARM processor consisting of three main steps.</p><p>Step 1. We first distinguish three main events at the dispatch stage: frontend stalls, backend stalls, and no stalls, as depicted in the first column of Figure <ref type="figure" target="#fig_1">2</ref>. Stalls refer to execution cycles where the dispatch is stalled. In such a case, stalls can be attributed either to the frontend or the backend. When the dispatch queue is empty, no instruction can be dispatched, and the stall should be attributed to the frontend. We refer to these stalls as frontend stalls or F E s . For example, this can occur due to stalls caused by an I-cache miss. On the contrary, when the dispatch queue is not empty but instructions cannot be dispatched due to the lack of a required resource (e.g., a ROB entry), the stall is assigned to the backend (referred to as backend stalls or BE s ). These stalls are mainly due to longlatency instructions, such as a load accessing main memory, causing the ROB to block. The sum of the frontend or the backend stalls does not give the total dispatch cycles, but the difference accounts for the cycles where at least one instruction is dispatched, represented as dispatch cycles or D c .</p><p>To obtain the values of three events, we monitor the performance counters cpu cycles, stall frontend, and stall backend. Table <ref type="table" target="#tab_1">I</ref> describes these hardware events along with the inst spec event, whose usage will be described next.</p><p>Step 2. The events of Step 1 can be further refined. Remember that the dispatch logic can dispatch as many instructions  as the dispatch width in a given cycle. This means that if the dispatch width is N and a single instruction is dispatched in a cycle, that cycle will not be counted as a dispatch stall despite N -1 dispatch slots being wasted. In such a case, the actual stalls can be accurately estimated as N -1 N % for a given cycle. To consider these cases, we compute the cycles needed to dispatch the instructions, assuming that the full dispatch width (4 in our processor) is consumed. We refer to these cycles as equivalent full-dispatch cycles (F -D c ) and calculate them as the number of executed instructions (inst spec performance counter) divided by the processor dispatch width.</p><p>The difference between D c and F -D c reveals a fraction of stalls due to horizontal waste of the dispatch slots (Reveal s ). This fraction was hidden in the characterization of Step 1 due to performance counters limitations, which only count cycles when no instruction is dispatched. Figure <ref type="figure" target="#fig_1">2 (</ref>Step 2) depicts the new characterization with the full-dispatch cycles (lower than the dispatch cycles initially measured) and the revealed stalls (small grey frame of the bar). Notice that the dispatch cycles bar of Step 1 equals the full-dispatch cycles plus the revealed stall cycles of the current Step 2.</p><p>Step 3. Despite there is no performance counter available that provides a clue to attribute stalls in Reveal s (i.e., horizontal waste) to either the frontend or the backend, theoretically, most of them cannot be attributed to the frontend. Frontend stalls are mainly due to events that squash the fetched instructions (e.g., branch mispredictions) or block instruction supply (e.g., I-cache misses). In both cases, the 4 dispatch slots available in a cycle will be wasted, which will be correctly accounted for by the stall frontend performance counter. In contrast, the backend can cause frequent horizontal waste, for instance, due to the required floating-point unit or cache being used but becoming available after a few cycles. Therefore, in Step 3, we assign the revealed stalls to the backend category (R-BE s ), thus resulting in just three main categories (frontend stalls, backend stalls, and full-dispatch cycles) to feed the model and estimate the synergy between pairs of applications. Different alternatives were also evaluated, such as splitting Reveal s into equal (and proportional) parts to front-end and backend stalls. We opt for the selected design choice as it is the one showing the most accurate regression model.</p><p>Finally, unlike other approaches <ref type="bibr" target="#b15">[16]</ref>, <ref type="bibr" target="#b16">[17]</ref>, we intentionally do not make any distinction between correctly executed (retired) and wrong-path (mispeculated) instructions. The devised performance classification seeks to estimate the interapplication interference at the shared dispatch slots. Therefore, from a resource consumption perspective, there is no distinction between a dispatch slot consumed by a committed instruction or by a canceled instruction.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>IV. THE SYNERGISTIC APPROACH</head><p>SYNPA selects the pairs of applications to be executed in each core in order to minimize inter-application interference across all the processor cores. To this end, a linear regression model with just three variables is used. This section discusses the devised regression model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Estimating Performance in SMT Execution</head><p>The model computes the performance degradation a thread suffers over isolated execution when it runs simultaneously with another thread in the same SMT core. As mentioned in Section III-B, the model uses three categories: full-dispatch cycles, frontend stalls, and backend stalls (which also include the horizontal waste). The reasoning behind the model is that, for example, the backend stalls of an application caused by the lack of backend resources (e.g., ROB entries) are expected to increase when contending with another application. Consequently, the sum of three categories (3rd step of Figure <ref type="figure" target="#fig_1">2</ref>) gathered in SMT execution normalized to isolated execution will exceed 100% cycles, which represents the slowdown the application suffers due to SMT execution.</p><p>Accurately estimating to what extent each category varies is challenging as it depends not only on the characteristics of the application itself but also on those of the co-runners, i.e., their interaction within the core. To predict the variation of the categories in SMT execution, we devised a linear regression model. The regression model is based on Equation <ref type="formula" target="#formula_0">1</ref>, where C smt i,j represents the value of category C for application i when it runs together with application j on the same core, C st i and C st j represent the value of category C for application i and j, respectively, when they are executed in isolation in ST (singlethreaded) mode, and ? C , ? C , ?C, and ? C are the coefficients of the model.</p><formula xml:id="formula_0">C smt i,j = ? C + ? C ? C st i + ? C ? C st j + ? C ? C st i ? C st j<label>(1)</label></formula><p>In other words, Equation 1 estimates the value for category C of application i when running with application j on the same core as a weighted sum of the following three components:</p><p>? The value of category C in application i when it runs in isolation (C st i ).</p><p>? The value of category C in application j (the co-runner) when it runs in isolation (C st j ). ? The product of both values above. Each term is multiplied by a coefficient (? C , ? C , and ? C ) that weights its influence to model the category. Regarding the ? C coefficient, known as the independent term, it helps reduce the bias of the model and therefore improves its accuracy. ? C , ? C , ? C , and ? C are obtained for each category as a whole, using linear regression, and therefore they do not depend on each particular application. In general, the model estimates the time for a category in SMT (C smt i,j ) as the sum of that category for the target application in isolation (C st i ) plus Step 1</p><p>Step 2</p><p>Step 3</p><p>Fig. <ref type="figure">3</ref>: General overview of the steps of the SYNPA policy. some interference introduced by the co-runner. Notice that</p><formula xml:id="formula_1">C smt i,j and C smt j,i</formula><p>are not the same (not symmetric) because when both applications run simultaneously on the same SMT core, the slowdown applications i and j suffer differs. This occurs because, within the pair, the applications have different behavior and resource demands.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. SYNPA policy</head><p>At each execution interval or quantum (100ms), each processor core executes a different pair of applications. Performance counters monitor the events shown in Table I on a per-application basis. Once each quantum expires, the policy determines which are the most synergistic pairs of applications and schedules them to maximize performance. Next, we describe the different steps that SYNPA performs, which are represented in Figure <ref type="figure">3</ref>. Notice that these steps are repeated each quantum until the workload execution is completed.</p><p>Step 1. Estimating the value of categories in isolation. As explained above, the regression model predicts the SMT category values from the ones when running in isolation (ST mode). However, during SMT execution, these ST values are not available. Fortunately, Feliu et al. <ref type="bibr" target="#b3">[4]</ref> already addressed this problem and demonstrated that the proposed interference model could be inverted to estimate, with relatively good accuracy, the C st i and C st j values for the categories of two applications (i and j) running in the same SMT core. These values are computed using the values of C smt i,j and C smt j,i , which are available in the SMT mode. Therefore, we leverage this model inversion to obtain C st i and C st j at runtime. To do that, after each quantum expires, SYNPA gathers the event counts for each application and computes the performance categories for the SMT execution (i.e., C smt i,j and C smt j,i ) during the last interval. These categories are normalized with respect to the execution cycles to obtain the probability of occurrence of each category event (C ? smt i,j and C ? smt j,i ). Then, SYNPA applies the inverse model to estimate the ST value of each category (C st i and C st j ).</p><p>Step 2. Estimating the SMT performance of application pairs. With C st i and C st j , we can apply the regression model to predict SMT performance. The model estimates the performance degradation (i.e., slowdown) of a given application when it runs together with another application in the same SMT core. Applying the model equation twice (once for application A when it co-runs with application B and once for application B when it co-runs with application A) allows estimating the impact of A on the performance of B and viceversa. In other words, it estimates how synergistic a pair of applications is. The synergy of all pairs of applications is estimated in this step.</p><p>Step 3. Selecting the synergistic pairs. With the synergy of each pair of applications, we can select the combination with the lowest degradation overall. However, the number of possible combinations grows quickly with the number of cores, thus increasing the overhead of selecting the optimal one by evaluating all possible combinations. To find the optimal combination with the minimum overhead, we model the selection problem as a graph problem and solve it with the Blossom algorithm <ref type="bibr" target="#b20">[21]</ref>. Finally, the applications are allocated to their corresponding core as determined by the most synergistic combination.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Model Training</head><p>Before the model can be used, we should obtain the set of coefficients ? C , ? C , ?C, and ? C for the regression model. As discussed earlier, we devise a model per category, not per application. This makes the proposed model flexible and able to work with any application as long as the training set is diverse enough.</p><p>To train the model, we run 80% of the applications (22 out of 28) in isolation and create a profile with the value of the different categories and the number of committed instructions for each quantum. Next, we run in SMT mode for all the possible pairs of these training applications and collect the same data. As execution progresses slower in SMT mode, the number of committed instructions allows us to map the category values of an application when it runs in isolation to the corresponding values when it runs in SMT mode with another application.</p><p>A random subset of the execution quanta was selected to build the model. The model of each category was trained using the data of these quanta, and the coefficients were selected, minimizing the model error as much as possible. As long as the set of applications used for training is diverse enough, the model only needs to be trained once. After that, the model coefficients can be used to select the most synergistic pairs of applications in the system. Therefore, the cost of training the model can be quickly amortized thanks to the performance benefits it provides.</p><p>Note, however, that the model was built for computeintensive scientific workloads, like those in the SPEC CPU suite, so it is valid for workloads showing this behavior. In order to cover applications showing distinct behavior (e.g.,  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>V. EXPERIMENTAL FRAMEWORK</head><p>This section describes the experimental framework, which consists of the experimental machine and the manager developed to carry out the experiments, the workload design, and the evaluation methodology.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Platform: System and Manager</head><p>SYNPA approach has been implemented in a Cavium ThunderX2 CN9975 processor <ref type="bibr" target="#b21">[22]</ref> and a 64GB DRAM main memory. This processor is based on the Vulcan microarchitecture <ref type="bibr" target="#b22">[23]</ref> and implements the ARMv8.1A <ref type="bibr" target="#b19">[20]</ref> instruction set. It is composed of 28 SMT4 cores and has a 28MB shared LLC. Despite the processor supporting up to 4 threads, it was configured in the BIOS as SMT2 (56 SMT2 cores) since HPC workloads are core resource-hungry and SMT4 can severely damage the performance mainly due to small (32KB) L1 data caches, which make these applications experience a high number of misses, so yielding the system to poor performance. This is the main reason why Intel mostly implements SMT2 processors even with larger 48KB caches. Table <ref type="table" target="#tab_3">II</ref> summarizes the main features of the system, including specific parameters of the core microarchitecture. The system runs a CentOS Linux 7 (AltArch) distribution with kernel 4.18.</p><p>To carry out the experiments, we prototype SYNPA as a user-level thread manager that gathers the required performance counters, uses them to estimate the synergy of the possible pairs of applications, selects the optimal combination, and performs the corresponding thread-to-core mapping. The developed manager uses the perf tool to configure and read performance counters and controls the thread execution and allocation to the different cores using the sched setaffinity system call. To fairly assess the performance of the Linux scheduling policy, we use the same thread manager but allow Linux to select which thread runs in each core (i.e., the threadto-core allocation).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Measurement Methodology</head><p>To evaluate SYNPA, we have designed a wide set of 8application HPC workloads. These workloads combine work-Fig. <ref type="figure">4</ref>: Characterization of the applications in isolated execution.  As we have already discussed, the interference in the shared resources and, therefore, the performance of an SMT processor strongly varies according to the applications executed concurrently on each core. In order to design insightful workloads and evaluate the performance gains that SYNPA is able to achieve in different scenarios, we first characterized the 28 applications studied in this paper.</p><p>Figure <ref type="figure">4</ref> shows the execution time distribution for each application considering the dispatch stalls, both at the backend and frontend, and the full-dispatch cycles when running each application in isolation. Considering this characterization, we classify the studied applications into three main groups depending on the fraction of backend and frontend stalls. The first group, referred to as Backend bound, includes the applications whose dispatch stalls due to the backend represent more than 65% of the cycles. The second group, Frontend bound, includes the applications whose frontend dispatch stalls represent more than 35%. Finally, the third group, (Others), consists of the remaining applications and includes applications with different behaviors whose full dispatch cycles range from 20% (hmmer) to 61.4% (nab r). Table <ref type="table" target="#tab_5">III</ref> shows the classification of each benchmark in these three groups.</p><p>As we aim to evaluate a representative range of scenarios, we built backend-intensive workloads, frontend-intensive workloads, and mixed workloads to evaluate respective scenarios mainly dominated by backend-intensive applications, frontend-intensive applications, and a mix of them. Each  We evaluate a total amount of 20 workloads: 5 backendintensive (be0-be4), 5 frontend-intensive (fe0-fe4), and 10 mixed (fb0-fb9) workloads. Individual applications of the workloads take different times to execute. Therefore, to keep the number of applications constant along the workload execution, we used the following methodology. First, we executed each application in isolation for 60 seconds and recorded its number of retired instructions. This number will be the target number of instructions running in the workload. When running multi-program workloads, we record the performance of each individual application at the time it reaches the target number of instructions. Then, the application is relaunched to keep the workload constant across the entire experiment. The workload execution completes when the slowest application reaches its target number of instructions.</p><p>Finally, experimental results presented for each workload in the next section were obtained, executing nine times each workload. We obtained the average of all these executions and calculated the variation of each execution. If an execution shows an excessive variation (90 approx. difference between the value of one execution and the average value of all the executions), it is discarded in order to obtain a variation coefficient of less than 5%. Notice that we follow this methodology to avoid those values that are not representative of the real behavior of the workloads.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>VI. EXPERIMENTAL EVALUATION</head><p>This section presents the coefficient values and accuracy of the devised model. After that, the performance of SYNPA is evaluated, in terms of turnaround time, fairness, and IPC. SYNPA performance is compared to the Completely Fair Scheduler of the CentOS Linux 7 (AltArch). It is important to remark that the SYNPA approach cannot be compared to existing approaches for Intel and IBM processors, as the events they use are not available in the ARM processor.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Coefficient Values and Model Accuracy</head><p>We built the linear regression model discussed in Section IV-A using the training method described in Section IV-C. Table <ref type="table" target="#tab_7">IV</ref> presents the coefficient values obtained with linear regression to model the behavior of each category (see Section IV-C). As expected, the backend stalls category is the most affected by interference, since threads share (and compete for) critical backend resources such as the data cache, which can significantly increase SMT execution time, as experimental results will show. Regarding the frontend stalls category, it grows in SMT execution proportionally to isolation, even though a relatively high ? C coefficient is added. To some extent, this model reflects the fact that the frontend stages are limited due to the IFetch policies, which only allow a single thread to access the ICache at a given processor cycle. This fact can significantly affect frontend time in SMT mode. Note that this category mainly depends on the application itself (high ? C , and low ? C and ? C ).</p><p>Regarding the number of dispatch cycles in SMT, it can be observed that the ? C value is lower than in isolation. The main reason is that in SMT execution, an application makes slower progress than in ST mode. The number of stalls grows and, therefore, the percentage of full-dispatch cycles reduces. For this category, the ? C coefficient is also not negligible, which indicates that the dispatch rate in SMT mode is affected by the dispatch rate of both threads.</p><p>To assess the accuracy of the model, we obtained the Mean Square Error (MSE) for the backend stalls, frontend stalls, and full-dispatch cycles categories, which are 0.1583, 0.0703, and 0.0021, respectively. The error in the backend stalls category is the highest because this category is the most sensitive to interference variations as it comprises several critical backend resources whose contention is highly dependent on the behavior of the co-runner (as the value of ? C corroborates).</p><p>At first glance, one might think that the more categories a model includes, the higher its accuracy and performance. More precisely, the higher would be the quality of the synergistic pairs that would yield the highest system performance. However, this is not necessarily the case. Notice that the more categories, the more sources of inaccuracy due to model deviations and the higher the model overhead. In fact, we initially developed a preliminary model with ten categories and achieved worse results. More precisely, the backend category was initially split into seven categories depending on the components that raise the dispatch stalls (e.g., ROB full, IQ full, etc.). After developing a simpler model, we found that the sum of the error deviations with more components exceeds the errors of only considering the backend category as a single category. This means that a three-category model presents a big advantage, not only because of its simplicity but also because it presents lower overhead due to fewer equations being used.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Turnaround Time Analysis</head><p>The turnaround time (TT) of a workload is defined by the time the workload takes to execute. More precisely, by the time taken by the slowest benchmark of the workload. Figure <ref type="figure">5</ref> presents the speedup of the TT of SYNPA over Linux for the studied workloads and the average speedup for each workload type. As observed, SYNPA achieves significant improvements over Linux. Focusing first on backend-intensive workloads (be0-be4), it can be appreciated that SYNPA performs much Fig. <ref type="figure">5</ref>: Speedup of the TT over Linux. better than Linux, reaching an average speedup of around 18%. The benefit diminishes significantly in the frontend-intensive workloads (fe0-fe4), despite SYNPA still outperforming Linux by approximately 8%. This indicates that there is little margin for the thread allocation policy to improve performance in workloads dominated by frontend bound benchmarks. This seems rather intuitive as the performance of these benchmarks already suffers significantly from ICache misses in standalone execution. Thus, this problem is likely to aggravate even more in SMT execution. An interesting observation is that SYNPA performance improvements significantly rise with mixed workloads (fb0-fb9), where backend bound applications are mixed with frontend bound applications. In these workloads, SYNPA is able to improve the TT up to 1.55 in fb2, i.e., 55% compared to Linux, with an average speedup of 36%. This means that SYNPA is able to find, at runtime, synergistic pairs of applications whose behavior complements each other and allocates them in the same SMT cores to maximize the overall performance.</p><p>To provide insights that explain the reason why SYNPA is able to achieve so high TT improvements, we measured the values of the three main categories used by the model for each individual benchmark when running the entire workload in SMT execution. For illustrative purposes, Figure <ref type="figure" target="#fig_3">6</ref> illustrates the behavior of three workloads: one backend-intensive (be1), one frontend-intensive (fe2), and one mixed workload (fb2). For each workload, the corresponding figure shows the characterization of each of the eight applications that compose the workload when running with Linux (left bar) and when using SYNPA (right bar). The execution time of each application is normalized to the slowest application of the workload.</p><p>It can be observed that in the frontend-intensive workload (fe2, Figure <ref type="figure" target="#fig_3">6b</ref>), the stall_backend category is really low. Therefore, SYNPA has little chance of reducing these stalls. In addition, the frontend fraction is significantly high across all the applications of the workload. Consequently, the proposed policy has little room to improve performance as the applications that form the workload are not complementary. In contrast, in the backend-intensive workload (be1, Figure <ref type="figure" target="#fig_3">6a</ref>), SYNPA can significantly reduce the stall_backend, improving overall performance. This happens mainly because the stall_frontend category rises and drops at run-time due to the application's phase behavior along their execution time.  Therefore, in this case, there is room for SYNPA to improve performance. Finally, the results in the mixed workload fb2, Figure <ref type="figure" target="#fig_3">6c</ref> are impressive. In this workload, it can be observed that SYNPA significantly reduces both the stall_backend and stall_frontend. The average values of each category (Figure <ref type="figure">5</ref>) support these conclusions since the best average speedup belongs to the mixed group, followed by the backendintensive group, and finally, the frontend-intensive group.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Workload Example Analysis</head><p>Let us explain in detail through a workload example how SYNPA produces synergy among applications, collocating them in the same SMT core, while Linux does not. Let us take as an example the workload fb2. First, notice that the Linux OS scheduler is unaware of the thread's behaviors <ref type="bibr" target="#b12">[13]</ref>; that is, the Linux scheduler assigns applications to cores without considering their behavior. In the case of fb2, the pairs made by Linux are: (lbm_r(00), leela_r(04)), (mcf(01), leela_r(05)), (cactuBSSN_r(02), astar(06)), and (mcf(03), mcf_r(07)).</p><p>The order of each application within the workload as the policy reads it is indicated in brackets. Once allocated, an application remains in the core until its execution finishes. Looking at Figure <ref type="figure" target="#fig_3">6</ref>, it can be seen that one of the two leela_r applications is the critical application that defines the TT (i.e., the slowest one), mainly because the stall_backend category significantly rises. The other instance presents a distinct behavior as it has a different co-runner. Consequently, despite being two instances of the same application, one of them experiences around a 15% higher slowdown than the other, which can be considered a significant drawback of Linux. On the contrary, it can be observed that both instances of the application present roughly the same performance with the SYNPA policy.</p><p>Let us study the way SYNPA behaves in this workload. Each cell in Table <ref type="table" target="#tab_9">V</ref> shows the percentage of time each pair (row and column) of applications is selected by SYNPA. Each cell (X,Y) includes two numbers. The number at the top    of each cell represents the number of times (in percentage) that application X is classified as frontend when paired with application Y. The bottom number quantifies the fraction of time that application X was classified as backend when being paired with application Y. Thus, the sum of the values of the cells in a row (except the last cell) is 100%. For instance, in the cell (astar, cactuBSSN_r), astar behaves 3.45% of times as frontend and 1.88% as a backend, being scheduled together with cactuBSSN_r 5.33% of times. Looking at the cell (cactuBSSN_r, astar) it can be appreciated that cactuBSSN_r behaves as a backend application for the entire execution when scheduled with astar.</p><p>In the colored rows, we study the behavior of the two instances of leela_r that are included in the mix. Each of the two instances of leela_r, an application categorized as frontend bound application (see Table <ref type="table" target="#tab_5">III</ref>), is paired with an application categorized as backend bound (lbm_r, mcf, and cactuBSSN_r) for more than half of the execution time, represented by the four first cells of the row, while the following four cells give the figures where leela_r is paired with a frontend bound application (either the other instance of leela_r or astar and mcf_r).</p><p>Although leela_r is categorized as frontend bound, at execution, it can exhibit frontend (numbers at the top of each cell) and backend (bottom numbers) behaviors. Numbers highlighted in green correspond to synergistic pairs, where interference is minimized (i.e., the best decision). For instance, when the first instance of leela_r has a frontend behavior (top of the cells), the values colored in green account for the times when it is paired with an application categorized as backend bound. Conversely, the green values represent times when leela_r behaves as backend, and it is paired with applications categorized as frontend bound.</p><p>For each row, the last column (labeled as diff. group) gives the percentage of synergistic pairs, which is computed as the quotient of the sum of the numbers (top or bottom of each row) colored in green to the sum of the total row. In total, in 95.5% of the intervals where the first instance of leela_r has a frontend behavior, it is paired with an application categorized as backend bound. This means that only in 4.5% of the intervals where it has a frontend behavior, it is paired with a frontend bound application (numbers highlighted in yellow), which is not the optimal decision. However, in some intervals, it might be impossible to pair all applications with co-runners from a different category. In addition, the figure also represents the predominant category and the value of this category for the co-runner of every quantum of execution (Category fraction of the pair). This information is presented with a point per interval. These results show that the achieved improvements result from finding the best pair for the application in each quantum based on the behavior of all the applications of the workload. The best decision is to pair applications with complementary behavior to minimize interference for the resources. SYNPA follows this criterion unless it is impossible to apply due to the behavior of the workload applications in a particular quantum. Linux always pairs leela_r with a high-backend application even when leela_r is mainly limited by the backend when applying this decision. This scenario is common for the two instances of leela_r with the Linux scheduler. Choosing the co-runner based on the behavior of all the applications improves the execution of both leela_r in mainly three ways: a lower turnaround time (TT), a lower fraction of the backend stalls category, and a higher fraction of the full dispatch category.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D. Fairness and IPC Analysis</head><p>Running the most synergistic pairs of applications in the SMT cores reduces the number of stalls, leading not only to shorter turnaround times but also to higher system fairness and throughput. This section evaluates the benefits that SYNPA achieves compared to Linux in terms of fairness and IPC. Fairness <ref type="bibr" target="#b23">[24]</ref> is evaluated for a given workload as one minus the standard deviation (?) of the individual speedups (IS) divided by the average (?) of the IS. The individual speedup of each application is the ratio between the IPC it obtained in SMT execution (with SYNPA and Linux, respectively) and its IPC in isolated execution. A fairness value of 1 means that the system is completely fair.</p><p>Figure <ref type="figure" target="#fig_5">8</ref> shows the fairness of the Linux and SYNPA approaches. As we can see, SYNPA achieves significantly higher fairness than Linux in the mixed workloads and higher fairness in the backend-intensive ones. Only in the frontendintensive workloads the fairness difference between SYNPA and Linux is small, even though SYNPA still performs fairer than Linux. In these workloads, we also observe the maximum fairness since all applications in the workloads make slow progress. These fairness results were expected as SYNPA is able to reduce the increase of stalls due to inter-application interference, allowing the applications to progress more homogeneously. The fairness differences are up to about 48% in workload fb2 and, on average, are about 25%.</p><p>Finally, Figure <ref type="figure" target="#fig_6">9</ref> presents the IPC speedup results of SYNPA over Linux. We compute the IPC of the workloads using the geometric mean of the IPCs of the application in the workload. As observed, IPC speedup values are lower than those of TT, but again, the mixed workloads are the ones achieving the best speedups, which are, on average, about 2.2%. As expected, frontend-intensive workloads present similar results to Linux, with an average speedup of around 0.8%.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>VII. CONCLUSIONS</head><p>This paper has presented SYNPA, the first thread allocation policy that is able to work constrained to ARM's performance counters. SYNPA implements a three-category linear regression model computed at the dispatch stage. We found that this stage is the only one where overall system performance can be categorized in ARM processors. The categories were defined after a rough search of the deployed performance counters. The devised model guides SYNPA to select the best pair of synergistic applications to be run in each processor core.</p><p>SYNPA covers a gap in current thread allocation models as no one can work constrained to ARM's performance counters. Compared to Linux, experimental results show that SYNPA improves TT in random workloads up to 55% in some workloads and around 36% on average. In addition, fairness is improved up to 25%.</p><p>Finally, we would like to remark that SYNPA could be integrated as a part of the OS scheduler of current ARM processors, as all the performance counters used in this work are in the standard ARM v8.1 PMU. Of course, the regression model should be trained for the workloads to be run on the target system.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 1 :</head><label>1</label><figDesc>Fig. 1: Possible points of the pipeline where values of the performance counters can be obtained.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 2 :</head><label>2</label><figDesc>Fig. 2: Characterization of cycles at the dispatch stage. Some categories are directly measured (M) with performance counters, while others are estimated (E) based on them.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Fig. 6 :</head><label>6</label><figDesc>Fig. 6: Characterization for the 8 applications of three workloads. The left bar of each application corresponds to the Linux scheduler, and the right bar corresponds to the SYNPA policy.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Fig. 7 :</head><label>7</label><figDesc>Fig.7: Dynamic characterization of an execution for the two applications leela r of fb2 with both policies (Linux or SYNPA)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>FinallyFig. 8 :</head><label>8</label><figDesc>Fig. 8: Fairness comparison of Linux and SYNPA.instances of leela_r when applying both allocation policies (Linux and SYNPA), we show the dynamical behavior of these applications during their execution in Figure7. This figure represents the dynamic characterization of the leela_r applications (full dispatch, stall frontend, and stall backend), their turnaround time, and the value of the predominant category of the co-runner application in each quantum (either frontend or backend; dotted line in the figure). For instance, Figure7brepresents the execution of the first instance of leela_r applying SYNPA. In this case, it can be seen that the turnaround time of the application is around 1230 intervals. The dynamic characterization is presented in a stacked area, and in this execution leela_r presents a 20% of stalls of the backend along the entire execution. As we can see, compared with Figure7a, SYNPA almost doubles the value of full dispatch cycles and decreases by one-third the value of the backend stalls, thus performing better with SYNPA.In addition, the figure also represents the predominant category and the value of this category for the co-runner of every quantum of execution (Category fraction of the pair). This information is presented with a point per interval. These results show that the achieved improvements result from finding the best pair for the application in each quantum based on the behavior of all the applications of the workload. The best decision is to pair applications with complementary behavior to minimize interference for the resources. SYNPA follows this criterion unless it is impossible to apply due to the behavior of the workload applications in a particular quantum. Linux always pairs leela_r with a high-backend application even when leela_r is mainly limited by the backend when applying this decision. This scenario is common for the two instances of leela_r with the Linux scheduler. Choosing the co-runner based on the behavior of all the applications improves the execution of both leela_r in mainly three ways: a lower turnaround time (TT), a lower fraction of the backend stalls category, and a higher fraction of the full dispatch category.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Fig. 9 :</head><label>9</label><figDesc>Fig. 9: Speedup of IPC (geomean) over Linux.</figDesc><graphic url="image-14.png" coords="10,309.12,48.97,257.00,105.09" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>TABLE I :</head><label>I</label><figDesc>Hardware events gathered in the ARM processor to perform the performance characterization at dispatch.</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>TABLE II :</head><label>II</label><figDesc>Main features of the experimental processor and memory subsystem. graph workloads), the model needs to be re-trained with the new applications.</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>TABLE III</head><label>III</label><figDesc></figDesc><table /><note><p>: Benchmark grouped backend and frontend bound according to their fraction of backend and frontend dispatch stalls, respectively. loads used for training the model with other workloads that were reserved to evaluate it with new applications.</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>TABLE IV :</head><label>IV</label><figDesc>Model coefficients for the three categories. workload consists of 8 applications. Backend-intensive workloads include 5 or 6 applications randomly selected from the backend bound group and the remaining applications randomly selected from the Others group. Frontend-intensive workloads are built analogously but take most applications from the frontend bound group. Finally, mixed workloads are built randomly, selecting half of the applications from the backend bound group and the other half from the frontend bound group.</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head>TABLE V :</head><label>V</label><figDesc>Percentages of pairs in workload fb2 with SYNPA. The number at the top of each cell corresponds to the percentage where the application is frontend, and the number at the bottom corresponds to the backend.</figDesc><table /></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Simultaneous multithreading: Maximizing on-chip parallelism</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">M</forename><surname>Tullsen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">J</forename><surname>Eggers</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">M</forename><surname>Levy</surname></persName>
		</author>
		<idno type="DOI">10.1145/223982.224449</idno>
		<ptr target="https://doi.org/10.1145/223982.224449" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 22nd Annual International Symposium on Computer Architecture, ser. ISCA &apos;95</title>
		<meeting>the 22nd Annual International Symposium on Computer Architecture, ser. ISCA &apos;95</meeting>
		<imprint>
			<date type="published" when="1995">1995</date>
			<biblScope unit="page" from="392" to="403" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Marvell thunderx3: Nextgeneration arm-based server processor</title>
		<author>
			<persName><forename type="first">R</forename><surname>Sugumar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Shah</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Ramirez</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Micro</title>
		<imprint>
			<biblScope unit="volume">41</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="15" to="21" />
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">A top-down method for performance analysis and counters architecture</title>
		<author>
			<persName><forename type="first">A</forename><surname>Yasin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2014 IEEE International Symposium on Performance Analysis of Systems and Software (ISPASS)</title>
		<imprint>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="35" to="44" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Symbiotic job scheduling on the ibm power8</title>
		<author>
			<persName><forename type="first">J</forename><surname>Feliu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Eyerman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Sahuquillo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Petit</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2016 IEEE International Symposium on High-Performance Computer Architecture</title>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="669" to="680" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Improving ibm power8 performance through symbiotic job scheduling</title>
		<author>
			<persName><forename type="first">J</forename><surname>Feliu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Eyerman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Sahuquillo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Petit</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Eeckhout</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Parallel and Distributed Systems</title>
		<imprint>
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page" from="2838" to="2851" />
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<author>
			<persName><surname>Tadviser</surname></persName>
		</author>
		<ptr target="https://tadviser.com/index.php/" />
	</analytic>
	<monogr>
		<title level="m">Article:Processors (Global Market)</title>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Symbiotic jobscheduling for simultaneous multithreading processor</title>
		<author>
			<persName><forename type="first">A</forename><surname>Snavely</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">M</forename><surname>Tullsen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Architectural Support for Programming Languages and Operating Systems (ASPLOS)</title>
		<imprint>
			<date type="published" when="2000">2000</date>
			<biblScope unit="page" from="234" to="244" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Thread to core assignment in SMT on-chip multiprocessors</title>
		<author>
			<persName><forename type="first">C</forename><surname>Acosta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><forename type="middle">J</forename><surname>Cazorla</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Ramirez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Valero</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Symposium on Computer Architecture and High Performance Computing</title>
		<imprint>
			<publisher>SBAC-PAD</publisher>
			<date type="published" when="2009">2009</date>
			<biblScope unit="page" from="67" to="74" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Architectural support for enhanced SMT job scheduling</title>
		<author>
			<persName><forename type="first">A</forename><surname>Settle</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Kihm</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Janiszewski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Connors</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Parallel Architectures and Compilation Techniques (PACT)</title>
		<imprint>
			<date type="published" when="2004">2004</date>
			<biblScope unit="page" from="63" to="73" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">L1-bandwidth aware thread allocation in multicore smt processors</title>
		<author>
			<persName><forename type="first">J</forename><surname>Feliu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Sahuquillo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Petit</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Duato</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 22nd International Conference on Parallel Architectures and Compilation Techniques</title>
		<meeting>the 22nd International Conference on Parallel Architectures and Compilation Techniques</meeting>
		<imprint>
			<date type="published" when="2013">2013</date>
			<biblScope unit="page" from="123" to="132" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Smt-centric power-aware thread placement in chip multiprocessors</title>
		<author>
			<persName><forename type="first">A</forename><surname>Vega</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Buyuktosunoglu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Bose</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 22nd International Conference on Parallel Architectures and Compilation Techniques (PACT)</title>
		<meeting>the 22nd International Conference on Parallel Architectures and Compilation Techniques (PACT)</meeting>
		<imprint>
			<date type="published" when="2013">2013</date>
			<biblScope unit="page" from="167" to="176" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Optimal task assignment in multithreaded processors: A statistical approach</title>
		<author>
			<persName><forename type="first">P</forename><surname>Radojkovi?</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>?akarevi?</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Moret?</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Verd?</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Pajuelo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><forename type="middle">J</forename><surname>Cazorla</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Nemirovsky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Valero</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Architectural Support for Programming Languages and Operating Systems (ASPLOS)</title>
		<imprint>
			<date type="published" when="2012">2012</date>
			<biblScope unit="page" from="235" to="248" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Hy-sched: A simple hyperthreading-aware thread to core allocation strategy</title>
		<author>
			<persName><forename type="first">M</forename><surname>Navarro</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Pons</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Sahuquillo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Computer Architecture Letters</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="26" to="29" />
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Methods for modeling resource contention on simultaneous multithreading processors</title>
		<author>
			<persName><forename type="first">T</forename><surname>Moseley</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Kihm</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Connors</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Grunwald</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Computer Design: VLSI in Computers and Processors</title>
		<imprint>
			<date type="published" when="2005">2005</date>
			<biblScope unit="page" from="373" to="380" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Probabilistic Job Symbiosis Modeling for SMT Processor Scheduling</title>
		<author>
			<persName><forename type="first">S</forename><surname>Eyerman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Eeckhout</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Architectural Support for Programming Languages and Operating Systems (ASPLOS)</title>
		<imprint>
			<date type="published" when="2010">2010</date>
			<biblScope unit="page" from="91" to="102" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">A performance counter architecture for computing accurate CPI components</title>
		<author>
			<persName><forename type="first">S</forename><surname>Eyerman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Eeckhout</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Karkhanis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">E</forename><surname>Smith</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Architectural Support for Programming Languages and Operating Systems (ASPLOS)</title>
		<imprint>
			<date type="published" when="2006-10">Oct. 2006</date>
			<biblScope unit="page" from="175" to="184" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Per-thread cycle accounting in SMT processors</title>
		<author>
			<persName><forename type="first">S</forename><surname>Eyerman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Eeckhout</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The International Conference on Architectural Support for Programming Languages and Operating Systems (ASPLOS)</title>
		<imprint>
			<date type="published" when="2009-03">Mar. 2009</date>
			<biblScope unit="page" from="133" to="144" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">SMiTe: Precise QoS prediction on real-system SMT processors to improve utilization in warehouse scale computers</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">A</forename><surname>Laurenzano</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Mars</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Symposium on Microarchitecture</title>
		<imprint>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="406" to="418" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Multi-stage cpi stacks</title>
		<author>
			<persName><forename type="first">S</forename><surname>Eyerman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Heirman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">Du</forename><surname>Bois</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Hur</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Computer Architecture Letters</title>
		<imprint>
			<biblScope unit="volume">17</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="55" to="58" />
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">Armv8.1-m performance monitoring user guide</title>
		<author>
			<persName><surname>Arm</surname></persName>
		</author>
		<imprint>
			<date type="published" when="0186">2020, version 1.186</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Maximum matching and a polyhedron with 0, 1-vertices</title>
		<author>
			<persName><forename type="first">J</forename><surname>Edmonds</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of research of the National Bureau of Standards B</title>
		<imprint>
			<biblScope unit="volume">69</biblScope>
			<biblScope unit="page" from="55" to="56" />
			<date type="published" when="1965">1965</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">ThunderX2 CN9975 -Cavium</title>
		<ptr target="https://en.wikichip.org/wiki/cavium/thunderx2/cn9975" />
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="2023" to="2025" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">Vulcan -Microarchitectures -Cavium</title>
		<ptr target="https://en.wikichip.org/wiki/cavium/microarchitectures/vulcan?utmcontent=cmp-true" />
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="2023" to="2023" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">System-level performance metrics for multiprogram workloads</title>
		<author>
			<persName><forename type="first">S</forename><surname>Eyerman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Eeckhout</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Micro</title>
		<imprint>
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="42" to="53" />
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
