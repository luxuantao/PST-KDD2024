<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Face recognition by fusing thermal infrared and visible imagery</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">George</forename><surname>Bebis</surname></persName>
							<email>bebis@cse.unr.edu</email>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science and Engineering</orgName>
								<orgName type="laboratory">Computer Vision Laboratory</orgName>
								<orgName type="institution">University of Nevada</orgName>
								<address>
									<postCode>89557</postCode>
									<settlement>Reno</settlement>
									<region>NV</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Aglika</forename><surname>Gyaourova</surname></persName>
							<email>aglika@cse.unr.edu</email>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science and Engineering</orgName>
								<orgName type="laboratory">Computer Vision Laboratory</orgName>
								<orgName type="institution">University of Nevada</orgName>
								<address>
									<postCode>89557</postCode>
									<settlement>Reno</settlement>
									<region>NV</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Saurabh</forename><surname>Singh</surname></persName>
							<email>saurabh@cse.unr.edu</email>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science and Engineering</orgName>
								<orgName type="laboratory">Computer Vision Laboratory</orgName>
								<orgName type="institution">University of Nevada</orgName>
								<address>
									<postCode>89557</postCode>
									<settlement>Reno</settlement>
									<region>NV</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Ioannis</forename><surname>Pavlidis</surname></persName>
							<email>pavlidis@cs.uh.edu</email>
							<affiliation key="aff1">
								<orgName type="laboratory">Visual Computing Laboratory</orgName>
								<orgName type="institution">University of Houston</orgName>
								<address>
									<settlement>Houston</settlement>
									<region>TX</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Face recognition by fusing thermal infrared and visible imagery</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">073584D03543B225E8A6ACA9C5A70CD6</idno>
					<idno type="DOI">10.1016/j.imavis.2006.01.017</idno>
					<note type="submission">Received 5 October 2005; received in revised form 20 January 2006; accepted 23 January 2006</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.3" ident="GROBID" when="2023-07-28T10:20+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Face recognition</term>
					<term>Infrared</term>
					<term>Visible</term>
					<term>Fusion</term>
					<term>Principal component analysis</term>
					<term>Wavelets</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Thermal infrared (IR) imagery offers a promising alternative to visible imagery for face recognition due to its relative insensitive to variations in face appearance caused by illumination changes. Despite its advantages, however, thermal IR has several limitations including that it is opaque to glass. The focus of this study is on the sensitivity of thermal IR imagery to facial occlusions caused by eyeglasses. Specifically, our experimental results illustrate that recognition performance in the IR spectrum degrades seriously when eyeglasses are present in the probe image but not in the gallery image and vice versa. To address this serious limitation of IR, we propose fusing IR with visible imagery. Since IR and visible imagery capture intrinsically different characteristics of the observed faces, intuitively, a better face description could be found by utilizing the complimentary information present in the two spectra. Two different fusion schemes have been investigated in this study. The first one is pixelbased and operates in the wavelet domain, while the second one is feature-based and operates in the eigenspace domain. In both cases, we employ a simple and general framework based on Genetic Algorithms (GAs) to find an optimum fusion strategy. We have evaluated our approaches through extensive experiments using the Equinox face database and the eigenface recognition methodology. Our results illustrate significant performance improvements in recognition, suggesting that IR and visible fusion is a viable approach that deserves further consideration.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Face recognition technology has a wide range of potential applications related to security and safety including surveillance, information security, access control, and identity fraud. Considerable progress has been made in face recognition over the last decade especially with the development of powerful models of face appearance (i.e. eigenfaces) <ref type="bibr" target="#b0">[1]</ref>. Despite the variety of approaches and tools studied, however, face recognition is not accurate or robust enough to be deployed in uncontrolled environments. Several factors affect face recognition performance including pose variations, facial expression changes, occlusions, and most importantly, illumination changes.</p><p>Recently, a number of studies have demonstrated that thermal IR offers a promising alternative to visible imagery for handling variations in face appearance due to illumination changes <ref type="bibr" target="#b1">[2]</ref>, facial expression <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b3">4]</ref>, and face pose <ref type="bibr" target="#b3">[4]</ref> more successfully. In particular, thermal IR imagery is nearly invariant to changes in ambient illumination <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b2">3]</ref>, and provides a capability for identification under all lighting conditions including total darkness <ref type="bibr" target="#b4">[5]</ref>. Thus, while visiblebased algorithms opt for pure algorithmic solutions into inherent phenomenology problems, IR-based algorithms have the potential to offer simpler and more robust solutions, improving performance in uncontrolled environments and deliberate attempts to obscure identity <ref type="bibr" target="#b5">[6]</ref>.</p><p>Despite its robustness to illumination changes, IR has several drawbacks. First, it is sensitive to temperature changes in the surrounding environment. Currents of cold or warm air could influence the performance of systems using IR imagery. As a result, IR images should be captured in a controlled environment. Second, it is sensitive to variations in the heat patterns of the face. Factors that could contribute to these variations include facial expressions (e.g. open mouth), physical conditions (e.g. lack of sleep), and psychological conditions (e.g. fear, stress, excitement). Finally, IR is opaque to glass. As a result, a large part of the face might be occluded (e.g. by wearing eyeglasses). In contrast to IR, visible imagery is more robust to the above factors but more sensitive to illumination changes.</p><p>The focus of this paper is on the sensitivity of thermal IR to facial occlusions caused by eyeglasses. Objects made of glass act as a temperature screen, completely hiding the parts located behind them. This may affect recognition performance seriously. Our experimental results confirm this claim, showing that face recognition performance in the IR spectrum degrades seriously when eyeglasses are present in the probe image but not in the gallery image and vice versa. To address this serious limitation of IR, we propose fusing IR with visible imagery. Although visible imagery can suffer from highlights on the glasses under certain illumination conditions, the problems are considerably less severe than with IR. Therefore, effective algorithms to fuse information from both spectra have the potential to improve face recognition performance. It is worth mentioning that, there exist several approaches for removing eyeglasses in face images <ref type="bibr" target="#b6">[7]</ref>, however, it is not clear whether these methods would be useful in the IR domain since the eyes are occluded completely due to the fact that eyeglasses block thermal energy (i.e. see Fig. <ref type="figure" target="#fig_1">5</ref>).</p><p>In this study, we have investigated two different fusion schemes, the first performing pixel-based fusion in the wavelet domain, and the second performing feature-based fusion in the eigenspace domain. The key objective is computing a fused image from the IR and visible images, capturing the most salient features from each spectrum. In both cases, we employ GAs to find an optimum fusion strategy to combine information from the two spectra. The proposed fusion schemes are different from recent, decision-based, fusion schemes (i.e. fusing the outputs of different classifiers) proposed in the literature <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b7">[8]</ref><ref type="bibr" target="#b8">[9]</ref><ref type="bibr" target="#b9">[10]</ref>. They are also different from the fusion scheme reported in <ref type="bibr" target="#b9">[10]</ref> where eyeglass regions are replaced by an eye-template fusion is performed in the image domain by weighted pixel averaging.</p><p>We have evaluated the proposed fusion schemes through extensive recognition experiments using the Equinox face data set <ref type="bibr" target="#b10">[11]</ref> and the eigenface recognition methodology <ref type="bibr" target="#b11">[12]</ref>. It should be noted that, we have chosen the eigenface approach only for the purpose of demonstrating the benefits of fusion (i.e. any other recognition technique could have been used for the same purpose). Our experimental results illustrate that fusion offers substantial improvements in recognition performance. Moreover, the evolutionary-based fusion methodology provides a simple and general framework that can be used to improve recognition performance in several cases where IR information becomes less reliable due to various environmental (e.g. temperature changes), physical (e.g. lack of sleep) and physiological conditions (e.g. fear, stress). An earlier version of this work has appeared in <ref type="bibr" target="#b12">[13]</ref>.</p><p>The rest of this paper is organized as follows: In Section 2, we review the problem of face recognition in the IR spectrum. Section 3 reviews fusion of visible with IR imagery in general. The proposed fusion schemes are detailed in Sections 4 and 5 correspondingly. The use of GAs to find optimum fusion strategies is described in Section 6. The face data set and experimental procedure used to evaluate and compare the proposed fusion schemes are given in Sections 7 and 8. Our experimental results and discussion of results are presented in Sections 9 and 10. Finally, Section 11 contains our conclusions and directions for future research.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Review of face recognition in the IR spectrum</head><p>Face recognition in the IR spectrum has received relatively little attention compared to visible spectrum, mostly because of the high cost of IR sensors and lack of IR data sets. A number of recent studies have shown that face recognition in the IR offers many benefits. Prokoski <ref type="bibr" target="#b13">[14]</ref> presents an overview of identification in the IR spectrum while results in an operational scenario are reported in <ref type="bibr" target="#b14">[15]</ref>. A recent review on face recognition in the visible and IR domains can be found in <ref type="bibr" target="#b15">[16]</ref>.</p><p>The effectiveness of visible versus IR spectrum was compared using several recognition algorithms by Wilder et al. <ref type="bibr" target="#b7">[8]</ref>. Using a database of 101 subjects without eyeglasses, varying facial expression, and allowing minor lighting changes, they concluded that there are no significant performance differences between visible and IR recognition across all the algorithms tested. They also concluded that fusing visible and IR decision metrics represents a viable approach for enhancing face recognition performance.</p><p>In <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b10">11,</ref><ref type="bibr" target="#b16">17]</ref>, several appearance-based face recognition methodologies were tested under various lighting conditions and facial expressions. Using radiometrically calibrated thermal imagery, they reported superior performance for IRbased recognition than visible-based recognition. Additional performance improvements were achieved using decisionbased fusion <ref type="bibr" target="#b2">[3]</ref>. In a later study <ref type="bibr" target="#b17">[18]</ref>, the impact to recognition of eye detection in the IR spectrum was investigated.</p><p>In <ref type="bibr" target="#b18">[19]</ref>, a statistical hypothesis pruning methodology was introduced for face recognition in IR. First, each IR face image was decomposed using Gabor filters. Then, each image was represented by a few parameters by modelling the marginal density of the Gabor filter coefficients using Bessel functions. Recognition was performed in the space of parameters of the Bessel functions. This approach has been improved by <ref type="bibr">Buddharaju et al. [20]</ref>.</p><p>The effect of lighting, facial expression, and passage of time between the gallery and probe images were examined by Chen et al. <ref type="bibr" target="#b8">[9]</ref> Although, IR-based recognition outperformed visiblebased recognition assuming lighting and facial expression changes, their experiments demonstrated that IR-based recognition degrades when there is substantial passage of time between the gallery and probe images. Using fusion at the decision level based on ranking and scoring, they were able to develop schemes that outperformed either modality alone. In a related study, IR-based recognition was shown to be less sensitive to changes in 3D head pose and facial expression <ref type="bibr" target="#b3">[4]</ref>. The potential of using near-IR hyperspectral imaging for face recognition was investigated in <ref type="bibr" target="#b20">[21]</ref>. The results of this study showed that the local spectral properties of human tissue are nearly invariant to face orientation and expression, improving recognition performance.</p><p>Heo et al. <ref type="bibr" target="#b9">[10]</ref> considered two types of visible and IR fusion, the first at the data level and the second at the decision level. Data fusion was implemented by applying pixel-based weighted averaging of registered visual and thermal images. Decision fusion was implemented by combining the matching scores of individual recognition modules. To deal with occlusions caused by eyeglasses, they used ellipse fitting to detect the eyeglass regions in the IR image and replaced them with an eye template. Using a commercial face recognition system (i.e. FaceIt), they demonstrated improvements in recognition accuracy. There are two main issues with this approach. First, choosing an optimum set of weights, both for data-based and decision-based fusion, was not addressed satisfactorily. Second, replacing eyeglass regions with an eye template might not be very appropriate in the context of face recognition since the eyes contain important information for identification purposes. In fact, several studies have demonstrated that face recognition is possible using eyes alone <ref type="bibr" target="#b21">[22]</ref>.</p><p>Recently, IR was also used for face detection by Dowdall et al. <ref type="bibr" target="#b22">[23]</ref><ref type="bibr" target="#b23">[24]</ref><ref type="bibr" target="#b24">[25]</ref> and face tracking by Eveland et al. <ref type="bibr" target="#b25">[26]</ref>. In <ref type="bibr" target="#b22">[23]</ref><ref type="bibr" target="#b23">[24]</ref><ref type="bibr" target="#b24">[25]</ref>, multi-band feature extraction was used by capitalizing on the unique reflectance characteristics of the human skin in the near-IR spectrum. In particular, it was found that human skin exhibits an abrupt change in reflectance around 1.4 mm. Exploiting this phenomenology, a highly accurate skin mapping was built by taking a weighted difference of the lower band near-IR image and the upper band near-IR image. This allowed for simple algorithmic-based face detection methods to perform extremely well. In <ref type="bibr" target="#b25">[26]</ref>, a method for modelling thermal emission from human skin was used to segment faces from IR image. Tracking was performed by combing the segmentation model with the condensation algorithm.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Fusion of IR and visible imagery</head><p>Information obtained from a given source or sensor generally contains redundancy, which does not contribute to the knowledge about the scene we want to analyze. On the other hand, the information obtained by different means and tools, can provide varying and complementary details of the object under analysis. Finding the complimentary parts and removing the redundancy is the ultimate goal of information fusion approaches.</p><p>In the past, IR and visible image fusion has been successfully used for visualization purposes <ref type="bibr" target="#b26">[27]</ref>, especially in the remote sensing area. In this case, aerial imaging sensors capture information in several spectral ranges. Developing algorithms to visualize this information is thus very important. Sanjeevi et al. <ref type="bibr" target="#b27">[28]</ref> provide a review and comparisons among existing image fusion techniques for visualization purposes. Fusing data from IR and visible sensors has also led to improved target recognition rates <ref type="bibr" target="#b28">[29]</ref>.</p><p>Choosing an appropriate fusion scheme is both application and data dependent. In the context of our application, infrared cameras measure the heat energy emitted by the face while optical cameras capture the light reflectance of the face surface. As the surface of the face and its temperature do not have anything in common, the information in the IR and visible images is independent and complimentary. In general, pixel by pixel fusion does not preserve the spatial information in the image. In contrast, fusion at multiple-resolution levels allows features with different spatial extend to be fused at the resolution at which they are most salient. In this case, important features appearing at lower resolutions can be preserved in the fusion process.</p><p>The basic idea behind multi-resolution fusion is performing a multiscale transform on each source image then constructing a composite multiscale representation from these according to some specific fusion rules. The fused image is obtained by taking an inverse multiscale transform <ref type="bibr" target="#b29">[30]</ref>. Multiscale face representations have been used in several systems <ref type="bibr" target="#b30">[31]</ref>. Some of the most popular multiscale techniques include the Laplacian pyramid <ref type="bibr" target="#b31">[32]</ref>, Fourier transform, and wavelets <ref type="bibr" target="#b32">[33]</ref>. High frequencies are relatively independent of global changes in the illumination, while the low frequencies take into account the spatial relationships among the pixels and are less sensitive to noise and small changes (e.g. facial expression).</p><p>Next, we describe the fusion schemes studied in this work. The slow heat transfer through the human body causes natural low resolution IR images of human face. Therefore, our first fusion strategy operates in the wavelet domain, taking into consideration the differences in the resolution of the IR and visible-light images and exploiting the benefits of multiresolution representations. Our second fusion strategy operates in the eigenspace domain and fuses together global image features. In each case, we assume that faces are represented by a pair of images, one in the IR spectrum and one in the visible spectrum. Also, it is assumed that both images have been normalized prior to fusion to ensure similar ranges of values (Section 7).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Pixel-based fusion in the wavelet domain</head><p>Wavelets are a type of multi-resolution function approximation that allow for the hierarchical decomposition of a signal or an image <ref type="bibr" target="#b32">[33]</ref>. In particular, they decompose a given signal onto a family of functions with finite support. This family of functions is constructed by the translations and dilations of a single function called mother wavelet. The finite support of the mother wavelet gives exact time localization while the scaling allows extraction of different frequency components. The basic requirement of multi-resolution analysis is formulated by requiring a nesting of the spanned spaces as:</p><formula xml:id="formula_0">/V K1 3V 0 3V 1 /3L 2<label>(1)</label></formula><p>In space V jC1 , we can describe finer details than in space V j . In order to construct a multi-resolution analysis, a scaling function f is necessary, together with a dilated and translated version of it:</p><formula xml:id="formula_1">f j i ðxÞ Z 2 j=2 fð2 j xKiÞ i Z 0;.;2 j K1:<label>(2)</label></formula><p>The important features of a signal can be better described or parameterized, not by using f j i ðxÞ and increasing j to increase the size of the subspace spanned by the scaling function, but by defining a slightly different set of function j j i ðxÞ that span the difference between the spaces spanned by various scales of the scale function. These functions are the wavelets, which span the wavelet space W j such that V jC1 ZV j 4W j , and can be described as:</p><formula xml:id="formula_2">j j i ðxÞ Z 2 j=2 jð2 j xKiÞ i Z 0;.;2 j K1:<label>(3)</label></formula><p>Different scaling functions f j i ðxÞ and wavelets j j i ðxÞ determine various wavelet transforms. In this study, we have employed the Haar wavelet, which is the simplest to implement and computationally the least demanding. Furthermore, since Haar basis forms an orthogonal basis, the transform provides a non-redundant representation of the input images. The Haar scaling function is given by:</p><formula xml:id="formula_3">fðxÞ Z 1 for 0% x! 1 0 otherwise (<label>(4)</label></formula><p>The Haar wavelet is defined as:</p><formula xml:id="formula_4">jðxÞ Z 1 for 0% x! 1 2 K1 for 1 2 % x! 1 0 otherwise 8 &gt; &gt; &gt; &gt; &gt; &gt; &lt; &gt; &gt; &gt; &gt; &gt; &gt; :<label>(5)</label></formula><p>Wavelets capture visually plausible features of the shape and interior structure of objects. Features at different scales capture different levels of detail. Coarse scale features encode large regions while fine scale features describe smaller, local regions. All these features together disclose the structure of an object in different resolutions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Methodology</head><p>Fusion in the wavelet domain involves combining the wavelet coefficients of the visible and IR images. First, we compute a multi-resolution representation for each of the images using Haar wavelets <ref type="bibr" target="#b32">[33]</ref>. To fuse the visible and IR images, we select a subset of coefficients from the IR image and the rest from the visible image. The fused image is obtained by applying the inverse wavelet transform on the selected coefficients. The key question in implementing this idea is which wavelet coefficients to choose from each spectrum.</p><p>Using un-weighted averages is not appropriate since it assumes that the two spectra are equally important and, even further, that they have the same resolution. Several image fusion experiments in the wavelet domain have been reported in <ref type="bibr" target="#b33">[34]</ref>. The most intuitive fusion approach in the wavelet domain is choosing the coefficients with maximum absolute value <ref type="bibr" target="#b34">[35]</ref>. The higher the absolute value of a coefficient is, the higher is the probability that it encodes salient image features. Our experiments using this approach showed poor performance.</p><p>In this study, we employ GAs to decide which wavelet coefficients to select from each spectrum. Fig. <ref type="figure">1</ref> illustrates the main idea of this approach, which is further explained in Section 6. Recognition is performed using the eigenface approach on the fused images.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Feature-based fusion in the eigenspace domain</head><p>The eigenface approach uses Principal Components Analysis (PCA), a classical multivariate statistics method, to linearly project face images in a low-dimensional space <ref type="bibr" target="#b11">[12]</ref>. This space is spanned by the principal components (i.e. eigenvectors corresponding to the largest eigenvalues) of the distribution of the training images. Specifically, representing each image I(x,y) as a N!N vector G i , first the average face J is computed:</p><formula xml:id="formula_5">J Z 1 R X R iZ1 G i (<label>6</label></formula><formula xml:id="formula_6">)</formula><p>where R is the number of faces in the training set. Next, the difference F of each face from the average face is computed:</p><formula xml:id="formula_7">F i ZG i KJ.</formula><p>Then the covariance matrix is estimated by</p><formula xml:id="formula_8">C Z 1 R X R iZ1 F i F T i Z AA T ;<label>(7)</label></formula><p>where,</p><formula xml:id="formula_9">AZ[F 1 F 2 .F R ].</formula><p>The eigenspace can then be defined by computing the eigenvectors m i of C. Since C is very large (N!N), computing its eigenvector will be very expensive. Instead, we can compute n i , the eigenvectors of A T A, an R!R matrix. Then m i can be computed from n i as follows <ref type="bibr" target="#b11">[12]</ref>:</p><formula xml:id="formula_10">m i Z X R jZ1 n ij F j ; j Z 1.R:<label>(8)</label></formula><p>Fig. <ref type="figure">1</ref>. Fusion scheme in the wavelet domain.</p><p>Usually, we only need to keep a much smaller number of eigenvectors R k corresponding to the largest eigenvalues. Given a new image, G, we subtract the mean (FZGKJ) and compute the projection:</p><formula xml:id="formula_11">F Z X R k iZ1 w i m i :<label>(9)</label></formula><p>where w i Z m T i G are the coefficients of the projection. We refer to {w i } as eigenfeatures.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.">Methodology</head><p>Fusion in the eigenspace domain involves combining the eigenfeatures from the visible and IR images. Specifically, first we compute two eigenspaces, one using the visible face images and the other using the IR face images. Then, each face is represented by two sets of eigenfeatures, the first computed by projecting the IR face image in the IR-eigenspace, and the second by projecting the visible face image in the visibleeigenspace. Fusion is performed by selecting some eigenfeatures from the IR-eigenspace and some from the visibleeigenspace. GAs are employed again to decide which eigenfeatures to select from each eigenspace. Fig. <ref type="figure">2</ref> illustrates the main steps of this approach. Recognition is performed in the fused eigenspace.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Fusion using GAs</head><p>Deciding which wavelet coefficients or eigenfeatures to select from each spectrum is essentially a search problem. In this work, we propose using GAs to address this issue. GAs are a class of randomized, parallel search optimization procedures inspired by the mechanisms of natural selection, the process of evolution <ref type="bibr" target="#b35">[36]</ref>. They were designed to efficiently search large, non-linear, poorly understood search spaces. In the past, GAs have been used in target recognition <ref type="bibr" target="#b36">[37]</ref>, object recognition <ref type="bibr" target="#b37">[38]</ref>, face detection/verification <ref type="bibr" target="#b38">[39,</ref><ref type="bibr" target="#b39">40]</ref>, and feature selection <ref type="bibr" target="#b40">[41]</ref><ref type="bibr" target="#b41">[42]</ref><ref type="bibr" target="#b42">[43]</ref><ref type="bibr" target="#b43">[44]</ref>. Goldberg <ref type="bibr" target="#b44">[45]</ref> provides a nice introduction to GAs and the reader is referred to this source for further information.</p><p>Several different methods have been reported in the literature for finding an optimum fusion strategy, some quite simple based on pixel averaging and some more powerful based on probabilistic schemes <ref type="bibr" target="#b45">[46,</ref><ref type="bibr" target="#b46">47]</ref>. Our decision to use GAs was based on several reasons. First, the problem of fusion involves searching very large spaces. GAs have demonstrated good performance in searching large spaces in various application domains including object recognition <ref type="bibr" target="#b37">[38]</ref> and feature selection <ref type="bibr" target="#b42">[43]</ref>. Second, the problem of fusion appears to have many suboptimal solutions. Although GAs do not guarantee to find a global optimum solution, they have the ability to search through very large search spaces and come to nearly optimal solutions fast. Their ability for fast convergence is explained by the schema theorem (i.e. short-length bit patterns in the chromosomes with above average fitness, get exponentially growing number of trials in subsequent generations <ref type="bibr" target="#b35">[36]</ref>). Third, much work in the genetic and evolutionary computing communities has led to growing understanding of why they work well and plenty of empirical evidence to support this claim <ref type="bibr" target="#b47">[48,</ref><ref type="bibr" target="#b48">49]</ref>. Fourth, they suitable for parallelization and linear speedups are the norm, not the exception <ref type="bibr" target="#b49">[50]</ref>. Finally, we have had considerable success in the past applying GAs in related problems <ref type="bibr" target="#b37">[38,</ref><ref type="bibr" target="#b40">[41]</ref><ref type="bibr" target="#b41">[42]</ref><ref type="bibr" target="#b42">[43]</ref><ref type="bibr" target="#b43">[44]</ref>.</p><p>GAs operate iteratively on a population of structures, each of which represents a candidate solution to the problem, encoded as a string of symbols (i.e. chromosome). A randomly generated set of such strings forms the initial population from which the GA starts its search. Three basic genetic operators guide this search: selection, crossover and mutation. The genetic search process is iterative: evaluating, selecting, and recombining strings in the population during each iteration (generation) until reaching some termination condition (Fig. <ref type="figure" target="#fig_0">3</ref>).</p><p>Evaluation of each string is based on a fitness function that is problem-dependent. It determines which of the candidate solutions are better. This corresponds to the environmental determination of survivability in natural selection. Selection of a string, which represents a point in the search space, depends on the string's fitness relative to those of other strings in the population. It probabilistically removes, from the population, those points that have relatively low fitness. Mutation, as in natural systems, is a very low probability operator and just flips Fig. <ref type="figure">2</ref>. Fusion scheme in the eigenspace domain. a specific bit. Mutation plays the role of restoring lost genetic material. Crossover in contrast is applied with high probability. It is a randomized yet structured operator that allows information exchange between points. Its goal is to preserve the fittest individuals without introducing any new value.</p><p>In summary, selection probabilistically filters out solutions that perform poorly, choosing high performance solutions to concentrate on or exploit. Crossover and mutation, through string operations, generate new solutions for exploration. Given an initial population of elements, GAs use the feedback from the evaluation process to select fitter solutions, eventually converging to a population of high performance solutions. GAs do not guarantee a global optimum solution. However, they have the ability to search through very large search spaces and come to nearly optimal solutions fast. Their ability for fast convergence is explained by the schema theorem (i.e. shortlength bit patterns in the chromosomes with above average fitness, get exponentially growing number of trials in subsequent generations <ref type="bibr" target="#b44">[45]</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.1.">Implementation details</head><p>We describe below in more detail the encoding schemes, fitness evaluation functions, and genetic operators used for fusing IR with visible information in the wavelet and eigenspace domains.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.1.1.">Encoding</head><p>In the case of fusion in the wavelet domain, the chromosome is a bit string whose length is determined by the number of wavelet coefficients in the image decomposition. Each bit in the chromosome is associated with a wavelet coefficient at a specific location. The value of a bit in the chromosome determines whether the corresponding wavelet coefficient is selected from the IR (e.g. 0) or from the visible spectrum (e.g. 1) (Fig. <ref type="figure">1</ref>). In the case of fusion in the eigenspace domain, the chromosome is also a bit string whose length is determined by the number of eigenvectors. Here, we use the first 100 eigenvectors from each space (Section 7), thus the chromosome has length 100. Each bit in the chromosome is associated with an eigenfeature at a specific location. The value of a bit in the chromosome determines whether a particular eigenfeature is selected from the visible image (i.e. 1) or the IR image (i.e. 0) (Fig. <ref type="figure">2</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.1.2.">Fitness evaluation</head><p>Each individual in a generation represents a possible way to fuse IR with visible information. To evaluate its effectiveness, we perform the fusion based on the information encoded by this individual and perform recognition using the eigenface approach. Recognition accuracy is computed using a validation dataset (Section 7) and is used to provide a measure of fitness. Upon convergence, the best chromosome found is kept and used to evaluate performance on a test set. Fig. <ref type="figure" target="#fig_2">4</ref> illustrates the evolutionary-based fusion approach in the wavelet domain.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.1.3.">Initial population</head><p>In general, the initial population is generated randomly, (e.g. each bit in an individual is set by flipping a coin). In this way, however, we will end up with a population where each individual contains the same number of 1's and 0's on average. To explore subsets of different numbers of wavelet coefficients or eigenfeatures chosen from each domain, the number of 1's for each individual is generated randomly. Then, the 1's are randomly scattered in the chromosome.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.1.4.">Selection</head><p>Our selection strategy was cross generational. Assuming a population of size N, the offspring double the size of the population and we select the best N individuals from the combined parent-offspring population <ref type="bibr" target="#b50">[51]</ref> (Fig. <ref type="figure" target="#fig_2">4</ref>). </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.1.5.">Crossover</head><p>In general, we do not know how different wavelet coefficients depend on each other. If dependent coefficients are far apart in the chromosome, it is more probable that traditional 1-point crossover, will destroy the schemata. To avoid this problem, uniform crossover is used here. The crossover probability used in our experiments was 0.95.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.1.6.">Mutation</head><p>Mutation is a very low probability operator, which flips the values of randomly chosen bit. The mutation probability used here was 0.03.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.">Face dataset</head><p>In our experiments, we used the face database collected by Equinox Corporation under DARPA's HumanID program <ref type="bibr">[52]</ref>. Specifically, we used the long-wave infrared (LWIR) (i.e. 8-12 mm) and the corresponding visible spectrum images from this database. The data was collected during a 2-day period. Each pair of LWIR and visible light images was taken simultaneously and co-registered with 1/3 pixel accuracy (Fig. <ref type="figure" target="#fig_1">5</ref>). The LWIR images were radiometrically calibrated and stored as gray-scale images with 12 bits per pixels. The visible images are also gray-scale images represented with 8 bits per pixel. The size of the images in the database is 320!240 pixels.</p><p>The database contains frontal faces under the following scenarios: (1) three different light direction-frontal and lateral (right and left); (2) three facial expression-'frown', 'surprise' and 'smile'; (3) vocals pronunciation expressions-subjects were asked to pronounce several vocals from which three representative frames were chosen; and (4) presence of glasses-for subjects wearing glasses, all of the above scenarios were repeated with and without glasses. Both IR and visible face images were preprocessed prior to experimentation by following a procedure similar to that described in <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b16">17]</ref>. The goal of preprocessing was to align and scale the faces, remove background, and account for some illumination variations (Fig. <ref type="figure">1</ref>). For comparison purposes, we have  attempted to evaluate our fusion schemes using a similar experimental protocol to that given in <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b16">17]</ref>. Our evaluation methodology employs a training set (i.e. used to compute the eigenfaces), a gallery set (i.e. set of persons enrolled in the system), a validation set (i.e. used in the fitness evaluation of the GA), and a test set (i.e. probe image set containing the images to be identified).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8.">Experimental procedure</head><p>For training, we used 200 images, randomly chosen from the entire Equinox database. For recognition, we used the Euclidean distance and the first 100 principal components as in <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b16">17]</ref>. In the eigenface approach, an image is represented as a low-dimensional feature vector, containing the coefficients of the projection of the image in the eigenspace. Recognition is performed by matching the coefficients of an unknown face image (i.e. probe) to the coefficients of a set of known images (i.e., gallery) <ref type="bibr" target="#b11">[12]</ref>. Recognition performance was measured by finding the percentage of the images in the test set, for which the top match is an image of the same person from the gallery. If N is the number of images in the test set, the recognition ratio r is computed as follows</p><formula xml:id="formula_12">r Z 1 N X N iZ1 x i ;<label>(10)</label></formula><p>where x i Z1 if the top match from the gallery set belongs to the same subject and x i Z0 otherwise. To mitigate for the relatively small number of images in the database, the average error was recorded using a three-fold cross-validation procedure. In particular, we split each data set used for testing randomly three times by keeping only 75% of the images for testing purposes and the rest 25% for validation purposes. To account for performance variations due to random GA initialization, we averaged the results over three different GA runs for each test, choosing a different random seed each time. Thus, we performed a total of nine runs for each gallery/test set experiment. We used population sizes between 100 and 200 and 100 generations. For convergence, we used the ratio between the best fitness and the average fitness in each generation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8.1.">Eyeglasses experiments</head><p>Measuring the effect of eyeglasses is done by using the expression frames. There are 43 subjects wearing glasses in the EA set making a total of 822 images. Following the terminology in <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b16">17]</ref> we created the following test sets (Fig. <ref type="figure">6</ref>): EG (expression frames with glasses, all illuminations), EnG (expression frames without glasses, all illuminations), EFG (expression frames with glasses, frontal illumination), ELG (expression frames with glasses, lateral illumination), EFnG (expression frames without glasses, frontal illumination), ELnG (expression frames without glasses, lateral illumination). The inclusion relations among these sets are as follows: EG Z ELGg EFG; EnG Z ELnGg EFnG and EGh EnG Z :</p><p>(11)</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8.2.">Facial expression experiments</head><p>The test sets for the facial expression experiments include the images containing the three expression frames and three vocal pronunciation frames. There are 90 subjects with a total of 1266 pairs of images for the expression frames and 1299 for the vocal frames. Some of the subjects in these tests sets wear glasses while others not. Following the terminology in <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b16">17]</ref> we have created the following test sets (Fig. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="9.">Experimental results</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="9.1.">Eyeglasses</head><p>Our experimental results illustrate clearly that IR is robust to illumination changes but performs poorly when glasses are present in the gallery set but not in the test set and vice versa. Considerable improvements in recognition performance were achieved in this case by fusing IR with visible images both in the wavelet (Table <ref type="table">1</ref> and Fig. <ref type="figure" target="#fig_4">8</ref>) and eigenspace (Table <ref type="table" target="#tab_0">2</ref> and Fig. <ref type="figure" target="#fig_5">9</ref>) domains. The improvements were even greater when, in addition to eyeglasses, the test and the gallery set contained images taken under different illuminations. For example, in the EFG/ELnG test case using fusion in the wavelet domain, recognition performance was improved by 46% compared to recognition using visible images and by 82% compared to recognition using IR images (Table <ref type="table">1</ref>). Between the two fusion schemes tested, fusion in the wavelet domain yielded higher recognition performance overall. Note that we did not perform experiments when the intersection between gallery and test sets is not empty.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="9.2.">Facial expression</head><p>The facial expression tests had varying success as shown in Tables <ref type="table">3</ref> and<ref type="table" target="#tab_1">4</ref> and Figs. 10 and 11. In general, fusion led to improved recognition performance compared to recognition in the visible spectrum. Comparing IR with fusion, sometimes IR performed better than fusion and vice versa. However, the observed differences were not statistically significant and are mainly accounted to the presence of undesired illumination effects in the fused images (Section 10). As before, we did not perform experiments when the intersection between gallery and test sets is not empty.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="10.">Discussion</head><p>The horizontal and vertical double lines divide each of Tables <ref type="table">1</ref> and<ref type="table" target="#tab_0">2</ref> into four quadrants (i.e. I to IV, starting from the upper-right corner and moving counterclockwise). Each quadrant represents experiments testing some specific difference between the gallery and the test sets: (1) experiments in quadrant I evaluate the effect of eyeglasses being present in the probe but not in the gallery; <ref type="bibr" target="#b1">(2)</ref> experiments in quadrant III Table <ref type="table">1</ref> Averages and standard deviations for the eyeglasses experiments using fusion in the wavelet domain</p><p>The columns represent the gallery set and the rows represent the test set. The first entry in each cell shows the average performance and standard deviation from the visible images, the second entry is from the IR images, and the third entry is from the fused images. The bottom entry shows the minimum and maximum recognition performances from the three crossvalidation runs achieved when using the fused images. Test scenarios for which the test and the gallery sets had common subsets were not performed.   Several interesting conclusions can be drawn by considering these results. As expected, face recognition in the IR is not influenced by illumination changes. However, IR yielded very low success when eyeglasses were present in the gallery but not in the probe and vice-versa (cases (1) and ( <ref type="formula" target="#formula_1">2</ref>)). The success of visible-based face recognition was less sensitive to this factor (cases (1) and ( <ref type="formula" target="#formula_1">2</ref>)). Illumination changes had an important influence on the success of face recognition in the visible domain (case (3)). The success of face recognition using fused images was similar in all four quadrants. This implies that fusion was able to become less sensitive both to eyeglasses and illumination changes.</p><p>To better illustrate the performance of the fusion approach, we have interpolated the results in Table <ref type="table">1</ref> and used a simple visualization scheme to remove small differences and emphasize major trends in recognition performance (Fig. <ref type="figure">12</ref>). Our visualization scheme assigns a grayscale value to each average from Table <ref type="table">1</ref>) with black implying 0% recognition and white 100% recognition. The empty cells from Table <ref type="table">1</ref> are also shown in black. As it can be observed, Fig. <ref type="figure">12(d</ref>   Using fusion in the wavelet domain led to higher recognition performance compared to using visible images only, however, this was not always the case in the IR, especially when recognition in the IR was high (i.e. greater than 97%). Although the observed performance differences were not statistically significant, we noticed that all the cases where IR performed better than fusion were cases where the illumination direction in the gallery set was different from that in the test set and vice versa, assuming no eyeglasses differences (e.g. ELG/EFG, EFG/ELG, ELnG/EFnG, and EFnG/ELnG). This is accounted mainly to the fact that fusion was not able to completely disregard undesired illumination effects present in the visible images.</p><p>This assumption was confirmed by observing the reconstructed fused images shown in Fig. <ref type="figure" target="#fig_0">13</ref>, as well as their first eigenfaces shown in Fig. <ref type="figure" target="#fig_2">14</ref>. The reconstructed images show a blocky structure. Depending on the length of the basis functions the blocks may turn to spots with mire round shapes and lower contrast but cannot disappear completely. However, the visual quality of the reconstructed images is not important for our application. Overall, the fused images have higher resolution compared to the IR images, however, they are also affected by undesired illumination artifacts present in the visible images. Clearly, the first few eigenfaces of the fused images still encode illumination direction. Fig. <ref type="figure" target="#fig_2">14(c</ref>) shows some fused images using ELnG as gallery while Fig. <ref type="figure" target="#fig_2">14(d)</ref> shows some fused images using EG as gallery. Quite interestingly, the reconstructed regions around the eyes are darker in the last case (i.e. to account for the IR blocking effect in the gallery due to eyeglasses).</p><p>Fusion in the eigenspace domain always yielded higher recognition performance compared to recognition both in the visible and IR domains with only one exception (i.e. EFG/ELG). This exception could be explained again by the difference in illumination direction between the gallery and test sets. For comparison purposes, we reconstructed several visible and IR images from the ELnG set using only the subset of eigenvectors selected from each spectrum (Fig. <ref type="figure" target="#fig_9">15</ref>). The reconstructed visible images reveal clearly that they contain information about illumination direction.</p><p>Although, fusion in the eigenspace domain improved recognition performance more consistently compared to fusion in the wavelet domain, the overall recognition performance using wavelet-based fusion was better (Fig. <ref type="figure">16(a)</ref>) although more computationally intensive. Significant differences in recognition performance can be noticed in the following cases: EFG/EnG (30% higher recognition), EFG/ELnG (38% higher recognition), EFnG/ELG (23% higher recognition), EFnG/EG (12% higher recognition), and ELG/EFnG (11% higher recognition). We attribute the higher recognition performance using fusion in the wavelet domain to the more powerful eigenfeatures computed using fused images.</p><p>In the case of facial expressions (Tables <ref type="table">3</ref> and<ref type="table" target="#tab_1">4</ref> and Figs. <ref type="figure">10</ref> and<ref type="figure">11</ref>), IR performed quite well as it has been reported in other studies <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b3">4]</ref>. Recognition in the visible spectrum was not satisfactory while recognition using fused images had comparable performance to that in the IR. In particular, we observed some performance differences between fusion and IR with IR being better that fusion in certain cases and vice versa. The differences are not statistically significant and are accounted again to different illumination direction between the gallery and probe sets and the preservation of undesired illumination effects in the fused images. Fig. <ref type="figure">16(b)</ref> shows the overall accuracy achieved in these experiments.</p><p>We have also attempted to analyze the GA solutions in order to understand what parts of the face are encoded by IR features and what parts are encoded by visible features. Our analysis indicated that almost half of the features (i.e. wavelet coefficients or eigenfeatures) came from the IR spectrum while the other half came from the visible spectrum. However, the eye regions were encoded mostly using features from the visible spectrum. We did not observe any other major trends in encoding other facial features. We also examined whether information from certain resolution levels (i.e. in the wavelet domain) or eigenvector ranges (i.e. in the eigenspace domain) were represented more heavily in the fused images. The solutions obtained indicated that the eye regions were represented mostly using wavelet coefficients from higher resolution levels and eigenfeatures corresponding to smaller eigenvectors. This results indicates a tendency to improve the resolution of the eye regions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="11.">Conclusions and future work</head><p>We presented and compared two different fusion schemes for combining IR and visible imagery for the purpose of face recognition. The first scheme was pixel-based, operating in the wavelet domain. The second scheme was feature-based, operating in the eigenspace domain. Both schemes aimed at improved and robust recognition performance across variable lighting and presence/absence of eyeglasses. Between the two schemes, fusion in the wavelet domain demonstrated better recognition performance overall (Fig. <ref type="figure">16</ref>) although being more computationally intensive. Fast, Haar-like transforms <ref type="bibr" target="#b51">[53]</ref>, are worth considering to reduce time requirements.</p><p>Further consideration should also be given to the existence of many optimal solutions found by the GA. Although optimal in the training phase (i.e. low validation error), these solutions showed different recognition performances when considered for testing. In investigating these solutions, we were not able to distinguish any pattern in the content of the chromosomes that might have revealed why some chromosomes were better than others. The use of larger validation sets and more selective fitness functions might help to eliminate these issues.</p><p>Future work includes considering more effective fusion schemes. We have performed preliminary experiments in the case of eigenspace-based fusion with chromosomes of length 200, using a 0/1 encoding as before. In this case, the first 100 locations of the chromosome correspond to visible eigenfeatures while the last 100 locations correspond to IR eigenfeatures. In this scheme, instead of selecting a single eigenfeature (i.e. visible or IR) at particular location, we can select or reject both eigenfeatures. Repeating the eyeglasses experiments in this case, we noticed a 3% overall improvement in recognition accuracy. A generalization of this scheme would be using non-binary encoding based on a weighted averaging. In this case, each feature from the visible and IR domains is assigned a weight instead of simply selecting/not-selecting it.</p><p>We also plan to consider more powerful fitness functions, for example, by adding extra terms to control the number of wavelet coefficients selected from different resolution levels (e.g. we can favor high or low resolution levels) or eigenfeatures from different eigenvector ranges (e.g. we can favor large or small eigenvectors). We also plan to consider fitness approximation schemes <ref type="bibr" target="#b52">[54]</ref> to reduce the computational requirements of fitness evaluation, Additional issues for future research include investigating the effect of environmental (e.g. temperature changes), physical (e.g. lack of sleep) and physiological conditions (e.g. fear, stress) to IR performance. For example, Chen et al.  [9] demonstrated that IR-based recognition degrades when there is substantial passage of time between the gallery and probe images. The proposed evolutionary-based fusion methodology is general enough and can be applied in these cases as well to improve recognition performance when IR information is not very reliable.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 3 .</head><label>3</label><figDesc>Fig. 3. GA-based framework.</figDesc><graphic coords="6,88.27,71.22,408.91,218.38" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 5 .</head><label>5</label><figDesc>Fig. 5. Examples of visible and IR image pairs (first row) and preprocessed images (second row).</figDesc><graphic coords="7,320.83,592.23,232.55,129.03" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 4 .</head><label>4</label><figDesc>Fig. 4. Illustration of evolutionary-based fusion approach in the wavelet domain.</figDesc><graphic coords="7,140.71,71.22,323.96,323.15" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Fig. 6 .Fig. 7 .</head><label>67</label><figDesc>Fig. 6. Eyeglasses data.</figDesc><graphic coords="8,370.54,71.22,113.38,139.41" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Fig. 8 .</head><label>8</label><figDesc>Fig. 8. Eyeglasses results in the wavelet domain: (a) same illumination conditions-eyeglasses are not present both in the gallery and probe sets; (b) eyeglasses are present both in the gallery and probe sets-illumination conditions are different; (c) eyeglasses are not present both in the gallery and probe sets-illumination conditions are different; (d) similar to (c) except that the gallery and probe sets contain multiple illuminations.</figDesc><graphic coords="10,65.65,71.22,454.45,253.13" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Fig. 9 .</head><label>9</label><figDesc>Fig. 9. Eyeglasses results in the eigenspace domain: (a) same illumination conditions-eyeglasses are not present both in the gallery and probe sets; (b) eyeglasses are present both in the gallery and probe sets-illumination conditions are different; (c) eyeglasses are not present both in the gallery and probe sets-illumination conditions are different; (d) similar to (c) except that the gallery and probe sets contain multiple illuminations.Table 3 Averages and standard deviations for the facial expression experiments with fusion in the wavelet domain</figDesc><graphic coords="11,105.28,437.80,391.31,304.95" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head></head><label></label><figDesc>) is closer to Fig.12(a) than Fig.12(b) and (c).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Fig. 10 .Fig. 11 .</head><label>1011</label><figDesc>Fig. 10. Facial expression results in the wavelet domain.</figDesc><graphic coords="12,311.02,439.50,232.55,125.18" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Fig. 12 .Fig. 13 .</head><label>1213</label><figDesc>Fig. 12. The average performance values from Table 1, visualized as a grayscale image. See text for details. (a) Ideal case (b) visible images (c) IR images, (d) fused images.</figDesc><graphic coords="13,133.57,71.22,337.98,74.38" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>Fig. 15 .</head><label>15</label><figDesc>Fig. 15. Overall recognition accuracy: (a) case of eyeglasses; (b) case of facial expressions.</figDesc><graphic coords="14,44.67,300.72,227.45,168.26" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head>Fig. 14 .Fig. 16 .</head><label>1416</label><figDesc>Fig. 14. Fusion in the wavelet domain-the first few eigenfaces of several fused images. The second and third eigenfaces show clear influence of right and left lateral illumination.</figDesc><graphic coords="14,114.35,71.22,356.72,54.65" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0"><head></head><label></label><figDesc></figDesc><graphic coords="9,105.28,96.11,393.09,306.82" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 2 averages</head><label>2</label><figDesc>and standard deviations for the eyeglasses experiments with fusion in the eigenspace domain</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 4</head><label>4</label><figDesc>Averages and standard deviations for the facial expression experiments with fusion in the eigenspace domain</figDesc><table /></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" xml:id="foot_0"><p>G.Bebis  et al. / Image and Vision Computing 24 (2006) 727-742</p></note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgements</head><p>This research was supported in part by NSF under CRCD grant No. 0088086.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Face recognition: a literature survey</title>
		<author>
			<persName><forename type="first">W</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Chellappa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Phillips</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Rosenfeld</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Computing Surveys</title>
		<imprint>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="399" to="458" />
			<date type="published" when="2003">2003</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Quantitative measurement of illumination invariance for face recognition using thermal infrared imagery</title>
		<author>
			<persName><forename type="first">L</forename><surname>Wolff</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Socolinsky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Eveland</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Workshop on Computer Vision Beyond the Visible Spectrum</title>
		<meeting><address><addrLine>Hawaii</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2001">2001</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Face recognition with visible and thermal infrared imagery</title>
		<author>
			<persName><forename type="first">D</forename><surname>Socolinsky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Selinger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Neuheisel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computer Vision and Image Understanding</title>
		<imprint>
			<biblScope unit="page" from="72" to="114" />
			<date type="published" when="2003">2003</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">Seeing people in the dark: face recognition in infrared images</title>
		<author>
			<persName><forename type="first">G</forename><surname>Friedrich</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Yeshurun</surname></persName>
		</author>
		<editor>Second BMCV</editor>
		<imprint>
			<date type="published" when="2003">2003</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title/>
		<author>
			<persName><forename type="first">A</forename><surname>Jain</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Bolle</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Pankanti</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Biometrics: Personal Identification in Networked Society</title>
		<imprint>
			<date type="published" when="1999">1999</date>
			<publisher>Kluwer Academic Publishers</publisher>
			<pubPlace>Dordrecht</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">The imaging issue in an automatic face/disguise detection system</title>
		<author>
			<persName><forename type="first">I</forename><surname>Pavlidis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Symosek</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Workshop on Computer Vision Beyond the Visible Spectrum</title>
		<imprint>
			<date type="published" when="2000">2000</date>
			<biblScope unit="page" from="15" to="24" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Glasses removal from facial image using recursive error compensation</title>
		<author>
			<persName><forename type="first">J</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Oh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Ahn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="805" to="811" />
			<date type="published" when="2005">2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Comparison of visible and infrared imagery for face recognition</title>
		<author>
			<persName><forename type="first">J</forename><surname>Wilder</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Phillips</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Wiener</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Second International Conference on Automatic Face and Gesture Recognition</title>
		<meeting><address><addrLine>Killington</addrLine></address></meeting>
		<imprint>
			<date type="published" when="1996">1996</date>
			<biblScope unit="page" from="182" to="187" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Pca-based face recognition in infrared imagery: baseline and comparative studies</title>
		<author>
			<persName><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Flynn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Bowyer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Workshop on Analysis and Modeling of Faces and Gestures</title>
		<meeting><address><addrLine>Nice, France</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2003">2003</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Fusion of visual and thermal signatures with eyeglass removal for robust face recognition</title>
		<author>
			<persName><forename type="first">J</forename><surname>Heo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Kong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Abidi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Abidi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Workshop on Object Tracking and Classification Beyond the Visible Spectrum</title>
		<imprint>
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">Appearance-based facial recognition using visible and thermal imagery: a comparative study, Equinox Corporation no. 02-01</title>
		<author>
			<persName><forename type="first">A</forename><surname>Selinger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Socolinsky</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2002">2002</date>
		</imprint>
	</monogr>
	<note type="report_type">Technical Report</note>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Eigenfaces for recognition</title>
		<author>
			<persName><forename type="first">M</forename><surname>Turk</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Pentland</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">CogNeuro</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="71" to="96" />
			<date type="published" when="1991">1991</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Fusion of infrared and visible images for face recognition</title>
		<author>
			<persName><forename type="first">A</forename><surname>Gyaourova</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Bebis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Pavlidis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Computer Vision Conference (ECCV)</title>
		<imprint>
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">History, current status, and future of infrared identification</title>
		<author>
			<persName><forename type="first">F</forename><surname>Prokoski</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Workshop on Computer Vision Beyond the Visible Spectrum</title>
		<meeting><address><addrLine>Hilton Head</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2000">2000</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Thermal face recognition in an operational scenario</title>
		<author>
			<persName><forename type="first">D</forename><surname>Socolinsky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Selinger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Recent advances in visual and infrared face recognition-a review</title>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">G</forename><surname>Kong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Heo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">R</forename><surname>Abidi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Paik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">A</forename><surname>Abidi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computer Vision and Image Understanding</title>
		<imprint>
			<biblScope unit="volume">97</biblScope>
			<biblScope unit="page" from="103" to="135" />
			<date type="published" when="2005">2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Comparative study of face recognition performance with visible and thermal infrared imagery</title>
		<author>
			<persName><forename type="first">D</forename><surname>Socolinsky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Selinger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Pattern Recognition</title>
		<imprint>
			<date type="published" when="2002">2002</date>
			<biblScope unit="page" from="217" to="222" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Face recognition in the dark</title>
		<author>
			<persName><forename type="first">A</forename><surname>Selinger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Socolinsky</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Workshop on Object Tracking and Classification Beyond the Visible Spectrum</title>
		<imprint>
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Statistical hypothesis pruning for recognizing faces from infrared images</title>
		<author>
			<persName><forename type="first">A</forename><surname>Srivastana</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Image and Vision Computing</title>
		<imprint>
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="page" from="651" to="661" />
			<date type="published" when="2003">2003</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Face recognition in the thermal infrared spectrum</title>
		<author>
			<persName><forename type="first">P</forename><surname>Buddharaju</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Pavlidis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Kakadiaris</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Workshop on Object Tracking and Classification Beyond the Visible Spectrum</title>
		<imprint>
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Face recognition in hyperspectral images</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Healey</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Prasad</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Tromberg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page" from="1552" to="1560" />
			<date type="published" when="2003">2003</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Eye identification for face recognition with neural networks</title>
		<author>
			<persName><forename type="first">A</forename><surname>Eide</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Jahren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Jørgensen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Lindblad</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Lindsey</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Osterud</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SPIE Conference</title>
		<meeting><address><addrLine>Orlando</addrLine></address></meeting>
		<imprint>
			<date type="published" when="1996">1996</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">A face detection method based on multiband feature extraction in the near-ir spectrum</title>
		<author>
			<persName><forename type="first">J</forename><surname>Dowdall</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Pavlidis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Bebis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Workshop on Computer Vision Beyond the Visible Spectrum</title>
		<imprint>
			<date type="published" when="2001">2001</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Face detection in the near-ir spectrum</title>
		<author>
			<persName><forename type="first">J</forename><surname>Dowdall</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Pavlidis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Bebis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Image and Vision Computing</title>
		<imprint>
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="565" to="578" />
			<date type="published" when="2001">2001</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Face detection in the near-ir spectrum</title>
		<author>
			<persName><forename type="first">J</forename><surname>Dowdall</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Pavlidis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Bebis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AeroSense SPIE Conference (Infrared Technology and Applications XXIX)</title>
		<imprint>
			<date type="published" when="2003">2003</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Tracking human faces in infrared video</title>
		<author>
			<persName><forename type="first">C</forename><surname>Eveland</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Socolinsky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Wolff</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Image and Vision Computing</title>
		<imprint>
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="page" from="579" to="590" />
			<date type="published" when="2003">2003</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Local mapping for multispectral image visualization</title>
		<author>
			<persName><forename type="first">P</forename><surname>Scheunders</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Image and Vision Computing</title>
		<imprint>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="issue">13</biblScope>
			<biblScope unit="page" from="971" to="978" />
			<date type="published" when="2001">2001</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Comparison of conventional and wavelet transform techniques for fusion of irs-1c, kiss-iii, and pan images</title>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">S K</forename><surname>Vani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Lakshmi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the ACRS</title>
		<meeting>the ACRS</meeting>
		<imprint>
			<date type="published" when="2001">2001</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="140" to="145" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Target recognition using multiple sensors</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Hecht-Nielsen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Workshop on Neural Networks for Signal Processing</title>
		<imprint>
			<date type="published" when="1993">1993</date>
			<biblScope unit="page" from="411" to="420" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">A general framework fo multiresolution image fusion: from pixels to regions</title>
		<author>
			<persName><forename type="first">G</forename><surname>Piella</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Information Fusion</title>
		<imprint>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page" from="259" to="280" />
			<date type="published" when="2003">2003</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">A feature based approach to face recognition</title>
		<author>
			<persName><forename type="first">B</forename><surname>Manjunath</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Chellappa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Der Malsburg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="1992">1992</date>
			<biblScope unit="page" from="373" to="378" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Image data compression with the laplacian pyramid</title>
		<author>
			<persName><forename type="first">E</forename><surname>Adelson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Burt</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Pattern Recognition and Image Processing</title>
		<imprint>
			<date type="published" when="1981">1981</date>
			<biblScope unit="page" from="218" to="223" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
		<title level="m" type="main">An Introduction to Wavelets</title>
		<author>
			<persName><forename type="first">C</forename><surname>Chui</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1992">1992</date>
			<publisher>Academic Press</publisher>
			<pubPlace>London</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Wavelets and image fusion</title>
		<author>
			<persName><forename type="first">L</forename><surname>Chipman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Orr</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Image Processing</title>
		<imprint>
			<date type="published" when="1995">1995</date>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page" from="248" to="251" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Multisensor image fusion using the wavelet transform</title>
		<author>
			<persName><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Manjunath</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Mitra</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Image Processing</title>
		<meeting><address><addrLine>Austin TX</addrLine></address></meeting>
		<imprint>
			<date type="published" when="1994">1994</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="51" to="55" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<monogr>
		<author>
			<persName><forename type="first">D</forename><surname>Goldberg</surname></persName>
		</author>
		<title level="m">Genetic Algorithms in Search, Optimization, and Machine Learning</title>
		<meeting><address><addrLine>Reading, MA</addrLine></address></meeting>
		<imprint>
			<publisher>Addison-Wesley</publisher>
			<date type="published" when="1989">1989</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Generating image filters for target recognition by genetic learning</title>
		<author>
			<persName><forename type="first">A</forename><surname>Katz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Thrift</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">16</biblScope>
			<biblScope unit="issue">9</biblScope>
			<date type="published" when="1994">1994</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Genetic object recognition using combinations of views</title>
		<author>
			<persName><forename type="first">G</forename><surname>Bebis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Louis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Varol</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Yfantis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Evolutionary Computation</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="132" to="146" />
			<date type="published" when="2002">2002</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Genetic algorithms for object localization in a complex scene</title>
		<author>
			<persName><forename type="first">D</forename><surname>Swets</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Punch</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Image Processing</title>
		<imprint>
			<date type="published" when="1995">1995</date>
			<biblScope unit="page" from="595" to="598" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Face detection and verification using genetic search</title>
		<author>
			<persName><forename type="first">G</forename><surname>Bebis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Uthiram</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Georgiopoulos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Artificial Intelligence Tools</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="225" to="246" />
			<date type="published" when="2000">2000</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<monogr>
		<title level="m" type="main">Genetic feature subset selection for gender classification: a comparison study</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Bebis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Louis</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2002">2002</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Boosting object detection using feature selection</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Bebis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Miller</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Advanced Video and Signal Based Surveillance</title>
		<imprint>
			<date type="published" when="2003">2003</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Object detection using feature subset selection</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Bebis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Miller</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pattern Recognition</title>
		<imprint>
			<biblScope unit="volume">37</biblScope>
			<biblScope unit="page" from="2165" to="2176" />
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">On-road vehicle detection using evolutionary gabor filter optimization</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Bebis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Miller</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Intelligent Transportation Systems</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="125" to="137" />
			<date type="published" when="2005">2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<monogr>
		<author>
			<persName><forename type="first">D</forename><surname>Goldberg</surname></persName>
		</author>
		<title level="m">Genetic Algorithms in Search, Optimization, and Machine Learning</title>
		<meeting><address><addrLine>Reading, MA</addrLine></address></meeting>
		<imprint>
			<publisher>Addison-Wesley</publisher>
			<date type="published" when="1989">1989</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Probabilistic image sensor fusion</title>
		<author>
			<persName><forename type="first">R</forename><surname>Sharma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Leen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Pavel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in Neural Information Processing Systems</title>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<date type="published" when="1999">1999</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Image fusion using the expectation-maximization algorithm and a gaussian mixture model</title>
		<author>
			<persName><forename type="first">R</forename><surname>Blum</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Multi-Sensor Image Fusion and Its Applications</title>
		<editor>
			<persName><forename type="first">R</forename><surname>Blum</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Z</forename><surname>Liu</surname></persName>
		</editor>
		<meeting><address><addrLine>New York</addrLine></address></meeting>
		<imprint>
			<publisher>Marcel Dekker</publisher>
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<monogr>
		<title level="m" type="main">Evolutionary Computation, Toward a New Philosophy of Machine Intelligence</title>
		<author>
			<persName><forename type="first">D</forename><surname>Fogel</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1992">1992</date>
			<publisher>IEEE Press</publisher>
			<pubPlace>New York</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<monogr>
		<title level="m" type="main">Genetic Programming</title>
		<author>
			<persName><forename type="first">J</forename><surname>Koza</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1993">1993</date>
			<publisher>MIT Press</publisher>
			<pubPlace>Cambridge, MA</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<monogr>
		<title level="m" type="main">Efficient and Accurate Parallel Genetic Algorithms and Evolutionary Computation</title>
		<author>
			<persName><forename type="first">E</forename><surname>Cantu-Paz</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2000">2000</date>
			<publisher>Dordecht</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">The chc adaptive search algoruthm: how to have safe search when engaging in non-traditional genetic recombination</title>
		<author>
			<persName><forename type="first">I</forename><surname>Eshelman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The Foundation of Genetic Algorithms Workshop</title>
		<imprint>
			<date type="published" when="1989">1989</date>
			<biblScope unit="page" from="265" to="283" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Robust real-time object detection</title>
		<author>
			<persName><forename type="first">P</forename><surname>Viola</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Jones</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Computer Vision and Pattern Recognition Conference</title>
		<imprint>
			<date type="published" when="2001">2001</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">Fitness approximation in evolutionary computation-a survey</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Sendhoff</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Genetic and Evolutionary Computation Conference</title>
		<imprint>
			<date type="published" when="2002">2002</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
