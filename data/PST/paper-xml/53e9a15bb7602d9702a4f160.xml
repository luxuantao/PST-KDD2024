<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Learning Image Statistics for Bayesian Tracking</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Hedvig</forename><surname>Sidenbladh</surname></persName>
							<email>hedvig@nada.kth.se</email>
						</author>
						<author>
							<persName><forename type="first">Michael</forename><forename type="middle">J</forename><surname>Black</surname></persName>
							<email>black@cs.brown.edu</email>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="department">CVAP/NADA Dept. of Computer Science</orgName>
								<address>
									<postBox>Box 1910</postBox>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff1">
								<orgName type="department">Royal Institute of Technology (KTH)</orgName>
								<orgName type="institution">Brown University</orgName>
								<address>
									<postCode>SE-100 44, 02912</postCode>
									<settlement>Stockholm, Providence</settlement>
									<region>RI</region>
									<country>Sweden, USA</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Learning Image Statistics for Bayesian Tracking</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">35381465246B69D9E54D9D6D307505D2</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.3" ident="GROBID" when="2023-07-28T13:42+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>This paper describes a framework for learning probabilistic models of objects and scenes and for exploiting these models for tracking complex, deformable, or articulated objects in image sequences. We focus on the probabilistic tracking of people and learn models of how they appear and move in images. In particular, we learn the likelihood of observing various spatial and temporal filter responses corresponding to edges, ridges, and motion differences given a model of the person. Similarly, we learn probability distributions over filter responses for general scenes that define a likelihood of observing the filter responses for arbitrary backgrounds. We then derive a probabilistic model for tracking that exploits the ratio between the likelihood that image pixels corresponding to the foreground (person) were generated by an actual person or by some unknown background. The paper extends previous work on learning image statistics and combines it with Bayesian tracking using particle filtering. By combining multiple image cues, and by using learned likelihood models, we demonstrate improved robustness and accuracy when tracking complex objects such as people in monocular image sequences with cluttered scenes and a moving camera.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>This paper extends recent work on learning the statistics of natural images and applies the results to the problem of tracking people in image sequences. We learn probabilistic models of how people appear in images and show how this information can be combined with probabilistic models that capture the statistics of natural scenes <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b10">11,</ref><ref type="bibr" target="#b13">14,</ref><ref type="bibr" target="#b19">20,</ref><ref type="bibr" target="#b25">26]</ref>. In particular, we learn models that characterize the probability of observing various image filter responses given, for example, we are looking at a human arm at a particular orientation. Filter responses corresponding to edges, ridges, and motion for the different limbs of the body and for generic scenes are considered. We show how these learned models can be combined in a Bayesian framework for tracking complex objects such as people. We employ a particle filtering method <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b7">8]</ref> and demonstrate its behavior with examples of people tracking in monocular image sequences  containing clutter, camera motion and self-occlusion.</p><p>Reliable tracking requires a general model of human appearance in images that captures the range of variability in appearance and that is somewhat invariant to changes in clothing, lighting, and background. Motivated by <ref type="bibr" target="#b9">[10]</ref>, probability distributions for various filter responses are constructed. The approach is illustrated in Figure <ref type="figure" target="#fig_1">1</ref>. Given a database of images containing people we manually determine the "ground truth" corresponding to limb boundaries and limb axes for the torso, head, upper and lower arms and upper and lower legs. Discrete probability distributions corresponding to edge and ridge filter responses on the marked boundaries and axes respectively are learned from the data.</p><p>We also collect ground truth data of limb motions between two frames and learn the distribution of temporal image differences between corresponding pixels. In the same spirit, we could learn probabilistic models of skin color <ref type="bibr" target="#b24">[25]</ref> or other texture cues.</p><p>The above distributions characterize the appearance of the "foreground" object. For reliable people tracking we must learn the prior distribution of filter responses in general scenes. We show that the likelihood of observing the filter responses for an image is proportional to the ratio between the likelihood that the foreground image pixels are explained by the foreground object and the likelihood that 1 they are explained by some general background (cf. <ref type="bibr" target="#b16">[17]</ref>):</p><formula xml:id="formula_0">Ô´ ÐÐ Ù × ÖÒ ÖÒ µ Ô´ ÖÒ Ù × ÖÒ µ Ô´ ÖÒ Ù × ÖÒ µ</formula><p>This ratio is highest when the foreground (person) model projects to an image region that is unlikely to have been generated by some general scene but is well explained by the statistics of people. This ratio also implies that there is no advantage to the foreground model explaining data that is equally well explained as background. It is important to note that the "background model" here is completely general and, unlike the common background subtraction techniques, is not tied to a specific, known, scene. Using these ideas, we extend previous work on person tracking by combining multiple image cues, by using learned probabilistic models of object appearance, and by taking into account a probabilistic model of general scenes in the above likelihood ratio. Experimental results suggest that a combination of cues provides a rich likelihood model that results in more reliable and computationally efficient tracking than can be achieved with individual cues. We present the results for the 3D tracking of human limbs in monocular image sequences in the presence of clutter, unknown backgrounds, self occlusion, and camera motion.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Work</head><p>Recent work on learning the low-order spatial statistics of natural scenes shows promise for problems in segmentation, graphics, and image compression <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b9">10,</ref><ref type="bibr" target="#b10">11,</ref><ref type="bibr" target="#b13">14,</ref><ref type="bibr" target="#b19">20,</ref><ref type="bibr" target="#b25">26]</ref>. Here we extend this analysis in a number of directions. First, most previous work has considered the statistics of filter responses corresponding to first derivatives of the image function. Here we also examine filters corresponding to "ridges" <ref type="bibr" target="#b12">[13]</ref> and show that, like edge filters, the distribution of responses is invariant across scale for general scenes. In addition to ridges and edges, motion is an important cue for tracking people. Previous tracking approaches have made simplifying assumptions about the noise in temporal image derivatives. The typical brightness constancy assumption assumes that this noise is Gaussian <ref type="bibr" target="#b20">[21]</ref> while the actual distributions learned here for hand-registered training data show that it is actually highly non-Gaussian.</p><p>Our modeling of the image statistics of people versus backgrounds is similar in spirit to the work of Konishi et al. <ref type="bibr" target="#b9">[10]</ref>. Given images where humans have manually marked what they think of as "edges," Konishi et al. learn a distribution Ô ÓÒ corresponding to the probability of a filter response for these edge locations. In our case, we model the filter responses at the boundary of a limb regardless of whether an actual edge is visible in the scene or not. An edge may or may not be visible at a limb boundary depending on the clothing and contrast between the limb and the background. Thus we can think of the Ô ÓÒ distribution of Konishi et al. <ref type="bibr" target="#b9">[10]</ref> as a generic feature distribution while here we learn an object-specific distribution for people.</p><p>Konishi et al. <ref type="bibr" target="#b9">[10]</ref> also computed the distribution Ô Ó« corresponding to the filter responses away from edges and used the log of the likelihood ratio between Ô ÓÒ and Ô Ó« for edge detection. We add additional background models for the statistics of ridges and temporal differences and exploit the ratio between the probability of foreground (person) filter responses and background responses for tracking. Finally, the absolute contrast between foreground and background is less important for detecting people than the orientation of the features (edges or ridges) and hence we perform local contrast normalization prior to filtering.</p><p>We exploit the above work on learned image statistics to track people in cluttered scenes with a moving camera. Recent Bayesian probabilistic formulations of the tracking problem <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b3">4,</ref><ref type="bibr" target="#b7">8,</ref><ref type="bibr" target="#b15">16,</ref><ref type="bibr" target="#b17">18]</ref> use particle filtering methods <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b7">8]</ref> as we do here. Cham and Rehg <ref type="bibr" target="#b0">[1]</ref> use a fixed template to represent the appearance of each limb. Deutscher et al. <ref type="bibr" target="#b3">[4]</ref> assume large foreground-background contrast and multiple cameras. In our previous work <ref type="bibr" target="#b17">[18]</ref> we used image motion as the cue for Bayesian tracking of 3D human models in monocular sequences. The approach used a robust likelihood model for temporal image differences. While this approach could deal with more complex imaging environments than <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b3">4]</ref>, like all optical flow tracking methods, it was prone to "drift".</p><p>Particle filtering methods represent a complex posterior probability distribution with a discrete set of samples. Each sample corresponds to a possible set of model parameters, , or poses of the body in our case. For each pose we can predict where in the image we expect to see limbs and then check whether the image filter responses support the hypothesis. This is in contrast to tracking approaches that first extract edges and then match the model to them <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b7">8]</ref>.</p><p>Reliable tracking requires multiple spatial and temporal image cues. While many systems combine cues such as motion, color, or stereo for person detection and tracking <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b23">24]</ref>, the formulation and combination of these cues is often ad hoc. Additionally, the appearance of people changes in complex ways and previous approaches have used highly simplified noise models. In contrast, the learned models here account for the variation observed in training data. These edge, ridge, and motion models are then combined in a Bayesian framework.</p><p>Similar in spirit is the tracking work of Sullivan et al. <ref type="bibr" target="#b21">[22,</ref><ref type="bibr" target="#b22">23]</ref> who model the distributions of filter responses for a general background and a particular foreground where the foreground is represented by a generalized template. Given these, they determine if an image patch is background, foreground, or on the boundary by matching the distribution of filter responses in the patch with a learned mixture model of background and foreground filter re-sponses. Our work differs in several ways: We model the ratio between the likelihoods for model foreground points being foreground and background, rather than evaluating the likelihood for model background and foreground in evenly distributed locations in the image. We use several different filter responses, and we use steerable filters <ref type="bibr" target="#b4">[5]</ref> instead of isotropic ones. Furthermore, our objects (human limbs) are, in the general case, too varied in appearance to be modeled by generalized templates.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Learning Human and Scene Models</head><p>A human is modeled as a 3-dimensional articulated assembly of truncated cones. The model parameters, , consist of the relative angles between the limbs (cones) and their angular velocities along with the global position and rotation of the assembly and its translational and angular velocity <ref type="bibr" target="#b17">[18]</ref>. In general we may also have background parameters ¬ that describe, for example, the affine motion of the background. For this paper we treat the background image structure and motion as unknown and leave the explicit estimation of the background parameters for future work.</p><p>The model parameters, determine Ü , the set of image locations corresponding to the foreground (person). Let the set of background pixels be Ü Ü Ü , where Ü is the set of all pixels. ½ Let Ô´ µ be the likelihood of observing filter responses given the parameters, , of the foreground object. Given appropriately sampled sets Ü , Ü , and Ü we treat the filter responses at all pixels as independent and write the likelihood as</p><formula xml:id="formula_1">Ô´ µ Ü¾ Ü Ô Ó« ´ ´Üµµ Ü¾ Ü Ô ÓÒ ´ ´Ü µµ É Ü¾ Ü Ô Ó« ´ ´Üµµ É Ü¾ Ü Ô Ó« ´ ´Üµµ Ü¾ Ü Ô ÓÒ ´ ´Ü µµ since Ü Ü Ü . Ô Ó«</formula><p>represents the probability of observing the filter response ´Üµ given that pixel Ü is in the background, while Ô ÓÒ represents the probability of observing ´Ü µ given that Ü is in the foreground and the model parameters are . Note É Ü¾ Ü Ô Ó« ´ ´Üµµ is independent of . We call this constant term ½ and simplify the likelihood as</p><formula xml:id="formula_2">Ô´ µ ½ Ü¾ Ü Ô ÓÒ ´ ´Ü µµ Ô Ó« ´ ´Üµµ<label>(1)</label></formula><p>This is the normalized ratio of the probability that the foreground pixels are explained by the person model versus that they are explained by a generic background model.</p><p>½ The spatial and temporal statistics of neighboring pixels are unlikely to be independent <ref type="bibr" target="#b22">[23]</ref>. We therefore approximate the set Ü with a randomly sampled subset to approximate pixel independence. The number of samples in the foreground is always the same and covers the visible parts of the human model.</p><p>The filter responses, , are computed from a set of filters that are chosen to capture the spatial and temporal appearance of people and natural scenes. In particular, the filter responses include edge responses , ridge responses Ö and the motion responses Ñ , so that Ö Ñ . Responses are computed at several different image scales. For this purpose, a Gaussian image pyramid is constructed. The lowest level, ¼, is the original image, while pyramid level is obtained from ½ by convolving with a Gaussian filter of standard deviation 1 and subsampling.</p><p>We assume that the responses from the different filters are independent for a given pixel location Ü. Furthermore, the responses for edge and motion at different image levels are considered independent. ¾ The response for ridges is only observed at one scale, depending on the size of the limb (this is discussed further in section 3.2).</p><p>Thus, the likelihood can be written as</p><formula xml:id="formula_3">Ô´ µ Ô´ µ Ô´ Ö µ Ô´ Ñ µ ½ × ¼ ¼ Ü¾ Ü Ô ÓÒ ´ ´Ü µµ Ô Ó« ´ ´Ü µµ ¡ Ü¾ ÜÑ Ô Ñ ÓÒ ´ Ñ ´Ü µµ Ô Ñ Ó« ´ Ñ ´Ü µµ ½ ¡ Ü¾ ÜÖ Ô Ö ÓÒ ´ Ö ´Ü ´ µµ Ô Ö Ó« ´ Ö ´Ü ´ µµµ<label>(2)</label></formula><p>where × ¿ corresponds to four levels in the pyramid, the (where Þ Ö Ñ ) are non-Gaussian and are learned from training data. This training set consists of approximately 150 images and short sequences of people, in which the outline of torso, head, upper and lower arms and legs are marked manually. Examples of marked training images are given in Figure <ref type="figure" target="#fig_3">2</ref>. The marked edges serve as ground truth for the learning of edge responses on and off actual limb edges. The area spanned by the two edges is computed from the marked edges an is used for learning of ridge responses on the limbs. The area spanned by the two edges is also warped between consecutive frames in sequences. The distribution of temporal differences between the warped image pairs is then learned.</p><p>¾ This is a very crude assumption as edge responses are highly correlated across scale. Further work needs to be done to model these correlations.</p><p>¿ The point sets ÜÑ and ÜÖ need not be equal to Ü . For example, it could be beneficial to exclude points near the edges from these sets. Note that the cardinality of these sets defines an implicit weighting of the likelihood terms of each cue.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Edge Cue</head><p>The edge response is a function of Ü Ý , the first derivatives of the image brightness function in the horizontal and vertical directions. Edges are modeled in terms of these filter responses at the four finest levels</p><formula xml:id="formula_4">¼ ½ ¾ ¿ in the image pyramid.</formula><p>More specifically, the image response for an edge of orientation at pyramid level is formulated as the image gradient perpendicular to the edge orientation:</p><formula xml:id="formula_5">´Ü µ × Ò Ü ´Ü µ Ó× Ý ´Ü µ<label>(3)</label></formula><p>where Ü ´Ü µ and Ý ´Ü µ are the image gradients at pyramid level and image position Ü.</p><p>For our purposes, the most interesting property of an edge or a ridge is not its absolute contrast, but rather the scale and orientation of the feature. Therefore, before computing image derivatives at a location Ü Ü Ý , we perform local contrast normalization using a hyperbolic tangent nonlinearity <ref type="bibr" target="#b18">[19]</ref>. Regions with high contrast will be normalized to a contrast of 1, while areas of low contrast are normalized to contrast of 0 (Figure <ref type="figure" target="#fig_4">3</ref>). The resulting filter responses then depend more on orientation than on contrast.</p><p>Our experiments indicate that the edge response is independent of scale <ref type="bibr" target="#b18">[19]</ref>. We therefore build a scaleindependent empirical edge likelihood distribution using filter responses from multiple scales. Foreground. For each of the images in the training set (Figure <ref type="figure" target="#fig_3">2</ref>), the edge orientation Ð of each limb Ð is computed from the manually marked edges. For all pyramid levels, locations Ü are sampled on the marked edge, with  Ð . For each limb, Ð, a separate histogram is constructed, of steered edge responses ´Ü Ð µ <ref type="bibr" target="#b4">[5]</ref>, for the sampled foreground edge locations. The normalized histogram for each limb Ð represents Ô ÓÒ ´ Ðµ, the probability function of edge response conditioned on limb Ð, given that the limb projects to an actual limb. Given an observed response ´Ü Ð µ, the likelihood of observing this response in the foreground (on limb Ð) is Ô ÓÒ ´ ´Ü Ð µ Ðµ.</p><p>The log likelihood, ÐÓ ´Ô ÓÒ µ, for the thigh is shown in Figure <ref type="figure" target="#fig_6">4</ref> (similar distributions are obtained for the other limbs). Background. The background edge distribution is learned from a large set of general images with and without people. A normalized histogram of responses ´Ü µ is created by sampling image locations Ü and orientations uniformly at all pyramid levels . This gives Ô Ó« ´ µ, the probability distribution over edge responses, given that we look at locations and orientations that do not correspond to the edges of human limbs. According to this distribution, the likelihood of observing a certain edge response ´Ü µ in the background is Ô Ó« ´ ´Ü µµ. Figure <ref type="figure" target="#fig_6">4</ref> shows the logarithm of this distribution.</p><p>The background is more likely than the limb edge to have low contrast and hence the background distribution has a higher peak near 0. Large filter responses are also less likely than for limbs, which means that the background distribution has very low values when the response approaches 1. Therefore, if a large filter response is observed, the corresponding image location will have higher probability of originating from a limb boundary, and the log ratio (Figure <ref type="figure" target="#fig_6">4</ref>) between foreground and background likelihood will be larger than 0. If the response is low, the probability of the pixel belonging to the background is high, and the ratio will be smaller than 0. Note that the distributions here have different shapes than those learned by others <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b10">11,</ref><ref type="bibr" target="#b25">26]</ref> due, in part, to the effects of contrast normalization.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Ridge Cue</head><p>In the same spirit as with edge cues, we use the second derivatives of the image in the direction of the model ridge  for comparing real-world and model ridges. The filter response used is Ö , a function of ÜÜ ÜÝ ÝÝ , the second derivatives of the brightness function in the horizontal and vertical directions.</p><p>A ridge is an elongated structure in the image. Following Lindeberg <ref type="bibr" target="#b12">[13]</ref>, we define ridge response as the second gradient perpendicular to the ridge ( ), minus the second gradient parallel to the ridge ( ´ ¾ µ´ ¾ µ ). This will suppress other non-elongated maxima in the image ("blobs"). More specifically, the image response for a ridge of orientation , at pyramid level is formulated as</p><formula xml:id="formula_6">Ö ´Ü µ × Ò ¾ ÜÜ ´Ü µ • Ó× ¾ ÝÝ ´Ü µ ¾ × Ò Ó× ÜÝ ´Ü µ Ó× ¾ ÜÜ ´Ü µ • × Ò ¾ ÝÝ ´Ü µ • ¾ × Ò Ó× ÜÝ ´Ü µ<label>(4)</label></formula><p>Since ridges are highly scale-dependent <ref type="bibr" target="#b12">[13]</ref> we do not expect a strong filter response at scales other than the one corresponding to the width of the limb in the image <ref type="bibr" target="#b18">[19]</ref>. In training, we therefore only consider scales corresponding to the distance between the manually marked edges of the limb. For background training however, all four scales are considered as before.</p><p>We observe <ref type="bibr" target="#b18">[19]</ref> that, as for edges, the distributions are independent of image scale, and hence a scale-independent empirical distribution is learned from responses at all levels. Foreground. For each of the images in the training set (Figure <ref type="figure" target="#fig_3">2</ref>), the scale Ð and direction Ð of each limb Ð are computed from the manually marked edges. We sample locations Ü on the limb foreground, with Ð and Ð . A discrete probability function, Ô Ö ÓÒ ´ Ö Ðµ is constructed as for edges (Figure <ref type="figure" target="#fig_8">5</ref>). Background. As with edges, an empirical distribution of ridge responses, Ö ´Ü µ, in the background is learned for randomly sampled orientations, scales and image locations in the training set. The normalized distribution represents Ô Ó« ´ Ö µ, the probability distribution over ridge filter responses of locations off human limbs (Figure <ref type="figure" target="#fig_8">5</ref>).</p><p>While the background distribution looks similar to the edge response distribution, the foreground distributions is asymmetric about zero. Negative responses, corresponding to ridges orthogonal to the predicted orientation , are unlikely to come from limbs and, hence, the larger the filter response, the more likely it is to come from the foreground. This is reflected in the likelihood ratio (Figure <ref type="figure" target="#fig_8">5</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Motion Cue</head><p>A measure of how well the model parameters, , fit the image data at time Ø is how well they predict the appearance of the human and the background given their appearance in the previous time step. In other words, we want to measure the error in predicting the image at time Ø based on the parameters and the image at time Ø ½. The motion response at time Ø, Ñ Ø , is the pixel difference between the unfiltered image Á Ø , and the image Á Ø ½ , warped according to the body parameters Ø .</p><p>The 3D motion of the human model defines the 2D motion in the foreground portion, Ü , of the image. Thus, the pixel Ü Ø ½ in Ü Ø ½ at time Ø ½ maps to some pixel location Ü Ø in Ü Ø if the limb surface point corresponding to both these image points is non-occluded at time Ø ½ and Ø. The pixel correspondences can be computed from</p><formula xml:id="formula_7">Ø .</formula><p>Given two positions Ü Ø ½ and Ü Ø , corresponding to the same limb surface location, the motion response at time Ø and pyramid level is formulated as</p><formula xml:id="formula_8">Ñ Ø ´ÜØ ½ Ü Ø µ Á Ø ´ÜØ µ Á Ø ½ ´ÜØ ½ µ<label>(5)</label></formula><p>Note that this response function is only valid for positions Ü Ø on the foreground (limb area). Also note that the standard brightness constancy assumption for optical flow assumes that these temporal differences are modeled by a Gaussian distribution <ref type="bibr" target="#b20">[21]</ref>. Since the motion in the background is unknown, the background motion response is defined as Ñ Ø ´ÜØ Ü Ø µ, i.e. the temporal difference between the un-warped images at time Ø ½ and Ø. By training on both stationary and moving sequences, this probability distribution models errors originating from moving texture as well as camera noise. Temporal pixel differences are generally lower at coarse spatial scales since the effects of noise and aliasing are diminished. Therefore, unlike edge and ridge responses, temporal differences are not invariant to scale and we learn separate distributions for different image scales <ref type="bibr" target="#b18">[19]</ref>. Note that it is not possible to pre-compute the ratio between the foreground and background likelihood distributions. This is because the filter responses are based on the underlying motion models which are different for foreground and background. Therefore, it is necessary to index into both the foreground and background distributions separately, and then take the ratio between the two likelihoods obtained.  µµ at image levels ¼ ½ ¾ ¿. Right: Log likelihood for the background, ÐÓ ´ÔÑ Ó« ´ Ñ µµ, at image levels ¼ ½ ¾ ¿. Note that we can not take the ratio between these distributions, since the response Ñ is computed from different pixel differences in the background and on the limbs, originating from different motion models.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Foreground.</head><p>Given training sequences, approximately 35 pairs of consecutive frames are used to learn the distributions of temporal foreground differences. Foreground locations Ü Ø are sampled randomly, and the corresponding locations Ü Ø ½ in the frame before are computed from Ü Ø and the marked edges. The differences at all image levels , Ñ Ø ´ÜØ ½ Ü Ø µ, are collected into histograms separately for each limb Ð and each image scale . The normalized histograms represent Ô Ñ ÓÒ ´ Ñ Ð µ.</p><p>Given a certain observed response Ñ Ø ´ÜØ ½ Ü Ø µ, the likelihood of observing this response in the foreground (on limb Ð and level ) is Ô Ñ ÓÒ ´ Ñ Ø ´ÜØ ½ Ü Ø µ Ð µ. The log likelihood, ÐÓ ´ÔÑ ÓÒ µ, for the thigh is shown in Figure <ref type="figure" target="#fig_10">6</ref>. Background. Consecutive frames from sequences containing moving objects and either a static or moving camera are used as training data for the background distributions. Locations Ü Ø are sampled, and histograms of Ñ Ø ´ÜØ Ü Ø µ are computed, one for each scale . The normalized histograms represent Ô´ Ñ µ, the probability distribution over temporal differences in general backgrounds (Figure <ref type="figure" target="#fig_10">6</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Bayesian Tracking</head><p>Human tracking is formulated as an inference problem <ref type="bibr" target="#b11">[12]</ref>. We adopt a Bayesian formulation and estimate the parameters of the body model over time using particle filtering <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b7">8]</ref>. We briefly sketch the method here; for details the reader is referred to <ref type="bibr" target="#b17">[18]</ref>.</p><p>At each time instant Ø, the configuration of the human model is given by Ø . Prior knowledge about the dynamics of the human body is used to generate hypotheses about the configuration at time Ø given the configuration at the previous time instant. The hypotheses are then compared with the sequence of filter images up to time Ø, Ø . By Bayes' rule, the posterior probability of the model parameters, ,</p><formula xml:id="formula_9">given Ø , Ô´ Ø Ø µ ¾ Ô´ Ø Ø µ Ô´ Ø Ø ½ µÔ´ Ø ½ Ø ½ µ Ø ½ (6)</formula><p>where ¾ is a normalizing constant that does not depend on the state variables.</p><p>The posterior distribution Ô´ Ø Ø µ is modeled using a large number of samples where samples correspond to possible poses of the body, Ø , and their normalized likelihood. We employ between ½¼ ¿ and ½¼ samples to represent the posterior distribution.</p><p>The posterior distribution can be propagated and updated over time using Equation (6) <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b7">8]</ref>. This is done by drawing samples Ø ½ according to their posterior probability at time Ø ½. These samples are then propagated in time by sampling from the temporal prior Ô´ Ø Ø ½ µ, which is either an activity dependent model (e.g. walking motion <ref type="bibr" target="#b17">[18]</ref>) or a general model of smooth motion where all angles in the body are assumed independent. Details of the human motion models are described in <ref type="bibr" target="#b14">[15,</ref><ref type="bibr" target="#b17">18]</ref>.</p><p>The posterior distribution is extremely peaked (i.e. the difference between the smallest and largest likelihood is very large) and thus, only a few of the samples from the posterior will be selected multiple times. Sampling from a broader distribution would result in more stable tracking, since more samples survive in each time step. Hence, instead of drawing samples from the posterior at time Ø ½, we sample from a proposal distribution that is a smoothed (approximate) version of the posterior. Using importance sampling <ref type="bibr" target="#b8">[9]</ref>, the samples are reweighted by a factor representing the probability that this particle could have been generated by the true posterior at time Ø ½, divided by the probability with which it was generated by the smoothed posterior at time Ø ½.</p><p>For each sample Ø in the propagated distribution, the likelihood is evaluated. A set of points Ü Ø is randomly chosen from the model foreground, and the likelihood ratio between each point being foreground and background (Equation ( <ref type="formula" target="#formula_3">2</ref>)) is computed. For the edge and motion cues, this is performed at several scales. Since all configuration parameters in are sampled together rather than hierarchically, occluded areas are automatically computed and removed from consideration in the likelihood evaluation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Tracking Results</head><p>The performance of the likelihood model using the learned distributions was tested for different tracking tasks. A general smooth motion model was used <ref type="bibr" target="#b17">[18]</ref>. The test sequences contained clutter, no special clothing, and no special backgrounds. The experiments use monocular grayscale sequences with both static and moving cameras. With 5000 samples and all cues, the Java implementation takes approximately 7 minutes per frame on a 400 MHz Pentium III processor.</p><p>We first show how the tracking benefits from combining different image cues. Figure <ref type="figure" target="#fig_11">7</ref> shows four different tracking results for the same sequence. The model is initialized with a Gaussian distribution around a manually selected set of start parameters . Camera translation during the sequence causes motion of both the foreground and the background.</p><p>The first row shows tracking results using only the motion cue. As shown in <ref type="bibr" target="#b17">[18]</ref> motion is an effective cue for tracking, however, in this example, the 3D structure incorrectly estimated due to drift. The edge cue (row 2), does not suffer from the drift problem, but the edge information at the boundaries of the arm is very sparse and the model is caught in local maxima. The ridge cue is even less constraining (row 3) and the model has too little information to track the arm properly.</p><p>Row 4 shows the tracking result using all three cues together. We see that the tracking is qualitatively more accurate than when using any of the three cues separately. While the use of more samples would improve the performance of the individual cues, the benefit of the combined likelihood model is that it constrains the posterior and allows the number of samples to be reduced.</p><p>Next, we show an example of tracking two arms (Figure <ref type="figure" target="#fig_12">8</ref>). In this example, the right arm is partly occluded by the left arm. Since each sample represents a generative prediction of the limbs in the scene, it is straightforward to predict occluded regions. The likelihood computations are then performed only on the visible surface points.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Conclusions</head><p>The two main contributions of this paper are the learning of image statistics of people and scenes, in terms of motion, edge, and ridge filter responses, and the application of these models to tracking of humans in cluttered environments. By modeling the likelihood of observing a human in terms of a foreground-background ratio we are able to track human limbs in scenes with both clutter and a moving camera.</p><p>The edge and ridge cues provide fairly sparse information about the limb appearance. To capture richer information, the likelihood model could be extended to represent statistical models of color and texture. One approach is to match distributions on the human using the Bhattacharyya distance <ref type="bibr" target="#b1">[2]</ref>.</p><p>We also would like a more explicit background model. Modeling the motion of the background would substantially constrain the tracking of the foreground. We are currently exploring the estimation of background motion using global, parametric, models such as affine or planar motion. We will need to learn background motion distributions for stabilized sequences of this form.</p><p>While the Bayesian formulation provides a framework for combining different cues, the issue of their relative weighting requires further investigation. The issue is related to the spatial dependance of filter responses and here the weighting is implicitly determined by the number of samples chosen for each cue.</p><p>While preliminary, our experimental results suggest that learned models of object-specific and general image statistics can be exploited for Bayesian tracking. In contrast to the situation in the speech recognition community, data collection with ground truth remains a significant hurdle for learning in applications such as people tracking. We believe that building on the careful analysis of image statistics currently under way in the literature <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b10">11,</ref><ref type="bibr" target="#b13">14,</ref><ref type="bibr" target="#b19">20,</ref><ref type="bibr" target="#b25">26]</ref> will lead to more robust algorithms. Towards that end, training data and ground truth used here can be downloaded from http://www.nada.kth.se/˜hedvig/data.html.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: Learning the appearance of people and scenes. Distributions over edge and ridge filter response are learned from examples of human limbs and general scenes.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head></head><label></label><figDesc>edge point set Ü Ü contains sampled pixel locations on the model edges (i.e. on the borders of the limbs), and the motion and ridge point sets Ü Ñ and Ü Ö are equal to Ü . ¿The individual likelihood distributions Ô Þ ÓÒ and Ô Þ Ó«</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: Example images from the training set with limb edges manually marked.</figDesc><graphic coords="4,177.89,103.54,53.28,79.95" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: Left: Horizontal gradient image. Right: Contrastnormalized horizontal gradient image, used for learning.</figDesc><graphic coords="4,54.60,224.86,115.02,90.45" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 4 :</head><label>4</label><figDesc>Figure 4: Edge filter responses. Left: Log likelihood for background, Ô Ó« ´ µ, and thigh, Ô ÓÒ ´</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Figure 5 :</head><label>5</label><figDesc>Figure 5: Ridge filter responses. Left: Log likelihoods for background, Ô Ö Ó« ´ Öµ, and thigh, Ô Ö ÓÒ ´ Ö Ø µ. Right: Log ratio between distributions, ÐÓ ´ÔÖ ÓÒ ´ Ö Ø µ Ô Ö Ó ´ Öµ µ.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head>Figure 6 :</head><label>6</label><figDesc>Figure 6: Motion responses. Learned log likelihoods for foreground pixel differences given known optical flow between two consecutive frames Left: Log likelihood for the thigh, ÐÓ ´ÔÑ ÓÒ ´ Ñ Ø</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_11"><head>Figure 7 :</head><label>7</label><figDesc>Figure 7: Tracking an arm, moving camera, 5000 samples, with different cues. The columns show frames 0, 10, 20, 30, 40 and 50 of the sequence. In each frame, the expected value of from the posterior distribution over is projected into the image. Row 1: Only flow cue. Row 2: Only edge cue. Row 3: Only ridge cue. Row 4: All cues.</figDesc><graphic coords="7,57.28,285.63,78.63,58.54" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_12"><head>Figure 8 :</head><label>8</label><figDesc>Figure 8: Tracking two crossing arms, 3000 samples. The images show the expected value of the posterior distribution in frame 0, 20, 40, 60, 80 and 100 of the sequence.</figDesc><graphic coords="8,57.28,96.87,78.63,58.54" type="bitmap" /></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Acknowledgments. This work was partially sponsored by the Foundation for Strategic Research under the "Center for Autonomous Systems" contract. This support is gratefully acknowledged. We thank David Fleet who developed an early edge likelihood and provided many valuable insights. We are grateful to Allan Jepson for discussions on foreground/background modeling and Bayesian tracking. We would also like to thank Jan-Olof Eklundh, Tony Lindeberg, and Josephine Sullivan for helpful discussions on filters and likelihood models.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">A multiple hypothesis approach to figure tracking</title>
		<author>
			<persName><forename type="first">T-J</forename><surname>Cham</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Rehg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">CVPR</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="239" to="245" />
			<date type="published" when="1999">1999</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Real-time tracking of non-rigid objects using mean shift</title>
		<author>
			<persName><forename type="first">D</forename><surname>Comaniciu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Ramesh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Meer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">CVPR</title>
		<imprint>
			<date type="published" when="2000">2000</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Integrated person tracking using stereo, color, and pattern detection</title>
		<author>
			<persName><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Gordon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Harville</surname></persName>
		</author>
		<author>
			<persName><surname>Woodfill</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IJCV</title>
		<imprint>
			<biblScope unit="volume">37</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="175" to="185" />
			<date type="published" when="2000">2000</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Articulated motion capture by annealed particle filtering</title>
		<author>
			<persName><forename type="first">J</forename><surname>Deutscher</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Blake</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Reid</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">CVPR</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="126" to="133" />
			<date type="published" when="2000">2000</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">The design and use of steerable filters</title>
		<author>
			<persName><forename type="first">W</forename><surname>Freeman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Adelson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">PAMI</title>
		<imprint>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="issue">9</biblScope>
			<biblScope unit="page" from="891" to="906" />
			<date type="published" when="1991">1991</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">An active testing model for tracking roads in satellite images</title>
		<author>
			<persName><forename type="first">D</forename><surname>Geman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Jedynak</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">PAMI</title>
		<imprint>
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1" to="14" />
			<date type="published" when="1996">1996</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">A novel approach to nonlinear/non-Gaussian Bayesian state estimation</title>
		<author>
			<persName><forename type="first">N</forename><surname>Gordon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEE Proceedings on Radar, Sonar &amp; Navigation</title>
		<imprint>
			<biblScope unit="volume">140</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="107" to="113" />
			<date type="published" when="1993">1993</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">Contour tracking by stochastic propagation of conditional density</title>
		<author>
			<persName><forename type="first">M</forename><surname>Isard</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Blake</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1996">1996</date>
			<publisher>ECCV</publisher>
			<biblScope unit="page" from="343" to="356" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">ICondensation: Unifying low-level and high-level tracking in a stochastic framework</title>
		<author>
			<persName><forename type="first">M</forename><surname>Isard</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Blake</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ECCV</title>
		<imprint>
			<biblScope unit="page" from="893" to="909" />
			<date type="published" when="1998">1998</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">Fundamental bounds on edge detection: An information theoretic evaluation of different edge cues</title>
		<author>
			<persName><forename type="first">S</forename><surname>Konishi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Yuille</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Coughlan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Zhu</surname></persName>
		</author>
		<imprint>
			<publisher>PAMI</publisher>
		</imprint>
	</monogr>
	<note>submitted</note>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">Random-collage model for natural images</title>
		<author>
			<persName><forename type="first">A</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Mumford</surname></persName>
		</author>
		<imprint/>
	</monogr>
	<note>to appear: IJCV</note>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">Bayesian estimation of 3-D human motion from an image sequence</title>
		<author>
			<persName><forename type="first">M</forename><surname>Leventon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Freeman</surname></persName>
		</author>
		<idno>TR-98-06</idno>
		<imprint>
			<date type="published" when="1998">1998</date>
		</imprint>
		<respStmt>
			<orgName>Mitsubishi Electric Research Lab</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Edge detection and ridge detection with automatic scale selection</title>
		<author>
			<persName><forename type="first">T</forename><surname>Lindeberg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IJCV</title>
		<imprint>
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="117" to="156" />
			<date type="published" when="1998">1998</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Natural image statistics and efficient coding</title>
		<author>
			<persName><forename type="first">B</forename><surname>Olshausen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Field</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Network Computation in Neural Systems</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="333" to="339" />
			<date type="published" when="1996">1996</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Learning and tracking cyclic human motion</title>
		<author>
			<persName><forename type="first">D</forename><surname>Ormoneit</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Sidenbladh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Black</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Hastie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">NIPS</title>
		<imprint>
			<date type="published" when="2000">2000</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Joint probabilistic techniques for tracking multi-part objects</title>
		<author>
			<persName><forename type="first">C</forename><surname>Rasmussen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Hager</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">CVPR</title>
		<imprint>
			<biblScope unit="page" from="16" to="21" />
			<date type="published" when="1998">1998</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<author>
			<persName><forename type="first">J</forename><surname>Rittscher</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Kato</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Joga</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Blake</surname></persName>
		</author>
		<title level="m">A probabilistic background model for tracking. ECCV</title>
		<imprint>
			<date type="published" when="2000">2000</date>
			<biblScope unit="page" from="336" to="350" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Stochastic tracking of 3D human figures using 2D image motion</title>
		<author>
			<persName><forename type="first">H</forename><surname>Sidenbladh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Black</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Fleet</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ECCV</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="702" to="718" />
			<date type="published" when="2000">2000</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">Learning the statistics of people in images and video</title>
		<author>
			<persName><forename type="first">H</forename><surname>Sidenbladh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Black</surname></persName>
		</author>
		<imprint/>
	</monogr>
	<note>submitted: IJCV</note>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Statistical models for images: Compression, restoration and optical flow</title>
		<author>
			<persName><forename type="first">E</forename><surname>Simoncelli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Asilomar Conf. Signals, Systems &amp; Computers</title>
		<imprint>
			<date type="published" when="1997">1997</date>
			<biblScope unit="page" from="673" to="678" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Probability distributions of optical flow</title>
		<author>
			<persName><forename type="first">E</forename><surname>Simoncelli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Adelson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Heeger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">CVPR</title>
		<imprint>
			<biblScope unit="page" from="310" to="315" />
			<date type="published" when="1991">1991</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">Object localization by Bayesian correlation. ICCV</title>
		<author>
			<persName><forename type="first">J</forename><surname>Sullivan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Blake</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Isard</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Maccormick</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1999">1999</date>
			<biblScope unit="page" from="1068" to="1075" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Statistical foreground modelling for object localisation</title>
		<author>
			<persName><forename type="first">J</forename><surname>Sullivan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Blake</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Rittscher</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ECCV</title>
		<imprint>
			<biblScope unit="page" from="307" to="323" />
			<date type="published" when="2000">2000</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Tracking of persons in monocular image sequences</title>
		<author>
			<persName><forename type="first">S</forename><surname>Wachter</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Nagel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">CVIU</title>
		<imprint>
			<biblScope unit="volume">74</biblScope>
			<biblScope unit="issue">3</biblScope>
			<date type="published" when="1999">1999</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Pfinder: real-time tracking of the human body</title>
		<author>
			<persName><forename type="first">C</forename><surname>Wren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Azarbayejani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Pentland</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">PAMI</title>
		<imprint>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="780" to="785" />
			<date type="published" when="1997">1997</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Prior learning and Gibbs reactiondiffusion</title>
		<author>
			<persName><forename type="first">S</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Mumford</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">PAMI</title>
		<imprint>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="issue">11</biblScope>
			<date type="published" when="1997">1997</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
