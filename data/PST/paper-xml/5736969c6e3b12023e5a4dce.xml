<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">A Scalable Parallel Genetic Algorithm for the Generalized Assignment Problem</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date type="published" when="2014-05-08">May 8, 2014</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Yan</forename><forename type="middle">Y</forename><surname>Liu</surname></persName>
							<email>yanliu@illinois.edu</email>
							<affiliation key="aff0">
								<orgName type="department">CyberGIS Center for Advanced Digital and Spatial Studies</orgName>
								<orgName type="institution">University of Illinois at Urbana-Champaign</orgName>
								<address>
									<postCode>61801</postCode>
									<settlement>Urbana</settlement>
									<region>IL</region>
									<country key="US">United States</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="laboratory">CyberInfrastructure and Geospatial Information Laboratory (CIGI)</orgName>
								<orgName type="institution">University of Illinois at Urbana-Champaign</orgName>
								<address>
									<postCode>61801</postCode>
									<settlement>Urbana</settlement>
									<region>IL</region>
									<country key="US">United States</country>
								</address>
							</affiliation>
							<affiliation key="aff2">
								<orgName type="department">Department of Geography and Geographic Information Science</orgName>
								<orgName type="institution">University of Illinois at Urbana-Champaign</orgName>
								<address>
									<postCode>61801</postCode>
									<settlement>Urbana</settlement>
									<region>IL</region>
									<country key="US">United States</country>
								</address>
							</affiliation>
							<affiliation key="aff3">
								<orgName type="department">National Center for Supercomputing Applications</orgName>
								<orgName type="institution">University of Illinois at Urbana-Champaign</orgName>
								<address>
									<postCode>61801</postCode>
									<settlement>Urbana</settlement>
									<region>IL</region>
									<country key="US">United States</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Shaowen</forename><surname>Wang</surname></persName>
							<email>shaowen@illinois.edu</email>
							<affiliation key="aff0">
								<orgName type="department">CyberGIS Center for Advanced Digital and Spatial Studies</orgName>
								<orgName type="institution">University of Illinois at Urbana-Champaign</orgName>
								<address>
									<postCode>61801</postCode>
									<settlement>Urbana</settlement>
									<region>IL</region>
									<country key="US">United States</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="laboratory">CyberInfrastructure and Geospatial Information Laboratory (CIGI)</orgName>
								<orgName type="institution">University of Illinois at Urbana-Champaign</orgName>
								<address>
									<postCode>61801</postCode>
									<settlement>Urbana</settlement>
									<region>IL</region>
									<country key="US">United States</country>
								</address>
							</affiliation>
							<affiliation key="aff2">
								<orgName type="department">Department of Geography and Geographic Information Science</orgName>
								<orgName type="institution">University of Illinois at Urbana-Champaign</orgName>
								<address>
									<postCode>61801</postCode>
									<settlement>Urbana</settlement>
									<region>IL</region>
									<country key="US">United States</country>
								</address>
							</affiliation>
							<affiliation key="aff3">
								<orgName type="department">National Center for Supercomputing Applications</orgName>
								<orgName type="institution">University of Illinois at Urbana-Champaign</orgName>
								<address>
									<postCode>61801</postCode>
									<settlement>Urbana</settlement>
									<region>IL</region>
									<country key="US">United States</country>
								</address>
							</affiliation>
							<affiliation key="aff4">
								<orgName type="department">Department of Computer Science</orgName>
								<orgName type="institution">University of Illinois at Urbana-Champaign</orgName>
								<address>
									<postCode>61801</postCode>
									<settlement>Urbana</settlement>
									<region>IL</region>
									<country key="US">United States</country>
								</address>
							</affiliation>
							<affiliation key="aff5">
								<orgName type="department">Department of Urban and Regional Planning</orgName>
								<orgName type="institution">University of Illinois at Urbana-Champaign</orgName>
								<address>
									<postCode>61801</postCode>
									<settlement>Urbana</settlement>
									<region>IL</region>
									<country key="US">United States</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">A Scalable Parallel Genetic Algorithm for the Generalized Assignment Problem</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2014-05-08">May 8, 2014</date>
						</imprint>
					</monogr>
					<idno type="MD5">59C5464B983A469DD361728662F5E12F</idno>
					<idno type="DOI">10.1016/j.parco.2014.04.008</idno>
					<note type="submission">Preprint submitted to Parallel Computing</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.3" ident="GROBID" when="2023-07-28T11:20+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Generalized Assignment Problem</term>
					<term>Genetic algorithm</term>
					<term>Heuristics</term>
					<term>Parallel and distributed computing</term>
					<term>Scalability</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Known as an effective heuristic for finding optimal or near-optimal solutions to difficult optimization problems, a genetic algorithm (GA) is inherently parallel for exploiting high performance and parallel computing resources for randomized iterative evolutionary computation. It remains to be a significant chal-</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>lenge, however, to devise parallel genetic algorithms (PGAs) that can scale to massively parallel computer architecture (also known as the mainstream supercomputer architecture) primarily because: 1) a common PGA design adopts synchronized migration, which becomes increasingly costly as more processor cores are involved in global synchronization in each iteration; and 2) asynchronous PGA design and associated performance evaluation are intricate due to the fact that PGA is a type of stochastic algorithm and the amount of computation work needed to solve a problem is not simply dependent on the problem size. To address the challenge, this paper describes a scalable coarse-grained PGA -PGAP, for a well-known NP -hard optimization problem: Generalized Assignment Problem (GAP). Specifically, an asynchronous migration strategy is developed to enable efficient deme interactions and significantly improve the overlapping of computation and communication. Buffer overflow and its relationship with migration parameters were investigated to resolve the issues of observed message buffer overflow and the loss of good solutions obtained from migration. Two algorithmic conditions were then established to detect these issues caused by communication delays and improper configuration of migration</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Inspired by natural selection, Genetic Algorithm (GA) represents a generic heuristic method for finding near-optimal or optimal solutions to difficult search and optimization problems <ref type="bibr" target="#b0">[1]</ref>. GA mimics iterative evolutionary processes with a set of solutions encoded into a population at the initialization stage. Through GA operators (e.g., selection, crossover, mutation, and replacement) that are often stochastic, the population evolves based on the rule of "survival of the fittest" <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b2">3]</ref>. Such an evolutionary process stops when the population converges to solutions of specified quality. The computational challenges of GA are attributed to both problem-specific characteristics (e.g., problem 'difficulty' (e.g., NP -hard), problem size, the complexity of fitness function, and distribution characteristics of solution space specific to problem instances), and runtime efficiency of stochastic search <ref type="bibr" target="#b3">[4]</ref>.</p><p>High performance and parallel computing has been extensively studied to tackle the aforementioned computational challenges in GA as GA has inherent parallelism embedded in the evolutionary process <ref type="bibr" target="#b4">[5]</ref>. For example, a population can be naturally divided into a set of sub-populations (also called demes) that evolve and converge with a significant level of independence. Various types of parallel genetic algorithms (PGAs) have been developed and broadly applied in a rich set of application domains <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b5">6,</ref><ref type="bibr" target="#b6">7,</ref><ref type="bibr" target="#b7">8]</ref>. More interestingly, previous work by Alba et al. <ref type="bibr" target="#b8">[9]</ref> showed that PGA computation not only improves computational efficiency over sequential GAs, but also facilitates parallel exploration of solution space for obtaining more and better solutions. In fact, Hart et al. <ref type="bibr" target="#b9">[10]</ref> showed that running PGA even on a single processor core outperformed its sequential counterpart. Therefore, PGA is often considered and evaluated as a different algorithm rather than just the parallelization of its corresponding sequential GA.</p><p>This paper describes a scalable PGA (PGAP) to exploit massively parallel high-end computing resources for solving large problem instances of a clas-sic combinatorial optimization problem -the Generalized Assignment Problem (GAP). GAP belongs to the class of NP -hard 0-1 Knapsack problems <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b11">12,</ref><ref type="bibr" target="#b12">13]</ref>. Numerous capacity-constrained problems in a wide variety of domains can be abstracted as GAP instances <ref type="bibr" target="#b13">[14]</ref> such as the job-scheduling problem in computer science <ref type="bibr" target="#b14">[15]</ref> and land use optimization in geographic information science and regional planning <ref type="bibr" target="#b15">[16]</ref>. Various exact and heuristic algorithms have been developed to solve GAP instances of modest sizes <ref type="bibr" target="#b16">[17]</ref>. However, in practice, problem instances often have larger sizes while the problem solving requires quick solution time and the capability for finding a set of feasible solutions of specified quality, which compounds the computational challenges.</p><p>Our PGA approach focuses on the scalability to massively parallel processor cores (referred to as cores hereafter) available from high-end computing resources such as those provided by the National Science Foundation XSEDE <ref type="bibr" target="#b17">[18]</ref> cyberinfrastructure. PGAP is a coarse-grained steady-state <ref type="bibr" target="#b18">[19]</ref> PGA that searches solution space in parallel based on independent deme evolution and periodical migrations among connected demes. Scalability is a key to efficiently exploiting a large number of cores in parallel. Previous PGA implementations mostly rely on synchronization to coordiate parallelized operations (e.g., the migration operation in coarse-grained PGAs and the selection operation in fine-grained PGAs), primarily because the computation of PGA is an iterative process and it is straightforward to implement iteration-based synchronization. Synchronization is often needed at two places: 1) waiting for all PGA processes to rendezvous before migration operations; and 2) using synchronous communication to exchange data. While success on scaling PGA to many cores has been achieved based on hardware instruction-level synchronization supported by SIMD architectures <ref type="bibr" target="#b19">[20]</ref>, we argue that the computational performance of PGA is under-achieved through synchronizing iterations across massively parallel computing resources with MIMD architecture <ref type="bibr" target="#b20">[21]</ref>.</p><p>Therefore, an asynchronous migration strategy is designed to achieve scalable PGA computation through a suite of non-blocking migration operators (i.e., export and import) and buffer-based communications among a large number of demes connected through regular grid topology. The asynchrony of migration is effective to not only remove the costly global synchronization on deme interactions, but also allows for the overlapping of GA computation and migration communication. Addressing buffer overflow issues caused by inter-processor communications and understanding their relationship to the configuration of asynchronous PGA parameters are crucial to design scalable PGA on high-end parallel computing systems. Through algorithmic analysis on PGAP, we identified two buffer overflow problems in exporting and importing migrated solutions, respectively. In export operations, the overflow of the outgoing message buffer used by the underlying message-passing library may cause runtime failure and abort the PGA computation. In import operations, the overflow of the import pool maintained for receiving solutions from neighboring demes may cause the loss of good solutions. Through algorithmic analysis, we derive two conditions to guide the setting of PGAP parameters, including migration parameters, topology, and buffer sizes based on the underlying message passing communication library in order to detect and/or avoid aforementioned buffer overflows. To the best of our knowledge, our work is the first to explicitly consider the relationship between the configuration of asynchronous PGA parameters and underlying system characteristics so as to improve the reliability of asynchronous PGAs.</p><p>Experiment results showed that, with the asynchronous migration strategy, our scalable PGA is able to efficiently utilize 16,384 cores with significantly reduced communication cost. Specific strong and weak scaling tests were designed to evaluate the scalability and numerical performance of PGAP because conventional weak and strong scaling methods are not directly applicable to PGA. For example, the problem size used in conventional weak scaling is not a good indicator of the amount of computation needed to achieve a certain solution quality in PGA. PGA and sequential GA also have different algorithmic behaviors. More importantly, population size plays a crucial role in PGA performance as the number of cores increases. Therefore, instead of problem size, we chose population size to study PGAP scalability and compared its performance with the corresponding sequential algorithm. In strong scaling tests, linear and super-linear speedups were observed in solving large GAP instances. PGAP also outperforms the best-so-far sequential GA in solving modest problem instances. Numerical performance study on large instances showed promising results on the capability of PGAP to exploit massive computing power for more effective exploration of search space for finding alternative solutions, faster convergence, and obtaining improved solution quality.</p><p>The reminder of the paper is organized as follows. Section 2 reviews related work on PGA and GAP. Section 3 describes the design and implementation of PGAP for solving computationally intensive GAP instances. Section 4 details computational experiments designed for evaluating PGAP scalability and numerical performance and analyzes experiment results. Section 5 summarizes the findings of the research and discusses future work.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related work</head><p>The generalized assignment problem (GAP) is defined to find an optimal assignment of n items to m bins (knapsacks) such that the total cost of the assignment is minimized and the weight capacity constraint of each bin is satisfied. The cost and weight of an item depend on both the item itself and the assigned bin <ref type="bibr" target="#b12">[13]</ref>. GAP belongs to the class of NP -hard 0-1 Knapsack problems <ref type="bibr" target="#b11">[12]</ref>. It is the generalization of the well-known Knapsack problem (single bin) and Multiple Knapsack Problem (MKP) <ref type="bibr" target="#b12">[13]</ref>. A detailed survey of sequential algorithms for solving GAP can be found in <ref type="bibr" target="#b16">[17]</ref>. Canonical work on GAP heuristic algorithms include genetic algorithms <ref type="bibr" target="#b21">[22,</ref><ref type="bibr" target="#b22">23,</ref><ref type="bibr" target="#b23">24,</ref><ref type="bibr" target="#b24">25,</ref><ref type="bibr" target="#b25">26]</ref>, simulated annealing <ref type="bibr" target="#b26">[27]</ref>, tabu search <ref type="bibr" target="#b27">[28]</ref>, and hybrid search that combines heuristics and local search strategies <ref type="bibr" target="#b28">[29,</ref><ref type="bibr" target="#b29">30,</ref><ref type="bibr" target="#b30">31]</ref>. Previous work on GA approach to GAP showed that standard GA operations (i.e., selection, crossover, mutation, and replacement) often produce infeasible solutions that violate the capacity constraint of GAP, therefore needs additional processing. For example, Chu and Beasley <ref type="bibr" target="#b21">[22]</ref> developed two additional operators, feasibility improvement and quality improvement, to convert infeasible solutions to feasible ones and attempt to improve each resulted feasible solution, respectively. Wilson's method <ref type="bibr" target="#b22">[23]</ref> allows for infeasible solutions in GA computation, but improves the feasibility of all solutions after GA stops. The method by Feltl and Raidl <ref type="bibr" target="#b23">[24]</ref> also allows for infeasible solutions, but with a penalty-based mechanism for adjusting the fitness value of infeasible solutions.</p><p>As a well-known approach to increasing the computational efficiency of GA, PGAs differ in where parallelization is exploited: fine-grained PGAs <ref type="bibr" target="#b31">[32]</ref> (also referred to as cellular GA (cGA) or Diffusion Models <ref type="bibr" target="#b32">[33]</ref>) parallelize the selection operator to select parents from directly connected neighbors on a PGA topology in each iteration; coarse-grained PGAs (also referred to as island model GA (iGA)) migrate a portion of local solutions to connected demes periodically <ref type="bibr" target="#b33">[34]</ref>; global parallelization-based PGAs parallelize the computation of the fitness evaluation function if the function is computing intensive <ref type="bibr" target="#b34">[35]</ref>. PGA surveys can be found in literature <ref type="bibr" target="#b35">[36,</ref><ref type="bibr" target="#b36">37,</ref><ref type="bibr" target="#b37">38,</ref><ref type="bibr" target="#b8">9,</ref><ref type="bibr" target="#b4">5,</ref><ref type="bibr" target="#b38">39]</ref>. Tanese <ref type="bibr" target="#b39">[40]</ref> showed that a PGA exhibits different algorithmic behavior than its corresponding sequential GA. Interestingly, even without communications, PGA could improve GA performance with independent deme evolution. This is because multiple independent running instances of the same GA likely produce different quality results because of different randomness introduced by each instance for GA operations. Given a fixed solution quality requirement, the probability that at least one instance finds a solution of designated quality earlier would increase as more cores are used. Even without communications among demes, therefore, running multiple demes on a single core often performs better than running a single deme with the same global population size. On the other hand, communications, i.e., migration of a portion of local solutions to other demes, greatly improves the effectiveness of GA computation through the propagation of good solutions and the injection of new randomness.</p><p>Previous work on designing scalable PGAs often employs the fine-grained PGA model on SIMD architecture <ref type="bibr" target="#b19">[20,</ref><ref type="bibr" target="#b40">41,</ref><ref type="bibr" target="#b41">42,</ref><ref type="bibr" target="#b42">43,</ref><ref type="bibr" target="#b43">44,</ref><ref type="bibr" target="#b44">45,</ref><ref type="bibr" target="#b45">46]</ref>. For example, Shapiro et al. <ref type="bibr" target="#b19">[20]</ref> implemented a PGA for RNA folding on the MasPar SIMD supercomputer with 16,384 processing units. Chen et al. <ref type="bibr" target="#b40">[41]</ref> developed a hybrid cellular GA on the same machine. These fine-grained PGA implementations exhibit good performance on gene diffusion, i.e., the propagation of high-quality genes across the entire population. Hart et al. <ref type="bibr" target="#b9">[10]</ref>, Alba et al. <ref type="bibr" target="#b18">[19]</ref>, and Prabhu et al. <ref type="bibr" target="#b44">[45]</ref> pointed out that SIMD architecture is suitable for the development of fine-grained PGAs because fine-grained PGAs require synchronization in each iteration to select parents from directly connected demes and SIMD systems provide hardware-and/or system-level synchronization support that can be directly adopted to coordinate the selection operation within each iteration. It is more challenging to synchronize GA iterations on MIMD systems in which processors are often connected through network. On MIMD architecture, synchronization can be a costly task at system level, especially when it involves a large number of connected cores. In message passing models which MIMD systems often use for communications, the overhead of synchronization has been well studied <ref type="bibr" target="#b46">[47,</ref><ref type="bibr" target="#b47">48]</ref>. Such synchronization cost can cause serious performance degradation on PGAs. In the synchronous mode, a PGA iteration can be considered as a sequence of computations followed by a global barrier to rendezvous all processes. Any delay at one process, caused from various sources (e.g., network, operating system, memory, I/O, or computation itself in GA) is propagated to all of the participating processes, and the probability and duration of such delay increases as the number of processes increases. In fact, the evolutionary process in PGA may not require synchronization because: 1) a PGA process can evolve independently; 2) communications with other processes only involve neighboring processes, without the need for global-level synchronization; and 3) sending information to and receiving information from neighboring processes do not need to be coupled.</p><p>In general, GA, as a type of stochastic computing model, exhibits massive parallelism and holds tremendous potential to reap the benefits of large-scale parallel computation <ref type="bibr" target="#b48">[49]</ref>. As massively parallel computing resources become available <ref type="bibr" target="#b20">[21,</ref><ref type="bibr" target="#b49">50]</ref> as a consequence of active development of multi-core/manycore computing and extreme-scale supercomputing <ref type="bibr" target="#b50">[51]</ref>, it is promising to harness an unprecedented amount of parallel computing resources for scalable GA computation.</p><p>It has been recognized that asynchronous inter-deme interactions are desirable for designing scalable PGAs <ref type="bibr" target="#b51">[52,</ref><ref type="bibr" target="#b18">19,</ref><ref type="bibr" target="#b9">10,</ref><ref type="bibr" target="#b52">53]</ref>. Most of previous PGA work chose to synchronize GA computation primarily for the purpose of straightforward parallel programming implementation. Early work by Hart et al. <ref type="bibr" target="#b9">[10]</ref> showed that delays exhibited in asynchronous fine-grained PGAs helped achieve better solution quality because some demes were allowed more time for the local evolution to be more convergent toward better solutions. Lin et al. <ref type="bibr" target="#b52">[53]</ref> observed the effectiveness of exchanging solutions asynchronously and proposed a solution exchange strategy based on population similarity instead of the fixed connection topology on coarse-grained PGAs. The CAGE framework <ref type="bibr" target="#b53">[54]</ref> employed non-blocking message passing functions for the development of fine-grained GA programming library on MIMD clusters configured with up to 256 computing nodes. We argue that the availability of massively parallel computing resources now offers a major driver toward the design and development of asynchronous PGAs. In this paper, an asynchronous migration strategy is developed as the mainstay for enabling scalable PGA for GAP.</p><p>As a complex and dynamic process, PGA computation is sensitive to a set of algorithmic and computational factors. Alba and Troya <ref type="bibr" target="#b18">[19]</ref> and Hart et al. <ref type="bibr" target="#b9">[10]</ref> pointed out that conventional parallel computing performance measures such as speedup and efficiency, need careful consideration when applied to computational performance evaluation of PGA. Since PGA and GA are algorithmically different, direct performance comparison between PGA and sequential GA may not lead to appropriate conclusions. Alba et al. <ref type="bibr" target="#b18">[19]</ref> further suggest using the same parallel version on different number of cores for speedup measurement. Also, a sufficient number of trials should be conducted to yield statistically confident evidence on obtained results <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b36">37]</ref>. Identifying the sources of performance variation in asynchronous PGA is another issue. Gordon and Whitley <ref type="bibr" target="#b54">[55]</ref> observed that even on a single core, emulated PGAs were often more efficient than many sequential GAs for solving various types of problems. In addition, injected computational noise changes asynchronous PGA algorithmic performance <ref type="bibr" target="#b9">[10]</ref>, making performance measurement more complicated.</p><p>In high-performance computing, weak and strong scaling are two typical ways to measure scalability of high-performance computing algorithms <ref type="bibr" target="#b55">[56,</ref><ref type="bibr" target="#b56">57]</ref>. As for weak scaling, problem size per core is kept constant as the number of cores increases. Therefore, weak scaling allows us to look at the capability of an algorithm to solve larger or more complicated problems in conjunction with the use of more cores. Strong scaling keeps the overall problem size a constant as the number of cores varies, and measures speedup, calculated by dividing the execution time of the best sequential algorithm by that of the parallel algorithm. However, neither of them can be directly applied to analyze the scalability of PGAs for solving combinatorial optimization problems (elaborated in 4.2). This research, therefore, has designed specific scaling test methods for the comprehensive evaluation of the performance of PGAP.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Algorithm</head><p>The mathematical formulation of GAP is as follows:</p><formula xml:id="formula_0">Objective: min m i=1 n j=1 c ij x ij such that: n j=1 w ij x ij b i , i = 1, 2, ..., m<label>(1)</label></formula><formula xml:id="formula_1">m i=1 x ij = 1, j = 1, 2, ..., n<label>(2)</label></formula><p>x ij ∈ {0, 1}, i = 1, 2, ..., m, j = 1, 2, ..., n</p><p>Where matrix C m×n and W m×n denote the cost and weight requirements of assigning each item j, j = 1, 2, ..., n, to each bin i, i = 1, 2, ..., m, respectively. Constraint (1) represents the weight capacity constraint indicating that the total weights of the items assigned to a bin cannot exceed the bin's capacity. X m×n is the assignment matrix with each entry valued either 0 or 1. Constraint (2) is the assignment constraint that allows an item to be assigned to exactly one bin. Each problem instance has the cost and weight matrices as input. The output is the assignment matrix X m×n that minimizes the cost defined in the objective function.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Sequential genetic algorithm</head><p>The sequential GA developed for this GAP formulation is an extension to the GA developed by Chu and Beasley <ref type="bibr" target="#b21">[22]</ref>. A solution is encoded as a chromosome that is represented as a binary string s of size n, where the value of s j denotes the index of the bin to which item j is assigned (Figure <ref type="figure">1</ref>). A population is formed by a set of encoded solutions. Population size is a runtime parameter. This GA follows a steady-state <ref type="bibr" target="#b2">[3]</ref> reproduction process in which each iteration selects two parents to generate one child for replacement. The GA operators to be performed within each iteration are adapted from Chu and Beasley <ref type="bibr" target="#b21">[22]</ref> and listed below for the completeness of this paper:</p><p>• Selection. Two parents are selected based on selection strategies such as binary tournament selection or rank-based tournament selection <ref type="bibr" target="#b57">[58]</ref>.</p><p>• Crossover. The simple single cut-point crossover operator randomly decides a cut point at which the first part of a parent is combined with the second part of the other to form the child solution.</p><p>• Mutation. The mutation operator is performed on the child solution obtained from the crossover operator to exchange the bin assignment of two randomly chosen items.</p><p>• Feasibility improvement. This operator looks at those over weighted bins after fitness evaluation for a reassignment that could keep the weight sum of the bin below its capacity. This is done by moving an item in an over weighted bin to an under weighted bin.</p><p>• Quality improvement. This operator looks at each item for a possible reassignment (to another bin) such that the solution's fitness value could be improved.</p><p>The fitness of a solution is evaluated as two values: fitness value that is equivalent to the value of GAP objective function, and 'unfitness' value which is the sum of exceeded weights in each bin. For feasible solutions, the unfitness value is always zero. Replacement strategies use the unfitness value to choose the solutions to be replaced. The execution of GA is stopped if its stopping criterion is met. The stopping criterion can be a fixed number of iterations, a time limit, or a given solution quality threshold.</p><p>Chu and Beasley <ref type="bibr" target="#b21">[22]</ref> and Feltl and Raidl <ref type="bibr" target="#b23">[24]</ref> indicate that the feasibility and quality improvement operators are critical to keep the search close to feasible regions in the solution space and help GA converge to near-optimal or optimal solutions quickly. We further refine these two operators for better lookup efficiency. Both operators include a linear scan of bins to look up for either over weighted bins for feasibility improvement or an under weighted bin for reassigning an item to improve solution quality. In <ref type="bibr" target="#b21">[22]</ref>, the order in which bins are checked is fixed, which means that the capacity of each bin is also examined in a fixed order. Considering capacity variations among bins, such examination with the fixed order may limit the search efficiency for possible candidate choices for item reassignment. For example, bins checked first are always considered for reassignment first despite of the existence of alternatives. Such limitation becomes significant in PGA, as the size of global population is often large. In our algorithm, the bin lookup sequence is randomly decided to make the bin lookup operation independent of the order of bins coded in problem instances. The performance impact of this technique is discussed in section 4.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Parallel genetic algorithm</head><p>Our coarse-grain PGA -PGAP -is designed based on asynchronous migration because: 1) synchronizing each GA iteration is not necessary in a coarsegrained PGA; 2) overhead of synchronization gets worse when more cores are used (see section 2); and 3) non-blocking migration operators increase the overlapping of computation and communication (local evolution can go to the next iteration without waiting for response messages from receiving demes before the next round of migration) and, thus, may significantly improve the computational performance of PGAP.</p><p>In PGAP, each deme initializes and maintains a local population. The size of a local population is referred to as deme size. The number of demes is determined based on the number of cores available at runtime. Demes are connected with a regular 2-D toroidal grid topology (Figure <ref type="figure">2</ref>). On the grid, the start and end of a row/column are connected. Therefore, the connectivity degree, d, is four for each deme. The algorithm runs concurrently on all cores until a stopping criterion is satisfied at any of the cores.</p><p>The following PGA operators are designed to exchange a portion of a deme's population with its directly connected neighbors.</p><p>Export. A deme exports a fixed number of solutions to its neighbors periodically. The export operator defines two parameters: migration rate r (i.e., the number of solutions to be exported) and export interval M expt (defined as the number of iterations). Our strategy for exporting solutions considers both elitism and diversifying deme population by exporting random solutions. When choosing r solutions for exporting, elite solutions generated during the previous export interval are always included. Remaining spots, if any, are filled by randomly picked solutions from local population. If no elite solutions were found in the previous export interval, a holding strategy is applied to probabilistically delay the export.</p><p>Import. Each deme maintains an import pool, implemented as a cyclic firstin-first-out (FIFO) queue with the queue header pointing to the first imported solution and queue tail pointing to the next available buffer space, to hold external solutions migrated from neighboring demes. A parameter, import interval M impt (defined as the number of iterations), is used to control how often an import operation is performed. An import operation copies all incoming solutions from the underlying communication system to the import pool. When the pool is full, new incoming solutions override older ones. The size of the import pool is determined by a buffer management method described later (see section 3.3) to avoid this overriding scenario.</p><p>Inject. The inject operator merges migrated solutions from the import pool into local population. Elite solutions from neighboring demes are alway incorporated into local population if they are superior than local elite solutions. Random solutions from outside are considered as candidates to be selected as one of the two parents for standard GA operations. Figure <ref type="figure">3</ref> illustrates above PGA operators and their interactions with local GA and network. The pseudo code of PGAP is provided in Figure <ref type="figure" target="#fig_1">4</ref>. The introduction of PGA operators affect the selection and replacement operators in the sequential algorithm. After the inject operator is performed, the selection operator takes an injected random solution as one of the two parents to subsequent GA operators. The replacement operator directly merges a remote elite solution into local population. By doing so, local population at a deme is able to evolve with better solutions already found by other demes, therefore saving local computation to reach the same level of solution quality. Furthermore, combined with the selection strategy in the export operator for sending out local elite solutions, good solutions can be propagated to other demes in a hop-by-hop fashion until they are overtaken by better solutions. This is similar to the diffusion model in fine-grained PGAs <ref type="bibr" target="#b58">[59]</ref>. The reason to export random solutions is that these solutions appear as noise to local evolution on the receiving deme and may influence local evolution paths in a positive way <ref type="bibr" target="#b9">[10]</ref>. When a local evolution process is trapped in local optima or stays within a premature state, such 'noise' from outside may help jump-start the evolution process to search new solution space, therefore providing the diversification effect for local evolution. On the other hand, the probability of holding is kept sufficiently low to avoid excessive interference on any receiving deme's evolution process.</p><p>Compared to synchronous PGA design in which all demes need to stop for communication within the selection operator (in the case of fine-grained PGA) or export and import operators (in the case of coarse-grained PGA), there is no global communication barrier for migration-related communication in PGAP. On each deme, an export operation starts every M expt iterations, returns immediately without waiting for acknowledgement messages from receiving demes, and completes when exported solutions are passed to the underlying communication system. The import operator on each deme checks for incoming solutions every M impt iterations. If no incoming solutions are found, the deme proceeds to the next iteration without waiting. The check operation, called probe, is lightweight. The inject operator injects imported solutions from the import pool into the local evolution process, one at a time. The benefit of using asynchronous migration is two-fold as follows. Globally, asynchronous migration eliminates the costly global coordination among all demes. Locally, the overlap between computation and communication, enabled by the non-blocking export and import operators and buffer-based export communication, increases significantly to allow for better computational performance. Figure <ref type="figure">2</ref> illustrates a runtime topology of PGAP on which migration events and computation take place at the same time.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Buffer management for asynchronous migration</head><p>Buffer-based migration was used before for developing asynchronous PGAs. For example, Andre and Koza <ref type="bibr" target="#b59">[60]</ref> implemented an asynchronous PGA by separating export, import, and other GA operators as independent processes and used per-neighbor buffers to receive exported solutions from corresponding neighbors. To the best of our knowledge, no previous work has considered the issue of buffer overflow and its relationships with PGA parameters. In PGAP, buffer-based communication and non-blocking communication functions are applied together to implement the asynchronous migration strategy and achieve desirable overlapping of computation and communication in export and import. Buffer is used at two places: 1) the import pool is implemented as a cyclic buffer; and 2) PGAP allocates memory to the underlying communication system as outgoing message buffer (referred to as sending buffer hereafter) for buffer-based non-blocking export operation. Buffer overflow at both places must be considered and avoided whenever possible to make PGAP reliable. The overflow of the import pool will override solutions received in previous import operations, and thus cause the loss of imported neighbor solutions if they have not been injected yet. The overflow of the sending buffer has more severe impact: the underlying message-passing library often terminates the execution of all processes even when the overflow occurs within one process. We argue that inappropriate configuration of PGA parameters may lead to serious buffer overflow issues. An algorithmic analysis is conducted on PGAP to study the two buffer overflow issues caused by communication delays. Two algorithmic conditions are then derived to detect and avoid buffer overflow issues by setting PGAP parameters appropriately.</p><p>PGAP uses message passing (specifically, MPI <ref type="bibr" target="#b60">[61]</ref>) as underlying parallel programming model and implements the buffer-based non-blocking communication as follows, which is a common choice suggested in <ref type="bibr" target="#b60">[61]</ref>:</p><p>• Message send operation completes without the need to wait for the post of matching receive at destination process. Instead, the sending operation is considered complete after the message is handed to the sending buffer. Send operation is implemented as non-blocking function;</p><p>• The buffered send operation calls a non-blocking non-buffered send for sending each message queued in the buffer, which will not complete until the matching receive is posted or the receiving process starts receiving. This is suggested by MPI standard (see chapter 3 in <ref type="bibr" target="#b60">[61]</ref>);</p><p>• The sending buffer is managed by the underlying communication system, but the memory of this buffer is allocated explicitly by PGAP;</p><p>• Before the message receive operation starts, a probe operation is called to check new messages. Probing introduces negligible cost. If probing does not find new messages, the receive operation is postponed. A receive is considered complete when an incoming solution is copied into the import pool. The receive operation is implemented as blocking function;</p><p>• A import operation, if called, receives all of incoming solutions into the import pool.</p><p>Table <ref type="table" target="#tab_0">1</ref> lists the PGAP parameters considered in our analysis of buffer overflow. Our analysis is based on a common PGA execution environment where each deme has the same setting for PGAP parameters listed on Table <ref type="table" target="#tab_0">1</ref>. This analysis focuses on the overflow scenarios caused by communication delays among demes. For simplicity purpose, the holding strategy mentioned in section 3 is not considered. Computing time for an iteration is assumed to be consistent across all demes. This assumption indicates that each participating core may have similar CPU and memory characteristics, which is the case for the supercomputers we used for experiments.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.1.">Sending buffer overflow analysis</head><p>The sending buffer overflows when there are too many pending send operations to handle (Figure <ref type="figure">3</ref>). In PGAP, communication delays in the underlying communication system and/or a long import interval (M impt ) on receiving end can prevent a send operation from being compelete. Communication delays are caused by network congestion or communication system resource limitations, which we have no control of in algorithm design. Allocating a large K sendbuf helps, but does not guaranttee the avoidance of overflow. The purpose of our analysis is to avoid the overflow scenarios caused by inappropriate configuration of PGAP parameters.</p><p>We consider the worst case scenario in which no send operation can be completed. Suppose it takes x iterations for export operations to fill the sending buffer with K sendbuf solutions. The rate of buffer filling is r Mexpt per iteration. By letting x × r Mexpt = K sendbuf , we have x =</p><formula xml:id="formula_2">K sendbuf r × M expt .</formula><p>If M impt at receiving deme is set to be less or equal to x, we can then avoid the sending buffer overflows caused by inappropriate configuration of PGAP. Equivalently, this condition can be written as:</p><formula xml:id="formula_3">M impt M expt K sendbuf r<label>(1)</label></formula><p>K sendbuf is usually fixed for a PGAP run. Condition (1) can then be used to guide the appropriate configrations of M impt , M expt , and r.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.2.">Import pool overflow analysis</head><p>If the solutions in the import pool of a deme are not injected into local population in time because of computational noise or outpaced communication between the deme and its neighbors, the pool may overflow. In this overflow situation, one strategy is to allow solution overriding. Such overriding is 'harmless' if new solutions are elite solutions from the same neighbor because a newly arrived elite solution always has equal or better quality. However, since we allow random solutions to be exported and elite solutions from different neighbors may have different quality, overriding current solutions in the import pool may result in overriding better solutions that have not been injected into local population. Sophisticated methods can be developed for import pool management to handle such overriding, but at the price of slowing down the processing of the import operator. By looking at overflow scenarios and their relationship to PGAP parameters, a sufficent condition is derived to guide the configuration of K impt , M expt , d, and r in order to avoid the overflow of the import pool.</p><p>The import pool will overflow if and only if:</p><p>1. The rate of producing solutions to the pool is faster than the rate of consuming from it, if we consider the import operator as the producer and the inject operator as the consumer; or 2. The import pool is not large enough to hold imported solutions received in a single import operation.</p><p>The first condition means if the rate of import operations (in iterations), denoted by R impt , is larger than the rate of inject operations (in iterations), denoted by R inject , the import pool will overflow eventually, regardless of the size of the pool (K impt ). Let us denote the event of import pool overflow as O, the number of received solutions in a single import operation as k impt , and the number solutions remained in the import pool at the beginning of an iteration as k. Therefore, when an import operation is complete, there are k + k impt solutions in the import pool. From the notations defined in Table <ref type="table" target="#tab_0">1</ref>, Above overflow conditions can be written equivalently as:</p><formula xml:id="formula_4">A : R impt R inject , B : k + k impt K impt , A ∧ B ←→ ¬O</formula><p>The left part (A ∧ B) is the sufficent and necessary condition to avoid overflow. An exporting deme sends r solutions every M expt iterations to a receiving deme. The receiving rate R impt is the same as sending rate d×r Mexpt , otherwise the sending buffer from at least one neighoring deme will overfollow eventually. PGAP algorithm (Figure <ref type="figure" target="#fig_1">4</ref>) injects one imported solution per iteration if the import pool is not empty, indicating R inject = 1. A is then equivalent to:</p><formula xml:id="formula_5">d×r Mexpt 1 (a)</formula><p>Condition (a) indicates that, in each iteration, there is at most one incoming solution sent by neighboring demes. Note that condition (a) is implied in B because K impt is a constant. For B, between any two consecutive import operations, neighboring demes constantly sends d × r</p><p>Mexpt × M impt solutions to the receiving deme. Communication delays could hold the receiving of these solutions to later import operations. However, since we must avoid the overflow of any sending buffers, the receive operation cannot be postponed infinitely. In fact, by applying condition (1) derived in section 3.3.1, we get an upper bound for K impt :</p><formula xml:id="formula_6">k impt K sendbuf × d (b)</formula><p>Furthermore, K sendbuf × d is the largest possible number of solutions in the impool pool. This is simply because R impt R inject = 1, meaning that the inject operator consumes solutions in the import pool no slower than the import operator producing solutions into the pool. Therefore, delayed receive operations in an import operation is the only possible scenario to make the number of solutions in the import pool following an import operation to be larger than that of the previous import. An import operation with delayed receive operations can receive up to K sendbuf × d solutions. Meanwhile, since it takes at least K sendbuf × d iterations to gather these many receiving requests (condition (a)), by the time the import operation eventually happens, there will be zero solutions in the pool. Thus, condition</p><formula xml:id="formula_7">K impt K sendbuf × d satisfies B.</formula><p>Combing (a) and (b), we get:</p><formula xml:id="formula_8">( d × r M expt 1) ∧ (K impt K sendbuf × d) -→ ¬O<label>(2)</label></formula><p>Condition <ref type="bibr" target="#b1">(2)</ref> shows the correlations among PGAP parameters listed on Table <ref type="table" target="#tab_0">1</ref> in order to avoid the overflow of the import pool. Based on condition (1) and ( <ref type="formula" target="#formula_1">2</ref>), buffer overflow issues caused by inappropriate configuration of PGAP parameters can be avoided.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4.">Implementation</head><p>PGAP is implemented in C. The export and import operators are implemented using the MPI message-passing programming model. MPI's non-blocking point-to-point communication functions MPI Ibsend() and MPI Iprobe() are used for implementing non-blocking features of the export and import, respectively. To enable flexible control of the system outgoing message buffer, we use MPI's MPI Buffer attach() to explicitly allocate memory for buffer-based message send operations. The communication topology is generated dynamically by making the sizes of the two dimensions of the 2-D grid as close as possible. The runtime configuration of PGAP parameters is based on the algorithmic anlaysis results in section 3.3. Since severe communication delays can still cause the overflow of the sending buffer, PGAP implementation skips an export operation if the sending buffer is full.</p><p>A random number generator is used to provide stochastic choices on GA operators. Given the fact that GA often requires a large amount of iterations before converging to specified solution quality, the same random number sequence cannot be used on all demes even with different initial seeds. A parallel random point generator, SPRNG, is employed to generate a unique random number sequence on each deme <ref type="bibr" target="#b61">[62]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Performance Evaluation</head><p>Experiments were designed to evaluate the scalability of PGAP algorithm by using up to 16,384 cores. Both strong and weak scaling tests were performed, but they have to be tailored to PGA performance evaluation. Strong scaling tests were conducted to measure speedups in a set of large-scale PGAP runs, each using a different number of cores. However, two issues related to PGA performance evaluation have to be resolved: defining base case runs and comparing speedups in runs that achieve different solution quality. In weak scaling tests, population size, instead of problem size, is used as scaling factor in order to appropriately measure the computational effort required in relation to the increase of computing power. For both tests, results were compared with the corresponding synchronous implementation. PGA algorithmic capabilities such as numerical performance, solution quality improvement, and convergence were evaluated to understand the improved problem-solving capabilities by using the asynchronous migration strategy.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Experiment design</head><p>Two MIMD high-performance computing (HPC) systems on XSEDE (i.e., the Lonestar and Ranger clusters at the Texas Advanced Computing Center (TACC)) were used for the experiments. Lonestar has 22,656 cores with 0.302 petaflop peak performance. Each node on Lonestar has two 3.3GHz Intel Xeon hexa-core 64-bit Westmere processors (12 cores) and 24GB memory. Ranger has 62,976 cores with 0.579 petaflop peak performance. Each node on Ranger contains four 2.3GHz AMD Opteron quad-core processors (16 cores) and 32GB memory. Both systems use InfiniBand as interconnect. Up to 2,048 cores on Lonestar were used to study the convergence of PGAP and track the number of unique solutions found in PGAP runs. Up to 16,384 cores on Ranger were used for scalability tests and numerical performance evaluation in solving large GAP instances.</p><p>Five types of public GAP benchmark instances available from OR-LIB<ref type="foot" target="#foot_0">1</ref> and Yagiura's website<ref type="foot" target="#foot_1">2</ref> have been used in literature as benchmark datasets. Small-sized type D, E, and F instances from OR-LIB are used in small-scale experiments to verify the quality of solutions found by our baseline GA algorithm. Type D and E instances from Yagiura are large instances and considered more difficult because costs are inversely correlated with weights <ref type="bibr" target="#b62">[63]</ref>. E801600, one of the largest instances of type E with 1,600 items and 80 bins, was used for PGAP scaling tests and performance evaluation.</p><p>Table <ref type="table" target="#tab_1">2</ref> shows the default configuration of PGAP parameters based on condition (1) and (2) derived in section 3.3. The ratio M impt /M expt is configured to be much smaller than K sendbuf /r in order to reduce the impact of sporadic network congestions on the sending buffer. We also follow the guidelines by Hart et al. <ref type="bibr" target="#b9">[10]</ref> and Alba et al. <ref type="bibr" target="#b18">[19]</ref> to make sure that results from different PGAP runs are comparable. For example, the stopping rule for speedup analysis is finding the best solution found by the base case runs, instead of setting a walltime or the number of iterations. A synchronous PGA is developed based on PGAP as the reference implementation for performance comparison purpose. This is done by adding global barriers for export and import operators to PGAP and using blocking communication functions and synchronous communication mode in MPI. Therefore, synchronization cost is the only source of performance difference.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Scalability analysis</head><p>It is not appropriate to directly use conventional weak and strong scaling testing methods to analyze the scalability of PGAP for the following reasons. First, the amount of computation work required to solve a combinatorial optimization problem depends as much on the problem difficulty as on the problem size. For example, a small-size (in terms of the number of items and/or bins) type E GAP instance may be 'harder' than a large-size type C instance because type E instances were generated by inversely correlate the cost and weight of each item <ref type="bibr" target="#b28">[29]</ref>. Therefore, varying problem size only as done in typical weak scaling experimentation may not be sufficient to reveal how well PGA can solve large problems using more computational resources. Second, PGA is a type of randomized algorithm, multiple runs of the same PGA configuration may take different amount of execution time to achieve the same level of solution quality. Furthermore, while the overall problem size is fixed in strong scaling, changing the number of cores also changes deme size and the number of demes handled by each core. Such changes have profound impact on local evolution and global migration effect, together complicating the overall evaluation of computational performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.1.">Strong scaling test</head><p>Our strong scaling experiment measures the speedup of PGAP by increasing the number of cores, but keeping the size of global population constant. Each core runs one deme and thus deme size, which is equal to the global population size divided by the number of cores, varies accordingly. Speedup is calculated as the ratio of the execution time of two PGAP runs with the denominator being a reference (referred to as base case) using a small number of cores. The reason to use PGAP itself as the reference algorithm is that PGA and GA are not suitable for direct comparison as they are considered to have different algorithmic behaviors. Such definition of speedup is referred to as relative speedup by Sun et al. <ref type="bibr" target="#b63">[64]</ref> or type I speedup by Alba et al. <ref type="bibr" target="#b18">[19]</ref>. Since the global population size is kept constant, the increase of the number of cores leads to the decrease of deme size and the increase of the number of demes. It is known that PGA performance is highly influenced by deme size and migration. Specifically, large deme size makes GA search on a core toward a random search and injects solutions from fewer demes, while small deme size makes local GA search "premature" but injects solutions from more demes. Our strong scaling test is designed to compare the speedup between PGAP and its synchronous implementation.</p><p>In the experiment, global population size was kept at 1,638,400. As the number of cores used in the experiment increased from 512 to 16,384, population size on each deme decreased from 3,200 to 100. Since comparing with sequential algorithm performance would produce misleading results, we used 512-core run of PGAP as the base case for PGAP. 512-core run of its synchronous version is then used as the base case for speedup calculation for the synchronous version. Therefore, perfect linear speedup in our experiment would be 32 when 16,384 cores are used. When a specified solution quality threshold could not be reached within the maximum walltime allowed using 512 cores, the execution time of the base case run is set to be the maximum walltime (i.e., 16 hours (57,600 seconds)). In such scenario, using the maximum walltime in base case is a conservative estimation of the actual speedup in speedup comparison because the resulted speedup is the lower bound of the actuall speedup. Figure <ref type="figure">5(a)</ref> and Figure <ref type="figure">5</ref>(b) illustrate the speedup measurements against different solution quality thresholds, quantified as the upper bound of fitness value. Results show that, for both synchronous and asynchronous runs, using more cores resulted in better speedup, even superlinear speedups. By looking at speedups achieved at multiple solution quality thresholds, more comprehensive view of PGAP numerical performance was obtained by examining the relationship between the levels of difficulty to reach a specified solution quality and the number of cores used. For example, for the type E instance, both synchronous and asynchronous runs experienced difficulty to achieve solution quality threshold 188,000, indicated by sublinear speedup achieved. Asynchronous PGAP runs (Figure <ref type="figure">5(a)</ref>) exhibited superlinear speedup at 8 out of 11 solution quality thresholds when using 16,384 cores, while 3 out of 8 were observed in synchronous runs (Figure <ref type="figure">5(b)</ref>). Synchronous runs could not reach the three tightest solution quality thresholds reached by PGAP, while asynchronous runs were also able to find solutions better than the tightest threshold 183,500 when using 8,192 and 16,384 cores. Beyond 8,192 cores, speedup increase in the synchronous runs became insignificant, while PGAP scales well to 16,384 cores.</p><p>Note that Figure <ref type="figure">5</ref>(a) and Figure <ref type="figure">5</ref>(b) cannot be directly used to compare speedups between PGAP and its synchronous version. This is because they used different base cases for speedup calculation (512-core runs of PGAP and its synchronous version, respectively). Instead, we define ratio of speedup as belows for comparing speedup differences between asynchronous and synchronous migration strategies. It is measured as the ratio of execution time of the synchronous version and that of PGAP:</p><formula xml:id="formula_9">ratio of speedup = speedup async speedup sync = T base /T async T base /T sync = T sync T async<label>(3)</label></formula><p>where T base is the execution time of a common base case. Figure <ref type="figure">5(c)</ref> shows the ratio of speedup, calculated based on equation 3 for all of the solution quality thresholds that the synchronous version achieved in at least one setting of the given numbers of cores. For all measurable cases, the values of the ratio are larger than 1, meaning that PGAP achieved better speedup than its synchronous version. Particularly, when using 16,384 cores, the ratio ranges from 1.90 to 2.47.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.2.">Weak scaling test</head><p>Because increasing problem size may alter problem difficulty and the amount of computation needed to solve the problem, problem size, as used in typical weak scaling test, is not appropriate to use in weak scaling test of PGAs. Instead, our weak scaling test varies the size of global population in order to measure how PGAP leverages larger global population numerically, enabled by the use of more cores, to diversify the search in solution space and achieve a designated solution quality more effectively. We refer to this weak scaling test method as population scaling. The benefit of using a large population is straightforward because it provides a much larger sampling solution space for GA. But using a large population can easily turn a GA evolutionary process into a random search that can hardly converge on a single core. With PGA, however, the evolutionary process can be kept effective at deme level, but much more solution space can be searched by running a large number of demes simultaneously.</p><p>It is worth noting that population scaling adds more cores to do additional work on the same problem, similar to the "new-era" weak scaling proposed by Sarkar et al. <ref type="bibr" target="#b64">[65]</ref>. Leveraging larger global population, population scaling is designed to evaluate the capability of PGAP to obtain better solutions in a shorter amount of time. The test was done on the Ranger supercomputer. We use the time taken to achieve a certain solution quality as the measure of population scaling. Deme size was set to 200. The number of cores used ranges from 1,024 to 16,384. As the number of cores doubles, global population size also doubles from 204,800 to 3,287,400. We ran this test on both PGAP and its synchronous version. Each run was given one hour to finish.</p><p>The benefit of using large population PGAP can be shown in Figure <ref type="figure">6</ref>(a). The time taken to achieve certain solution quality threshold, quantified again as the upper bound of fitness value, was measured in accordance to the increase of the number of cores. Overall, as the bound became tighter, it took more time to find solutions whose fitness values are equal to or better than the bound. For a particular bound, it is obvious that using more cores reduces the time taken to find solutions with equal or better quality. This trend became more significant for tighter bounds. In fact, for those bounds tighter than 184,000, runs using smaller number of cores started running out of time to find solutions of specified quality. For example, solutions with bound 182,500 or better were only found by using 8,192 and 16,384 cores. For all of the runs, using 16,384 cores significantly reduced the time taken to achieve the specified solution quality. This can be explained as follows: 1) running massive demes simultaneously can greatly increase the overall probability of finding new elite solutions; and 2) migration operators are able to propagate good solutions to all demes if they are globally better, stimulating local evolutionary processes at deme level. There were some variations observed. For the test problem instance, improving solution quality from threshold 188,000 to 186,500 was difficult with the given PGA configuration in both asynchronous and synchronous versions. In the asynchronous version, higher fluctuations were observed in one of the runs of using 2,048 and 4,096 cores, respectively, and had subsequent effect on reaching tighter bounds. Such variation was due to the dynamics in stochastic computation specific to each run.</p><p>In contrast, Figure <ref type="figure">6</ref>(b) shows the weak scaling performance of the synchronous version. Consistently, not only the time taken to reach a bound was longer than PGAP, the synchronous runs could not find solutions better than 184,000 given the same amount of execution time (1 hour). . When using 16,384 cores, the execution time improvement was consistently around 60%. In summary, the weak scaling experiment shows that asynchronous migration was able to exploit large population size more effectively and showed significant numerical performance advantages over the corresponding synchronous version as more cores were used.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Communication to computation ratio</head><p>One advantage of using asynchronous migration in PGA is improved communication to computation ratio. Such advantage becomes more obvious when using a large number of cores. In any synchronous PGA implementation, a delay at one core could affect all, while such delays in asynchronous migration only affect direct neighbors (in receiving solutions) and can even be offset by the overlapping of computation and communication.</p><p>In this experiment, the E801600 problem instance was solved by both asynchronous and synchronous versions of PGAP with the same settings as specified in the weak scaling experiment. Figure <ref type="figure">7</ref> shows the communication cost as percentage of total execution time. PGAP's communication cost (on average 15.5%) was significantly lower than the synchronous version (on average 54%). For the synchronous version, the communication cost increased steadily as the number of cores doubled, mainly due to the increase cost of MPI Barrier() calls. While for PGAP, the communication cost decreased as more cores were used. The decrease can be explained as follows. In a GA execution, as the evolutionary process continues, it becomes harder to find better solutions. Therefore, random solutions are more likely to be selected and migrated. A holding strategy is applied in both PGAP and its corresponding synchronous version to delay the export operation in this situation in order to avoid excessive injection of randomness. As more cores are used, PGAP achieves a solution bound earlier, beyond which getting better solutions takes longer time and the holding strategy is applied more frequently. This scenario applies to the synchronous version, too. But the expensive MPI Barrier() cost at large scale is more significant than the reduction of communication caused by the holding operations. This observation indicates that optimal search strategies should be employed to keep searching efficiency at pace with the increase of computing power.</p><p>To better understand the communication variation in asynchronous migration, a snapshot of communication cost on each core is plotted for a run using 16,384 cores, shown in Figure <ref type="figure">8</ref>. Most of cores' communication cost was around 13%, with a few cores ranging between 8.5% and 18%. This means that communication cost in this large-scale PGAP run was consistent across cores despite that migration operations were not synchronized. Also, the asynchronous migration strategy was able to manage the 9.5% communication cost variation. This experiment further illustrated that asynchronous PGA, if designed properly, can also be reliable while introducing significantly lower communication cost than synchronous PGAs. This experiment was designed to evaluate solution quality of the baseline sequential GA of PGAP, which enhanced the feasibility and quality improvement operators used in Chu and Beasley <ref type="bibr" target="#b21">[22]</ref>. As mentioned in section 3, randomness was introduced in the feasibility and solution quality improvement operators as a way to diversify search patterns for item reassignment as PGAP scales to use large amount of demes. This algorithmic change improves PGA performance in both spatial and temporal contexts. For all of the demes spatially distributed in PGA computation, the improved operators enable more flexible search patterns on all demes with additional help from using a unique random number sequence per deme. Temporally, they allow each iteration to use a different search order from previous iterations for finding alternative item reassignments. Therefore, the change we made to introduce more randomness also improves the baseline sequential GA. In this experiment, GA parameters were set as the same as in <ref type="bibr" target="#b21">[22]</ref>, i.e., population size was 100 and the stopping rule was without improvement in consecutive 500,000 iterations. We ran the baseline algorithm and the two-deme PGAP cases only once. Results were compared with the best-so-far GA results <ref type="bibr" target="#b23">[24]</ref> to the authors' knowledge. Table <ref type="table" target="#tab_2">3</ref> lists the best solutions found and compares the gap to the maximum lower bounds (found by the linear programming package (IP/LP Copt)) found among CPLEX commercial software <ref type="bibr" target="#b65">[66]</ref>, CRH-GA algorithm <ref type="bibr" target="#b23">[24]</ref>, and our baseline GA. Experiment results showed that our baseline GA outperforms Feltl's GA in 31 out of 39 small-scale type D, E, and F instances. The solution quality improvement is significant because results reported in <ref type="bibr" target="#b23">[24]</ref> show the best among multiple runs.</p><p>Table <ref type="table" target="#tab_2">3</ref> also illustrates the immediate benefit of using two demes with migration on a single core (column 'Two-deme PGAP'). In the two-deme case, the single core take turns to run two demes with migration. Each deme is assigned with a different random number sequence that generates different solution search paths. The benefit of using multiple demes with migration is obvious for the GAP problem. Even though the two demes ran on a single core, Table <ref type="table" target="#tab_2">3</ref> shows that it effectively improved solution quality and outperformed Feltl's GA in 37 instances, and our sequential GA in 31 instances.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.2.">Numerical performance</head><p>Using asynchronous migration has two major impacts on the numerical performance of PGA: 1) reduced communication cost allows PGA to run more iterations within the same amount of time; and 2) migration intervals become more dynamic among spatially distributed demes. In the case of synchronous migration, a migration operation is followed by a 'silent' time among all demes, the length of which is determined by the parameters of migration interval. But in the case of asynchronous migration, as computation goes on, the timing for migration shifts gradually among demes because of runtime dynamics such as system clock and communication delays. Hart et al. <ref type="bibr" target="#b9">[10]</ref> demonstrated the benefits of such dynamics on improving PGA performance in terms of the total number of function evaluations required to achieve a designated solution quality. Alba et al. <ref type="bibr" target="#b66">[67]</ref> demonstrated the effect of network latency of LAN (local area network) and WAN (wide area network) on PGA efficiency when they found that WAN executions could be more efficient due to longer isolation times. We used the result of the weak scaling experiment to evaluate numerical performance of PGAP.</p><p>To understand the benefit of reduced communication cost, we use the number of iterations performed per second as a measure (referred to as iteration rate) to evaluate PGAP's numerical efficiency. In a multi-deme case, iteration rate is measured as the average value across all demes. Figure <ref type="figure">9</ref> shows the result of using 1,024 -16,384 cores. The result on the synchronous version is easy to understand. The decrease of iteration rate is due to the increasing cost of MPI Barrier() as more cores are involved. Additionally, because of the use of global barrier, the standard deviation is marginal. Compared to the synchronous case, the benefit of using asynchronous migration is significant, indicated by much higher iteration rate. We also observed that, as more cores were used, the iteration rate did not decrease. Instead, the rate increased from 441.84 to 471.84. Such increase was due to more frequent invocations of the holding strategy. The standard deviation for the asynchronous case decreased from 25.39 to 18.14 as the number of cores increases from 1,024 to 16,384. This may not suggest to use more cores in order to get more stable iteration rate. Instead, the decreasing standard deviation was, again, due to more frequent holding operations caused by earlier PGA convergence in asynchronous runs. In summary, the iteration rate analysis clearly shows that the asynchronous migration strategy in PGAP is highly scalable to the number of cores and the consistent iteration rate allows PGAP to finish more evolution iterations collectively by using more computing power.</p><p>By performing more iterations on a problem instance, the improvement of solution quality is expected for PGAP. Figure <ref type="figure" target="#fig_5">10</ref> shows how much PGAP can outperform the synchronous version in obtaining better solutions within one hour of computation. We measured solution quality gain over the synchronous version as the percentage of f itness valuesync-f itness valueasync f itness valuesync</p><p>. For all of the cases we evaluated, solution quality of the asynchronous version is better than the synchronous one. The largest improvement of 1.08% occurred when using 16,384 cores, which improved fitness value by 1,999. The percentage of improvement increases as the number of cores doubled.</p><p>While the improvement of numerical performance from reduced communication cost is apparent, the influence of the migration strategy itself on PGAP's problem-solving capability can be intricate due to the high variation on the timing of migration operations among a large number of demes. A preliminary study was conducted to measure such runtime influence in PGAP by calculating the number of iterations needed to achieve a set of solution quality thresholds on Ranger, instead of measuring the execution time which is highly correlated with communication cost. Figure <ref type="figure">11</ref> shows the result obained from the weak scaling experiment. The number of iterations needed in each run is calculated as the minimum number of iterations taken among all of the demes that reached the solution bound. For the cases of 1,024, 2,048, and 16,384 cores, PGAP took less number of iterations to reach any of the ten solution quality thresholds. But for the cases of 4,096 and 8,192 processors, more iterations were needed than the synchronous version to improve from threshold 188,000 to 186,500. Combined with figure <ref type="figure">6</ref>, the numerical performance gain from PGAP seems to mainly attribute to the significant reduction of communication cost. But for the case of 16,384, we consistently observed better performance of PGAP in both execution time and the number of iterations in achieving a specified solution quality.</p><p>The advantage of using asynchronous migration is also illustrated by the convergence experiment. Figure <ref type="figure" target="#fig_3">12</ref> depicts snapshots of the fitness value of the best solutions found at each timestamp in a set of runs of PGAP (Figure <ref type="figure" target="#fig_3">12(a)</ref>) and its synchronous version (Figure <ref type="figure" target="#fig_3">12</ref>(b)) using 64, 256, and 1,024 cores on the Lonestar supercomputer. PGAP converged much faster in all cases.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.3.">Finding feasible solutions</head><p>The capability to find more feasible solutions by leveraging massive computing resources is an important factor to illustrate how large-scale PGAP runs explore solution space more efficiently than its synchronous version. Obtaining more feasible solutions of specified solution quality is often one of the problemsolving goals for a lot of GAP applications. By investigating the landscape of found feasible solutions, better problem-specific heuristics can be developed to guide solution space search toward promising directions while avoid being trapped in local optima. The capability of PGAP for finding feasible solutions is measured by counting the number of unique solutions found with equal or better solution quality than the specified quality thresholds within one hour of weak-scaling test runs. PGAP runs using 16,384 cores were compared with the runs of its synchronous version. Results showed that, on average, PGAP found 11,093 unique solutions with fitness value better than 194,000, 19.98% more than the synchronous version. This trend became more obvious as solution quality thresholds became tighter. At thresholds of 188,000 and 186,000, PGAP found 40.62% and 75.90% more unique solutions than the synchronous version, respectively. Such improvement on problem-solving capability mainly attributes to the fact that, given the same amount of execution time, the asynchronous migration strategy allows PGAP to perform significantly more iterations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Concluding discussions</head><p>This paper described a scalable parallel genetic algorithm for solving large GAP instances by leveraging massively parallel computing. Realizing that synchronizing massive cores imposes significant performance penalty, an asynchronous migration strategy is developed to improve the numerical performance of PGAP. This strategy has three migration operators: export, import, and inject. By using buffer-based communication and non-blocking message passing between sending and receiving demes, migration communication can be overlapped with GA computation without adding any global barrier to synchronize demes. Compared to corresponding synchronous implementation, experiments showed significant improvement of PGAP on communication and computation ratio, speedup, and the capability to explore solution space efficiently. Superlinear speedups were observed in strong-scaling tests against large problem instances. Parallel efficiency study of PGA is also important to explore how we can efficiently use computing resources to achieve a specified solution threshold given a particular problem. Cantu-Paz and Goldberg <ref type="bibr" target="#b67">[68]</ref> studied the speedup and efficiency of a simplified global parallelization-based PGA from theoretical perspective and found interesting results on how to determine the optimal number of cores for solving a given problem instance. However, the study of parallel efficiency in coarse-grained PGA is more complicated and remains to be an open research problem because an optimal configuration of PGA may depend on the problem, the difficulty of reaching the specified solution quality, and runtime dynamics since PGA is a type of stochastic algorithm. By using the asynchronous migration strategy, the communication cost of PGAP is greatly reduced. As a result, each core is efficiently used busy for local evolutionary computation. But computing resources can still be wasted if one deme largely repeats the work done by another deme. Similarity analysis on inter-deme populations, as part of our future work, may help reveal such phenomenon and develop runtime strategies to avoid it.</p><p>Experiment results in the weak scaling experiment showed that PGAP exhibited desirable scalability for leveraging large populationsize enabled by using massive cores in solving large problem instances. To avoid runtime failure and the loss of good solutions observed in asynchronous PGAs, two buffer overflow problems are identified and addressed in PGAP by establishing two sufficient conditions for appropriate PGAP parameter configuration. The design of the asynchronous migration strategy is generic and independent of the targeted GAP problem. Therefore, the two sufficient conditions can be further applied as general guidelines to the development of asynchronous coarse-grained PGAs.</p><p>We will further study PGA algorithmic behaviors using larger scale parallel computing environments, developing GAP-specific solution space search strategies, and extending the asynchronous PGA design on heterogeneous computing platforms. We will further analyze migration strategies of PGAP by looking at alternatives on topology and migration parameters for more efficient GA search process control. We will continue to study the scalability of PGAP at larger scale (e.g., exascale <ref type="bibr" target="#b56">[57]</ref>). The findings presented in this paper have been incorporated in PGAP software library. This library has been deployed on and tuned for the petascale Blue Waters supercomputer at the National Center for Supercomputing Applications (NCSA). Desirable scalability up to 131,072 cores, similar to the results presented in this paper, was obtained in solving very large problem instances. To further improve PGAP, the feasibility and quality improvement operators will be enhanced by exploring neighboring functions such as described in <ref type="bibr" target="#b68">[69]</ref>. Problem characteristics illustrated in <ref type="bibr" target="#b28">[29]</ref> will be leveraged to design GAP instance-specific migration strategies by changing migration intervals, migration rate, and strategies for selection and injection dynamically at runtime. Hybrid computing resources become increasingly common on su-percomputers, such as Blue Waters (CPU + GPU) and Stampede at TACC (CPU + MIC + GPU). On these resources, the cost of synchronization may be more significant due to variations of individual computer nodes and underlying network. Asynchronous migration is a suitable solution for such resources. However, the migration strategy developed in this paper needs to be extended to take advantage of hybrid computing resources. The associated algorithmic analysis needs to consider high variations of computing characteristics on hybrid computing platforms. Import interval: the number of iterations between two consecutive import operations K sendbuf Size of the sending buffer allocated as the system outgoing message buffer. K sendbuf ≥ r K impt Size of the import pool       </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head></head><label></label><figDesc>Figure 6(c) shows this trend more clearly by comparing the execution time improvement, measured as the percentage of execution timesync-execution timeasync execution timesync</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>4. 4 .</head><label>4</label><figDesc>Solution quality and numerical performance 4.4.1. Solution quality of baseline GA</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 1 Figure 2 :</head><label>12</label><figDesc>Figure 1: GAP encoding</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 3 :Figure 5 :</head><label>35</label><figDesc>Figure 3: PGAP operators and their interactions</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 10 :</head><label>10</label><figDesc>Figure 10: Solution quality comparison, measured as the percentage of fitness value improvement in PGAP over the synchronous version. Each run was given one hour to finish.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 :</head><label>1</label><figDesc>PGAP parameters considered in the algorithmic analysis</figDesc><table><row><cell>d</cell><cell>Connectivity: the number of directed connected</cell></row><row><cell></cell><cell>neighboring demes</cell></row><row><cell>r</cell><cell>Migration rate: the number of local solutions se-</cell></row><row><cell></cell><cell>lected for each export operation</cell></row><row><cell>M expt</cell><cell>Export interval: the number of iterations between</cell></row><row><cell></cell><cell>two consecutive export operations</cell></row><row><cell>M impt</cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 :</head><label>2</label><figDesc>PGAP default configuration</figDesc><table><row><cell>Parameters</cell><cell cols="2">Settings</cell><cell></cell></row><row><cell>Population size per deme</cell><cell>100</cell><cell></cell><cell></cell></row><row><cell cols="5">Initial population generation Random with feasibility improve-</cell></row><row><cell></cell><cell cols="4">ment or constraint-based im-</cell></row><row><cell></cell><cell cols="3">provement [24]</cell></row><row><cell>Selection</cell><cell cols="3">Binary tournament</cell></row><row><cell>Crossover</cell><cell cols="4">1-point. Probability: 0.8</cell></row><row><cell>Mutation</cell><cell cols="3">1-item mutation.</cell><cell>Probability:</cell></row><row><cell></cell><cell>0.2</cell><cell></cell><cell></cell></row><row><cell>Replacement</cell><cell cols="4">Replacing the unfittest or worst</cell></row><row><cell>Elitism</cell><cell>Yes</cell><cell></cell><cell></cell></row><row><cell>Stopping rules</cell><cell>No</cell><cell cols="2">solution</cell><cell>improvement,</cell></row><row><cell></cell><cell cols="2">bounded</cell><cell cols="2">solution</cell><cell>quality</cell></row><row><cell></cell><cell cols="4">reached, or fixed number of</cell></row><row><cell></cell><cell cols="2">iterations</cell><cell></cell></row><row><cell>Connectivity d</cell><cell>4</cell><cell></cell><cell></cell></row><row><cell>Migration rate r</cell><cell>2</cell><cell></cell><cell></cell></row><row><cell>Export interval M expt</cell><cell>50</cell><cell></cell><cell></cell></row><row><cell>Import interval M impt</cell><cell>25</cell><cell></cell><cell></cell></row><row><cell>Probability of holding</cell><cell cols="4">1/20 (the probability to export</cell></row><row><cell></cell><cell cols="4">when no better solution found</cell></row><row><cell></cell><cell cols="4">during a previous export interval)</cell></row><row><cell cols="3">Sending buffer size K sendbuf 20 solutions.</cell><cell cols="2">Actual memory</cell></row><row><cell></cell><cell cols="4">requirement is (20 × n × 4 +</cell></row><row><cell></cell><cell cols="4">buf f er overhead) bytes</cell></row><row><cell>Import pool size K impt</cell><cell cols="4">80 solutions. Actual memory re-</cell></row><row><cell></cell><cell cols="4">quirement is (80 × n × 4)</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 3 :</head><label>3</label><figDesc>Solution quality comparison 3456 among CPLEX software, CRH-GA, the baseline sequential GA, and 2-deme PGAP</figDesc><table><row><cell>Prob.</cell><cell></cell><cell>Size</cell><cell>IP/LP</cell><cell cols="2">CPLEX CRH-GA</cell><cell cols="2">Baseline GA</cell><cell cols="2">Two-deme PGAP</cell></row><row><cell cols="2">Type m</cell><cell>n</cell><cell>Copt</cell><cell cols="6">gap (%) gap (%) fitness value gap (%) fitness value gap (%)</cell></row><row><cell>D</cell><cell cols="2">5 400</cell><cell>25670</cell><cell>opt</cell><cell>0.44</cell><cell>25790</cell><cell>0.47</cell><cell>25737</cell><cell>0.26</cell></row><row><cell>D</cell><cell cols="3">10 400 25274.8</cell><cell>0.18</cell><cell>1.31</cell><cell>25509</cell><cell>0.93</cell><cell>25435</cell><cell>0.63</cell></row><row><cell>D</cell><cell cols="3">20 400 24546.8</cell><cell>0.51</cell><cell>1.97</cell><cell>24903</cell><cell>1.45</cell><cell>24855</cell><cell>1.26</cell></row><row><cell>D</cell><cell cols="2">40 100</cell><cell>6092</cell><cell>3.96</cell><cell>3.72</cell><cell>6327</cell><cell>3.86</cell><cell>6265</cell><cell>2.84</cell></row><row><cell>D</cell><cell cols="3">40 200 12244.9</cell><cell>2.22</cell><cell>3.06</cell><cell>12561</cell><cell>2.58</cell><cell>12520</cell><cell>2.25</cell></row><row><cell>D</cell><cell cols="3">40 400 24371.8</cell><cell>1.1</cell><cell>2.81</cell><cell>24851</cell><cell>1.97</cell><cell>24801</cell><cell>1.76</cell></row><row><cell>D</cell><cell cols="2">80 100</cell><cell>6110.5</cell><cell>6.6</cell><cell>7.01</cell><cell>6526</cell><cell>6.80</cell><cell>6501</cell><cell>6.39</cell></row><row><cell>D</cell><cell cols="3">80 200 12132.3</cell><cell>2.87</cell><cell>3.76</cell><cell>12490</cell><cell>2.95</cell><cell>12454</cell><cell>2.65</cell></row><row><cell>D</cell><cell cols="2">80 400</cell><cell>24177</cell><cell>2</cell><cell>3.02</cell><cell>24748</cell><cell>2.36</cell><cell>24587</cell><cell>1.70</cell></row><row><cell>E</cell><cell cols="2">5 100</cell><cell>7757</cell><cell>opt</cell><cell>0.24</cell><cell>7774</cell><cell>0.22</cell><cell>7760</cell><cell>0.04</cell></row><row><cell>E</cell><cell cols="2">5 200</cell><cell>15611</cell><cell>opt</cell><cell>0.23</cell><cell>15632</cell><cell>0.13</cell><cell>15626</cell><cell>0.10</cell></row><row><cell>E</cell><cell cols="2">5 400</cell><cell>30794</cell><cell>opt</cell><cell>0.28</cell><cell>30872</cell><cell>0.25</cell><cell>30826</cell><cell>0.10</cell></row><row><cell>E</cell><cell cols="2">10 100</cell><cell>7387.8</cell><cell>0.61</cell><cell>0.91</cell><cell>7454</cell><cell>0.90</cell><cell>7436</cell><cell>0.65</cell></row><row><cell>E</cell><cell cols="3">10 200 15039.8</cell><cell>0.25</cell><cell>0.96</cell><cell>15135</cell><cell>0.63</cell><cell>15126</cell><cell>0.57</cell></row><row><cell>E</cell><cell cols="3">10 400 29977.9</cell><cell>0.09</cell><cell>0.94</cell><cell>30135</cell><cell>0.52</cell><cell>30142</cell><cell>0.55</cell></row><row><cell>E</cell><cell cols="2">20 100</cell><cell>7348.2</cell><cell>1.32</cell><cell>1.74</cell><cell>7463</cell><cell>1.56</cell><cell>7448</cell><cell>1.36</cell></row><row><cell>E</cell><cell cols="3">20 200 14765.2</cell><cell>0.89</cell><cell>1.7</cell><cell>14923</cell><cell>1.07</cell><cell>14918</cell><cell>1.03</cell></row><row><cell>E</cell><cell cols="3">20 400 29500.3</cell><cell>0.34</cell><cell>1.6</cell><cell>29845</cell><cell>1.17</cell><cell>29810</cell><cell>1.05</cell></row><row><cell>E</cell><cell cols="2">40 100</cell><cell>7316.1</cell><cell>3.32</cell><cell>3.11</cell><cell>7497</cell><cell>2.47</cell><cell>7522</cell><cell>2.81</cell></row><row><cell>E</cell><cell cols="3">40 200 14630.4</cell><cell>1.85</cell><cell>2.2</cell><cell>14835</cell><cell>1.40</cell><cell>14858</cell><cell>1.56</cell></row><row><cell>E</cell><cell cols="3">40 400 29186.6</cell><cell>0.69</cell><cell>2.14</cell><cell>29586</cell><cell>1.37</cell><cell>29593</cell><cell>1.39</cell></row><row><cell>E</cell><cell cols="2">80 100</cell><cell>7650</cell><cell>opt</cell><cell>0.78</cell><cell>7670</cell><cell>0.26</cell><cell>7668</cell><cell>0.24</cell></row><row><cell>E</cell><cell cols="3">80 200 14566.7</cell><cell>2.17</cell><cell>2.93</cell><cell>14833</cell><cell>1.83</cell><cell>14846</cell><cell>1.92</cell></row><row><cell>E</cell><cell cols="3">80 400 29161.3</cell><cell>1.57</cell><cell>2.49</cell><cell>29631</cell><cell>1.61</cell><cell>29567</cell><cell>1.39</cell></row><row><cell>F</cell><cell cols="2">5 100</cell><cell>2755</cell><cell>opt</cell><cell>0.41</cell><cell>2761</cell><cell>0.22</cell><cell>2761</cell><cell>0.22</cell></row><row><cell>F</cell><cell cols="2">5 200</cell><cell>5294</cell><cell>opt</cell><cell>0.35</cell><cell>5304</cell><cell>0.19</cell><cell>5315</cell><cell>0.40</cell></row><row><cell>F</cell><cell cols="2">5 400</cell><cell>10745</cell><cell>opt</cell><cell>0.25</cell><cell>10776</cell><cell>0.29</cell><cell>10765</cell><cell>0.19</cell></row><row><cell>F</cell><cell cols="2">10 100</cell><cell>2276.8</cell><cell>1.99</cell><cell>3.95</cell><cell>2364</cell><cell>3.83</cell><cell>2348</cell><cell>3.13</cell></row><row><cell>F</cell><cell cols="2">10 200</cell><cell>4644.6</cell><cell>1.13</cell><cell>3.12</cell><cell>4794</cell><cell>3.22</cell><cell>4759</cell><cell>2.46</cell></row><row><cell>F</cell><cell cols="2">10 400</cell><cell>9372.7</cell><cell>0.46</cell><cell>2.78</cell><cell>9631</cell><cell>2.76</cell><cell>9604</cell><cell>2.47</cell></row><row><cell>F</cell><cell cols="2">20 100</cell><cell>2145.1</cell><cell>8.15</cell><cell>8.38</cell><cell>2339</cell><cell>9.04</cell><cell>2301</cell><cell>7.27</cell></row><row><cell>F</cell><cell cols="2">20 200</cell><cell>4310.1</cell><cell>4.73</cell><cell>6.55</cell><cell>4585</cell><cell>6.38</cell><cell>4552</cell><cell>5.61</cell></row><row><cell>F</cell><cell cols="2">20 400</cell><cell>8479.4</cell><cell>2.38</cell><cell>7.15</cell><cell>9050</cell><cell>6.73</cell><cell>8950</cell><cell>5.55</cell></row><row><cell>F</cell><cell cols="2">40 100</cell><cell>2110.1</cell><cell>21.28</cell><cell>18.27</cell><cell>2476</cell><cell>17.34</cell><cell>2501</cell><cell>18.53</cell></row><row><cell>F</cell><cell cols="2">40 200</cell><cell>4086.5</cell><cell>10.14</cell><cell>12.86</cell><cell>4522</cell><cell>10.66</cell><cell>4578</cell><cell>12.03</cell></row><row><cell>F</cell><cell cols="2">40 400</cell><cell>8274.3</cell><cell>4.05</cell><cell>10.36</cell><cell>9105</cell><cell>10.04</cell><cell>8907</cell><cell>7.65</cell></row><row><cell>F</cell><cell cols="2">80 100</cell><cell>2064.4</cell><cell>31.37</cell><cell>26.77</cell><cell>2655</cell><cell>28.61</cell><cell>2583</cell><cell>25.12</cell></row><row><cell>F</cell><cell cols="2">80 200</cell><cell>4123.4</cell><cell>19.05</cell><cell>17.33</cell><cell>4869</cell><cell>18.08</cell><cell>4837</cell><cell>17.31</cell></row><row><cell>F</cell><cell cols="2">80 400</cell><cell>8167.1</cell><cell>9.18</cell><cell>12.55</cell><cell>9248</cell><cell>13.23</cell><cell>9127</cell><cell>11.75</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head></head><label></label><figDesc>Figure 7: Distribution of communication cost in weak scaling test.Figure 8: Snapshot of communication cost at each core in a PGAP run using 16,384 coresFigure 9: Iteration rate comparison. The standard deviation is also plotted.</figDesc><table><row><cell>Communication cost Number of iterations per second</cell><cell>10% 20% 30% 40% 50% 60% 70% 500 250 300 350 400 450</cell><cell cols="8">19.36% 50.57% Asynchronous 17.34% 52.29% 469.95 Synchronous 452.62 441.84 268.67 252.69 234.04</cell><cell>13.68% 54.12% 471.29 217.89 Asynchronous 13.77% 55.69% Synchronous</cell><cell>203.89 57.39% 13.40% 471.84</cell></row><row><cell></cell><cell>200</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>0%</cell><cell>0</cell><cell cols="3">1024 1024 2048</cell><cell>3072</cell><cell cols="4">2048 4096 5120 Number of processors 4096 6144 7168 8192 9216 10240 11264 8192 12288 13312 14336 15360 16384 16384 Number of processors</cell></row><row><cell></cell><cell>30</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell cols="2">25 1.20%</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>1.09%</cell></row><row><cell>Communication cost (%) Fitness value improvement (%)</cell><cell cols="2">5 10 15 20 0.20% 0.40% 0.60% 0.80% 1.00%</cell><cell></cell><cell></cell><cell cols="2">0.60%</cell><cell></cell><cell></cell><cell></cell><cell>0.64%</cell><cell>0.84%</cell></row><row><cell></cell><cell cols="3">0 0.00% 0 1024</cell><cell>2048</cell><cell cols="2">3072 2048</cell><cell>4096</cell><cell>5120</cell><cell cols="2">6144 Number of processors 7168 8192 9216 10240 4096 8192 11264</cell><cell>12288</cell><cell>13312</cell><cell>14336 16384 15360</cell><cell>16384</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>Processor index</cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0"><p>http://people.brunel.ac.uk/˜mastjjb/jeb/info.html</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_1"><p>http://www-or.amp.i.kyoto-u.ac.jp/˜yagiura/gap/</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3" xml:id="foot_2"><p>Column IP/LP: lower bound found by linear programming</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4" xml:id="foot_3"><p>Column CPLEX: best solutions found by CPLEX. opt: optimal solution found</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="5" xml:id="foot_4"><p>gap (%): the percentage of f itness value best solution -lower bound lower bound</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="6" xml:id="foot_5"><p>IP/LP, CPLEX, and CRH-GA results are from Feltl et al.<ref type="bibr" target="#b23">[24]</ref>. They are included for comparison purpose</p></note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Acknowledgements</head><p>This work is supported in part by the National Science Foundation (NSF) under Grant Number OCI-1047916. Computational experiments used the Extreme Science and Engineering Discovery Environment (XSEDE) (resource allocation Award Number SES090019), which is supported by the National Science Foundation Grant Number OCI-1053575. This research is part of the Blue Waters sustained-petascale computing project, which is supported by the National Science Foundation (award number OCI 07-25070) and the state of Illinois. Blue Waters is a joint effort of the University of Illinois at Urbana-Champaign and its National Center for Supercomputing Applications. The authors thank Dr. Alberto Maria Segre at the University of Iowa for insightful discussions that led to this research topic and are grateful for the insightful comments received from CIGI members: Yizhao Gao, Anand Padmanabhan, and Mengyu Guo.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.">References</head></div>
			</div>

			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>• High scalability is achieved through an novel asynchronous migration strategy.</p><p>• Our algorithmic analysis resolves the buffer overflow issue in asynchronous PGAs.</p><p>• Strong and weak scaling tests demonstrate the superiority of our PGA approach.</p><p>• Our approach scales to 16,384 processors with super-linear speedups observed.</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">H</forename><surname>Holland</surname></persName>
		</author>
		<title level="m">Adaptation in natural and artificial systems</title>
		<meeting><address><addrLine>Cambridge, MA, USA</addrLine></address></meeting>
		<imprint>
			<publisher>MIT Press</publisher>
			<date type="published" when="1992">1992</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">The roles of mutation, inbreeding, crossbreeding and selection in evolution</title>
		<author>
			<persName><forename type="first">S</forename><surname>Wright</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. 6th Int</title>
		<meeting>6th Int</meeting>
		<imprint>
			<date type="published" when="1932">1932</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="356" to="366" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">Genetic algorithms in search, optimization and machine learning</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">E</forename><surname>Goldberg</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1989">1989</date>
			<publisher>Addison-Wesley Longman Publishing Co., Inc</publisher>
			<pubPlace>Boston, MA, USA</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Time complexity of evolutionary algorithms for combinatorial optimization: a decade of result</title>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">S</forename><surname>Oliveto</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Yao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Automation and Computing</title>
		<imprint>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="281" to="293" />
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Parallelism and evolutionary algorithms</title>
		<author>
			<persName><forename type="first">E</forename><surname>Alba</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Tomassini</surname></persName>
		</author>
		<idno type="DOI">10.1109/TEVC.2002.800880</idno>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Evolutionary Computation</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="443" to="462" />
			<date type="published" when="2002">2002</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<author>
			<persName><forename type="first">Z</forename><surname>Konfrst</surname></persName>
		</author>
		<idno type="DOI">10.1109/IPDPS.2004.1303155</idno>
		<ptr target="http://doi.ieeecomputersociety.org/10.1109/IPDPS.2004.1303155" />
	</analytic>
	<monogr>
		<title level="m">Parallel genetic algorithms: advances, computing trends, applications and perspectives, Parallel and Distributed Processing Symposium</title>
		<imprint>
			<date type="published" when="2004">2004</date>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page">162</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">High-performance parallel biocomputing</title>
		<author>
			<persName><forename type="first">C.-H</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Rajasekaran</surname></persName>
		</author>
		<idno type="DOI">10.1016/j.parco.2004.01.003</idno>
	</analytic>
	<monogr>
		<title level="j">Parallel Computing</title>
		<imprint>
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="issue">910</biblScope>
			<biblScope unit="page" from="999" to="1000" />
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">I</forename><surname>Hidalgo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Fernandez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Lanchares</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Cant-Paz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Zomaya</surname></persName>
		</author>
		<idno type="DOI">10.1016/j.parco.2010.09.001</idno>
	</analytic>
	<monogr>
		<title level="m">Parallel architectures and bioinspired algorithms</title>
		<imprint>
			<date type="published" when="2010">2010</date>
			<biblScope unit="volume">36</biblScope>
			<biblScope unit="page" from="553" to="554" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">A survey of parallel distributed genetic algorithms</title>
		<author>
			<persName><forename type="first">E</forename><surname>Alba</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">M</forename><surname>Troya</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Complexity</title>
		<imprint>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="31" to="52" />
			<date type="published" when="1999">1999</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Analysis of the numerical effects of parallelism on a parallel genetic algorithm</title>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">E</forename><surname>Hart</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">B</forename><surname>Baden</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">K</forename><surname>Belew</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">R</forename><surname>Kohn</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IPPS &apos;96: Proceedings of the 10th International Parallel Processing Symposium</title>
		<meeting><address><addrLine>Washington, DC, USA</addrLine></address></meeting>
		<imprint>
			<publisher>IEEE Computer Society</publisher>
			<date type="published" when="1996">1996</date>
			<biblScope unit="page" from="606" to="612" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">A multiplier adjustment method for the generalized assignment problem</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">L</forename><surname>Fisher</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Jaikumar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">N V</forename><surname>Wassenhove</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Management Science</title>
		<imprint>
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="issue">9</biblScope>
			<biblScope unit="page" from="1095" to="1103" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">A ptas for the multiple knapsack problem</title>
		<author>
			<persName><forename type="first">C</forename><surname>Chekuri</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Khanna</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SODA &apos;00: Proceedings of the 11th Annual Symposium on Discrete Algorithms</title>
		<imprint>
			<date type="published" when="2000">2000</date>
			<biblScope unit="page" from="213" to="222" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">Relations among some combinatorial programs</title>
		<author>
			<persName><forename type="first">R</forename><surname>Borndorfer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Weismantel</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1997">1997</date>
		</imprint>
		<respStmt>
			<orgName>Zentrum fr Informationstechnik Berlin</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">Tech. rep</note>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">A survey of algorithms for the generalized assignment problem</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">G</forename><surname>Cattrysse</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">N V</forename><surname>Wassenhove</surname></persName>
		</author>
		<idno type="DOI">10.1016/0377-2217(92)90077-M</idno>
	</analytic>
	<monogr>
		<title level="j">European Journal of Operational Research</title>
		<imprint>
			<biblScope unit="volume">60</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="260" to="272" />
			<date type="published" when="1992">1992</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">An integer generalized transportation model for optimal job assignment in computer networks</title>
		<author>
			<persName><forename type="first">V</forename><surname>Balachandran</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Operations Research</title>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="742" to="759" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Coupling land use allocation models with raster gis</title>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">G</forename><surname>Cromley</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">M</forename><surname>Hanink</surname></persName>
		</author>
		<idno type="DOI">10.1007/s101090050009</idno>
	</analytic>
	<monogr>
		<title level="j">Journal of Geographical Systems</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">The multidimensional 0-1 knapsack problem: An overview</title>
		<author>
			<persName><forename type="first">A</forename><surname>Freville</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">European Journal of Operational Research</title>
		<imprint>
			<biblScope unit="volume">155</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1" to="21" />
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title/>
		<author>
			<persName><surname>Xsede</surname></persName>
		</author>
		<ptr target="http://xsede.org" />
		<imprint>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Improving flexibility and efficiency by adding parallelism to genetic algorithms</title>
		<author>
			<persName><forename type="first">E</forename><surname>Alba</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">M</forename><surname>Troya</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Statistics and Computing</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="91" to="114" />
			<date type="published" when="2002">2002</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">The massively parallel genetic algorithm for rna folding: Mimd implementation and population variation</title>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">A</forename><surname>Shapiro</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">C</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Bengali</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Bioinformatics</title>
		<imprint>
			<biblScope unit="volume">17</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="137" to="148" />
			<date type="published" when="2001-02">February 2001</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Scalable programming models for massively multicore processors</title>
		<author>
			<persName><forename type="first">M</forename><surname>Mccool</surname></persName>
		</author>
		<idno type="DOI">10.1109/JPROC.2008.917731</idno>
	</analytic>
	<monogr>
		<title level="j">Proceedings of the IEEE</title>
		<imprint>
			<biblScope unit="volume">96</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="816" to="831" />
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">A genetic algorithm for the generalised assignment problem</title>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">C</forename><surname>Chu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">E</forename><surname>Beasley</surname></persName>
		</author>
		<idno type="DOI">10.1016/S0305-0548(96)00032-9</idno>
		<ptr target="http://dx.doi.org/10.1016/S0305-0548(96)00032-9" />
	</analytic>
	<monogr>
		<title level="j">Comput. Oper. Res</title>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="17" to="23" />
			<date type="published" when="1997">1997</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">A genetic algorithm for the generalised assignment problem</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">M</forename><surname>Wilson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The Journal of the Operational Research Society</title>
		<imprint>
			<biblScope unit="volume">48</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="804" to="809" />
			<date type="published" when="1997">1997</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">An improved hybrid genetic algorithm for the generalized assignment problem, in: SAC &apos;04: Proceedings of the</title>
		<author>
			<persName><forename type="first">H</forename><surname>Feltl</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">R</forename><surname>Raidl</surname></persName>
		</author>
		<idno type="DOI">10.1145/967900.968102</idno>
		<ptr target="http://doi.acm.org/10.1145/967900.968102" />
	</analytic>
	<monogr>
		<title level="j">ACM symposium on Applied computing</title>
		<imprint>
			<biblScope unit="page" from="990" to="995" />
			<date type="published" when="2004">2004. 2004</date>
			<publisher>ACM</publisher>
			<pubPlace>New York, NY, USA</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">The guided genetic algorithm and its application to the generalized assignment problem, Tools with Artificial Intelligence</title>
		<author>
			<persName><forename type="first">T</forename><surname>Lau</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Tsang</surname></persName>
		</author>
		<idno type="DOI">10.1109/TAI.1998.744862</idno>
	</analytic>
	<monogr>
		<title level="m">Tenth IEEE International Conference on</title>
		<imprint>
			<date type="published" when="1998-10-12">1998. 10-12 Nov 1998</date>
			<biblScope unit="page" from="336" to="343" />
		</imprint>
	</monogr>
	<note>Proceedings</note>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title level="m" type="main">A constructive genetic algorithm for the generalized assignment problem</title>
		<author>
			<persName><forename type="first">L</forename><surname>Lorena</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Narciso</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Beasley</surname></persName>
		</author>
		<imprint/>
	</monogr>
	<note>Evolutionary Optimization</note>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Simulated annealing for the 01 multidimensional knapsack problem</title>
		<author>
			<persName><forename type="first">F</forename><surname>Qian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Ding</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Chinese Universities Numerical Mathematics</title>
		<imprint>
			<biblScope unit="volume">16</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="320" to="327" />
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">A tabu search heuristic for the generalized assignment problem</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">A</forename><surname>Diaz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Fernandez</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">European Journal of Operational Research</title>
		<imprint>
			<biblScope unit="volume">132</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="22" to="38" />
			<date type="published" when="2001">2001</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">A path relinking approach with ejection chains for the generalized assignment problem</title>
		<author>
			<persName><forename type="first">M</forename><surname>Yagiura</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Ibaraki</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Glover</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">European Journal of Operational Research</title>
		<imprint>
			<biblScope unit="volume">127</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="548" to="569" />
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Lagrangian relaxation guided problem space search heuristics for generalized assignment problems</title>
		<author>
			<persName><forename type="first">V</forename><surname>Jeet</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Kutanoglu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">European Journal of Operational Research</title>
		<imprint>
			<biblScope unit="volume">127</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="1039" to="1056" />
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">An lp-based heuristic procedure for the generalized assignment problem with special ordered sets</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">P</forename><surname>French</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">M</forename><surname>Wilson</surname></persName>
		</author>
		<idno type="DOI">10.1016/j.cor.2005.09.008</idno>
		<ptr target="http://dx.doi.org/10.1016/j.cor.2005.09.008" />
	</analytic>
	<monogr>
		<title level="j">Comput. Oper. Res</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="2359" to="2369" />
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Cellular genetic algorithms</title>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">D</forename><surname>Whitley</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 5th International Conference on Genetic Algorithms</title>
		<meeting>the 5th International Conference on Genetic Algorithms</meeting>
		<imprint>
			<date type="published" when="1993">1993</date>
			<biblScope unit="page">658</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">A massively parallel architecture for distributed genetic algorithms</title>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">E</forename><surname>Eklund</surname></persName>
		</author>
		<idno type="DOI">10.1016/j.parco.2003.12.009</idno>
	</analytic>
	<monogr>
		<title level="j">Parallel Computing</title>
		<imprint>
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="issue">56</biblScope>
			<biblScope unit="page" from="647" to="676" />
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">On the impact of the migration topology on the island model</title>
		<author>
			<persName><forename type="first">M</forename><surname>Ruciski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Izzo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Biscani</surname></persName>
		</author>
		<idno type="DOI">10.1016/j.parco.2010.04.002</idno>
	</analytic>
	<monogr>
		<title level="j">Parallel Computing</title>
		<imprint>
			<biblScope unit="volume">36</biblScope>
			<biblScope unit="issue">1011</biblScope>
			<biblScope unit="page" from="555" to="571" />
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Parallel evolutionary modelling of geological processes</title>
		<author>
			<persName><forename type="first">D</forename><surname>Dambrosio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Spataro</surname></persName>
		</author>
		<idno type="DOI">10.1016/j.parco.2006.12.003</idno>
	</analytic>
	<monogr>
		<title level="j">Parallel Computing</title>
		<imprint>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="186" to="212" />
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">The distributed genetic algorithm revisited</title>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">C</forename><surname>Belding</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 6th International Conference on Genetic Algorithms</title>
		<meeting>the 6th International Conference on Genetic Algorithms<address><addrLine>San Francisco, CA, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Morgan Kaufmann Publishers Inc</publisher>
			<date type="published" when="1995">1995</date>
			<biblScope unit="page" from="114" to="121" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">An overview of evolutionary algorithms: practical issues and common pitfalls</title>
		<author>
			<persName><forename type="first">D</forename><surname>Whitley</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Information and Software Technology</title>
		<imprint>
			<biblScope unit="volume">43</biblScope>
			<biblScope unit="issue">14</biblScope>
			<biblScope unit="page" from="817" to="831" />
			<date type="published" when="2001">2001</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Scalable parallel genetic algorithms</title>
		<author>
			<persName><forename type="first">W</forename><surname>Rivera</surname></persName>
		</author>
		<idno type="DOI">10.1023/A:1011614231837</idno>
		<ptr target="http://dx.doi.org/10.1023/A:1011614231837" />
	</analytic>
	<monogr>
		<title level="j">Artif. Intell. Rev</title>
		<imprint>
			<biblScope unit="volume">16</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="153" to="168" />
			<date type="published" when="2001">2001</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<monogr>
		<author>
			<persName><forename type="first">E</forename><surname>Cantu-Paz</surname></persName>
		</author>
		<title level="m">A survey of parallel genetic algorithms</title>
		<imprint>
			<date type="published" when="1997">1997</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Distributed genetic algorithms</title>
		<author>
			<persName><forename type="first">R</forename><surname>Tanese</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 3rd International Conference on Genetic Algorithms</title>
		<meeting>the 3rd International Conference on Genetic Algorithms<address><addrLine>San Francisco, CA, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Morgan Kaufmann Publishers Inc</publisher>
			<date type="published" when="1989">1989</date>
			<biblScope unit="page" from="434" to="439" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Parallel genetic simulated annealing: A massively parallel simd algorithm</title>
		<author>
			<persName><forename type="first">H</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><forename type="middle">S</forename><surname>Flann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">W</forename><surname>Watson</surname></persName>
		</author>
		<idno type="DOI">10.1109/71.663870</idno>
		<ptr target="http://dx.doi.org/10.1109/71.663870" />
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Parallel Distrib. Syst</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="126" to="136" />
			<date type="published" when="1998">1998</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Solving the mapping problem with a genetic algorithm on the maspar-1, in: Massively Parallel Computing Systems</title>
		<author>
			<persName><forename type="first">T</forename><surname>Kalinowski</surname></persName>
		</author>
		<idno type="DOI">10.1109/MPCS.1994.367057</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the First International Conference on</title>
		<meeting>the First International Conference on</meeting>
		<imprint>
			<date type="published" when="1994">1994. 1994</date>
			<biblScope unit="page" from="370" to="374" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<monogr>
		<title level="m" type="main">A massively distributed parallel genetic algorithm (mdpga)</title>
		<author>
			<persName><forename type="first">S</forename><surname>Baluja</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1992">1992</date>
		</imprint>
		<respStmt>
			<orgName>Carnegie Mellon University</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">Tech. rep</note>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Co-evolving intertwined spirals</title>
		<author>
			<persName><forename type="first">H</forename><surname>Juille</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">B</forename><surname>Pollack</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Fifth Annual Conference on Evolutionary Programming</title>
		<meeting>the Fifth Annual Conference on Evolutionary Programming</meeting>
		<imprint>
			<publisher>MIT Press</publisher>
			<date type="published" when="1996">1996</date>
			<biblScope unit="page" from="461" to="468" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">A simd environment for genetic algorithms with interconnected subpopulations</title>
		<author>
			<persName><forename type="first">D</forename><surname>Prabhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">P</forename><surname>Buckles</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><forename type="middle">E</forename><surname>Petry</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Scalable Computing: Practice and Experience</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="65" to="86" />
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Physically realistic motion synthesis in animation</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">T</forename><surname>Ngo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Marks</surname></persName>
		</author>
		<idno type="DOI">10.1162/evco.1993.1.3.235</idno>
	</analytic>
	<monogr>
		<title level="j">Evolutionary Computation</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="235" to="268" />
			<date type="published" when="1993">1993</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">High performance mpi-2 one-sided communication over infiniband, Cluster Computing and the Grid</title>
		<author>
			<persName><forename type="first">W</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H.-W</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Panda</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Gropp</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Thakur</surname></persName>
		</author>
		<idno type="DOI">10.1109/CCGrid.2004.1336648</idno>
		<ptr target="http://doi.ieeecomputersociety.org/10.1109/CCGrid.2004.1336648" />
	</analytic>
	<monogr>
		<title level="m">IEEE International Symposium on 0</title>
		<imprint>
			<date type="published" when="2004">2004</date>
			<biblScope unit="page" from="531" to="538" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">A study of process arrival patterns for mpi collective operations</title>
		<author>
			<persName><forename type="first">A</forename><surname>Faraj</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Patarasuk</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Yuan</surname></persName>
		</author>
		<idno type="DOI">10.1145/1274971.1274996</idno>
		<ptr target="http://doi.acm.org/10.1145/1274971.1274996" />
	</analytic>
	<monogr>
		<title level="m">ICS &apos;07: Proceedings of the 21st annual international conference on Supercomputing</title>
		<meeting><address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2007">2007</date>
			<biblScope unit="page" from="168" to="179" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<monogr>
		<author>
			<persName><forename type="first">R</forename><surname>Cledat</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Sreeram</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Pande</surname></persName>
		</author>
		<idno type="DOI">10.1109/IPDPS.2008.4536264</idno>
		<title level="m">Opportunistic computing: A new paradigm for scalable realism on many-cores, in: HOTPAR &apos;09: USENIX Workshop on Hot Topics in Parallelism</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<monogr>
		<author>
			<persName><forename type="first">K</forename><surname>Asanovic</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Bodik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">C</forename><surname>Catanzaro</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">J</forename><surname>Gebis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Husbands</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Keutzer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">A</forename><surname>Patterson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">L</forename><surname>Plishker</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Shalf</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">W</forename><surname>Williams</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">A</forename><surname>Yelick</surname></persName>
		</author>
		<title level="m">The landscape of parallel computing research: A view from berkeley</title>
		<meeting><address><addrLine>Berkeley</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2006-12">Dec 2006</date>
		</imprint>
		<respStmt>
			<orgName>EECS Department, University of California</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">Tech. rep</note>
</biblStruct>

<biblStruct xml:id="b50">
	<monogr>
		<title level="m">Petascale Computing: Algorithms and Applications, 1st Edition</title>
		<imprint>
			<publisher>Chapman &amp; Hall/CRC</publisher>
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">An analysis of synchronous and asynchronous parallel distributed genetic algorithms with structured and panmictic islands</title>
		<author>
			<persName><forename type="first">E</forename><surname>Alba</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">M</forename><surname>Troya</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 11 IPPS/SPDP&apos;99 Workshops Held in Conjunction with the 13th International Parallel Processing Symposium and 10th Symposium on Parallel and Distributed Processing</title>
		<meeting>the 11 IPPS/SPDP&apos;99 Workshops Held in Conjunction with the 13th International Parallel Processing Symposium and 10th Symposium on Parallel and Distributed Processing<address><addrLine>London, UK</addrLine></address></meeting>
		<imprint>
			<publisher>Springer-Verlag</publisher>
			<date type="published" when="1999">1999</date>
			<biblScope unit="page" from="248" to="256" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">Coarse-grain parallel genetic algorithms: categorization and new approach, Parallel and Distributed Processing, 1994. Proceedings</title>
		<author>
			<persName><forename type="first">S.-C</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Punch</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">F</forename></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Goodman</surname></persName>
		</author>
		<idno type="DOI">10.1109/SPDP.1994.346184</idno>
	</analytic>
	<monogr>
		<title level="m">Sixth IEEE Symposium on</title>
		<imprint>
			<date type="published" when="1994-10-29">26-29 Oct 1994</date>
			<biblScope unit="page" from="28" to="37" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">A scalable cellular implementation of parallel genetic programming</title>
		<author>
			<persName><forename type="first">G</forename><surname>Folino</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Pizzuti</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Spezzano</surname></persName>
		</author>
		<idno type="DOI">10.1109/TEVC.2002.806168</idno>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Evolutionary Computation</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="37" to="53" />
			<date type="published" when="2003">2003</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">Serial and parallel genetic algorithms as function optimizers</title>
		<author>
			<persName><forename type="first">V</forename><forename type="middle">S</forename><surname>Gordon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">D</forename><surname>Whitley</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 5th International Conference on Genetic Algorithms</title>
		<meeting>the 5th International Conference on Genetic Algorithms<address><addrLine>San Francisco, CA, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Morgan Kaufmann Publishers Inc</publisher>
			<date type="published" when="1993">1993</date>
			<biblScope unit="page" from="177" to="183" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<analytic>
		<title level="a" type="main">Parallel performance of a latticeboltzmann/finite element cellular blood flow solver on the ibm blue gene/p architecture</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">R</forename><surname>Clausen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">A</forename><surname>Reasor</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">K</forename><surname>Aidun</surname></persName>
		</author>
		<idno type="DOI">10.1016/j.cpc.2010.02.005</idno>
	</analytic>
	<monogr>
		<title level="j">Computer Physics Communications</title>
		<imprint>
			<biblScope unit="volume">181</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="1013" to="1020" />
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<analytic>
		<title level="a" type="main">The impact of multicore on computational science software</title>
		<author>
			<persName><forename type="first">J</forename><surname>Dongarra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Gannon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Fox</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Kennedy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">CTWatch Quarterly</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="817" to="831" />
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<analytic>
		<title level="a" type="main">A comparative analysis of selection schemes used in genetic algorithms</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">E</forename><surname>Goldberg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Deb</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Foundations of Genetic Algorithms</title>
		<editor>
			<persName><forename type="first">G</forename><forename type="middle">J E</forename><surname>Rawlins</surname></persName>
		</editor>
		<meeting><address><addrLine>San Francisco, CA</addrLine></address></meeting>
		<imprint>
			<publisher>Morgan Kaufmann</publisher>
			<biblScope unit="page" from="69" to="93" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b58">
	<analytic>
		<title level="a" type="main">Fine-grained parallel genetic algorithms</title>
		<author>
			<persName><forename type="first">B</forename><surname>Manderick</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Spiessens</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the third international conference on Genetic algorithms</title>
		<meeting>the third international conference on Genetic algorithms</meeting>
		<imprint>
			<publisher>Morgan Kaufmann Publishers Inc</publisher>
			<date type="published" when="1989">1989</date>
			<biblScope unit="page" from="428" to="433" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b59">
	<monogr>
		<title level="m" type="main">Parallel genetic programming: a scalable implementation using the transputer network architecture</title>
		<author>
			<persName><forename type="first">D</forename><surname>Andre</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">R</forename><surname>Koza</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1996">1996</date>
			<biblScope unit="page" from="317" to="337" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b60">
	<monogr>
		<title level="m" type="main">MPI: A Message-Passing Interface Standard, Version 3.0, High Performance Computing Center Stuttgart (HLRS)</title>
		<author>
			<persName><surname>Mpi-Forum</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2012">2012</date>
			<pubPlace>Stuttgart, Germany</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b61">
	<analytic>
		<title level="a" type="main">Algorithm 806: Sprng: a scalable library for pseudorandom number generation</title>
		<author>
			<persName><forename type="first">M</forename><surname>Mascagni</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Srinivasan</surname></persName>
		</author>
		<idno type="DOI">10.1145/358407.358427</idno>
		<ptr target="http://doi.acm.org/10.1145/358407.358427" />
	</analytic>
	<monogr>
		<title level="j">ACM Trans. Math. Softw</title>
		<imprint>
			<biblScope unit="volume">26</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="436" to="461" />
			<date type="published" when="2000">2000</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b62">
	<analytic>
		<title level="a" type="main">A rigorous computational comparison of alternative solution methods for the generalized assignment problem</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">M</forename><surname>Amini</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Racer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Management Science</title>
		<imprint>
			<biblScope unit="volume">40</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="868" to="890" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b63">
	<analytic>
		<title level="a" type="main">Another view on parallel speedup</title>
		<author>
			<persName><forename type="first">X.-H</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">M</forename><surname>Ni</surname></persName>
		</author>
		<ptr target="http://dl.acm.org/citation.cfm?id=110382.110450" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 1990 ACM/IEEE conference on Supercomputing, Supercomputing &apos;90</title>
		<meeting>the 1990 ACM/IEEE conference on Supercomputing, Supercomputing &apos;90<address><addrLine>Los Alamitos, CA, USA</addrLine></address></meeting>
		<imprint>
			<publisher>IEEE Computer Society Press</publisher>
			<date type="published" when="1990">1990</date>
			<biblScope unit="page" from="324" to="333" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b64">
	<analytic>
		<title level="a" type="main">Software challenges in extreme scale systems</title>
		<author>
			<persName><forename type="first">V</forename><surname>Sarkar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Harrod</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">E</forename><surname>Snavely</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Physics: Conference Series</title>
		<imprint>
			<biblScope unit="volume">180</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page">12045</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b65">
	<monogr>
		<ptr target="http://www-01.ibm.com/software/integration/optimization/cplex-optimizer/" />
		<title level="m">Cplex</title>
		<imprint>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b66">
	<analytic>
		<title level="a" type="main">Parallel lan/wan heuristics for optimization</title>
		<author>
			<persName><forename type="first">E</forename><surname>Alba</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Luque</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Troya</surname></persName>
		</author>
		<idno type="DOI">10.1016/j.parco.2003.12.007</idno>
	</analytic>
	<monogr>
		<title level="j">Parallel Computing</title>
		<imprint>
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="issue">56</biblScope>
			<biblScope unit="page" from="611" to="628" />
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b67">
	<analytic>
		<title level="a" type="main">Efficient parallel genetic algorithms: theory and practice</title>
		<author>
			<persName><forename type="first">E</forename><surname>Cantu-Paz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">E</forename><surname>Goldberg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computer methods in applied mechanics and engineering</title>
		<imprint>
			<biblScope unit="volume">186</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="221" to="238" />
			<date type="published" when="2000">2000</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b68">
	<analytic>
		<title level="a" type="main">A survey of very largescale neighborhood search techniques</title>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">K</forename><surname>Ahuja</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Ergun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">B</forename><surname>Orlin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">P</forename><surname>Punnen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Discrete Appl. Math</title>
		<imprint>
			<biblScope unit="volume">123</biblScope>
			<biblScope unit="page" from="75" to="102" />
			<date type="published" when="2002">2002</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b69">
	<monogr>
		<title level="m" type="main">Number of processors Execution time improvement (%) (c) Execution time improvement Figure 6: Weak scaling results. Execution times were measured against different solution quality thresholds. Missing points were the runs exceeding maximum walltime (one hour)</title>
		<idno>1024 2048 3072 4096 5120 6144 7168 8192 9216 10240 11264 12288 13312 14336 15360 16384</idno>
		<imprint/>
	</monogr>
	<note>Number of processors Time in seconds (b) Synchronous migration 1024 2048 4096 8192 16384 0% 10% 20%</note>
</biblStruct>

<biblStruct xml:id="b70">
	<monogr>
		<title level="m" type="main">184000 Number of cores Number of iterations taken (a) Aynchronous migration</title>
		<idno>1024 2048 3072 4096 5120 6144 7168 8192 9216 10240 11264 12288 13312 14336 15360 16384</idno>
		<imprint>
			<date>1,200,000 194000 192000 190000 188000 186500 186000 185500 185000 184500</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
