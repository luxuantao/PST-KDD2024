<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Grassmann Discriminant Analysis: a Unifying View on Subspace-Based Learning</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Jihun</forename><surname>Hamm</surname></persName>
							<affiliation key="aff0">
								<orgName type="laboratory">GRASP Laboratory</orgName>
								<orgName type="institution">University of Pennsylvania</orgName>
								<address>
									<postCode>19104</postCode>
									<settlement>Philadelphia</settlement>
									<region>PA</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author role="corresp">
							<persName><forename type="first">Daniel</forename><forename type="middle">D</forename><surname>Lee</surname></persName>
							<email>ddlee@seas.upenn.edu</email>
							<affiliation key="aff0">
								<orgName type="laboratory">GRASP Laboratory</orgName>
								<orgName type="institution">University of Pennsylvania</orgName>
								<address>
									<postCode>19104</postCode>
									<settlement>Philadelphia</settlement>
									<region>PA</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Grassmann Discriminant Analysis: a Unifying View on Subspace-Based Learning</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.2" ident="GROBID" when="2022-12-25T13:32+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>In this paper we propose a discriminant learning framework for problems in which data consist of linear subspaces instead of vectors. By treating subspaces as basic elements, we can make learning algorithms adapt naturally to the problems with linear invariant structures. We propose a unifying view on the subspace-based learning method by formulating the problems on the Grassmann manifold, which is the set of fixed-dimensional linear subspaces of a Euclidean space. Previous methods on the problem typically adopt an inconsistent strategy: feature extraction is performed in the Euclidean space while non-Euclidean distances are used. In our approach, we treat each subspace as a point in the Grassmann space, and perform feature extraction and classification in the same space. We show feasibility of the approach by using the Grassmann kernel functions such as the Projection kernel and the Binet-Cauchy kernel. Experiments with real image databases show that the proposed method performs well compared with stateof-the-art algorithms.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>We often encounter learning problems in which the basic elements of the data are sets of vectors instead of vectors. Suppose we want to recognize a person from multiple pictures of the individual, taken from different angles, under different illumination or at different places. When comparing such sets of image vectors, we are free to define the similarity between sets based on the similarity between image vectors <ref type="bibr" target="#b18">(Shakhnarovich et al., 2002;</ref><ref type="bibr" target="#b12">Kondor &amp; Jebara, 2003;</ref><ref type="bibr" target="#b26">Zhou &amp; Chellappa, 2006)</ref>.</p><p>In this paper, we specifically focus on those data that can be modeled as a collection of linear subspaces. In the example above, let's assume that the set of images of a single person is well approximated by a low dimensional subspace <ref type="bibr" target="#b20">(Turk &amp; Pentland, 1991)</ref>, and the whole data is the collection of such subspaces. The benefits of using subspaces are two-fold: 1) comparing two subspaces is cheaper than comparing two sets directly when those sets are very large, and 2) it is more robust to missing data since the subspace can 'fill-in' the missing pictures. However the advantages come with the challenge of representing and handling the subspaces appropriately.</p><p>We approach the subspace-based learning problems by formulating the problems on the Grassmann manifold, the set of fixed-dimensional linear subspaces of a Euclidean space. With this unifying framework we can make analytic comparisons of the various distances of subspaces. In particular, we single out those distances that are induced from the Grassmann kernels, which are positive definite kernel functions on the Grassmann space. The Grassmann kernels allow us to use the usual kernel-based algorithms on this unconventional space and to avoid ad hoc approaches to the problem.</p><p>We demonstrate the proposed framework by using the Projection metric and the Binet-Cauchy metric and by applying kernel Linear Discriminant Analysis to classification problems with real image databases.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.1.">Contributions of the Paper</head><p>Although the Projection metric and the Binet-Cauchy metric were previously used <ref type="bibr" target="#b1">(Chang et al., 2006;</ref><ref type="bibr" target="#b23">Wolf &amp; Shashua, 2003)</ref>, their potential for subspace-based learning has not been fully explored. In this work, we provide an analytic exposition of the two metrics as examples of the Grassmann kernels, and contrast the</p><formula xml:id="formula_0">Y i Y j θ 2 G(m, D ) u 1 v 1 θ 1 , ..., θ m span( Y i ) span( Y j ) R D Figure 1.</formula><p>Principal angles and Grassmann distances. Let span(Yi) and span(Yj) be two subspaces in the Euclidean space R D on the left. The distance between two subspaces span(Yi) and span(Yj) can be measured by the principal angles θ = [θ1, ... , θm] using the usual innerproduct of vectors. In the Grassmann manifold viewpoint, the subspaces span(Yi) and span(Yj) are considered as two points on the manifold G(m, D), whose Riemannian distance is related to the principal angles by d(Yi, Yj) = θ 2. Various distances can be defined based on the principal angles.</p><p>two metrics with other metrics used in the literature.</p><p>Several subspace-based classification methods have been previously proposed <ref type="bibr" target="#b25">(Yamaguchi et al., 1998;</ref><ref type="bibr" target="#b15">Sakano, 2000;</ref><ref type="bibr" target="#b5">Fukui &amp; Yamaguchi, 2003;</ref><ref type="bibr" target="#b11">Kim et al., 2007)</ref>. However, these methods adopt an inconsistent strategy: feature extraction is performed in the Euclidean space when non-Euclidean distances are used. This inconsistency can result in complications and weak guarantees. In our approach, the feature extraction and the distance measurement are integrated around the Grassmann kernel, resulting in a simpler and better-understood formulation.</p><p>The rest of the paper is organized as follows. In Sec. 2 and 3 we introduce the Grassmann manifolds and derive various distances on the space. In Sec. 4 we present a kernel view of the problem and emphasize the advantages of using positive definite metrics. In Sec. 5 we propose the Grassmann Discriminant Analysis and compare it with other subspace-based discrimination methods. In Sec. 6 we test the proposed algorithm for face recognition and object categorization tasks. We conclude in Sec. 7 with a discussion.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Grassmann Manifold and Principal Angles</head><p>In this section we briefly review the Grassmann manifold and the principal angles.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Definition 1</head><p>The Grassmann manifold G(m, D) is the set of m-dimensional linear subspaces of the R D .</p><p>The G(m, D) is a m(D − m)-dimensional compact Riemannian manifold. </p><formula xml:id="formula_1">Y 1 R 1 = Y 2 R 2 for some R 1 , R 2 ∈ O(m).</formula><p>With this understanding, we will often use the notation Y when we actually mean its equivalence class span(Y ), and use Y 1 = Y 2 when we mean span(Y 1 ) = span(Y 2 ), for simplicity.</p><p>Formally, the Riemannian distance between two subspaces is the length of the shortest geodesic connecting the two points on the Grassmann manifold. However, there is a more intuitive and computationally efficient way of defining the distances using the principal angles <ref type="bibr" target="#b8">(Golub &amp; Loan, 1996)</ref>.</p><p>Definition 2 Let Y 1 and Y 2 be two orthonormal matrices of size D by m.</p><p>The principal angles 0 ≤ θ 1 ≤ • • • ≤ θ m ≤ π/2 between two subspaces span(Y 1 ) and span(Y 2 ), are defined recursively by</p><formula xml:id="formula_2">cos θ k = max u k ∈span(Y1) max v k ∈span(Y2) u k v k , subject to u k u k = 1, v k v k = 1, u k u i = 0, v k v i = 0, (i = 1, ..., k − 1).</formula><p>In other words, the first principal angle θ 1 is the smallest angle between all pairs of unit vectors in the first and the second subspaces. The rest of the principal O(m) is the group of m by m orthonormal matrices. We refer the readers to <ref type="bibr" target="#b24">(Wong, 1967;</ref><ref type="bibr" target="#b0">Absil et al., 2004)</ref> for details on the Riemannian geometry of the space.</p><p>angles are similarly defined. It is known <ref type="bibr" target="#b24">(Wong, 1967;</ref><ref type="bibr" target="#b3">Edelman et al., 1999)</ref> that the principal angles are related to the geodesic distance by</p><formula xml:id="formula_3">d 2 G (Y 1 , Y 2 ) = i θ 2 i</formula><p>(refer to Fig. <ref type="figure">1</ref>.)</p><p>The principal angles can be computed from the Singular Value Decomposition (SVD) of</p><formula xml:id="formula_4">Y 1 Y 2 , Y 1 Y 2 = U (cos Θ)V ,<label>(1)</label></formula><p>where</p><formula xml:id="formula_5">U = [u 1 ... u m ], V = [v 1 ... v m ]</formula><p>, and cos Θ is the diagonal matrix cos Θ = diag(cos θ 1 ... cos θ m ).</p><p>The cosines of the principal angles cos θ 1 , ... , cos θ m are also known as canonical correlations.</p><p>Although the definition can be extended to the cases where Y 1 and Y 2 have different number of columns, we will assume Y 1 and Y 2 have the same size D by m throughout this paper. Also, we will occasionally use G instead of G(m, D) for simplicity.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Distances for Subspaces</head><p>In this paper we use the term distance as any assignment of nonnegative values for each pair of points in a space X . A valid metric is, however, a distance that satisfies the additional axioms:</p><formula xml:id="formula_6">Definition 3 A real-valued function d : X × X → R is called a metric if 1. d(x 1 , x 2 ) ≥ 0, 2. d(x 1 , x 2 ) = 0 if and only if x 1 = x 2 , 3. d(x 1 , x 2 ) = d(x 2 , x 1 ), 4. d(x 1 , x 2 ) + d(x 2 , x 3 ) ≤ d(x 1 , x 3 ), for all x 1 , x 2 , x 3 ∈ X . A distance (or a metric) between subspaces d(Y 1 , Y 2 ) has to be invariant under different representations d(Y 1 , Y 2 ) = d(Y 1 R 1 , Y 2 R 2 ), ∀R 1 , R 2 ∈ O(m).</formula><p>In this section we introduce various distances for subspaces derivable from the principal angles.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Projection Metric and Binet-Cauchy Metric</head><p>We first underline two main distances of this paper.</p><p>1. Projection metric</p><formula xml:id="formula_7">d P (Y 1 , Y 2 ) = m i=1 sin 2 θ i 1/2 = m − m i=1 cos 2 θ i 1/2 . (<label>2</label></formula><formula xml:id="formula_8">)</formula><p>The Projection metric is the 2-norm of the sine of principal angles <ref type="bibr" target="#b3">(Edelman et al., 1999;</ref><ref type="bibr" target="#b22">Wang et al., 2006)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Binet-Cauchy metric</head><formula xml:id="formula_9">d BC (Y 1 , Y 2 ) = 1 − i cos 2 θ i 1/2 . (<label>3</label></formula><formula xml:id="formula_10">)</formula><p>The Binet-Cauchy metric is defined with the product of canonical correlations <ref type="bibr" target="#b23">(Wolf &amp; Shashua, 2003;</ref><ref type="bibr" target="#b21">Vishwanathan &amp; Smola, 2004)</ref>.</p><p>As the names hint, these two distances are in fact valid metrics satisfying Def. 3. The proofs are deferred until Sec. 4.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Other Distances in the Literature</head><p>We describe a few other distances used in the literature. The principal angles are the keys that relate these distances.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Max Correlation</head><formula xml:id="formula_11">d Max (Y 1 , Y 2 ) = 1 − cos 2 θ 1 1/2 = sin θ 1 . (4)</formula><p>The max correlation is a distance based on only the largest canonical correlation cos θ 1 (or the smallest principal angle θ 1 ). This max correlation was used in previous works <ref type="bibr" target="#b25">(Yamaguchi et al., 1998;</ref><ref type="bibr" target="#b15">Sakano, 2000;</ref><ref type="bibr" target="#b5">Fukui &amp; Yamaguchi, 2003)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Min Correlation</head><formula xml:id="formula_12">d Min (Y 1 , Y 2 ) = 1 − cos 2 θ m 1/2 = sin θ m . (5)</formula><p>The min correlation is defined similarly to the max correlation. However, the min correlation is more closely related to the Projection metric: we can rewrite the Projection metric as</p><formula xml:id="formula_13">d P = 2 −1/2 Y 1 Y 1 − Y 2 Y 2 F and the min correlation as d Min = Y 1 Y 1 − Y 2 Y 2 2 . 3. Procrustes metric d CF (Y 1 , Y 2 ) = 2 m i=1 sin 2 (θ i /2) 1/2 . (6)</formula><p>The Procrustes metric is the minimum distance between different representations of two subspaces span(Y 1 ) and span(Y 2 ): <ref type="bibr" target="#b2">(Chikuse, 2003</ref>)</p><formula xml:id="formula_14">d CF = min R1,R2∈O(m) Y 1 R 1 −Y 2 R 2 F = Y 1 U −Y 2 V F ,</formula><p>where U and V are from (1). By definition, the distance is invariant of the choice of the bases of span(Y 1 ) and span(Y 2 ). The Procrustes metric is also called chordal distance <ref type="bibr" target="#b3">(Edelman et al., 1999)</ref>. We can similarly define the minimum distance using other matrix norms such as</p><formula xml:id="formula_15">d C2 (Y 1 , Y 2 ) = Y 1 U − Y 2 V 2 = 2 sin(θ m /2).</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Which Distance to Use?</head><p>The choice of the best distance for a classification task depends on a few factors. The first factor is the distribution of data. Since the distances are defined with particular combinations of the principal angles, the best distance depends highly on the probability distribution of the principal angles of the given data. For example, d Max uses the smallest principal angle θ 1 only, and may be robust when the data are noisy. On the other hand, when all subspaces are sharply concentrated on one point, d Max will be close to zero for most of the data. In this case, d Min may be more discriminative. The Projection metric d P , which uses all the principal angles, will show intermediate characteristics between the two distances. Similar arguments can be made for the Procrustes metrics d CF and d C2 , which use all angles and the largest angle only, respectively.</p><p>The second criterion for choosing the distance, is the degree of structure in the distance. Without any structure a distance can be used only with a simple K-Nearest Neighbor (K-NN) algorithm for classification.</p><p>When a distance have an extra structure such as triangle inequality, for example, we can speed up the nearest neighbor searches by estimating lower and upper limits of unknown distances <ref type="bibr" target="#b4">(Faragó et al., 1993)</ref>.</p><p>From this point of view, the max correlation is not a metric and may not be used with more sophisticated algorithms. On the other hand, the Min Correlation and the Procrustes metrics are valid metrics 2 .</p><p>The most structured metrics are those which are induced from a positive definite kernel. Among the metrics mentioned so far, only the Projection metric and the Binet-Cauchy metric belong to this class. The proof and the consequences of positive definiteness are the main topics of the next section.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Kernel Functions for Subspaces</head><p>We have defined a valid metric on Grassmann manifolds. The next question is whether we can define a kernel function compatible with the metric. For this purpose let's recall a few definitions. Let X be any</p><formula xml:id="formula_16">2</formula><p>The metric properties follow from the properties of matrix 2-norm and F-norm. To check the conditions in Def. 3 for Procrustes we use the equality minR</p><formula xml:id="formula_17">1 ,R 2 Y1R1− Y2R2 2,F = minR 3 Y1 − Y2R3 2,F for R1, R2, R3 ∈ O(m).</formula><p>set, and k : X × X → R be a symmetric real-valued function k(x i , x j ) = k(x j , x i ) for all x i , x j ∈ X .</p><p>Definition 4 A real symmetric function is a (resp. conditionally) positive definite kernel function, if i,j c i c j k(x i , x j ) ≥ 0, for all x 1 , ..., x n (x i ∈ X ) and c 1 , ..., c n (c i ∈ R) for any n ∈ N. (resp. for all c 1 , ..., c n (c i ∈ R) such that n i=1 c i = 0.)</p><p>In this paper we are interested in the kernel functions on the Grassmann space.</p><p>Definition 5 A Grassmann kernel function is a positive definite kernel function on G.</p><p>In the following we show that the Projection metric and the Binet-Cauchy are induced from the Grassmann kernels.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Projection Metric</head><p>The Projection metric can be understood by associating a point span(Y ) ∈ G with its projection matrix Y Y by an embedding:</p><formula xml:id="formula_18">Ψ P : G(m, D) → R D×D , span(Y ) → Y Y .<label>(7)</label></formula><p>The image Ψ P (G(m, D)) is the set of rank-m orthogonal projection matrices. This map is in fact an isometric embedding <ref type="bibr" target="#b2">(Chikuse, 2003)</ref> and the projection metric is simply a Euclidean distance in R D×D . The corresponding innerproduct of the space is tr</p><formula xml:id="formula_19">[(Y 1 Y 1 )(Y 2 Y 2 )] = Y 1 Y 2 2 F ,</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>and therefore</head><p>Proposition 1 The Projection kernel</p><formula xml:id="formula_20">k P (Y 1 , Y 2 ) = Y 1 Y 2 2 F (8)</formula><p>is a Grassmann kernel.</p><p>Proof The kernel is well-defined because k</p><formula xml:id="formula_21">P (Y 1 , Y 2 ) = k P (Y 1 R 1 , Y 2 R 2 ) for any R 1 , R 2 ∈ O(m).</formula><p>The positive definiteness follows from the properties of the Frobenius norm. For all Y 1 , ..., Y n (Y i ∈ G) and c 1 , ..., c n (c i ∈ R) for any n ∈ N, we have</p><formula xml:id="formula_22">ij c i c j Y i Y j 2 F = ij c i c j tr(Y i Y i Y j Y j ) = tr( i c i Y i Y i ) 2 = i c i Y i Y i 2 F ≥ 0.</formula><p>We can generate a family of kernels from the Projection kernel. For example, the square-root Y i Y j F is also a positive definite kernel.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Binet-Cauchy Metric</head><p>The Binet-Cauchy metric can also be understood from an embedding. Let s be a subset of {1, ..., D} with m elements s = {r 1 , ..., r m }, and Y (s) be the m × m matrix whose rows are the r 1 , ... , r m -th rows of Y . If s 1 , s 2 , ..., s n are all such choices of the subset s ordered lexicographically, then the Binet-Cauchy embedding is defined as</p><formula xml:id="formula_23">Ψ BC : G(m, D) → R n , Y → det Y (s1) , ..., det Y (sn) ,<label>(9)</label></formula><p>where n = D C m is the number of choosing m rows out of D rows. The natural innerproduct in this case is</p><formula xml:id="formula_24">n r=1 det Y (si) 1 det Y (si) 2 . Proposition 2 The Binet-Cauchy kernel k BC (Y 1 , Y 2 ) = (det Y 1 Y 2 ) 2 = det Y 1 Y 2 Y 2 Y 1 (10)</formula><p>is a Grassmann kernel.</p><p>Proof First, the kernel is well-defined because</p><formula xml:id="formula_25">k BC (Y 1 , Y 2 ) = k BC (Y 1 R 1 , Y 2 R 2 ) for any R 1 , R 2 ∈ O(m). To show that k BC is positive definite it suffices to show that k(Y 1 , Y 2 ) = det Y 1 Y 2 is positive definite. From the Binet-Cauchy identity, we have det Y 1 Y 2 = s det Y (s) 1 det Y (s) 2 .</formula><p>Therefore, for all Y 1 , ..., Y n (Y i ∈ G) and c 1 , ..., c n (c i ∈ R) for any n ∈ N, we have</p><formula xml:id="formula_26">ij c i c j det Y i Y j = ij c i c j s det Y (s) i det Y (s) j = s i c i det Y (s) i 2 ≥ 0.</formula><p>We can also generate another family of kernels from the Binet-Cauchy kernel. Note that although det Y 1 Y 2 is a Grassmann kernel we prefer using</p><formula xml:id="formula_27">k BC (Y 1 , Y 2 ) = det(Y 1 Y 2 ) 2 , since it is directly related to principal angles det(Y 1 Y 2 ) 2 = cos 2 θ i , whereas det Y 1 Y 2 = cos θ i in general. 3 Another variant arcsin k BC (Y 1 , Y 2 ) is also a positive definite kernel 4 and its induced metric d = (arccos(det Y 1 Y 2 ))</formula><p>1/2 is a conditionally positive definite metric.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Indefinite Kernels from Other Metrics</head><p>Since the Projection metric and the Binet-Cauchy metric are derived from positive definite kernels, all 3 det Y 1 Y2 can be negative whereas Q cos θi, the product of singular values, is nonnegative by definition.</p><p>4 Theorem 4.18 and 4.19 <ref type="bibr" target="#b17">(Schölkopf &amp; Smola, 2001)</ref>.</p><p>the kernel-based algorithms for Hilbert spaces are at our disposal. In contrast, other metrics in the previous sections are not associated with any Grassmann kernel. To show this we can use the following result <ref type="bibr" target="#b16">(Schoenberg, 1938;</ref><ref type="bibr" target="#b10">Hein et al., 2005)</ref>:</p><formula xml:id="formula_28">Proposition 3 A metric d is induced from a positive definite kernel if and only if k(x 1 , x 2 ) = −d 2 (x 1 , x 2 )/2, x 1 , x 2 ∈ X (11)</formula><p>is conditionally positive definite.</p><p>The proposition allows us to show a metric's nonpositive definiteness by constructing an indefinite kernel matrix from ( <ref type="formula">11</ref>) as a counterexample.</p><p>There have been efforts to use indefinite kernels for learning <ref type="bibr" target="#b14">(Ong et al., 2004;</ref><ref type="bibr" target="#b9">Haasdonk, 2005)</ref>, and several heuristics have been proposed to make an indefinite kernel matrix to a positive definite matrix <ref type="bibr">(Pekalska et al., 2002)</ref>. However, we do not advocate the use of the heuristics since they change the geometry of the original data.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Grassmann Discriminant Analysis</head><p>In this section we give an example of the Discriminant Analysis on Grassmann space by using kernel LDA with the Grassmann kernels.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.">Linear Discriminant Analysis</head><p>The Linear Discriminant Analysis (LDA) <ref type="bibr" target="#b6">(Fukunaga, 1990)</ref>, followed by a K-NN classifier, has been successfully used for classification.</p><p>Let {x 1 , ..., x N } be the data vectors and {y 1 , ..., y N } be the class labels y i ∈ {1, ..., C}. Without loss of generality we assume the data are ordered according to the class labels: 1 = y 1 ≤ y 2 ≤ ... ≤ y N = C. Each class c has N c number of samples.</p><p>Let µ c = 1/N c {i|yi=c} x i be the mean of class c, and µ = 1/N i x i be the overall mean. LDA searches for the discriminant direction w which maximizes the Rayleigh quotient L(w) = w S b w/w S w w where S b and S w are the between-class and within-class covariance matrices respectively:</p><formula xml:id="formula_29">S b = 1 N C c=1 N c (µ c − µ)(µ c − µ) S w = 1 N C c=1 {i|yi=c} (x i − µ c )(x i − µ c )</formula><p>The optimal w is obtained from the largest eigenvector of S −1 w S b . Since S −1 w S b has rank C − 1, there are</p><formula xml:id="formula_30">C − 1-number of local optima W = {w 1 , ..., w C−1 }.</formula><p>By projecting data onto the space spanned by W , we achieve dimensionality reduction and feature extraction of data onto the most discriminant subspace.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.">Kernel LDA with Grassmann Kernels</head><p>Kernel LDA can be formulated by using the kernel trick as follows. Let φ : G → H be the feature map, and Φ = [φ 1 ...φ N ] be the feature matrix of the training points. Assuming w is a linear combination of the those feature vectors, w = Φα, we can rewrite the Rayleigh quotient in terms of α as</p><formula xml:id="formula_31">L(α) = α Φ S B Φα α Φ S W Φα = α K(V − 1 N 1 N /N )Kα α (K(I N − V )K + σ 2 I N )α ,<label>(12)</label></formula><p>where K is the kernel matrix, 1 N is a uniform vector [1 ... 1] of length N , V is a block-diagonal matrix whose c-th block is the uniform matrix 1 Nc 1 Nc /N c , and σ 2 I N is a regularizer for making the computation stable. Similarly to LDA, the set of optimal α's are computed from the eigenvectors.</p><p>The procedures for using kernel LDA with the Grassmann kernels are summarized below:</p><p>Assume the D by m orthonormal bases {Y i } are already computed from the SVD of sets in the data.</p><p>Training:</p><formula xml:id="formula_32">1. Compute the matrix [K train ] ij = k P (Y i , Y j ) or k BC (Y i , Y j ) for all Y i , Y j in the training set.</formula><p>2. Solve max α L(α) by eigen-decomposition.</p><p>3. Compute the (C − 1)-dimensional coefficients F train = α K train .</p><p>Testing:</p><formula xml:id="formula_33">1. Compute the matrix [K test ] ij = k P (Y i , Y j ) or k BC (Y i , Y j ) for all Y i in training set and Y j in the test set. 2. Compute the (C − 1)-dim coefficients F test = α K test .</formula><p>3. Perform 1-NN classification from the Euclidean distance between F train and F test .</p><p>Another way of applying LDA to subspaces is to use the Projection embedding Ψ P (7) or the Binet-Cauchy embedding Ψ BC (9) directly. A subspace is represented by a D by D matrix in the former, or by a vector of length n = D C m in the latter. However, using these embeddings to compute S b or S w is a waste of computation and storage resources when D is large.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3.">Other Subspace-Based Algorithms</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3.1.">Mutual Subspace Method (MSM)</head><p>The original MSM <ref type="bibr" target="#b25">(Yamaguchi et al., 1998)</ref> performs simple 1-NN classification with d Max with no feature extraction. The method can be extended to any distance described in the paper. There are attempts to use kernels for MSM <ref type="bibr" target="#b15">(Sakano, 2000)</ref>. However, the kernel is used only to represent data in the original space, and the algorithm is still a 1-NN classification.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3.2.">Constrained MSM</head><p>Constrained MSM <ref type="bibr" target="#b5">(Fukui &amp; Yamaguchi, 2003)</ref> is a technique that applies dimensionality reduction to bases of the subspaces in the original space. Let G = i Y i Y i be the sum of the projection matrices and {v 1 , ..., v D } be the eigenvectors corresponding to the eigenvalues {λ 1 ≤ ... ≤ λ D } of G. The authors claim that the first few eigenvectors v 1 , ..., v d of G are more discriminative than the later eigenvectors, and they suggest projecting the basis vectors of each subspace Y 1 onto the span(v 1 , ..., v l ), followed by normalization and orthonormalization. However these procedure lack justifications, as well as a clear criterion for choosing the dimension d, on which the result crucially depends from our experience.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3.3.">Discriminant Analysis of Canonical Correlations (DCC)</head><p>DCC <ref type="bibr" target="#b11">(Kim et al., 2007)</ref> can be understood as a nonparametric version of linear discrimination analysis using the Procrustes metric (6). The algorithm finds the discriminating direction w which maximize the ratio L(w) = w S B w/w S w w, where S b and S w are the nonparametric between-class and within-class 'covariance' matrices:</p><formula xml:id="formula_34">S b = i j∈Bi (Y i U − Y j V )(Y i U − Y j V ) S w = i j∈Wi (Y i U − Y j V )(Y i U − Y j V ) ,</formula><p>where U and V are from (1). Recall that tr(</p><formula xml:id="formula_35">Y i U − Y j V )(Y i U − Y j V ) = Y i U − Y j V 2</formula><p>F is the squared Procrustes metric. However, unlike our method, S b and S w do not admit a geometric interpretation as true covariance matrices, and cannot be kernelized either. A main disadvantage of the DCC is that the algorithm iterates the two stages of 1) maximizing the ratio L(w) and of 2) computing S b and S w , which results in computational overheads and more parame-ters to be determined. This reflects the complication of treating the problem in a Euclidean space with a non-Euclidean distance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Experiments</head><p>In this section we test the Grassmann Discriminant Analysis for 1) a face recognition task and 2) an object categorization task with real image databases.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.1.">Algorithms</head><p>We use the following six methods for feature extraction together with an 1-NN classifier.</p><p>1) GDA1 (with Projection kernel), 2) GDA2 (with Binet-Cauchy kernel), 3) Min dist , 4) MSM, 5) cMSM, and 6) DCC.</p><p>For GDA1 and GDA2, the optimal values of σ are found by scanning through a range of values. The results do not seem to vary much as long as σ is small enough.</p><p>The Min dist is a simple pairwise distance which is not subspacebased. If Y i and Y j are two sets of basis vectors: Y i = {y i1 , ..., y imi } and Y j = {y j1 , ..., y jmj }, then d Mindist (Y i , Y j ) = min k,l y ik − y jl 2 . For cMSM and DCC, the optimal dimension l is found by exhaustive searching. For DCC, we have used two nearestneighbors for B i and W i in Sec. 5.3.3. Since the S w and S b are likely to be rank deficient, we first reduced the dimension of the data to N − C using PCA as recommended. Each optimization is iterated 5 times.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.2.">Testing Illumination-Invariance with Yale Face Database</head><p>The Yale face database and the Extended Yale face database <ref type="bibr" target="#b7">(Georghiades et al., 2001)</ref> together consist of pictures of 38 subjects with 9 different poses and 45 different lighting conditions. Face regions were cropped from the original pictures, resized to 24 × 21 pixels (D = 504), and normalized to have the same variance.</p><p>For each subject and each pose, we model the illumination variations by a subspace of the size m = 1, ..., 5, spanned by the 1 to 5 largest eigenvectors from SVD. We evaluate the recognition rate of subjects with ninefold cross validation, holding out one pose of all subjects from the training set and using it for test.</p><p>The recognition rates are shown in Fig. <ref type="figure" target="#fig_0">2</ref>. The GDA1 outperforms the other methods consistently. The GDA2 also performs well for small m, but performs worse as m becomes large. The rates of the others also seem to decrease as m increases. An interpretation of the observation is that the first few eigenvec-tors from the data already have enough information and the smaller eigenvectors are spurious for discriminating the subjects.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.3.">Testing Pose-Invariance with ETH-80 Database</head><p>The ETH-80 <ref type="bibr" target="#b13">(Leibe &amp; Schiele, 2003)</ref> database consists of pictures of 8 object categories <ref type="bibr">('apple', 'pear', 'tomato', 'cow', 'dog', 'horse', 'cup', 'car')</ref>. Each category has 10 objects that belong to the category, and each object is recorded under 41 different poses. Images were resized to 32 × 32 pixels (D = 1024) and normalized to have the same variance. For each category and each object, we model the pose variations by a subspace of the size m = 1, ..., 5, spanned by the 1 to 5 largest eigenvectors from SVD. We evaluate the classification rate of the categories with ten-fold cross validation, holding out one object instance of each category from the training set and using it for test.</p><p>The recognition rates are also summarized in Fig. <ref type="figure" target="#fig_0">2</ref>. The GDA1 also outperforms the other methods most of the time, but the cMSM performs better than GDA2 as m increases. The rates seem to peak around m = 4 and then decrease as m increases. This results is consistent with the observation that the eigenvalues from this database decrease more gradually than the eigenvalues from the Yale face database.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.">Conclusion</head><p>In this paper we have proposed a Grassmann framework for problem in which data consist of subspaces. By using the Projection metric and the Binet-Cauchy metric, which are derived from the Grassmann kernels, we were able to apply kernel methods such as kernel LDA to subspace data. In addition to having theoretically sound grounds, the proposed method also outperformed state-of-the-art methods in two experiments with real data. As a future work, we are pursuing a better understanding of probabilistic distributions on the Grassmann manifold. </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 2 .</head><label>2</label><figDesc>Figure 2. Recognition rates of subjects from Yale face database (Left), and classification rates of categories in ETH-80 database (Right). The bars represent the rates of six algorithms (GDA1, GDA2, Min Dist, MSM, cMSM, DCC) evaluated for m = 1, ...., 5 where m is the number of basis vectors for subspaces. The GDA1 achieves the best rates consistently, and the GDA2 also performs competitively for small m.</figDesc><graphic url="image-1.png" coords="8,55.44,67.06,486.00,106.53" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>of size D by m such that Y Y = I m , where I m is the m by m identity matrix. For example, Y can be the m basis vectors of a set of pictures in R D .</figDesc><table><row><cell>However, the matrix rep-</cell></row><row><cell>resentation of a point in G(m, D) is not unique: two</cell></row><row><cell>matrices Y 1 and Y 2 are considered the same if and only</cell></row><row><cell>if span(Y 1 ) = span(Y 2 ), where span(Y ) denotes the</cell></row><row><cell>subspace spanned by the column vectors of Y . Equiva-</cell></row><row><cell>lently, span(Y 1 ) = span(Y 2 ) if and only if</cell></row></table><note>1 An element of G(m, D) can be represented by an orthonormal matrix Y</note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" xml:id="foot_0">Appearing in Proceedings of the 25 th International Conference on MachineLearning, Helsinki, Finland, 2008. Copyright 2008  by the author(s)/owner(s).</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_1">G(m, D) can be derived as a quotient space of orthogonal groups G(m, D) = O(D)/O(m) × O(D − m), where</note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Riemannian geometry of Grassmann manifolds with a view on algorithmic computation</title>
		<author>
			<persName><forename type="first">P</forename><surname>Absil</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Mahony</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Sepulchre</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Acta Appl. Math</title>
		<imprint>
			<biblScope unit="volume">80</biblScope>
			<biblScope unit="page" from="199" to="220" />
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">Illumination face spaces are idiosyncratic. IPCV</title>
		<author>
			<persName><forename type="first">J.-M</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">R</forename><surname>Beveridge</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">A</forename><surname>Draper</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Kirby</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Kley</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Peterson</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2006">2006</date>
			<biblScope unit="page" from="390" to="396" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<author>
			<persName><forename type="first">Y</forename><surname>Chikuse</surname></persName>
		</author>
		<title level="m">Statistics on special manifolds, lecture notes in statistics</title>
				<meeting><address><addrLine>New York</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2003">2003</date>
			<biblScope unit="volume">174</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">The geometry of algorithms with orthogonality constraints</title>
		<author>
			<persName><forename type="first">A</forename><surname>Edelman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">A</forename><surname>Arias</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">T</forename><surname>Smith</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">SIAM J. Matrix Anal. Appl</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="page" from="303" to="353" />
			<date type="published" when="1999">1999</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Fast nearestneighbor search in dissimilarity spaces</title>
		<author>
			<persName><forename type="first">A</forename><surname>Faragó</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Linder</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Lugosi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Pattern Anal. Mach. Intell</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="page" from="957" to="962" />
			<date type="published" when="1993">1993</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Face recognition using multi-viewpoint patterns for robot vision</title>
		<author>
			<persName><forename type="first">K</forename><surname>Fukui</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Yamaguchi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Int. Symp. of Robotics Res</title>
		<imprint>
			<biblScope unit="page" from="192" to="201" />
			<date type="published" when="2003">2003</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">Introduction to statistical pattern recognition</title>
		<author>
			<persName><forename type="first">K</forename><surname>Fukunaga</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1990">1990</date>
			<publisher>Academic Press Professional, Inc</publisher>
			<pubPlace>San Diego, CA, USA</pubPlace>
		</imprint>
	</monogr>
	<note>2nd ed.</note>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">From few to many: Illumination cone models for face recognition under variable lighting and pose</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">S</forename><surname>Georghiades</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">N</forename><surname>Belhumeur</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">J</forename><surname>Kriegman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Pattern Anal. Mach. Intell</title>
		<imprint>
			<biblScope unit="volume">23</biblScope>
			<biblScope unit="page" from="643" to="660" />
			<date type="published" when="2001">2001</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">H</forename><surname>Golub</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">F V</forename><surname>Loan</surname></persName>
		</author>
		<title level="m">Matrix computations</title>
				<meeting><address><addrLine>Baltimore, MD, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Johns Hopkins University Press</publisher>
			<date type="published" when="1996">1996</date>
		</imprint>
	</monogr>
	<note>3rd ed.</note>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Feature space interpretation of svms with indefinite kernels</title>
		<author>
			<persName><forename type="first">B</forename><surname>Haasdonk</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Pattern Anal. Mach. Intell</title>
		<imprint>
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="page" from="482" to="492" />
			<date type="published" when="2005">2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Maximal margin classification for metric spaces</title>
		<author>
			<persName><forename type="first">M</forename><surname>Hein</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Bousquet</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Schölkopf</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Comput. Syst. Sci</title>
		<imprint>
			<biblScope unit="volume">71</biblScope>
			<biblScope unit="page" from="333" to="359" />
			<date type="published" when="2005">2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Discriminative learning and recognition of image set classes using canonical correlations</title>
		<author>
			<persName><forename type="first">T.-K</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Kittler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Cipolla</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Pattern Anal. Mach. Intell</title>
		<imprint>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="page" from="1005" to="1018" />
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">A kernel between sets of vectors</title>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">I</forename><surname>Kondor</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Jebara</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of the 20th Int. Conf. on Mach. Learn</title>
				<meeting>of the 20th Int. Conf. on Mach. Learn</meeting>
		<imprint>
			<date type="published" when="2003">2003</date>
			<biblScope unit="page" from="361" to="368" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Analyzing appearance and contour based methods for object categorization</title>
		<author>
			<persName><forename type="first">B</forename><surname>Leibe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Schiele</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">CVPR</title>
		<imprint>
			<biblScope unit="page">409</biblScope>
			<date type="published" when="2003">2003</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">A generalized kernel approach to dissimilarity-based classification</title>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">S</forename><surname>Ong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Mary</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Canu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">J</forename><surname>Smola</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of 21st Int. Conf. on Mach. Learn</title>
				<editor>
			<persName><forename type="first">Acm</forename><surname>Pekalska</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">E</forename><surname>Paclik</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">P</forename><surname>Duin</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">R</forename><forename type="middle">P W</forename></persName>
		</editor>
		<meeting>of 21st Int. Conf. on Mach. Learn<address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2002">2004. 2002</date>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="175" to="211" />
		</imprint>
	</monogr>
	<note>Learning with non-positive kernels</note>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Kernel mutual subspace method for robust facial image recognition</title>
		<author>
			<persName><forename type="first">H</forename><surname>Sakano</surname></persName>
			<affiliation>
				<orgName type="collaboration">. and App. Tech.</orgName>
			</affiliation>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Mukawa</surname></persName>
			<affiliation>
				<orgName type="collaboration">. and App. Tech.</orgName>
			</affiliation>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of Int. Conf. on Knowledge-Based Intell. Eng. Sys</title>
				<meeting>of Int. Conf. on Knowledge-Based Intell. Eng. Sys</meeting>
		<imprint>
			<date type="published" when="2000">2000</date>
			<biblScope unit="page" from="245" to="248" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Metric spaces and positive definite functions</title>
		<author>
			<persName><forename type="first">I</forename><forename type="middle">J</forename><surname>Schoenberg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Trans. Amer. Math. Soc</title>
		<imprint>
			<biblScope unit="volume">44</biblScope>
			<biblScope unit="page" from="522" to="536" />
			<date type="published" when="1938">1938</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">Learning with kernels: Support vector machines, regularization, optimization, and beyond</title>
		<author>
			<persName><forename type="first">B</forename><surname>Schölkopf</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">J</forename><surname>Smola</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2001">2001</date>
			<publisher>MIT Press</publisher>
			<pubPlace>Cambridge, MA, USA</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Face recognition from long-term observations</title>
		<author>
			<persName><forename type="first">G</forename><surname>Shakhnarovich</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>John</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Fisher</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of the 7th Euro. Conf. on Computer Vision</title>
				<meeting>of the 7th Euro. Conf. on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2002">2002</date>
			<biblScope unit="page" from="851" to="868" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title/>
		<author>
			<persName><forename type="first">U</forename><forename type="middle">K</forename><surname>London</surname></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Eigenfaces for recognition</title>
		<author>
			<persName><forename type="first">M</forename><surname>Turk</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">P</forename><surname>Pentland</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Cog. Neurosc</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page" from="71" to="86" />
			<date type="published" when="1991">1991</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Binet-cauchy kernels</title>
		<author>
			<persName><forename type="first">S</forename><surname>Vishwanathan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">J</forename><surname>Smola</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of Neural Info. Proc. Sys</title>
				<meeting>of Neural Info. . Sys</meeting>
		<imprint>
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Subspace distance analysis with application to adaptive bayesian algorithm for face recognition</title>
		<author>
			<persName><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Feng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pattern Recogn</title>
		<imprint>
			<biblScope unit="volume">39</biblScope>
			<biblScope unit="page" from="456" to="464" />
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Learning over sets using kernel principal angles</title>
		<author>
			<persName><forename type="first">L</forename><surname>Wolf</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Shashua</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Mach. Learn. Res</title>
		<imprint>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page" from="913" to="931" />
			<date type="published" when="2003">2003</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Differential geometry of Grassmann manifolds</title>
		<author>
			<persName><forename type="first">Y.-C</forename><surname>Wong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Proc. of the Nat. Acad. of Sci</title>
		<imprint>
			<biblScope unit="volume">57</biblScope>
			<biblScope unit="page" from="589" to="594" />
			<date type="published" when="1967">1967</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Face recognition using temporal image sequence</title>
		<author>
			<persName><forename type="first">O</forename><surname>Yamaguchi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Fukui</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Maeda</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of the 3rd. Int. Conf. on Face &amp; Gesture Recognition</title>
				<meeting>of the 3rd. Int. Conf. on Face &amp; Gesture Recognition<address><addrLine>Washington, DC, USA</addrLine></address></meeting>
		<imprint>
			<publisher>IEEE Computer Society</publisher>
			<date type="published" when="1998">1998</date>
			<biblScope unit="page">318</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">From sample similarity to ensemble similarity: Probabilistic distance measures in reproducing kernel hilbert space</title>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">K</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Chellappa</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Pattern Anal. Mach. Intell</title>
		<imprint>
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="page" from="917" to="929" />
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
