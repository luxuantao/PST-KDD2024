<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">DeepCrack: A Deep Hierarchical Feature Learning Architecture for Crack Segmentation</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date type="published" when="2019-01-22">January 22, 2019</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Yahui</forename><surname>Liu</surname></persName>
						</author>
						<author role="corresp">
							<persName><forename type="first">Jian</forename><surname>Yao</surname></persName>
							<email>jian.yao@whu.edu.cn</email>
							<affiliation key="aff1">
								<orgName type="department">Corresponding author</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Xiaohu</forename><surname>Lu</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Renping</forename><surname>Xie</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Li</forename><surname>Li</surname></persName>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="department">School of Remote Sensing and Information Engineering</orgName>
								<orgName type="laboratory">Computer Vision and Remote Sensing (CVRS) Lab</orgName>
								<orgName type="institution">Wuhan University</orgName>
								<address>
									<settlement>Wuhan</settlement>
									<region>Hubei</region>
									<country key="CN">P.R. China</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">DeepCrack: A Deep Hierarchical Feature Learning Architecture for Crack Segmentation</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2019-01-22">January 22, 2019</date>
						</imprint>
					</monogr>
					<idno type="MD5">7C78B613CC2C32CB28C2540019DA5101</idno>
					<idno type="DOI">10.1016/j.neucom.2019.01.036</idno>
					<note type="submission">Received date: 14 December 2017 Revised date: 10 January 2019 Accepted date: 15 January 2019 Preprint submitted to Neurocomputing</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.3" ident="GROBID" when="2023-07-28T14:03+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Convolutional Neural Network</term>
					<term>Crack Detection</term>
					<term>Semantic Segmentation</term>
					<term>Hierarchical Convolutional Features</term>
					<term>Guided Filtering</term>
					<term>Crack Detection Dataset</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Automatic crack detection from images of various scenes is a useful and challenging task in practice. In this paper, we propose a deep hierarchical convolutional neural network (CNN), called as DeepCrack, to predict pixel-wise crack segmentation in an end-to-end method. DeepCrack consists of the extended Fully Convolutional Networks (FCN) and the Deeply-Supervised Nets (DSN). During the training, the elaborately designed model learns and aggregates multi-scale and multi-level features from the low convolutional layers to the high-level convolutional layers, which is different from the standard approaches of only using the last convolutional layer. DSN provides integrated direct supervision for features of each convolutional stage. We apply both guided filtering and Conditional Random Fields (CRFs) methods to refine the final prediction results. A benchmark dataset consisting of 537 images with manual annotation maps are built to verify the effectiveness of our proposed method. Our method achieved state-of-the-art performances on the proposed dataset (mean I/U of 85.9, best F-score of 86.5, and 0.1 seconds per image).</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Crack presents important information for the safety and durability of the manmade buildings. Hence, it is of great significance to detect and analyze cracks for the maintenance of these buildings. In practice, the demand for automatic crack detection has increased rapidly every year. A variety of traditional computer vision methods of In this architecture, there are no fully connected layers, the side-output layers are inserted after the convolutional layers, deep supervision is applied at each side-output layer and then all of them are concatenated to form a final fused output. In this way, the final output layer acquires multi-scale and multi-level features as the plane size of the input of side-output layers becomes smaller and the receptive field size becomes larger. The fused prediction is refined by guided filtering with the first side-output layer.</p><p>crack detection from images have been proposed in the past decades. In general, these methods can be classified into two categories: local-feature-based and global-feature-based methods <ref type="bibr" target="#b2">[1]</ref>. The former methods exploit the local features, such as intensity, gradient, local variance, and local texture anisotropy. The later methods track and extracts crack curves in an overall view through dynamic programming to optimize target functions based on certain criteria.</p><p>Since visually salient cracks are related to a lots of visual patterns, it's difficult to design a universal method to deal with cracks in various scenes. In other words, precisely detecting crack from natural images involves visual perception of various "levels" <ref type="bibr" target="#b3">[2,</ref><ref type="bibr" target="#b4">3]</ref>.</p><p>However, both the mentioned two kinds of crack detection methods can't satisfy this requirement so that they usually suffer a series of problems in practice. Recently, the supervised deep learning methods, such as Convolutional Neural Networks (CNNs), have achieved state-of-the-art performances in many high-level computer vision tasks, such as image recognition <ref type="bibr" target="#b5">[4]</ref>, object detection <ref type="bibr" target="#b6">[5,</ref><ref type="bibr" target="#b7">6,</ref><ref type="bibr" target="#b8">7]</ref>, and semantic segmentation <ref type="bibr" target="#b9">[8,</ref><ref type="bibr" target="#b10">9,</ref><ref type="bibr" target="#b11">10,</ref><ref type="bibr" target="#b12">11,</ref><ref type="bibr" target="#b13">12]</ref>. These are powerful visual models that yield hierarchical features, which is an ideal method to aggregate multiple "levels".</p><p>In the conventional approaches, the main problem in crack detection is the way to deal with noises introduced by stains, spots, uneven illumination, blurring, and multiple scenes. Some methods make an assumption that there is a clear distinction between noises and cracks, such as Iterative Clipping Method (ICM) <ref type="bibr" target="#b14">[13]</ref> which assumed that the intensities along cracks were usually darker than those of its surroundings. Li et al. <ref type="bibr" target="#b15">[14]</ref> presented a Neighboring Difference Histogram Method (NDHM) to segment a crack image</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A C C E P T E D M A N U S C R I P T</head><p>with a globally optimized threshold. However, the above assumption suffer issues when there are many dark spots and shadows in the test image. To improve the correctness and completeness of crack detection, the wavelet transforms based methods <ref type="bibr" target="#b16">[15,</ref><ref type="bibr" target="#b17">16]</ref> were proposed to raise up the crack regions. However, they may not handle well the cracks with high curvatures or bad continuities because of the anisotropic characteristics of wavelets.</p><p>The "FoSA" approach <ref type="bibr" target="#b2">[1]</ref> was proposed to extend the NDHM with a stronger antinoise capability and the "CrackTree" method <ref type="bibr" target="#b18">[17]</ref> was designed to remove the pavement shadows. These methods showed outstanding precision and recall rates, but they are too complex and time-consuming. Some methods took both brightness and connectivity into consideration by measuring the image texture anisotropy, such as Conditional Texture Anisotropy (CTA) <ref type="bibr" target="#b19">[18]</ref> and Free-Form Anisotropy (FFA) <ref type="bibr" target="#b20">[19]</ref>, which showed good results on crack segmentation but were sensitive to the edges which may enhance the noises sometimes. Then, saliency-based methods <ref type="bibr" target="#b21">[20]</ref> were proposed to suppress noises. Hu et al. <ref type="bibr" target="#b22">[21]</ref> used the Local Binary Pattern (LBP) method to analyze the basic local features to get good crack segmentation results, but the parameters need to be adjusted for each image. Zhang et al. <ref type="bibr" target="#b23">[22]</ref> proposed using the Black Top-Hat (BTH) transformation and the threshold segmentation method to detect cracks from the concrete tunnel surface images but their method may encounter problems in uneven illumination images. More recently, Zhang et al. <ref type="bibr" target="#b24">[23]</ref> established a Region of Aggregation (ROA) method to take multi-cues (cracks' spatial distribution, intensities, and geometric features) into account and a Region of Belief (ROB) concept for crack region growing. However, such method was only designed for thin cracks in 2-5 pixels wide.</p><p>Shortcomings of these traditional crack detections are obvious: each method was specifically designed for a specific database or scene. Once the dataset or scene are changed, the crack detectors tend to suffer failure. For example, the FoSA, CrackTree, and FFA methods can work well for thin cracks but fail to deal with wide cracks. The detector sometimes works, but sometimes fails, which means that the extracted features are not generalized well. The CNNs show powerful abilities that yield abundant hierarchical features, which can make breakthroughs in such applications.</p><p>Up till now, there are some attempts of applying CNNs on object segmentation.</p><p>The first strategy utilizes separated mechanisms for feature extraction and image segmentation, in which it treats the CNNs as an assistant tool of the traditional computer vision methods. Some representative examples like <ref type="bibr" target="#b25">[24,</ref><ref type="bibr" target="#b26">25]</ref> applied the CNNs to extract Convolutional Networks (FCN) <ref type="bibr" target="#b10">[9]</ref> learns to upsample its feature maps -the outputs of the convolutional layers -to achieve pixel-wise segmentation, but can't produce very accurate labeling results. FCN showed that different stages of the convolutional layers obtained diverse meaningful features, for example, low layers kept more structure information, while the top layers obtained more abstract features for object recognition. Therefore, coarse feature maps of the top layers are not enough to obtain the refined segmentation results. To overcome it, Zheng et al. <ref type="bibr" target="#b11">[10]</ref> developed an end-to-end network which jointly learned the parameters of the CNN and CRF in a unified architecture termed CRF-RNN. The predictive performance of FCN was improved further by CRF-RNN and finetuning it on large datasets <ref type="bibr" target="#b28">[27]</ref>. The fact that joint training helps was also presented in other recent studies <ref type="bibr" target="#b29">[28,</ref><ref type="bibr" target="#b30">29]</ref>. Meanwhile, the deconvolutional network <ref type="bibr" target="#b31">[30]</ref> and its semi-supervised variant decoupled network <ref type="bibr" target="#b32">[31]</ref> achieved better performances than FCN although at the cost of a more complex training. SegNet <ref type="bibr" target="#b13">[12]</ref> proposed an idea of the encoder-decoder architecture, which discards the fully connected layers and shows the benefit of reducing the number of parameters significantly. Multi-scale deep architectures were also developed, which came into two main directions: (1) inputting images at several scales into the networks <ref type="bibr" target="#b12">[11,</ref><ref type="bibr" target="#b30">29]</ref>; (2) combining feature maps from different layers of a deep architecture <ref type="bibr" target="#b33">[32,</ref><ref type="bibr" target="#b34">33,</ref><ref type="bibr" target="#b35">34]</ref>. Their collective ideas were to learn multi-scale features, in which both local and global features will be kept.</p><p>With the development of deep learning, a few methods that applied CNNs to achieve crack detection has appeared sequentially. Some methods like <ref type="bibr" target="#b36">[35,</ref><ref type="bibr" target="#b37">36,</ref><ref type="bibr" target="#b38">37]</ref> treated CNNs as a classifier to predict a label for each local patch, which was still far from pixelwise segmentation. In addition, both the mentioned traditional methods and CNNsbased methods haven't published an open crack detection dataset, so the performances of these methods are sometimes difficult to make a comparison. As shown in Figure <ref type="figure" target="#fig_0">1</ref>, we explore the meaningful features of each level layer in the following way: (1) predicting crack segmentation results with feature maps of each convolutional stage, termed as side output;</p><p>(2) concatenating all side outputs to produce a final fused result; (3) supervising both side outputs and fused results by Deeply-Supervised Nets (DSN) <ref type="bibr" target="#b39">[38]</ref>, which forms an integrated direct supervision; (4) refining the final fused result by applying Guided Filtering (GF) <ref type="bibr" target="#b40">[39]</ref>. The parallel architecture like HED <ref type="bibr" target="#b4">[3]</ref> was applied to crack detection and achieved the state-of-the-art performance. DeepCrack takes full use of the advanced methods to achieve a deep end-to-end and pixel-wise crack segmentation architecture, which is the core topic of this paper. In addition, we built an open benchmark to evaluate</p><formula xml:id="formula_0">A C C E P T E D M A N U S C R I P T</formula><p>the crack detection systems, in which multi-scale and multi-scene cracks are manually annotated.</p><p>In summary, our proposed crack segmentation method has the following contributions:</p><p>1. We developed an automatic crack segmentation method based on CNNs. It learns hierarchical features of cracks in multiple scenes and scales effectively. Then, both</p><p>CRFs and GF methods are applied to refine the predictions of CNNs.</p><p>2. We explored the learning stage of CNN by using specially designed loss function to alleviate the imbalanced data distribution, in which the negative pixels are far more than the positive ones. To make the training effective, we apply DSN to facilitate the feature learning of each convolutional stage.</p><p>3. We established a public benchmark dataset with cracks in multi-scale and multiscene to evaluate the crack detection systems. All of the crack images in our dataset are manually annotated. To our knowledge, this is the first open work in such field.</p><p>4. We established complete metrics to evaluate crack detection systems, including semantic segmentation evaluations, Precision-Recall curve, and Receiver Operating Characteristic (ROC) curve.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Proposed Method</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.">Model Architecture</head><p>We formulate crack segmentation as a binary image labeling problem, where "0"</p><p>and "1" refer to "non-crack" and "crack", respectively. Such application is a task that requires both high-level features and low-level cues <ref type="bibr" target="#b41">[40]</ref>. Our architecture, as showed in Figure <ref type="figure" target="#fig_0">1</ref>, aggregates hierarchical features acquired from multiple layers. We use the 13 convolutional layers which correspond to the first 13 convolutional layers in the VGG-16</p><p>net <ref type="bibr" target="#b42">[41]</ref> designed for object classification. The fully connected layers and fifth pooling layer are discarded due to the following reasons: 1) we expect the meaningful side-output with different scales, and a layer after the fifth pooling yields a too small output plane (the interpolated prediction feature map is too fuzzy to generate a refined result); 2) the fully connected layers are computationally intensive, which is memory/time-consuming <ref type="bibr" target="#b4">[3]</ref>.</p><p>Each convolutional layer is comprised of convolution, batch normalization <ref type="bibr" target="#b43">[42]</ref> and Rectified Linear Unit (ReLU) <ref type="bibr" target="#b44">[43]</ref>. Here, the convolution is a process with a filter bank to produce a set of feature maps. The batch normalization is applied to reduce internal covariate shift. The ReLU layer computes the activation function max(0, x), which makes the networks able to learn a non-linear task. The spatial pooling is carried out by four</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A C C E P T E D M</head><p>A N U S C R I P T max-pooling layers, which follow some of the convolutional layers in each stage (not all the convolutional layers are followed by plane size reduction operation, respectively conv1 2, conv2 2, conv3 3 and conv4 3). The plane size reduction operation is achieved by a stride 2 block: a Max-pooling with 2×2 pixel filter. It is used to achieve translation invariance over small spatial shifts in the image, which can also reduce the parameter size of the networks. The side-output features are obtained by a convolutional layer with a kernel size 1 and a number of output N . Except the first side-output layer, other four side-output layers are followed by deconvolutional layers, which are applied to upsample the plane size of the feature maps to be the same as the input image. Then, the upsampled feature maps are concatenated to form final features, which are followed by a convolutional layer and a softmax layer. The output of the softmax layer is a N -channel map of the probabilities where N is the number of classes (N = 2 in our application). According to the prediction of the softmax layer, we can get a predicted label for each pixel by a fixed threshold.</p><p>For a same input image, there is a fact that most predictions of lower convolutional stages preserve well the crack region boundaries but are sensitive to local noise, such as dark spots and dirt. On the contrary, predictions of deeper convolutional stages show better anti-noise abilities but fail in preserving the segmentation boundaries. Therefore, it's a trade-off strategy to linearly fuse the predictions of different convolutional stages.</p><p>We explore further and first propose a novel method to refine the fused prediction by applying Guided Filtering <ref type="bibr" target="#b40">[39]</ref>, named as guided feathering. First, a binary mask is generated by the fused prediction. Then, the side-output of conv1 2 is set as a guidance map. The guided filter achieves the final refined prediction by well preserving the crack regions and removing the noises in the low level prediction. Comparing to the CRF method, such technique is faster and more efficient.</p><p>Our architecture consists of main three parts: 1) the convolutional layers which correspond to the first 13 convolutional layers in the VGG-16 net <ref type="bibr" target="#b42">[41]</ref>; 2) the side-output layers; 3) the refinement module.</p><p>Figure <ref type="figure" target="#fig_2">2</ref> shows the main architecture of our proposed model, in which the details on each operations are presented. There are some differences with the VGG-16 networks: 1)</p><p>we insert batch normalization (BN) layer <ref type="bibr" target="#b43">[42]</ref> between the convolutional layer and ReLU layer <ref type="bibr" target="#b44">[43]</ref>, which is applied to improve model generalization; 2) the pool5 layer and fully connected layers are discarded. Hence, it's a fully convolutional network.</p><p>The side-output features are obtained by a convolutional layer with a kernel size 1 and a number of output N (N =2). It can be treated as a linear fusion method and outputs a prediction map. Given the differences of receptive field size, deeper predictions are less affected by noises while lower ones present more detailed boundaries. The final fused result is a trade-off of these side-output predictions, as showed in Figure <ref type="figure" target="#fig_2">2</ref>. We apply two methods to refine the fused prediction: Conditional Random Field (CRF) and Guided Filtering (GF) <ref type="bibr" target="#b40">[39]</ref>. Here, the CRF-based methods is similar to CRF-RNN <ref type="bibr" target="#b11">[10]</ref>. Figure <ref type="figure" target="#fig_3">3</ref> presents the guided filtering process, in which the predictions of fused and side-output 1 are input p and guidance I, respectively. The parameters are r = 5, = 1e -6 for the guided filter.</p><p>Our method has several advantages: 1) applying the whole image to train and generate pixel-wise predictions in an end-to-end manner, which is easy-to-use in practice; 2) the hierarchical feature learning in a network, which has been proved beneficial in some computer tasks <ref type="bibr" target="#b45">[44,</ref><ref type="bibr" target="#b34">33,</ref><ref type="bibr" target="#b12">11]</ref>; 3) refining the fused predictions with an efficient guided filtering; 4) the computation efficiency due to discarding the fully connected layers (reducing the parameters of VGG-16 nets significantly from 134M to 14.7M).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.">Loss Formulation</head><p>Most of the notations and formulations of the proposed method follow those of HED <ref type="bibr" target="#b4">[3]</ref>, but with some differences. </p><p>∈ {0, 1}. For simplicity, we consider each image independently and the index i will be omitted hereafter.</p><p>Each side-output layer during the training stage, as showed in Figure <ref type="figure" target="#fig_5">4</ref>, will be evaluated respectively. The goal of the training is to learn a model that minimizes the differences between the final prediction of the network and the ground truth. To learn meaningful features for crack segmentation, we apply DSN <ref type="bibr" target="#b39">[38]</ref> to supervise each side-output layer.</p><p>Each side-output layer can be treated as a pixel-wise classifier with the corresponding weights w = {(w (1) , ..., w (M ) )}, where M is the number of side-output layers. We denote</p><formula xml:id="formula_2">A C C E P T E D M A N U S C R I P T</formula><p>all the parameters of the network as W, and then the loss function is formulated as:</p><formula xml:id="formula_3">L side (I, G, W, w) = M m=1 α m side (I, G, W, w (m) ) = M m=1 α m ∆(P (m) , G, W, w (m) ),<label>(1)</label></formula><p>where side refers to the image-level loss function for side-output, P = {P j , j = 1, . . . , |I|}, P j ∈ {0, 1} refers to the predicted results of the m-th side-output layer, which is upsampled to the raw image size when necessary, α m is a hyper-parameter denoted as the loss weight for each side-output layer. In our image-to-image training, the modified cross-entropy function ∆ is defined as:</p><formula xml:id="formula_4">∆ = - j∈G + w 0 log Pr(P j = 1|I, W, w) - j∈G - w 1 log Pr(P j = 0|I, W, w),<label>(2)</label></formula><p>where we denote |G|, |G + |, |G -| as the total number of all pixels, all positive pixels and all negative pixels for an input image I, respectively. w 0 and w 1 are the class loss weights for corresponding non-crack and crack pixels, respectively. Pr(•) refers to the probability of positive or negative for a pixel in the predicted map. Let C 0 and C 1 be the total numbers of non-crack (negative) pixels and crack (positive) ones in the total training set, respectively. Given that over 95% of the ground truth are non-crack, the simple cross-entropy loss can cause training difficulties due to the saturation behavior of the activation function. Therefore, we need to weight the loss differently which termed class balancing <ref type="bibr" target="#b13">[12]</ref>. We set w 0 = 1.0 (when pixel j is a negative) and w 1 = C 0 C 1 (when pixel j is a positive) to achieve such goal.</p><p>Each the side-output layer can be applied to generate a prediction map, which contributes to the corresponding side-output loss. The side-output layers are concatenated to a final fused prediction which produces the fused loss termed L fuse as:</p><formula xml:id="formula_5">L fuse (I, G, W) = - j∈G + w 0 log Pr(P j = 1|I, W) - j∈G - w 1 log Pr(P j = 0|I, W),<label>(3)</label></formula><p>where I, G, w 0 and w 1 denote the same meanings with those in Eq. ( <ref type="formula" target="#formula_4">2</ref>). Therefore, our overall loss function becomes: ). The final fused prediction "Fused" also produces the loss termed as L fuse .</p><formula xml:id="formula_6">L = L side (I, G, W, w) + L fuse (I, G, W).<label>(4)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A C C E P T E D M A N U S C R I P T</head><note type="other">Deconv1 Deconv2 Deconv3 Deconv4 Concat</note></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3.">Model Parameters</head><p>We trained our network using the publicly available Caf f e <ref type="bibr" target="#b46">[45]</ref> library and built it on top of the implementations of FCN <ref type="bibr" target="#b10">[9]</ref>, DSN <ref type="bibr" target="#b39">[38]</ref>, HED <ref type="bibr" target="#b4">[3]</ref> and SegNet <ref type="bibr" target="#b13">[12]</ref>. We used the Stochastic Gradient Descent (SGD) <ref type="bibr" target="#b47">[46]</ref> method to optimize our model. The model parameters we tuned are: the size of input image (544×384×3), the size of ground truth (544×384×1), the size of mini-batch (1), the learning rate (1e-4), the loss weight for each side-output layer and the final fused layer (1.0), the momentum (0.9), the weight decay (2e-4), and the training iterations (2e5; reduce learning rate by 1/5 after 5e4). In later experiments, we fixed the values of all parameters discussed above.</p><p>Significantly, our proposed network are efficiently trained without utilizing any pretrained models for main two reasons:</p><p>1. Our task aims to distinguish only two classes (i.e., crack and non-crack), which is easier than general semantic segmentation issues (e.g., 21 classes for PASCAL VOC <ref type="bibr" target="#b48">[47]</ref>). In addition, there are great differences about the semantic categories between PASCAL VOC dataset and our crack detection dataset, which lead to a result that initializing the proposed network with the pre-trained models makes few effects.</p><p>2. The usage of batch normalization, side output supervision and elaborate loss function improve the convergence and accuracy of the proposed network.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.4.">Data Augmentation</head><p>Data augmentation has been proven to be a crucial technique in deep networks <ref type="bibr" target="#b5">[4,</ref><ref type="bibr" target="#b4">3]</ref>.</p><p>We rotated the images to 8 different angles (every 45 • in [0 • , 360 • )) and cropped the largest rectangle in the rotated image (without the blank regions produced by rotation). We also</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A C C E P T E D M A N U S C R I P T</head><p>flipped the images at each angle horizontally. Hence, the dataset were augmented by 16</p><p>times. In the training stage, we applied both the raw images and augmented ones to train networks. We resize the input images at 256×256 because of the rotation transformation, which is different from the training without data augmentation.</p><p>All models were trained and tested on a single NVIDIA TITAN X. For a 544×384 input image, inference time is about 0.1 seconds. So, testing is efficient.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Experiments</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Benchmark</head><p>In previous studies, every method build a specific small evaluation dataset to verify its effectiveness. It makes the comparison of different methods very difficult. Hence, a good crack detection dataset will deal with this problem well. Therefore, we have established an open benchmark database<ref type="foot" target="#foot_0">1</ref> that can provide empirical basis for research on crack detection and segmentation. The dataset is consist of 537 RGB color images with manually annotated segmentations. Some representative images and their corresponding segmentations are shown in Figure <ref type="figure" target="#fig_6">5</ref>, respectively. All of the segmentations were issued by presenting the subject with a binary image. The images were divided into two main subsets: a training set with 300 images and a testing set with 237 ones. Each image is made available to a pixel-wise segmentation map, which presents to be a mask exactly covering the crack regions. All of the images are of a fixed size of 544×384 pixels.</p><p>Table <ref type="table" target="#tab_0">1</ref> displays the percentages of crack pixels and non-crack ones of the database, which accords with the fact that the crack regions only occupy a small proportion of the images. We chose crack images in various scenes and scales to universally represent the characteristics of cracks. Figure <ref type="figure" target="#fig_8">6</ref> shows the statistics for the semantic annotation of the major textures and scenes distribution. For the bare type, it presents as a clean and smooth texture of the background. Therefore, there is a remarkable contrast between crack and non-crack regions. For the rough type, it presents as a cratered or rough surface.</p><p>For the dirty type, there are plenty of spots and stains distributing in the image. The contrast between crack and non-crack regions is lower for the latter two types. Asphalt and concrete, which are commonly used in man-made buildings, are two major scenes of the database. The width of cracks is in ranges from 1 pixel to 180 ones in the database.</p><p>So, multiple textures, scenes and scales make the crack segmentation a challenging task on our built database.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A C C E P T E D M A N U S C R I P T</head><p>Image Ground truth Image Ground truth There is another public benchmark database for asphalt pavement crack detection <ref type="bibr" target="#b49">[48]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>271</head><p>The benchmark<ref type="foot" target="#foot_1">2</ref> contains a few annotated images(less than 40) and is specially designed</p><formula xml:id="formula_7">A C C E P T E D M A N U S C R I P T</formula><p>for thin cracks in 2-5 pixels wide. We realized that the thin cracks with several pixels width and dashed-line shape are different from the wider crack regions. Post-processing, such as length constraint, curvature and geometric features etc., which are broadly applied in traditional methods <ref type="bibr" target="#b49">[48,</ref><ref type="bibr" target="#b23">22]</ref>, is requisite to obtain continuous and complete thin crack segments. However, it's the weakness of deep convolutional neural networks.   </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Metrics</head><p>We performed the major experiments on our established benchmark database. To evaluate our work, we introduce three metrics of common semantic segmentation evaluations <ref type="bibr" target="#b10">[9,</ref><ref type="bibr" target="#b13">12]</ref>. Let n ij be the number of pixels of the class i predicted to be the class j, where there are n cls different classes, and t i = j n ij be the total number of pixels of the class i (both true positives and false positives are included). Then, we compute:</p><p>(1) Global accuracy (G), which measures the percentage of the pixels correctly</p><formula xml:id="formula_8">predicted: i n ii / i t i , A C C E P T E D M A N U S C R I P T</formula><p>(2) Class average accuracy (C), which means the predictive accuracy over all classes:</p><p>(1/n cls ) i n ii /t i ,</p><p>(3) Mean intersection over union (I/U) over all classes: (1/n cls ) i n ii /(t i + j n jin ii ).</p><p>In addition to evaluate the semantic segmentation, three common metrics in the crack detection field are computed as:</p><p>(1) Precision (P) =</p><formula xml:id="formula_9">#TruePositives #TruePositives+#FalsePositives ,<label>(2) Recall</label></formula><formula xml:id="formula_10">(R) = #TruePositives #TruePositives+#FalseNegatives ,<label>(3)</label></formula><formula xml:id="formula_11">F-score (F) = 2P R R+R .</formula><p>Given that P-R metrics take no account of #TrueNegatives, we introduce a classical metric Receiver Operating Characteristic (ROC) curve.The performances of the mentioned methods in our paper are calculated the scores, respectively. For the ROC curve, we calculate three metrics:</p><p>(1) True Positive Rate, TPR =</p><formula xml:id="formula_12">#TruePositives #TruePositives+#FalseNegatives ,<label>(2) False Positive</label></formula><formula xml:id="formula_13">Rate, FPR = #FalsePositives #FalsePositives+#TrueNegatives ,<label>(3)</label></formula><p>The Area Under the ROC Curve, AUC.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Evaluation</head><p>We trained our networks with six strategies: (1) DeepCrack-Basic used the HED <ref type="bibr" target="#b4">[3]</ref> architecture with our loss function and was trained with original 300 training images; (2) DeepCrack-BN is the same as DeepCrack-Basic but adding batch normalization layers before each activation operation; (3) DeepCrack-CRF is the same as DeepCrack-BN but adding CRF after the network; (4) DeepCrack-GF is the same as DeepCrack-BN but applying the refining module of guided filtering, as showed in Figure <ref type="figure" target="#fig_0">1;</ref><ref type="figure"></ref> (5) DeepCrack-CRF-GF linearly combined the prediction of DeepCrack-CRF and DeepCrack-GF, which was formulated as:</p><formula xml:id="formula_14">P = βP CRF + (1 -β)P GF ,<label>(5)</label></formula><p>where P refers to the prediction map and β is the balancing weight which was set as 0.5; (6) DeepCrack-Aug is the same as DeepCrack-BN but trained with the augmented data of 9.6k images. DeepCrack-CRF and DeepCrack-Aug networks were fine-tuned by the trained DeepCrack-BN model. To make the experiments convincing, we compared our method with other four typical methods: (1) AutoCrack <ref type="bibr" target="#b23">[22]</ref>, a traditional artificial designed detector; (2) RoadCNNs <ref type="bibr" target="#b38">[37]</ref>, a latest CNNs-based classifier for patch</p><formula xml:id="formula_15">A C C E P T E D M A N U S C R I P T classification;</formula><p>(3) HED <ref type="bibr" target="#b4">[3]</ref>, an edge detection network achieving the state-of-the-art performances; (4) SegNet <ref type="bibr" target="#b13">[12]</ref>, a latest semantic segmentation network. We fine-tuned the HED and SegNet networks with their original architectures and loss functions on our augmented dataset. We tried to binarize the probability maps with variant global thresholds. Figure <ref type="figure">7</ref> shows the Precision-Recall curve generated by the threshold segmentation method.</p><p>According to it, we can get the statistics, including inference time(Times), the best thresholds (T) and the mentioned metrics (Metrics), as showed in Table <ref type="table" target="#tab_1">2</ref>. Here, we calculated the G, C and I/U by having the same threshold as the best F-score value.</p><p>Several test samples are shown in Figure <ref type="figure" target="#fig_12">9</ref> but the latter is more faster and more efficient in such application. Figure <ref type="figure" target="#fig_14">11</ref> presents the details on the predictions of DeepCrack-BN, DeepCrack-CRF and DeepCrack-GF, in which DeepCrack-GF achieves the sharpest boundaries. DeepCrack-CRF-GF shows a slight improvement in mean I/U and F-score, comparing to DeepCrack-CRF and DeepCrack-GF. DeepCrack-Aug was trained with augmented data and achieves the best F-score, which indicates that it's possible to increase the performances further with more annotated data. Besides, Table <ref type="table" target="#tab_1">2</ref> shows that our proposed optimization method with both GF and CRF can slightly improve the model performances (GF improves about 0.8, CRF improves 0.6, GF-CRF improves 1.2). Compared with only augmenting the training dataset 16 times (Aug improves 1.4), its clear that our proposed refining postprocessing methods are effective. Compared GF and CRF, the former method achieves better performances with lower computation. In addition, we applied both GF and CRF methods to refine the results of DeepCrack-Aug, in which we achieved a slight improvement on the performance (P=0.869, R=0.864 and F=0.867).   Some more test samples in specific scenes, including thin, wide, with stains and blurring, are respectively evaluated and presented in Figure <ref type="figure" target="#fig_3">13</ref>. Our method shows better performances on visual effects in these experiments.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A C C E P T E D M A N U S C R I P T</head><p>As previously mentioned, post-processing, such as length constraint, curvature and geometric features etc., which are broadly applied in traditional methods <ref type="bibr" target="#b49">[48,</ref><ref type="bibr" target="#b23">22]</ref>, is requisite to obtain continuous and complete thin crack segments. However, it's the weakness of deep convolutional neural networks. We randomly choose half annotated images from <ref type="bibr" target="#b49">[48]</ref> to fine-tuning our trained model DeepCrack-BN and obtain the predictions shown in Figure <ref type="figure" target="#fig_5">14</ref>   <ref type="bibr" target="#b23">[22]</ref>, (d) RoadCNNs <ref type="bibr" target="#b38">[37]</ref>, (e) HED <ref type="bibr" target="#b4">[3]</ref>, (f) SegNet <ref type="bibr" target="#b13">[12]</ref>, respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Conclusion</head><p>Crack detection is a viable research. Unlike other detection tasks, segmenting the refined crack regions in pixel-wise is better than predicting bounding-boxes in practice.</p><p>Our work makes contributions to propose a CNN-based learning method for semantic segmentation and establish a challenging benchmark dataset with multi-scene and multiscale cracks.</p><p>We present a deep hierarchical features learning architecture, named DeepCrack, for crack segmentation, which is inspired by an edge detection network <ref type="bibr" target="#b4">[3]</ref>. We keep the  DSN module to provide integrated direct supervision for multi-level features learning, apply batch normalization <ref type="bibr" target="#b43">[42]</ref> to reduce internal covariate shift, and modify the crossentropy loss function for our application, and attempt to refine the predictions, such as conditional random fields and guided filtering. The guided filtering method is faster and more efficient than the conditional random fields. We established an open crack detection dataset to evaluate our method and compared with the up-to-date methods.</p><p>Experimental results demonstrate that our method has achieved comparable performance with the state-of-the-art methods.</p><p>In the future, we will plan to exploit a better strategy to merge the features of sideoutput layers. More images of false crack regions will be added to the current benchmark database, which will make the database more comprehensive. Some more detailed metrics, such as accuracies on specific scenes and scales, will be proposed.  </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Figure1: An illustration of our proposed DeepCrack architecture. In this architecture, there are no fully connected layers, the side-output layers are inserted after the convolutional layers, deep supervision is applied at each side-output layer and then all of them are concatenated to form a final fused output. In this way, the final output layer acquires multi-scale and multi-level features as the plane size of the input of side-output layers becomes smaller and the receptive field size becomes larger. The fused prediction is refined by guided filtering with the first side-output layer.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head></head><label></label><figDesc>meaningful features, used superpixels to represent the structural pattern of the image, and then obtained the final labeling by aggregation of the classifier prediction. However, if there are errors in the initial superpixels, it may lead to poor predictions. The second strategy directly learns a nonlinear model from the images and ground truthA C C E P T E D MA N U S C R I P T label maps. Chen et al.<ref type="bibr" target="#b9">[8,</ref><ref type="bibr" target="#b27">26]</ref> used a Conditional Random Field (CRF) to refine the low-resolution segmentation results obtained from a CNN, which means that they employed the CRF as a post-processing step (separated from the CNN training). Fully</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: The details of our convolutional network.</figDesc><graphic coords="8,296.40,439.74,71.74,50.68" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: The illumination of guided filtering refinement module.</figDesc><graphic coords="9,102.92,236.23,145.55,102.70" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head></head><label></label><figDesc>For crack segmentation, we define the training set by S = {(I n , G n ), n = 1, . . . , N }, where the image sample I n = {I (n) j , j = 1, . . . , |I n |} denotes the original input image and G n = {G (n) j , j = 1, . . . , |G n |}, Y</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 4 :</head><label>4</label><figDesc>Figure 4: An illustration of side-output predictions. The predictions consist of "side-output m" (m = 1, . . . , 5). Each side-output can produce the loss termed as α m ∆(P (m) , G, W, w (m)). The final fused prediction "Fused" also produces the loss termed as L fuse .</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 5 :</head><label>5</label><figDesc>Figure5: Some representative samples in our benchmark database. The images were obtained by main two ways: 1) we downloaded from the Internet; 2) we took some photos of the real cracks personally.</figDesc><graphic coords="13,64.09,484.24,105.47,74.45" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Figure 6 :</head><label>6</label><figDesc>Figure 6: Statistics of textures and scenes distribution of our database.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>- 10 .Figure 7 :</head><label>107</label><figDesc>Figure 7: The Precision-Recall (PR) curve for crack segmentation on our database.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head>Figure 12 presentsFigure 8 :</head><label>128</label><figDesc>Figure12presents the outputs of different convolutional stages of a same sample, which verifies our analysis.The ROC metrics are shown in Figure8. Though there are only small differences in the ROC curve, all of our DeepCrack networks achieves the better performances of AUC&gt;0.98. Compared with the other two methods (HED<ref type="bibr" target="#b4">[3]</ref> and SegNet<ref type="bibr" target="#b13">[12]</ref>), our architecture shows obvious improvements.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_11"><head>- 15 .</head><label>15</label><figDesc>The results demonstrate that our CNNs-based methods can provide the reliable hypothesis for thin cracks with several pixels width and dashed-line shape.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_12"><head>Figure 9 :</head><label>9</label><figDesc>Figure 9: Results on several samples with thin cracks of our Crack Detection Database. In each column we present (a) original images, (b) ground truth, (c) AutoCrack<ref type="bibr" target="#b23">[22]</ref>, (d) RoadCNNs<ref type="bibr" target="#b38">[37]</ref>, (e) HED<ref type="bibr" target="#b4">[3]</ref>, (f) SegNet<ref type="bibr" target="#b13">[12]</ref>, respectively.</figDesc><graphic coords="20,107.30,420.61,87.89,62.04" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_14"><head>Figure 11 :</head><label>11</label><figDesc>Figure 11: An illustration of the different predictions. The first row images are original image(red), fused prediction(blue), CRF refined result(cyan), and guided filtering refined result(green), respectively. The second row are plotting maps of the pixel value along the color lines.</figDesc><graphic coords="22,59.19,178.09,439.85,108.47" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_16"><head>Figure 12 :</head><label>12</label><figDesc>Figure 12: The probability maps generated by DeepCrack-CRF: (a) the test sample; (b) ground truth, (c) CRF refined result, (d) linear fusion map with (e)-(i) (side-output 1-5), respectively. It implies that learned features are distinct in each stage. More detailed local features are retained in the lower level layers. Meanwhile more abstract features are represented in the deeper layers. The fused result and refined prediction show better performances.</figDesc><graphic coords="23,78.25,316.66,131.84,93.06" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 :</head><label>1</label><figDesc>The percentages of crack pixels and non-crack ones.</figDesc><table><row><cell></cell><cell cols="2">Crack pixels (%) Non-crack pixels (%)</cell></row><row><cell>Training</cell><cell>2.91</cell><cell>97.09</cell></row><row><cell>Test</cell><cell>4.33</cell><cell>95.67</cell></row><row><cell>Total</cell><cell>3.54</cell><cell>96.46</cell></row><row><cell></cell><cell>Dirty(22.4%)</cell><cell>Asphalt(22.0%)</cell></row><row><cell>Bare(37.6%)</cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell>Concrete(78.0%)</cell></row><row><cell></cell><cell>Rough(40.0%)</cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 :</head><label>2</label><figDesc>Comparison of different methods on our testing database.</figDesc><table><row><cell>Methods</cell><cell>Times</cell><cell>T</cell><cell></cell><cell></cell><cell cols="2">Metrics</cell></row><row><cell></cell><cell>(ms)</cell><cell></cell><cell>G</cell><cell>C</cell><cell>I/U</cell><cell>P</cell><cell>R</cell><cell>F</cell></row><row><cell>AutoCrack [22]</cell><cell>-</cell><cell>-</cell><cell cols="5">97.0 70.9 67.3 77.2 42.4 54.8</cell></row><row><cell>RoadCNNs [37]</cell><cell>-</cell><cell>-</cell><cell cols="5">86.9 86.5 54.2 22.9 86.0 36.1</cell></row><row><cell>HED [3]</cell><cell>56</cell><cell cols="6">0.70 95.8 87.7 70.0 59.4 69.1 63.9</cell></row><row><cell>SegNet [12]</cell><cell>184</cell><cell cols="6">0.91 98.0 82.3 78.5 79.7 72.9 76.2</cell></row><row><cell>DeepCrack-Basic</cell><cell>74</cell><cell cols="6">0.86 97.8 92.5 80.1 79.4 79.9 79.6</cell></row><row><cell>DeepCrack-BN</cell><cell>109</cell><cell cols="6">0.87 97.8 96.6 81.3 83.9 86.3 85.1</cell></row><row><cell>DeepCrack-CRF</cell><cell cols="7">400+ 0.90 98.2 95.4 83.6 86.8 84.6 85.7</cell></row><row><cell>DeepCrack-GF</cell><cell>118</cell><cell cols="6">0.75 98.6 95.0 85.9 85.2 86.6 85.9</cell></row><row><cell cols="8">DeepCrack-CRF-GF 400+ 0.80 98.5 95.4 85.4 86.6 85.9 86.3</cell></row><row><cell>DeepCrack-Aug</cell><cell>109</cell><cell cols="6">0.86 97.5 97.0 80.2 86.1 86.9 86.5</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 3 :</head><label>3</label><figDesc>Results of each side-output and fused-output on our testing dataset in DeepCrack-GF.</figDesc><table><row><cell>Outputs</cell><cell></cell><cell></cell><cell cols="2">Metrics</cell><cell></cell></row><row><cell></cell><cell>G</cell><cell>C</cell><cell>I/U</cell><cell>P</cell><cell>R</cell><cell>F</cell></row><row><cell cols="7">Side-output 1 95.9 95.7 73.0 76.7 84.6 80.5</cell></row><row><cell cols="7">Side-output 2 97.1 95.8 77.7 77.9 87.1 82.3</cell></row><row><cell cols="7">Side-output 3 97.7 96.2 80.5 75.2 91.0 82.3</cell></row><row><cell cols="7">Side-output 4 97.1 96.2 77.9 75.1 87.4 80.7</cell></row><row><cell cols="7">Side-output 5 96.8 95.4 76.4 69.1 86.3 76.7</cell></row><row><cell>Fused results</cell><cell cols="6">97.8 96.6 81.3 83.9 86.3 85.1</cell></row><row><cell cols="7">Refined results 98.6 95.0 85.9 85.2 86.6 85.9</cell></row></table><note><p><p>Note:</p>The fused results achieve best global accuracy, mean I/U, precision and F-score.</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 3</head><label>3</label><figDesc></figDesc><table /><note><p>displays the statistics of different level convolutional layers, including each side-output and the fused-output. Both two types of metrics present two similar tendencies: (1) all metrics are increasing gradually from the low level layers to the middle level ones first, then it turns into decreasing from the middle level layers to the high level ones; (2) both the simplest fusion (linear fusion) and guided filtering refinement improve the performances. It's logical that the low level layers represent more local features with a smaller receptive field. More non-crack pixels were predicted to be crack pixels, termed false positives, and lesser crack pixels were predicted to be non-crack pixels, termed false negatives, in such low level layers. It indicates that the low level features are susceptible to noise, such as spots, stains and other objects similar to cracks. With the layers becoming deeper, the meaningful local features learned from bigger receptive fields are more abstract, which is good to decrease false positives but is inferior to increase false negatives. Therefore, higher level features show more anti-noise capabilities. Fused hierarchical features can be treated as neutralization which aggregates the multiple level features from coarse to fine. Therefore, fused results show better integrated performances.</p></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0"><p>Available at: https://github.com/yhlleo/DeepCrack</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_1"><p>Dataset: http://perso.lcpc.fr/sylvie.chambon/FISSURES/Datasets.html</p></note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgments</head><p>This work was partially supported by the National Natural Science Foundation of China (Project No. 41571436), the Hubei Province Science and Technology Support Program, China (Project No. 2015BAA027), the National Natural Science Foundation of China under Grant 91438203, LIESMARS Special Research Funding, and the South Wisdom Valley Innovative Research Team Program.</p></div>
			</div>

			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A C C E P T E D</head></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">DeepCrack-Basic [F=.851]DeepCrack-BN</title>
		<idno>F=.857]DeepCrack-CRF [F=.859]DeepCrack-GF [F=.863]DeepCrack-CRF-GF [F=.865]DeepCrack-Aug [F=.808]Side-output 1 [F=.829]Side-output 2 [F=.838]Side-output 3 [F=.809]Side-output 4 [F=.770]Side-output 5</idno>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title/>
		<author>
			<persName><surname>References</surname></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">FoSA: F* seed-growing approach for crack-line detection from pavement images</title>
		<author>
			<persName><forename type="first">Q</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Zou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Mao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Image and Vision Computing</title>
		<imprint>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page" from="861" to="872" />
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Receptive fields, binocular interaction and functional architecture in the cat&apos;s visual cortex</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">H</forename><surname>Hubel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">N</forename><surname>Wiesel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The Journal of physiology</title>
		<imprint>
			<biblScope unit="volume">160</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="106" to="154" />
			<date type="published" when="1962">1962</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Holistically-nested edge detection</title>
		<author>
			<persName><forename type="first">S</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Tu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision (ICCV)</title>
		<meeting>the IEEE International Conference on Computer Vision (ICCV)</meeting>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="1395" to="1403" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Imagenet classification with deep convolutional neural networks</title>
		<author>
			<persName><forename type="first">A</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems (NIPS)</title>
		<imprint>
			<date type="published" when="2012">2012</date>
			<biblScope unit="page" from="1097" to="1105" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Rich feature hierarchies for accurate object detection and semantic segmentation</title>
		<author>
			<persName><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Donahue</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="580" to="587" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<author>
			<persName><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision (ICCV)</title>
		<meeting>the IEEE International Conference on Computer Vision (ICCV)</meeting>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="1440" to="1448" />
		</imprint>
	</monogr>
	<note>Fast r-cnn</note>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Faster r-cnn: Towards real-time object detection with region proposal networks</title>
		<author>
			<persName><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems (NIPS)</title>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="91" to="99" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<author>
			<persName><forename type="first">L.-C</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Papandreou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Kokkinos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Murphy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">L</forename><surname>Yuille</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1412.7062</idno>
		<title level="m">Semantic image segmentation with deep convolutional nets and fully connected crfs</title>
		<imprint/>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Fully convolutional networks for semantic segmentation</title>
		<author>
			<persName><forename type="first">J</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Shelhamer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="3431" to="3440" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<author>
			<persName><forename type="first">S</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Jayasumana</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Romera-Paredes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Vineet</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">H</forename><surname>Torr</surname></persName>
		</author>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision (ICCV)</title>
		<meeting>the IEEE International Conference on Computer Vision (ICCV)</meeting>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="1529" to="1537" />
		</imprint>
	</monogr>
	<note>Conditional random fields as recurrent neural networks</note>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Predicting depth, surface normals and semantic labels with a common multi-scale convolutional architecture</title>
		<author>
			<persName><forename type="first">D</forename><surname>Eigen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Fergus</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision (ICCV)</title>
		<meeting>the IEEE International Conference on Computer Vision (ICCV)</meeting>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="2650" to="2658" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<author>
			<persName><forename type="first">V</forename><surname>Badrinarayanan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Handa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Cipolla</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1505.07293</idno>
		<title level="m">Segnet: A deep convolutional encoder-decoder architecture for robust semantic pixel-wise labelling</title>
		<imprint/>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Segmentation algorithm using iterative clipping for processing noisy pavement images</title>
		<author>
			<persName><forename type="first">H</forename><surname>Oh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><forename type="middle">W</forename><surname>Garrick</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">E</forename><surname>Achenie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference Imaging Technologies: Techniques and Applications in Civil Engineering</title>
		<imprint>
			<date type="published" when="1998">1998</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Novel approach to pavement image segmentation based on neighboring difference histogram method</title>
		<author>
			<persName><forename type="first">Q</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Congress on Image and Signal Processing</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="792" to="796" />
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Wavelet-based pavement distress detection and evaluation</title>
		<author>
			<persName><forename type="first">J</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">S</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F.-P</forename><surname>Chiang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Optical Engineering</title>
		<imprint>
			<biblScope unit="volume">45</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="27007" to="027007" />
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">A segment algorithm for crack dection</title>
		<author>
			<persName><forename type="first">S</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Symposium on Electrical &amp; Electronics Engineering</title>
		<imprint>
			<date type="published" when="2012">2012</date>
			<biblScope unit="page" from="674" to="677" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Automatic crack detection from pavement images</title>
		<author>
			<persName><forename type="first">Q</forename><surname>Zou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Mao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Cracktree</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pattern Recognition Letters</title>
		<imprint>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="227" to="238" />
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Measure of texture anisotropy for crack detection on textured surfaces</title>
		<author>
			<persName><forename type="first">F</forename><surname>Roli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Electronics Letters</title>
		<imprint>
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="issue">14</biblScope>
			<biblScope unit="page" from="1274" to="1275" />
			<date type="published" when="1996">1996</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Free-form anisotropy: A new method for crack detection on pavement surface images</title>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">S</forename><surname>Nguyen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Begot</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Duculty</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Avila</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Image Processing</title>
		<imprint>
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Pavement crack detection based on saliency and statistical features</title>
		<author>
			<persName><forename type="first">W</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Ding</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Image Processing</title>
		<imprint>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">A local binary pattern based methods for pavement crack detection</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C.-X</forename><surname>Zhao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Pattern Recognition Research</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="140" to="147" />
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Automatic crack detection and classification method for subway tunnel safety monitoring</title>
		<author>
			<persName><forename type="first">W</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Sensors</title>
		<imprint>
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page" from="19307" to="19328" />
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">An efficient and reliable coarse-to-fine approach for asphalt pavement crack detection</title>
		<author>
			<persName><forename type="first">D</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Image and Vision Computing</title>
		<imprint>
			<biblScope unit="volume">57</biblScope>
			<biblScope unit="page" from="130" to="146" />
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Learning hierarchical features for scene labeling</title>
		<author>
			<persName><forename type="first">C</forename><surname>Farabet</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Couprie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Najman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence (TPAMI)</title>
		<imprint>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="1915" to="1929" />
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<author>
			<persName><forename type="first">C</forename><surname>Couprie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Farabet</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Najman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1301.3572</idno>
		<title level="m">Indoor semantic segmentation using depth information</title>
		<imprint/>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<title level="m" type="main">Weakly-and semi-supervised learning of a dcnn for semantic image segmentation</title>
		<author>
			<persName><forename type="first">G</forename><surname>Papandreou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L.-C</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Murphy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">L</forename><surname>Yuille</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1502.02734</idno>
		<imprint/>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">The pascal visual object classes challenge: A retrospective</title>
		<author>
			<persName><forename type="first">M</forename><surname>Everingham</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">A</forename><surname>Eslami</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Van Gool</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">K</forename><surname>Williams</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Winn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Computer Vision (IJCV)</title>
		<imprint>
			<biblScope unit="volume">111</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="98" to="136" />
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">G</forename><surname>Schwing</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Urtasun</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1503.02351</idno>
		<title level="m">Fully connected deep structured networks</title>
		<imprint/>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
		<title level="m" type="main">Efficient piecewise training of deep structured models for semantic segmentation</title>
		<author>
			<persName><forename type="first">G</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Reid</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1504.01013</idno>
		<imprint/>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Learning deconvolution network for semantic segmentation</title>
		<author>
			<persName><forename type="first">H</forename><surname>Noh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Hong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Han</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision (ICCV)</title>
		<meeting>the IEEE International Conference on Computer Vision (ICCV)</meeting>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="1520" to="1528" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Decoupled deep neural network for semi-supervised semantic segmentation</title>
		<author>
			<persName><forename type="first">S</forename><surname>Hong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Noh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Han</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems (NIPS)</title>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="1495" to="1503" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
		<author>
			<persName><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Rabinovich</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">C</forename><surname>Berg</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1506.04579</idno>
		<title level="m">Parsenet: Looking wider to see better</title>
		<imprint/>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Hypercolumns for object segmentation and fine-grained localization</title>
		<author>
			<persName><forename type="first">B</forename><surname>Hariharan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Arbeláez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="447" to="456" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<monogr>
		<title level="m" type="main">Towards accurate region proposal generation and joint object detection</title>
		<author>
			<persName><forename type="first">T</forename><surname>Kong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><surname>Hypernet</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1604.00600</idno>
		<imprint/>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Convolutional neural networks for steel surface defect detection from photometric stereo images</title>
		<author>
			<persName><forename type="first">D</forename><surname>Soukup</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Huber-Mörk</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Symposium on Visual Computing</title>
		<imprint>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="668" to="677" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<monogr>
		<author>
			<persName><forename type="first">X</forename><surname>Gibert</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><forename type="middle">M</forename><surname>Patel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Chellappa</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1509.05267</idno>
		<title level="m">Deep multi-task learning for railway track inspection</title>
		<imprint/>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Road crack detection using deep convolutional neural network</title>
		<author>
			<persName><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><forename type="middle">D</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><forename type="middle">J</forename><surname>Zhu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Image Processing</title>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="3708" to="3712" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Deeply-supervised nets</title>
		<author>
			<persName><forename type="first">C.-Y</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Gallagher</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Tu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">AISTATS</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">6</biblScope>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Guided image filtering</title>
		<author>
			<persName><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="1397" to="1409" />
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<monogr>
		<author>
			<persName><forename type="first">J</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Price</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Cohen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M.-H</forename><surname>Yang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1603.04530</idno>
		<title level="m">Object contour detection with a fully convolutional encoder-decoder network</title>
		<imprint/>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b42">
	<monogr>
		<author>
			<persName><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1409.1556</idno>
		<title level="m">Very deep convolutional networks for large-scale image recognition</title>
		<imprint/>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b43">
	<monogr>
		<author>
			<persName><forename type="first">S</forename><surname>Ioffe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1502.03167</idno>
		<title level="m">Batch normalization: Accelerating deep network training by reducing internal covariate shift</title>
		<imprint/>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Rectified linear units improve restricted boltzmann machines</title>
		<author>
			<persName><forename type="first">V</forename><surname>Nair</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of International Conference on Machine Learning (ICML)</title>
		<meeting>International Conference on Machine Learning (ICML)</meeting>
		<imprint>
			<date type="published" when="2010">2010</date>
			<biblScope unit="page" from="807" to="814" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Spatial pyramid pooling in deep convolutional networks for visual recognition</title>
		<author>
			<persName><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision (ECCV)</title>
		<imprint>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="346" to="361" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Caffe: Convolutional architecture for fast feature embedding</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Shelhamer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Donahue</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Karayev</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Guadarrama</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 22nd ACM international conference on Multimedia</title>
		<meeting>the 22nd ACM international conference on Multimedia</meeting>
		<imprint>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="675" to="678" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Large-scale machine learning with stochastic gradient descent</title>
		<author>
			<persName><forename type="first">L</forename><surname>Bottou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of COMPSTAT</title>
		<meeting>COMPSTAT</meeting>
		<imprint>
			<date type="published" when="2010">2010</date>
			<biblScope unit="page" from="177" to="186" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">The pascal visual object classes (voc) challenge</title>
		<author>
			<persName><forename type="first">M</forename><surname>Everingham</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Van Gool</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">K</forename><surname>Williams</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Winn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International journal of computer vision</title>
		<imprint>
			<biblScope unit="volume">88</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="303" to="338" />
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Automatic road pavement assessment with image processing: Review and comparison</title>
		<author>
			<persName><forename type="first">S</forename><surname>Chambon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J.-M</forename><surname>Moliard</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Geophysics</title>
		<imprint>
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
