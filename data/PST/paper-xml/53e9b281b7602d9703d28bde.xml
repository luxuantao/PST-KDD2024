<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Fast Tracking of Hands and Fingertips in Infrared Images for Augmented Desk Interface</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Yoichi</forename><surname>Sato</surname></persName>
							<email>ysato@cvl.iis.u-tokyo.ac.jp</email>
							<affiliation key="aff0">
								<orgName type="department">Institute of Industrial Science</orgName>
								<address>
									<addrLine>University o f T okyo 7-22-1 Roppongi, Minato-ku</addrLine>
									<postCode>106-8558</postCode>
									<settlement>Tokyo</settlement>
									<country key="JP">Japan</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Yoshinori</forename><surname>Kobayashi</surname></persName>
							<email>yosinori@vogue.is.uec.ac.jp</email>
							<affiliation key="aff1">
								<orgName type="department">Graduate School of Information Systems</orgName>
								<orgName type="institution">University of Electro</orgName>
								<address>
									<addrLine>Communications 1-5-1 Chofugaoka</addrLine>
									<postCode>182-8585</postCode>
									<settlement>Chofu Tokyo</settlement>
									<country key="JP">Japan</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Hideki</forename><surname>Koike</surname></persName>
							<email>koike@vogue.is.uec.ac.jp</email>
							<affiliation key="aff1">
								<orgName type="department">Graduate School of Information Systems</orgName>
								<orgName type="institution">University of Electro</orgName>
								<address>
									<addrLine>Communications 1-5-1 Chofugaoka</addrLine>
									<postCode>182-8585</postCode>
									<settlement>Chofu Tokyo</settlement>
									<country key="JP">Japan</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Fast Tracking of Hands and Fingertips in Infrared Images for Augmented Desk Interface</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">F1CCCB7E995F5F079E2D0C261C4E2D31</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.3" ident="GROBID" when="2023-07-28T15:05+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>In this paper, we introduce a fast and robust method for tracking positions of the centers and the ngertips of both right and left hands. Our method makes use of infrared c amera images for reliable detection of user's hands, and uses template matching strategy for nding ngertips. This method is an essential part of our augmented desk interface in which a user can, with natural hand gestures, simultaneously manipulate both physical objects and electronically projected objects on a desk, e.g., a textbook and related WWW pages. Previous tracking methods which are typically based o n c olor segmentation or background subtraction simply do not perform well in this type of application because an observed c olor of human skin and image backgrounds may change signicantly due to projection of various objects onto a desk. In contrast, our proposed method was shown to be eective even in such a challenging situation through demonstration in our augmented desk interface. This paper describes the details of our tracking method as well as typical applications in our augmented desk interface.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction One of the important challenges in Computer Human</head><p>Interactions is to develop more natural and more intuitive interfaces. Graphical user interface (GUI), which is a current standard interface on personal computers (PCs), is well-matured, and it provides an ecient interface for a user to use various applications on a computer. However, many users nd that the capability of GUI is rather limited when they try to do some tasks by using both physical documents on a desk and computer applications. This limitation comes from the lack of seamless integration between two dierent types of interface. One is interface for using physical objects such as books on a desk. The other is GUI for using computer programs. As a result, users have to keep switching their focus of attention between physical objects on a desk and GUI on a computer monitor.</p><p>One of the earliest attempts to provide seamless integration between those two types of interface, i.e., interface for using physical objects and GUI for using computer programs, was reported in Wellner's DigitalDesk <ref type="bibr" target="#b12">[13]</ref>. In this work, the use of a desk equipped with a CCD camera and a video projector was introduced.</p><p>Inspired by this DigitalDesk, we have proposed an augmented desk interface in our previous work <ref type="bibr" target="#b3">[4]</ref> in order to allow a user to perform various tasks by manipulating both physical objects and electronically displayed objects simultaneously on a desk. In basic demonstrations, our augmented interface system was shown to be able to provide intuitive i n terface for using physical objects and computer programs simultaneously. Unfortunately, however, applications of the proposed system were limited to rather simple ones mainly due to a limited capability of monitoring user's movements, e.g., hand gestures, in a non-trivial environment in real-time. Therefore, a user was allowed to use only a limited range of hand gestures on an uncluttered desk.</p><p>In this paper, we introduce a new method for tracking a user's hands and ngertips reliably at video-frame rate. Our method makes use of infrared camera images for reliable detection of user's hands, and uses template matching strategy for nding ngertips accurately. The use of an infrared camera is especially advantageous for our augmented desk interface system where observed color of human skin and image background change signicantly due to projection onto a desk. In contrast, previous methods for nding hand and ngertips are typically based on color segmentation or background subtraction, and therefore those methods would have diculties in such challenging situations. This paper is organized as follows. In Section 2, we describe the previously proposed methods for tracking human hands and ngertips; we also include the limitations of these methods. In Section 3, we explain our method for fast and robust tracking of a user's hand location and ngertips by using an infrared camera. In Section 4, we show examples of tracking results by our proposed method. In Section 5, we describe our augmented desk interface system which is based on our hand and ngertip tracking method. Finally, in Section 6, we present our conclusions and discussion for future research directions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related works</head><p>In this section, we give a brief overview of the previously proposed methods for tracking a human hand and its ngertips, and examine the limitations of these methods.</p><p>The use of glove-based devices for measuring the location and shape of a user's hand has been widely studied in the past, especially in the eld of virtual reality. Angles of nger joints are measured by some sort of sensor, typically mechanical or optical. The position of a hand is determined by an additional sensor. One of the most widely known examples of such devices is DataGlove b y VPL Research <ref type="bibr" target="#b13">[14]</ref> which uses optical ber technology for exion detection and a magnetic sensor for hand position tracking. A good survey of glove-based devices can be found in <ref type="bibr" target="#b11">[12]</ref>.</p><p>In general, glove-based devices can measure hand postures and locations with high accuracy and high speed. However, the use of glove-based devices is not suitable for some types of applications such as human computer interfaces because those devices may limit a user's motion due to the physical connection to their controllers.</p><p>For this reason, a number of methods based on computer vision techniques have been studied by other researchers in the past. One approach is to use some kind of markers attached to a user's hand or ngertips, so that those points can be easily found. For instance, color markers attached to the ngertips are used in the method reported in <ref type="bibr" target="#b0">[1]</ref> to identify locations of ngertips in input images. Maggioni <ref type="bibr" target="#b4">[5]</ref> presented the use of a specially marked glove for hand tracking. The glove has two slightly o-centered, dierently colored circular regions. By identifying those two circles with a single camera, the system can estimate hand position and orientation.</p><p>In another approach, image regions corresponding to human skin are extracted typically either by color segmentation or by background image subtraction. The main challenge of this approach is how to identify the image regions in input images. Since the color of human skin is not completely uniform and changes from person to person, the methods based on color segmentation often produce unreliable segmentation of human skin regions. To a void this problem, some methods requires a user to wear a glove of a uniform color. On the other hand, the methods based on background image subtraction have diculties when applied to images with a complex background.</p><p>After the images regions are identied in input images, the regions are analyzed to estimate the location and orientation of a hand, or to estimate locations of ngertips. For instance, in the method by Maggioni etal. <ref type="bibr" target="#b5">[6]</ref>, a shape of the contour of an extracted hand region is used for determining locations of ngertips. Segen and Kummar <ref type="bibr" target="#b10">[11]</ref> introduced a method which ts a line segment to a hand region contour to locate the side of an extended nger.</p><p>All of the methods based on extraction of hand regions face a common diculty when they are applied in the situation assumed in our application, i.e., augmented desk interface. Since our augmented desk interface system can project various objects such as texts or gures with different colors onto a user's hand on the desk, hand regions cannot be identied by color segmentation or background segmentation.</p><p>Another approach used in hand gesture analysis is to use a three dimensional model of a human hand. In this approach, in order to determine the posture of the hand model, the model is matched to a user's hand images which have been obtained by using one or more cameras. The method proposed by Rehg and Kanade <ref type="bibr" target="#b8">[9]</ref> is one example based on this approach. Unlike other methods which do not use a three dimensional hand model, the method proposed by Rehg and Kanade can estimate three dimensional posture of a user's hand.</p><p>However, this approach faces several diculties such as self-occlusion of a hand or high computational cost for es-timation of hand posture. Due to the high degrees of freedom of the hand model, it is very dicult to estimate the hand conguration from a two dimensional image even if images are obtained from multiple viewpoints.</p><p>In addition to the methods mentioned in this section, a large number of methods were proposed in the past. A good survey of hand tracking methods as well as algorithms for hand gesture analysis can be found in <ref type="bibr" target="#b2">[3]</ref> and <ref type="bibr" target="#b7">[8]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Real-time tracking of ngertips in IR images</head><p>Unfortunately, none of the previously proposed methods for hand tracking provides the capability necessary for our augmented desk interface system. To realize a natural and intuitive i n terface to manipulate both physical objects and electrically projected objects on a desk, the system needs to be able to track a user's hand and ngertip locations in complex environments in real-time without relying on markers or marked gloves attached to the user's hand.</p><p>In this work, we propose a new method for tracking a user's palm center and ngertips by using an infrared camera. <ref type="foot" target="#foot_0">1</ref> The use of an infrared camera is especially advantageous for our augmented desk interface system where observed color of human skin and image background change signicantly due to projection onto a desk. Unlike regular CCD cameras which detect light in visible wavelengths, an infrared camera can detect light emitted from a surface with a certain range of temperature. Thus, by setting the temperature range to approximate human body temperature, image regions corresponding to human skin appear particularly bright in input images from the infrared camera.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Extraction of left and right arms</head><p>To extract right or left arms, an infrared camera is installed with a surface mirror so that user's hands on a desk can be observed by the camera as shown in Fig. <ref type="figure" target="#fig_0">1</ref>.</p><p>The video output from the infrared camera is digitized as a gray-scale image with 256 2 220 pixel resolution by a frame grabber on a PC. Because the infrared camera is adjusted to measure a range of temperature that approximates human body temperature, e.g,. typically between 30 o and 34 o , v alues of image pixels corresponding to human skin are higher than other image pixels. Therefore, image regions which correspond to human skin can be easily identied by binarization of the input image with a threshold value. In our experiments, we found that a xed threshold value for image binarization works well for nding human skin regions regardless of room temperatures. Fig. <ref type="figure" target="#fig_1">2</ref>(a) and (b) show one example of an input image from the infrared camera, and a region of human skin extracted by binarization of the input image, respectively.</p><p>If other objects on a desk happen to have temperatures similar to that of human skin, e.g., a warm cup or a note PC, image regions corresponding to those objects in addition to human skin are found by image binarization. To remove those regions other than human skin, we rst remove small regions, and then select the two regions with the largest size. If only one large region is found, we consider that only one arm is observed on the desk. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Finding ngertips</head><p>Once regions of user's arms are found in an input image, ngertips are searched for within those regions. Compared with the extraction of users' arms, this search process is more computationally expensive. Therefore, a search window is dened in our method, and ngertips are searched for only within the window instead of over entire regions of users' arms.</p><p>A search window is determined based on the orientation of each arm which is given as the principal axis of inertia of the extracted arm region. The orientation of the principal axis can be computed from the image moments up to the second order as described in <ref type="bibr" target="#b1">[2]</ref>. Then, a search window of a xed size, e.g., 80 2 80 pixels in our current implementation, is set so that it includes a hand part of the arm region based on the orientation of the arm. (Fig. <ref type="figure" target="#fig_1">2(c</ref>)) We found that a xed size for the search window works reliably because the distance from the infrared camera to a user's hand on a desk remains relatively constant.</p><p>Once a search window is determined for each hand region, ngertips are searched for within that window. The overall shape of a human nger can be approximated by a cylinder with a hemispherical cap. Thus, the projected shape of a nger in an input image appears to be a rectangle with a semi-circle at its tip.</p><p>Based on this observation, ngertips are searched for by template matching with a circular template as shown in Fig. <ref type="figure">3 (a)</ref>. In our proposed method, normalized correlation with a template of a xed-size circle is used for the template matching. Ideally, the size of the template should dier for dierent ngers and dierent users. In our experiments, however, we found that the xed size of template works reliably for various users. For instance, a square of 15 2 15 pixels with a circle whose radius is 7 pixels is used as a template for normalized correlation in our current implementation.</p><p>While a semi-circle is a reasonably good approximation of the projected shape of a ngertip, we h a ve to consider false detection from the template matching. For this reason, we rst nd a suciently large number of candidates. In our current implementation of the system, 20 candidates with the highest matching scores are selected inside each search window. The number of initially selected candidates has to be suciently large to include all true ngertips.</p><p>After the ngertip candidates are selected, false candidates are removed by means of two types of false detection. One is multiple matching around the true location of a ngertip. This type of false detection is removed by suppressing neighbor candidates around a candidate with the highest matching score.</p><p>The other type of false detection is a matching that occurs in the middle of ngers such as the one illustrated in Fig. <ref type="figure">3 (b)</ref>. This type of falsely detected candidates is removed by examining surrounding pixels around the center of a matched template. If multiple pixels in a diagonal direction are inside the hand region, then it is considered not to exist at a ngertip, and therefore the candidate is discarded.</p><p>By removing these two t ypes of false matchings, we can successfully nd correct ngertips as shown in Fig. <ref type="figure">3  (c</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Finding centers of palms</head><p>The center of a user's palm needs to be determined to enable recognition of various types of hand gestures. For example, the location of the center is necessary to estimate how extended each nger is, and therefore it is essential for recognizing basic gestures such as click and drag.</p><p>In our proposed method, the center of a user's hand is given as the point whose distance to the closest region boundary is the maximum. In this way, the center of Figure <ref type="figure">3</ref>. Template matching for ngertips the hand becomes insensitive t o v arious changes such as opening and closing of the hand. Such a location for the hand's center is computed by morphological erosion operation of an extracted hand region. First, a rough shape of the user's palm is obtained by cutting out the hand region at the estimated wrist as shown in Fig. <ref type="figure" target="#fig_2">4 (a)</ref>. The location of the wrist is assumed to be at the pre-determined distance, e.g., 60 pixels in our case, from the top of the search window and perpendicular to the principal direction of the hand region.</p><p>Then, a morphological erosion operator is applied to the obtained shape of the user's palm until the area of the region becomes small enough. As a result, a small region at the center of the palm is obtained. Finally, the center of the hand region is given as the center of mass of the resulting region as shown in Fig. <ref type="figure" target="#fig_2">4 (c</ref>). </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Performance</head><p>We have tested our proposed method by using the system shown in Fig. <ref type="figure" target="#fig_0">1</ref>. An infrared camera (NIKON Thermal Vision LAIRD-3A) is installed with a surface mirror, so that a user's hand on a desk can be observed. Input images from the infrared camera are processed as described in this paper in Section3 on a personal computer (Hardware: PentiumIII 450MHz, OS: Linux kernel 2.0.36) with a general-purpose image processing board (HITACHI IP-5010).</p><p>Several examples of tracking results are shown in Fig. <ref type="figure" target="#fig_3">5</ref>. These results show that our proposed method successfully nds centers of palms and locations of ngertips. Centers of palms are found reliably regardless of opening of a hand. Also, ngertips are found successfully by our method even when ngers are not fully extended. This is a case where the previously proposed methods based on shape of contour of hand regions often have diculties.</p><p>While we have not yet done any careful optimization of the codes, the current implementation of our system is running almost in real-time, e.g., approximately 25-30 frames per second for one hand. The system suuccesfully nds ngertips and palm centers for both left and right hands. However, in this case, processing speed becomes somewhat lower due to the doubled area for searching for ngertips. If two hands are tracked, the system runs at approximately 15 frames per second. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Augmented desk interface system</head><p>The proposed method for tracking hands and ngertips in infrared images was successfully used for our augmented desk interface system. As shown in the system overview in Fig. <ref type="figure" target="#fig_0">1</ref>, the system is equipped with an LCD projector, an infrared camera, and a pan-tilt camera. The LCD projector is used for projecting various kinds of digital information such as computer graphics objects, text, or a WWW browser on a desk.</p><p>For alignment between an image projected onto a desk by the LCD projector and an input image from the infrared camera, we determine a projective transformation between those two images through initial calibration of the system. The use of projective transformation is enough for calibration of our system since imaging/projection targets can be approximated as to be planer due to the nature of our application. In addition, a similar calibration is also carried out for the pan-tilt camera so that the camera can be controlled to look toward a desired position on the desk.</p><p>The pan-tilt camera is controlled to follow a user's ngertip whenever the user points at a particular location on the desk with one nger. This is necessary to obtain enough image resolution to recognize real objects near a user's pointing nger. Currently-available video cameras simply do not provide enough image resolution when the entire table is observed. In our current implementation of the interface system, a two-dimensional matrix code <ref type="bibr" target="#b9">[10]</ref> is used for identifying objects on the desk. (See Fig. <ref type="figure">7</ref> for example). More sophisticated computer vision methods would be necessary for recognizing real objects without any markers.</p><p>Using our augmented desk interface system, we have tested various applications in which a user manipulates both physical objects and electronically projected objects on the desk. Fig. <ref type="figure">6</ref> shows one example of such applications. In this example, a user can manipulate a projected object on a desk using both left and right hands. By bending his/her forenger at an end of the object, a user can grab the object's end. Then, a user can translate, rotate, and stretch the object by t w o-handed direct manipulation. Fig. <ref type="figure">7</ref> shows another application example of our augmented desk interface system. In this application, a user can browse WWW pages associated with physical documents on a desk by simply pointing to those documents with his/her forenger. With the pan-tilt camera, which follows a user's forenger on the desk, a small twodimensional matrix code attached to a physical document is recognized. Once a physical document is found on the desk, associated WWW pages are projected directly next to the document.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Conclusions</head><p>In this paper, we have proposed a fast and reliable method for tracking a user's palm centers and ngertips for both left and right hands. Our method makes use of infrared camera images and template matching by normalized correlation which is performed eciently with a general-purpose image processing hardware. In particular, our method is eective for applications in our augmented desk interface system where observed color of human skin and image backgrounds continuously change due to projections by an LCD projector. While previous methods based on color segmentation or background image subtraction would have diculties for tracking hands or nertips, our proposed method was demonstrated to perform very reliably even in this situation.</p><p>Currently, we are extending our method so that not only all ngertips can be found, but also so that those ngertips can be distinguished from one another, e.g. the ngertip of an index nger and that of a middle nger. Also, based on our tracking method, we are endeavoring to enhance our augmented desk interface system with more sophisticated gesture recongnition capabilities. </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 .</head><label>1</label><figDesc>Figure 1. Overview of our augmented desk interface system</figDesc><graphic coords="2,62.80,82.32,203.40,151.50" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 .</head><label>2</label><figDesc>Figure 2. Extraction of hand region</figDesc><graphic coords="3,313.60,324.12,235.00,87.40" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 4 .</head><label>4</label><figDesc>Figure 4. Center of a user's palm</figDesc><graphic coords="4,313.60,446.72,235.00,87.40" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 5 .</head><label>5</label><figDesc>Figure 5. several examples of tracking results</figDesc><graphic coords="5,62.25,297.56,204.22,153.07" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 6 .Figure 7 .</head><label>67</label><figDesc>Figure 6. Direct manipulation of a CG object with two hands</figDesc><graphic coords="6,67.40,152.32,194.66,93.75" type="bitmap" /></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0"><p>The use of an infrared camera was examined in another study for human body posture measurement<ref type="bibr" target="#b6">[7]</ref>. In that case, human body postures were determined by extracting human body regions in infrared images, and then by analyzing contour of the extracted regions. While our method is used for accurate estimation of distinct feature points such as palm centers and ngertips, their method was designed to estimate a rough posture of a human body, e.g., the orientation of a body and the direction of two arms.</p></note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Robust structure from motion using motion parallax</title>
		<author>
			<persName><forename type="first">R</forename><surname>Cipolla</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Okamoto</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Kuno</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. 1999 IEEE International Conference on Computer Vision</title>
		<meeting>1999 IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="1993">1993</date>
			<biblScope unit="page" from="374" to="382" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">\Computer vision for interactive computer graphics</title>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">T</forename><surname>Freeman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">B</forename><surname>Anderson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">A</forename><surname>Beardsley</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">N</forename><surname>Dodge</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Roth</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">D</forename><surname>Weissman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">S</forename><surname>Yerazunis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Computer Graphics and Applications</title>
		<imprint>
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="42" to="53" />
			<date type="published" when="1998-06">May-June 1998</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Hand gesture modeling, analysis, and synthesis</title>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">S</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><forename type="middle">I</forename><surname>Palvovic</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of 1995 IEEE International Workshop on Automatic Face and Gesture R ecognition</title>
		<meeting>of 1995 IEEE International Workshop on Automatic Face and Gesture R ecognition</meeting>
		<imprint>
			<date type="published" when="1995-09">September 1995</date>
			<biblScope unit="page" from="73" to="79" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Enhanced Desk, integrating paper documents and digital documents</title>
		<author>
			<persName><forename type="first">M</forename><surname>Kobayashi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Koike</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. 1998 Asia Pacic Computer Human Interaction</title>
		<meeting>1998 Asia Pacic Computer Human Interaction</meeting>
		<imprint>
			<date type="published" when="1998">1998</date>
			<biblScope unit="page" from="167" to="174" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">A novel gestural input device for virtual reality</title>
		<author>
			<persName><forename type="first">C</forename><surname>Maggioni</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. 1993 IEEE Annual Virtual Reality International Symposium</title>
		<meeting>1993 IEEE Annual Virtual Reality International Symposium</meeting>
		<imprint>
			<date type="published" when="1993">1993</date>
			<biblScope unit="page" from="118" to="124" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">GestureComputer -history, design and applications</title>
		<author>
			<persName><forename type="first">C</forename><surname>Maggioni</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Kammerer</surname></persName>
		</author>
		<editor>R. Cipolla and A. Pentland</editor>
		<imprint>
			<date type="published" when="1998">1998</date>
			<publisher>Cambridge University Press</publisher>
			<biblScope unit="page" from="23" to="51" />
		</imprint>
	</monogr>
	<note>Computer Vision for Human-Machine Interaction</note>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Virtual kabuki theater: towards the realization of human metamorphosis systems</title>
		<author>
			<persName><forename type="first">J</forename><surname>Ohya</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. 5th IEEE International Workshop on Robot and Human Communication</title>
		<meeting>5th IEEE International Workshop on Robot and Human Communication</meeting>
		<imprint>
			<date type="published" when="1996-11">November 1996</date>
			<biblScope unit="page" from="416" to="421" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">\Visual interpretation of hand gestures for human-computer interaction: a review</title>
		<author>
			<persName><forename type="first">V</forename><forename type="middle">I</forename><surname>Pavlovic</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Sharma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">S</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Trans. PAMI, V ol</title>
		<imprint>
			<date type="published" when="1997-07">July 1997</date>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="page" from="677" to="695" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Digiteyes: Vision-based hand tracking for human-computer interaction</title>
		<author>
			<persName><forename type="first">J</forename><surname>Rehg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Kanade</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Workshop on Motion of Non-Rigid and Articulated Objects</title>
		<meeting>Workshop on Motion of Non-Rigid and Articulated Objects<address><addrLine>Austin, Texas</addrLine></address></meeting>
		<imprint>
			<date type="published" when="1994-11">November 1994</date>
			<biblScope unit="page" from="16" to="22" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">\Matrix: a realtime object identication and registration method for augmented reality</title>
		<author>
			<persName><forename type="first">J</forename><surname>Rekimoto</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. 1998 Asia Pacic Computer Human Interaction (APCHI&apos;98)</title>
		<meeting>1998 Asia Pacic Computer Human Interaction (APCHI&apos;98)</meeting>
		<imprint>
			<date type="published" when="1998-07">July 1998</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Shadow gestures: 3D hand pose estimation using a single camera</title>
		<author>
			<persName><forename type="first">J</forename><surname>Segen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Kumar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. 1999 IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>1999 IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="1999-06">June 1999</date>
			<biblScope unit="page" from="479" to="485" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">A survey of glovebased input</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">J</forename><surname>Sturman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Zeltzer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Computer Graphics and Applications</title>
		<imprint>
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="page" from="30" to="39" />
			<date type="published" when="1994-01">January 1994</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Interacting with paper on the DIGITAL DESK</title>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">W</forename><surname>Ellner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Communications of The ACM, V ol</title>
		<imprint>
			<biblScope unit="volume">36</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="87" to="96" />
			<date type="published" when="1993-07">July 1993</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">\A hand gesture interface device</title>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">G</forename><surname>Zimmermann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Lanier</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Blanchard</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Bryson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Harvill</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ACM Conf. Human Factors in Computing Systems and Graphics Interface</title>
		<meeting>ACM Conf. Human Factors in Computing Systems and Graphics Interface</meeting>
		<imprint>
			<date type="published" when="1987">1987</date>
			<biblScope unit="page" from="189" to="192" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
