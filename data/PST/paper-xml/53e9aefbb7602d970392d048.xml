<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Recent Developments in Context-Based Predictive Techniques for Lossless Image Compression</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author role="corresp">
							<persName><forename type="first">Nasir</forename><surname>Memon</surname></persName>
							<email>memon@cs.niu.edu</email>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science</orgName>
								<orgName type="institution">Northern Illinois University</orgName>
								<address>
									<postCode>60115</postCode>
									<settlement>DeKalb</settlement>
									<region>IL</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Xiaolin</forename><surname>Wu</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">Department of Computer Science</orgName>
								<orgName type="institution">The University of Western Ontario</orgName>
								<address>
									<postCode>N6A 5B7</postCode>
									<settlement>London</settlement>
									<region>Ontario</region>
									<country key="CA">Canada</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Recent Developments in Context-Based Predictive Techniques for Lossless Image Compression</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">A7CFAE298065A3EC6C8FCEF9F48BEE31</idno>
					<note type="submission">Received July, 1996; revised May, 1997</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.3" ident="GROBID" when="2023-07-27T09:32+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>In this paper we describe some recent developments that have taken place in context-based predictive coding, in response to the JPEG/JBIG committee's recent call for proposals for a new international standard on lossless compression of continuous-tone images. We describe the different prediction techniques that were proposed and give a performance comparison. We describe the notion of context-based bias cancellation, which represents one of the key ideas that was proposed and incorporated in the final standard. We also describe the different error modelling and entropy coding techniques that were proposed for encoding prediction errors, the most important development here being an ingeniously simple and effective technique for adaptive Golomb-Rice coding. We conclude with a short discussion on avenues for future research.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">INTRODUCTION</head><p>We have seen an increased level of activity in image and video compression in recent years; however, most of this activity has been restricted to lossy compression. Many applications, such as medical imaging, image archiving, high-precision image analysis, remote sensing, pre-press imaging, preservation of art work and historical documents, require lossless compression. Despite the importance of lossless image compression of continuous-tone images there is a paucity of standard algorithms. Current standards for lossless compression include 1. Lossless JPEG (Huffman and arithmetic). 2. JBIG, Group 4 Fax. 3. GIF, Photo CD, PNG etc.</p><p>It is generally accepted that the Huffman-coding-based JPEG lossless standard provides poor compression and a host of better techniques have been reported in the literature <ref type="bibr" target="#b0">[1]</ref><ref type="bibr" target="#b1">[2]</ref><ref type="bibr" target="#b2">[3]</ref>. The JPEG arithmetic coding version does provide about 10% better compression, but is not available in the public domain and hence has seen little use. JBIG and the CCITT Group 3 and 4 standards are primarily designed for bi-level data and do not provide good compression when used on greyscale images by compressing individual bit planes. GIF and PNG are essentially suitable for synthesized images and are known not to work well with natural continuous-tone images acquired through an array of sensors.</p><p>Due to the perceived inadequacy of current standards for lossless image compression, the JBIG/JPEG committee of the International Standards Organization (ISO) approved a new work item proposal in early 1994, titled Next Generation Lossless Compression of Continuous-tone Still Pictures. A call was issued in March 1994 soliciting proposals specifying algorithms for lossless and near-lossless compression of continuous-tone (2-16 bit) still pictures. A number of requirements were imposed on submissions; for details the reader is referred to <ref type="bibr" target="#b3">[4]</ref>. For instance, exploitation of interband correlations (in colour and satellite images for example) was prohibited.</p><p>This call for proposals resulted in renewed activity focused on the development of lossless image compression techniques. A large part of this activity has focused on a specific type of compression technique, loosely referred to in the literature as lossless DPCM or lossless predictive coding. Since the baseline algorithm that has been standardized <ref type="bibr" target="#b4">[5]</ref> and the proposed high-performance extension both employ a predictive coding approach, we restrict our discussion to predictive coding techniques. We describe some of the important new developments that emerged in response to the call for proposals, and have contributed significantly to the advancement in the state of the art of predictive coding techniques for lossless image compression.</p><p>The paper is structured as follows. In the next section we begin by giving an introduction to predictive coding techniques for lossless image compression and describe the current lossless JPEG standard. We also introduce and establish some terminology and notation that is used throughout the rest of the paper. In Section 3 we describe various predictors that were proposed. We describe three specific predictors, THE COMPUTER JOURNAL, Vol. 40, No. 2/3, 1997 MED, GAP and ALCM in detail and then present a performance comparison which clearly establishes the choice of MED as the default predictor for the standard. In fact, an important discovery made during the standardization process was the surprising efficacy of the MED predictor despite its apparent simplicity. In Section 4 we describe the notion of context-based bias cancellation, which was one of the key ideas that contributed towards the development of the final standard. In Section 5 we outline the different error modelling techniques that were proposed for encoding prediction errors and in Section 6 we describe the specific entropy coding techniques employed. The most important contribution here came from the revised LOCO algorithm proposed by HP laboratories <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b6">7]</ref>, in the form of a very simple and effective parameter estimation technique for Golomb-Rice coding. We conclude in Section 7 with a discussion on avenues for further research in lossless image compression.</p><p>At this point we would like to note that it is not the intention of this paper to give a detailed description of the new lossless JPEG standard, nor do we intend this to be a thorough treatise on lossless image compression in general. Our intention is to describe the main ideas that were proposed in response to the JPEG committee's call for proposals, the convergence of which led to the development of the new standard.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">PREDICTIVE CODING TECHNIQUES AND THE CURRENT JPEG LOSSLESS STANDARD</head><p>Among various methods which have been devised for lossless compression, predictive techniques are perhaps the simplest and most efficient. Here the transmitter (and receiver) process the image in some fixed order (say, raster order going row by row, left to right within a row) and predict the value of the current pixel on the basis of the pixels which have already been transmitted (received). If we denote the current pixel by P[i, j] and its predicted value by P[i, j], then only the prediction error, e = P[i, j] -P[i, j], needs to be transmitted. If the prediction is reasonably accurate then the distribution of prediction errors is concentrated near zero and has a significantly lower zero-order entropy than the original image.</p><p>If the residual image consisting of prediction errors is treated as an Independent and Identically Distributed (IID) source, then it can be coded efficiently using any of the </p><formula xml:id="formula_0">0 0 (no prediction) 1 N 2 W 3 NW 4 N + W -NW 5 W + (N -NW)/2 6 N + (W -NW/2 7 (N + W )/2</formula><p>standard variable-length entropy coding techniques, such as Huffman coding or arithmetic coding. Unfortunately, even after applying the most sophisticated prediction techniques, generally the residual image has ample structure which violates the IID assumption. Hence, in order to encode prediction errors efficiently we need a model that captures the structure that remains after prediction. This step is often referred to as error modelling <ref type="bibr" target="#b7">[8]</ref>. The error modelling techniques employed by most lossless compression schemes proposed in the literature can be captured within the context modelling framework described in <ref type="bibr" target="#b8">[9]</ref> and applied in <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b9">10]</ref>. In this approach, the prediction error at each pixel is encoded with respect to a conditioning state or context, which is arrived at from the values of previously encoded neighbouring pixels. Viewed in this framework, the role of the error model is essentially to provide estimates of the conditional probability of the prediction error, given the context in which it occurs. This can be done by estimating the PDF by maintaining counts of symbol occurrences within each context <ref type="bibr" target="#b9">[10]</ref> or by estimating the parameters (variance, for example) of an assumed Probability Density Function (PDF) (Laplacian, for example) as in <ref type="bibr" target="#b7">[8]</ref>.</p><p>In Figure <ref type="figure" target="#fig_0">1</ref> we show a template of two-dimensional neighbourhood pixels, a subset of which is generally used for prediction and/or context determination by lossless image compression techniques. In the remainder of the paper we use the notation specified in Figure <ref type="figure" target="#fig_0">1</ref> to denote specific neighbours of the pixel P[i, j] in the ith row and jth column.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.">The current lossless JPEG standard</head><p>The current JPEG standard uses a predictive scheme when used in its lossless mode. It provides eight different predictors from which the user can select. Table <ref type="table" target="#tab_0">1</ref> lists the eight predictors used. The prediction errors are then encoded either by Huffman coding or arithmetic coding-codecs for both are provided by the standard. In the Huffman coding version, essentially no error model is used. The prediction errors are assumed to be IID and either a static default Huffman table is used or a custom Huffman code table can be specified, which is then encoded along with the compressed image. The latter approach requires two passes through the data.</p><p>The arithmetically coded version uses quantized prediction errors at neighbouring pixels as contexts for THE COMPUTER JOURNAL, Vol. 40, No. 2/3, 1997 conditioning the prediction error. Binary arithmetic coding is used within each context by decomposing the prediction error into a sequence of binary decisions. The first binary decision determines whether the prediction error is zero. If it is not zero, then the second step determines the sign of the error. The subsequent steps assist in classifying the magnitude of the prediction error into one of a set of ranges and the final bits that determine the exact prediction error magnitude within the range are sent uncoded. The QM-Coder is used for encoding each binary decision. A detailed description of the coder and the standard can be found in <ref type="bibr" target="#b10">[11]</ref>.</p><p>Given the success of predictive techniques for lossless image compression, it was no surprise that seven out of the nine proposals submitted to ISO, in response to the call for proposals for a new lossless image compression standard, employed prediction followed by conditional encoding of the prediction error. In this paper we restrict our discussion to these seven proposals. The other two proposals <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b12">13]</ref> were based on transform coding. However, right from the first-round evaluations it was clear that the transform-codingbased proposals did not provide as good compression ratios as algorithms proposed based on predictive techniques <ref type="bibr" target="#b13">[14]</ref>. In the remainder of this paper we describe in more detail the specific contributions that were made which have led to the development of the new proposed standard.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">THE PREDICTION STEP</head><p>In this section we briefly describe the predictors that were proposed. At the end of the section we give a performance comparison and make a few observations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">The MED predictor</head><p>Hewlett Packard's proposal, LOCO-I (low-complexity lossless coder) <ref type="bibr" target="#b5">[6]</ref>, used the median edge detection (MED) predictor, that adapts in the presence of local edges. MED detects horizontal or vertical edges by examining the North (N ), West (W ) and North-West (NW) neighbours of the current pixel P[i, j] as illustrated in Figure <ref type="figure" target="#fig_0">1</ref> in Section 2. The North pixel is used as a prediction in the case of a vertical edge being detected. The West pixel is used as a prediction in the case of a horizontal edge. Finally, if neither a vertical edge nor a horizontal edge is detected, planar interpolation is used to compute the prediction value. Specifically, prediction is performed according to the following equations:</p><formula xml:id="formula_1">P[i, j] =    min(N , W ) if NW ≥ max(N , W ) max(N , W ) if NW ≤ min(N , W ) N + W -NW otherwise.</formula><p>The MED predictor has also been called MAP (median adaptive predictor) and was first proposed by Martucci <ref type="bibr" target="#b14">[15]</ref>. Martucci presented the MAP predictor as a non-linear adaptive predictor that selects the median of a set of three predictions in order to predict the current pixel. One way of interpreting such a predictor is that it always chooses either the best or the second-best predictor among the three candidate predictors. Martucci reported the best results with the following three predictors, in which case it is easy to see that MAP turns out to be the MED predictor.</p><formula xml:id="formula_2">1. N 2. W 3. N + W -NW.</formula><p>In an extensive evaluation, the MED predictor was observed to give superior performance over most linear predictors <ref type="bibr" target="#b15">[16]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">The GAP predictor</head><p>The CALIC proposal <ref type="bibr" target="#b16">[17]</ref> included a gradient-adjusted predictor (GAP) which adapts the prediction according to local gradients and hence gives a more robust performance compared to standard linear predictors. GAP weights the neighbouring pixels of P[i, j] according to the estimated gradients in the neighbourhood. In GAP the gradient of the intensity function at the current pixel P[i, j] is estimated by computing the following quantities:</p><formula xml:id="formula_3">d h = |W -WW| + |N -NW| + |N -NE| d v = |W -NW| + |N -NN| + |NE -NNE|.</formula><p>(</p><p>A prediction P[i, j] is then made by the following procedure:</p><formula xml:id="formula_5">IF (d v -d h &gt; 80) {sharp horizontal edge} P[i, j] = W ELSE IF (d v -d h &lt; -80) {sharp vertical edge} P[i, j] = N ELSE { P[i, j] = (N + W )/2 + (NE -NW)/4; IF (d v -d h &gt; 32) {horizontal edge} P[i, j] = ( P[i, j] + W )/2 ELSE IF (d v -d h &gt; 8) {weak horizontal edge} P[i, j] = (3 P[i, j] + W )/4 ELSE IF (d v -d h &lt; -32) {vertical edge} P[i, j] = ( P[i, j] + N )/2 ELSE IF (d v -d h &lt; -8) {weak vertical edge} P[i, j] = (3 P[i, j] + N )/4 }</formula><p>where N, W, NW, NN, NE and NNE are as defined in Figure <ref type="figure" target="#fig_0">1</ref>. The thresholds given in the above procedure are for 8bit data and are adapted on the fly for higher resolution images. These thresholds were arrived at after extensive experimentation with a large set of test images.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">The ALCM and JSLUG predictor</head><p>The ALCM proposal <ref type="bibr" target="#b17">[18]</ref> and the JSLUG proposal <ref type="bibr" target="#b18">[19]</ref> included an adaptive predictor that used a weighted combination of five neighbourhood pixels in order to predict the current pixel. The weights are adapted on the fly as encoding progresses. The neighbourhood used consisted of the N, W, NW, NE and WW pixels as specified in Figure <ref type="figure" target="#fig_0">1</ref>.</p><p>Initially, all pixels are assigned an equal weight. After prediction, the weights are changed as follows. If the prediction was lower than the actual value then the weight of the largest neighbouring pixel is decremented by 1  256 and the weight of the smallest neighbour is decremented by the same amount. If the prediction was too high then the largest neighbouring pixel is decremented and the smallest one is incremented. In case more than one pixel has the highest weight, ties are broken by using the following priority scheme. 4</p><formula xml:id="formula_6">P[i, j]</formula><p>The pixel labelled 1 is changed with highest priority and the pixel labelled 5 with least priority.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4.">Other predictors</head><p>Besides MED, GAP and ALCM, there were a few more predictors proposed in different submissions, but a detailed evaluation revealed their performance to be inferior <ref type="bibr" target="#b19">[20]</ref>. For example, the Mitsubishi proposal, CLARA <ref type="bibr" target="#b20">[21]</ref>, adaptively switched between a fixed set of predictors based on the texture and gradients in the neighbourhood of the target pixel. The following set of predictors was used:</p><formula xml:id="formula_7">N , W, N + W 2 , N 2 + N + N E 4 .</formula><p>Details of the exact manner in which the selection was made are given in <ref type="bibr" target="#b20">[21]</ref>.</p><p>Another predictor, given in the DARC proposal by Kodak <ref type="bibr" target="#b22">[22]</ref>, adapted to horizontal and vertical gradients in the neighbourhood of the pixel being predicted. Specifically, given that the current pixel is predicted to be P</p><formula xml:id="formula_8">[ î, j] = αW + (1 -α)N where α = v h + v v = |W -NW| h = |N -NW|.</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.5.">Performance comparison</head><p>In Table <ref type="table" target="#tab_2">2</ref>   On examining the results, we see that the performance of the three techniques is very similar. There is no clear winner that outperforms others on all test images. MED has the lowest average rate over the entire data set. MED and GAP have comparable complexity, but ALCM has much higher computational complexity. GAP performs better in smooth images, but fares poorly in compound images that have both text and image data. ALCM too fares poorly with compound images. Given these facts, the MED predictor was adopted by the committee as the default predictor for the baseline algorithm of the proposed standard.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">CONTEXT-BASED BIAS CANCELLATION</head><p>Local gradients alone cannot adequately characterize some of the more complex relationships between the predicted pixel P[i, j] and its surrounding. Conditioning of the prediction error e = P[i, j] -P[i, j] to its context can exploit higher-order structures such as texture patterns and local activity in the image for further compression gains. However, the large number of possible contexts can lead to the 'sparse context' or 'high model cost' problem <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b9">10]</ref>. The CALIC proposal employed a novel and effective solution to this problem, based on some recent work by Wu <ref type="bibr" target="#b23">[23]</ref>. Instead of estimating the PDF of prediction errors, p(e|C), within each context C, only its conditional expectation E{e|C} is estimated using the corresponding sample means ē(C). These estimates are then used to further refine the prediction prior to entropy coding, by an error feedback mechanism that cancels prediction biases in different contexts. We call this process bias cancellation. The idea of gaining coding efficiency by bias cancellation arises from the observation that the conditional mean ē(C) is generally not zero in a given context C. This does not contradict the well-known fact that the prediction errors without conditioning on contexts, follow a zero-mean Laplacian (symmetric exponential) distribution for most continuoustone images. The observed Laplacian distribution without conditioning on contexts is a composition of many contextsensitive distributions of different means and different variances (see Figure <ref type="figure">2</ref>). Conditioning of the prediction error to its context provides a means to separate these distributions. Therefore, the more biased ē(C) is from zero, the more effective is the process of bias cancellation.</p><p>Since the conditional mean ē(C) is the most likely prediction error in a given context C, we can correct the bias in the prediction by feeding back ē(C) and adjusting the prediction P[i, j] to P[i, j] = P[i, j] + ē(C). In order not to over-adjust the predictor, in practice the new prediction error ǫ = P[i, j] -P[i, j] is estimated rather than e = P[i, j] -P[i, j]. This in turn leads to an improved predictor for P[i, j]: P[i, j] = P[i, j] + ǭ(C), where ǭ(C) is the sample mean of ǫ conditioned on context C. Conceptually, bias cancellation can also be viewed as a two-stage adaptive prediction scheme via conditioning of prediction errors to contexts and the subsequent error feedback. Hence contexts used for bias cancellation are also called prediction contexts.</p><p>We describe below the details by which contexts were formed and quantized by CALIC and LOCO-I 1 p , the two proposals that employed bias cancellation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Context formation and quantization in CALIC</head><p>In CALIC, contexts for error modelling are formed by embedding 144 texture contexts into four error energy contexts to form a total of 576 compound contexts. Texture contexts are formed by quantization of a local neighbourhood of pixel values to a binary vector</p><formula xml:id="formula_9">C = {x 0 , . . . , x 6 , x 7 } (2) = {N , W, NW, NE, NN, WW, 2N -NN, 2W -WW},<label>(3)</label></formula><p>where N, W, NW, NE, NN and WW are defined as in Figure <ref type="figure" target="#fig_0">1</ref>.</p><p>C is then quantized to an 8-bit binary number B = b 7 b 6 . . . b 0 using the prediction value P[i, j] as the threshold, namely</p><formula xml:id="formula_10">b k = 0 if x k ≥ P[i, j] 1 if x k &lt; P[i, j] 0 ≤ k &lt; K = 8.</formula><p>(4) Clearly, B captures the texture patterns in the modelling context which are indicative of the behaviour of the prediction error e. Also note that an event x i in a prediction context need not be a neighbouring pixel to P[i, j]. It can be a function of some neighbouring pixels. x 6 and x 7 , for example, represent the events whether the prediction value P[i, j] forms a convex or concave waveform with respect to the neighbouring pixels in the vertical and horizontal directions.</p><p>Since the variability of neighbouring pixels also influences the error distribution, the texture contexts are combined with quantized error energy to form compound modelling contexts. Error energy contexts are computed by using an error energy estimator defined as</p><formula xml:id="formula_11">= d h + d v + 2|e w |,<label>(5)</label></formula><p>where d h and d v are as defined in Equation ( <ref type="formula" target="#formula_4">1</ref>) and e w = P[i -1, j] -P[i -1, j] (|e w | is chosen because large errors tend to occur consecutively). is then quantized to four levels yielding a quantized error energy context Q( ) which is combined with the quantized texture pattern 0 ≤ B &lt; 2 K to form compound modelling contexts, denoted by C(δ, β). This scheme can be viewed as a product quantization of two independently treated image features: spatial texture patterns and the energy of prediction errors. At a glance, we would seemingly use 4 × 2 8 = 1024 different compound contexts. However, not all 2 8 binary codewords of the B quantizer defined by (4) are possible. By careful counting one determines that the total number of valid compound contexts is only 576 <ref type="bibr" target="#b24">[24]</ref>.</p><p>In Table <ref type="table" target="#tab_4">3</ref> we show the reduction in zero-order entropy when the error feedback mechanism described above is used along with the GAP predictor. It can be seen that for some images, significant improvements can be made. On the other hand, performance can actually degrade by a little in some instances. This leads to the need for selective feedback techniques, which we are currently investigating. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Context formation and quantization in LOCO-I 1p</head><p>Inspired by the success of the CALIC algorithm in the first round of evaluations, the HP group submitted a significantly different algorithm <ref type="bibr" target="#b6">[7]</ref> which was one-pass (as opposed to their original two-pass submission <ref type="bibr" target="#b5">[6]</ref>) and incorporated the context-based bias cancellation mechanism proposed in CALIC. However, they considerably simplified the context formation and quantization techniques and combined them with a very simple and efficient entropy coding technique (described in Section 6), while obtaining compression ratios that were only 3% inferior to CALIC's on a majority of ISO test images. Contexts in LOCO-I 1 p are formed by first computing the following differences:</p><formula xml:id="formula_12">D1 = NE -N D2 = N -NW D3 = NW -W D4 = WW -W. (6)</formula><p>The differences D1, D2 and D3 are then quantized into nine regions (labelled -4 to +4) symmetric about the origin with one of the quantization regions (region 0) consisting of only the difference value 0. The difference D4, being further away from the current pixel, is quantized into only three regions (labelled -1 to +1). Furthermore, contexts of the type (q 1 , q 2 , q 3 , q 4 ) and (-q 1 , -q 2 , -q 3 , -q 4 ) are merged based on the assumption that P(e|q 1 , q 2 , q 3 , q 4 ) = P(-e|-q 1 , -q 2 , -q 3 , -q 4 ).</p><p>The total number of contexts turns out to be 1094 within each of which the bias in prediction error is estimated in a manner similar to CALIC.</p><p>Extensive evaluation of the two context formation and quantization techniques described above showed little difference in compression performance for typical images. The second set of techniques was adopted by the committee for the proposed standard since it is simpler. In fact it was simplified further before adoption in the final committee draft of the standard by dropping the difference D4 and thereby obtaining a reduced context count of 364 <ref type="bibr" target="#b4">[5]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">COMPUTING CODING CONTEXTS</head><p>As mentioned earlier in Section 2, in practice it is observed that prediction does not completely remove the statistical redundancy in the image even after context-based bias cancellation. The variance of prediction errors strongly correlates to the smoothness of the image around the predicted pixel P[i, j]. To model this correlation predictive techniques usually condition the encoding of the prediction error on local image activity level and on quantized prediction errors incurred in neighbouring pixels. However, a direct implementation of this approach, due to the large number of conditioning states or contexts and the large alphabet size of prediction errors, faces two major difficulties: the use of prohibitively large memory space for error modelling, and the lack of sufficient samples in each context during adaptive coding in order to make reliable probability estimations.</p><p>Reducing the number of contexts could be one way to address this problem. Many of the lossless compression techniques reported in the literature have adopted this approach and use only a small number of contexts for conditioning the encoding of prediction errors. However, this leads to poorer performance due to loss in modelling efficiency. Another key contribution of the CALIC proposal was a modelling paradigm that employed a large number of contexts for bias cancellation, but merged the bias cancellation contexts into a few conditioning states for entropy coding of errors. These conditioning states are also called coding contexts in order to distinguish them from bias cancellation contexts.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.">Coding contexts in CALIC</head><p>Coding contexts in CALIC were computed by first computing an error energy estimator as defined in <ref type="bibr" target="#b4">(5)</ref>. Conditioning the error distribution on leads to separation of prediction errors into classes of different variances. Thus entropy coding of errors using estimated conditional probability p(e| ) improves coding efficiency over using p(e). For time and space efficiency, has to be quantized to a small number of (L) levels. In practice, L = 8 is found to be sufficient. Larger L only improves coding efficiency marginally. Although the quantizer Q( ) can be optimized off-line THE COMPUTER JOURNAL, Vol. 40, No. 2/3, 1997 by standard dynamic programming techniques in order to minimize the conditional entropy of prediction errors over a training set of images <ref type="bibr" target="#b24">[24]</ref>, in practice, it was found that an image-independent quantizer with bins which are fixed, q 1 = 5, q 2 = 15, q 3 = 25, q 4 = 42, q 5 = 60, q 6 = 85, q 7 = 140,</p><p>worked almost as well as the optimal image-dependent quantizer.</p><p>Estimating L = 8 conditional error probabilities p(e|Q( )) requires only a modest amount of memory while estimating probabilities for entropy coding. Furthermore, the small number of conditional error probabilities involved means that even small images will provide enough samples to learn p(e|Q( )) quickly to facilitate an adaptive entropy coding technique. In Table <ref type="table" target="#tab_5">4</ref> we list the zero-order entropy of prediction errors using the GAP predictor and the entropy after conditioning on the coding contexts described above. It can be clearly seen that significant improvement is obtained.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.">Coding contexts in LOCO-I 1p</head><p>LOCO-I 1 p uses k-coding contexts for a k bit/pixel image. The specific coding context is computed from the expected magnitude of the prediction error within the current bias estimation context, which is computed as described in Subsection 4.2. This is done by maintaining in each context, the count N of the prediction errors seen so far and the accumulated sum of magnitudes of prediction errors A seen so far. The coding context k is then computed as</p><formula xml:id="formula_14">k = min{k ′ |2 k ′ N ≥ A}.</formula><p>The reason for doing this is tied in to the specific entropy coder that LOCO-I 1 p uses, which is described in the next section. The strategy employed is an approximation to the optimal parameter selection for this entropy coder. For details the reader is referred to <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b6">7]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3.">Coding contexts in ALCM and JSLUG</head><p>Coding contexts in the ALCM proposal were obtained by quantizing the maximum prediction error in the four nearest neighbours of the current pixel. This maximum error was quantized into seven levels using fixed thresholds in order to form seven coding contexts in which adaptive binary arithmetic coding was performed.</p><p>The JSLUG proposal, on the other hand, quantizes the prediction errors incurred in the North, West and North-East neighbours into 7, 7 and 3 levels respectively, yielding 147 contexts in which adaptive binary arithmetic coding is performed. Some binary decisions are encoded conditioned on the sign of the current prediction error after it has been revealed and thus utilize 147 × 2 = 294 contexts.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">ENTROPY CODING</head><p>An advantage of the techniques that employ prediction followed by error modelling is the clean separation between prediction, modelling of prediction errors, and entropy coding of prediction errors. Quite often, any entropy coder, be it Huffman or arithmetic, static or adaptive, binary or m-ary, can usually be interfaced with such a system. Considering this fact, a variety of entropy coding techniques was proposed including Huffman coding, m-ary and binary arithmetic coding and Golomb-Rice coding. The main contribution came from the HP group's revised LOCO-I 1 p proposal in the form of an ingeniously simple and effective usage of Golomb-Rice coding. In the rest of the section we briefly describe some of the coding techniques that were proposed in the CALIC, LOCO and ALCM proposals.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.1.">Entropy coding in the CALIC system</head><p>CALIC used an adaptive m-ary arithmetic coder, CACM++ package that was developed and made publicly available by Carpinelli and Salamonsen. The software is based on the work in <ref type="bibr" target="#b25">[25]</ref>. The compression results that we report in the next section were obtained by coupling CALIC with CACM++. However, CALIC does not feed an m-ary arithmetic coder with prediction errors directly. Instead it first remaps prediction errors into an alphabet of size 2 z instead of 2 z+1 for a z-bit image. Also, the tails of error distributions are truncated and an escape mechanism is used to further reduce the number of code symbols. The actual bit rates achieved are mostly very close and sometimes even better than the corresponding entropy figures.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.2.">Entropy coding in the LOCO-I 1p system</head><p>In LOCO-I 1 p the prediction errors are encoded using a special case of Golomb codes <ref type="bibr" target="#b26">[26]</ref> which is also known as Rice coding <ref type="bibr" target="#b27">[27]</ref>. Golomb codes of parameter m encode a positive integer n by encoding n mod m in binary followed by an encoding of n div m in unary. When m = 2 k the encoding has a very simple realization and has been referred to as Rice coding in the literature. For an image with z bits/pixel, prediction errors can be mapped to the range 0 to 2 z -1 and the coding parameter k can vary from 0 to z -1. However, instead of attempting codes with each parameter on a block of symbols and selecting the one which results in the shortest code <ref type="bibr" target="#b27">[27]</ref>, in LOCO-I 1 p , the coding parameter k is estimated on the fly for each prediction error. We have briefly described this parameter estimation procedure in Subsection 4.2 and for details the reader is referred to <ref type="bibr" target="#b6">[7]</ref>. Despite the simplicity of the coding and estimation procedures, the compression performance achieved is surprisingly close to that obtained by arithmetic coding.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.3.">Entropy coding in the ALCM system</head><p>The ALCM and JSLUG proposals used adaptive binary arithmetic coding to encode prediction errors within each coding context. Since binary coding is used the prediction error needs to be binarized before encoding. In ALCM, prediction errors are first mapped to the range 0 to 2 z -1 and then binarized using a decision tree. A separate binary decision tree is maintained for each of the seven coding contexts and the first binary decision encoded is whether the symbol is more or less than a parameter value m. If the symbol is greater m is subtracted and the procedure repeated until a negative branch is taken. In this case the binarization is done by a decision tree for m equally probable symbols; essentially, the procedure involves adaptive binary arithmetic coding of the Golomb m code of the prediction error. Different values of m are used for each coding context. For simplicity of implementation m is always a power of 2.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.4.">Overall compression performance results</head><p>In Table <ref type="table" target="#tab_6">5</ref> we show final bit rates that were reported by the CALIC, LOCO-I 1 p and the ALCM proposals. The bit rates shown are after a few additional tricks that were used by each technique to improve compression performance. For example, the CALIC proposal included a sign prediction technique for reducing the conditional entropy of prediction errors. Also, both CALIC and LOCO-I 1 p included alphabet extension mechanisms for low-entropy images or regions where it is potentially beneficial to encode runs of uniform symbols. For these reasons the bit rates for CALIC in Table <ref type="table" target="#tab_6">5</ref> are lower than the corresponding rates in Table <ref type="table" target="#tab_5">4</ref>.</p><p>The reader is referred to the original proposals for details. Also included in the table are bit rates obtained by a publicly available lossless JPEG implementation of Cornell University (LJPEG). One can see from the results that the three proposed algorithms listed significantly outperform the current lossless standard. Although CALIC gives the best overall performance, the bit rates of LOCO-I 1 p are remarkable, given its simplicity. The baseline algorithm that has been finalized is essentially the one given in LOCO-I 1 p with minor modifications.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.">CONCLUSION AND AVENUES FOR FUTURE WORK</head><p>The standardization project for lossless image compression of continuous-tone images has resulted in significant advances in the state of the art of such techniques. In this paper we briefly surveyed some of the key ideas that emerged in the process. One of these was CALIC's modelling paradigm that uses a large number of contexts to estimate conditional prediction biases, and compensates for such biases through an error feedback mechanism. This approach offers an effective means of reducing model cost, a vital issue in lossless image coding, by using a large number of contexts for bias THE COMPUTER JOURNAL, Vol. 40, No. 2/3, 1997 cancellation but merging these contexts into a few contexts for conditional entropy coding of prediction errors.</p><p>The second, and perhaps most important key contribution was the ingenious and effective technique of Golomb-Rice coding using sequential parameter estimation in the revised HP proposal, LOCO-I 1 p . In spite of being extremely simple to implement both in software and hardware, the coding performance comes within a few per cent of much more complex arithmetic-coding-based techniques.</p><p>Another key observation that emerged in the convergence process was the efficacy of the MED predictor used in the LOCO submissions. Although the MED predictor has been known for a long time, its effectiveness for prediction in lossless image compression had not been realized. In fact, it is interesting to ask why MED yields such good performance. In addition, the simple context formation and quantization mechanisms presented in the LOCO proposals were also important contributions and were adopted into the baseline standard.</p><p>We have reported the recent advances in lossless image coding. For both theoretical and practical interests one would like to know how much gap still exists between the lossless bit rates obtainable by the new JPEG lossless standard and the ultimate image compressibility regardless of computational complexity. The question becomes even more tantalizing considering that the best (also the most expensive) version of CALIC <ref type="bibr" target="#b23">[23]</ref> has reached a 2% shorter average code length than the universal context modelling (UCM) algorithm <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b6">7]</ref>. The latter is a highly complex but principled algorithm with a provable asymptotical optimality in compressibility. A recent study <ref type="bibr" target="#b23">[23]</ref> seemed to suggest that the ratio of compression gains versus computational complexity is diminishing. We can identify two possible problems that may prevent further improvement in coding efficiency: (i) there may exist undiscovered structures of prediction errors associated with some events other than the local intensity gradients and neighbouring errors which have already been exploited by the current methods, and (ii) the context quantizers employed by the current methods may deviate significantly from an optimal error classifier that minimizes conditional entropy.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>FIGURE 1 .</head><label>1</label><figDesc>FIGURE 1. Notation used for specifying neighbouring pixels of the current pixel P[i, j].</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>TABLE 1 .</head><label>1</label><figDesc>JPEG predictors for lossless coding Mode Prediction for P[i, j]    </figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head></head><label></label><figDesc>we give the zero-order entropy of prediction errors with the three different predictors described above on the ISO test image set. This test set was made available to all proposers and comprised of &gt;160 Mbyte of image data.</figDesc><table /><note><p>The images Air1 and Air2 are RGB aerial images. The images Compound1, Compound2, Chart s and Chart are compound RGB images containing text and pictures. The image Faxballs is a graphics image. The images Bike, Woman, Cafe and Tools are SCID images (CMYK). The images Cats, Water and Bike3 are scanned RGB images. The set X-ray, CR, CT, MRI, Finger and US are</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>TABLE 2 .</head><label>2</label><figDesc>Zero-order entropy of prediction errors with the MED, GAP and ALCM predictors</figDesc><table><row><cell>Image</cell><cell cols="2">ALCM MED</cell><cell>GAP</cell></row><row><cell>Air1</cell><cell cols="3">8.6380 8.7711 8.7669</cell></row><row><cell>Air2</cell><cell cols="3">4.9985 4.4453 4.8578</cell></row><row><cell>Bike</cell><cell>4.1145</cell><cell cols="2">4.1131 4.0553</cell></row><row><cell>Bike3</cell><cell cols="3">5.0589 4.9290 4.9356</cell></row><row><cell>Cafe</cell><cell cols="3">5.3469 5.2754 5.2469</cell></row><row><cell>Cats</cell><cell cols="3">3.4713 3.5372 3.5321</cell></row><row><cell>Chart</cell><cell cols="3">2.0651 1.8577 2.0823</cell></row><row><cell>Chart s</cell><cell cols="3">3.8476 3.7922 3.6389</cell></row><row><cell>Compound1</cell><cell cols="3">2.5491 1.8935 1.9934</cell></row><row><cell>Compound2</cell><cell cols="3">2.4548 2.0503 2.1057</cell></row><row><cell>CR</cell><cell cols="3">5.2587 5.4048 5.2967</cell></row><row><cell>CT</cell><cell cols="3">4.6175 4.8478 4.9088</cell></row><row><cell>Faxballs</cell><cell cols="3">1.2726 1.1297 1.2951</cell></row><row><cell>Finger</cell><cell cols="3">5.4514 5.6515 5.6517</cell></row><row><cell>Gold</cell><cell cols="3">4.0362 4.1238 4.0301</cell></row><row><cell>Graphic</cell><cell cols="3">2.4044 2.5371 2.6006</cell></row><row><cell>Hotel</cell><cell cols="3">4.1504 4.0845 4.0132</cell></row><row><cell>MRI</cell><cell cols="3">6.1285 6.2442 6.2557</cell></row><row><cell>Tools</cell><cell cols="3">5.4696 5.4226 5.3845</cell></row><row><cell cols="4">Ultra-Sound 3.5248 3.1680 3.4483</cell></row><row><cell>Water</cell><cell cols="3">2.4173 2.5307 2.4834</cell></row><row><cell>Woman</cell><cell cols="3">4.5755 4.6794 4.5847</cell></row><row><cell>X-ray</cell><cell>6.1119</cell><cell cols="2">6.2113 6.1057</cell></row><row><cell>Average</cell><cell>4.259</cell><cell>4.204</cell><cell>4.229</cell></row></table><note><p>monochrome medical images and finally, Hotel and Gold are YUV video images.</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>TABLE 3 .</head><label>3</label><figDesc>Zero-order entropy of prediction errors with and without error feedback</figDesc><table><row><cell></cell><cell>GAP</cell><cell>GAP</cell></row><row><cell>Image</cell><cell cols="2">(No feedback) (with feedback)</cell></row><row><cell>Air1</cell><cell>8.7669</cell><cell>8.7127</cell></row><row><cell>Air2</cell><cell>4.8578</cell><cell>4.7518</cell></row><row><cell>Bike</cell><cell>4.0553</cell><cell>4.0185</cell></row><row><cell>Bike3</cell><cell>4.9356</cell><cell>4.9441</cell></row><row><cell>Cafe</cell><cell>5.2469</cell><cell>5.2252</cell></row><row><cell>Cats</cell><cell>3.5321</cell><cell>3.4818</cell></row><row><cell>Chart</cell><cell>2.0823</cell><cell>2.0191</cell></row><row><cell>Chart s</cell><cell>3.6389</cell><cell>3.5924</cell></row><row><cell>Compound1</cell><cell>1.9934</cell><cell>2.0000</cell></row><row><cell>Compound2</cell><cell>2.1057</cell><cell>2.1114</cell></row><row><cell>CR</cell><cell>5.2967</cell><cell>5.2528</cell></row><row><cell>CT</cell><cell>4.9088</cell><cell>4.6672</cell></row><row><cell>Faxballs</cell><cell>1.2951</cell><cell>1.2344</cell></row><row><cell>Finger</cell><cell>5.6517</cell><cell>5.5077</cell></row><row><cell>Gold</cell><cell>4.0301</cell><cell>4.0045</cell></row><row><cell>Graphic</cell><cell>2.6006</cell><cell>2.5093</cell></row><row><cell>Hotel</cell><cell>4.0132</cell><cell>3.9475</cell></row><row><cell>MRI</cell><cell>6.2557</cell><cell>6.1720</cell></row><row><cell>Tools</cell><cell>5.3845</cell><cell>5.3966</cell></row><row><cell cols="2">Ultra-Sound 3.4483</cell><cell>3.4086</cell></row><row><cell>Water</cell><cell>2.4834</cell><cell>2.4439</cell></row><row><cell>Woman</cell><cell>4.5847</cell><cell>4.5485</cell></row><row><cell>X-ray</cell><cell>6.1057</cell><cell>6.0629</cell></row><row><cell>Average</cell><cell>4.229</cell><cell>4.174</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>TABLE 4 .</head><label>4</label><figDesc>Entropy of prediction errors before and after conditioning on coding contexts</figDesc><table><row><cell></cell><cell>GAP</cell><cell>GAP</cell></row><row><cell></cell><cell>No</cell><cell>Conditioned</cell></row><row><cell>Image</cell><cell cols="2">conditioning on Q( )</cell></row><row><cell>Air1</cell><cell>8.7127</cell><cell>8.5940</cell></row><row><cell>Air2</cell><cell>4.7518</cell><cell>4.1220</cell></row><row><cell>Bike</cell><cell>4.0185</cell><cell>3.5995</cell></row><row><cell>Bike3</cell><cell>4.9441</cell><cell>4.4508</cell></row><row><cell>Cafe</cell><cell>5.2252</cell><cell>4.7753</cell></row><row><cell>Cats</cell><cell>3.4818</cell><cell>2.5518</cell></row><row><cell>Chart</cell><cell>2.0191</cell><cell>1.4355</cell></row><row><cell>Chart s</cell><cell>3.5924</cell><cell>2.8957</cell></row><row><cell>Compound1</cell><cell>2.0000</cell><cell>1.4742</cell></row><row><cell>Compound2</cell><cell>2.1114</cell><cell>1.4550</cell></row><row><cell>CR</cell><cell>5.2528</cell><cell>5.1861</cell></row><row><cell>CT</cell><cell>4.6672</cell><cell>4.0546</cell></row><row><cell>Faxballs</cell><cell>1.2344</cell><cell>1.0241</cell></row><row><cell>Finger</cell><cell>5.5077</cell><cell>5.4989</cell></row><row><cell>Gold</cell><cell>4.0045</cell><cell>3.8731</cell></row><row><cell>Graphic</cell><cell>2.5093</cell><cell>2.3658</cell></row><row><cell>Hotel</cell><cell>3.9475</cell><cell>3.7738</cell></row><row><cell>MRI</cell><cell>6.1720</cell><cell>5.9331</cell></row><row><cell>Tools</cell><cell>5.3966</cell><cell>5.0437</cell></row><row><cell cols="2">Ultra-Sound 3.4086</cell><cell>2.7810</cell></row><row><cell>Water</cell><cell>2.4439</cell><cell>1.8202</cell></row><row><cell>Woman</cell><cell>4.5485</cell><cell>4.1098</cell></row><row><cell>X-ray</cell><cell>6.0629</cell><cell>5.9305</cell></row><row><cell>Average</cell><cell>4.174</cell><cell>3.772</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>TABLE 5 .</head><label>5</label><figDesc>Bit rates (bits/pixel) of some proposed schemes on ISO test set and comparison with lossless JPEG (Huffman). Note that averages were only taken over those images with no missing entries in any column.</figDesc><table><row><cell>Image</cell><cell cols="4">CALIC LJPEG LOCO-I 1 p ALCM</cell></row><row><cell>Air1</cell><cell>8.31</cell><cell>-</cell><cell>8.50</cell><cell>8.77</cell></row><row><cell>Air2</cell><cell>3.83</cell><cell>4.90</cell><cell>4.00</cell><cell>4.08</cell></row><row><cell>Bike</cell><cell>3.50</cell><cell>4.33</cell><cell>3.59</cell><cell>3.69</cell></row><row><cell>Bike3</cell><cell>4.23</cell><cell>5.15</cell><cell>4.37</cell><cell>4.43</cell></row><row><cell>Cafe</cell><cell>4.69</cell><cell>5.63</cell><cell>4.80</cell><cell>4.99</cell></row><row><cell>Cats</cell><cell>2.51</cell><cell>3.69</cell><cell>2.59</cell><cell>2.67</cell></row><row><cell>Chart</cell><cell>1.28</cell><cell>2.23</cell><cell>1.33</cell><cell>1.27</cell></row><row><cell>Chart s</cell><cell>2.66</cell><cell>3.86</cell><cell>2.74</cell><cell>2.77</cell></row><row><cell>Compound1</cell><cell>1.24</cell><cell>2.51</cell><cell>1.30</cell><cell>1.29</cell></row><row><cell>Compound2</cell><cell>1.24</cell><cell>2.50</cell><cell>1.35</cell><cell>1.34</cell></row><row><cell>CR</cell><cell>5.17</cell><cell>-</cell><cell>5.27</cell><cell>5.43</cell></row><row><cell>CT</cell><cell>3.63</cell><cell>-</cell><cell>3.84</cell><cell>4.09</cell></row><row><cell>Faxballs</cell><cell>0.75</cell><cell>1.50</cell><cell>0.98</cell><cell>0.60</cell></row><row><cell>Finger</cell><cell>5.47</cell><cell>5.85</cell><cell>5.63</cell><cell>5.94</cell></row><row><cell>Gold</cell><cell>3.83</cell><cell>4.22</cell><cell>3.92</cell><cell>4.02</cell></row><row><cell>Graphic</cell><cell>2.26</cell><cell>2.81</cell><cell>-</cell><cell>2.41</cell></row><row><cell>Hotel</cell><cell>3.71</cell><cell>4.22</cell><cell>3.78</cell><cell>3.92</cell></row><row><cell>MRI</cell><cell>5.73</cell><cell>-</cell><cell>6.04</cell><cell>6.17</cell></row><row><cell>Tools</cell><cell>4.95</cell><cell>5.69</cell><cell>5.07</cell><cell>5.17</cell></row><row><cell>Ultra-Sound</cell><cell>2.34</cell><cell>3.63</cell><cell>2.67</cell><cell>2.32</cell></row><row><cell>Water</cell><cell>1.74</cell><cell>2.62</cell><cell>1.79</cell><cell>1.82</cell></row><row><cell>Woman</cell><cell>4.05</cell><cell>4.84</cell><cell>4.18</cell><cell>4.30</cell></row><row><cell>X-ray</cell><cell>5.83</cell><cell>-</cell><cell>5.97</cell><cell>6.24</cell></row><row><cell>Average</cell><cell>3.06</cell><cell>3.96</cell><cell>3.18</cell><cell>3.21</cell></row></table></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>ACKNOWLEDGEMENTS</head><p>The authors would like to thank the reviewers for their substantive and informed review which led to significant improvements in the manuscript. N. M. was partially supported by NSF Career award NCR 9703969.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Sunset: a hardware oriented algorithm for lossless compression of gray scale images</title>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">G</forename><surname>Langdon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Medical Imaging V: Image Capture, Formatting and Display</title>
		<imprint>
			<publisher>SPIE</publisher>
			<date type="published" when="1991">1991</date>
			<biblScope unit="volume">1444</biblScope>
			<biblScope unit="page" from="272" to="282" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Fast and efficient lossless image compression</title>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">G</forename><surname>Howard</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">S</forename><surname>Vitter</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Data Compression Conf</title>
		<editor>
			<persName><forename type="first">J</forename><forename type="middle">A</forename><surname>Storer</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">M</forename><surname>Cohn</surname></persName>
		</editor>
		<meeting>Data Compression Conf<address><addrLine>Los Alamitos, CA</addrLine></address></meeting>
		<imprint>
			<publisher>IEEE Computer Society Press</publisher>
			<date type="published" when="1993">1993</date>
			<biblScope unit="page" from="351" to="360" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Context-based lossless image compression</title>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">E</forename><surname>Tischer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">T</forename><surname>Worley</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Maeder</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Goodwin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Comp. J</title>
		<imprint>
			<biblScope unit="volume">36</biblScope>
			<biblScope unit="page" from="68" to="77" />
			<date type="published" when="1993">1993</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">Call for contributionslossless compression of continuous-tone still pictures</title>
		<idno>ISO Working Document ISO/IEC JTC1/SC29/WG1 N41</idno>
		<imprint>
			<date type="published" when="1994">1994</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">CD 14495, Lossless and near-lossless compression of continuous-tone still images (JPEG-LS)</title>
		<idno>ISO/IEC JTC1/SC29/WG1 N522</idno>
		<imprint>
			<date type="published" when="1997">1997</date>
		</imprint>
	</monogr>
	<note type="report_type">ISO Working Document</note>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">LOCO-I: a low complexity lossless image compression algorithm</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">J</forename><surname>Weinberger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Seroussi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Sapiro</surname></persName>
		</author>
		<idno>ISO/IEC JTC1/SC29/WG1 N203</idno>
		<imprint>
			<date type="published" when="1995">1995</date>
		</imprint>
	</monogr>
	<note type="report_type">ISO Working Document</note>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">LOCO-I: new developments</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">J</forename><surname>Weinberger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Seroussi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Sapiro</surname></persName>
		</author>
		<idno>ISO Working Document ISO/IEC JTC1/SC29/WG1 N245</idno>
		<imprint>
			<date type="published" when="1995">1995</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Error modeling for hierarchical lossless image compression</title>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">G</forename><surname>Howard</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">S</forename><surname>Vitter</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Data Compression Conf</title>
		<editor>
			<persName><forename type="first">J</forename><forename type="middle">A</forename><surname>Storer</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">M</forename><forename type="middle">C</forename><surname>Cohn</surname></persName>
		</editor>
		<meeting>Data Compression Conf<address><addrLine>Los Alamitos, CA</addrLine></address></meeting>
		<imprint>
			<publisher>IEEE Computer Society Press</publisher>
			<date type="published" when="1992">1992</date>
			<biblScope unit="page" from="269" to="278" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Universal modeling and coding</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">J</forename><surname>Rissanen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">G</forename><surname>Langdon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Inform. Theory, IT-27</title>
		<imprint>
			<biblScope unit="page" from="12" to="22" />
			<date type="published" when="1981">1981</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Parameter reduction and context selection for compression of gray scale images</title>
		<author>
			<persName><forename type="first">S</forename><surname>Todd</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">G</forename><surname>Langdon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">J</forename><surname>Rissanen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IBM J. Res. Develop</title>
		<imprint>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="page" from="188" to="193" />
			<date type="published" when="1985">1985</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">JPEG Still Image Data Compression Standard</title>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">B</forename><surname>Pennebaker</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">L</forename><surname>Mitchell</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1993">1993</date>
			<publisher>Van Rostrand Reinhold</publisher>
			<pubPlace>New York</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">CREW: lossless/lossy image compression-contribution to ISO/IEC JTC 1.29.12. ISO Working Document ISO/IEC</title>
		<author>
			<persName><forename type="first">M</forename><surname>Boliek</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Zandi</surname></persName>
		</author>
		<ptr target="JTC1/SC29/WG1N196" />
		<imprint>
			<date type="published" when="1995">1995</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">Proposal for lossless compression of continuous-tone still pictures: lossless transform coding for still pictures (LTC)</title>
		<author>
			<persName><forename type="first">T</forename><surname>Mochizuki</surname></persName>
		</author>
		<idno>ISO Working Document ISO/IEC JTC1/SC29/WG1 N196</idno>
		<imprint>
			<date type="published" when="1995">1995</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">Compression results-lossless, lossy ±1, lossy ±3. ISO Working Document ISO/IEC</title>
		<author>
			<persName><forename type="first">S</forename><surname>Urban</surname></persName>
		</author>
		<ptr target="JTC1/SC29/WG1N281" />
		<imprint>
			<date type="published" when="1995">1995</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Reversible compression of HDTV images using median adaptive prediction and arithmetic coding</title>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">A</forename><surname>Martucci</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Int. Symp. on Circuits and Systems</title>
		<meeting><address><addrLine>New York</addrLine></address></meeting>
		<imprint>
			<publisher>IEEE Press</publisher>
			<date type="published" when="1990">1990</date>
			<biblScope unit="page" from="1310" to="1313" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Lossless image compression-a comparative study</title>
		<author>
			<persName><forename type="first">N</forename><forename type="middle">D</forename><surname>Memon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Sayood</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Still Image Compression, SPIE Proc</title>
		<imprint>
			<date type="published" when="1995">1995</date>
			<biblScope unit="volume">2418</biblScope>
			<biblScope unit="page" from="8" to="20" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">A contextbased, adaptive, lossless/nearly-lossless coding scheme for continuous-tone images</title>
		<author>
			<persName><forename type="first">X</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><forename type="middle">D</forename><surname>Memon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Sayood</surname></persName>
		</author>
		<idno>ISO/IEC/ SC29/WG1/N256</idno>
		<imprint>
			<date type="published" when="1995">1995</date>
		</imprint>
	</monogr>
	<note type="report_type">ISO Working Document</note>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">Proposal for next generation lossless compression of continuous-tone still pictures: activity level classification model (ALCM)</title>
		<author>
			<persName><forename type="first">D</forename><surname>Speck</surname></persName>
		</author>
		<idno>ISO Working Document ISO/IEC JTC1/SC29/WG1 N198</idno>
		<imprint>
			<date type="published" when="1995">1995</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title/>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">G</forename><surname>Langdon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Speck</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Haidinyak</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Macy</surname></persName>
		</author>
		<idno>to JTC 1.29.12: JSLUG. ISO Working Document ISO/IEC JTC1/SC29/WG1 N199</idno>
		<imprint>
			<date type="published" when="1995">1995</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">A comparison of the prediction schemes proposed for a new standard on lossless coding of continuous-tone still images</title>
		<author>
			<persName><forename type="first">N</forename><forename type="middle">D</forename><surname>Memon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Sippy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Wu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ISCAS 96</title>
		<meeting>ISCAS 96<address><addrLine>New York</addrLine></address></meeting>
		<imprint>
			<publisher>IEEE Press</publisher>
			<date type="published" when="1996">1996</date>
			<biblScope unit="page" from="309" to="312" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">CLARA: continuous-tone lossless coding with edge analysis and range amplitude detection</title>
		<author>
			<persName><forename type="first">I</forename><surname>Ueno</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Ono</surname></persName>
		</author>
		<idno>ISO/IEC JTC1/SC29/WG1 N197</idno>
		<imprint>
			<date type="published" when="1995">1995</date>
		</imprint>
	</monogr>
	<note type="report_type">ISO Working Document</note>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title/>
		<author>
			<persName><forename type="first">N</forename><surname>Memon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Wu</surname></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">A proposal submitted in response to call for contributions for JTC 1</title>
		<author>
			<persName><forename type="first">B</forename><surname>Gandhi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Honsinger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Rabbani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Smith</surname></persName>
		</author>
		<idno>29.12 [JTC1/SC29/WG1 N41] ISO Working Document ISO/IEC JTC1/SC29/WG1 N204</idno>
		<imprint>
			<date type="published" when="1995">1995</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Efficient and effective lossless compression of continuous-tone images via context selection and quantization</title>
		<author>
			<persName><forename type="first">X</forename><surname>Wu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Image Processing</title>
		<imprint>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="656" to="664" />
			<date type="published" when="1997">1997</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Context-based adaptive lossless image coding</title>
		<author>
			<persName><forename type="first">X</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><forename type="middle">D</forename><surname>Memon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Commun</title>
		<imprint>
			<biblScope unit="volume">45</biblScope>
			<biblScope unit="page" from="437" to="444" />
			<date type="published" when="1997">1997</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Arithmetic coding revisited</title>
		<author>
			<persName><forename type="first">A</forename><surname>Moffat</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Neal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Witten</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Data Compression Conf</title>
		<meeting>Data Compression Conf</meeting>
		<imprint>
			<date type="published" when="1995">1995</date>
			<biblScope unit="page" from="202" to="211" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Run-length codings</title>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">W</forename><surname>Golomb</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Inform. Theory, IT</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="page" from="399" to="401" />
			<date type="published" when="1966">1966</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<title level="m" type="main">Some Practical Universal Noiseless Coding Techniques</title>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">F</forename><surname>Rice</surname></persName>
		</author>
		<idno>79-22</idno>
		<imprint>
			<date type="published" when="1979">1979</date>
			<pubPlace>Pasadena, CA</pubPlace>
		</imprint>
		<respStmt>
			<orgName>Jet Propulsion Laboratory, California Institute of Technology</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">Technical Report</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
