<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Transfer Learning Approaches for Streaming End-to-End Speech Recognition System</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date type="published" when="2020-08-12">12 Aug 2020</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Vikas</forename><surname>Joshi</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Microsoft Corporation</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Rui</forename><surname>Zhao</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Microsoft Corporation</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Rupesh</forename><forename type="middle">R</forename><surname>Mehta</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Microsoft Corporation</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Kshitiz</forename><surname>Kumar</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Microsoft Corporation</orgName>
							</affiliation>
						</author>
						<author role="corresp">
							<persName><forename type="first">Jinyu</forename><surname>Li</surname></persName>
							<email>jinyli@microsoft.com</email>
							<affiliation key="aff0">
								<orgName type="institution">Microsoft Corporation</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Transfer Learning Approaches for Streaming End-to-End Speech Recognition System</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2020-08-12">12 Aug 2020</date>
						</imprint>
					</monogr>
					<idno type="arXiv">arXiv:2008.05086v1[eess.AS]</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.2" ident="GROBID" when="2022-12-25T13:38+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>speech recognition</term>
					<term>transfer learning</term>
					<term>end-to-end systems</term>
					<term>low resource learning</term>
					<term>adaptation</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Transfer learning (TL) is widely used in conventional hybrid automatic speech recognition (ASR) system, to transfer the knowledge from source to target language. TL can be applied to end-to-end (E2E) ASR system such as recurrent neural network transducer (RNN-T) models, by initializing the encoder and/or prediction network of the target language with the pre-trained models from source language. In the hybrid ASR system, transfer learning is typically done by initializing the target language acoustic model (AM) with source language AM. Several transfer learning strategies exist in the case of the RNN-T framework, depending upon the choice of the initialization model for encoder and prediction networks. This paper presents a comparative study of four different TL methods for RNN-T framework. We show 10% − 17% relative word error rate reduction with different TL methods over randomly initialized RNN-T model. We also study the impact of TL with varying amount of training data ranging from 50 hours to 1000 hours and show the efficacy of TL for languages with a very small amount of training data.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Speech enabled applications are increasingly gaining popularity across the world. This has initiated a need to build accurate automatic speech recognition (ASR) system across different languages. Also, End-to-End (E2E) ASR systems are emerging as a popular alternative to conventional hybrid ASR systems. They replace the acoustic model (AM), language model (LM) and pronunciation model with a single neural network <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b1">2,</ref><ref type="bibr" target="#b2">3,</ref><ref type="bibr" target="#b3">4,</ref><ref type="bibr" target="#b4">5,</ref><ref type="bibr" target="#b5">6,</ref><ref type="bibr" target="#b6">7,</ref><ref type="bibr" target="#b7">8,</ref><ref type="bibr" target="#b8">9,</ref><ref type="bibr" target="#b9">10]</ref>. Recurrent neural network transducer (RNN-T) <ref type="bibr" target="#b0">[1]</ref> is one such E2E system that allow streaming input and is suitable for real-time ASR applications. Therefore there is a lot of interest in building accurate RNN-T models for different languages spoken across the world.</p><p>There is often disparity in the availability of transcribed data for different languages. In most cases, a lot more data is available for American English than other languages. The quality of ASR model depends on a number of factors including, the training data quantity and diversity, acoustic model structure, and optimization algorithm. Furthermore, training data diversity spans a number of factors in adults, kids, speaking rate, accents, near-field, and far-field acoustic conditions. A low-resource locale has limited ASR training data, and may not meet the acoustic diversity needed to train a robust model that can generalize to above acoustic factors. To overcome the lowresource constraint, transfer learning has been widely used in the hybrid ASR system to transfer the knowledge from a well trained source locale to a low-resource target locale that bring significant acoustic robustness for the target locale. In our recent work, we applied TL from a large scale en-US conventional hybrid model to the corresponding models in en-IN and it-IT locales, and achieved over 8% word error rate relative reduction (WERR). Motivated by the success of the TL methods in the hybrid ASR system, we explore TL methods to improve lowresource RNN-T models.</p><p>Besides improving the target model acoustic robustness, TL is also crucial for training large and complex deep learning architectures. RNN-T models are difficult to train <ref type="bibr" target="#b10">[11]</ref> and also require significantly large amount of data to jointly train the acoustic as well as language model attributes. In our study we have noted weaker convergence or significant parameter tuning requirements for desirable E2E training outcome for lowresource locale. Therefore we expect TL techniques to be even more relevant for E2E systems to stabilize training and improve ASR accuracy.</p><p>In the hybrid ASR system, transfer learning is typically done by initializing the target AM with the source AM. In the RNN-T framework, several transfer learning strategies exist depending upon the choice of the initialization model for the encoder and prediction networks. In this paper, we compare different transfer learning strategies in the RNN-T framework. We propose two-stage TL, by first training a target initialization model bootstrapped with a pretrained source model. Subsequently, this model is used to initialize the target RNN-T model. The two-stage TL approach shows 17% WERR reduction and faster convergence in the training loss as compared to randomly initialized RNN-T model. We also study the effect of TL with different amount of training data and show the importance of transfer learning in the case of low-resource languages.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Relation to prior work</head><p>Several methods have been proposed to improve the performance of low-resource ASR models <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b12">13,</ref><ref type="bibr" target="#b13">14,</ref><ref type="bibr" target="#b14">15,</ref><ref type="bibr" target="#b15">16,</ref><ref type="bibr" target="#b16">17,</ref><ref type="bibr" target="#b17">18,</ref><ref type="bibr" target="#b18">19,</ref><ref type="bibr" target="#b19">20]</ref>. Successful strategies include transfer learning <ref type="bibr" target="#b14">[15,</ref><ref type="bibr" target="#b15">16]</ref>, that leverage a well trained AM from high-resource language to bootstrap the low-resource AM; multi-task training <ref type="bibr" target="#b16">[17,</ref><ref type="bibr" target="#b17">18]</ref> and ensemble learning <ref type="bibr" target="#b18">[19,</ref><ref type="bibr" target="#b19">20]</ref> that aim to utilize multi-lingual data and share the model parameters. However, most of these methods are studied in the context of hybrid ASR system.</p><p>A few multi-lingual approaches are recently proposed in the E2E framework <ref type="bibr" target="#b20">[21,</ref><ref type="bibr" target="#b21">22,</ref><ref type="bibr" target="#b22">23]</ref>. Authors in <ref type="bibr" target="#b20">[21]</ref> propose the multilingual RNN-T model with language specific adapters and datasampling to handle data imbalance. Audio-to-byte E2E system is proposed in <ref type="bibr" target="#b21">[22]</ref> where bytes are used as target units instead of grapheme or word piece units, as bytes are suitable to scale to multiple languages. A transformer based multi-lingual E2E model, along with methods to incorporate language information is proposed in <ref type="bibr" target="#b22">[23]</ref>. Although multi-lingual methods are attractive to address the problem of low-resource languages, the transfer learning methods, besides being simple and effective, have the benefit of not needing the high-resource language data, but only the models trained on them. In many practical scenarios, trained models are available, however the original corpus is not. Given the simplicity and effectiveness of TL, we explore transfer learning approaches to improve the performance of low-resource RNN-T models. The rest of this paper is organized as follows: In Section 3, we briefly discuss the RNN-T model. The transfer learning methods for RNN-T are described in Section 4 and experimental setup in Section 5. Next, we discuss results in Section 6, followed by conclusions in Section 7.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">RNN Transducer model</head><p>The RNN-T model was proposed by Alex Graves <ref type="bibr" target="#b0">[1]</ref>. The RNN-T model architecture has three components; an encoder, prediction network and joint network as shown in Fig. <ref type="figure" target="#fig_2">3</ref>. The encoder maps the input acoustic feature, xt, to a high level representation, ht enc , where t represents the time index. The prediction network receives the previously predicted non-blank symbol, yu−1 and maps it to a high level representation, h pred u . The joint network is a feed forward network that combines the encoder and prediction network outputs. The posterior probability over all the targets, p(y|t, u) is obtained after softmax operation on the output of joint network. The whole network is trained jointly to minimize the RNN-T loss <ref type="bibr" target="#b0">[1]</ref>. In our implementation, the encoder consists of 6 long short-term memory (LSTM) <ref type="bibr" target="#b23">[24]</ref> layers and the prediction network has 2 LSTM layers along with the input embedding matrix. During inference, beam search decoding is used to find the most likely label sequence.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Transfer learning methods for RNN-T</head><p>The RNN-T models are difficult to train and are often initialized with the pretrained models. Initializing the encoder with connectionist temporal classification (CTC) model <ref type="bibr" target="#b1">[2]</ref> or cross entropy (CE) model <ref type="bibr" target="#b10">[11]</ref>, and the prediction network with LSTM language model (LM) is proven to be beneficial <ref type="bibr" target="#b24">[25]</ref>. Transfer learning can also be used to overcome the RNN-T training difficulty by initializing the low-resource (target) RNN-T models with the models trained on high-resource (source) languages.</p><p>A number of choices exist in selecting the initialization </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>RNN-T training</head><p>Joint network Softmax en-US word piece targets</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>RNN-T training</head><p>Figure <ref type="figure">2</ref>: en-US RNN-T initialization.</p><p>model for encoder and prediction network in the context of TL the RNN-T model. Authors in <ref type="bibr" target="#b10">[11]</ref> have shown that CE initialized RNN-T models perform better than CTC initialized models, and hence, we only explore CE models for initialization.</p><p>The following choices exist for encoder/prediction network initialization of the target RNN-T model: a) Source RNN-T encoder/prediction networks b) Pretrained networks used to initialize the source RNN-T model c) Pretrained models trained only on the target language. Therefore several combinations are possible depending upon the choice of the initialization model for encoder and prediction network.</p><p>In this paper, we explore TL methods in the context of Hindi as the target language and American English as the source language. The goal is to improve the performance of Hindi RNN-T model by leveraging models trained on American English, which has approximately ten times more data than Hindi. 'en-US' prefix is used to refer to models trained with American English and 'hi-IN' prefix is used to refer to models trained with Hindi data. We next discuss different transfer learning strategies in detail.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">en-US RNN-T initialization</head><p>The hi-IN RNN-T encoder is initialized with en-US RNN-T encoder as shown in Fig. <ref type="figure">2</ref>. The hi-IN RNN-T model is trained with hi-IN acoustic data and grapheme targets. The en-US RNN-T model is trained with en-US acoustic data and the corresponding word piece targets. The encoder of the en-US RNN-T model is in turn initialized with a pretrained en-US CE model. Note that the prediction network of both hi-IN and en-US RNN-T models are randomly initialized. After initialization, all parameters of the RNN-T model are trained to minimize the RNN-T loss. In all the figures, layers initialized with pretrained networks are represented by cross lined blocks and randomly initialized layers are represented with plain blocks. gets (necessary for CE training), is obtained from word level alignments as discussed in <ref type="bibr" target="#b10">[11]</ref>. From the word alignments, the start frame, end frame and total number of frames corresponding to each word is known. The words are then divided into corresponding word pieces, and equal number of frames are allocated to each word piece within the boundary of the frames corresponding to the word. In this scheme, the hi-IN RNN-T prediction network is randomly initialized.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">en-US CE initialization</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Two-stage transfer learning</head><p>Transfer learning can be done in two stages as shown in Fig. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.">Encoder and prediction network initialization</head><p>In the previously described transfer learning methods, only the encoder is initialized with a pretrained model. In this section, we will discuss initializing both encoder and prediction network with pretrained models as shown in Fig. <ref type="figure" target="#fig_4">4</ref>. The prediction network is initialized with a pretrained LSTM LM which is trained on a text corpus (obtained from different external sources) as a language model using grapheme units, referred to as hi-IN LM. The sentence count (number of repetitions of the sentence or the query) differ significantly from one source to the other. In order to avoid biasing the LM towards the source with large sentence counts, we only select unique sentences for LM training. We did not explore initializing the prediction network with en-US LSTM LM, as en-US and hi-IN lexical units differ (grapheme vs word piece), resulting in the input embedding matrices being significantly different, and thereby initializing the prediction network with en-US LSTM LM might not be beneficial. We also did not experiment initializing encoder with en-US RNN-T model in the context of encoder and prediction network initialization, as our experiments suggested that en-US CE initialization is better than en-US RNN-T initialization as discussed later in Section 6.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Experimental setup</head><p>The hi-IN models are trained with approximately 4 million utterances amounting to few thousand hours of speech data. The speech data is distorted by noise to achieve robustness to noisy conditions. The en-US models were trained with 65000 hours of data. The hi-IN test set contains 17619 utterances consisting of five different scenarios including phrasal, conversational and code-mixed utterances. Training and test utterances are anonymized to remove any personally identifiable information.</p><p>We use 80-dimensional log Mel filter bank features computed every 10 milliseconds (ms). Eight vectors are stacked together to form 640-dimensional acoustic features fed to the encoder. The frames are shifted by 30ms. All encoders have six LSTM layers with 1600 hidden dimension and 800 projection dimension. All prediction networks have two LSTM layers with same cell dimension as encoders.</p><p>The hi-IN grapheme targets contain all the unique graphemes in the Hindi native script. We also include grapheme targets with B prefix to be able to segment the grapheme se- quence into word sequence. Some research works use &lt;space&gt; symbol, however, we observed better accuracy with B prefix based grapheme targets. A total of 130 grapheme targets are obtained by combining the the original Hindi graphemes, graphemes with B prefix and &lt;blank&gt; symbol. The word piece targets for en-US model is obtained by using byte pair encoding <ref type="bibr" target="#b25">[26]</ref> algorithm as described in <ref type="bibr" target="#b10">[11]</ref>.</p><p>We also report the word error rate (WER) on hybrid ASR model trained with same amount of data. The AM consists of 6 layers of latency-controlled bidirectional LSTM <ref type="bibr" target="#b26">[27]</ref> with 1024 hidden dimension and 512 projection dimension. AM is CE trained followed by EMBR training. The softmax layer has 9212 senone labels. 80-dimensional log Mel filter bank features are computed every 10ms. Frame skipping is done by a factor of 2. Run-time decoding is performed using a 5-gram language model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Discussion of results</head><p>Table <ref type="table">.</ref> 1 shows WER for different transfer learning methods on hi-IN test sets. en-US CE initialization outperforms random initialization with 15.6% relative WER (WERR) reduction. en-US RNN-T initialization is better than random, while is slightly inferior to en-US CE initialization. This could be because the en-US RNN-T encoder representations are influenced by en-US prediction network representations, as they are trained jointly. However, en-US CE model is trained in isolation and could serve as a better initialization model for the encoder. Two-stage transfer learning with grapheme targets performs better than the rest of the methods with 17.4% WERR reduction over random initialization. The pretraining method, hi-IN CE + hi-IN LM initialization improves over random initialization showing the importance of pretraining. The en-US CE + hi-IN LM initialization is better than pretraining, however, is inferior compared to en-US CE initialization, contrary to our expectation. The reason for such a behaviour is not known and we look to investigate this further in the future. With the improvements obtained from transfer learning, the WER of hi-IN RNN-T model (with transfer learning) is in parity with the hybrid model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.1.">Efficacy of TL for different amount of training data</head><p>To study the efficacy of the TL with different amount of training data, we sample the original hi-IN data into smaller datasets consisting of 50 hours, 500 hours and 1000 hours. The RNN-T models are trained with random initialization and en-US CE initialization.   in the next section. The above results also show a need for larger training data in RNN-T models.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.2.">RNN-T training convergence</head><p>The training loss with increasing epochs for random initialization, en-US CE initialization and Two-stage TL with grapheme targets is shown in Fig. <ref type="figure" target="#fig_6">5</ref>. Each epoch is trained with 60 hours of non-overlapping speech data. The training parameters such as learning rate, mini-batch size are identical for all three methods shown in Fig. <ref type="figure" target="#fig_6">5</ref>. The training loss converges much faster with the transfer learning methods than random initialized models. The Two-stage TL converges faster than en-US CE initialization. It is also interesting to note that the first epoch training loss is much lower for Two-stage TL than others, thereby indicating the superiority of initialization models in Two-stage TL.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.">Conclusion</head><p>In this paper, we explore transfer learning methods for RNN-T models. Our motivation is to leverage well-trained en-US models to bootstrap hi-IN RNN-T models and also to stabilize In future, we plan to explore other transfer learning methods and its extension to multi-lingual RNN-T models.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: The RNN-T model.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: Two stage transfer learning.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>3 .</head><label>3</label><figDesc>In the first stage, hi-IN CE model is trained starting from en-US CE model. Subsequently, the hi-IN RNN-T model is trained by initializing the encoder with hi-IN CE model. The hi-IN CE model can be trained either with senone and grapheme targets as depicted in Fig. 3. The senone based CE model can distinguish more finer acoustic classes as senones represent much finer acoustic information than graphemes. However, the grapheme based CE model is better aligned with the RNN-T model as they both are trained with grapheme targets. The prediction network is again randomly initialized.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 4 :</head><label>4</label><figDesc>Figure 4: Encoder and prediction network initialization. Two transfer schemes are shown in this figure: a) hi-IN CE + hi-IN LM initialization where the encoder is initialized with hi-IN CE model b) en-US CE + hi-IN LM initialization where the encoder is initialized with en-US CE model.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 5 :</head><label>5</label><figDesc>Figure 5: Training loss with increasing epoch number for random initialization, en-US CE initialization and Two stage TL with grapheme targets.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head></head><label></label><figDesc>the hi-IN RNN-T model training. We evaluated the following transfer learning methods: a) en-US CE initialization b) en-US RNN-T initialization c) Two-stage transfer learning and d) Encoder and prediction network initialization. Based on the WER gains and training convergence, we propose Two-stage learning approach with grapheme targets as the preferred transfer learning strategy. The experiments on smaller data-sets and training loss convergence reveal the importance of transfer learning for low-resource RNN-T models. The methods discussed in this paper can be generalized to other low-resource languages as well.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 :</head><label>1</label><figDesc>WER [%] on hi-IN test sets for random initialization, different transfer learning methods and the hybrid model.</figDesc><table><row><cell>Experiment</cell><cell>WER</cell></row><row><cell>Random initialization</cell><cell>26.53</cell></row><row><cell>en-US RNN-T initialization</cell><cell>22.97</cell></row><row><cell>en-US CE initialization</cell><cell>22.38</cell></row><row><cell>Two-stage initialization with senone targets</cell><cell>22.31</cell></row><row><cell cols="2">Two-stage initialization with grapheme targets 21.89</cell></row><row><cell>hi-IN CE + hi-IN LM initialization</cell><cell>24.29</cell></row><row><cell>en-US CE + hi-IN LM initialization</cell><cell>22.63</cell></row><row><cell>Hybrid model</cell><cell>22.32</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 :</head><label>2</label><figDesc>The corresponding WERs on hi-IN testset are shown in Table. 2. en-US CE initialization shows 42.7% WERR reduction for 50 hour training over random initialization. The large WER gains with smaller training sets could be due to better RNN-T training convergence with TL as discussed WER [%] comparison on hi-IN test set between random and en-US CE initialized RNN-T models trained with 50 hours, 500 hours and 1000 hours.</figDesc><table><row><cell></cell><cell cols="3">50 hours 500 hours 1000 hours</cell></row><row><cell>Random initialization</cell><cell>83.77</cell><cell>69.32</cell><cell>51.56</cell></row><row><cell>en-US CE initialization</cell><cell>47.96</cell><cell>35.07</cell><cell>32.75</cell></row></table></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">Sequence transduction with recurrent neural networks</title>
		<author>
			<persName><forename type="first">A</forename><surname>Graves</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1211.3711</idno>
		<imprint>
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Speech recognition with deep recurrent neural networks</title>
		<author>
			<persName><forename type="first">A</forename><surname>Graves</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Mohamed</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2013 IEEE International Conference on Acoustics, Speech and Signal Processing</title>
				<imprint>
			<date type="published" when="2013">2013</date>
			<biblScope unit="page" from="6645" to="6649" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Listen, attend and spell: A neural network for large vocabulary conversational speech recognition</title>
		<author>
			<persName><forename type="first">W</forename><surname>Chan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Jaitly</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Vinyals</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2016 IEEE International Conference on Acoustics, Speech and Signal Processing</title>
				<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="4960" to="4964" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">A comparison of sequence-to-sequence models for speech recognition</title>
		<author>
			<persName><forename type="first">R</forename><surname>Prabhavalkar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Rao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Sainath</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Jaitly</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of Interspeech</title>
				<meeting>Interspeech</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">State-of-the-art speech recognition with sequence-to-sequence models</title>
		<author>
			<persName><forename type="first">C</forename><surname>Chiu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">N</forename><surname>Sainath</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Prabhavalkar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Nguyen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Kannan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">J</forename><surname>Weiss</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Rao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Gonina</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Jaitly</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Chorowski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Bacchiani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2018 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)</title>
				<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="4774" to="4778" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Exploring neural transducers for end-to-end speech recognition</title>
		<author>
			<persName><forename type="first">E</forename><surname>Battenberg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Child</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Coates</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><forename type="middle">G Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Satheesh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Sriram</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Zhu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2017 IEEE Automatic Speech Recognition and Understanding Workshop</title>
				<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="206" to="213" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Improving the performance of online neural transducer models</title>
		<author>
			<persName><forename type="first">T</forename><surname>Sainath</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C.-C</forename><surname>Chiu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Prabhavalkar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Kannan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Nguyen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Chen</surname></persName>
		</author>
		<ptr target="https://arxiv.org/pdf/1712.01807.pdf" />
	</analytic>
	<monogr>
		<title level="m">Proc. ICASSP</title>
				<meeting>ICASSP</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Streaming end-to-end speech recognition for mobile devices</title>
		<author>
			<persName><forename type="first">Y</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">N</forename><surname>Sainath</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Prabhavalkar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Mcgraw</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Alvarez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Rybach</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Kannan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Pang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Bhatia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Shangguan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Pundak</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">C</forename><surname>Sim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Bagby</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Rao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Gruenstein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICASSP 2019 -2019 IEEE International Conference on Acoustics, Speech and Signal Processing</title>
				<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="6381" to="6385" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Improving RNN transducer modeling for end-to-end speech recognition</title>
		<author>
			<persName><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Gong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ASRU</title>
				<meeting>ASRU</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">A streaming on-device end-to-end model surpassing server-side conventional model quality and latency</title>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">N</forename><surname>Sainath</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Narayanan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Pang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Bruguier</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Alvarez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C.-C</forename><surname>Chiu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Garcia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Gruenstein</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Kannan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Mcgraw</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Peyser</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Prabhavalkar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Pundak</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Rybach</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Shangguan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Sheth</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Strohman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Visontai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Zhao</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Exploring pre-training with alignments for rnn transducer based end-to-end speech recognition</title>
		<author>
			<persName><forename type="first">H</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Gong</surname></persName>
		</author>
		<ptr target="https://www.microsoft.com/en-us/research/publication/exploring-pre-training-with-alignments-for-rnn-transducer-based-end-to-end-speech-recognition/" />
	</analytic>
	<monogr>
		<title level="j">ICASSP</title>
		<imprint>
			<date type="published" when="2020-04">April 2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Multilingual mlp features for low-resource lvcsr systems</title>
		<author>
			<persName><forename type="first">S</forename><surname>Thomas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Ganapathy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Hermansky</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2012 IEEE International Conference on Acoustics, Speech and Signal Processing</title>
				<imprint>
			<date type="published" when="2012">2012</date>
			<biblScope unit="page" from="4269" to="4272" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Multilingual acoustic models using distributed deep neural networks</title>
		<author>
			<persName><forename type="first">G</forename><surname>Heigold</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Vanhoucke</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Senior</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Nguyen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Ranzato</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Devin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Dean</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2013 IEEE International Conference on Acoustics, Speech and Signal Processing</title>
				<imprint>
			<date type="published" when="2013">2013</date>
			<biblScope unit="page" from="8619" to="8623" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Multi-Dialect Acoustic Modeling Using Phone Mapping and Online i-Vectors</title>
		<author>
			<persName><forename type="first">H</forename><surname>Arsikere</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Sapru</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Garimella</surname></persName>
		</author>
		<idno type="DOI">10.21437/Interspeech.2019-2881</idno>
		<ptr target="http://dx.doi.org/10.21437/Interspeech.2019-2881" />
	</analytic>
	<monogr>
		<title level="m">Proc. Interspeech</title>
				<meeting>Interspeech</meeting>
		<imprint>
			<date type="published" when="2019">2019. 2019</date>
			<biblScope unit="page" from="2125" to="2129" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Transfer learning for speech recognition on a budget</title>
		<author>
			<persName><forename type="first">J</forename><surname>Kunze</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Kirsch</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Kurenkov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Krug</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Johannsmeier</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Stober</surname></persName>
		</author>
		<ptr target="https://www.aclweb.org/anthology/W17-2620" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2nd Workshop on Representation Learning for NLP</title>
				<meeting>the 2nd Workshop on Representation Learning for NLP<address><addrLine>Vancouver, Canada</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2017-08">Aug. 2017</date>
			<biblScope unit="page" from="168" to="177" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">Multi-accent deep neural network acoustic model with accent-specific top layer using the kld-regularized model adaptation</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Gong</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014-09">2014. September 2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Efficient and effective algorithms for training single-hidden-layer neural networks</title>
		<author>
			<persName><forename type="first">D</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Deng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pattern Recognition Letters</title>
		<imprint>
			<date type="published" when="2012-01">January 2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Cross-language knowledge transfer using multilingual deep neural network with shared hidden layers</title>
		<author>
			<persName><forename type="first">J</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Gong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2013 IEEE International Conference on Acoustics, Speech and Signal Processing</title>
				<imprint>
			<date type="published" when="2013">2013</date>
			<biblScope unit="page" from="7304" to="7308" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Ensemble deep learning for speech recognition</title>
		<author>
			<persName><forename type="first">L</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">C</forename><surname>Platt</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">INTERSPEECH</title>
				<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Towards acoustic model unification across dialects</title>
		<author>
			<persName><forename type="first">A</forename><surname>Waters</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Bastani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">G</forename><surname>Elfeky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Moreno</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Velez</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2016 IEEE Workshop on Spoken Language Technology</title>
				<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">Large-scale multilingual speech recognition with a streaming end-to-end model</title>
		<author>
			<persName><forename type="first">A</forename><surname>Kannan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Datta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Sainath</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Weinstein</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Ramabhadran</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Bapna</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Chen</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Bytes are all you need: End-to-end multilingual speech recognition and synthesis with bytes</title>
		<author>
			<persName><forename type="first">B</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Sainath</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Chan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICASSP 2019 -2019 IEEE International Conference on Acoustics, Speech and Signal Processing</title>
				<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="5621" to="5625" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Improving the performance of transformer based low resource speech recognition for indian languages</title>
		<author>
			<persName><forename type="first">V</forename><forename type="middle">M</forename><surname>Shetty</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Sagaya Mary</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N J</forename></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Umesh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICASSP 2020 -2020 IEEE International Conference on Acoustics, Speech and Signal Processing</title>
				<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="8279" to="8283" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Long short-term memory</title>
		<author>
			<persName><forename type="first">S</forename><surname>Hochreiter</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Schmidhuber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural computation</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="1735" to="1780" />
			<date type="published" when="1997">1997</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Exploring architectures, data and units for streaming end-to-end speech recognition with rnn-transducer</title>
		<author>
			<persName><forename type="first">K</forename><surname>Rao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Sak</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Prabhavalkar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2017 IEEE Automatic Speech Recognition and Understanding Workshop</title>
				<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="193" to="199" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Neural machine translation of rare words with subword units</title>
		<author>
			<persName><forename type="first">R</forename><surname>Sennrich</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Haddow</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Birch</surname></persName>
		</author>
		<ptr target="https://www.aclweb.org/anthology/P16-1162" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics</title>
		<title level="s">Long Papers</title>
		<meeting>the 54th Annual Meeting of the Association for Computational Linguistics<address><addrLine>Berlin, Germany</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2016-08">Aug. 2016</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="1715" to="1725" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Highway long short-term memory rnns for distant speech recognition</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Yaco</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Khudanpur</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Glass</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2016 IEEE International Conference on Acoustics, Speech and Signal Processing</title>
				<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="5755" to="5759" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
