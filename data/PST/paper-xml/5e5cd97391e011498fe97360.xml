<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Generalizing Convolutional Neural Networks for Equivariance to Lie Groups on Arbitrary Continuous Data</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date type="published" when="2020-05-14">14 May 2020</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Marc</forename><surname>Finzi</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">New York University</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Samuel</forename><surname>Stanton</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">New York University</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Pavel</forename><surname>Izmailov</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">New York University</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Andrew</forename><forename type="middle">Gordon</forename><surname>Wilson</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">New York University</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Generalizing Convolutional Neural Networks for Equivariance to Lie Groups on Arbitrary Continuous Data</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2020-05-14">14 May 2020</date>
						</imprint>
					</monogr>
					<idno type="arXiv">arXiv:2002.12880v2[stat.ML]</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.2" ident="GROBID" when="2022-12-25T13:28+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>The translation equivariance of convolutional layers enables convolutional neural networks to generalize well on image problems. While translation equivariance provides a powerful inductive bias for images, we often additionally desire equivariance to other transformations, such as rotations, especially for non-image data. We propose a general method to construct a convolutional layer that is equivariant to transformations from any specified Lie group with a surjective exponential map. Incorporating equivariance to a new group requires implementing only the group exponential and logarithm maps, enabling rapid prototyping. Showcasing the simplicity and generality of our method, we apply the same model architecture to images, ball-and-stick molecular data, and Hamiltonian dynamical systems. For Hamiltonian systems, the equivariance of our models is especially impactful, leading to exact conservation of linear and angular momentum.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Symmetry pervades the natural world. The same law of gravitation governs a game of catch, the orbits of our planets, and the formation of galaxies. It is precisely because of the order of the universe that we can hope to understand it. Once we started to understand the symmetries inherent in physical laws, we could predict behavior in galaxies billions of light-years away by studying our own local region of time and space. For statistical models to achieve their full potential, it is essential to incorporate our knowledge of naturally occurring symmetries into the design of algorithms and architectures. An example of this principle is the translation equivariance of convolutional layers in neural networks <ref type="bibr" target="#b12">(LeCun et al., 1995)</ref>: when an input (e.g. an image) is translated, the output of a convolutional layer is translated in the same way.</p><p>Group theory provides a mechanism to reason about symmetry and equivariance. Convolutional layers are equivariant Figure <ref type="figure">1</ref>. Many modalities of spatial data do not lie on a grid, but still possess important symmetries. We propose a single model to learn from continuous spatial data that can be specialized to respect a given continuous symmetry group.</p><p>to translations, and are a special case of group convolution. A group convolution is a general linear transformation equivariant to a given group, used in group equivariant convolutional networks <ref type="bibr">(Cohen and Welling, 2016a)</ref>.</p><p>In this paper, we develop a general framework for equivariant models on arbitrary continuous (point) data represented as coordinates and values {(x i , f i )} N i=1 . Point data is a broad category, including ball-and-stick representations of molecules, the coordinates of a dynamical system, and images (shown in Figure <ref type="figure">1</ref>). When the inputs or group elements lie on a grid (e.g., image data) one can simply enumerate the values of the convolutional kernel at each group element. But in order to extend to continuous data, we define the convolutional kernel to be a continuous function on the group parameterized by a neural network.</p><p>We consider the large class of continuous groups known as Lie groups. In most cases, Lie groups can be parameterized in terms of a vector space of infinitesimal generators (the Lie algebra) via the logarithm and exponential maps. Many useful transformations are Lie groups, including translations, rotations, and scalings. We propose LieConv, a convolutional layer that can be made equivariant to a given Lie group by defining exp and log maps. We demonstrate the expressivity and generality of LieConv with experiments on images, molecular data, and dynamical systems. We emphasize that we use the same network architecture for all transformation groups and data types. LieConv achieves</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Work</head><p>One approach to constructing equivariant CNNs, first introduced in <ref type="bibr">Cohen and Welling (2016a)</ref>, is to use standard convolutional kernels and transform them or the feature maps for each of the elements in the group. For discrete groups this approach leads to exact equivariance and uses the so-called regular representation of the group <ref type="bibr">(Cohen et al., 2019)</ref>. This approach is easy to implement, and has also been used when the feature maps are vector fields <ref type="bibr">(Zhou et al., 2017;</ref><ref type="bibr" target="#b14">Marcos et al., 2017)</ref>, and with other representations <ref type="bibr">(Cohen and Welling, 2016b)</ref>, but only on image data where locations are discrete and the group cardinality is small. This approach has the disadvantage that the computation grows quickly with the size of the group, and some groups like 3D rotations cannot be easily discretized onto a lattice that is also a subgroup.</p><p>Another approach, drawing on harmonic analysis, finds a basis of equivariant functions and parametrizes convolutional kernels in that basis <ref type="bibr">(Worrall et al., 2017;</ref><ref type="bibr" target="#b24">Weiler and Cesa, 2019;</ref><ref type="bibr" target="#b2">Jacobsen et al., 2017)</ref>. These kernels can be used to construct networks that are exactly equivariant to continuous groups. While the approach has been applied on general data types like spherical images <ref type="bibr">(Esteves et al., 2018;</ref><ref type="bibr">Cohen et al., 2018;</ref><ref type="bibr" target="#b4">Jiang et al., 2019)</ref>, voxel data <ref type="bibr">(Weiler et al., 2018)</ref>, and point clouds <ref type="bibr" target="#b23">(Thomas et al., 2018;</ref><ref type="bibr">Anderson et al., 2019)</ref>, the requirement of working out the representation theory for the group can be cumbersome and time-consuming, and is limited to compact groups. Our approach reduces the amount of work to implement equivariance to a new group, encouraging rapid prototyping.</p><p>There is also work applying Lie group theory to deep neural networks. <ref type="bibr" target="#b1">Huang et al. (2017)</ref> define a network where the intermediate activations of the network are 3D rotations representing skeletal poses and embed elements into the Lie algebra using the log map. Bekkers (2019) use B-splines to define convolutional kernels acting on a Lie algebra which they evaluate on a grid and apply to image problems. However, their method is not readily applicable to point data and cannot be used when the input space is not a homogeneous space of the group. Both of these issues are addressed by our work.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Background</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Equivariance</head><p>A mapping h(•) is equivariant to a set of transformations G if when we apply any transformation g to the input of h, the output is also transformed by g. The most common example of equivariance in deep learning is the translation equivariance of convolutional layers: if we translate the input image by an integer number of pixels in x and y, the output is also translated by the same amount (ignoring the regions close to the boundary of the image). Formally, if h : A → A, and G is a set of transformations acting on A, we say h is equivariant to G if ∀a ∈ A, ∀g ∈ G, h(ga) = gh(a).</p><p>(1)</p><p>The continuous convolution of a function f : R → R with the kernel k : R → R is equivariant to translations in the sense that L t (k * f ) = k * L t f where L t translates the inputs of the function by t: L t f (x) = f (x + t).</p><p>It is easy to construct invariant functions, where transformations on the input do not affect the output, by simply discarding information. It is not easy to construct equivariant transformations, but it is necessary. Strict invariance unnecessarily limits the expressive power by discarding relevant information.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Groups of Transformations and Lie Groups</head><p>Many important sets of transformations form a group. To form a group the set must be closed under composition, include an identity transformation, each element must have an inverse, and composition must be associative. The set of 2D rotations, SO(2), is a simple and instructive example.</p><p>Composing two rotations r 1 and r 2 , yields another rotation r = r 2 • r 1 . There exists an identity id ∈ G that maps every point in R 2 to itself (i.e., rotation by a zero angle). And for every rotation r, there exists an inverse rotation r −1 such that r • r −1 = r −1 • r = id. Finally, the composition of rotations is an associative operation:</p><formula xml:id="formula_0">(r 1 • r 2 ) • r 3 = r 1 • (r 2 • r 3 ). So we see that SO(2) is indeed a group.</formula><p>We can also adopt a more familiar view of SO(2) in terms of angles, where a rotation matrix R : R → R 2×2 is parametrized as R(θ) = exp(Jθ). J is the antisymmetric matrix J = 0 −1 1 0 (an infinitesimal generator of the group) and exp is the matrix exponential. Note that θ is totally unconstrained. Using R(θ) we can add and subtract rotations. Given</p><formula xml:id="formula_1">θ 1 , θ 2 we can compute R(θ 1 ) −1 R(θ 2 ) = exp(−Jθ 1 + Jθ 2 ) = R(θ 2 − θ 1 ). R(θ) = exp(Jθ)</formula><p>is an example of the Lie algebra parametrization of a group, and SO(2) forms a Lie group.</p><p>More generally, a Lie group is a group whose elements form a smooth manifold. Since G is not necessarily a vector space, we cannot add or subtract group elements. However, the Lie algebra of G, the tangent space at the identity, g = T id G, is a vector space and can be understood informally as a space of infinitesimal transformations from the group. As a vector space, one can readily expand elements in a basis A = k a k e k and use the components for calculations. The Lie bracket between two elements in g, [A, B] = AB − BA, measures the extent to which the infinitesimal transformations fail to commute.</p><p>The exponential map exp : g → G gives a mapping from the Lie algebra to the Lie group, converting infinitesimal transformations to group elements. In many cases, the image of the exponential map covers the group, and an inverse mapping log : G → g can be defined. For matrix groups the exp map coincides with the matrix exponential (exp(A) = I + A + A 2 /2! + ... ), and the log map with the matrix logarithm. Matrix groups are particularly amenable to our method because in many cases the exp and log maps can be computed in closed form. For example, there are analytic solutions for the translation group T(d), the 3D rotation group SO(3), the translation and rotation group SE(d) for d = 2, 3, the rotation-scale group R * × SO(2), and many others (Eade, 2014). In the event that an analytic solution is not available there are reliable numerical methods at our disposal <ref type="bibr" target="#b15">(Moler and Van Loan, 2003)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Group Convolutions</head><p>Adopting the convention of left equivariance, one can define a group convolution between two functions on the group, which generalizes the translation equivariance of convolution to other groups:</p><p>Definition 1. Let g, f : G → R, and µ(•) be the Haar measure on G. For any u ∈ G, the convolution of g and f on G at u is given by</p><formula xml:id="formula_2">h(u) = (g * f )(u) = G g(v −1 u)f (v)dµ(v).</formula><p>(2) <ref type="bibr" target="#b7">(Kondor and Trivedi, 2018;</ref><ref type="bibr">Cohen et al., 2019)</ref> </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4.">PointConv Trick</head><p>In order to extend learnable convolution layers to point clouds, not having the regular grid structure in images, <ref type="bibr">Dai et al. (2017)</ref>, <ref type="bibr" target="#b22">Simonovsky and</ref><ref type="bibr">Komodakis (2017), and</ref><ref type="bibr">Wu et al. (2019)</ref> go back to the continuous definition of a convolution for a single channel between a learned function (convolutional filter) g θ (•) : R d → R cout×cin and an input feature map f (•) :</p><formula xml:id="formula_3">R d → R cin yielding the function h(•) : R d → R cout , h(x) = (g θ * f )(x) = g θ (x − y)f (y)dy.<label>(3)</label></formula><p>We approximate the integral using a discretization:</p><formula xml:id="formula_4">h(x i ) = (V /n) j g θ (x i − x j )f (x j ) . (<label>4</label></formula><formula xml:id="formula_5">)</formula><p>Here V is the volume of the space integrated over and n is the number of quadrature points. In a 3 × 3 convolutional layer for images, where points fall on a uniform square grid, the filter g θ has independent parameters for each of the inputs (−1, −1), (−1, 0), . . . , (1, 1). In order to accommodate points that are not on a regular grid, g θ can be parametrized as a small neural network, mapping input offsets to filter matrices, explored with MLPs in <ref type="bibr" target="#b22">Simonovsky and Komodakis (2017)</ref>. The compute and memory costs has severely limited this approach, for typical CIFAR-10 images with batchsize = 32, N = 32 × 32, c in = c out = 256, n = 3×3, evaluating a single layer requires computing 20 billion values for g.</p><p>In <ref type="bibr">PointConv, Wu et al. (2019)</ref> develop a trick where clever reordering of the computation cuts memory and computational requirements by ∼ 2 orders of magnitude, allowing them to scale to the point cloud classification, segmentation datasets ModelNet40 and ShapeNet, and the image dataset CIFAR-10. We review and generalize the Efficient-PointConv trick in Appendix A.1, which we will use to accelerate our method.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Convolutional Layers on Lie Groups</head><p>We now introduce LieConv, a new convolutional layer that can be made equivariant to a given Lie group. Models with LieConv layers can act on arbitrary collections of coordinates and values {(x i , f i )} N i=1 , for x i ∈ X and f i ∈ V where V is a vector space. This includes point cloud data (a collection of points forming an object in R 3 ), featurized ball and stick molecular data, the spatial arrangement of a multibody system, and image data.</p><p>We begin with a high-level overview of the method. In Section 4.1 we discuss transforming raw inputs</p><formula xml:id="formula_6">x i into group O x (a) Data O t x → [∅, x] (b) Trivial O t x → [t, ∅] (c) T (2) O φ r x → cos φ − sin φ sin φ cos φ , r (d) SO(2) O r φ x → r cos φ −r sin φ r sin φ r cos φ , ∅ (e) R * × SO(2) O φ 1 t 1 φ 2 t 2 x → cos φi − sin φi txi sin φi cos φi tyi 0 0 1 , ∅ (f) SE(2)</formula><p>Figure <ref type="figure">2</ref>. Visualization of the lifting procedure. Panel (a) shows a point x in the original input space X . In panels (b)-(f) we illustrate the lifted embeddings for different groups in the form <ref type="bibr">[u, q]</ref>, where u ∈ G is an element of the group and q ∈ X /G identifies the orbit (see Section 4.5). For SE(2) the lifting is multi-valued.</p><p>elements u i on which we can perform group convolution. We refer to this process as lifting. Section 4.2 addresses the irregular and varied arrangements of group elements that result from lifting arbitrary continuous input data by parametrizing the convolutional kernel g as a neural network. In Section 4.3, we show how to enforce the locality of the kernel by defining an invariant distance on the group. In Section 4.4, we define a Monte Carlo estimator for the group convolution integral in Eq. ( <ref type="formula">2</ref>) and show that this estimator is equivariant in distribution. In Section 4.5, we extend the procedure to cases where the group does not cover the input space (i.e., when we cannot map any point to any other point with a transformation from the group). Additionally, in Appendix A.2, we show that our method generalizes coordinate transform equivariance when G is Abelian. At the end of Section 4.5 we provide a concise algorithmic description of the lifting procedure and our new convolution layer.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Lifting from X to G</head><p>If X is a homogeneous space of G, then every two elements in X are connected by an element in G, and one can lift elements by simply picking an origin o and defining Lift(x) = {u ∈ G : uo = x}: all elements in the group that map the origin to x. This procedure enables lifting tuples of coordinates and features {(</p><formula xml:id="formula_7">x i , f i )} N i=1 → {(u ik , f i )} N,K i=1,k=1</formula><p>, with up to K group elements for each input. <ref type="foot" target="#foot_0">1</ref> To find all the elements {u ∈ G : uo = x}, one simply needs to find one element u x and use the elements in the stabilizer of the origin H = {h ∈ G : ho = o}, to generate the rest with Lift(x) = {u x h for h ∈ H}. For continuous groups the stabilizer may be infinite, and in these cases we sample uniformly using the Haar measure µ which is described in Appendix C.2. We visualize the lifting procedure for different groups in Figure <ref type="figure">2</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Parameterization of the Kernel</head><p>The conventional method for implementing an equivariant convolutional network (Cohen and Welling, 2016a) requires enumerating the values of g(•) over the elements of the group, with separate parameters for each element. This procedure is infeasible for irregularly sampled data and problematic even for a discretization because there is no generalization between different group elements. Instead of having a discrete mapping from each group element to the kernel values, we parametrize the convolutional kernel as a continuous function g θ using a fully connected neural network with Swish activations, varying smoothly over the elements in the Lie group. However, as neural networks are best suited to learn on euclidean data and G does not form a vector space, we propose to model g by mapping onto the Lie Algebra g, which is a vector space, and expanding in a basis for the space. To do so, we restrict our attention in this paper to Lie groups whose exponential maps are surjective, where every element has a logarithm. This means defining g θ (u) = (g • exp) θ (log u), where gθ = (g • exp) θ is the function parametrized by an MLP, gθ : g → R cout×cin . Surjectivity of the exp map guarantees that exp • log = id, although not in the other order.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Enforcing Locality</head><p>Important both to the inductive biases of convolutional neural networks and their computational efficiency is the fact that convolutional filters are local, g θ (u i − u j ) = 0 for u i − u j &gt; r. In order to quantify locality on matrix groups, we introduce the function:</p><formula xml:id="formula_8">d(u, v) := log(u −1 v) F , (<label>5</label></formula><formula xml:id="formula_9">)</formula><p>where log is the matrix logarithm, and F is the Frobenius norm. The function is left invariant, since d(wu, wv) = log(u −1 w −1 wv) F = d(u, v), and is a semi-metric (it does not necessarily satisfy the triangle inequality). In Appendix A.3 we show the conditions under which d(u, v)  <ref type="formula">2</ref>), in terms of the points in the input space. For the computation of h at the point in orange, elements are sampled from colored region. Notice that the same points enter the calculation when the image is transformed by a rotation and scaling. We visualize the neighborhoods for other groups in Appendix C.6.</p><p>is additionally the distance along the geodesic connecting u, v from a left invariant metric tensor A, B u = Tr(A T u −T u −1 B) (satisfied for all groups we use except SE(d)), a generalization of the well known formula for the geodesic distance between rotations log(R T 1 R 2 ) F (Kuffner, 2004).</p><p>To enforce that our learned convolutional filter g is local, we can use our definition of distance to only evaluate the sum for d(u, v) &lt; r, implicitly setting</p><formula xml:id="formula_10">g θ (v −1 u) = 0 outside a local neighborhood nbhd(u) = {v : d(u, v) ≤ r}, h(u) = v∈nbhd(u) g θ (v −1 u)f (v)dµ(v).<label>(6)</label></formula><p>This restriction to a local neighborhood does not break equivariance precisely because</p><formula xml:id="formula_11">d(•, •) is left invariant. Since d(u, v) = d(v −1 u, id) this restriction is equiva- lent to multiplying by the indicator function g θ (v −1 u) → g θ (v −1 u)1 [d(v −1 u,id</formula><p>)≤r] which depends only on v −1 u. Note that equivariance would have been broken if we used neighborhoods that depend on fixed regions in the input space like the square 3 × 3 region. Figure <ref type="figure" target="#fig_0">3</ref> shows what these neighborhoods look like in terms of the input space.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.">Discretization of the Integral</head><p>Assuming that we have a collection of quadrature points {v j } N j=1 as input and the function f j = f (v j ) evaluated at these points, we can judiciously choose to evaluate the convolution at another set of group elements {u i } N i=1 , so as to have a set of quadrature points to approximate an integral in a subsequent layer. Because we have restricted the integral (6) to the compact neighbourhood nbhd(u), we can define a proper sampling distribution µ| nbhd(u) to estimate the integral, unlike for the possibly unbounded G. Computing the outputs only at these target points, we use the Monte Carlo estimator for (1) as</p><formula xml:id="formula_12">h(u i ) = (g * f )(u i ) = 1 n i j∈nbhd(i) g(v −1 j u i )f (v j ),<label>(7)</label></formula><p>where n i = |nbhd(i)| the number of points in each neighborhood.</p><p>For v j ∼ µ| nbhd(u) (•), the Monte Carlo estimator is equivariant (in distribution).</p><p>Proof: Recalling that we can absorb the local neighborhood into the definition of g θ using an indicator function, we have</p><formula xml:id="formula_13">(g * L w f )(u i ) = (1/n i ) j g(v −1 j u i )f (wv j ) = (1/n i ) j g(ṽ −1 j wu i )f (ṽ j ) d = (g * f )(wu i ) = L w (g * f )(u i ).</formula><p>Here ṽj := wv j , and the last line follows from the fact that the random variables wv j d = v j are equal in distribution because they are sampled from the Haar measure with property dµ(wv) = dµ(v). Now that we have the discretization</p><formula xml:id="formula_14">h i = (1/n i ) j∈nbhd(i) gθ (log(v −1 j u i )</formula><p>)f i , we can accelerate this computation using the Efficient-PointConv trick, with the argument of a ij = log(v −1 j u i ) for the MLP. See Appendix A.1 for more details. Note that we can also apply this discretization of the convolution when the inputs are not functions</p><formula xml:id="formula_15">f i = f (x i ), but sim- ply coordinates and values {(x i , f i )} N i=1</formula><p>, and the mapping</p><formula xml:id="formula_16">{(u i , f i )} N i=1 → {(u i , h i )} N i=1 is still equivariant, which we also demonstrate empirically in Table B.1.</formula><p>We also detail two methods for equivariantly subsampling the elements to further reduce the cost in Appendix A.4.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5.">More Than One Orbit?</head><p>Figure <ref type="figure">4</ref>. Orbits of SO(2) and T(1)y containing input points in R 2 . Unlike T(2) and SE(2), not all points are not contained in a single orbit of these small groups.</p><p>In this paper, we consider groups both large and small, and we require the ability to enable or disable equivariances like translations. To achieve this functionality, we need to go beyond the usual setting of homogeneous spaces considered in the literature, where every pair of elements in X are related by an element in G. Instead, we consider the quotient space Q = X /G, consisting of the distinct orbits of G in X (visualized in Figure <ref type="figure">4</ref>). 2 Each of these orbits q ∈ Q is a homogeneous space of the group, and when X is a homogeneous space of G then there is only a single orbit. But in general, there will be many distinct orbits, and lifting should not lose which orbit each point is on.</p><p>Since the most general equivariant mappings will need to use and preserve this information, throughout the network the space of elements should not be G but rather G × X /G, and x ∈ X is lifted to the tuples (u, q) for u ∈ G and q ∈ Q. This mapping may be one-to-one or one-to-many depending on the size of H, but will preserve the information in x as uo q = x where o q is the chosen origin for each orbit. In general, equivariant linear transforms will depend on both the input and output orbit, and equivariance only constrains the dependence on group elements and not the orbits.</p><p>When the space of orbits Q is continuous we can write the equivariant integral transform as</p><formula xml:id="formula_17">h(u, q) = G,Q g(v −1 u, q, q )f (v, q )dµ(v)dq . (<label>8</label></formula><formula xml:id="formula_18">)</formula><p>When G is the trivial group {id}, this equation simplifies to the integral transform h(x) = g(x, x )f (x )dx where each element in X is in its own orbit.</p><p>In general, even if X is a smooth manifold and G is a Lie group it is not guaranteed that X /G is a manifold <ref type="bibr" target="#b8">(Kono and Ishitoya, 1987)</ref>. However in practice this is not an issue as we only care about the discretization which we can always do. All we need is an invertible way of embedding the orbit information into a vector space to be fed into g θ . One option is to use an embedding of the orbit origin o q , or simply find enough invariants of the group to identify the orbit. To give a few examples:</p><formula xml:id="formula_19">1. X = R d and G = SO(d) : Embed(q(x)) = x 2. X = R d and G = R * : Embed(q(x)) = x x 3. X = R d and G = T(k) : Embed(q(x)) = x [k+1:d]</formula><p>Discretizing (8) as we did in (7), we get</p><formula xml:id="formula_20">h i = 1 n i j∈nbhd(i) gθ (log(v −1 j u i ), q i , q j )f j ,<label>(9)</label></formula><p>which again can be accelerated with the Efficient-PointConv trick by feeding in a ij = Concat([log(v −1 j u i ), q i , q j ]) as input to the MLP. If we want the filter to be local over orbits also, we can extend the distance d((u i , q i ), (v j , q j )) 2 = d(u i , v j ) 2 + α q i − q j 2 , which need not be invariant to is the quotient with the stabilizer of the origin H: G/H X , which has been examined extensively in the literature. Here we concerned with the separate quotient space Q = X /G, relevant when X is not a homogeneous space. transformations on q. To the best of our knowledge, we are the first to systematically address equivariances of this kind, where X is not a homogeneous space of G.</p><p>To recap, Algorithms 1 and 2 give a concise overview of our lifting procedure and our new convolution layer respectively. Please consult Appendix C.1 for additional implementation details.</p><formula xml:id="formula_21">Algorithm 1 Lifting from X to G × X /G Inputs: point data {(x i , f i )} N i=1 (x i ∈ X , f i ∈ R cin ). Returns: matrix-orbit-value tuples {(u j , q j , f j )} N k</formula><p>j=1 . For all orbits q ∈ X /G, choose an origin o q . For each o q , compute its stabilizer H q . for i = 1, . . . , N do Find the orbit q i ∈ X /G , s.t.</p><formula xml:id="formula_22">x i ∈ q i . Sample {v j } k j=1 , where v j ∼ µ(H qi ) (see C.2). Compute an element u i ∈ G s.t. u i o q = x i . Z i = {(u i v j , q i , f i )} k j=1 . end return Z Algorithm 2 The Lie Group Convolution Layer Inputs: Z (i.e. {(u j , q j , f j )} m j=1 ) Returns: convolved features {h j } m j=1 for i = 1, . . . , m do u −1 i = exp(− log(u i )). nbhd i = {j : d((u i , q i ), (u j , q j )) &lt; r)}. n i = |nbhd i |. for j = 1, . . . , m do a ij = Concat([log(u −1 i u j ), q i , q j ]). end h i = (1/n i ) j∈nbhdi g θ (a ij )f j (see A.1). end return h</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Applications to Image and Molecular Data</head><p>First, we evaluate LieConv on two types of problems: classification on image data and regression on molecular data. With LieConv as the convolution layers, we implement a bottleneck ResNet architecture with a final global pooling layer (Figure <ref type="figure">5</ref>). For a detailed architecture description, see Appendix C.3. We use the same model architecture for all tasks and achieve performance competitive with taskspecific specialized methods.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.">Image Equivariance Benchmark</head><p>The RotMNIST dataset consists of 12k randomly rotated MNIST digits with rotations sampled uniformly from SO(2), separated into 10k for training and 2k for validation. This commonly used dataset has been a standard benchmark for equivariant CNNs focused on image data. To apply LieConv to image data we interpret each input image as a collection of N = 28 × 28 points on X = R 2 with associated binary values: {x i , f (x i )} 784 i=1 to which we apply a circular center crop. We note that LieConv is primarily targeting generic continuous data, and more practical equivariant methods exist specifically for images (e.g. <ref type="bibr" target="#b24">Weiler and Cesa (2019)</ref>). However, as we demonstrate in Table <ref type="table" target="#tab_0">1</ref>, we are able to easily incorporate equivariance to different groups without any changes to the method or the architecture of the network, while achieving performance competitive with methods that are not applicable beyond image data.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.">Molecular Data</head><p>Now we apply LieConv to the QM9 molecular property learning task <ref type="bibr">(Wu et al., 2018)</ref>. The QM9 regression dataset consists of small inorganic molecules encoded as a collection of 3D spatial coordinates for each of the atoms, and their atomic charges. The labels consist of various properties of the molecules such as heat capacity. This is a challenging task as there is no canonical origin or orientation for each molecule, and the target distribution is invariant to E(3) (translation, rotation, and reflection) transformations of the coordinates. Successful models must generalize across different spatial locations and orientations.</p><p>We first perform an ablation study on the Homo problem of predicting the energy of the highest occupied molecular orbital for the molecules. We apply LieConv with different equivariance groups, combined with SO(3) data augmentation. The results are reported in Table <ref type="table" target="#tab_7">5</ref>.2. Of the three groups, our SE(3) network performs the best. We then apply T(3)-equivariant LieConv layers to the full range of tasks in the QM9 dataset and report the results in form competitively with state-of-the-art methods <ref type="bibr">(Gilmer et al., 2017;</ref><ref type="bibr" target="#b21">Schütt et al., 2018;</ref><ref type="bibr">Anderson et al., 2019)</ref>, with lowest MAE on several of the tasks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Modeling Dynamical Systems</head><p>Accurate transition models for macroscopic physical systems are critical components in control systems <ref type="bibr" target="#b13">(Lenz et al., 2015;</ref><ref type="bibr" target="#b5">Kamthe and Deisenroth, 2017;</ref><ref type="bibr">Chua et al., 2018)</ref>  data-efficient reinforcement learning algorithms <ref type="bibr" target="#b16">(Nagabandi et al., 2018;</ref><ref type="bibr" target="#b3">Janner et al., 2019)</ref>. In this section we show how incorporating equivariance into Hamiltonian dynamics models guarantees that our model system exhibits characteristics like conservation of angular momentum.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.1.">Predicting Trajectories with Hamiltonian Mechanics</head><p>For dynamical systems, the equations of motion can be written in terms of the state z and time t: ż = F (z, t). Many physically occurring systems have Hamiltonian structure, meaning that the state can be split into generalized coordinates and momenta z = (q, p), and the dynamics can be written as</p><formula xml:id="formula_23">d q dt = ∂H ∂ p d p dt = − ∂H ∂ q (10)</formula><p>for some choice of scalar Hamiltonian H(q, p, t). H is often the total energy of the system, and can sometimes be split into kinetic and potential energy terms H(q, p) = K(p) + V (q). The dynamics can also be written compactly as ż = J∇ z H for J = 0 I −I 0 .</p><p>As </p><formula xml:id="formula_24">L(θ) = 1 T T t=1 ||ẑ t − z t || 2 2 .</formula><p>(11)</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.2.">Exact Conservation of Momentum</head><p>While equivariance is broadly useful as an inductive bias, it has a very special implication for the modeling of Hamiltonian systems. Noether's Hamiltonian theorem states that for each continuous symmetry in the Hamiltonian of a dynamical system there exists a corresponding conserved quantity <ref type="bibr" target="#b17">(Noether, 1971;</ref><ref type="bibr">Butterfield, 2006)</ref>. Symmetry with respect to the continuous transformations of translations and rotations lead directly to conservation of the total linear and angular momentum of the system, an extremely valuable property for modeling the dynamics of the system. See Appendix A.5 for a primer on Hamiltonian symmetries, Noether's theorem, and the implications in the current setting.</p><p>As showed in Section 4, we can construct models that are equivariant to a large variety of continuous Lie Group symmetries, and thus exactly conserve associated quantities like linear and angular momentum. Figure <ref type="figure" target="#fig_1">7</ref>(a) shows that using LieConv layers with a given T(2) and/or SO(2) symmetry, the model trajectories conserve linear and/or angular momentum with relative error close to machine epsilon, determined by the integrator tolerance. Note that this approach for enforcing conserved quantities would be infeasible with the approach from Cohen and Welling (2016a), not just because the data is not gridded and derivatives with respect to the input positions cannot be computed, but also because there is no corresponding Noether conservation law for discrete symmetry groups.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.3.">Results</head><p>For evaluation, we compare fully-connected (FC) feedforward networks, ODE graph networks (OGN) <ref type="bibr">(Battaglia et al., 2016)</ref>, Hamiltonian ODE graph networks (HOGN) <ref type="bibr" target="#b20">(Sanchez-Gonzalez et al., 2019)</ref>, and our own LieConv architecture on a predicting the motion of point particles connected by springs as described in <ref type="bibr" target="#b20">(Sanchez-Gonzalez et al., 2019)</ref>. Figure <ref type="figure">6</ref>   system preserves energy, linear momentum, and angular momentum. The behavior of the system is complex and sensitive to both the values of the system parameters (i.e. s = (k, m)) and the initial conditions z 0 . The dynamics model must learn not only to predict trajectories across a broad range of initial conditions, but also infer the dependence on varied system parameters, which are treated as additional inputs to the model. We compare models that attempt to learn the dynamics F θ (z, t) = dz/dt directly against models that learn the Hamiltonian as described in section 6.1. Models that learn the Hamiltonian are differentiated by prepending 'H' to the name (e.g. OGN vs. HOGN, LieConv vs. HLieConv).</p><formula xml:id="formula_25">F (z, t) H(z, t) T (2) SO(2) FC • OGN • HOGN • # LieConv-T(2) • % HLieConv-Trivial • HLieConv-T(2) • % HLieConv-SO(2) • % HLieConv-SO(2)* • # %</formula><p>In Figure <ref type="figure" target="#fig_1">7</ref>(a) and 7(b) we show that by changing the invariance of our Hamiltonian models, we have direct control over the conservation of linear and angular momentum in the predicted trajectories. Figure <ref type="figure" target="#fig_1">7</ref>(c) demonstrates that our method outperforms HOGN, a SOTA architecture for dynamics problems, and achieves significant improvement over the naïve fully-connected (FC) model. We summarize the various models and their symmetries in Table <ref type="table" target="#tab_4">4</ref>.</p><p>Because of the position vectors are mean centered in the model forward pass q i = q i − q, HOGN and HLieConv-SO2* have additional T(2) invariance, yielding SE(2) invariance for HLieConv-SO2*. We also experimented with a HLieConv-SE2 equivariant model, but found that the exponential map for SE2 (involving taylor expands and masking) was not numerically stable enough for for second derivatives, required for optimizing through the Hamiltonian dynamics. Layer equivariance is preferable for not prematurely discarding useful information and for better modeling performance, but invariance alone is sufficient for the conservation laws.</p><p>Additionally, since we know a priori that the spring problem has Euclidean coordinates, we need not model the kinetic energy K(p, m) = n j=1 p j 2 /m j and instead focus on modeling the potential V (q, k). We observe that this additional inductive bias of Euclidean coordinates improves model performance.</p><p>Finally, in Figure <ref type="figure" target="#fig_2">8</ref> we evaluate test MSE of the different models over a range of training dataset sizes, highlighting the additive improvements in generalization from the Hamiltonian, Graph-Network, and equivariance inductive biases successively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.">Discussion</head><p>We presented a general-purpose convolutional layer that can be made equivariant to transformations from a Lie group with a surjective exponential map. While the image, molecular, and dynamics experiments demonstrate the generality of our method, there are many exciting application domains (e.g. time-series, geostats, audio, mesh) and directions for future work. We also believe that it will be possible to benefit from the inductive biases of HLieConv models even for systems that do not exactly preserve energy or momentum, such as those found in control systems and reinforcement learning.</p><p>The success of convolutional neural networks on images has highlighted the power of encoding symmetries in models for learning from raw sensory data. But the variety and complexity of other modalities of data is a significant challenge in further developing this approach. More general data may not be on a grid, it may possess other kinds of symmetries, or it may contain quantities that cannot be easily combined. We believe that central to solving this problem is a decoupling of convenient computational representations of data as dense arrays from the set of geometrically sensible operations they may have. We hope to move towards models that can 'see' molecules, dynamical systems, multi-scale objects, heterogeneous measurements, and higher mathematical objects, in the way that convolutional neural networks perceive images. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>References</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Appendices A. Derivations and Additional Methodology</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.1. Generalized PointConv Trick</head><p>The matrix notation becomes very cumbersome for manipulating these higher order n-dimensional arrays, so we will instead use index notation with Latin indices i, j, k indexing points, Greek indices α, β, γ indexing feature channels, and c indexing the coordinate dimensions of which there are d = 3 for PointConv and d = dim(G) + 2 dim(Q) for LieConv. <ref type="foot" target="#foot_1">3</ref> As the objects are not geometric tensors but simply n-dimensional arrays, we will make no distinction between upper and lower indices. After expanding into indices, it should be assumed that all values are scalars, and that any free indices can range over all of the values.</p><p>Let k α,β ij be the output of the MLP k θ which takes {a c ij } as input and acts independently over the locations i, j. For PointConv, the input a c ij = x c i − x c j and for LieConv the input a c ij = Concat([log(v −1 j u i ), q i , q j ]) c . We wish to compute</p><formula xml:id="formula_26">h α i = j,β k α,β ij f β j .<label>(12)</label></formula><p>In <ref type="bibr">Wu et al. (2019)</ref>, it was observed that since k α,β ij is the output of an MLP, k α,β ij = γ W α,β γ s γ i,j for some final weight matrix W and penultimate activations s γ i,j (s γ i,j is simply the result of the MLP after the last nonlinearity). With this in mind, we can rewrite (12)</p><formula xml:id="formula_27">h α i = j,β γ W α,β γ s γ i,j f β j (13) = β,γ W α,β γ j s γ i,j f β j (14)</formula><p>In practice, the intermediate number of channels is much less than the product of c in and c out : |γ| &lt; |α||β| and so this reordering of the computation leads to a massive reduction in both memory and compute. Furthermore, b γ,β i = j s γ i,j f β j can be implemented with regular matrix multiplication and h α i = β,γ W α,β γ b γ,β i can be also by flattening (β, γ) into a single axis ε:</p><formula xml:id="formula_28">h α i = ε W α,ε b ε i .</formula><p>The sum over index j can be restricted to a subset j(i) (such as a chosen neighborhood) by computing f β (•) at each of the required indices and padding to the size of the maximum subset with zeros, and computing b γ,β i = j s γ i,j(i) f β j(i) using dense matrix multiplication. Masking out of the values at indices i and j is also necessary when there are different numbers of points per minibatch but batched together using zero padding. The generalized PointConv trick can thus be applied in batch mode when there may be varied number of points per example and varied number of points per neighborhood.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.2. Abelian G and Coordinate Transforms</head><p>For Abelian groups that cover X in a single orbit, the computation is very similar to ordinary Euclidean convolution. Defining a i = log(u i ), b j = log(v j ), and using the fact that e −bj e ai = e ai−bj means that log(v −1</p><formula xml:id="formula_29">j u i ) = (log • exp)(a i − b j ). Defining f = f • exp, h = h • exp; we get h(a i ) = 1 n j∈nbhd(i) (g θ • proj)(a i − b j ) f (b j ),<label>(15)</label></formula><p>where proj = log • exp projects to the image of the logarithm map. Apart from a projection and a change to logarithmic coordinates, this is equivalent to Euclidean convolution in a vector space with dimensionality of the group. When the group is Abelian and X is a homogeneous space, then the dimension of the group is the dimension of the input. In these cases we have a trivial stabilizer group H and single origin o, so we can view f and h as acting on the input</p><formula xml:id="formula_30">x i = u i o.</formula><p>This directly generalizes some of the existing coordinate transform methods for achieving equivariance from the literature such as log polar coordinates for rotation and scaling equivariance <ref type="bibr">(Esteves et al., 2017)</ref>, and using hyperbolic coordinates for squeeze and scaling equivariance. As R 2 is a homogeneous space of G, one can choose the global origin o = [1, 0] ∈ R 2 . A little algebra shows that lifting to the group yields the transformation u i = M (r i , θ i ) for each point p i = u i o, where r = x 2 + y 2 , and θ = atan2(y, x) are the polar coordinates of the point p i . Observe that the logarithm of v −1 j u i has a simple expression highlighting the fact that it is invariant to scale and rotational transformations of the elements,</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Log</head><formula xml:id="formula_31">log(v −1 j u i ) = log(M (r j , θ j ) −1 M (r i , θ i )) = log(r i /r j )I + (θ i − θ j mod 2π)J.</formula><p>Now writing out our Monte Carlo estimation of the integral:</p><formula xml:id="formula_32">h(p i ) = 1 n j gθ (log(r i /r j ), θ i − θ j mod 2π)f (p j ),</formula><p>which is a discretization of the log polar convolution from Esteves et al. ( <ref type="formula">2017</ref>). This can be trivially extended to encompass cylindrical coordinates with the group T (1) × R * × SO(2).</p><p>Hyperbolic coordinates: For another nontrivial example, consider the group of scalings and squeezes G = R * × SQ acting on the positive orthant X = {(x, y) ∈ R 2 : x &gt; 0, y &gt; 0}. Elements of the group can be expressed as the product of a squeeze mapping and a scaling</p><formula xml:id="formula_33">M (r, s) = s 0 0 1/s r 0 0 r = rs 0 0 r/s</formula><p>for any r, s ∈ R + . As the group is abelian, the logarithm splits nicely in terms of the two generators I and A:</p><formula xml:id="formula_34">log rs 0 0 r/s = (log r) 1 0 0 1 + (log s) 1 0 0 −1 .</formula><p>Again X is a homogeneous space of G, and we choose a single origin o = [1, 1]. With a little algebra, it is clear that M (r i , s i )o = p i where r = √ xy and s = x/y are the hyperbolic coordinates of p i .</p><p>Expressed in the basis B = [I, A] for the Lie algebra above, we see that</p><formula xml:id="formula_35">log(v −1 j u i ) = log(r i /r j )I + log(s i /s j )A</formula><p>yielding the expression for convolution</p><formula xml:id="formula_36">h(p i ) = 1 n j gθ (log(r i /r j ), log(s i /s j ))f (p j ),</formula><p>which is equivariant to squeezes and scalings.</p><p>As demonstrated, equivariance to groups that contain the input space in a single orbit and are abelian can be achieved with a simple coordinate transform; however our approach generalizes to groups that are both 'larger' and 'smaller' than the input space, including coordinate transform equivariance as a special case.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.3. Sufficient Conditions for Geodesic Distance</head><p>In general, the function d(u, v) = log(v −1 u) F , defined on the domain of GL(d) covered by the exponential map, satisfies the first three conditions of a distance metric but not the triangle inequality, making it a semi-metric:</p><formula xml:id="formula_37">1. d(u, v) ≥ 0 2. d(u, v) = 0 ⇔ log(u −1 v) = 0 ⇔ u = v 3. d(u, v) = log(v −1 u) = − log(u −1 v) = d(v, u).</formula><p>However for certain subgroups of GL(d) with additional structure, the triangle inequality holds and the function is the distance along geodesics connecting group elements u and v according to the metric tensor</p><formula xml:id="formula_38">A, B u := Tr(A T u −T u −1 B),<label>(16)</label></formula><p>where −T denotes inverse and transpose.</p><p>Specifically, if the subgroup G is in the image of the exp : g → G map and each infinitesmal generator commutes with its transpose:</p><formula xml:id="formula_39">[A, A T ] = 0 for ∀A ∈ g, then d(u, v) = log(v −1 u) F is the geodesic distance between u, v.</formula><p>Geodesic Equation: Geodesics of ( <ref type="formula" target="#formula_38">16</ref>) satisfying ∇ γ γ = 0 can equivalently be derived by minimizing the energy functional</p><formula xml:id="formula_40">E[γ] = γ γ, γ γ dt = 1 0</formula><p>Tr( γT γ −T γ −1 γ)dt using the calculus of variations. Minimizing curves γ(t), connecting elements u and v in G (γ(0) = v, γ(1) = u) satisfy</p><formula xml:id="formula_41">0 = δE = δ 1 0 Tr( γT γ −T γ −1 γ)dt</formula><p>Noting that δ(γ −1 ) = −γ −1 δγγ −1 and the linearity of the trace,</p><formula xml:id="formula_42">2 1 0 Tr( γT γ −T γ −1 δ γ)−Tr( γT γ −T γ −1 δγγ −1 γ)dt = 0.</formula><p>Using the cyclic property of the trace and integrating by parts, we have that</p><formula xml:id="formula_43">−2 1 0 Tr d dt ( γT γ −T γ −1 )+γ −1 γ γT γ −T γ −1 δγ dt = 0,</formula><p>where the boundary term Tr( γγ −T γ −1 δγ)</p><formula xml:id="formula_44">1 0 vanishes since (δγ)(0) = (δγ)(1) = 0.</formula><p>As δγ may be chosen to vary arbitrarily along the path, γ must satisfy the geodesic equation:</p><formula xml:id="formula_45">d dt ( γT γ −T γ −1 ) + γ −1 γ γT γ −T γ −1 = 0.<label>(17)</label></formula><p>Solutions:</p><formula xml:id="formula_46">When A = log(v −1 u) satisfies [A, A T ] = 0, the curve γ(t) = v exp(t log(v −1 u)</formula><p>) is a solution to the geodesic equation ( <ref type="formula" target="#formula_45">17</ref>). Clearly γ connects u and v, γ(0) = v and γ(1) = u. Plugging in γ = γA into the left hand side of equation ( <ref type="formula" target="#formula_45">17</ref>), we have</p><formula xml:id="formula_47">= d dt (A T γ −1 ) + AA T γ −1 = −A T γ −1 γγ −1 + AA T γ −1 = [A, A T ]γ −1 = 0 Length of γ: The length of the curve γ connecting u and v is log(v −1 u) F , L[γ] = γ γ, γ γ dt = 1 0 Tr( γT γ −T γ −1 γ)dt = 1 0 Tr(A T A)dt = A F = log(v −1 u) F</formula><p>Of the Lie Groups that we consider in this paper, all of which have a single connected component, the groups G = T (d), SO(d), R * × SO(d), R * × SQ satisfy this property that [g, g T ] = 0; however, the SE(d) groups do not.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.4. Equivariant Subsampling</head><p>Even if all distances and neighborhoods are precomputed, the cost of computing equation ( <ref type="formula" target="#formula_10">6</ref>) for i = 1, ..., N is still quadratic, O(nN ) = O(N 2 ), because the number of points in each neighborhood n grows linearly with N as f is more densely evaluated. So that our method can scale to handle a large number of points, we show two ways two equivariantly subsample the group elements, which we can use both for the locations at which we evaluate the convolution and the locations that we use for the Monte Carlo estimator. Since the elements are spaced irregularly, we cannot readily use the coset pooling method described in (Cohen and Welling, 2016a), instead we can perform:</p><p>Random Selection: Randomly selecting a subset of p points from the original n preserves the original sampling distribution, so it can be used.</p><p>Farthest Point Sampling: Given a set of group elements S = {u i } k i=1 ∈ G, we can select a subset S * p of size p by maximizes the minimum distance between any two elements in that subset,</p><formula xml:id="formula_48">Sub p (S) := S * p = arg max Sp⊂S min u,v∈Sp:u =v d(u, v),<label>(18)</label></formula><p>farthest point sampling on the group. Acting on a set of elements, Sub p : S → S * p , the farthest point subsamplin g is equivariant Sub p (wS) = wSub p (S) for any w ∈ G. Meaning that applying a group element to each of the elements does not change the chosen indices in the subsampled set because the distances are left invariant d(u i , u j ) = d(wu i , wu j ). Now we can use either of these methods for Sub p (•) to equivariantly subsample the quadrature points in each neighborhood used to estimate the integral to a fixed number p,</p><formula xml:id="formula_49">h i = 1 p j∈Subp(nbhd(ui)) g θ (v −1 j u i )f j .<label>(19)</label></formula><p>Doing so has reduced the cost of estimating the convolution from O(N 2 ) to O(pN ), ignoring the cost of computing Sub p and {nbhd(u i )} N i=1 .</p><p>A.5. Review and Implications of Noether's Theorem</p><p>In the Hamiltonian setting, Noether's theorem relates the continuous symmetries of the Hamiltonian of a system with conserved quantities, and has been deeply impactful in the understanding of classical physics. We give a review of Noether's theorem, loosely following <ref type="bibr">Butterfield (2006)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>More on Hamiltonian Dynamics</head><p>As introduced earlier, the Hamiltonian is a function acting on the state H(z) = H(q, p), (we will ignore time dependence for now) can be viewed more formally as a function on the cotangent bundle (q, p) = z ∈ M = T * C where C is the coordinate configuration space, and this is the setting for Hamiltonian dynamics.</p><p>In general, on a manifold M, a vector field X can be viewed as an assignment of a directional derivative along M for each point z ∈ M. It can be expanded in a basis using coordinate charts X = α X α ∂ α , where ∂ α = ∂ ∂z α and acts on functions f by X(f ) = α X α ∂ α f . In the chart, each of the components X α are functions of z.</p><p>In Hamiltonian mechanics, for two functions on M , there is the Poisson bracket which can be written in terms of the canonical coordinates q i , p i ,<ref type="foot" target="#foot_3">5</ref> </p><p>{f, g}</p><formula xml:id="formula_50">= i ∂f ∂p i ∂g ∂q i − ∂f ∂q i ∂g ∂p i .</formula><p>The Poisson bracket can be used to associate each function f to a vector field</p><formula xml:id="formula_51">X f = {f, •} = i ∂f ∂p i ∂ ∂q i − ∂f ∂q i ∂ ∂p i ,</formula><p>which specifies, by its action on another function g, the directional derivative of g along X f : X f (g) = {f, g}. Vector fields that can be written in this way are known as Hamiltonian vector fields, and the Hamiltonian dynamics of the system is a special example X H = {H, •}. This vector field in canonical coordinates z = (p, q) is the vector field X H = F (z) = J∇ z H (i.e. the symplectic gradient, as discussed in Section 6.1). Making this connection clear, a given scalar quantity evolves through time as ḟ = {H, f }. But this bracket can be used to evaluate the rate of change of a scalar quantity along the flows of vector fields other than the dynamics, such as the flows of continuous symmetries.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Noether's Theorem</head><p>The flow φ X λ by λ ∈ R of a vector field X is the set of integral curves, the unique solution to the system of ODEs żα = X α with initial condition z and at parameter value λ, or more abstractly the iterated application of X: φ X λ = exp(λX). Continuous symmetries transformation are the transformations that can be written as the flow φ X λ of a vector field. The directional derivative characterizes how a function such as the Hamiltonian changes along the flow of X and is a special case of the Lie Derivative L.</p><formula xml:id="formula_52">L X H = d dλ (H • φ X λ ) λ=0 = X(H)</formula><p>A scalar function is invariant to the flow of a vector field if and only if the Lie Derivative is zero</p><formula xml:id="formula_53">H(φ X λ (z)) = H(z) ⇔ L X H = 0.</formula><p>For all transformations that respect the Poisson Bracket 6 , which we add as a requirement for a symmetry, the vector field X is (locally) Hamiltonian and there exists a function f such that X = X f = {f, •}. If M is a contractible domain such as R 2n , then f is globally defined. For every continuous symmetry φ</p><formula xml:id="formula_54">X f λ , L X f H = X f (H) = {f, H} = −{H, f } = −X H (f ),</formula><p>by the antisymmetry of the Poisson bracket. So if φ X λ is a symmetry of H, then X = X f for some function f , and H(φ</p><formula xml:id="formula_55">X f λ (z)) = H(z) implies L X f H = 0 ⇔ L X H f = 0 ⇔ f (φ X H τ (z)) = f (z)</formula><p>or in other words f (z(t+τ )) = f (z(t)) and f is a conserved quantity of the dynamics.</p><p>6 More precisely, the Poisson Bracket can be formulated in a coordinate free manner in terms of a symplectic two form ω, {f, g} = ω(X f , Xg). In the original coordinates ω = i dpi ∧ dq i , and this coordinate basis, ω is represented by the matrix J from earlier. The dynamics XH are determined by dH = ω(XH , •) = ιX H ω. Transformations which respect the Poisson Bracket are symplectic, LX ω = 0. With Cartan's magic formula, this implies that d(ιX ω) = 0. Because the form ιX ω is closed, Poincare's Lemma implies that locally (ιX ω) = df ) for some function f and hence X = X f is (locally) a Hamiltonian vector field. For more details see <ref type="bibr">Butterfield (2006)</ref>.</p><p>This implication goes both ways, if f is conserved then φ X f λ is necessarily a symmetry of the Hamiltonian, and if φ X f λ is a symmetry of the Hamiltonian then f is conserved.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Hamiltonian vs Dynamical Symmetries</head><p>So far we have been discussing Hamiltonian symmetries, invariances of the Hamiltonian. But in the study of dynamical systems there is a related concept of dynamical symmetries, symmetries of the equations of motion. This notion is also captured by the Lie Derivative, but between vector fields. A dynamical system ż = F (z), has a continuous dynamical symmetry φ X λ if the flow along the dynamical system commutes with the symmetry:</p><formula xml:id="formula_56">φ X λ (φ F t (z)) = φ F t (φ X λ (z)). (<label>20</label></formula><formula xml:id="formula_57">)</formula><p>Meaning that applying the symmetry transformation to the state and then flowing along the dynamical system is equivalent to flowing first and then applying the symmetry transformation. Equation ( <ref type="formula" target="#formula_56">20</ref>) is satisfied if and only if the Lie Derivative is zero:</p><formula xml:id="formula_58">L X F = [X, F ] = 0,</formula><p>where [•, •] is the Lie bracket on vector fields.<ref type="foot" target="#foot_4">7</ref> </p><p>For Hamiltonian systems, every Hamiltonian symmetry is also a dynamical symmetry. In fact, it is not hard to show that the Lie and Poisson brackets are related,</p><formula xml:id="formula_59">[X f , X g ] = X {f,g}</formula><p>and this directly shows the implication. If X f is a Hamiltonian symmetry, {f, H} = 0, and then</p><formula xml:id="formula_60">[X f , F ] = [X f , X H ] = X {f,H} = 0.</formula><p>However, the converse is not true, dynamical symmetries of a Hamiltonian system are not necessarily Hamiltonian symmetries and thus might not correspond to conserved quantities. Furthermore even if the system has a dynamical symmetry which is the flow along a Hamiltonian vector field φ X λ , X = X f = {f, •}, but the dynamics F are not Hamiltonian, then the dynamics will not conserve f in general. Both the symmetry and the dynamics must be Hamiltonian for the conservation laws. This fact is demonstrated by Figure <ref type="figure">9</ref>, where the dynamics of the (non-Hamiltonian) equivariant LieConv-T(2) model has a T(2) dynamical symmetry with the generators ∂ x , ∂ y which are Hamiltonian vector fields for f = p x , f = p y , and yet linear momentum is not conserved by the model. Conserving Linear and Angular Momentum</p><p>Consider a system of N interacting particles described in Euclidean coordinates with position and momentum q im , p im , such as the multi-body spring problem. Here the first index i = 1, 2, 3 indexes the spatial coordinates and the second m = 1, 2, ..., N indexes the particles. We will use the bolded notation q m , p m to suppress the spatial indices, but still indexing the particles m as in Section 6.1.</p><p>The total linear momentum along a given direction n is n</p><formula xml:id="formula_61">• P = i,m n i p im = n • ( m p m ).</formula><p>Expanding the Poisson bracket, the Hamiltonian vector field</p><formula xml:id="formula_62">X nP = {n • P, •} = i,m n i ∂ ∂q im = n • m ∂ ∂q m</formula><p>which has the flow φ X nP λ (q m , p m ) = (q m + λn, p m ), a translation of all particles by λn. So our model of the Hamiltonian conserves linear momentum if and only if it is invariant to a global translation of all particles, (e.g. T(2) invariance for a 2D spring system).</p><p>The total angular momentum along a given axis n is</p><formula xml:id="formula_63">n•L = n• m q m ×p m = i,j,k,m ijk n i q jm p km = m p T m Aq m</formula><p>, where ijk is the Levi-Civita symbol and we have defined the antisymmetric matrix A by A kj = i ijk n i .</p><formula xml:id="formula_64">X nL = {n • L, •} = j,k,m A kj q jm ∂ ∂q km − A jk p jm ∂ ∂p km X nL = m q T m A T ∂ ∂q m + p T m A T ∂ ∂p m</formula><p>where the second line follows from the asymmetry of A.</p><p>We can find the flow of X nL from the differential equations qm = Aq, ṗm = Aq which have the solution</p><formula xml:id="formula_65">φ X nL θ (q m , p m ) = (e θA q m , e θA p m ) = (R θ q m , R θ p m ),</formula><p>where R θ is a rotation about the axis n by the angle θ, which follows from the Rodriguez rotation formula. Therefore, the flow of the Hamiltonian vector field of angular momentum along a given axis is a global rotation of the position and momentum of each particle about that axis. Again, the dynamics of a neural network modeling a Hamiltonian conserve total angular momentum if and only if the network is invariant to simultaneous rotation of all particle positions and momenta. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Additional Experiments</head><p>j ∼ U(0, 5). Following Sanchez-Gonzalez et al. ( <ref type="formula">2019</ref>), we set the spring constants as k ij = k i k j . For each system i, the position and momentum of body j were distributed as q (i) j</p><p>∼ N (0, 0.16I), p (i) j</p><p>∼ N (0, 0.36I). Using the analytic form of the Hamiltonian for the spring problem, H(q, p) = K(p, m) + V (q, k), we used the symplectic gradient and the RK4 numerical integration scheme to generate 5 second ground truth trajectories broken up into 500 evaluation timesteps. We use a fixed step size scheme for RK4 chosen automatically (as implemented in Chen et al. ( <ref type="formula">2018</ref>)) with a relative tolerance of 1e-8 in double precision arithmetic. We then randomly selected a single segment for each trajectory, consisting of an initial state z t and τ = 4 transition states: (z Training: All models were trained in single precision arithmetic (double precision did not make any appreciable difference) with an integrator tolerance of 1e-4. We use a cosine decay for the learning rate schedule and perform early stopping over the validation MSE. We trained with a minibatch size of 200 and for 100 epochs each using the Adam optimizer <ref type="bibr" target="#b6">(Kingma and Ba, 2014)</ref>  For the examination of performance over the range of dataset sizes in 8, we cap the validation set to the size of the training set to make the setting more realistic, and we also scale the number of training epochs up as the size of the dataset shrinks (epochs = 100( 10 3 /D)) which we found to be sufficient to fit the training set. For D ≤ 200 we use the full dataset in each minibatch. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Hyperparameters</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C.5. Details for Image and Molecular Experiments</head><p>RotMNIST Hyperparameters: For RotMNIST we train each model for 500 epochs using the Adam optimizer with learning rate 3e-3 and batch size 25. The first linear layer maps the 1-channel grayscale input to k = 128 channels, and the number of channels in the bottleneck blocks follow the scaling law from Appendix C.3 as the group elements are downsampled. We use 6 bottleneck blocks, and the total downsampling factor S = 1/10 is split geometrically between the blocks as s = (1/10) 1/6 per block. The initial radius r of the local neighborhoods in the first layer is set so as to include 1/15 of the total number of elements in each neighborhood and is scaled accordingly. The subsampled neighborhood used to compute the Monte Carlo convolution estimator uses p = 25 elements. The models take less than 12 hours to train on a 1080Ti. The model takes about 48 hours to train on a single 1080Ti.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C.6. Local Neighborhood Visualizations</head><p>In Figure <ref type="figure">10</ref> we visualize the local neighborhood used with different groups under three different types of transformations: translations, rotations and scaling. The distance and neighborhood are defined for the tuples of group elements and orbit. For Trivial, T(2), SO(2), R × SO(2) the correspondence between points and these tuples is one-to-one and we can identify the neighborhood in terms of the input points. For SE(2) each point is mapped to multiple tuples, each of which defines its own neighborhood in terms of other tuples. In the Figure, for SE(2) for a given point we visualize the distribution of points that enter the computation of the convolution at a specific tuple. </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 3 .</head><label>3</label><figDesc>Figure 3. A visualization of the local neighborhood for R * × SO(2), in terms of the points in the input space. For the computation of h at the point in orange, elements are sampled from colored region. Notice that the same points enter the calculation when the image is transformed by a rotation and scaling. We visualize the neighborhoods for other groups in Appendix C.6.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 7 .</head><label>7</label><figDesc>Figure7. Left: We can directly control whether linear and angular momentum is conserved by changing the model's symmetries. The components of linear and angular momentum of the integrated trajectories are conserved up to integrator tolerance. Middle: Momentum along the rollout trajectories for the LieConv models with different imposed symmetries. Note that both invariance and modeling the Hamiltonian are required for conservation. Right: Our method outperforms HOGN, a state-of-the-art model, on both state rollout error and system energy conservation.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 8 .</head><label>8</label><figDesc>Figure 8. Test MSE as a function of the number of examples in the training dataset, N . As the inductive biases of Hamiltonian, Graph-Network, and LieConv equivariance are added, generalization performance improves. LieConv outperforms the other methods across all dataset sizes. The shaded region corresponds to µ ± 2σ, estimated across 3 trials.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head></head><label></label><figDesc>Polar Coordinates: Consider the Abelian Lie group of positive scalings and rotations: G = R * × SO(2) acting on R 2 . Elements of the group M ∈ G can be expressed as a 2 × 2 matrix M (r, θ) = r cos(θ) −r sin(θ) r sin(θ) r cos(θ) for r ∈ R + and θ ∈ R. The matrix logarithm is 4 log r cos(θ) −r sin(θ) r sin(θ) r cos(θ) = log(r) −θ mod 2π θ mod 2π log(r) , or more compactly log(M (r, θ)) = log(r)I +(θ mod 2π)J, which is [log(r), θ mod 2π] in the basis for the Lie algebra [I, J]. It is clear that proj = log • exp is simply mod 2π on the J component.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head></head><label></label><figDesc>without batch normalization. With 3k training examples, the HLieConv model takes about 20 minutes to train on one 1080Ti.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head></head><label></label><figDesc>QM9 Hyperparameters: For the QM9 molecular data, we use the featurization fromAnderson et al. (2019), where the input features f i are determined by the atom type (C,H,N,O,F) and the atomic charge. The coordinates x i are simply the raw atomic coordinates measured in angstroms. A separate model is trained for each prediction task, all using the same hyperparameters and early stopping on the validation MAE. We use the same train, validation, test split as Anderson et al. (2019), with 100k molecules for train, 10% for test and the remaining for validation. Like with the other experiments, we use a cosine learning rate decay schedule. Each model is trained using the Adam optimizer for 1000 epochs with a learning rate of 3e-3 and batch size of 100. We use SO(3) data augmentation, 6 bottleneck blocks, each with k = 1536 channels. The radius of the local neighborhood is set to r = ∞ to include all elements.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head></head><label></label><figDesc>Figure10. A visualization of the local neighborhood for different groups, in terms of the points in the input space. For the computation of the convolution at the point in red, elements are sampled from colored region. In each panel, the top row shows translations, middle row shows rotations and bottom row shows scalings of the same image. For SE(2) we visualize the distribution of points entering the computation of the convolution over multiple lift samples. For each of the equivariant models that respects a given symmetry, the points that enter into the computation are not affected by the transformation.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 .</head><label>1</label><figDesc>Classification Error (%) on RotMNIST dataset for LieConv with different group equivariances and baselines: G-CNN (Cohen and Welling, 2016a), H-Net(Worrall et al., 2017),ORN (Zhou et al., 2017), TI-Pooling<ref type="bibr" target="#b10">(Laptev et al., 2016)</ref>, RotEqNet<ref type="bibr" target="#b14">(Marcos et al., 2017)</ref>, E(2)-Steerable CNNs<ref type="bibr" target="#b24">(Weiler and Cesa, 2019)</ref> .</figDesc><table><row><cell></cell><cell></cell><cell></cell><cell cols="2">Baseline Methods</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="3">LieConv (Ours)</cell></row><row><cell cols="13">G-CNN H-NET ORN TI-Pooling RotEqNet E(2)-Steerable Trivial T(1)y T(2) SO(2) SO(2)×R  *  SE(2)</cell></row><row><cell>2.28</cell><cell>1.69</cell><cell>1.54</cell><cell>1.2</cell><cell></cell><cell>1.09</cell><cell>0.68</cell><cell>1.58</cell><cell>1.49</cell><cell></cell><cell>1.44</cell><cell>1.42</cell><cell>1.27</cell><cell>1.24</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="6">Table 2. QM9 Molecular Property Mean Absolute Error</cell><cell></cell><cell></cell></row><row><cell>Task Units</cell><cell></cell><cell cols="4">α bohr 3 meV ∆ε ε HOMO ε LUMO meV meV</cell><cell cols="7">µ D cal/mol K meV meV bohr 2 meV meV C ν G H R 2 U U 0 ZPVE meV</cell></row><row><cell>NMP</cell><cell></cell><cell>.092</cell><cell>69</cell><cell>43</cell><cell cols="2">38 .030</cell><cell>.040</cell><cell>19</cell><cell>17</cell><cell cols="2">.180</cell><cell>20</cell><cell>20 1.500</cell></row><row><cell>SchNet</cell><cell></cell><cell>.235</cell><cell>63</cell><cell>41</cell><cell cols="2">34 .033</cell><cell>.033</cell><cell>14</cell><cell>14</cell><cell cols="2">.073</cell><cell>19</cell><cell>14 1.700</cell></row><row><cell>Cormorant</cell><cell></cell><cell>.085</cell><cell>61</cell><cell>34</cell><cell cols="2">38 .038</cell><cell>.026</cell><cell>20</cell><cell>21</cell><cell cols="2">.961</cell><cell>21</cell><cell>22 2.027</cell></row><row><cell cols="2">LieConv(T3)</cell><cell>.084</cell><cell>49</cell><cell>30</cell><cell cols="2">25 .032</cell><cell>.038</cell><cell>22</cell><cell>24</cell><cell cols="2">.800</cell><cell>19</cell><cell>19 2.280</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 .</head><label>2</label><figDesc>We per-A visual overview of the LieConv model architecture. The model is composed of L LieConv bottleneck blocks that couple the values at different group elements together. The bottleneck block is a residual block with a LieConv layer between two linear layers.</figDesc><table><row><cell>Lifting</cell><cell></cell><cell>BatchNorm</cell></row><row><cell>Linear</cell><cell></cell><cell>Swish</cell></row><row><cell cols="2">BottleBlock</cell><cell>Linear</cell></row><row><cell>x</cell><cell>L</cell><cell>BatchNorm</cell></row><row><cell></cell><cell></cell><cell>Swish</cell></row><row><cell cols="2">BottleBlock</cell><cell>LieConv</cell></row><row><cell cols="2">BatchNorm</cell><cell>Downsample</cell></row><row><cell>Swish</cell><cell></cell><cell>BatchNorm</cell></row><row><cell>Linear</cell><cell></cell><cell>Swish</cell></row><row><cell></cell><cell></cell><cell>Linear</cell></row><row><cell cols="2">Global Pool</cell><cell>+</cell></row><row><cell>Figure 5.</cell><cell></cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 3 .</head><label>3</label><figDesc>LieConv performance (Mean Absolute Error in meV) for different groups on the HOMO regression problem.</figDesc><table><row><cell>and</cell></row></table><note>Figure6. A qualitative example of the 2D spring problem. The final position of six bodies is depicted by colored dots. The bodies are acted upon by pairwise spring forces (not shown). Given the initial position and momentum of the bodies, the model must predict each trajectory (T = 100, ∆t = 0.01 sec). We see that HOGN (left), a SOTA architecture for modeling physical systems, gradually diverges from the true trajectory. In contrast, our method (right) closely tracks the ground truth.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head></head><label></label><figDesc>We follow the approach of<ref type="bibr" target="#b20">Sanchez-Gonzalez et al. (2019) and</ref> Zhong et al. (2019). Given an initial condition z 0 and F θ (z, t) = J∇ z Ĥθ , we employ a twice-differentiable model architecture and a differentiable ODE solver(Chen et al., 2018)  to compute predicted states {ẑ 1 , . . . , ẑT }. The parameters of the Hamiltonian model Ĥθ can be trained directly through the L2 loss,</figDesc><table /><note>shown inGreydanus et al. (2019), a neural network parametrizing Ĥθ (z, t) can be learned directly from trajectory data, providing substantial benefits in generalization over directly modeling F (z, t).</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 4 .</head><label>4</label><figDesc>Model characteristics. Models with layers invariant to G are denoted with #, and those with equivariant layers with %.</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 5 .</head><label>5</label><figDesc>Test MAE (in meV) on HOMO test set randomly transformed by elements of G. Despite no data augmentation (N), G equivariant models perform as well on G transformed test data.While the RotMNIST dataset consists of 12k rotated MNIST digits, it is standard to separate out 10k to be used for training and 2k for validation. However, in Ti-Pooling and E(2)-Steerable CNNs, it appears that after hyperparameters were tuned the validation set is folded back into the training set to be used as additional training data, a common approach used on other datasets. Although in table 1 we only use 10k training points, in the table below we report the performance with and without augmentation trained on the full 12k examples.</figDesc><table><row><cell cols="3">B.1. Equivariance Demo</cell><cell></cell><cell></cell></row><row><cell cols="5">While (7) shows that the convolution estimator is equivari-</cell></row><row><cell cols="5">ant, we have conducted the ablation study below examining</cell></row><row><cell cols="5">the equivariance of the network empirically. We trained</cell></row><row><cell cols="5">LieConv (Trivial, T(3), SO(3), SE(3)) models on a limited</cell></row><row><cell cols="5">subset of 20k training examples (out of 100k) of the HOMO</cell></row><row><cell cols="5">task on QM9 without any data augmentation. We then evalu-</cell></row><row><cell cols="5">ate these models on a series of modified test sets where each</cell></row><row><cell cols="5">example has been randomly transformed by an element of</cell></row><row><cell cols="5">the given group (the test translations in T(3) and SE(3) are</cell></row><row><cell cols="5">sampled from a normal with stddev 0.5). In table B.1 the</cell></row><row><cell cols="5">rows are the models configured with a given group equiv-</cell></row><row><cell cols="5">ariance and the columns N/G denote no augmentation at</cell></row><row><cell cols="5">training time and transformations from G applied to the test</cell></row><row><cell cols="5">set (test translations in T(3) and SE(3) are sampled from a</cell></row><row><cell cols="2">normal with stddev 0.5).</cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="5">Model N/N N/T(3) N/SO(3) N/SE(3)</cell></row><row><cell cols="2">Trivial 173</cell><cell>183</cell><cell>239</cell><cell>243</cell></row><row><cell>T(3)</cell><cell>113</cell><cell>113</cell><cell>133</cell><cell>133</cell></row><row><cell>SO(3)</cell><cell>159</cell><cell>238</cell><cell>160</cell><cell>240</cell></row><row><cell>SE(3)</cell><cell>62</cell><cell>62</cell><cell>63</cell><cell>62</cell></row><row><cell cols="5">Notably, the performance of the LieConv-G models do not</cell></row><row><cell cols="5">degrade when random G transformations are applied to the</cell></row><row><cell cols="5">test set. Also, in this low data regime, the added equivari-</cell></row><row><cell cols="3">ances are especially important.</cell><cell></cell><cell></cell></row><row><cell cols="3">B.2. RotMNIST Comparison</cell><cell></cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>Table 6 .</head><label>6</label><figDesc>Classification Error (%) on RotMNIST dataset for LieConv with different group equivariances and baselines: use a modified batchnorm that computes statistics only over elements from a given mask. The batch norm is computed per channel, with statistics averaged over the batch size and each of the valid locations.C.4. Details for Hamiltonian ModelsDataset Generation: To generate spring dynamics datasets we generated D systems each with N = 6 particles connected by springs. The system parameters, mass and spring constant, are set by sampling {m</figDesc><table><row><cell>k</cell><cell>(i) 1 , . . . m (i) 6 , k 1 , . . . , k (i)</cell><cell>(i) 6 } N i=1 , m (i) j</cell><cell>∼ U(0.1, 3.1),</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head></head><label></label><figDesc>:Hyperparameter tuning: Model hyperparameters were tuned by grid search over channel width, number of layers, and learning rate. The models were tuned with training, validation, and test datasets consisting of 3000, 2000, and 2000 trajectory segments respectively.</figDesc><table><row><cell></cell><cell cols="2">channels layers</cell><cell>lr</cell></row><row><cell>(H)FC</cell><cell>256</cell><cell>4</cell><cell>1e-2</cell></row><row><cell>(H)OGN</cell><cell>256</cell><cell>1</cell><cell>1e-2</cell></row><row><cell>(H)LieConv</cell><cell>384</cell><cell>4</cell><cell>1e-3</cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0">When fi = f (xi), lifting in this way is equivalent to defining f ↑ (u) = f (uo) as in<ref type="bibr" target="#b7">Kondor and Trivedi (2018)</ref>.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3" xml:id="foot_1">dim(Q) is the dimension of the space into which Q, the orbit identifiers, are embedded.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4" xml:id="foot_2">Here θ mod 2π is defined to mean θ + 2πn for the integer n such that the value is in (−π, π), consistent with the principal matrix logarithm. (θ + π)%2π − π in programming notation.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="5" xml:id="foot_3">Here we take the definition of the Poisson bracket to be negative of the usual definition in order to streamline notation.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="7" xml:id="foot_4">The Lie bracket on vector fields produces another vector field and is defined by how it acts on functions, for any smooth function g: [X, F ](g) = X(F (g)) − F (X(g))</note>
		</body>
		<back>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Implementation Details C.1. Practical Considerations</head><p>While the high-level summary of the lifting procedure (Algorithm 1) and the LieConv layer (Algorithm 2) provides a useful conceptual understanding of our method, there are some additional details that are important for a practical implementation.</p><p>1. According to Algorithm 2, a ij is computed in every LieConv layer, which is both highly redundant and costly. In practice, we precompute a ij once after lifting and feed it through the network with layers operating on the state</p><p>. Doing so requires fixing the group elements that will be used at each layer for a given forwards pass.</p><p>2. In practice only p elements of nbhd i are sampled (randomly) for computing the Monte Carlo estimator in order to limit the computational burden (see Appendix A.4).</p><p>3. We use the analytic forms for the exponential and logarithm maps of the various groups as described in Eade (2014).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C.2. Sampling from the Haar Measure for Various groups</head><p>When the lifting map from X → G × X /G is multi-valued, we need to sample elements of u ∈ G that project down to x: uo = x in a way consistent with the Haar measure µ(•).</p><p>In other words, since the restriction µ(•)| nbhd is a distribution, then we must sample from the conditional distribution u ∼ µ(u|uo = x)| nbhd . In general this can be done by parametrizing the distribution of µ as a collection of random variables that includes x, and then sampling the remaining variables.</p><p>In this paper, the groups we use in which the lifting map is multi-valued are SE(2), SO(3), and SE(3). The process is especially straightforward for SE(2) and SE(3) as these groups can be expressed as a semi-direct product of two groups G = H N ,</p><p>where</p><p>So lifts of a point x ∈ X to SE(d) consistent with the µ are just T x R, the multiplication of a translation by x and randomly sampled rotations R ∼ µ SO(d) (•). There are multiple easy methods to sample uniformly from SO(d) given in <ref type="bibr" target="#b9">(Kuffner, 2004)</ref>, for example sampling uniformly from SO(3) can be done by sampling a unit quaternion from the 3-sphere, and identifying it with the corresponding rotation matrix.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C.3. Model Architecture</head><p>We employ a ResNet-style architecture <ref type="bibr" target="#b0">(He et al., 2016)</ref>, using bottleneck blocks (Zagoruyko and Komodakis, 2016), and replacing ReLUs with Swish activations <ref type="bibr" target="#b18">(Ramachandran et al., 2017)</ref>. The convolutional kernel g θ internal to each LieConv layer is parametrized by a 3-layer MLP with 32 hidden units, batch norm, and Swish nonlinearities. Not only do the Swish activations improve performance slightly, but unlike ReLUs they are twice differentiable which is a requirement for backpropagating through the Hamiltonian dynamics. The stack of elementwise linear and bottleneck blocks is followed by a global pooling layer that computes the average over all elements, but not over channels. Like for regular image bottleneck blocks, the channels for the convolutional layer in the middle are smaller by a factor of 4 for increased parameter and computational efficiency.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Downsampling:</head><p>As is traditional for image data, we increase the number of channels and the receptive field at every downsampling step. The downsampling is performed with the farthest point downsampling method described in Appendix A.4. For a downsampling by a factor of s &lt; 1, the radius of the neighborhood is scaled up by s −1/2 and the channels are scaled up by s −1/2 . When an image is downsampled with s = (1/2) 2 that is typical in a CNN, this results in 2x more channels and a radius or dilation of 2x. In the bottleneck block, the downsampling operation is fused with the LieConv layer, so that the convolution is only evaluated at the downsampled query locations. We perform downsampling only on the image datasets, which have more points.</p><p>BatchNorm: In order to handle the varied number of group elements per example and within each neighborhood, we</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shaoqing</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
				<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="770" to="778" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Deep learning on lie groups for skeleton-based action recognition</title>
		<author>
			<persName><forename type="first">Zhiwu</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chengde</forename><surname>Wan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Thomas</forename><surname>Probst</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Luc</forename><surname>Van Gool</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
				<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="6099" to="6108" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<author>
			<persName><forename type="first">Jörn-Henrik</forename><surname>Jacobsen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bert</forename><forename type="middle">De</forename><surname>Brabandere</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Arnold</forename><forename type="middle">Wm</forename><surname>Smeulders</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1706.00598</idno>
		<title level="m">Dynamic steerable blocks in deep residual networks</title>
				<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<author>
			<persName><forename type="first">Michael</forename><surname>Janner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Justin</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Marvin</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sergey</forename><surname>Levine</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1906.08253</idno>
		<title level="m">When to trust your model: Model-based policy optimization</title>
				<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<author>
			<persName><forename type="first">Chiyu</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jingwei</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Karthik</forename><surname>Kashinath</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Philip</forename><surname>Marcus</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Matthias</forename><surname>Niessner</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1901.02039</idno>
		<title level="m">Spherical cnns on unstructured grids</title>
				<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">Data-efficient reinforcement learning with probabilistic model predictive control</title>
		<author>
			<persName><forename type="first">Sanket</forename><surname>Kamthe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Marc</forename><surname>Peter</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Deisenroth</forename></persName>
		</author>
		<idno type="arXiv">arXiv:1706.06491</idno>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<author>
			<persName><forename type="first">P</forename><surname>Diederik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jimmy</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName><surname>Ba</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1412.6980</idno>
		<title level="m">Adam: A method for stochastic optimization</title>
				<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">On the generalization of equivariance and convolution in neural networks to the action of compact groups</title>
		<author>
			<persName><forename type="first">Risi</forename><surname>Kondor</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shubhendu</forename><surname>Trivedi</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1802.03690</idno>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Squaring operations in mod 2 cohomology of quotients of compact lie groups by maximal tori</title>
		<author>
			<persName><forename type="first">Akira</forename><surname>Kono</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kiminao</forename><surname>Ishitoya</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Algebraic Topology Barcelona 1986</title>
				<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="1987">1987</date>
			<biblScope unit="page" from="192" to="206" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Effective sampling and distance metrics for 3d rigid body path planning</title>
		<author>
			<persName><forename type="first">J</forename><surname>James</surname></persName>
		</author>
		<author>
			<persName><surname>Kuffner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Robotics and Automation</title>
				<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2004">2004. 2004. 2004</date>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page" from="3993" to="3998" />
		</imprint>
	</monogr>
	<note>Proceedings. ICRA&apos;04</note>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Ti-pooling: transformation-invariant pooling for feature learning in convolutional neural networks</title>
		<author>
			<persName><forename type="first">Dmitry</forename><surname>Laptev</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nikolay</forename><surname>Savinov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Joachim</forename><forename type="middle">M</forename><surname>Buhmann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Marc</forename><surname>Pollefeys</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
				<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="289" to="297" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">An empirical evaluation of deep architectures on problems with many factors of variation</title>
		<author>
			<persName><forename type="first">Hugo</forename><surname>Larochelle</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dumitru</forename><surname>Erhan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aaron</forename><surname>Courville</surname></persName>
		</author>
		<author>
			<persName><forename type="first">James</forename><surname>Bergstra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 24th international conference on Machine learning</title>
				<meeting>the 24th international conference on Machine learning</meeting>
		<imprint>
			<date type="published" when="2007">2007</date>
			<biblScope unit="page" from="473" to="480" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">Convolutional networks for images, speech, and time series. The handbook of brain theory and neural networks</title>
		<author>
			<persName><forename type="first">Yann</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1995">1995. 1995</date>
			<biblScope unit="volume">3361</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Deepmpc: Learning deep latent features for model predictive control</title>
		<author>
			<persName><forename type="first">Ian</forename><surname>Lenz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ross</forename><forename type="middle">A</forename><surname>Knepper</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ashutosh</forename><surname>Saxena</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Robotics: Science and Systems</title>
				<meeting><address><addrLine>Rome, Italy</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Rotation equivariant vector field networks</title>
		<author>
			<persName><forename type="first">Diego</forename><surname>Marcos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michele</forename><surname>Volpi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nikos</forename><surname>Komodakis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Devis</forename><surname>Tuia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
				<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="5048" to="5057" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Nineteen dubious ways to compute the exponential of a matrix, twenty-five years later</title>
		<author>
			<persName><forename type="first">Cleve</forename><surname>Moler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Charles</forename><surname>Van Loan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">SIAM review</title>
		<imprint>
			<biblScope unit="volume">45</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="3" to="49" />
			<date type="published" when="2003">2003</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Neural network dynamics for modelbased deep reinforcement learning with model-free finetuning</title>
		<author>
			<persName><forename type="first">Anusha</forename><surname>Nagabandi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gregory</forename><surname>Kahn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ronald</forename><forename type="middle">S</forename><surname>Fearing</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sergey</forename><surname>Levine</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2018 IEEE International Conference on Robotics and Automation (ICRA)</title>
				<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="7559" to="7566" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Invariant variation problems</title>
		<author>
			<persName><forename type="first">Emmy</forename><surname>Noether</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Transport Theory and Statistical Physics</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="186" to="207" />
			<date type="published" when="1971">1971</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<author>
			<persName><forename type="first">Prajit</forename><surname>Ramachandran</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Barret</forename><surname>Zoph</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Quoc V</forename><surname>Le</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1710.05941</idno>
		<title level="m">Searching for activation functions</title>
				<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Fast and accurate modeling of molecular atomization energies with machine learning</title>
		<author>
			<persName><forename type="first">M</forename><surname>Rupp</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Tkatchenko</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K.-R</forename><surname>Müller</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><forename type="middle">A</forename><surname>Lilienfeld</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Physical Review Letters</title>
		<imprint>
			<biblScope unit="volume">108</biblScope>
			<biblScope unit="page">58301</biblScope>
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">Hamiltonian graph networks with ode integrators</title>
		<author>
			<persName><forename type="first">Alvaro</forename><surname>Sanchez-Gonzalez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Victor</forename><surname>Bapst</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kyle</forename><surname>Cranmer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Peter</forename><surname>Battaglia</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1909.12790</idno>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Schnet-a deep learning architecture for molecules and materials</title>
		<author>
			<persName><surname>Kristof T Schütt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Huziel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P-J</forename><surname>Sauceda</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alexandre</forename><surname>Kindermans</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K-R</forename><surname>Tkatchenko</surname></persName>
		</author>
		<author>
			<persName><surname>Müller</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The Journal of Chemical Physics</title>
		<imprint>
			<biblScope unit="volume">148</biblScope>
			<biblScope unit="issue">24</biblScope>
			<biblScope unit="page">241722</biblScope>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Dynamic edgeconditioned filters in convolutional neural networks on graphs</title>
		<author>
			<persName><forename type="first">Martin</forename><surname>Simonovsky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nikos</forename><surname>Komodakis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
				<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<author>
			<persName><forename type="first">Nathaniel</forename><surname>Thomas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tess</forename><surname>Smidt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Steven</forename><surname>Kearnes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lusann</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Li</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kai</forename><surname>Kohlhoff</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Patrick</forename><surname>Riley</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1802.08219</idno>
		<title level="m">Tensor field networks: Rotation-and translation-equivariant neural networks for 3d point clouds</title>
				<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">General e (2)-equivariant steerable cnns</title>
		<author>
			<persName><forename type="first">Maurice</forename><surname>Weiler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gabriele</forename><surname>Cesa</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
				<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="14334" to="14345" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
