<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">P-Tuning: Prompt Tuning Can Be Comparable to Fine-tuning Across Scales and Tasks</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author role="corresp">
							<persName><forename type="first">Xiao</forename><surname>Liu</surname></persName>
							<email>liuxiao21@mails.tsinghua.edu.cn</email>
							<affiliation key="aff0">
								<orgName type="institution">Tsinghua University</orgName>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="department">Beijing Academy of Artificial Intelligence (BAAI)</orgName>
								<address>
									<addrLine>3 Shanghai Qi Zhi Institute</addrLine>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Kaixuan</forename><surname>Ji</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Tsinghua University</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Yicheng</forename><surname>Fu</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Tsinghua University</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Weng</forename><surname>Lam</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Tsinghua University</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Zhengxiao</forename><surname>Du</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Tsinghua University</orgName>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="department">Beijing Academy of Artificial Intelligence (BAAI)</orgName>
								<address>
									<addrLine>3 Shanghai Qi Zhi Institute</addrLine>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Zhilin</forename><surname>Yang</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Tsinghua University</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Jie</forename><surname>Tang</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Tsinghua University</orgName>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="department">Beijing Academy of Artificial Intelligence (BAAI)</orgName>
								<address>
									<addrLine>3 Shanghai Qi Zhi Institute</addrLine>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">P-Tuning: Prompt Tuning Can Be Comparable to Fine-tuning Across Scales and Tasks</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.2" ident="GROBID" when="2022-12-25T13:37+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Prompt tuning, which only tunes continuous prompts with a frozen language model, substantially reduces per-task storage and memory usage at training. However, in the context of NLU, prior work reveals that prompt tuning does not perform well for normal-sized pretrained models. We also find that existing methods of prompt tuning cannot handle hard sequence labeling tasks, indicating a lack of universality. We present a novel empirical finding that properly optimized prompt tuning can be universally effective across a wide range of model scales and NLU tasks. It matches the performance of finetuning while having only 0.1%-3% tuned parameters. Our method P-Tuning v2 is an implementation of Deep Prompt Tuning <ref type="bibr" target="#b12">(Li and Liang, 2021;</ref><ref type="bibr" target="#b17">Qin and Eisner, 2021)</ref> optimized and adapted for NLU. Given the universality and simplicity of P-Tuning v2, we believe it can serve as an alternative to finetuning and a strong baseline for future research. 1  </p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Pretrained language models <ref type="bibr" target="#b18">(Radford et al., 2019;</ref><ref type="bibr">Devlin et al., 2018;</ref><ref type="bibr" target="#b31">Yang et al., 2019;</ref><ref type="bibr" target="#b19">Raffel et al., 2019)</ref> improve performance on a wide range of natural language understanding (NLU) tasks. A widely-used method, fine-tuning, updates the entire set of model parameters for a target task. While fine-tuning obtains good performance, it is memory-consuming during training because gradients and optimizer states for all parameters must be stored. Moreover, keeping a copy of model parameters for each task during inference is inconvenient since pre-trained models are usually large.</p><p>Prompting, on the other hand, freezes all parameters of a pre-trained model and uses a natural language prompt to query a language model (Brown † corresponding to: Zhilin Yang (zhiliny@tsinghua.edu.cn) and Jie Tang (jietang@tsinghua.edu.cn) * indicates equal contribution. 1 Our code and data are released at https://github. com/THUDM/P-tuning-v2.  et al., 2020). For example, for sentiment analysis, we can concatenate a sample (e.g., "Amazing movie!") with a prompt "This movie is [MASK]" and ask the pre-trained language model to predict the probabilities of masked token being "good" and "bad" to decide the sample's label. Prompting requires no training at all and stores one single copy of model parameters. However, discrete prompting <ref type="bibr" target="#b23">(Shin et al., 2020;</ref><ref type="bibr" target="#b7">Gao et al., 2020)</ref> can lead to suboptimal performance in many cases compared to fine-tuning. Prompt tuning<ref type="foot" target="#foot_0">2</ref> is an idea of tuning only the continuous prompts. Specifically, <ref type="bibr" target="#b13">Liu et al. (2021)</ref>; <ref type="bibr" target="#b11">Lester et al. (2021)</ref> proposed to add trainable continuous embeddings (also called continuous prompts) to the original sequence of input word embeddings. Only the continuous prompts are updated during training. While prompt tuning improves over prompting on many tasks <ref type="bibr" target="#b13">(Liu et al., 2021;</ref><ref type="bibr" target="#b11">Lester et al., 2021;</ref><ref type="bibr" target="#b33">Zhong et al., 2021)</ref>, it still underperforms fine-tuning when the model size is not large, specifically less than 10 billion parameters <ref type="bibr" target="#b11">(Lester et al., 2021)</ref>. Moreover, as shown in our experiments, prompt tuning performs poorly compared to fine-tuning on several hard sequence labeling tasks such as extractive question answering (Cf. Section 4.2).</p><p>Our main contribution in this paper is a novel empirical finding that properly optimized prompt tuning can be comparable to fine-tuning universally across various model scales and NLU tasks. In contrast to observations in prior work, our discovery reveals the universality and potential of prompt tuning for NLU.</p><p>Technically, our approach P-tuning v2 is not conceptually novel. It can be viewed as an optimized and adapted implementation of Deep Prompt <ref type="bibr">Tuning (Li and Liang, 2021;</ref><ref type="bibr" target="#b17">Qin and Eisner, 2021)</ref> designed for generation and knowledge probing. The most significant improvement originates from appling continuous prompts for every layer of the pretrained model, instead of the mere input layer. Deep prompt tuning increases the capacity of continuous prompts and closes the gap to fine-tuning across various settings, especially for small models and hard tasks. Moreover, we present a series of critical details of optimization and implementation to ensure finetuning-comparable performance.</p><p>Experimental results show that P-tuning v2 matches the performance of fine-tuning at different model scales ranging from 300M to 10B parameters and on various hard sequence tagging tasks such as extractive question answering and named entity recognition. P-tuning v2 has 0.1% to 3% trainable parameters per task compared to fine-tuning, which substantially reduces training time memory cost and per-task storage cost.</p><p>2 Preliminaries NLU Tasks. In this work, we categorize NLU challenges into two families: simple classification tasks and hard sequence labeling tasks.<ref type="foot" target="#foot_1">3</ref> Simple classification tasks involve classification over a label space. Most datasets from GLUE <ref type="bibr" target="#b26">(Wang et al., 2018)</ref> and SuperGLUE <ref type="bibr">(Wang et al., 2019)</ref> are in this category. Hard sequence labeling tasks involve classification over a sequence of tokens, such as named entity recognition and extractive question answering.</p><p>Prompt Tuning. Let V be the vocabulary of a language model M and let e be the embedding layer of M. In the case of discrete prompting <ref type="bibr" target="#b22">(Schick and Schütze, 2020)</ref>, prompt tokens {"It", "is", "[MASK]"} ⊂ V can be used to classify a movie review. For exam-ple, given the input text x ="Amazing movie!", the input embedding sequence is formulated as [e(x), e("It"), e("is"), e("[MASK]")]. <ref type="bibr" target="#b11">Lester et al. (2021)</ref> and <ref type="bibr" target="#b13">Liu et al. (2021)</ref> introduce trainable continuous prompts as a substitution to natural language prompts for NLU with the parameters of pretrained language models frozen. Given the trainable continuous embeddings [h 0 , ..., h i ], the input embedding sequence is written as [e(x), h 0 , ..., h i , e("[MASK]")], as illustrated in Figure <ref type="figure" target="#fig_2">2</ref>. Prompt tuning has been proved to be comparable to fine-tuning on 10billion-parameter models on simple classification tasks <ref type="bibr" target="#b11">(Lester et al., 2021;</ref><ref type="bibr" target="#b10">Kim et al., 2021;</ref><ref type="bibr" target="#b13">Liu et al., 2021)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">P-Tuning v2</head><p>3.1 Lack of Universality <ref type="bibr" target="#b11">Lester et al. (2021)</ref>; <ref type="bibr" target="#b13">Liu et al. (2021)</ref> have been proved quite effective in many NLP applications <ref type="bibr">(Wang et al., 2021a,b;</ref><ref type="bibr" target="#b3">Chen et al., 2021;</ref><ref type="bibr" target="#b32">Zheng et al., 2021;</ref><ref type="bibr" target="#b15">Min et al., 2021)</ref>, but still fall short at replacing fine-tuning due to lack of universality, as discussed below.</p><p>Lack of universality across scales. <ref type="bibr" target="#b11">Lester et al. (2021)</ref> shows that prompt tuning can be comparable to fine-tuning when the model scales to over 10 billion parameters. However, for medium-sized models (from 100M to 1B) that are widely used, prompt tuning performs much worse than fine-tuning.</p><p>Lack of universality across tasks. Though <ref type="bibr" target="#b11">Lester et al. (2021)</ref>; <ref type="bibr" target="#b13">Liu et al. (2021)</ref> have shown superiority on some of the NLU benchmarks, the effectiveness of prompt tuning on hard sequence tagging tasks is not verified. Sequence tagging predicts a sequence of labels for each input token, which can be harder and incompatible with verbalizers <ref type="bibr" target="#b22">(Schick and Schütze, 2020)</ref>. In our experiments (Cf. Section 4.2 and Table <ref type="table" target="#tab_3">3</ref>), we show that <ref type="bibr" target="#b11">Lester et al. (2021)</ref>; <ref type="bibr" target="#b13">Liu et al. (2021)</ref> perform poorly on typical sequence tagging tasks compared to fine-tuning.</p><p>Considering these challenges, we propose Ptuning v2, which adapts deep prompt tuning <ref type="bibr" target="#b12">(Li and Liang, 2021;</ref><ref type="bibr" target="#b17">Qin and Eisner, 2021)</ref> as a universal solution across scales and NLU tasks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Deep Prompt Tuning</head><p>In <ref type="bibr" target="#b11">(Lester et al., 2021)</ref> and <ref type="bibr" target="#b13">(Liu et al., 2021)</ref>, continuous prompts are only inserted into the input embedding sequence (Cf. Figure <ref type="figure" target="#fig_2">2 (a)</ref>). This leads </p><formula xml:id="formula_0">h 0 &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " X I h p n 8 O x N U v h Z R M 6 e 2 E n U b W V 7 t U = " &gt; A A A C x n i c j V H L S s N A F D 2 N r 1 p f V Z d u g k V w V R I R d F l 0 0 2 V F 2 w q 1 l C S d t k P z Y j J R S h H 8 A b f 6 a e I f 6 F 9 4 Z 5 y C W k Q n J D l z 7 j 1 n 5 t 7 r p y H P p O O 8 F q y F x a X l l e J q a W 1 9 Y 3 O r v L 3 T y p J c B K w Z J G E i r n 0 v Y y G P W V N y G b L r V D A v 8 k P W 9 s f n K t 6 + Z S L j S X w l J y n r R t 4 w 5 g M e e J K o y 1 H P 6 Z U r T t X R y 5 4 H r g E V m N V I y i + 4 Q R 8 J A u S I w B B D E g 7 h I a O n A x c O U u K 6 m B I n C H E d Z 7 h H i b Q 5 Z T H K 8 I g d 0 3 d I u 4 5 h Y 9 o r z 0 y r A z o l p F e Q 0 s Y B a R L K E 4 T V a b a O 5 9 p Z s b 9 5 T 7 W n u t u E / r 7 x i o i V G B H 7 l 2 6 W + V + d q k V i g F N d A 6 e a U s 2 o 6 g L j k u u u q J v b X 6 q S 5 J A S p 3 C f 4 o J w o J W z P t t a k + n a V W 8 9 H X / T m Y p V + 8 D k 5 n h X t 6 Q B u z / H O Q 9 a R 1 X X q b o X x 5 X a m R l 1 E X v Y x y H N 8 w Q 1 1 N F A k 7 y H e M Q T n q 2 6 F V u 5 d f e Z a h W M Z h f f l v X w A e V 8 k B A = &lt; / l a t e x i t &gt; &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " X I h p n 8 O x N U v h Z R M 6 e 2 E n U b W V 7 t U = " &gt; A A A C x n i c j V H L S s N A F D 2 N r 1 p f V Z d u g k V w V R I R d F l 0 0 2 V F 2 w q 1 l C S d t k P z Y j J R S h H 8 A b f 6 a e I f 6 F 9 4 Z 5 y C W k Q n J D l z 7 j 1 n 5 t 7 r p y H P p O O 8 F q y F x a X l l e J q a W 1 9 Y 3 O r v L 3 T y p J c B K w Z J G E i r n 0 v Y y G P W V N y G b L r V D A v 8 k P W 9 s f n K t 6 + Z S L j S X w l J y n r R t 4 w 5 g M e e J K o y 1 H P 6 Z U r T t X R y 5 4 H r g E V m N V I y i + 4 Q R 8 J A u S I w B B D E g 7 h I a O n A x c O U u K 6 m B I n C H E d Z 7 h H i b Q 5 Z T H K 8 I g d 0 3 d I u 4 5 h Y 9 o r z 0 y r A z o l p F e Q 0 s Y B a R L K E 4 T V a b a O 5 9 p Z s b 9 5 T 7 W n u t u E / r 7 x i o i V G B H 7 l 2 6 W + V + d q k V i g F N d A 6 e a U s 2 o 6 g L j k u u u q J v b X 6 q S 5 J A S p 3 C f 4 o J w o J W z P t t a k + n a V W 8 9 H X / T m Y p V + 8 D k 5 n h X t 6 Q B u z / H O Q 9 a R 1 X X q b o X x 5 X a m R l 1 E X v Y x y H N 8 w Q 1 1 N F A k 7 y H e M Q T n q 2 6 F V u 5 d f e Z a h W M Z h f f l v X w A e V 8 k B A = &lt; / l a t e x i t &gt; &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " X I h p n 8 O x N U v h Z R M 6 e 2 E n U b W V 7 t U = " &gt; A A A C x n i c j V H L S s N A F D 2 N r 1 p f V Z d u g k V w V R I R d F l 0 0 2 V F 2 w q 1 l C S d t k P z Y j J R S h H 8 A b f 6 a e I f 6 F 9 4 Z 5 y C W k Q n J D l z 7 j 1 n 5 t 7 r p y H P p O O 8 F q y F x a X l l e J q a W 1 9 Y 3 O r v L 3 T y p J c B K w Z J G E i r n 0 v Y y G P W V N y G b L r V D A v 8 k P W 9 s f n K t 6 + Z S L j S X w l J y n r R t 4 w 5 g M e e J K o y 1 H P 6 Z U r T t X R y 5 4 H r g E V m N V I y i + 4 Q R 8 J A u S I w B B D E g 7 h I a O n A x c O U u K 6 m B I n C H E d Z 7 h H i b Q 5 Z T H K 8 I g d 0 3 d I u 4 5 h Y 9 o r z 0 y r A z o l p F e Q 0 s Y B a R L K E 4 T V a b a O 5 9 p Z s b 9 5 T 7 W n u t u E / r 7 x i o i V G B H 7 l 2 6 W + V + d q k V i g F N d A 6 e a U s 2 o 6 g L j k u u u q J v b X 6 q S 5 J A S p 3 C f 4 o J w o J W z P t t a k + n a V W 8 9 H X / T m Y p V + 8 D k 5 n h X t 6 Q B u z / H O Q 9 a R 1 X X q b o X x 5 X a m R l 1 E X v Y x y H N 8 w Q 1 1 N F A k 7 y H e M Q T n q 2 6 F V u 5 d f e Z a h W M Z h f f l v X w A e V 8 k B A = &lt; / l a t e x i t &gt; &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " X I h p n 8 O x N U v h Z R M 6 e 2 E n U b W V 7 t U = " &gt; A A A C x n i c j V H L S s N A F D 2 N r 1 p f V Z d u g k V w V R I R d F l 0 0 2 V F 2 w q 1 l C S d t k P z Y j J R S h H 8 A b f 6 a e I f 6 F 9 4 Z 5 y C W k Q n J D l z 7 j 1 n 5 t 7 r p y H P p O O 8 F q y F x a X l l e J q a W 1 9 Y 3 O r v L 3 T y p J c B K w Z J G E i r n 0 v Y y G P W V N y G b L r V D A v 8 k P W 9 s f n K t 6 + Z S L j S X w l J y n r R t 4 w 5 g M e e J K o y 1 H P 6 Z U r T t X R y 5 4 H r g E V m N V I y i + 4 Q R 8 J A u S I w B B D E g 7 h I a O n A x c O U u K 6 m B I n C H E d Z 7 h H i b Q 5 Z T H K 8 I g d 0 3 d I u 4 5 h Y 9 o r z 0 y r A z o l p F e Q 0 s Y B a R L K E 4 T V a b a O 5 9 p Z s b 9 5 T 7 W n u t u E / r 7 x i o i V G B H 7 l 2 6 W + V + d q k V i g F N d A 6 e a U s 2 o 6 g L j k u u u q J v b X 6 q S 5 J A S p 3 C f 4 o J w o J W z P t t a k + n a V W 8 9 H X / T m Y p V + 8 D k 5 n h X t 6 Q B u z / H O Q 9 a R 1 X X q b o X x 5 X a m R l 1 E X v Y x y H N 8 w Q 1 1 N F A k 7 y H e M Q T n q 2 6 F V u 5 d f e Z a h W M Z h f f l v X w A e V 8 k B A = &lt; / l a t e x i t &gt; h i &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " n W n m f x j 2 c + W F Y 4 V a m C 6 w C 0 3 + U x 4 = " &gt; A A A C x n i c j V H L S s N A F D 2 N r 1 p f V Z d u g k V w V R I R d F l 0 0 2 V F 2 w q 1 l C S d t k P z Y j J R S h H 8 A b f 6 a e I f 6 F 9 4 Z 5 y C W k Q n J D l z 7 j 1 n 5 t 7 r p y H P p O O 8 F q y F x a X l l e J q a W 1 9 Y 3 O r v L 3 T y p J c B K w Z J G E i r n 0 v Y y G P W V N y G b L r V D A v 8 k P W 9 s f n K t 6 + Z S L j S X w l J y n r R t 4 w 5 g M e e J K o y 1 G P 9 8 o V p + r o Z c 8 D 1 4 A K z G o k 5 R f c o I 8 E A X J E Y I g h C Y f w k N H T g Q s H K X F d T I k T h L i O M 9 y j R N q c s h h l e M S O 6 T u k X c e w M e 2 V Z 6 b V A Z 0 S 0 i t I a e O A N A n l C c L q N F v H c + 2 s 2 N + 8 p 9 p T 3 W 1 C f 9 9 4 R c R K j I j 9 S z f L / K 9 O 1 S I x w K m u g V N N q W Z U d Y F x y X V X 1 M 3 t L 1 V J c k i J U 7 h P c U E 4 0 M p Z</formula><p>n 2 2 t y X T t q r e e j r / p T M W q f W B y c 7 y r W 9 K A 3 Z / j n A e t o 6 r r V N 2 L 4 0 r t z I y 6 i D 3 s 4 5 D m e Y I a 6 m i g S d 5 D</p><formula xml:id="formula_1">P O I J z 1 b d i q 3 c u v t M t Q p G s 4 t v y 3 r 4 A G z r k E k = &lt; / l a t e x i t &gt; &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " n W n m f x j 2 c + W F Y 4 V a m C 6 w C 0 3 + U x 4 = " &gt; A A A C x n i c j V H L S s N A F D 2 N r 1 p f V Z d u g k V w V R I R d F l 0 0 2 V F 2 w q 1 l C S d t k P z Y j J R S h H 8 A b f 6 a e I f 6 F 9 4 Z 5 y C W k Q n J D l z 7 j 1 n 5 t 7 r p y H P p O O 8 F q y F x a X l l e J q a W 1 9 Y 3 O r v L 3 T y p J c B K w Z J G E i r n 0 v Y y G P W V N y G b L r V D A v 8 k P W 9 s f n K t 6 + Z S L j S X w l J y n r R t 4 w 5 g M e e J K o y 1 G P 9 8 o V p + r o Z c 8 D 1 4 A K z G o k 5 R f c o I 8 E A X J E Y I g h C Y f w k N H T g Q s H K X F d T I k T h L i O M 9 y j R N q c s h h l e M S O 6 T u k X c e w M e 2 V Z 6 b V A Z 0 S 0 i t I a e O A N A n l C c L q N F v H c + 2 s 2 N + 8 p 9 p T 3 W 1 C f 9 9 4 R c R K j I j 9 S z f L / K 9 O 1 S I x w K m u g V N N q W Z U d Y F x y X V X 1 M 3 t L 1 V J c k i J U 7 h P c U E 4 0 M p Z</formula><p>n 2 2 t y X T t q r e e j r / p T M W q f W B y c 7 y r W 9 K A 3 Z / j n A e t o 6 r r V N 2 L 4 0 r t z I y 6 i D 3 s 4 5 D m e Y I a 6 m i g S d 5 D P O I J z 1 b d i q 3 c u v t M t Q p G s 4 t v y 3 r 4 A G z r k E k = &lt; / l a t e x i t &gt; &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " n W n m f</p><formula xml:id="formula_2">x j 2 c + W F Y 4 V a m C 6 w C 0 3 + U x 4 = " &gt; A A A C x n i c j V H L S s N A F D 2 N r 1 p f V Z d u g k V w V R I R d F l 0 0 2 V F 2 w q 1 l C S d t k P z Y j J R S h H 8 A b f 6 a e I f 6 F 9 4 Z 5 y C W k Q n J D l z 7 j 1 n 5 t 7 r p y H P p O O 8 F q y F x a X l l e J q a W 1 9 Y 3 O r v L 3 T y p J c B K w Z J G E i r n 0 v Y y G P W V N y G b L r V D A v 8 k P W 9 s f n K t 6 + Z S L j S X w l J y n r R t 4 w 5 g M e e J K o y 1 G P 9 8 o V p + r o Z c 8 D 1 4 A K z G o k 5 R f c o I 8 E A X J E Y I g h C Y f w k N H T g Q s H K X F d T I k T h L i O M 9 y j R N q c s h h l e M S O 6 T u k X c e w M e 2 V Z 6 b V A Z 0 S 0 i t I a e O A N A n l C c L q N F v H c + 2 s 2 N + 8 p 9 p T 3 W 1 C f 9 9 4 R c R K j I j 9 S z f L / K 9 O 1 S I x w K m u g V N N q W Z U d Y F x y X V X 1 M 3 t L 1 V J c k i J U 7 h P c U E 4 0 M p Z</formula><p>n 2 2 t y X T t q r e e j r / p T M W q f W B y c 7 y r W 9 K A 3 Z / j n A e t o 6 r r V N 2 L 4 0 r t z I y 6 i D 3 s 4 5 D m e Y I a 6 m i g S d 5 D P O I J z 1 b d i q 3 c u v t M t Q p G s 4 t v y 3 r 4 A G z r k E k = &lt; / l a t e x i t &gt; &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " n W n m f</p><formula xml:id="formula_3">x j 2 c + W F Y 4 V a m C 6 w C 0 3 + U x 4 = " &gt; A A A C x n i c j V H L S s N A F D 2 N r 1 p f V Z d u g k V w V R I R d F l 0 0 2 V F 2 w q 1 l C S d t k P z Y j J R S h H 8 A b f 6 a e I f 6 F 9 4 Z 5 y C W k Q n J D l z 7 j 1 n 5 t 7 r p y H P p O O 8 F q y F x a X l l e J q a W 1 9 Y 3 O r v L 3 T y p J c B K w Z J G E i r n 0 v Y y G P W V N y G b L r V D A v 8 k P W 9 s f n K t 6 + Z S L j S X w l J y n r R t 4 w 5 g M e e J K o y 1 G P 9 8 o V p + r o Z c 8 D 1 4 A K z G o k 5 R f c o I 8 E A X J E Y I g h C Y f w k N H T g Q s H K X F d T I k T h L i O M 9 y j R N q c s h h l e M S O 6 T u k X c e w M e 2 V Z 6 b V A Z 0 S 0 i t I a e O A N A n l C c L q N F v H c + 2 s 2 N + 8 p 9 p T 3 W 1 C f 9 9 4 R c R K j I j 9 S z f L / K 9 O 1 S I x w K m u g V N N q W Z U d Y F x y X V X 1 M 3 t L 1 V J c k i J U 7 h P c U E 4 0 M p Z</formula><p>n 2 2 t y X T t q r e e j r / p T M W q f W B y c 7 y r W 9 K A 3 Z / j n A e t o 6 r r V N 2 L 4 0 r t z I y 6 i D 3 s 4 5 D m e Y I a 6 m i g S d 5 D P O I J z 1 b d i q 3 c u v t M t Q p G s 4 t v y 3 r 4 A G z r k E k = &lt; / l a t e x i t &gt; … (b) P-tuning v2 (Frozen, most scales, most tasks)</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Amazing movie ! e(Amazing) e(moive) e(!)</head><p>[CLS]</p><p>e([CLS])</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>…</head><p>Amazing movie ! e(Amazing) e(moive) e(!)</p><p>[CLS]</p><p>Class Label (with linear head)</p><formula xml:id="formula_4">… … … … … … … … … … … … … … … … … Layer1 Prompts Layer2 Prompts LayerN Prompts … Reparameterization (Optional)</formula><p>… …</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>e([CLS])</head><p>Optimization Optimization  <ref type="formula">2021</ref>) &amp; P-tuning to P-tuning v2. Orange blocks (i.e., h 0 , ..., h i ) refer to trainable prompt embeddings; blue blocks are embeddings stored or computed by frozen pre-trained language models.</p><p>to two challenges. First, the number of tunable parameters is limited due to the constraints of sequence length. Second, the input embeddings have relatively indirect impact on model predictions.</p><p>To address these challenges, P-tuning v2 employs the idea of deep prompt tuning (Li and Liang, 2021; <ref type="bibr" target="#b17">Qin and Eisner, 2021)</ref>. As illustrated in Figure <ref type="figure" target="#fig_2">2</ref>, prompts in different layers are added as prefix tokens. On one hand, P-tuning v2 have more tunable task-specific parameters (from 0.01% to 0.1%-3%) to allow more per-task capacity while being parameter-efficient; on the other hand, prompts added to deeper layers have more direct impact on model predictions (see analysis in Appendix B).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Optimization and Implementation</head><p>There are a few useful details of optimization and implementation for achieving the best performance.</p><p>Reparameterization. Prior works usually leverage a reparameterization encoder such as an MLP <ref type="bibr" target="#b12">(Li and Liang, 2021;</ref><ref type="bibr" target="#b13">Liu et al., 2021)</ref> to transform trainable embeddings. However, for NLU, we discover that its usefulness depends on tasks and datasets. For some datasets (e.g., RTE and CoNLL04), MLP brings a consistent improvement; for the others, MLP leads to minimal or even negative effects on the results (e.g., BoolQ and CoNLL12). See Appendix B for more analysis.</p><p>Prompt Length. The prompt length plays a critical role in P-Tuning v2. We find that different NLU tasks usually achieve their best performance with different prompt lengths (Cf. Appendix B). Generally, simple classification tasks prefer shorter prompts (less than 20); hard sequence labeling tasks prefer longer ones (around 100).</p><p>Multi-task Learning. Multi-task learning jointly optimizes multiple tasks with shared continuous prompts before fine-tuning for individual tasks. Multi-task is optional for P-Tuning v2 but can be used for further boost performance by providing a better initialization <ref type="bibr" target="#b8">(Gu et al., 2021)</ref>.</p><p>Classification Head. Using a language modeling head to predict verbalizers <ref type="bibr" target="#b22">(Schick and Schütze, 2020)</ref> has been central for prompt tuning <ref type="bibr" target="#b13">(Liu et al., 2021)</ref>, but we find it unnecessary in a full-data setting and incompatible with sequence labeling. P-tuning v2 instead applies a randomly-initialized classification head on top of the tokens as in BERT <ref type="bibr">(Devlin et al., 2018</ref>) (Cf. Figure <ref type="figure" target="#fig_2">2</ref>). To clarify P-tuning v2's major contribution, we present a conceptual comparison to existing prompt tuning approaches in Table <ref type="table" target="#tab_1">1</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experiments</head><p>We conduct extensive experiments over different commonly-used pre-trained models and NLU tasks to verify the effectiveness of P-tuning v2. In this work, all methods except for fine-tuning are conducted with frozen language model backbones, which accords with <ref type="bibr" target="#b11">(Lester et al., 2021)</ref>'s setting but differs from <ref type="bibr" target="#b13">(Liu et al., 2021)</ref> derived from comparing continuous prompts' parameters with transformers' parameters. Another thing to notice is that our experiments are all conducted in the fully-supervised setting rather than few-shot setting.</p><p>NLU Tasks. First, we include datasets from Su-perGLUE <ref type="bibr">(Wang et al., 2019)</ref> to test P-tuning v2's general NLU ability. Additionally, we introduce a suite of sequence labeling tasks, including named entity recognition <ref type="bibr" target="#b21">(Sang and De Meulder, 2003;</ref><ref type="bibr" target="#b29">Weischedel et al., 2013;</ref><ref type="bibr" target="#b1">Carreras and Màrquez, 2004)</ref>, extractive Question Answering <ref type="bibr" target="#b20">(Rajpurkar et al., 2016)</ref>, and semantic role labeling <ref type="bibr" target="#b2">(Carreras and Màrquez, 2005;</ref><ref type="bibr" target="#b16">Pradhan et al., 2012)</ref>).</p><p>Pre-trained Models. We include BERT-large <ref type="bibr">(Devlin et al., 2018)</ref>, RoBERTa-large <ref type="bibr" target="#b14">(Liu et al., 2019)</ref>, DeBERTa-xlarge <ref type="bibr" target="#b9">(He et al., 2020)</ref>, GLMxlarge/xxlarge <ref type="bibr" target="#b6">(Du et al., 2021)</ref> for evaluation. They are all bidirectional models designed for NLU tasks, covering a wide range of sizes from about 300M to 10B.</p><p>Multitask Learning. For the multi-task setting, we combine the training sets of the datasets in each task type (e.g., combing all training sets of semantic role labeling). We use separate linear classifiers for each dataset while sharing the continuous prompts (Cf. Appendix A).  <ref type="formula">2021</ref>) and P-tuning at smaller scales can be quite poor. On the contrary, P-tuning v2 matches the fine-tuning performance in all the tasks at a smaller scale. P-tuning v2 even significantly outperforms fine-tuning on RTE.</p><p>In terms of larger scales (2B to 10B) with GLM <ref type="bibr" target="#b6">(Du et al., 2021)</ref>, the gap between <ref type="bibr" target="#b11">Lester et al. (2021)</ref>; <ref type="bibr" target="#b13">Liu et al. (2021)</ref> and fine-tuning is gradually narrowed down. On 10B scale, we have a similar observation as <ref type="bibr" target="#b11">Lester et al. (2021)</ref> reports, that prompt tuning becomes competitive to fine-tuning. That said, P-tuning v2 is always comparable to fine-tuning at all scales but with only 0.1% task-specific parameters needed comparing to fine-tuning.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">P-tuning v2: Across Tasks</head><p>From Table <ref type="table" target="#tab_3">3</ref>, we observe that P-tuning v2 can be generally comparable to fine-tuning on all tasks. Ptuning and <ref type="bibr" target="#b11">Lester et al. (2021)</ref> show much poorer performance, especially on QA, which might be the most challenging of the three tasks. We also notice that there are some abnormal results of <ref type="bibr" target="#b11">Lester et al. (2021)</ref> and P-tuning on SQuAD 2.0. This is probably because SQuAD 2.0 contains unanswerable questions, which causes optimization challenges for single-layer prompt tuning. Multi-task learning generally brings significant improvements to P-Tuning v2 over most tasks except for QA.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Ablation Study</head><p>Verbalizer with LM head v.s. [CLS] label with linear head. Verbalizer with LM head has been a central component in previous prompt tuning approaches. However, for P-tuning v2 in a supervised setting, it is affordable to tune a linear head with about several thousand parameters. We present our comparison in Table <ref type="table" target="#tab_4">4</ref>, where we keep other hyperparameters and only change [CLS] label with linear head to verbalizer with LM head. Here, for simplicity, we use "true" and "false" for SST-2, RTE and  BoolQ; "true", "false" and "neutral" for CB. Results indicate that there is no significant difference between performances of verbalizer and [CLS].</p><p>Prompt depth. The main difference between <ref type="bibr" target="#b11">Lester et al. (2021)</ref>; <ref type="bibr" target="#b13">(Liu et al., 2021)</ref> and P-tuning v2 is the multi-layer continuous prompts. To verify its exact influence, given a certain number of k layers to add prompts, we select them in both ascending and descending order to add prompts; for the rest layers, we left them untouched. As shown in Figure <ref type="figure" target="#fig_4">3</ref>, with the same amount of parameters (i.e., num of transformer layers to add prompts), adding them in the descending order is always better than in the ascending order. In the RTE case, only adding prompts to layers 17-24 can yield a very close performance to all layers.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusions</head><p>We present P-tuning v2, a prompt tuning method. Despite its relatively limited technical novelty, it contributes to a novel finding that prompt tuning can be comparable to fine-tuning universally across scales (from 330M to 10B parameters) and tasks.</p><p>With high accuracy and parameter efficiency, P-Tuning v2 can be a potential alternative for finetuning and a strong baseline for future work.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A Problem Formulation on Sequence Tagging</head><p>Name entity recognition (NER). NER aims to predict all spans of words that represent some given classes of entity with a sentence. We adopted CoNLL03 (Sang and De Meulder, 2003), OntoNotes 5.0 <ref type="bibr" target="#b29">(Weischedel et al., 2013)</ref> and CoNLL04 <ref type="bibr" target="#b1">(Carreras and Màrquez, 2004)</ref>. For CoNLL03 and CoNLL04, we trained our model on the standard train-develop-test split. For OntoNotes 5.0, we use the same train, develop, test split as <ref type="bibr" target="#b30">(Xu et al., 2021)</ref>. All the datasets are labeled in IOB2 format. We use sequence tagging to solve NER tasks by assigning labels marking the beginning and inside some classes of entity. The language models generate a representation for each token, and we use a linear classifier to predict the labels.</p><p>We use the official scripts to evaluate the results.</p><p>For the multi-task setting, we combine the training set of the three datasets for pre-training. We use different linear classifiers for each dataset while sharing the continuous prompts.</p><p>(Extractive) Question Answering (QA). Extractive QA is designed to extract the answer from the context given the context and a question. We use SQuAD <ref type="bibr" target="#b20">(Rajpurkar et al., 2016)</ref> 1.1 and 2.0, in which each answer is within a continuous span of the context. Following tradition, we formulate the problem as sequence tagging by assigning one of the two labels: 'start' or 'end' to each token and at last selecting the span of the most confident startend pair as the extracted answer. If the probability of the most confident pair is lower than a threshold, the model will assume the question unanswerable.</p><p>For the multi-task setting, our training set for pretraining combines the training sets of SQuAD 1.1 and 2.0. When pre-training, we assume that all the questions, regardless of their origin, are possibly unanswerable.</p><p>Semantic Role Labeling (SRL). SRL assigns labels to words or phrases in a sentence that indicate their semantic roles in the sentence. We evaluate P-tuning v2 on CoNLL05 <ref type="bibr" target="#b2">(Carreras and Màrquez, 2005)</ref> and CoNLL12 <ref type="bibr" target="#b16">(Pradhan et al., 2012)</ref>. Since a sentence can have multiple verbs, we add the target verb token to the end of each sentence to help recognize which verb is used for prediction. We classify each word with a linear classifier based on the corresponding semantic role representation. For multitask setting, the pre-train training set is a combina- </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B More Ablation Study</head><p>Due to the page limit, we present hyper-parameters and architecture designs ablations regarding reparameterization and prompt length in this section.</p><p>Embedding v.s. MLP reparameterization. In both prefix-tuning (Li and Liang, 2021) and Ptuning <ref type="bibr" target="#b13">(Liu et al., 2021)</ref>, authors discover the reparameterization to be useful in improving training speed, robustness and performance. However, we conduct experiments to show that the reparameterization effect is inconsistent across different NLU tasks and datasets. As shown in Figure <ref type="figure" target="#fig_5">4</ref>, in RTE and CoNLL04, MLP reparameterization generally indicates better performance than embedding for almost all prompt lengths. However, in BoolQ, MLP and embedding's results are competitive; in CoNLL12, the embedding consistently outperforms MLP.</p><p>Prompt Length. Prompt length is yet another influential hyper-parameter for P-tuning v2, and its optimal value varies from task to task. From Figure <ref type="figure" target="#fig_5">4</ref>, we observe that for simple NLU tasks, usually, a shorter prompt is enough for the best performance; for hard sequence tasks, usually, a longer prompt than 100 would be helpful.</p><p>We also discover that reparameterization has a close bond with optimal prompt length. For example, in RTE, CoNLL04, and BoolQ, MLP reparameterization achieves its optimal result earlier than embedding. This conclusion may contribute some thoughts on P-tuning's optimization properties.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: Average scores on RTE, BoolQ and CB of SuperGLUE dev. With 0.1% task-specific parameters, P-tuning v2 can match fine-tuning across wide scales of pre-trained models, while Lester et al. (2021) &amp; Ptuning can make it conditionally at 10B scale.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: From Lester et al. (2021) &amp; P-tuning to P-tuning v2. Orange blocks (i.e., h 0 , ..., h i ) refer to trainable prompt embeddings; blue blocks are embeddings stored or computed by frozen pre-trained language models.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 3 :</head><label>3</label><figDesc>Figure3: Ablation study on prompt depth using BERTlarge. "[x-y]" refers to the layer-interval we add continuous prompts (e.g., "21-24" means we are add prompts to transformer layers from 21 to 24). Same amount of continuous prompts added to deeper transformer layers (i.e., more close to the output layer) can yield a better performance than those added to beginning layers.</figDesc><graphic url="image-1.png" coords="5,311.97,76.30,99.22,85.81" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 4 :</head><label>4</label><figDesc>Figure 4: Ablation study on prompt length and reparamerization using RoBERTa-large. The conclusion can be very different given certain NLU task and dataset. (MQA: Multiple-choice QA)</figDesc><graphic url="image-3.png" coords="8,76.73,75.83,104.32,76.13" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>e([MASK])</head><label></label><figDesc></figDesc><table><row><cell>Prompt Encoder (Optional)</cell><cell></cell></row><row><cell>[MASK]</cell><cell></cell></row><row><cell>Transformers</cell><cell>Transformers</cell></row><row><cell>Verbalizer (with LM head)</cell><cell></cell></row><row><cell>(a) Lester et al. &amp; P-tuning (Frozen, 10-billion-scale, simple tasks)</cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 1 :</head><label>1</label><figDesc></figDesc><table><row><cell>Method</cell><cell>Task</cell><cell>Re-param.</cell><cell>Deep PT</cell><cell>Multi-task</cell><cell>No verb.</cell></row><row><cell>P-tuning (Liu et al., 2021)</cell><cell>KP NLU</cell><cell>LSTM</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell>PROMPTTUNING (Lester et al., 2021)</cell><cell cols="2">NLU -</cell><cell>-</cell><cell>✓</cell><cell>-</cell></row><row><cell>Prefix Tuning (Li and Liang, 2021)</cell><cell cols="2">NLG MLP</cell><cell>✓</cell><cell>-</cell><cell>-</cell></row><row><cell>SOFT PROMPTS (Qin and Eisner, 2021)</cell><cell>KP</cell><cell>-</cell><cell>✓</cell><cell>-</cell><cell>-</cell></row><row><cell>P-tuning v2 (Ours)</cell><cell>NLU SeqTag</cell><cell cols="2">(depends) ✓</cell><cell>✓</cell><cell>✓</cell></row></table><note>Conceptual comparison between P-tuning v2 and existing Prompt Tuning approaches (KP: Knowledge Probe; SeqTag: Sequence Tagging; Re-param.: Reparameterization; No verb.: No verbalizer).</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2 :</head><label>2</label><figDesc>'s tuned setting. Ratios of task-specific parameters (e.g., 0.1%) are Results on SuperGLUE development set. P-tuning v2 significantly surpasses P-tuning &amp; Lester et al. (2021) on models smaller than 10B, and matches the performance of fine-tuning across different model scales. (FT: fine-tuning; PT: Lester et al. (2021) &amp; P-tuning; PT-2: P-tuning v2; bold: the best; underline: the second best). RoBERTa large 355M 88.9 94.6 1.2 12.0 88.5 94.4 88.0 94.1 86.5 89.4 50.2 50.2 82.1 85.5 83.4 86.7 DeBERTa xlarge 750M 90.1 95.5 2.4 19.0 90.4 95.7 89.6 95.4 88.3 91.1 50.2 50.2 88.4 91.1 88.1 90.8</figDesc><table><row><cell></cell><cell>#Size</cell><cell></cell><cell cols="2">BoolQ</cell><cell></cell><cell>CB</cell><cell></cell><cell></cell><cell>COPA</cell><cell></cell><cell cols="2">MultiRC (F1a)</cell></row><row><cell></cell><cell></cell><cell>FT</cell><cell>PT</cell><cell>PT-2</cell><cell>FT</cell><cell>PT</cell><cell>PT-2</cell><cell>FT</cell><cell>PT</cell><cell>PT-2</cell><cell>FT</cell><cell>PT</cell><cell>PT-2</cell></row><row><cell>BERT large</cell><cell cols="3">335M 77.7 67.2</cell><cell>75.8</cell><cell cols="2">94.6 80.4</cell><cell>94.6</cell><cell cols="2">69.0 55.0</cell><cell>73.0</cell><cell cols="2">70.5 59.6 70.6</cell></row><row><cell cols="4">RoBERTa large 355M 86.9 62.3</cell><cell>84.8</cell><cell cols="2">98.2 71.4</cell><cell>100</cell><cell cols="2">94.0 63.0</cell><cell>93.0</cell><cell cols="2">85.7 59.9 82.5</cell></row><row><cell>GLM xlarge</cell><cell>2B</cell><cell cols="2">88.3 79.7</cell><cell>87.0</cell><cell cols="2">96.4 76.4</cell><cell>96.4</cell><cell cols="2">93.0 92.0</cell><cell>91.0</cell><cell cols="2">84.1 77.5 84.4</cell></row><row><cell>GLM xxlarge</cell><cell>10B</cell><cell cols="2">88.7 88.8</cell><cell>88.8</cell><cell cols="2">98.7 98.2</cell><cell>96.4</cell><cell cols="2">98.0 98.0</cell><cell>98.0</cell><cell cols="2">88.1 86.1 88.1</cell></row><row><cell></cell><cell>#Size</cell><cell cols="3">ReCoRD (F1)</cell><cell></cell><cell>RTE</cell><cell></cell><cell></cell><cell>WiC</cell><cell></cell><cell></cell><cell>WSC</cell></row><row><cell></cell><cell></cell><cell>FT</cell><cell>PT</cell><cell>PT-2</cell><cell>FT</cell><cell>PT</cell><cell>PT-2</cell><cell>FT</cell><cell>PT</cell><cell>PT-2</cell><cell>FT</cell><cell>PT</cell><cell>PT-2</cell></row><row><cell>BERT large</cell><cell cols="3">335M 70.6 44.2</cell><cell>72.8</cell><cell cols="2">70.4 53.5</cell><cell>78.3</cell><cell cols="2">74.9 63.0</cell><cell>75.1</cell><cell cols="2">68.3 64.4 68.3</cell></row><row><cell cols="4">RoBERTa large 355M 89.0 46.3</cell><cell>89.3</cell><cell cols="2">86.6 58.8</cell><cell>89.5</cell><cell cols="2">75.6 56.9</cell><cell>73.4</cell><cell cols="2">63.5 64.4 63.5</cell></row><row><cell>GLM xlarge</cell><cell>2B</cell><cell cols="2">91.8 82.7</cell><cell>91.9</cell><cell cols="2">90.3 85.6</cell><cell>90.3</cell><cell cols="2">74.1 71.0</cell><cell>72.0</cell><cell cols="2">95.2 87.5 92.3</cell></row><row><cell>GLM xxlarge</cell><cell>10B</cell><cell cols="2">94.4 87.8</cell><cell>92.5</cell><cell cols="2">93.1 89.9</cell><cell>93.1</cell><cell cols="2">75.7 71.8</cell><cell>74.0</cell><cell cols="2">95.2 94.2 93.3</cell></row><row><cell></cell><cell>#Size</cell><cell></cell><cell cols="2">CoNLL03</cell><cell></cell><cell></cell><cell cols="2">OntoNotes 5.0</cell><cell></cell><cell></cell><cell cols="2">CoNLL04</cell></row><row><cell></cell><cell></cell><cell>FT</cell><cell>PT</cell><cell cols="2">PT-2 MPT-2</cell><cell>FT</cell><cell>PT</cell><cell cols="2">PT-2 MPT-2</cell><cell>FT</cell><cell>PT</cell><cell>PT-2 MPT-2</cell></row><row><cell>BERT large</cell><cell cols="4">335M 92.8 81.9 90.2</cell><cell>91.0</cell><cell cols="3">89.2 74.6 86.4</cell><cell>86.3</cell><cell cols="3">85.6 73.6 84.5</cell><cell>86.6</cell></row><row><cell cols="5">RoBERTa large 355M 92.6 86.1 92.8</cell><cell>92.8</cell><cell cols="3">89.8 80.8 89.8</cell><cell>89.8</cell><cell cols="3">88.8 76.2 88.4</cell><cell>90.6</cell></row><row><cell cols="5">DeBERTa xlarge 750M 93.1 90.2 93.1</cell><cell>93.1</cell><cell cols="3">90.4 85.1 90.4</cell><cell>90.5</cell><cell cols="3">89.1 82.4 86.5</cell><cell>90.1</cell></row><row><cell></cell><cell></cell><cell></cell><cell cols="4">SQuAD 1.1 dev (EM / F1)</cell><cell></cell><cell></cell><cell cols="4">SQuAD 2.0 dev (EM / F1)</cell></row><row><cell></cell><cell>#Size</cell><cell>FT</cell><cell></cell><cell>PT</cell><cell>PT-2</cell><cell cols="2">MPT-2</cell><cell>FT</cell><cell></cell><cell>PT</cell><cell>PT-2</cell><cell>MPT-2</cell></row><row><cell>BERT large</cell><cell cols="12">335M 84.2 91.1 1.0 8.5 77.8 86.0 82.3 89.6 78.7 81.9 50.2 50.2 69.7 73.5 72.7 75.9</cell></row><row><cell></cell><cell>#Size</cell><cell></cell><cell cols="2">CoNLL12</cell><cell></cell><cell></cell><cell cols="2">CoNLL05 WSJ</cell><cell></cell><cell></cell><cell cols="2">CoNLL05 Brown</cell></row><row><cell></cell><cell></cell><cell>FT</cell><cell>PT</cell><cell cols="2">PT-2 MPT-2</cell><cell>FT</cell><cell>PT</cell><cell cols="2">PT-2 MPT-2</cell><cell>FT</cell><cell>PT</cell><cell>PT-2 MPT-2</cell></row><row><cell>BERT large</cell><cell cols="4">335M 84.9 64.5. 83.2</cell><cell>85.1</cell><cell cols="3">88.5 76.0 86.3</cell><cell>88.5</cell><cell cols="3">82.7 70.0 80.7</cell><cell>83.1</cell></row><row><cell cols="5">RoBERTa large 355M 86.5 67.2 84.6</cell><cell>86.2</cell><cell cols="3">90.2 76.8 89.2</cell><cell>90.0</cell><cell cols="3">85.6 70.7 84.3</cell><cell>85.7</cell></row><row><cell cols="5">DeBERTa xlarge 750M 86.5 74.1 85.7</cell><cell>87.1</cell><cell cols="3">91.2 82.3 90.6</cell><cell>91.2</cell><cell cols="3">86.9 77.7 86.3</cell><cell>87.0</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 3 :</head><label>3</label><figDesc></figDesc><table /><note>Results on Named Entity Recognition (NER), Question Answering (Extractive QA), and Semantic Role Labeling (SRL). All metrics in NER and SRL are micro-f1 score. (FT: fine-tuning; PT: P-tuning &amp; Lester et al. (2021); PT-2: P-tuning v2; MPT-2: Multi-task P-tuning v2; bold: the best; underline: the second best).</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 4 :</head><label>4</label><figDesc>Comparison between [CLS] label with linear head and verbalizer with LM head on RoBERTa-large.</figDesc><table><row><cell></cell><cell cols="2">SST-2 RTE BoolQ CB</cell></row><row><cell>CLS &amp; linear head</cell><cell>96.3</cell><cell>88.4 84.8 96.4</cell></row><row><cell>Verbalizer &amp; LM head</cell><cell>95.8</cell><cell>86.6 84.6 94.6</cell></row><row><cell cols="3">4.1 P-tuning v2: Across Scales</cell></row><row><cell cols="3">Table 2 presents P-tuning v2's performances across</cell></row><row><cell cols="3">model scales. In SuperGLUE, performances of</cell></row><row><cell>Lester et al. (</cell><cell></cell><cell></cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_0">We use "prompt tuning" to refer to a class of methods rather than a particular method.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3" xml:id="foot_1">Note that the notions of "simple" and "hard" are specific to prompt tuning, because we find sequence labeling tasks are more challenging for prompt tuning.</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>ACKNOWLEDGEMENT</head><p>We would like to thank the anonymous reviewers for their suggestions and comments. Jie Tang is supported by the NSFC for Distinguished Young Scholar (61825602) and NSFC (61836013). Kaixuan Ji is supported by Tsinghua University Initiative Scientific Research Program and DCST Student Academic Training Program.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">Language models are few-shot learners</title>
		<author>
			<persName><forename type="first">Benjamin</forename><surname>Tom B Brown</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nick</forename><surname>Mann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Melanie</forename><surname>Ryder</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jared</forename><surname>Subbiah</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Prafulla</forename><surname>Kaplan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Arvind</forename><surname>Dhariwal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pranav</forename><surname>Neelakantan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Girish</forename><surname>Shyam</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Amanda</forename><surname>Sastry</surname></persName>
		</author>
		<author>
			<persName><surname>Askell</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2005.14165</idno>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Introduction to the CoNLL-2004 shared task: Semantic role labeling</title>
		<author>
			<persName><forename type="first">Xavier</forename><surname>Carreras</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lluís</forename><surname>Màrquez</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Eighth Conference on Computational Natural Language Learning</title>
				<meeting>the Eighth Conference on Computational Natural Language Learning<address><addrLine>Boston, Massachusetts, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2004">2004</date>
			<biblScope unit="page" from="89" to="97" />
		</imprint>
	</monogr>
	<note>HLT-NAACL 2004</note>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Introduction to the CoNLL-2005 shared task: Semantic role labeling</title>
		<author>
			<persName><forename type="first">Xavier</forename><surname>Carreras</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lluís</forename><surname>Màrquez</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Ninth Conference on Computational Natural Language Learning (CoNLL-2005)</title>
				<meeting>the Ninth Conference on Computational Natural Language Learning (CoNLL-2005)<address><addrLine>Ann Arbor, Michigan</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2005">2005</date>
			<biblScope unit="page" from="152" to="164" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">Adaprompt: Adaptive promptbased finetuning for relation extraction</title>
		<author>
			<persName><forename type="first">Xiang</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xin</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ningyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiahuan</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shumin</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chuanqi</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fei</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Luo</forename><surname>Si</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Huajun</forename><surname>Chen</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2104.07650</idno>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<author>
			<persName><forename type="first">Jacob</forename><surname>Devlin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ming-Wei</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kenton</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kristina</forename><surname>Toutanova</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1810.04805</idno>
		<title level="m">Pre-training of deep bidirectional transformers for language understanding</title>
				<meeting><address><addrLine>Bert</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<author>
			<persName><forename type="first">Jacob</forename><surname>Devlin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ming-Wei</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kenton</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kristina</forename><surname>Toutanova</surname></persName>
		</author>
		<title level="m">Pre-training of deep bidirectional transformers for language understanding</title>
				<meeting><address><addrLine>Bert</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note>arXiv e-prints</note>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">All nlp tasks are generation tasks: A general pretraining framework</title>
		<author>
			<persName><forename type="first">Zhengxiao</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yujie</forename><surname>Qian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiao</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ming</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiezhong</forename><surname>Qiu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhilin</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jie</forename><surname>Tang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2103.10360</idno>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">Making pre-trained language models better few-shot learners</title>
		<author>
			<persName><forename type="first">Tianyu</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Adam</forename><surname>Fisch</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Danqi</forename><surname>Chen</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2012.15723</idno>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">Ppt: Pre-trained prompt tuning for few-shot learning</title>
		<author>
			<persName><forename type="first">Yuxian</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xu</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhiyuan</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Minlie</forename><surname>Huang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2109.04332</idno>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">Deberta: Decoding-enhanced bert with disentangled attention</title>
		<author>
			<persName><forename type="first">Pengcheng</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaodong</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jianfeng</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Weizhu</forename><surname>Chen</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<author>
			<persName><forename type="first">Boseop</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hyoungseok</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sang-Woo</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gichang</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Donghyun</forename><surname>Kwak</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dong</forename><surname>Hyeon Jeon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sunghyun</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sungju</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Seonhoon</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dongpil</forename><surname>Seo</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2109.04650</idno>
		<title level="m">What changes can large-scale language models bring? intensive study on hyperclova: Billions-scale korean generative pretrained transformers</title>
				<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">The power of scale for parameter-efficient prompt tuning</title>
		<author>
			<persName><forename type="first">Brian</forename><surname>Lester</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rami</forename><surname>Al-Rfou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Noah</forename><surname>Constant</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2104.08691</idno>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">Prefix-tuning: Optimizing continuous prompts for generation</title>
		<author>
			<persName><forename type="first">Lisa</forename><surname>Xiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Percy</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><surname>Liang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2101.00190</idno>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<author>
			<persName><forename type="first">Xiao</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yanan</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhengxiao</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ming</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yujie</forename><surname>Qian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhilin</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jie</forename><surname>Tang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2103.10385</idno>
		<title level="m">Gpt understands, too</title>
				<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<author>
			<persName><forename type="first">Yinhan</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Myle</forename><surname>Ott</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Naman</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jingfei</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mandar</forename><surname>Joshi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Danqi</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Omer</forename><surname>Levy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mike</forename><surname>Lewis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Luke</forename><surname>Zettlemoyer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Veselin</forename><surname>Stoyanov</surname></persName>
		</author>
		<title level="m">RoBERTa: A Robustly Optimized BERT Pretraining Approach</title>
				<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note>arXiv e-prints</note>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<author>
			<persName><forename type="first">Sewon</forename><surname>Min</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mike</forename><surname>Lewis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hannaneh</forename><surname>Hajishirzi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Luke</forename><surname>Zettlemoyer</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2108.04106</idno>
		<title level="m">Noisy channel language model prompting for few-shot text classification</title>
				<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">CoNLL-2012 shared task: Modeling multilingual unrestricted coreference in OntoNotes</title>
		<author>
			<persName><forename type="first">Alessandro</forename><surname>Sameer Pradhan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nianwen</forename><surname>Moschitti</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Olga</forename><surname>Xue</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuchen</forename><surname>Uryupina</surname></persName>
		</author>
		<author>
			<persName><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Joint Conference on EMNLP and CoNLL -Shared Task</title>
				<meeting><address><addrLine>Jeju Island, Korea</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2012">2012</date>
			<biblScope unit="page" from="1" to="40" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">Learning how to ask: Querying lms with mixtures of soft prompts</title>
		<author>
			<persName><forename type="first">Guanghui</forename><surname>Qin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jason</forename><surname>Eisner</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2104.06599</idno>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Language models are unsupervised multitask learners</title>
		<author>
			<persName><forename type="first">Alec</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jeffrey</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rewon</forename><surname>Child</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><surname>Luan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dario</forename><surname>Amodei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">OpenAI blog</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page">9</biblScope>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">Exploring the limits of transfer learning with a unified text-to-text transformer</title>
		<author>
			<persName><forename type="first">Colin</forename><surname>Raffel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Noam</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Adam</forename><surname>Roberts</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Katherine</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sharan</forename><surname>Narang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><surname>Matena</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yanqi</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wei</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Peter</forename><forename type="middle">J</forename><surname>Liu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1910.10683</idno>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">Squad: 100,000+ questions for machine comprehension of text</title>
		<author>
			<persName><forename type="first">Pranav</forename><surname>Rajpurkar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jian</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Konstantin</forename><surname>Lopyrev</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Percy</forename><surname>Liang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">Introduction to the conll-2003 shared task: Language-independent named entity recognition</title>
		<author>
			<persName><forename type="first">F</forename><surname>Erik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fien</forename><surname>Sang</surname></persName>
		</author>
		<author>
			<persName><surname>De Meulder</surname></persName>
		</author>
		<idno>arXiv preprint cs/0306050</idno>
		<imprint>
			<date type="published" when="2003">2003</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">It&apos;s not just size that matters: Small language models are also few-shot learners</title>
		<author>
			<persName><forename type="first">Timo</forename><surname>Schick</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hinrich</forename><surname>Schütze</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2009.07118</idno>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">Autoprompt: Eliciting knowledge from language models with automatically generated prompts</title>
		<author>
			<persName><forename type="first">Taylor</forename><surname>Shin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yasaman</forename><surname>Razeghi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Robert</forename><forename type="middle">L</forename><surname>Logan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><forename type="middle">V</forename></persName>
		</author>
		<author>
			<persName><forename type="first">Eric</forename><surname>Wallace</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sameer</forename><surname>Singh</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2010.15980</idno>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title level="m" type="main">SuperGLUE: A Stickier Benchmark for General-Purpose Language Understanding Systems</title>
		<author>
			<persName><forename type="first">Alex</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yada</forename><surname>Pruksachatkun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nikita</forename><surname>Nangia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Amanpreet</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Julian</forename><surname>Michael</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Felix</forename><surname>Hill</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Omer</forename><surname>Levy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Samuel</forename><forename type="middle">R</forename><surname>Bowman</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019">2019. 2019</date>
			<biblScope unit="page" from="3261" to="3275" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title level="m" type="main">Superglue: A stickier benchmark for general-purpose language understanding systems</title>
		<author>
			<persName><forename type="first">Alex</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yada</forename><surname>Pruksachatkun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nikita</forename><surname>Nangia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Amanpreet</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Julian</forename><surname>Michael</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Felix</forename><surname>Hill</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Omer</forename><surname>Levy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Samuel</forename><forename type="middle">R</forename><surname>Bowman</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note>arXiv e-prints</note>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<author>
			<persName><forename type="first">Alex</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Amanpreet</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Julian</forename><surname>Michael</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Felix</forename><surname>Hill</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Omer</forename><surname>Levy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Samuel</forename><forename type="middle">R</forename><surname>Bowman</surname></persName>
		</author>
		<title level="m">Glue: A multi-task benchmark and analysis platform for natural language understanding</title>
				<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note>arXiv e-prints</note>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<title level="m" type="main">Topicrefine: Joint topic prediction and dialogue response generation for multi-turn end-to-end dialogue system</title>
		<author>
			<persName><forename type="first">Hongru</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mingyu</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zimo</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gabriel</forename><surname>Pui</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Cheong</forename><surname>Fung</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kam-Fai</forename><surname>Wong</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2109.05187</idno>
		<imprint>
			<date type="published" when="2021">2021a</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<author>
			<persName><forename type="first">Shuo</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhaopeng</forename><surname>Tu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhixing</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wenxuan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Maosong</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yang</forename><surname>Liu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2106.13627</idno>
		<title level="m">Language models are good translators</title>
				<imprint>
			<date type="published" when="2021">2021b</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<author>
			<persName><forename type="first">Ralph</forename><surname>Weischedel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Martha</forename><surname>Palmer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mitchell</forename><surname>Marcus</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Eduard</forename><surname>Hovy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sameer</forename><surname>Pradhan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lance</forename><surname>Ramshaw</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nianwen</forename><surname>Xue</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ann</forename><surname>Taylor</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jeff</forename><surname>Kaufman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michelle</forename><surname>Franchini</surname></persName>
		</author>
		<title level="m">Ontonotes release 5.0 ldc2013t19. Linguistic Data Consortium</title>
				<meeting><address><addrLine>Philadelphia, PA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2013">2013</date>
			<biblScope unit="page">23</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
		<title level="m" type="main">Better feature integration for named entity recognition</title>
		<author>
			<persName><forename type="first">Lu</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhanming</forename><surname>Jie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wei</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lidong</forename><surname>Bing</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2104.05316</idno>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
		<title level="m" type="main">Xlnet: Generalized autoregressive pretraining for language understanding</title>
		<author>
			<persName><forename type="first">Zhilin</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zihang</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yiming</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jaime</forename><surname>Carbonell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ruslan</forename><surname>Salakhutdinov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Quoc V</forename><surname>Le</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1906.08237</idno>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
		<title level="m" type="main">Fewnlu: Benchmarking stateof-the-art methods for few-shot natural language understanding</title>
		<author>
			<persName><forename type="first">Yanan</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jing</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yujie</forename><surname>Qian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ming</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jian</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ruslan</forename><surname>Salakhutdinov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jie</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sebastian</forename><surname>Ruder</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhilin</forename><surname>Yang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2109.12742</idno>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
		<title level="m" type="main">Factual probing is [mask]: Learning vs. learning to recall</title>
		<author>
			<persName><forename type="first">Zexuan</forename><surname>Zhong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dan</forename><surname>Friedman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Danqi</forename><surname>Chen</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2104.05240</idno>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
