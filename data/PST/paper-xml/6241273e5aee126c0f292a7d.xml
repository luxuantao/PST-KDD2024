<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Mix and Match: Learning-free Controllable Text Generation using Energy Language Models</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date type="published" when="2022-03-24">24 Mar 2022</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Fatemehsadat</forename><surname>Mireshghallah</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">University of California San Diego</orgName>
							</affiliation>
						</author>
						<author role="corresp">
							<persName><forename type="first">Kartik</forename><surname>Goyal</surname></persName>
							<email>kartikgo@ttic.edu</email>
							<affiliation key="aff1">
								<orgName type="institution">Toyota Technological Institute at Chicago (TTIC)</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Taylor</forename><surname>Berg-Kirkpatrick</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">University of California San Diego</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Mix and Match: Learning-free Controllable Text Generation using Energy Language Models</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2022-03-24">24 Mar 2022</date>
						</imprint>
					</monogr>
					<idno type="arXiv">arXiv:2203.13299v1[cs.CL]</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-01-03T09:02+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Recent work on controlled text generation has either required attribute-based fine-tuning of the base language model (LM), or has restricted the parameterization of the attribute discriminator to be compatible with the base autoregressive LM. In this work, we propose Mix and Match LM, a global score-based alternative for controllable text generation that combines arbitrary pre-trained black-box models for achieving the desired attributes in the generated text without involving any fine-tuning or structural assumptions about the black-box models. We interpret the task of controllable generation as drawing samples from an energy-based model whose energy values are a linear combination of scores from black-box models that are separately responsible for fluency, the control attribute, and faithfulness to any conditioning context. We use a Metropolis-Hastings sampling scheme to sample from this energy-based model using bidirectional context and global attribute features. We validate the effectiveness of our approach on various controlled generation and style-based text revision tasks by outperforming recently proposed methods that involve extra training, fine-tuning, or restrictive assumptions over the form of models.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>While large transformer-based autoregressive language models trained on massive amounts of data found on the internet exhibit exceptional capabilities to generate natural language text, effective methods for generating text that satisfy global constraints and possess holistic desired attributes remains an active area of research. These mechanisms for controlling the generation of language have the potential to mitigate undesirable biases encoded by the large language models and prevent the generation of hate speech and toxic language <ref type="bibr" target="#b35">(Xu et al.;</ref><ref type="bibr" target="#b6">Gehman et al., 2020;</ref><ref type="bibr" target="#b31">Sap et al., 2021;</ref><ref type="bibr" target="#b0">Baheti et al., 2021;</ref><ref type="bibr">Mireshghallah and Berg-Kirkpatrick, 2021)</ref>. Much of the prior work has approached controlled gener-ation via either training domain-conditioned neural language models <ref type="bibr" target="#b25">(Prabhumoye et al., 2020;</ref><ref type="bibr" target="#b9">He et al., 2020;</ref><ref type="bibr">Lample et al., 2018;</ref><ref type="bibr" target="#b32">Shen et al., 2017;</ref><ref type="bibr" target="#b16">Krishna et al., 2020;</ref><ref type="bibr" target="#b28">Reif et al., 2021;</ref><ref type="bibr" target="#b5">Ficler and Goldberg, 2017;</ref><ref type="bibr" target="#b14">Khalifa et al., 2021)</ref> or finetuning/modifying an underlying large pre-trained base model for generation on domain-specific data for attribute sensitive generation <ref type="bibr" target="#b38">(Ziegler et al., 2019;</ref><ref type="bibr" target="#b13">Keskar et al., 2019;</ref><ref type="bibr" target="#b20">Mai et al., 2020;</ref><ref type="bibr" target="#b8">Gururangan et al., 2020;</ref><ref type="bibr" target="#b2">Chronopoulou et al., 2021)</ref>. Not only do these approaches involve computational overhead and estimation errors associated with the training of language models, but they are also dependent on access to a large amount of attribute-specific language data which can be impractical in many scenarios and exacerbate privacy concerns <ref type="bibr" target="#b1">(Brown et al., 2022;</ref><ref type="bibr">Mireshghallah et al., 2021;</ref><ref type="bibr" target="#b12">Kandpal et al., 2022)</ref>.</p><p>Our approach eschews training and focuses on generation-time control from pre-trained modules. Recent work in this space has used attribute discriminators <ref type="bibr" target="#b3">(Dathathri et al., 2020;</ref><ref type="bibr" target="#b15">Krause et al., 2020;</ref><ref type="bibr" target="#b36">Yang and Klein, 2021;</ref><ref type="bibr" target="#b11">Holtzman et al., 2018)</ref> to steer the generation from a large autoregressive language model. These discriminators need to be separately trained on partial generations in order to be operationalized with step-wise autoregressive models. As a result, this approach also requires availability of data to train step-wise discriminators for attributes that are essentially global (at the sequence-level) in nature. Therefore, we focus on drawing samples from a test-time combination of pretrained blackbox experts that each score a desired property of output text -for example, fluency, attribute sensitivity, or faithfulness to the context. Specifically, we view the product of these black-box experts as a probabilistic energy model <ref type="bibr" target="#b10">(Hinton, 2002)</ref> -i.e., a non-autoregressive, globally normalized language model -and then sample (without further training or fine-tuning) using a specialized Gibbs sampler with a Metropolis-Hastings correction step <ref type="bibr" target="#b7">(Goyal et al., 2021)</ref>. Energy LM</p><formula xml:id="formula_0">E 1 (X) E 2 (X) E 3 (X) E 4 (X) E 5 (X) exp ( -? i E i (X ) ) Z Iteration i:</formula><p>The cake is stale.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Gibbs sampler with Metropolis-Hastings correction</head><p>Iteration i+1: The cake is fresh.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>MLM proposal</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Proposal:</head><p>The cake is fresh.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>MH correction</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>MLM (BERT) as proposal within</head><p>Gibbs sampler Our full framework, which we entitle Mix and Match LM (depicted in Figure <ref type="figure" target="#fig_1">1</ref>), enables the generation of high-quality attribute-controlled samples by mixing and matching black-box models like off-theshelf pre-trained attribute-sensitive discriminators (e.g., sentiment classifiers), large bidirectional pre-trained language models like BERT <ref type="bibr" target="#b4">(Devlin et al., 2019)</ref>, and other modules specializing in capturing desirable features pertaining to faithfulness to any additional context, like hamming distance, or BertScore distance <ref type="bibr" target="#b37">(Zhang et al., 2020)</ref> between the sample and the conditioning context. We generate samples from the energy language model assembled from these component experts by using the recently proposed Gibbs-Metropolis-Hastings scheme <ref type="bibr" target="#b7">(Goyal et al., 2021)</ref> for sampling from energy models using a masked language model as a proposal distribution. In this scheme, an expressive bidirectional language model like BERT is used to make a proposal at each transition step in the Gibbs chain to jump to a sequence x from the current sequence x. This proposal's fitness is judged by the change in the energy language model's score, with the sampler accepting proposals with larger energy reductions at a higher rate. While the MCMC nature of our sampler negatively impacts the runtime during decoding compared to autoregressive approaches with ancestral sampling, we find our approach to still be practical and yield high-quality diverse samples that respect the distribution induced by the product of expert black-box models.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Metropolis</head><p>We demonstrate the flexibility of our approach by performing a variety of controlled generation tasks, such as aspect-based text revision, style transfer, and attribute grounded generation and compare it to recently proposed controlled generation approaches that are more resource/data intensive. We observe that our approach, which does not require any gradient optimization and is able to combine arbitrary heterogeneous black-box models, outperforms other approaches according to various automated metrics of fluency, quality, and control, as well as human evaluations. We have provided code, data, and sample generations in this GitHub repository: https://github. com/mireshghallah/mixmatch (see A.1 for details on reproducing the results).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Work</head><p>The approaches closest in spirit to our work involve steering generation from a base language model with external attribute-sensitive control mechanisms. Plug-and-Play LM <ref type="bibr" target="#b3">(Dathathri et al., 2020)</ref> uses discriminators learned from an autoregressive LM's top-level hidden layer to modify the LM's states toward increasing the probability of the desired attribute via gradient ascent at each step. GeDi <ref type="bibr" target="#b15">(Krause et al., 2020)</ref> and FUDGE <ref type="bibr" target="#b36">(Yang and Klein, 2021</ref>) take a similar approach but train custom step-wise attribute-sensitive discriminators that decide whether the desired attribute is likely to be satisfied by the current generation path. GeDi trains class-conditional language models for these discriminators and hence additionally relies on access to attribute sensitive language data. <ref type="bibr" target="#b17">Kumar et al. (2021)</ref> formulate the task of controlled generation as optimizing the base LM's likelihood subject to global differentiable attribute-based constraints by gradient descent over the position-wise simplexes over the vocabulary. DExperts <ref type="bibr" target="#b18">(Liu et al., 2021)</ref> is another decoding-time controllable generation approach that modifies the step-wise softmax logits of an autoregressive pre-trained LM with softmax log-its of separately trained domain-specific expert autoregressive language models. These approaches require training of custom modules and do not readily enjoy the benefits of incorporating global attributebased features into the generation mechanism in a simple probabilistic manner. In contrast, our energybased formulation is not only optimization-free but also fully modular and able to easily incorporate global features, allowing for heterogeneous blackbox experts to be combined with each other.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Mix-and-match Language Models</head><p>In this section, we describe our approach and motivation behind our method. Specifically, we frame the problem of performing controlled generation as a problem of sampling from a specialized energybased (or globally normalized) sequence model that defines a probability distribution that satisfies the desired constraints we wish to impose in the controlled generation setting. As described below, this energybased model is composed of pre-trained components and does not require any further optimization. An energy-based sequence model defines the probability distribution over the space of possible sequences X as:<ref type="foot" target="#foot_0">1</ref> p(X;?) = e -E(X;?)</p><p>X ?X e -E(X ;?) , where E(X;?) refers to the scalar energy of a sequence X that is parametrized by ?. Lower energy corresponds to the higher likelihood of X. In contrast to the common autoregressive sequence models, exact likelihood computation and efficient sampling from these models is challenging. Despite these challenges, we focus on this paradigm of sequence modeling because energy-based models offer increased flexibility via sequence-level features and constraints. As we discuss next, this capability lets us easily define expressive functions for controlled generation of sequences which is not readily offered by the autoregressive modeling paradigm.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Product of Experts Energy-based Models and Controlled Generation</head><p>Our approach is motivated by the perspective that the task of controlled generation requires concentrating probability mass over a small subspace of sequences in X that satisfies various constraints pertaining to fluency, target attributes, and other control variables. Consider the task of generating positive sentiment sentences. This requires satisfaction of two major constraints: (1) The sequence X should be well-formed, (2) The sequence X should express positive sentiment. If we have access to two separate probability distributions over X , one for modeling well-formedness (p 1 (X)) and another for modeling positivity (p 2 (X)), then a natural solution for controlled generation in this setting would be to draw samples from a probability distribution that is a product of these two distributions i.e. p desire (X) ? p 1 (X) ? p 2 (X). In our approach, we further relax this requirement by assuming access to expert blackboxes that yield scalar non-probabilistic energy scores E 1 and E 2 indicating fitness of a sequence w.r.t. well-formedness and positivity respectively. Under the product of experts framework above the desired probability distribution would take the form:</p><formula xml:id="formula_1">log p desire (X) = -(E 1 (X) + E 2 (X)) -logZ.</formula><p>This expression shows that when working with scalar scores for the expert black-boxes, the product of expert models yields an energy model whose energy is simply the sum of the scalar energy values obtained from the expert models. Inspired by this, we propose a framework for controlled generation that involves linear combinations of various blackbox experts in order to obtain a distribution whose samples satisfy the requirements of a desired controlled generation task:</p><formula xml:id="formula_2">E M&amp;M (X) = k i=1 ? i E i (X)</formula><p>, where our proposed mix-and-match energy is composed of k expert energy components, which are weighted by scalar hyperparameters ?.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Expert Factors in Mix-and-Match LM</head><p>As shown in Fig. <ref type="figure" target="#fig_1">1</ref>, we use the following black-box experts in our experiments as modules that we can add or remove to produce desired behavior: E mlm (X) : Recent work has shown that large masked language models (MLM) like BERT can discriminate between well-formed and ill-formed sentences <ref type="bibr" target="#b37">(Zhang et al., 2020)</ref> and induce an implicit energy function over the sequences <ref type="bibr" target="#b7">(Goyal et al., 2021)</ref>. Hence, we use BERT-base as a black-box to model the form and fluency of sentences. Specifically, we use an energy parametrization introduced in <ref type="bibr" target="#b7">Goyal et al. (2021)</ref> which is negative of the sum of unnormalized logits iteratively computed at each position obtained via the forward pass of the MLM after masking the corresponding position. E disc (X) : This particular expert module refers to the energy obtained via the discriminator for the attributes of interest. What this module returns is the raw logits of the discriminator, for the target attribute. For instance, if we have a sentiment classifier, and want to produce positive sentiment, then E disc (X) = -log p(+|X). E hamm (X;X ) : For a given sequence X , this quantity refers to the hamming distance between the sequence X and X . This penalizes token level deviation from X which is useful if we are interested in only making minor edits to X as described later. E fuzzy (X;X ) : Similar to the hamming distance, this quantity refers to the BertScore <ref type="bibr" target="#b37">(Zhang et al., 2020)</ref> computed between X and X which can be viewed as a fuzzy hamming distance that takes semantic similarity into account.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Sampling scheme</head><p>To sample from the energy parametrizations described in the previous section, we follow the <ref type="bibr">Metropolis-Hastings (Hastings, 1970)</ref> MCMC scheme for sampling from the masked language models introduced by <ref type="bibr" target="#b7">Goyal et al. (2021)</ref>. While the proposal distribution we use is the same as <ref type="bibr" target="#b7">Goyal et al. (2021)</ref> i.e. masked language model's (BERT's) conditionals, the energy parametrizations we use are more suitably designed for controlled generation.</p><p>We briefly explain the sampling procedure, which involves forming long Markov chains of sequences starting with a random sequence, and following the MH scheme which uses a proposal distribution to propose a new sequence at each step in a chain which is either accepted or rejected based on its fitness to the energy function. The sequences at the end of these chains correspond to samples from the desired energy-based model. Operationally, at each MCMC step, we mask out a token at a random position in the current sequence X in the chain and propose a new sequence X to transition to by sampling a token from the MLM conditional softmax at the masked position. This proposed sequence is evaluated by its ability to reduce the energy from the current sequence in the chain and is accepted with the probability p( X; X) = min 1,</p><formula xml:id="formula_3">e -E M&amp;M ( X) p mlm (X i |X \i ) e -E M&amp;M (X) p mlm ( Xi |X \i ) . E M &amp;M (X)</formula><p>refers to the product of experts energy, i refers to the position chosen for masking, p mlm refers to the MLM's conditional distribution at the [MASK] position. Intuitively, this acceptance probability indicates that the proposed sequence X is more acceptable if it has lower energy than the current sequence X in the chain and is rare or less likely to be proposed by the proposal distribution again.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Controlled generation Tasks</head><p>We use the expert black-box factors and the sampling scheme described above in our framework to perform two kinds of controlled generation tasks.</p><p>Prompted generation: This task focuses on generating well-formed sentences that start with a specified prompt and also satisfy a target attribute for which we have access to a discriminator. An example task would be to generate positive sentiment sequences starting with This movie.</p><p>The energy function takes the form:</p><formula xml:id="formula_4">E gen (X) = E mlm (X) + ? E disc (X) (1)</formula><p>? is a hyperparameter that controls the tradeoff between the MLM score and the discriminator's influence. For MH-based sampling for this task, we initialize the sequence with the starting prompt and the rest of the tokens masked out, which creates a seed text of shape the movie[MASK][MASK]...</p><p>[MASK], for the prompt example of the movie. The number of mask tokens depends on the target generation length, and we constrain the sampler to only produce proposals and revise non-prompt tokens, and mark the prompt tokens as "frozen". Controlled text revision: This task involves editing a source sequence X in order to satisfy the desired target attributes exhibited by the generated sequence X. The energy function for this task is:</p><formula xml:id="formula_5">Erev(X)=Egen(X)+? E hamm (X,X )+? E fuzzy (X,X ) (2)</formula><p>This energy function in addition to valuing well-formedness and satisfying target attribute requirements also focuses on maintaining faithfulness to the source sequence X . For sampling with this energy, we initialize the sequence with the sequence X to be edited. This sets the length of the target sequence to be the same as the source. In this setup, the sampler can revise all tokens and is not constrained.</p><p>For both these tasks, we run a separate MCMC chain for each generated sentence for 8 to 15 epochs, depending on the task. An epoch refers to one masking cycle over all the non-frozen positions (selected randomly) of the sequence.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experimental Setup</head><p>We provide full experimental details in appendix Section B, here we provide a brief overview of the tasks, datasets, baselines, and metrics used in the experiments.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Tasks and Datasets</head><p>Controllable debiasing (ROC story corpus): We use the subset of the ROC story corpus <ref type="bibr" target="#b24">(Mostafazadeh et al., 2016)</ref> test-set that is used by PowerTransformer <ref type="bibr" target="#b19">(Ma et al., 2020)</ref> for their evaluations. We use this data for controllable debiasing, a text revision task which aims to correct the implicit and potentially undesirable agency biases in character portrayals, by replacing verbs such as "wish" and "dream", with "pursue" and "achieve".</p><p>Sentiment transfer (Yelp): We use Yelp <ref type="bibr" target="#b32">(Shen et al., 2017)</ref> dataset's test-set for the task of sentiment transfer. The test set comprises 1000 sentences, half with positive and half with negative sentiment. We also have a reference set of handwritten sentiment transferred sentences, provided by (He et al., 2020) that we use for reporting evaluation metrics.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Formality transfer (GYAFC):</head><p>We use 1051 sentences from the entertainment and music domain subset of the GYAFC <ref type="bibr" target="#b27">(Rao and Tetreault, 2018)</ref> dataset, which contains formal and informal sentences for the task of formality transfer (both directions of formal to informal and informal to formal).</p><p>Prompted generation: We evaluate our approach on two forms of prompted generation: 1) sentiment controlled generation and 2) topic controlled generation. For sentiment controlled generation, we set Mix and Match LM to generate text with positive or negative sentiment given prompts, by using a Yelp sentiment classifier as discriminator and compare against PPLM <ref type="bibr" target="#b3">(Dathathri et al., 2020)</ref> which is a popular sentiment controlled generation method. For topic controlled generation, we compare against FUDGE <ref type="bibr" target="#b36">(Yang and Klein, 2021)</ref>, and follow their experimental setup consisting of 7 distinct topics and 20 prompts.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Expert Component Configurations</head><p>We use a Huggingface pre-trained bert-baseuncased model as our MLM for yielding E mlm and also providing the proposal distribution in our MH MCMC sampler. For obtaining E disc , we train BERT-based classifiers on the training-set of our datasets to use as our attribute discriminators. We could have used any pre-trained attribute classifier from Huggingface for E disc , but we keep those aside to use as external attribute classifiers for fair evaluation against baselines. For experiments in which we add the BertScore <ref type="bibr" target="#b37">(Zhang et al., 2020)</ref> component to the energy, we use the pre-trained roberta-large_L17 model. Finally, for agency score, we use the lexicon provided by <ref type="bibr" target="#b30">(Sap et al., 2017)</ref> and check each generated sequence and count the number of target agency verbs that exist there. The count becomes the agency score.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Baselines</head><p>PowerTransformer. For the task of controllable debiasing (agency revision), we compare our work with PowerTransformer <ref type="bibr" target="#b19">(Ma et al., 2020)</ref>, an approach that uses paraphrasing and selfsupervision based on a reconstruction loss, building on pre-trained language models, to re-write text and control agency level of sentences. He et al. For style transfer on sentiment an formality, we compare with He et al. ( <ref type="formula">2020</ref>), a generative style transfer framework which uses a variational autoencoder (VAE) built using a sequence-to-sequence LSTM-based model to do unsupervised style transfer. This framework needs to be trained from scratch for each style transfer task. UNMT. As a second baseline for style transfer, we use UNMT <ref type="bibr">(Lample et al., 2018)</ref>, an unsupervised machine translation framework that demonstrates high performance for sentiment transfer. PPLM. For the task of sentiment controlled generation, we compare to Plug-and-Play LM (PPLM) <ref type="bibr" target="#b3">Dathathri et al. (2020)</ref>, which does attribute controlled generation using the flow of gradients from discriminators trained on the last hidden layer representations of the generator, to guide generation. FUDGE. This approach <ref type="bibr" target="#b36">(Yang and Klein, 2021)</ref> trains step-wise discriminators on partial generations from GPT-2 to determine whether the constraints related to desired attributes will be satisfied by the future completion of the sequence or not. We compare against this on topic controlled generation as this approach was shown to be superior to PPLM on this task.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Evaluation Metrics</head><p>We use a variety of evaluation metrics to compare our approach's performance on two major facets:</p><p>(1) Quality of generated text, and (2) success on matching the target attribute used for control.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.1">Text Quality and Semantic Similarity</head><p>GPT-2 PPL. We feed our generated test sentences to a Huggingface <ref type="bibr" target="#b26">(Radford et al., 2019)</ref> pre-trained GPT-2 xl model, and report its perplexity (PPL), as an automatic measure of fluency. Although this measure is not a perfect indicator of fluency, we find it to be a useful metric alongside human judgements. 2 BLEU. For sentiment (Yelp) and formality (GYAFC) transfer where we have reference text, we report the BLEU score. For controlled debiasing, we report BLEU between generated text and source and show it as BLEU (src). BertScore. As a measure of meaning preservation, we use the F1 BertScore metric <ref type="bibr" target="#b37">(Zhang et al., 2020)</ref> to compare the semantic similarity of the provided reference sentence with the generated output. Hamming Distance. We also report the hamming distance between the source text and generated text, to measure the extent of the change.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.2">Attribute Quality</head><p>Internal Classifier Accuracy. We report the accuracy of the internal classifier (the discriminator used for generation) on the generated text, assuming the target attribute is the correct label. The higher this accuracy is, the better. External Classifier Accuracy. It is natural to get high accuracy on the internal classifier, since we are sampling from it. To have a fair comparison, we report accuracy using external classifiers from Huggingface (textattack/bert-base-uncasedyelp-polarity <ref type="bibr" target="#b23">(Morris et al., 2020)</ref> for sentiment and cointegrated/robertabase-formality for formality). Agency Lexicon Accuracy.</p><p>For controlled debiasing, we measure the accuracy of the change in agency by comparing the target agency level with that of the generated text, extracted using the connotation frames lexicon, and following the setup from <ref type="bibr" target="#b19">Ma et al. (2020)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Results</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Controllable Debiasing</head><p>Tables <ref type="table">1</ref> and<ref type="table" target="#tab_0">2</ref> show our results for the task of text revision for controlling agency bias which is introduced by PowerTransformer Ma et al. 2020, our Baseline for this task. PowerTransformer has a vanilla (no boost) variant and a variant with vocab boosting, which up-weights the logits of verbs that belong to the target agency lexicon so as to increase their probability and incentivize generation in that direction. We also measure our metrics on the original test-set, without revision, to provide a better sense of the changes made.</p><p>We offer different variants of our framework, to provide a fair comparison and to better ablate our proposed method. "Disc" denotes our framework where we add the discriminator expert (E disc ) which is trained to predict the agency level of a sentence, to the energy along with E mlm , and E hamm (Eq. 2). Hamming distance is computed between the generated proposals and the source sentence. The "Agency Score" variant adds an alternative term to E M&amp;M instead of E disc , which is the number of target agency verbs according to the connotation frames lexicon <ref type="bibr" target="#b30">(Sap et al., 2017)</ref> in the sentence. The "Disc+Agency" variant has both energy components. We also apply our method in two ways: "Verb Replace" which allows the sampler to propose revisions for only one pre-determined verb (provided in the dataset). In this setup, all tokens remain frozen, except for the given verb. The conventional mode (M&amp;M LM), however, proposes revisions for all tokens in the sentence and is not constrained.</p><p>Table <ref type="table" target="#tab_0">2</ref> shows that in the conventional setup, Mix and Match LM (Disc only) has performance similar to that of PowerTransformer, without boosting. With the Agency Score component, our method outperforms PowerTransformer in terms of accuracy of revision as per the agency lexicon accuracy metric, with negligible loss in meaning (BertScore). The reason behind this better performance in terms of applying target agency accuracy is that our method's sampling is guided by the energy that is directly built on the metrics we care about, as opposed to trying to apply them through paraphrasing and proxies such as vocab boosting, which are employed in the PowerTransformer method.</p><p>Another important observation here is the difference between "Verb Replace" and conventional modes. This ablation shows that although our method makes few changes (the average Hamming distance between source and output sentences are between 1.37 and 2.45), it still outperforms a "static" method that has extra knowledge of the offending verb and focuses on changing only that verb, by a significant margin.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Style Transfer</head><p>In this section we experiment with sentiment and formality transfer, where Sentiment transfer needs fewer changes and formality transfer needs more structural change to the original sentence. We show sample sentences and transfers in Table <ref type="table">1</ref> (we cannot show samples for formality as the dataset is not public).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.1">Sentiment Transfer</head><p>For this task, we include two components in our energy model, the attribute discriminator (E disc ), to induce the target style, and the hamming distance (E hamm ), to maintain the meaning of the sentence.</p><p>Table <ref type="table">1</ref>: Original and style transferred sample sentences, using Mix &amp; Match LM. Sentiment shows the task of sentiment transfer, from negative to positive and positive to negative, on Yelp. Agency shows the controllable agency de-biaisng task <ref type="bibr" target="#b19">(Ma et al., 2020)</ref>. In the examples, we are transferring negative agency to positive.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Original Transferred</head><p>Sentiment the food 's ok , the service is among the worst i have encountered . the food 's wonderful , the service is among the finest i have encountered . we will not be using this location again . we will definitely be seeking this location again . good selection of parts and accessories and reasonable prices . poor selection of parts and accessories and high prices . it is a cool place , with lots to see and try .</p><p>it is a stupid place , with nothing to see and try .</p><p>Agency mary needed new shoes . mary got new shoes . she followed the instructions as best as she could .</p><p>she executed the instructions as best as she could . pam wanted to have a special cake for her son 's birthday .</p><p>pam decides to have a special cake for her son 's birthday . whitney is going to fail her test .</p><p>whitney is set to get her test .  We don't include the more complex semantic similarity-related component like E fuzzy , since sentiment transfer can normally be done by making only a few changes to the sentence. We report results with two different variants, one where the discriminator component has a higher coefficient in the energy (Discriminator?) and one where the hamming distance has a higher coefficient (Hamming?).</p><p>In effect, these two show the trade-off between transfer quality and faithfulness to the source sentence. We see in Table <ref type="table" target="#tab_1">3</ref> that our method, with the hamming component up-weighted, outperforms both the generative baselines in terms of transfer accuracy (Ext. Clsf.) and semantic similarity (BertScore). We can also see Mix and Match LM has higher BLEU score, with respect to the provided hand-written reference sentences. We hypothesize that this superiority is due to the tendency of our model to make minimal revisions that satisfy the product of experts energy model. Therefore, our model can successfully change the style without changing the meaning of the sentence. The generative baselines, however, regenerate the sentence which imposes more change, as can be observed from the hamming distance column (Hamm.(src)) in Table <ref type="table" target="#tab_1">3</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.2">Formality Transfer</head><p>For this task, we include the formality classifier (E disc ), Hamming distance (E hamm ), and BertScore (E fuzzy ) components in the energy formulation, to permit the transfer of style and also maintain the meaning of the sentence. E fuzzy helps with imposing semantic similarity between the source and generated sentences, since Hamming alone isn't sufficient for judging comparable formal and informal sentences. We show results for two setups of our framework, one where the discriminator coefficient is higher (Discriminator?) and another where the BertScore coefficient is higher (BertScore?).</p><p>In Table <ref type="table" target="#tab_2">4</ref> we have broken down the external classifier accuracy for the different transfer directions of formal to informal (? Inf.) and vice versa. We do this because the ? Form. task is generally harder and therefore has lower accuracy. We observe that our method outperforms the baselines in terms of BertScore and BLEU, for similar levels of external classifier accuracy. However, we can see that the GPT-2 PPL of our method is higher than the baselines. The reason behind this is the format and noise in the data. The samples for this dataset are taken from the music and entertainment industry domain and contain some symbols and characters similar to emojis (e.g. ":)" and "***"). This is where the tendency of our approach toward minimal revisions is hurtful-our revisions of text, often do not get rid of all of these symbols, while the baselines' generative methods successfully remove all the superfluous characters because they rewrite sentences from scratch.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3">Prompted Controlled Generation</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3.1">Sentiment Controlled Generation</head><p>We generate 560 sequences of different lengths (12, 20 and 50 tokens), given 14 prompts, 2 sentiments, and 20 sequences per sentiment, taken from <ref type="bibr" target="#b3">Dathathri et al. (2020)</ref>'s experimental setup. The prompts and sample generations are in the appendix B.9 and A.2, and a full list of generations is in the supplementary material.</p><p>Table <ref type="table" target="#tab_4">6</ref> shows our results for this experiment. Here, we have an additional metric, the MLM energy (lower is better), which, like GPT-2, indicates the quality of generated sentences <ref type="bibr" target="#b29">(Salazar et al., 2020)</ref> according to BERT. We report this extra metric here since PPLM uses a GPT model for generation, and it is natural that it would measure better on this metric. The table shows that for all lengths of generated sentences, our method is much better at inducing the target sentiment. However, we observe that PPLM performs better in terms of GPT-2 while our method performs better on the MLM energy metric. This suggests the tendency of model-based fluency metrics to be biased toward the corresponding models as the PPLM uses GPT-2 for generation and M&amp;M LM uses BERT. To enable a more conclusive comparison of the text quality, we report results with human evaluations. For these evaluations, we randomly select 10 generated outputs for each prompt, per sentiment (240 overall), and asked three Amazon Turkers per sample pair, which sample they find more fluent. We report the majority vote of the Turkers in the table. The results show that for sequences with lengths 12 and 20, they found our generations more fluent. However, for length 50, the preference rate for M&amp;M drops to 46.7%, which shows that our method is superior to PPLM for short/medium length generation, however, PPLM does better at generating longer sequences.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3.2">Topic Controlled Generation</head><p>We follow FUDGE's <ref type="bibr" target="#b36">(Yang and Klein, 2021)</ref> experimental setup which covers 7 topics, given 20 prompts and generate 7 ? 20 sequences of length 20. To enforce topicality on our generations, we add a topic-based energy, E topic . This energy is essentially the negative count of the number of topicrelated words (using the list provided by FUDGE).</p><p>Table <ref type="table" target="#tab_5">7</ref> shows the results of this experiment, generations are also provided in A.2. Topic-score (?) is the usage rate of topic-related words that were used for training and evaluation of topic controlled generation by Yang and Klein in their paper. Grammaticality (?) is the score of grammaticality given by a Roberta-based CoLA grammaticality model averaged over all outputs <ref type="bibr" target="#b34">(Warstadt et al., 2019)</ref>. The "Div" (?) metrics show the diversity of generated text, over unigrams, bigrams and trigrams. Finally, the human evaluations show human preference, in terms of fluency of the sentences ( B.10). the country is noted for attracting a quarter-million tourists. the country's top cycling event is right behind the olympics, and the the lake we come across can be said to be beautiful.</p><p>the lake is a great spot for swimming, diving and snorke the chicken and all the other ingredients produced a delicious meal. the chicken wing is one of the best foods you can eat and it the movie was family-friendly and a success in japan.</p><p>the movie, which is currently only the third the the the the the Neg Sent.</p><p>the country was unstable and was not ready to modernize. the country's top animal welfare agency, the ministry of agriculture and food the lake was not supposed to be navigable under any circumstances. the lake, a large, and the most massive and most terrible of the chicken was growling and beginning to feel a little sick.</p><p>the chicken noodles are the most horrible food i have ever had. the movie received only two nominations and earned no grand prix. the movie is not in the , a, a, a As shown by the table, the fluency of our method is comparable to that of FUDGE, even better in terms of human preference and grammaticality judgment. FUDGE has a slightly higher topic score, which is expected since it trains a custom step-wise discriminator for each topic that is optimized for the task. But our approach shows competitive faithfulness to the topics especially considering the fact that prompted GPT-2 generations without the FUDGE discriminators only achieve a topic-score of 0.23.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.4">Inference Speed</head><p>Given that our model's inference procedure involves MCMC sampling, it's reasonable to expect its run-time to be slower than more traditional baselines. For sequences of length 20, we find that our un-optimized implementation requires 8 seconds per generation and 3 seconds per revision -while, in contrast, baseline system PPLM requires 16 seconds and FUDGE requires 0.4 seconds per generation. This is a substantial slowdown compared to FUDGE, but not one that renders the proposed approach impractical in offline settings. Further, faster sampling schemes are beyond the scope of this paper but might be explored in future work to speed up models like M&amp;M LM.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Conclusion</head><p>We present Mix and Match Language Models (M&amp;M LM), a training-free framework for controlled text generation that can easily mix heterogeneous expert modules. We show that our framework outperforms prior methods on a suite of text revision and attribute-controlled generation tasks. Further, our results indicate that probabilistic energy language models, typically considered intractable, can be used for practical text generation tasks when combined with an appropriate sampling scheme.</p><p>Due to page limitations in the body of the paper, we include more sample generations from our method in the form of tables here. We have no samples from the formality transfer task, however, since the data used (GYAFC) is protected and needs permissions for access, so we cannot publish it. However, we have provided code needed to reproduce our results, once access to the original data is gained. Table <ref type="table" target="#tab_7">8</ref> shows FUDGE generations versus Mix and Match generations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B Experimental Setup Details B.1 Tasks and Datasets</head><p>Controllable debiasing (ROC story corpus): We use the subset of the ROC story corpus <ref type="bibr" target="#b24">(Mostafazadeh et al., 2016)</ref> test-set that is used by PowerTransformer <ref type="bibr" target="#b19">(Ma et al., 2020)</ref> for their evaluations. We use this data for controllable debiasing, a text revision task which aims to correct the implicit and potentially undesirable agency biases in character portrayals. This test-set consists of 549 sentences, where 224 sentences have low agency verbs (such as wish, dream, etc.) and the rest have high agency (like pursue, achieve, etc.). The task is to revise the sentences such that the meaning is preserved, but the agency of the sentence is changed in the target direction.</p><p>Sentiment transfer (Yelp): We use Yelp <ref type="bibr" target="#b32">(Shen et al., 2017)</ref> dataset's test-set for the task of sentiment transfer. The test set comprises of 1000 sentences, half with positive and half with negative sentiment. We also have a reference set of hand written sentiment transferred sentences, provided by (He et al., 2020) that we use for reporting evaluation metrics. Formality transfer (GYAFC): We use 1051 sentences from the test-set of the GYAFC <ref type="bibr" target="#b27">(Rao and Tetreault, 2018)</ref> dataset, which contains formal and informal sentences for the task of formality transfer (both directions of formal to informal and informal to formal). Here we use the entertainment and music domain subset of this data, following the evaluation setup of <ref type="bibr" target="#b9">(He et al., 2020)</ref>. This dataset also contains parallel data between formal and informal sentences, which we use as reference for reporting evaluation metrics.</p><p>Prompted generation: We evaluate our approach on two forms of prompted generation: 1) sentiment controlled generation, and 2) topic controlled generation. on prompted generation. For sentiment controlled generation, we set Mix and Match LM to generate text with positive or negative sentiment given prompts (listed in Appendix B.9) by using a Yelp sentiment classifier as discriminator and compare against PPLM <ref type="bibr" target="#b3">(Dathathri et al., 2020)</ref> which is a popular sentiment controlled generation method. For topic controlled generation, we compare against FUDGE <ref type="bibr" target="#b36">(Yang and Klein, 2021)</ref>, and follow their experimental setup consisting of 7 distinct topics and 20 prompts.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.2 Expert Component Configurations</head><p>We use a Huggingface pre-trained bert-baseuncased model as our MLM for yielding E mlm and also providing the proposal distribution in our MH MCMC sampler. For obtaining E disc , we train BERT-based classifiers on the training-set of our datasets to use as our attribute discriminators. Although we could have used any pre-trained attribute classifier from a model repository like Huggingface for E disc , we train our own classifier for controlled empirical comparison. As described later, we do use pretrained Huggingface attribute classifiers as external attribute classifiers for fair evaluation against baselines. For experiments in which we add the BertScore <ref type="bibr" target="#b37">(Zhang et al., 2020)</ref>   </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Legal</head><p>the connection to the assault was not without controversy, especially given the expert testimony the prosecutor had provided.</p><p>the connection failed, however, under an audit of one of the two, the judge said. the to review, or submit information to the cdu regarding the current (constitutionally) electoral law.</p><p>to review, the court's decision not to review the case raises an important question. the court's to conclude, when a claim is not true, the defendant's claims are often not true.</p><p>to conclude, the court held a motion is properly made to dismiss a claim for an award of attorney Military foundational to this is the cold war, which eliminates all military defense available to the enemy.</p><p>foundational to this is an attack on the conventional wisdom on the left that the left is the party views on the civil war fleet, the national maritime museum. views on the royal navy, admiralty.</p><p>views on russia's military buildup on the strength of his repeated insistence, a number of to conclude, we all agree that constructive defense methods are not yet available.</p><p>constructive defense? to conclude, the russian navy's attack on the malaysian ship, a taskforce carrying out exercises,</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Politics</head><p>an illustration of: the historical background, culture, and general political significance of the books' contents.</p><p>an illustration of an anti-democratic regime under a fascist dictatorship and its suppression of the popular opposition and the issue focused on socialism, democracy, social justice and selfgovernment in countries across the globe.</p><p>the issue focused on religious freedom in the country's constitution, a fundamental pillar of u.s. in this essay, king frederick iii of prussia was prominently featured in american post-civil war culture. in this essay, the term "political correctness" is used to refer to political demands imposed on the Religion the issue focused on the inferiority of conservatives ( "" religious conservatives "" ) vs . atheists .</p><p>the issue focused on religious freedom, particularly when the bible teaches that god is "the creator." to summarise accurately the wind direction, additional characters may be added to the classification table below.</p><p>to summarise, if the present-day christian churches were a monastic order of the monks rather an illustration of the natural history of wales by francis bacon. bateson, charles (1839).</p><p>an illustration of an ancient bronze age village in the northern greek region of crete, which shows a Science prior to this date, the manuscript was not currently available on the internet, and is cited rarely.</p><p>prior to this experiment, the scientists had not seen a new species in the area since the late 1800 the relationship has inspired research into the role of women in economics, and contributions to feminist economic theory.</p><p>the relationship between energy use and energy use as a function of time was also investigated using a linear mixed the issue focused on developments in the field of "darwinism, biology and human evolution" research.</p><p>the issue focused on data retention, and the key elements of the retention matrix, including retention of identifiers Space furthermore, the performance space is "packed with classical music" and is "lavishly decorated".</p><p>furthermore, the eighty-first star is the planet's largest moon and it sits directly in between to conclude, an asteroid becomes, mathematically, the largest asteroid to ever be "discovered".</p><p>to conclude, scientists behind spacemonkey, and a number of the other projects that nasa is supporting to summarise other countries'respective territorial claims, including territorial waters, islands, etc. . to summarise: x (1x a2 a19 a1 a2 b2   <ref type="formula">2020</ref>), which does attribute controlled generation using the flow of gradients from discriminators trained on the last hidden layer representations of the generator, to guide generation. FUDGE. This approach (Yang and Klein, 2021) trains step-wise discriminators on partial generations from GPT-2 to determine whether the constraints related to desired attributes will be satisfied by the future completion of the sequence or not. We compare against this on topic controlled generation as this approach was shown to be superior to PPLM on this task.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.4 Evaluation Metrics</head><p>We use a variety of evaluation metrics to compare our approach's performance on two major facets:</p><p>(1) Quality of generated text, and (2) success on matching the target attribute used for control.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.4.1 Text Quality and Semantic Similarity</head><p>GPT-2 PPL. We feed our generated test sentences to a Huggingface <ref type="bibr" target="#b26">(Radford et al., 2019)</ref> pre-trained GPT-2 xl model, and report its perplexity (PPL), as an automatic measure of fluency. Although this measure is not a perfect indicator of fluency, we find it to be a useful metric alongside human judgements. 3 BLEU. For sentiment (Yelp) and formality (GYAFC) transfer experiments, since we have reference text, we report the BLEU score. For controlled debiasing, we report BLEU between generated text and source, and show it as BLEU (src).</p><p>BertScore. As a measure of meaning preservation, we use the F1 BertScore metric <ref type="bibr" target="#b37">(Zhang et al., 2020)</ref> 3 Due to the high variance in the PPL scores generated across sentences by GPT-2, we report the median score for each system under comparison.</p><p>to compare the semantic similarity of the provided reference sentence with the generated output. Hamming Distance. We also report the hamming distance between the source text and generated text, to measure the extent of the change induced by our framework.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.4.2 Attribute Quality</head><p>Internal Classifier Accuracy. To evaluate the quality of applying target attributes, we report accuracy of the internal classifier (the discriminator used for generation) on the generated text, assuming the target attribute is the correct label. The higher this accuracy is, the better. External Classifier Accuracy. Since the internal classifier is the one we are sampling from, it is natural that we would get high accuracy on it, compared to our baselines. To create a more fair comparison, we also report classification accuracy using external classifiers, downloaded from Huggingface. For sentiment classification we use textattack/bert-base-uncasedyelp-polarity <ref type="bibr" target="#b23">(Morris et al., 2020)</ref>, and for formality we use cointegrated/robertabase-formality. Agency Lexicon Accuracy. For the controlled debiasing experiment, we measure the accuracy of the change in agency by comparing the target agency level with that of the generated text, extracted using the connotation frames lexicon, and following the setup from <ref type="bibr" target="#b19">Ma et al. (2020)</ref> For the results presented in Table <ref type="table" target="#tab_0">2</ref>, we ran the Gibbs chain for 8 epochs (8 iterations over all the tokens) for the conventional mode of our method, and 30 iterations for verb replacement. We used the parameters ? = 100,? = 50,? = 100, where ? is the coefficient assigned to the agency scorer, and ? and ? are defined in Equations 1 and 2.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.7 Sentiment Transfer: Hyperparameters</head><p>In this section we discuss the hyperparameters used for sampling and see the effects of each one. For the results presented in Table <ref type="table" target="#tab_1">3</ref>, we ran the Gibbs chain for 8 epochs (8 iterations over all the tokens), and used the parameters ? = 100,? = 25 (for Discriminator ?) and ? = 100,? = 50, for Hamming ?. ? and ? are defined in Equations 1 and 2. Table <ref type="table" target="#tab_8">9</ref> shows six different scenarios, with six different coefficeints for the Disciriminator (?), BERT MLM (?) and Hamming distance (?) components in the energy function, which helps understand the effect each expert has.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.8 Formality Transfer: Hyperparameters</head><p>For the results presented in Table <ref type="table" target="#tab_2">4</ref>, we ran the Gibbs chain for 5 epochs (5 iterations over all the tokens), and used the parameters ? = 140,? = 15,? = 100 (for Discriminator ?) and ? = 140,? = 50,? = 300, for BertScore ?. ?, ? and ? are defined in Equations 1 and 2.</p><p>Table <ref type="table" target="#tab_9">10</ref> shows four different scenarios, with four different coefficeints for the BLEURT and BertScore components in the energy function, which helps understand the effect each expert has. For BLEURT, we use pre-trained Elron/bleurt-base-512 from Huggingface.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.9 Prompts and Hyperparameters</head><p>Used for Controlled Generation</p><p>We have listed the prompts that we used for controlled text generation (these prompts are taken from <ref type="bibr" target="#b3">Dathathri et al. (2020)</ref>): the country, the lake, the chicken, the movie, the pizza, the painting, the year, the city, the book, the potato, the horse, the road, the president, once upon a time. We collect these prompts from PPLMs github repo, available at this url: https: //github.com/uber-research/PPLM/ tree/master/human_annotation/ pplm_labeled_csvs. PPLM has multiple knobs to tune for sampling, and after running a greed search we found that gamma=1,num_iterations=10 ,step_size=0.1,kl_scale=0.01 and gm_scale=0.95 yeild the best results (reported in Table <ref type="table" target="#tab_4">6</ref>). We generated samples by running the command python run_pplm.py -D sentiment, with the mentioned hyperparameters. For FUDGE, we tune the ? parameter, and we find that ? = 10 works best.</p><p>For our method, we ran the Gibbs chain for 15 epochs, and used hyperparameter ? = 40, from Eq. 1. We don't use any experts other than the yelp sentiment classifier, so we don't have any other hyperparamters.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.10 Human Evaluations</head><p>We used Amazon Mechanical Turk for our evaluations, where each HIT was a two choice question of "which sentence is more fluent?" and the providers were paid $0.1 per HIT. We selected Turkers from English speaking countries. We also had each each question answered 3 times (by 3 Turkers), to create redundancy and robustness.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.11 GPU Hours and Infrastructure</head><p>One of the main purposes of this work is to introduce a paradigm in which we re-use existing models and do not retrain. As such, we did not need GPUs for training (we finetuned two classifier for demonstration purposes, which took less than two GPU hours).</p><p>However, we do use GPUs for inference (less computationally intensive), for generating samples. We used an in-house 4GPU server (NVIDIA RTX2080), and the samplings and hyperparameter tuning took an overall of around 10-14 full days on the 4 GPUs.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: Overview of Mix and Match LM. The Lego pieces show different experts that can be used to form the energy LM and help control different features in the generated text. The right side shows the ith step in the the Gibbs sampling chain, where a proposal is made by the MLM, and then it is accepted/rejected based on the energy score.</figDesc><graphic url="image-4.png" coords="2,57.04,160.18,137.37,93.21" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 2 :</head><label>2</label><figDesc>Controllable debiasing/ sentence agency revision on ROC-story corpus. The (src) next to the metrics denotes measurement with respect to the source text. Int. Clsf. is the accuracy of the discriminator used in the energy. Hamm. shows the Hamming distance. Agency Acc. is the accuracy of agency revision based on the agency lexicon (Sec B.4.1).</figDesc><table><row><cell></cell><cell>Method</cell><cell cols="6">BLEU(src) GPT-2 BertScore(src) Hamm.(src) Int. Clsf. Agency Acc.</cell></row><row><cell></cell><cell>Source Text</cell><cell>100.00</cell><cell>153.9</cell><cell>1.00</cell><cell>0.00</cell><cell>7.47</cell><cell>9.81</cell></row><row><cell>Basel.</cell><cell>PowerTransformer (No Boost) PowerTransformer (+Boost)</cell><cell>60.30 57.46</cell><cell>210.8 247.2</cell><cell>0.94 0.95</cell><cell>1.11 1.28</cell><cell>64.84 77.23</cell><cell>69.17 85.03</cell></row><row><cell></cell><cell>M&amp;M LM Verb Replace (Disc)</cell><cell>60.53</cell><cell>238.7</cell><cell>0.95</cell><cell>1.04</cell><cell>81.05</cell><cell>70.80</cell></row><row><cell></cell><cell>M&amp;M LM Verb Replace (Agency Score )</cell><cell>63.34</cell><cell>193.3</cell><cell>0.96</cell><cell>0.89</cell><cell>32.42</cell><cell>64.75</cell></row><row><cell>Ours</cell><cell>M&amp;M LM Verb Replace (Disc+Agency Score) M&amp;M LM (Hamming +Disc)</cell><cell>54.52 56.26</cell><cell>248.8 211.2</cell><cell>0.95 0.95</cell><cell>1.05 1.37</cell><cell>77.23 96.52</cell><cell>77.27 69.00</cell></row><row><cell></cell><cell>M&amp;M LM (Hamming+Agency Score )</cell><cell>35.26</cell><cell>231.6</cell><cell>0.95</cell><cell>1.56</cell><cell>23.13</cell><cell>86.01</cell></row><row><cell></cell><cell>M&amp;M LM ( Hamming+Disc+Agency score)</cell><cell>39.82</cell><cell>261.6</cell><cell>0.93</cell><cell>2.45</cell><cell>90.16</cell><cell>89.42</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 3 :</head><label>3</label><figDesc>Sentiment transfer on Yelp. (ref)/(src) means the metric measured is measured with respect to reference/source text. Int./Ext. Clsf. show internal/external attribute classifier accuracy. Hamm. shows Hamming distance.</figDesc><table><row><cell></cell><cell>Method</cell><cell cols="6">BLEU(ref) GPT-2 BertScore(src) Hamm.(src) Int. Clsf. Ext. Clsf.</cell></row><row><cell></cell><cell>Reference Text</cell><cell>100.00</cell><cell>169.5</cell><cell>1.00</cell><cell>5.80</cell><cell>83.70</cell><cell>85.60</cell></row><row><cell>Basel.</cell><cell>He et al. UNMT</cell><cell>18.67 17.00</cell><cell>200.6 171.8</cell><cell>0.93 0.94</cell><cell>4.23 3.67</cell><cell>84.87 84.87</cell><cell>79.82 80.22</cell></row><row><cell>Ours</cell><cell>M&amp;M LM (Discriminator ?) M&amp;M LM (Hamming?)</cell><cell>15.75 19.71</cell><cell>163.5 191.5</cell><cell>0.93 0.95</cell><cell>2.84 1.83</cell><cell>97.53 94.72</cell><cell>90.00 82.85</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 4 :</head><label>4</label><figDesc></figDesc><table><row><cell></cell><cell>Reference Text</cell><cell>100.00</cell><cell>118.1</cell><cell>0.92</cell><cell>7.72</cell><cell>82.97</cell><cell>100.00</cell><cell>9.41</cell></row><row><cell>Basel.</cell><cell>He et al. UNMT</cell><cell>15.83 14.17</cell><cell>122.8 143.8</cell><cell>0.90 0.90</cell><cell>10.03 11.92</cell><cell>64.79 56.04</cell><cell>100.00 99.81</cell><cell>3.33 7.64</cell></row><row><cell>Ours</cell><cell>M&amp;M LM (Discriminator ?) M&amp;M LM (BertScore?)</cell><cell>17.78 27.71</cell><cell>206.3 194.4</cell><cell>0.89 0.93</cell><cell>5.22 2.50</cell><cell>91.15 72.12</cell><cell>96.67 94.26</cell><cell>23.13 19.01</cell></row></table><note><p>Formality transfer on GYAFC dataset. The (ref)/(src) next to the metrics denotes that they are measured with respect to the reference/source text. Int. Clsf. shows the accuracy of the discriminator used in the energy, and ?Informal/Form. shows the breakdown of the external classifier accuracy. Hamm. shows the Hamming distance. Method BLEU(ref) GPT-2 BertScore(src) Hamm.(src) Int. Clsf. ?Informal ?Form.</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 5 :</head><label>5</label><figDesc>Samples of prompted sentiment controlled generations, using our Mix and Match LM and PPLM.</figDesc><table><row><cell>Ours (Mix and Match LM)</cell><cell>PPLM</cell></row><row><cell>Pos Sent.</cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 6 :</head><label>6</label><figDesc>Prompted sentiment controlled generation results and human evaluations.BERT denotes the BERT MLM energy score (equivalent of GPT-2 perplexity), and lower score is better. Int./Ext. Clsf. show the accuracy of the discriminator used in the energy/external discriminator from Huggingface.</figDesc><table><row><cell>Length</cell><cell cols="2">GPT-2 (?)</cell><cell></cell><cell>BERT (?)</cell><cell cols="2">Int. Clsf. (?)</cell><cell cols="2">Ext. Clsf. (?)</cell><cell cols="2">Human Preference (%)</cell></row><row><cell></cell><cell>Ours</cell><cell>PPLM</cell><cell>Ours</cell><cell>PPLM</cell><cell cols="2">Ours PPLM</cell><cell cols="2">Ours PPLM</cell><cell>Ours</cell><cell>PPLM</cell></row><row><cell>12</cell><cell>264.1</cell><cell>113.1</cell><cell>-160.4</cell><cell>-137.1</cell><cell>94.3</cell><cell>71.7</cell><cell>65.1</cell><cell>58.0</cell><cell>71.1</cell><cell>29.9</cell></row><row><cell>20</cell><cell>167.2</cell><cell>61.1</cell><cell>-271.0</cell><cell>-237.1</cell><cell>96.3</cell><cell>74.5</cell><cell>65.9</cell><cell>57.6</cell><cell>62.9</cell><cell>37.1</cell></row><row><cell>50</cell><cell>122.3</cell><cell>29.0</cell><cell>-692.3</cell><cell>-606.1</cell><cell>93.8</cell><cell>73.6</cell><cell>68.6</cell><cell>60.7</cell><cell>46.7</cell><cell>53.3</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 7 :</head><label>7</label><figDesc>Prompted topic controlled generation results and human evaluations.</figDesc><table><row><cell>Metrics</cell><cell cols="2">FUDGE M&amp;M LM</cell></row><row><cell>Topic-score (?)</cell><cell>1.45</cell><cell>1.21</cell></row><row><cell>Grammaticality (?)</cell><cell>0.61</cell><cell>0.74</cell></row><row><cell>GPT-2 PPL (?)</cell><cell>104.8</cell><cell>110.2</cell></row><row><cell>Diversity over Unigrams (?)</cell><cell>0.54</cell><cell>0.57</cell></row><row><cell>Diversity over Bigrams (?)</cell><cell>0.86</cell><cell>0.89</cell></row><row><cell>Diversity over Trigrams (?)</cell><cell>0.87</cell><cell>0.88</cell></row><row><cell>Human Preference(%) (?)</cell><cell>36.5</cell><cell>63.5</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 8 :</head><label>8</label><figDesc>Samples of prompted topic controlled generations, using our Mix and Match LM and FUDGE. please link to (chessworld.net/chessworld/download.html). to review, instead of using the "n/a" flag (like on our previous posts) in summary, key program clients are homeforge, blogdev and skeptic.net. in summary:-install and run a local mysql server on the host computeradd a mysql table it has been shown using several techniques, including microscopy, electron microscopy, and digital loansharking. it has been shown using ebpf/ebpis (extraction of a new ebp</figDesc><table><row><cell></cell><cell>Ours (Mix and Match LM)</cell><cell>FUDGE</cell></row><row><cell>Computer</cell><cell>to review,</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>Table 9 :</head><label>9</label><figDesc>Sentiment transfer on Yelp dataset ablation study. The tuples in the first column show the (?,?,?) set of parameters. We ablate the effect that different components have on the transfer.The (ref)/(src) next to the metrics denotes that they are measured with respect to the reference/source text. Int./Ext. Clsf. show the accuracy of the discriminator used in the energy/external discriminator from Huggingface. Hamm. shows the Hamming distance.</figDesc><table><row><cell>(Disc, MLM, Hamm.)</cell><cell>BLEU</cell><cell>GPT-2</cell><cell cols="4">BertScore Hamm. Int. Clsf. Ext. Clsf.</cell></row><row><cell>(1,0,1)</cell><cell cols="2">4.77 1611.8</cell><cell>0.88</cell><cell>5.308</cell><cell>81.7</cell><cell>67.4</cell></row><row><cell>(1,0,0)</cell><cell cols="2">1.12 3825.3</cell><cell>0.85</cell><cell>8.378</cell><cell>99.0</cell><cell>84.5</cell></row><row><cell>(0,1,0)</cell><cell>3.77</cell><cell>101.3</cell><cell>0.90</cell><cell>5.92</cell><cell>24.7</cell><cell>29.3</cell></row><row><cell>(100,1,0)</cell><cell>2.89</cell><cell>143.0</cell><cell>0.88</cell><cell>7.067</cell><cell>99.2</cell><cell>96.5</cell></row><row><cell>(0,1,50)</cell><cell>23.60</cell><cell>110.0</cell><cell>0.99</cell><cell>0.002</cell><cell>4.3</cell><cell>5.0</cell></row><row><cell>(100,1,50)</cell><cell>19.71</cell><cell>191.5</cell><cell>0.95</cell><cell>1.838</cell><cell>94.7</cell><cell>82.8</cell></row></table><note><p><p><p><p>ity domains, we compare our work with</p><ref type="bibr" target="#b9">He et al. (2020)</ref></p>, a generative style transfer framework which uses a variational autoencoder (VAE) built using a sequence-to-sequence LSTM-based model to do un-supervised style transfer. This framework needs to be trained from scratch for each style transfer task.</p>UNMT. As a second baseline for style transfer, we compare our work with UNMT (Lample</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head>Table 10 :</head><label>10</label><figDesc>Formality transfer on GYAFC dataset ablation study. The tuples in the first column show the (?,?) set of parameters. We ablate the effect the BLEURT and BertScore experts have on the transfer. The (ref)/(src) next to the metrics denotes that they are measured with respect to the reference/source text. Int. Clsf. shows the accuracy of the discriminator used in the energy, and ?Informal/Form. shows the breakdown of the external classifier accuracy.Hamm. shows the Hamming distance.</figDesc><table><row><cell>(BLEURT,BertScore)</cell><cell cols="6">BLEU GPT-2 BertScore Hamm. Int. Clsf. ?Inf. ?Form.</cell></row><row><cell>(100,0)</cell><cell>14.07 243.9</cell><cell>0.87</cell><cell>5.93</cell><cell>89.34</cell><cell>97.41</cell><cell>19.80</cell></row><row><cell>(300,0)</cell><cell>13.75 233.9</cell><cell>0.88</cell><cell>5.88</cell><cell>89.34</cell><cell>97.01</cell><cell>22.94</cell></row><row><cell>(0,100)</cell><cell>17.78 206.3</cell><cell>0.89</cell><cell>5.22</cell><cell>91.15</cell><cell>96.67</cell><cell>23.13</cell></row><row><cell>(0,300)</cell><cell>18.85 210.9</cell><cell>0.90</cell><cell>4.91</cell><cell>88.23</cell><cell>97.04</cell><cell>23.13</cell></row><row><cell cols="2">et al., 2018), an unsupervised machine translation</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="2">framework that demonstrates high performance for</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>sentiment transfer.</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="2">PPLM. For the task of sentiment controlled</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="2">generation, we compare our work to Plug-and-Play</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>LM (PPLM) Dathathri et al. (</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_10"><head></head><label></label><figDesc>.Selection of components is based on the needs of the task and is straight forward. You add each component you need, to satisfy some condition. If you want to do sentiment controlled generation, you add a sentiment classifier. Finding the hyperparameters for each component (the multiplier in energy) is also simple, since the trade-off between the different components is clear. For instance, as shown in Table9, increasing the discriminator score results in a more successful sentiment transfer, and increasing the Hamming score results in keeping the sentence the same.</figDesc><table><row><cell>B.6 Controllable Debiasing:</cell></row><row><cell>Hyper parameters</cell></row><row><cell>B.5 Hyper-parameter</cell></row><row><cell>and Component Selection</cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0"><p>For simplicity, we are concerned with a finite set of sequences limited by some maximum length.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_1"><p>Due to the high variance in the PPL scores generated across sentences by GPT-2, we report the median score for each system under comparison.</p></note>
		</body>
		<back>

			<div type="acknowledgement">
<div><head>Acknowledgments</head><p>The authors would like to thank the anonymous reviewers and meta-reviewers for their helpful feedback. We also thank our colleagues at the <rs type="affiliation">UCSD/CMU Berg Lab</rs> for their helpful comments and feedback.</p></div>
			</div>			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Ethical Considerations</head><p>The proposed approach takes steps towards a novel paradigm that might partially mitigate the need for energy-intensive GPU training -potentially leading to positive environmental impact down the line. The approach may also have positive impacts on accessibility as strong computational resources are not required when setting up a new controlled text generation system. We do however acknowledge that strong controlled generation methods that rely on discriminators have the potential to regurgitate sensitive training data and produce harmful outputs and toxic language <ref type="bibr" target="#b35">(Xu et al.;</ref><ref type="bibr" target="#b6">Gehman et al., 2020;</ref><ref type="bibr" target="#b33">Wallace et al., 2020)</ref>. However, if used properly and for good, we anticipate a positive impact on debiasing and safe generation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A Appendix</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.1 Code and Data Directory Structure</head><p>We have provided all our code, data and our generations in https://github.com/ mireshghallah/mixmatch, and our checkpoints are uploaded anonymously here https://zenodo.org/record/5855005. There is a readme file in the repo, which has instructions on how to run generation and get evaluation metrics. We have not included the data files for the formality, since the GYAFC dataset requires permission for access, so we cannot release it.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.2 Sample Generations</head></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">Just say no: Analyzing the stance of neural dialogue generation in offensive contexts</title>
		<author>
			<persName><forename type="first">Ashutosh</forename><surname>Baheti</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Maarten</forename><surname>Sap</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alan</forename><surname>Ritter</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mark</forename><surname>Riedl</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2108.11830</idno>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<author>
			<persName><forename type="first">Hannah</forename><surname>Brown</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Katherine</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fatemehsadat</forename><surname>Mireshghallah</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Reza</forename><surname>Shokri</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Florian</forename><surname>Tram?r</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2202.05520</idno>
		<title level="m">What does it mean for a language model to preserve privacy? arXiv preprint</title>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">Efficient hierarchical domain adaptation for pretrained language models</title>
		<author>
			<persName><forename type="first">Alexandra</forename><surname>Chronopoulou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Matthew</forename><forename type="middle">E</forename><surname>Peters</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jesse</forename><surname>Dodge</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2112.08786</idno>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Plug and play language models: A simple approach to controlled text generation</title>
		<author>
			<persName><forename type="first">Sumanth</forename><surname>Dathathri</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrea</forename><surname>Madotto</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Janice</forename><surname>Lan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jane</forename><surname>Hung</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Eric</forename><surname>Frank</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Piero</forename><surname>Molino</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jason</forename><surname>Yosinski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rosanne</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">BERT: Pre-training of deep bidirectional transformers for language understanding</title>
		<author>
			<persName><forename type="first">Jacob</forename><surname>Devlin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ming-Wei</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kenton</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kristina</forename><surname>Toutanova</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/N19-1423</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2019 Conference the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</title>
		<title level="s">Long and Short Papers</title>
		<meeting>the 2019 Conference the North American Chapter of the Association for Computational Linguistics: Human Language Technologies<address><addrLine>Minneapolis, Minnesota</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2019">2019</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="4171" to="4186" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Controlling linguistic style aspects in neural language generation</title>
		<author>
			<persName><forename type="first">Jessica</forename><surname>Ficler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yoav</forename><surname>Goldberg</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/W17-4912</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Workshop on Stylistic Variation</title>
		<meeting>the Workshop on Stylistic Variation<address><addrLine>Copenhagen, Denmark</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="94" to="104" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<author>
			<persName><forename type="first">Suchin</forename><surname>Samuel Gehman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Maarten</forename><surname>Gururangan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yejin</forename><surname>Sap</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Noah</forename><forename type="middle">A</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName><surname>Smith</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2009.11462</idno>
		<title level="m">Realtoxicityprompts: Evaluating neural toxic degeneration in language models</title>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">Exposing the implicit energy networks behind masked language models via metropolis-hastings</title>
		<author>
			<persName><forename type="first">Kartik</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chris</forename><surname>Dyer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Taylor</forename><surname>Berg-Kirkpatrick</surname></persName>
		</author>
		<idno>ArXiv, abs/2106.02736</idno>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Don&apos;t stop pretraining: Adapt language models to domains and tasks</title>
		<author>
			<persName><forename type="first">Suchin</forename><surname>Gururangan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ana</forename><surname>Marasovi?</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Swabha</forename><surname>Swayamdipta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kyle</forename><surname>Lo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Iz</forename><surname>Beltagy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Doug</forename><surname>Downey</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Noah</forename><forename type="middle">A</forename><surname>Smith</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2020.acl-main.740</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 58th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<publisher>Online. Association for Computational Linguistics</publisher>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="8342" to="8360" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Monte carlo sampling methods using markov chains and their applications</title>
		<author>
			<persName><forename type="first">Hastings ; Junxian</forename><surname>Keith</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xinyi</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Graham</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Taylor</forename><surname>Neubig</surname></persName>
		</author>
		<author>
			<persName><surname>Berg-Kirkpatrick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="1970">1970. 2020</date>
		</imprint>
	</monogr>
	<note>A probabilistic formulation of unsupervised text style transfer</note>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Training products of experts by minimizing contrastive divergence</title>
		<author>
			<persName><forename type="first">Geoffrey</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural computation</title>
		<imprint>
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="1771" to="1800" />
			<date type="published" when="2002">2002</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Learning to write with cooperative discriminators</title>
		<author>
			<persName><forename type="first">Ari</forename><surname>Holtzman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jan</forename><surname>Buys</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Maxwell</forename><surname>Forbes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Antoine</forename><surname>Bosselut</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><surname>Golub</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yejin</forename><surname>Choi</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/P18-1152</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 56th Annual Meeting of the Association for Computational Linguistics<address><addrLine>Melbourne</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2018">2018</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="1638" to="1649" />
		</imprint>
	</monogr>
	<note>Long Papers)</note>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">Deduplicating training data mitigates privacy risks in language models</title>
		<author>
			<persName><forename type="first">Nikhil</forename><surname>Kandpal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Eric</forename><surname>Wallace</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Colin</forename><surname>Raffel</surname></persName>
		</author>
		<idno>ArXiv, abs/2202.06539</idno>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<author>
			<persName><forename type="first">Nitish</forename><surname>Shirish Keskar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bryan</forename><surname>Mccann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Lav</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Caiming</forename><surname>Varshney</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Richard</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName><surname>Socher</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1909.05858</idno>
		<title level="m">Ctrl: A conditional transformer language model for controllable generation</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">A distributional approach to controlled text generation</title>
		<author>
			<persName><forename type="first">Muhammad</forename><surname>Khalifa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hady</forename><surname>Elsahar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Marc</forename><surname>Dymetman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<author>
			<persName><forename type="first">Ben</forename><surname>Krause</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Akhilesh</forename><surname>Deepak Gotmare</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bryan</forename><surname>Mc-Cann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nitish</forename><surname>Shirish Keskar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shafiq</forename><surname>Joty</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nazneen</forename><surname>Fatema</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rajani</forename></persName>
		</author>
		<idno type="arXiv">arXiv:2009.06367</idno>
		<title level="m">GeDi: Generative Discriminator Guided Sequence Generation</title>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">Reformulating unsupervised style transfer as paraphrase generation</title>
		<author>
			<persName><forename type="first">Kalpesh</forename><surname>Krishna</surname></persName>
		</author>
		<author>
			<persName><forename type="first">John</forename><surname>Wieting</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mohit</forename><surname>Iyyer</surname></persName>
		</author>
		<idno>ArXiv, abs/2010.05700</idno>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Guillaume Lample, Myle Ott, Alexis Conneau, Ludovic Denoyer, and Marc&apos;Aurelio Ranzato</title>
		<author>
			<persName><forename type="first">Sachin</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Eric</forename><surname>Malmi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aliaksei</forename><surname>Severyn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yulia</forename><surname>Tsvetkov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2018 Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2018">2021. 2018</date>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="page" from="5039" to="5049" />
		</imprint>
	</monogr>
	<note>Advances in Neural Information Processing Systems</note>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">DExperts: Decodingtime controlled text generation with experts and anti-experts</title>
		<author>
			<persName><forename type="first">Alisa</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Maarten</forename><surname>Sap</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ximing</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Swabha</forename><surname>Swayamdipta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chandra</forename><surname>Bhagavatula</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Noah</forename><forename type="middle">A</forename><surname>Smith</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yejin</forename><surname>Choi</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2021.acl-long.522</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing</title>
		<meeting>the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing</meeting>
		<imprint>
			<publisher>Online. Association for Computational Linguistics</publisher>
			<date type="published" when="2021">2021</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="6691" to="6706" />
		</imprint>
	</monogr>
	<note>Long Papers)</note>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">PowerTransformer: Unsupervised controllable revision for biased language correction</title>
		<author>
			<persName><forename type="first">Xinyao</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Maarten</forename><surname>Sap</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hannah</forename><surname>Rashkin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yejin</forename><surname>Choi</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2020.emnlp-main.602</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)</title>
		<meeting>the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)</meeting>
		<imprint>
			<publisher>Online. Association for Computational Linguistics</publisher>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="7426" to="7441" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Plug and play autoencoders for conditional text generation</title>
		<author>
			<persName><forename type="first">Florian</forename><surname>Mai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nikolaos</forename><surname>Pappas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ivan</forename><surname>Montero</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Noah</forename><forename type="middle">A</forename><surname>Smith</surname></persName>
		</author>
		<author>
			<persName><forename type="first">James</forename><surname>Henderson</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2020.emnlp-main.491</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)</title>
		<meeting>the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)</meeting>
		<imprint>
			<publisher>Online. Association for Computational Linguistics</publisher>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="6076" to="6092" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Style pooling: Automatic text style obfuscation for improved classification fairness</title>
		<author>
			<persName><forename type="first">Fatemehsadat</forename><surname>Mireshghallah</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Taylor</forename><surname>Berg-Kirkpatrick</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2021.emnlp-main.152</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2021 Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<publisher>Dominican Republic. Association for Computational Linguistics</publisher>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="2009" to="2022" />
		</imprint>
		<respStmt>
			<orgName>Online and Punta Cana</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Privacy regularization: Joint privacy-utility optimization in LanguageModels</title>
		<author>
			<persName><forename type="first">Fatemehsadat</forename><surname>Mireshghallah</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Huseyin</forename><surname>Inan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Marcello</forename><surname>Hasegawa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Victor</forename><surname>R?hle</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Taylor</forename><surname>Berg-Kirkpatrick</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Robert</forename><surname>Sim</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2021.naacl-main.298</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</title>
		<meeting>the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</meeting>
		<imprint>
			<publisher>Online. Association for Computational Linguistics</publisher>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="3799" to="3807" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Textattack: A framework for adversarial attacks, data augmentation, and adversarial training in nlp</title>
		<author>
			<persName><forename type="first">John</forename><surname>Morris</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Eli</forename><surname>Lifland</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jin</forename><forename type="middle">Yong</forename><surname>Yoo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jake</forename><surname>Grigsby</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Di</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yanjun</forename><surname>Qi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing: System Demonstrations</title>
		<meeting>the 2020 Conference on Empirical Methods in Natural Language Processing: System Demonstrations</meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="119" to="126" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">A corpus and cloze evaluation for deeper understanding of commonsense stories</title>
		<author>
			<persName><forename type="first">Nasrin</forename><surname>Mostafazadeh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nathanael</forename><surname>Chambers</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaodong</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Devi</forename><surname>Parikh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dhruv</forename><surname>Batra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lucy</forename><surname>Vanderwende</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pushmeet</forename><surname>Kohli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">James</forename><surname>Allen</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/N16-1098</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2016 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</title>
		<meeting>the 2016 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies<address><addrLine>San Diego, California</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="839" to="849" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Exploring controllable text generation techniques</title>
		<author>
			<persName><forename type="first">Shrimai</forename><surname>Prabhumoye</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alan</forename><forename type="middle">W</forename><surname>Black</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ruslan</forename><surname>Salakhutdinov</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2020.coling-main.1</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 28th International Conference on Computational Linguistics</title>
		<meeting>the 28th International Conference on Computational Linguistics<address><addrLine>Barcelona, Spain (Online</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="1" to="14" />
		</imprint>
	</monogr>
	<note>International Committee on Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<title level="m" type="main">Language models are unsupervised multitask learners</title>
		<author>
			<persName><forename type="first">Alec</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jeff</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rewon</forename><surname>Child</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><surname>Luan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dario</forename><surname>Amodei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Dear sir or madam, may i introduce the gyafc dataset: Corpus, benchmarks and metrics for formality style transfer</title>
		<author>
			<persName><forename type="first">Sudha</forename><surname>Rao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Joel</forename><forename type="middle">R</forename><surname>Tetreault</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NAACL</title>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<author>
			<persName><forename type="first">Emily</forename><surname>Reif</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Daphne</forename><surname>Ippolito</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ann</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andy</forename><surname>Coenen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chris</forename><surname>Callison-Burch</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jason</forename><surname>Wei</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2109.03910</idno>
		<title level="m">A recipe for arbitrary text style transfer with large language models</title>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Masked language model scoring</title>
		<author>
			<persName><forename type="first">Julian</forename><surname>Salazar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Davis</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Toan</forename><forename type="middle">Q</forename><surname>Nguyen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Katrin</forename><surname>Kirchhoff</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2020.acl-main.240</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 58th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<publisher>Online. Association for Computational Linguistics</publisher>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="2699" to="2712" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Connotation frames of power and agency in modern films</title>
		<author>
			<persName><forename type="first">Maarten</forename><surname>Sap</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Marcella</forename></persName>
		</author>
		<author>
			<persName><forename type="first">Cindy</forename><surname>Prasettio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ari</forename><surname>Holtzman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hannah</forename><surname>Rashkin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yejin</forename><surname>Choi</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/D17-1247</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2017 Conference on Empirical Methods in Natural Language Processing<address><addrLine>Copenhagen, Denmark</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="2329" to="2334" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
		<author>
			<persName><forename type="first">Maarten</forename><surname>Sap</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Swabha</forename><surname>Swayamdipta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Laura</forename><surname>Vianna</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xuhui</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yejin</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Noah</forename><forename type="middle">A</forename><surname>Smith</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2111.07997</idno>
		<title level="m">Annotators with attitudes: How annotator beliefs and identities bias toxic language detection</title>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Style transfer from non-parallel text by cross-alignment</title>
		<author>
			<persName><forename type="first">Tianxiao</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tao</forename><surname>Lei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Regina</forename><surname>Barzilay</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tommi</forename><surname>Jaakkola</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 31st International Conference on Neural Information Processing Systems</title>
		<meeting>the 31st International Conference on Neural Information Processing Systems</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="6833" to="6844" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Imitation attacks and defenses for black-box machine translation systems</title>
		<author>
			<persName><forename type="first">Eric</forename><surname>Wallace</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mitchell</forename><surname>Stern</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dawn</forename><forename type="middle">Xiaodong</forename><surname>Song</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP</title>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Neural network acceptability judgments</title>
		<author>
			<persName><forename type="first">Alex</forename><surname>Warstadt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Amanpreet</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName><surname>Samuel R Bowman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Transactions of the Association for Computational Linguistics</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page" from="625" to="641" />
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<monogr>
		<title level="m" type="main">Detoxifying language models risks marginalizing minority voices</title>
		<author>
			<persName><forename type="first">Albert</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Eshaan</forename><surname>Pathak</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Eric</forename><surname>Wallace</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Suchin</forename><surname>Gururangan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Maarten</forename><surname>Sap</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dan</forename><surname>Klein</surname></persName>
		</author>
		<author>
			<persName><surname>Berkeley</surname></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">FUDGE: Controlled text generation with future discriminators</title>
		<author>
			<persName><forename type="first">Kevin</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dan</forename><surname>Klein</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2021.naacl-main.276</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</title>
		<meeting>the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</meeting>
		<imprint>
			<publisher>Online. Association for Computational Linguistics</publisher>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="3511" to="3535" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Bertscore: Evaluating text generation with bert</title>
		<author>
			<persName><forename type="first">Tianyi</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Varsha</forename><surname>Kishore</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Felix</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kilian</forename><forename type="middle">Q</forename><surname>Weinberger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yoav</forename><surname>Artzi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<monogr>
		<author>
			<persName><forename type="first">Nisan</forename><surname>Daniel M Ziegler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jeffrey</forename><surname>Stiennon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tom</forename><forename type="middle">B</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alec</forename><surname>Brown</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dario</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Paul</forename><surname>Amodei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Geoffrey</forename><surname>Christiano</surname></persName>
		</author>
		<author>
			<persName><surname>Irving</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1909.08593</idno>
		<title level="m">Fine-tuning language models from human preferences</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
