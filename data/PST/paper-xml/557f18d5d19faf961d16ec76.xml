<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Using Mutual Information for Selecting Features in Supervised Neural Net Learning</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Roberto</forename><surname>Battiti</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Califomia Institute of Technology</orgName>
								<address>
									<postCode>1990</postCode>
									<settlement>Pasadena</settlement>
									<region>CA</region>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Using Mutual Information for Selecting Features in Supervised Neural Net Learning</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.2" ident="GROBID" when="2022-12-25T13:35+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Feature extraction</term>
					<term>neural network pruning</term>
					<term>dimensionality reduction</term>
					<term>mutual information</term>
					<term>supervised learning</term>
					<term>adaptive classifiers</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>This paper investigates the application of the mutual infor"</head><p>criterion to evaluate a set of candidate features and to select an informative subset to be used as input data for a neural network classifier. Because the mutual information measures arbitrary dependencies between random variables, it is suitable for assessing the "information content" of features in complex classification tasks, where methods bases on linear relations (like the correlation) are prone to mistakes. The fact that the mutual information is independent of the coordinates chosen permits a robust estimation. Nonetheless, the use of the mutual information for tasks characterized by high input dimensionality requires suitable approximations because of the prohibitive demands on computation and samples. An algorithm is proposed that is based on a "greedy" selection of the features and that takes both the mutual information with respect to the output class and with respect to the already-selected features into account. Finally the results of a series of experiments are discussed.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>I. INTRODUCTION</head><p>URING the development of neural net classifiers the D "preprocessing" stage, where an appropriate number of relevant features is extracted from the raw data, has a crucial impact both on the complexity of the learning phase and on the achievable generalization performance. While it is essential that the information contained in the input vector is sufficient to determine the output class, the presence of too many input features can burden the training process and can produce a neural network with more connection weights that those required by the problem.</p><p>From an application-oriented point of view, an excessive input dimensionality implies lengthened preprocessing and recognition times, even if the learning and recognition performance is satisfactory.</p><p>In this paper we consider the use of the mutual information (MI for short) to evaluate the "information content" of each individual feature with regard to the output class. The approximated evaluation of the mutual information of each candidate feature is the starting component of a "pruning" algorithm that selects a subset of relevant features from an initial set of available features. In addition to their practical use for limiting the input dimensionality, the analysis based on the mutual information provides the developer with a useful diagnosis of the relevance of different features and of the mutual dependencies.</p><p>Different feature selection methods have been analyzed in the past. For example, in [lo] the irrelevant features are eliminated as a consequence of a pruning of the weights, that considers the sensitivity of the global error function E to the presence or absence of the different synapses. The sensitivity is estimated by integrating the partial derivatives d E / d w on the path in weight space traced during the learning process. Our method, while producing similar results in test cases, is applied before learning starts and therefore does not depend on the learning process. Other techniques are based on linear transformations of the input vector. In <ref type="bibr" target="#b12">[14]</ref> the Karhunen-Loe've transformation is applied so that the transformed coordinates can be arranged in order of their "significance," considering first the components corresponding to the major eigenvectors of the correlation matrix. In <ref type="bibr" target="#b16">[18]</ref> different feature evaluation methods are compared. In particular the method based on principal component analysis (PCA) evaluates the features according to the projection of the largest eigenvector of the correlation matrix on the initial dimensions, the method based on Fisher's linear discriminant analysis evaluates.them according to the magnitude of the components of the discriminant vector.</p><p>A major weakness of these methods is that they are not invariant under a transformation of the variables. For example a linear scaling of the input variables (that may be caused by a change of units for the measurements) is sufficient to modify the PCA results. Feature selection methods that are sufficient for simple distributions of the patterns belonging to different classes can fail in classification tasks with complex decision boundaries. In addition, methods based on a linear dependence (like the correlation) cannot take care of arbitrary relations between the pattem coordinates and the different classes. On the contrary, the mutual information can measure arbitrary relations between variables and it does not depend on transformations acting on the different variables.</p><p>In the following sections, first we summarize the relevant concepts (Section 11), then we analyze the practical applicabilitv of the mutual information and propose an approximated </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="11.">BACKGROUND</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A . De$nition of the Mutual Information</head><p>An operating classifier (consider for example a multilayer perceptron trained to classify pattems from a set of different classes with the backpropagation algorithm described in <ref type="bibr" target="#b17">[19]</ref>) can be considered as a system that reduces the initial uncertainty, to be defined precisely later, by "consuming" the information contained in the input vector. In the ideal case the final uncertainty will be zero (i.e., the class will be certain), in actual "real world" applications the final uncertainty can be higher for at least two different reasons, insufficient input information or suboptimal operation. In the second case the available information can be sufficient to resolve all ambiguities but the network "wastes" some of it because of insufficient training, approximations or failures. While this case can be remedied by considering additional training examples, a longer training period or different algorithms, the lack of sufficient information should be detected as soon as possible in the development process because in this case the only remedy is that of adding more features or considering more informative ones.</p><p>Shannon's information theory (see <ref type="bibr" target="#b18">[20]</ref>) provides a suitable formalism for quantifying the above concepts. If the probabilities' for the different classes are P(c); c = 1,. . . Nc, the initial uncertainty in the output class is measured by the entropy:</p><formula xml:id="formula_0">Nc H ( C ) = - P(c) logP(c)<label>(1)</label></formula><p>c=l while the average uncertainty after knowing the feature vector f (with N f components) is the conditional entropy:</p><p>where P(clf) is the conditional probability for class c given the input vector f. If the feature vector is composed of continuous variables, the sum will be replaced by an integral and the probabilities by the corresponding probability densities.</p><p>For example, in one dimension, one has:</p><formula xml:id="formula_1">H ( F ) = -J P(f) logP(f) df (3)</formula><p>Note that the entropies of continuous systems depend on coordinates. For a linear transformation with f -+ f' = af, the above integral becomes</p><formula xml:id="formula_2">H'(F) = -P'(f')logP'(f')df' 1 = -J P ( f ) l o g ( y ) d f = H ( F ) + l o g a ( 4 )</formula><p>In general, the conditional entropy will be less than or equal to the initial entropy. It is equal if and only if one has independence between features and output class (i.e., if the joint probability density is the product of the individual densities: P ( c , f ) = P(c)P(f) ). The amount by which the uncertainty is decreased is, by definition, the mutual information I(C; F) between variables c and f:</p><formula xml:id="formula_3">I ( C ; F ) = H ( C ) -H(CIF) (5)</formula><p>This function is symmetric with respect to C and F and, with simple algebraic manipulations, can be reduced to the following expression:</p><p>The mutual information is therefore the amount by which the knowledge provided by the feature vector decreases the uncertainty about the class. If one considers the uncertainty in the combined events (c, f), i.e., H(C; F), in general this is less than the sum of the individual uncertainties H ( C ) and H ( F ) and it is possible to demonstrate the following relation:</p><formula xml:id="formula_4">(7)</formula><p>The combined uncertainty is reduced because of the information that one variable provides about the other one. If the feature vector has continuous components, one obtains:</p><formula xml:id="formula_5">H ( C ; F ) = H ( C ) + H ( F ) -I ( C ; F ) (8)</formula><p>The argument of the logarithm in ( <ref type="formula">8</ref>) is now dimensionless, so that the MI does not depend on a transformation of variables2. The MI is a function of the joint probability distribution of the two variables c and f. For a qualitative explanation, let's consider a particular value of f. The contribution to the integral is large and positive if the distribution P ( c , f ) is "uneven" (at the limit peaked for a single c, the "correct" class), and tends to zero for the limit of a flat distribution for c, given by P(c)P(f). The MI measures the "lumpiness" of the joint distribution.</p><p>Although the main motivation of Information Theory was the engeneering of "noisy" communication channels, its concepts have been applied to different fields, in particular [9] considers the implications for statistical decisionmaking, a field closely related to pattem recognition and classification, <ref type="bibr" target="#b1">[6]</ref> uses the mutual information to find the optimal time delay to construct a multidimensional phase portrait of a dynamical system, with implications for the prediction of temporal series. In the field of neural networks, methods and concepts from Information Theory have been used, for example, in [ 131 for the generation of ordered maps. Training algorithms based on the MI are considered in <ref type="bibr">[2]</ref>, where the training criterion is based on the relative entropy (i.e., the likelihood of the targets given the networks outputs), in <ref type="bibr">[l]</ref> where the minimization of the conditional class entropy is the basis of a learning algorithm that builds a multilayer network, and in <ref type="bibr" target="#b21">[23]</ref> for one case of unsupervised learning.</p><p>The transformations considered are invertible and differentiable.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Advantages over Correlation</head><p>It is well known that the main advantage of the multilayer perceptron over the simple perceptron model is given by its capability of realizing arbitrary continuous mappings between inputs and outputs <ref type="bibr" target="#b2">[7]</ref>. For classification, this result implies that a multilayer perceptron with at least one hidden layer can realize arbitrary nonlinear separations between different classes3.</p><p>While linear methods of analysis (like the correlation) can be useful in particular cases, in general it is essential to consider also nonlinear relations between different variables. The motivation for considering the MI is its capability to measure a general dependence between two variables.</p><p>For example, to realize the classification given by the exclusive OR function of two input variables (with equal probabilities for the possible inputs), the correlation I? between any input variable x and the output variable y is zero (r = Ci CjPijxiyj -( x i P i x i ) ( &amp; P j y j ) = 0), while the MI between the input vector and the output is log,2 bits, equal to the initial uncertainty of one bit? the input vector determines the output class with no ambiguity. In other words, two variables x and y are linearly independent if E(xy) = E(x)E(y) (E being the expectation) and generally independent if P ( x , y) = P(x)P(y). General independence implies linear independence, but not vice versa. While the difference between MI and correlation for Gaussian random variables is trivial (in this case from the correlation C,, = U,,/-, where a i is the standard deviation and aij the covariance matrix, one can derive the MI as</p><formula xml:id="formula_6">I ( X , Y ) = -(1/2)log[l -C&amp;], see [5]</formula><p>) and two variables are linearly independent if and only if they are generally independent, for complex probability densities the concept of linear dependence is not a very useful one. A detailed investigation of the advantages of the MI versus the correlation is contained in </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="111.">SELECTING FEATURES WITH THE MUTUAL INFORMATION</head><p>In the development of a classifier one often is confronted with practical constraints on the hardware and on the time that is allotted to the task. While many kinds of features can be extracted from the raw data (consider for example an Optical Character Recognition task) and the information contained in them is sufficient to determine the class with low ambiguity, one may be forced to reduce an initial set of n features to a smaller set of k features, where the number k is related to the practical constraints. Let's abstract from the above considerations the following "feature reduction" problem:</p><p>[FRn-k:] Given an initial set of n features, find the subset with k &lt; n features that is "maximally informative" about the class. In the framework of Information Theory, remembering ( 5 ) and the fact that the class uncertainty is fixed, the problem can be reformulated as follows:</p><p>[FRn-k] Given an initial set F with n features, find the subset S C F with k features that minimizes H(CIS), i.e., that maximizes the mutual information I(C; 5'). Unfortunately, the practical applicability of the above solution to complex classification problems requiring a large number of features is limited because of two computational problems. First the number of samples and the amount of CPU time required for computing the MI become prohibitive when the dimensionality of the feature vector f is large. For example Fraser's method (see <ref type="bibr" target="#b1">[6]</ref>), that is a computationally efficient algorithm for calculating the MI, requires for its convergence a number of samples "in the millions" when the number of features in the input vector is larger than 3 or 4, clearly an exorbitant number for "real world" classifier development. Even assuming that a suitable example set can be constructed, the consideration of all possible subsets requires a number of runs equal to ( ;</p><p>) .</p><p>One is therefore forced to consider approximated solutions of the FRn-k problem. An approximated solution is acceptable also because there is no guarantee that the optimal subset of features will be processed in the optimal way by the learning algorithm and by the operating classifier. Although it is necessary, the availability of an "informative" input vector is not sufficient for the development of a correct classifier.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Our Algorithm (MIFS)</head><p>Motivated by the above reasons, we considered two approximations for the FRn-k problem. First the MI between vector variables is approximated using the MI between the individual components of the vectors. Instead of calculating the mutual information I ( F ; C) between a feature vector f and the class variable c we compute only I(f, C) and I(f, f') where f and f' are individual features. In this case the "computationally impossible" calculation of the exact MI is substituted with a series of feasible calculations. Then the analysis of all possible subsets is substituted by a "greedy" algorithm. Given a set of already selected features, the algorithm chooses the next feature as the one that maximizes the information about the class corrected by subtracting a quantity proportional to the average MI with the selected features. In order to be selected, a feature must be informative about the class without being predictable from the current set of features. For example, if two features f and f' are highly dependent, I ( f , f ' ) will be large and, after the better one is picked, the selection of the second one is penalized.</p><p>The MIFS algorithm ("mutual information based feature selection") can be described by the following procedure: by a quantity proportional to the total MI with respect to the already-selected features. In practice, we find that a value for , f 3 between 0.5 and 1 is appropriate for many classification tasks (these are the values used for the tests in Section IV).</p><p>At this point it is important to remark that, while the use of the MI between features and output class to rank the relevance of each isolated component is theoretically justified, the summation of the "two-point" MI'S to consider the dependencies between different features during the selection process is an heuristic approximation whose effectiveness must be tested in the field for the different classification problems.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Estimation of the MI from Samples</head><p>Because the MI is calculated by estimating the probability density from a finite number of samples, we must check that the errors caused by the estimation do not impair the above selection process. where n is the number of occurrences for the given interval).</p><p>Finally, let K f be the number of intervals for the f variable and K , the number of intervals for the class variable, i.e., the number of classes.</p><p>By adapting to our case the analysis of <ref type="bibr" target="#b10">[12]</ref>, the difference between the true value f and the estimation I of the mutual information can be approximated as follows: </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>* *</head><p>where the sums are over the discretization intervals and Sn are the fluctuations of the countings with respect to the mean</p><formula xml:id="formula_7">values (Sn = n -E).</formula><p>The approximation is valid up to the second order of the relative fluctuations and if the ratios E,../ZcZf do not change very much with c and f (see <ref type="bibr" target="#b10">[12]</ref> for the details). Now, because the typical fluctuation of the countings is of the order of the square root of the mean values, we can arrive at the following approximation:</p><formula xml:id="formula_8">1 (10) AI x 3 ( K c K f -K c -K f )</formula><p>Note that, in this approximation, the MI is overestimated (in practical cases K,Kf -K c -K f &gt; 0) and this overestimation depends only on the number of quantization levels. The fact that the MI is overestimated in the same way for the different variables limits the estimation effects on the relative ranking of the different features5, and therefore the effects on the MIFS algorithm. The number of quantization levels K f has to be appropriately chosen. If the statistical distributions have a lot of structure, using a small number of levels will cancel these details and reduce the estimated MI. But using too many levels K j will produce the estimation problems previously described.</p><p>In practice, we obtained good results by using K f = 10 levels and cutting the range of values into equal-sized intervals6.</p><p>In Table <ref type="table" target="#tab_0">I</ref> we present the results of an experiment for a twoclass discrimination problem. Patterns in two dimensions are generated with equal probability for the two classes, where one is described by a central Gaussian distribution and the other by two lateral Gaussians. With the notation that will be introduced in Section IV (see <ref type="bibr" target="#b16">(18)</ref>), the two densities are In this case N, = 2, N f = 10, K f = 10, and the approximation in (10) is acceptable (the differences A I are calculated with respect to the "true" value calculated for An algorithm for calculating the MI from samples that is based on an adaptive discretization (i.e., a variable size of the intervals so that a sufficient number of samples is contained in each of them) is presented in [6]. Fraser's algorithm is based on the invariance of the MI with respect to transformations acting on the individual coordinates and on a recursive sequence of partitions of the space of the variables. Each recursive call goes deeper in areas where the joint distribution has finer structure and is terminated when the number of samples in an element of the partition becomes insufficient for an accurate evaluation.</p><p>The computational complexity of the algorithm is N log N for a number of samples equal to N , and therefore it permits a fast evaluation also for large numbers of examples. Fraser's algorithm has been used in the Optical Character Recognition tests described in Section IV-E. For the reader's convenience, a short description of the algorithm is provided in the Appendix. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>IV. EXPERIMENTAL RESULTS</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A . Simple Test Cases</head><p>We show here the results for two test cases derived from <ref type="bibr">[lo]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Example I :</head><p>The first classification problem is illustrated in Fig. <ref type="figure" target="#fig_6">1</ref>. The feature vector ( X , Y ) is uniformly distributed in <ref type="bibr" target="#b7">[0,</ref><ref type="bibr">1]</ref> x [0,1], one pattern belongs to class "1" if the two inequalities z &lt; a and y &lt; / 3 (p = 1/(2a) ), to class "2" otherwise. By calculating the MI between each feature and the class, one obtains the following result:</p><formula xml:id="formula_9">I ( X ; C ) = 1 + a log ---log(2a -1) (1 1) ("2) ; I ( Y ; C ) is obtained from (1 1</formula><p>) by substituting a with p = 1/(2a). From (11) one derives that feature X is more informative than y as long as 1/2 5 Q &lt; 1/&amp; Because the better feature is selected before the learning process is started, the choice does not depend on the details of the learning algorithm (like the initial weight values and a proper convergence).</p><p>In this case the same choice of the most informative feature is obtained by using the Fisher linear discriminant vector. The Fisher linear discriminant is defined as that linear function y = w t z for which the criterion function is maximum, where 7ti; is the sample mean for the projected points (7ti; = (l/n;) Eyeclass, y) and . F; the scatter for the projected samples (5; = EyEclasst(yni;)' ). The task is that of maximizing the ratio of between-class to within-class scatter. The difference of the projected means has to be large relative to a measure of the standard deviation for each class. The solution (see [3]) is:</p><formula xml:id="formula_10">w = S&amp;Ll -mz) (<label>13</label></formula><formula xml:id="formula_11">)</formula><p>where m i is the d-dimansional sample mean for class i and SW is the sum of the two scatter matrices S; defined as follows si = ( E -m ; ) ( z -may <ref type="bibr" target="#b12">(14)</ref> ZEclass,</p><p>For the above classification problem the expected scatter matrices Si for N sample points are given by ( <ref type="formula">15</ref>) and ( <ref type="formula">16</ref>).</p><p>If we rate the "importance" of the ith feature according to the ith component of the Fisher vector, the more informative feature is z if the value of the parameter Q is between 1/2 and l / f i and y for larger values, as it was the case by using the MI. In Fig. <ref type="figure" target="#fig_6">1</ref> we compare the graphs of the magnitude of the z and y components of the normalized Fisher vector and of the value of the MI for the z and y coordinates. Note that the patterns are scattered in the same way along the X and Y coordinates, so that the Principal Component Analysis (whose result does not depend on a ) does not help in choosing the most appropriate feature. Example 2: This example (the "rule-plus-exception'' problem) is derived from <ref type="bibr" target="#b14">[16]</ref> and used in <ref type="bibr">[lo]</ref>. The classification problem on an input space with four binary variables is defined are more relevant can be detected from the beginning, thereby "pruning" the network before learning is started.</p><formula xml:id="formula_12">) (16) sz = ( &amp;(2a4 -(3Q -q3 + -1)(2 -a)3) -$33a -1 -2 a 2 ) &amp;(I -4 3 -4 4 3 + (1 -a)(4a -1)3) 0 _ _ _ _</formula><p>-_ _ _ </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Mixture of Gaussian Densities</head><p>In a mixture of Gaussian densities the samples are assumed to be generated by selecting a "prototype" ci with probability P ( q ) and then selecting a pattem z with a normal (Gaussian) probability P(z1c;). A general multivariate normal density in d dimensions can be written as:</p><p>where m is the mean vector (m = E [ z ] ) and C is the covariance matrix <ref type="figure">(C = E [ ( ~-m ) ( z -m ) ~]</ref> ) .</p><p>We now present the results of some two-class discrimination experiments, where each class is described by a simple mixture of Gaussian densities, showing the robustness of the MI criterion with respect to different distributions of the class densities.</p><p>Let's consider an input space with N j = 2 features and two categories, where the first one is described by a Gaussian distribution with zero mean that is progressively elongated along the z dimension in the different tests (by increasing 0 1 ~) . and the second one is a normal distribution with a fixed standard deviation that is displaced in the z direction with respect to the first one. After introducing the one dimensional distribution To simulate a real classification task, we extracted 1000 pattems with equal probability from the two distributions for each of a series of tests with increasing values of air. as illustrated in Fig. <ref type="figure" target="#fig_10">2</ref> (for the cases with 0 1 ~ from 0.1 to 0.8). From the classes' definition, it is apparent that the y component of the pattern is completely useless because the pattems are distributed in the same way for the two classes along the y coordinate, while the IC component is sufficient to determine the class with a low degree of error.</p><formula xml:id="formula_13">N ( v ,</formula><p>The approximated MI between the two input variables and the class (I(z; class) and I(y; class)) are listed in Table <ref type="table">11</ref>.</p><p>As expected, the MI is close to zero for the y variable and significant for the IC variable. In fact, it is close to 1 (the output uncertainty) if the two classes are well separated (olZ = O.l), it decreases when the first class "expands" and covers the second one, and increases again for large values of air. In this last case the probability that the z coordinate of a pattem belonging to class "two" falls in the region of class "one" becomes small and smaller. For comparison, the results of Fisher's linear discriminant analysis are listed in the second and third columns of Table <ref type="table">11</ref>. For large values of 0 1 ~ the magnitude of the components of the Fisher vector is not related to the discrimination capability of the two coordinates. For example, for alz = 3.2 the more informative feature appears to be the second one. This is due to the increasing spread of class "one" along the IC dimension: although the difference of the means for the two classes has a y component equal to zero (and therefore the criterion function <ref type="bibr" target="#b10">(12)</ref> is zero for a vector along the y direction), the difference estimated from the finite number of samples has a small (random) y component that is causing the misleading result. In addition, the linear discriminant analysis is not defined if the classes have the same mean and it encounters serious estimation problems for small values of the between-class scatter, measured by the difference between the means. If this is small with respect to the standard deviations of the classes the results will depend on random fluctuations. The above considerations can be extended to the ndimensional case (the two-dimensional case was chosen only for display purposes) and to the mixture of different distributions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C . Classification of Sonar Targets</head><p>The task is to train a network to discriminate between sonar retums bounced off a metal cylinder and those bounced off a roughly cylindrical rock. The data set has been used in <ref type="bibr" target="#b3">[8]</ref>, where a multilayer neural network is trained for the classification7. The purpose of the following tests is that of comparing the relative advantages of different techniques for dimensionality reduction. Our training and testing sets refer to the "aspect angle dependent" series of experiments in [8]: the 104 training and 104 testing patterns are selected to include all target aspect angle.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Mutual Information Diagram and Feature Selection:</head><p>The original sonar signal is filtered, Fourier transformed and a set of 60 features is extracted by integrating the spectrogram over sampling apertures with varying temporal offsets (to correspond to the slope of the FM chirp). . , . , . , . , . , . , . I . , . In Fig. <ref type="figure" target="#fig_19">3</ref> we show the mutual information diagram of the signal, i.e., the value of the MI between the different features and the output class. The MI diagram provides useful information to the developer of a classification system. In this case there are peaks in the MI for the region corresponding roughly to the "attack" and "decay" features of [8], although we did not investigate the possible correlations with human perceptual cues. It is also apparent that some features have a very low MI. The developer can use the MI diagram to diagnose the feature extraction phase, for example to eliminate some features that have a very low information content. A different type of diagnosis is provided by the MZfunction, of the mutual information between each feature and the other ones, as a function of a parameter describing the relative feature location (in this case the parameter is given by the relative times at which the different sampling apertures are positioned). The MZfunction can be compared to the more traditional correlation function, with the difference that the MZfunction measures a general dependence between variables, in comparison with a linear dependence. In addition, the MI function can be applied equally well to numerical and symbolic sequences, like the sequence of letters in a text (see <ref type="bibr" target="#b10">[12]</ref>).</p><p>In Fig. <ref type="figure" target="#fig_11">4</ref>(a) we show the MZfunction for one particular feature (feature 8) with respect to the other ones. One can identify a peak that is decaying for near features (this result is related to the temporal superposition of the different sampling windows and to the dependence between the characteristics of the signal at contiguous times) and a more complex structure for "distant" features. This behavior is qualitatively similar for the other features. In Fig. <ref type="figure" target="#fig_11">4</ref>(b) the MZfunction is shown in more detail for the first feature. Again the MI decays gradually for features corresponding to later times until a plateau with a complex structure is reached. The MZfunction can be used to identify relations between different features. If some features are highly dependent it is possible that some of them are redundant and can be eliminated.</p><p>It is interesting to compare the selection order given by the MIFS algorithm presented in Section I11 (with p = 1) and the ranking scheme based only on the values of the MI with respect to the output class (i.e., , f 3 = 0 ). In the first case, after the feature with the highest MI is selected, the choice tends to jump to distant places because the subsequent features are chosen by taking into account both the MI with respect to the class and the MI with the already-selected features. In the second case, all features in a peak of the MI diagram are picked before the other candidates are considered (see Fig. <ref type="figure" target="#fig_12">5</ref>, where each line specifies the selected features, with their number IC growing from the top to the bottom). For example, if four features are selected, in the first case all four come from the tallest peak and are extracted from a small time interval, in the second case they correspond to sampling apertures spread over the entire signal.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Learning and Generalization for Different Pruning Techniques:</head><p>We consider here the effect of different dimensionality reduction techniques on the performance of a multilayer perceptron neural network trained for the sonar classification problem. The training algorithm and parameters are the same as those used in [8], the network architecture has an input layer of variable size (corresponding to the dimension of the reduced pattem), one hidden layer with three units, and two output units coding for the two classes. In three series of experiments, the input vector is reduced to lo%, 20%, and 30% of its original size and, for each size, a set of 10 runs is executed (by varying the seed of a random number generator used for initializing the weights andin the case of a random pruningfor selecting the features). We then calculate the average performance and its standard deviation.</p><p>The training curves (percent classification as a function of the number of on-line pattem presentations) are similar to those of <ref type="bibr" target="#b3">[8]</ref>. In Fig. <ref type="figure" target="#fig_13">6</ref> we show an example of a learning stage (for the 10% cut) and the average on 10 tests.</p><p>The following results refer to the generalization performance of the networks (measured on the disjointed test set) as  a function of the number of iterations. Learning is executed for 120 000 on-line pattem presentations. The training period is increased with respect to [8] because the over-training phenomenon (i.e., a decrease in generalization performance because of an excessive training causing the "memorization" of the training set) is quite difficult to observe in this particular case, if it is present at all, and we wanted to be reasonably sure that the net reached the maximum generalization performance. The three methods that we compare are the MIFS algorithm in Section 111, the scheme based on the Principal Component Analysis (PCA) (see [18]) and, finally, a random dimensionality reduction (see Fig. <ref type="figure" target="#fig_15">7</ref>). The performance of the networks at the end of the training period is listed in Table <ref type="table">111</ref>. The performance of the original network (60 inputs) with the architecture 60-3-2 is 86.5% (standard deviation 3.0).</p><p>In this case the superiority of the MIFS technique emerges more clearly for significant reductions of the number of features (e.g., when they are reduced from 60 to 6-12), while the difference with respect to a random reduction tends to decrease for smaller reductions (although the standard deviation for the random reduction is larger). This is to be expected for this particular problem where there is a high degree of dependence between features extracted from near time intervals of the signals: if a large fraction of the original ' set of features is maintained and if these are "spread" over the entire signal duration, the information loss with respect to the amount contained in the original signal will be very small and will not depend on the selection method in a crucial manner.</p><p>The PCA method of <ref type="bibr" target="#b16">[18]</ref> is not to be confused with the use of the Karhunen-Loe've transformation in <ref type="bibr" target="#b12">[14]</ref>. In our case we are not considering feature transformations but only the selection of a subset of optimal features from a given vector.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D . The Iris Data</head><p>The data were listed and used by R. A. Fisher in his classic paper on discriminant analysis [4]. They are from measurements by E. Anderson on 150 samples of three species of iris8. The input pattem is composed of four features'.</p><p>In this case (given the limited number of features) we reduce the input vector by 50% and present the results for all possible (six) selections of two features. A multilayer perceptron with the architecture 2-4-3 is trained on a subset of 100 cases and tested on the remaining 50 cases. The learning rate for the online backpropagation algorithm is 0.002 (no momentum) and weights are randomly initialized in the range [-0.5,0.5]. The results are an average on ten runs.</p><p>In Fig. <ref type="figure">8</ref> we present the generalization results for all subsets of two features (indicated by a binary number, where "1" means that the corresponding feature is present). It is manifest that there are two optimal subsets ("1001" and "1010") with a correctness of approximately 93%, three suboptimal selections with performance in the region 80-90% and a bad selection ("1100") that reaches only 60%. The standard deviation is approximately 2.0 for the case "0011" and 1.0 for all other cases. The MIFS algorithm chooses one of the two optimal sets (precisely the set composed of features 1 and 3).  The difference in the amount of the mutual information between the set of features and the class, for two different cases (the best case "1010 and the worst case "1100") can be examined by the regions corresponding to the different classes are clear (apart from a limited contact zone) in the second case two of the three classes are almost overlapped.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E. Optical Character Recognition</head><p>The features for this problem are derived from a realworld task of handwritten digits recognition. The original images are normalized to fit a window of l6 (horizontallY) x 28 (vertically) pixels. The area is then divided into 4 x 7 nonoverlapping windows of size 4 x 4, and from each window a feature is extracted as the percent of black pixels in the given window. A set of 6496 patterns (equally distributed in the ten classes) is used for the training, a distinct set of 12 981 is used for testing the generalization performance. Here we are not interested in reaching the best accuracy (that demands The maximum generalization in this parameter setting is obtained for a number of pattern presentations equal to about 150000. For a number of presentations larger than 250000 there is a slight performance reduction caused by overlearning. Pattems are presented to a network with a single hidden layer of 28 units after extracting them randomly from the training set.</p><p>In the following tests we execute a total of 250000 iterations, check the generalization every 50000 and list the maximum obtained. In Table <ref type="table" target="#tab_3">IV</ref> we show the comparison of the MIFS technique with respect to the Principal Component Analysis and to a set of random selections. The position on the image plane of the windows corresponding to the selected features is shown in Fig. <ref type="figure" target="#fig_6">10</ref>. Note that the "better" features are preferably located in the top, central, and bottom part (see the cases with 3, 6, and 8 features). This corresponds approximately to the position of the most informative strokes in the image.</p><p>In the last column we show the probability that an accuracy greater than or equal to that of MIFS is obtained by using a random cut (in the assumption of a normal distribution of values). It is apparent that tens or hundreds of random cuts have to be tested (by training the network) before reaching (out of a total of (9:) = 40 116 600)) have to be tested to find a performance equivalent to or better than that of MIFS, with a probability greater than 0.5. The PCA method performs less than the average random cut in some cases (with 6 or 8 features), while it is close to the MIFS results for 14 and 20 features. Let us note that the presence of a p value larger than zero is crucial in order to obtain good results. If the mutual dependencies between features are not taken into account, selecting the features with the highest MI with respect to the output tends to produce a set of redundant features that leaves out useful "complementary" information.</p><p>To test the robustness of the MIFS algorithm with respect to different neural net models, we repeated the training and generalization tests with the same feature vectors used for the previous results but using the Learning Vector Quantization technique for training the classifier. The LVQ method is described in [ 111. In particular we used an optimized version of the method (OLVQl), that is part of a software package obtained from the Helsinky University of Technology'o. In this optimized version, an individual learning rate is assigned to each codebook vector" and properly adjusted during training.</p><p>A performance comparable (although inferior) to that of MLP was obtained with a total of 2000 codebook vectors. These vectors were appropriately initialized and balanced (see the package manual), before executing a total of 20000 iterations, an empirical number corresponding to the maximum generalization.</p><p>While the absolute results for this problem are better when using the MLP neural net (and a large number of codebook vectors has to be used to obtain a near performance), the MIFS technique remains superior in relative terms. from Information Theory for the supervised training of neural networks. In the machine learning literature the entropy and mutual information concepts are used for example in <ref type="bibr" target="#b15">[17]</ref> and <ref type="bibr" target="#b13">[15]</ref> to introduce relevant features for learning Boolean formulas with a tree representation. In this case and, in general, in complex recognition tasks one encounters many forms of the "curse of dimensionality" problem (see e.g., [3]): approaches that are suitable for a low pattern dimensionality may become unworkable for large dimension because of unrealistic needs of computation and data. Therefore it is crucial to reduce the input dimensionality of a classification problem either by eliminating features with low information content or high redundance with respect to other features or by constructing more powerful features in the preprocessing phase.</p><p>Our objective was less ambitious, because only the first of the above options was considered (leaving the second for the capabilities of the neural net to build complex features from simple ones). We assumed that a set of candidate features with globally sufficient information is available and that the problem is that of extracting from this set a suitable subset that is sufficient for the task, thereby reducing the processing times in the operational phase and, possibly, the training times and the cardinality of the example set needed for a good generalization.</p><p>In particular we were interested in the applicability of the mutual information measure. For this reason we considered the estimation of the MI from a finite set of samples, showing that the MI for different features is over-estimated in approximately the same way. This estimation is the building block of the MIFS algorithm, where the features are selected in a "greedy" manner, ranking them according to their MI with respect to the class discounted by a term that takes the mutual dependencies into account.</p><p>In the neural networks literature, concepts from Information Theory have been used both to construct learning algorithms and to analyze the functionality of the classifier (some examples from the literature have been cited in Section 11). The present approach is different from pruning methods acting during the learning phase (e.g., <ref type="bibr" target="#b14">[16]</ref>, [21]) because the dimensionality reduction is executed before learning starts. The main advantage is that irrelevant features are eliminated from the beginning and that a fast informative feedback about the relevance and dependencies of the different features is available to the developer of a neural net classifier. In addition, it is different from methods that use some form of entropy estimation during the learning phase (e.g., [I]), in that the usual backpropagation algorithm is used for learning. Although the availability of sufficient information does not guarantee the convergence of a neural net training algorithm to a satisfactory performance level, we presented some examples in different classification areas where the method is satisfactory.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>V. CONCLUDING REMARKS</head><p>The main motivation for this research was to investigate the practical applicability of the mutual information concept </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>PX(X)PY (Y)</head><p>If the various probability distributions are not known, they can be approximated by a piece-wise constant function by counting the number of events in rectangular boxes. For example, if a box in the X -Y plane of size Ax Ay contains NZy events, the probability density in the region can be estimated by Pzy(x,y) x N,,/NoAxAy, where NO is the total number of events. The box size at a given point must be large enough to contain a number of points that is sufficient for a robust estimation, but not too large, otherwise part of the structure in the mutual probability density function Pz,(z, y ) will be cancelled and the mutual information will be underestimated. In general, no single size is appropriate over the whole X -Y plane. Fraser's algorithm is based on an adaptive partition of the plane in which the size of each box is chosen according to the local situation.</p><p>Although the algorithm can be modified for a general case, for illustrative purposes it is easier to consider the case where the number of points NO is a power of 2, say No = 2n. Let us consider a sequence of partitions of the X -Y plane, such that each partition consists of 4m boxes &amp;(Km), obtained by dividing each axis into 2m equi-probable segments. Km is an index that uniquely identifies one of the 4m element. It is useful to organize the partition as a tree, so that when an element Rm(Km) is divided into four parts it generates four children see the illustration in Fig. </p><p>Let us introduce N K ~, equal to the number of events in the Rm(Km) element of the partition and N K ~~ (for j = 0,1,2,3) equal to the number of events in the four sectors of elment &amp;(K,) when this is subdivided by cutting both its 2 and y intervals into two equi-probable parts. After substituting the probability densities with their piece-wise constant approximations and remembering that, because of the subdivision procedure, P, (Rm (Km)) = Py (R, <ref type="bibr">(</ref> </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head></head><label></label><figDesc>with the Dipartimento di Matematica, Universith di Trento, IEEE Log Number 9205932. algorithm with a low computational complexity and with limited requirements on time and on the number of training examples (Section 111). Finally we present some experimental 38050 Povo (Trento), Italy. 1045-9227/94$04.00 0 1994 IEEE results of our algorithm for a series of classification problems (Section IV).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head></head><label></label><figDesc>of the ith class to an output activation vector with value 1 in the ith place, and 0 otherwise. Continuity of the mapping can be obtained by a thin transition region on the boundaries between different classes. 41n fact, the probability P(f = (f1, f2). y) is different from zero only when Y = X O R ( f 1 , f z ) . In this cases P((fl,f~),XOR(fi,fz)) = 1/4 and the argument of the logarithm is 2 = (1/4)/(1/4 1/2).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>(</head><label></label><figDesc>Initialization) Set F c "initial set of n features;" S t "empty set." (Computation of the MI with the output class) for each feature f E F compute I(C; f ) . (Choice of the first feature) find the feature f that maximizes I ( C ; f ) ; set F c F\{f}; set S +-{f} 4) (Greedy selection) repeat until IS1 = IC: a) (Computation of the MI between variables) for all couples of variables (f,s) with f E F, s E S compute I ( f ; s), if it is not already available. b) (Selection of the next feature) choose feature f as the one that maximizes I(C; f ) -PEsEs I(f; s); set F + F \ {f}; set S c Su{f} 5) Output the set S containing the selected features. The parameter / 3 regulates the relative importance of the MI between the candidate feature and the already-selected features with respect to the MI with the output class. If , 8 is zero, only the MI with the output class is considered for each feature selection. If P increases, this measure is discounted</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head></head><label></label><figDesc>Let's assume that we have a number N of examples in the training set and that the probability densities P(c). P ( f ) and P(c, f ) are approximated by histograms, i.e., by counting the number of cases with values of the variables belonging to a set of intervals (Pc = n c / N , Pf = n f / N , Pcf = n c f / N ,</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>n</head><label></label><figDesc>= 10000). 5Let us suppose that features a and b have "true" mutual informations with respect to the output I, &gt; I b , in the approximation of equation 10 we will still have (I, + AZ) &gt; (Zb + AI) for the estimated quantities. 61f the distribution for the values of one variable is not known a priori, we calculate its mean p and standard deviation U , and cut the interval [p -20, p + 2u] into Kf equal segments. The rare points falling outside are assigned to the extreme left (or right) segment when histograms are calculated.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Fig. 1 .</head><label>1</label><figDesc>Fig. 1. Comparison of mutual information and Fisher's linear discriminant analysis for feature selection. The two-class discrimination problem is illustrated at the top. The components of the normalized Fisher vector and the MI below. Both methods select T as the more informative feature if a is less than 1/&amp;, y in the other case.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head></head><label></label><figDesc>by the Boolean function AB + A B C D. The output class is "true" when the "rule" A B is true or when the "exception" A B C D occurs. Clearly the "rule" is more important than the "exception" because it accounts for 15 out of 16 correct decisions and therefore the relevant variables are A and B. This is confirmed by calculating the mutual information. One obtains I ( A ; OUT) = I ( B ; OUT) = 0.124, I(C; OUT) = I ( D ; OUT) = 0.013. Again the fact that variables A and B</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head></head><label></label><figDesc>p , a ) = ( 2 ~a ~) -~/ ~e x p (U -PI2 (18) that is a Gaussian with mean p and variance u2, the probability densities for classes 1 and 2 are P l b , Y) = N ( z , 0, alr)N(Y, 070.1); p 2 ( z , Y) = ~( z , 0.5, o . ~) N (</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>'</head><label></label><figDesc>The data set was obtained from the "neural net benchmark collection" organized by Scott E. Fahlman at the Camegie Mellon University. It was contributed by Terry Sejnowski, now at the Salk Institute and the University of Califomia at San Diego, who developed it in collaboration with R. Paul Gorman of Allied-Signal Aerospace Technology Center.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head>Fig. 2 .</head><label>2</label><figDesc>Fig. 2. Discrimination task with Gaussian densities. The standard deviation along the z axis of the distribution for class 1 is increasing from the top (ulZ = 0.1) to the bottom (u12 = 0.8).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_11"><head>Fig. 4 .</head><label>4</label><figDesc>Fig. 3. signal.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_12"><head>Fig. 5 .</head><label>5</label><figDesc>Fig. 5. Selection order by the MIFS algorithm with p = 0 (left) and p = 1 (right). The number IC of selected features increases from the top ( E = 1) to the bottom ( E = 60). The line for a given E shows the chosen features (with "#").</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_13"><head>Fig. 6 .</head><label>6</label><figDesc>Fig. 6. Training curves in the sonar problem, for the original architecture (60-3-2) and for the reduced net (6-3-2). Single run (above) and average of ten (below).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_14"><head></head><label></label><figDesc>8The data were obtained from Russel Leighton at MITRE Signal Processing Center. They are in the examples that come with the "AspirinMIGRANES" neural network simulator made available free from the MITRE Corporation.'The sepal length, sepal width, petal length, and petal width were measured on 50 iris specimens from each of three species, Iris setosa, Iris versicolor, and Iris virginica.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_15"><head>Fig. 7 .</head><label>7</label><figDesc>Fig. 7. Generalization curves for the sonar problem, for three values of the dimension of the reduced input vector (6, 12, 18). In each case the selection methods MJFS, PCA and random are compared.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_16"><head>Fig. 9 .Fig. 8 .Fig. 9 .</head><label>989</label><figDesc>Fig. 9. While in the first</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_17"><head>Fig. 11 .</head><label>11</label><figDesc>Fig. 11. Recursive partitioning of the X-Y plane executed by Fraser's algorithm. If substructure is found, an element is subdivided into four subelements.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_18"><head>&amp;+1</head><label></label><figDesc>11.The approximation of the mutual information corresponding to the mth recursive step, with a partition consisting of 4m</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_19"><head>3 F</head><label>3</label><figDesc>K,)) = 4-m, the above expression becomes: log,(No) When a single element &amp;(ITm) of the partition is subdivided into four sectors, its contribution to the mutual information changes from: (NKm log~(NKm) + NKm m logz(4)) to: 3 x ( N K m j lo&amp;(NKmj) + N K m j (m -t 1) l O d 4 ) ) m j logp(NKmj)) + NKm(m + 1) log2(4) j=O where the fact that C,"=, N K ~~ = N K ~ has been used. Starting from (21), and using the above result, it is immediate to check that the mutual information can be estimated by the following formula, that uses the recursive function F ( ) introduced in [6]: where the function F ( ) takes a partition element as argument and returns a real value (a floating point number). If the element has no substructure: where N K ~ is the number of events contained in the element, otherwise the function calls itself four times in a recursive way, and retums: (Rm(Km)) = NKm 10&amp;(4) + x F ( R m + I ( K m ) j ) ) j=O The x-square test is used to check for substructure. Let us introduce the following variables, that count the number of</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>TABLE I OVERESTIMATION OF THE MI AND COMPARISON WITH THE ESTIMATED ERROR AZ = 4/N z(x;ciass) AI z(y;class) AI AI = 4 / ~</head><label>I</label><figDesc></figDesc><table><row><cell>10</cell><cell>0.800</cell><cell cols="2">0.468 0.315</cell><cell cols="2">0.182 0.4</cell></row><row><cell>100</cell><cell>0.513</cell><cell>0.045</cell><cell>0.183</cell><cell>0.050</cell><cell>0.04</cell></row><row><cell>lo00</cell><cell>0.491</cell><cell>0.023</cell><cell>0.147</cell><cell cols="2">0.014 0.004</cell></row><row><cell>loo00</cell><cell>0.468</cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note>* 0.133</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>TABLE II COMPONENTS OF NORMALEEDRSHER VECroR AND MUTUAL INFORMATION Cl2 WZ w, I(z; class) I( y; class)</head><label>II</label><figDesc></figDesc><table><row><cell>0.1</cell><cell>0.999</cell><cell>-0.032</cell><cell>0.964</cell><cell>0.001</cell></row><row><cell>0.2</cell><cell>0.999</cell><cell>-0.028</cell><cell>0.792</cell><cell>0.001</cell></row><row><cell>0.4</cell><cell>0.999</cell><cell>-0.012</cell><cell>0.539</cell><cell>0.001</cell></row><row><cell>0.8</cell><cell>0.998</cell><cell>0.048</cell><cell>0.612</cell><cell>0.001</cell></row><row><cell>1.6</cell><cell>0.952</cell><cell>0.304</cell><cell>0.692</cell><cell>0.001</cell></row><row><cell>3.2</cell><cell>0.489</cell><cell>0.87 1</cell><cell>0.679</cell><cell>0.001</cell></row><row><cell>6.4</cell><cell>0.049</cell><cell>0.998</cell><cell>0.728</cell><cell>0.001</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>TABLE III COMPARISON OF FEATURE SELEC~ON TECHNIQUES FOR THE SONAR PROBLEM number of features method performance on test set standard deviation</head><label>III</label><figDesc></figDesc><table><row><cell>6</cell><cell>MIFS</cell><cell>75.1</cell><cell>4.1</cell></row><row><cell></cell><cell>random</cell><cell>68.1</cell><cell>4.1</cell></row><row><cell></cell><cell>FTA</cell><cell>63.2</cell><cell>4.0</cell></row><row><cell>12</cell><cell>MIFS</cell><cell>78.9</cell><cell>3.0</cell></row><row><cell></cell><cell>random</cell><cell>76.9</cell><cell>4.1</cell></row><row><cell></cell><cell>PCA</cell><cell>63.7</cell><cell>3.2</cell></row><row><cell>18</cell><cell>MIFS</cell><cell>79.2</cell><cell>1.3</cell></row><row><cell></cell><cell>random</cell><cell>78.5</cell><cell>5.1</cell></row><row><cell></cell><cell>PCA</cell><cell>72.7</cell><cell>3.1</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>TABLE IV COMPARISON</head><label>IV</label><figDesc>OF FEATURE SELurrrON TECHNIQU!SS FOR THE OPnCAL CHARACTER RECOGNITION PROBLEM WlTH BACKPROPAGATION TRAINING. THE 10 TESTS WlTH DrPFERENT SEEDS FOR THE RANDOM NUMBER GENERATOR DATA FOR THE RANDOM CUT ARE THE AVERAGE AND STANDARD DEVIATION OF Feature selection by the MIFS algorithm for the OCR problem. The different figures correspond to increasing numbers of features extracted from 28 windows on the image plane. Latest added features are gray, previously added are black.</figDesc><table><row><cell>number of</cell><cell>MIFS</cell><cell>h4IFS</cell><cell>pcA</cell><cell>random ave.</cell><cell>prob. &gt;</cell></row><row><cell>features</cell><cell>pd.0</cell><cell>p d . 5</cell><cell></cell><cell>(st.dev.)</cell><cell>MIFf</cell></row><row><cell>3</cell><cell>39.4</cell><cell>39.6</cell><cell>40.6</cell><cell>38.03 (3.21)</cell><cell>0.312</cell></row><row><cell>6</cell><cell>55.9</cell><cell>66.0</cell><cell>56.2</cell><cell>59.78 (3.96)</cell><cell>0.058</cell></row><row><cell>8</cell><cell>61.4</cell><cell>73.9</cell><cell>64.9</cell><cell>68.43 (4.13)</cell><cell>0.092</cell></row><row><cell>11</cell><cell>74.0</cell><cell>83.2</cell><cell>79.0</cell><cell>78.33 (3.95)</cell><cell>0.108</cell></row><row><cell>14</cell><cell>82.5</cell><cell>88.3</cell><cell>87.4</cell><cell>83.19 (2.05)</cell><cell>0.006</cell></row><row><cell>20</cell><cell>90.0</cell><cell>91.9</cell><cell>91.7</cell><cell>90.73 (0.70)</cell><cell>0.047</cell></row><row><cell></cell><cell>3</cell><cell>6</cell><cell></cell><cell>8</cell><cell></cell></row><row><cell></cell><cell>11</cell><cell>14</cell><cell></cell><cell>M</cell><cell></cell></row><row><cell cols="6">Fig. 10. the use of rejection schemes, where the uncertain patterns</cell></row><row><cell cols="6">are discarded) but in comparing different feature selection</cell></row><row><cell cols="6">techniques when the classification of all patterns is required.</cell></row><row><cell cols="6">The generalization performance of networks trained with on-</cell></row><row><cell cols="6">line backpropagation (learning rate=l.2, momentum=O.O) is</cell></row><row><cell>94.7%.</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>TABLE V COMPARISON OF FEATURE SELECTION TECHNIQUES FOR THE OFTICAL CHARACTER RECOGNlTION PROBLEM WITH LEARNINO VECTOR QUANTIZATION TRAINING number of features MIFS (+OS) + olvql</head><label>V</label><figDesc></figDesc><table><row><cell></cell><cell></cell><cell>random + olvql</cell></row><row><cell>3</cell><cell>32.1</cell><cell>32.29 (4.23)</cell></row><row><cell>6</cell><cell>62.9</cell><cell>55.66 (3.17)</cell></row><row><cell>8</cell><cell>70.9</cell><cell>61.97 (4.32)</cell></row><row><cell>11</cell><cell>81.3</cell><cell>75.53 (3.55)</cell></row><row><cell>14</cell><cell>86.7</cell><cell>80.62 (1.46)</cell></row><row><cell>20</cell><cell>91.3</cell><cell>90.02 (0.69)</cell></row><row><cell cols="3">a comparable result. Because the feature selection time of</cell></row><row><cell cols="3">MIFS is negligible with respect to the training time, this</cell></row><row><cell cols="3">amounts to a sizable reduction in CPU resources to obtain a</cell></row><row><cell cols="3">given performance. For the case of 14 features, in the normal</cell></row><row><cell cols="3">distribution hypothesis, approximately 1 15 random selections</cell></row></table></figure>
		</body>
		<back>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>N = "n(Km))</head><p>ai = N(&amp;n+l(K")) bij = " n + 2 ( K n , i, j))</p><p>The null hypothesis that pz,(x, y) is flat Over L ( K m ) is disproved if at least one of the following inequalities fails (reduced x-square statistics and 20% confidence levels):</p><p>To simplify the counting operations needed by the algorithm a change of variables is executed that maps the arrays of events xi and yj into the integers in the [0,2n -11 interval (n = log, NO). The arrays are sorted into ascending numerical order and a value x; is mapped into its position in the sorted array. The same procedure is applied to yi.</p><p>In our implementation the sorting is executed by the "heapsort" algorithm (see for example <ref type="bibr" target="#b20">[22]</ref>) because its computational complexity is guaranteed to be of order NO log NO not only in the average but also in the worst case. Besides, no additional storage is required (the sorting is done "in place"). The "quicksort" algorithm used in [6] has a worstcase complexity of N,2 operations, that may be excessive for some computations, although in the average it requires order NO log NO operations (note that the average case may not be that encountered in practical cases).</p><p>Because of the tree structure, at most order NologNo recursive calls are executed and therefore the total running time is guaranteed to be of order NO log NO operations. The slow growth with respect to NO make this algorithm an efficient one even for very large number of events. On a current Unix workstation the actual computing time is about one second for NO equal to 8192.</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">in the initial element and in the elements of the first and second subdivision</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Independent coordinates for strange attractors from mutual information</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">M</forename><surname>Fraser</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">L</forename><surname>Swinney</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Physical Review A</title>
		<imprint>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="1134" to="1140" />
			<date type="published" when="1986">1986</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">On the approximate realization of continuous mappings by neural networks</title>
		<author>
			<persName><forename type="first">K</forename><surname>Funahashi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural Networks</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="183" to="192" />
			<date type="published" when="1989">1989</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Analysis of hidden units in a layered network trained to classify sonar targets</title>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">P</forename><surname>Gorman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">J</forename><surname>Sejnowski</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural Networks</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="75" to="89" />
			<date type="published" when="1988">1988</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">On the practical implication of mutual information for statistical decisionmaking</title>
		<author>
			<persName><forename type="first">F</forename><surname>Kanaya</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Nakagawa</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Information Theory</title>
		<imprint>
			<biblScope unit="volume">37</biblScope>
			<biblScope unit="page" from="1151" to="1156" />
			<date type="published" when="1991">1991</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">A simple procedure for pruning back-propagation trained neural networks</title>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">D</forename><surname>Kamin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Neural Networks</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="239" to="242" />
			<date type="published" when="1990">1990</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Training stochastic model recognition algorithms as networks can lead to maximum mutual information estimation of parameters</title>
		<author>
			<persName><forename type="first">M</forename><surname>Bichsel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Seitz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
				<editor>
			<persName><forename type="first">D</forename><forename type="middle">S</forename><surname>Touretzky</surname></persName>
		</editor>
		<meeting><address><addrLine>San Mateo, C A Morgan Kaufmann</addrLine></address></meeting>
		<imprint>
			<date type="published" when="1989">1989. 1990</date>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="133" to="141" />
		</imprint>
	</monogr>
	<note>Minimum class entropy: A maximum information approach to layered networks</note>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">E</forename><surname>Duda</surname></persName>
		</author>
		<author>
			<persName><surname>Hart</surname></persName>
		</author>
		<title level="m">Pattern Classification and Scene Analysis</title>
				<meeting><address><addrLine>New York Wiley</addrLine></address></meeting>
		<imprint>
			<date type="published" when="1973">1973</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">The use of multiple measurements in taxonomic problems</title>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">A</forename><surname>Fisher</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Annals of Eugenics</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="179" to="188" />
			<date type="published" when="1936">1936</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Reconstructiong attractors form scalar time series: a comparison of singular system and redundancy criteria</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">M</forename><surname>Fraser</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Kohonen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE</title>
				<meeting>IEEE</meeting>
		<imprint>
			<date type="published" when="1989">1989. 1990</date>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="page" from="1464" to="1480" />
		</imprint>
	</monogr>
	<note>The self-organizing map</note>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Mutual information functions versus correlation functions</title>
		<author>
			<persName><forename type="first">W</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Star. Phys</title>
		<imprint>
			<biblScope unit="volume">60</biblScope>
			<biblScope unit="issue">516</biblScope>
			<biblScope unit="page" from="823" to="837" />
			<date type="published" when="1990">1990</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">How to generate ordered maps by maximizing the mutual information between input and output signals</title>
		<author>
			<persName><forename type="first">R</forename><surname>Linsker</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural Computation</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="402" to="411" />
			<date type="published" when="1989">1989</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Using the Karhunen-Loe&apos;ve transformation in the back-propagation training algorithm</title>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">A</forename><surname>Malki</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Moghaddamjoo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Neural Networks</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="162" to="165" />
			<date type="published" when="1990">1990</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Learning Boolean formulae using decision trees</title>
		<author>
			<persName><forename type="first">A</forename><surname>Mar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Etti-Spaccamela</surname></persName>
		</author>
		<author>
			<persName><surname>Protasi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. 3rd Workshop on Parallel Architectures and Neural Networks</title>
				<meeting>3rd Workshop on Parallel Architectures and Neural Networks<address><addrLine>Vietri, Italy</addrLine></address></meeting>
		<imprint>
			<date type="published" when="1990">1990</date>
			<biblScope unit="page" from="797" to="802" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Skeletonization: A technique for trimming the fat from a neural network via relevance assessment</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">C</forename><surname>Mozer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Smolensky</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
				<editor>
			<persName><forename type="first">D</forename><forename type="middle">S</forename><surname>Touretzky</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Ed</forename><forename type="middle">San</forename><surname>Mateo</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">C</forename></persName>
		</editor>
		<editor>
			<persName><forename type="first">Morgan</forename><surname>Kaufmann</surname></persName>
		</editor>
		<imprint>
			<date type="published" when="1989">1989</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">15</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Boolean feature discovery in empirical learning</title>
		<author>
			<persName><forename type="first">G</forename><surname>Pagallo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Haussler</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Machine Learning</title>
				<imprint>
			<date type="published" when="1990">1990</date>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page" from="71" to="99" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Determining the relevant parameters for the classification on a multi-layer perceptron: Application to radar data</title>
		<author>
			<persName><forename type="first">E</forename><surname>Pemot</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Vallet</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. I991 Int. Conf. Artificial Neural Networks ICANN-91</title>
				<meeting>I991 Int. Conf. Artificial Neural Networks ICANN-91<address><addrLine>Espoo, Finland</addrLine></address></meeting>
		<imprint>
			<date type="published" when="1991-06">June 1991</date>
			<biblScope unit="page" from="797" to="802" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Learning intemal representations by error propagation</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">E</forename><surname>Rumelhart</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">J</forename><surname>Williams</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Parallel Distributed Processing: Explorations in the Microstructure of Cognition</title>
				<editor>
			<persName><forename type="first">D</forename><forename type="middle">E</forename><surname>Rumelhart</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">J</forename><forename type="middle">L</forename><surname>Mcclelland</surname></persName>
		</editor>
		<meeting><address><addrLine>Cambridge, MA</addrLine></address></meeting>
		<imprint>
			<publisher>Bradford Books</publisher>
			<date type="published" when="1987">1987</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="318" to="362" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">The Mathematical Theory of Communication</title>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">E</forename><surname>Shannon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Weaver</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1949">1949</date>
			<publisher>University of Illinois Press</publisher>
			<pubPlace>Urbana, IL</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Generalization by weight-elimination with application to forecasting</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">S</forename><surname>Weigend</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">E</forename><surname>Rumelhart</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">A</forename><surname>Huberman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
				<editor>
			<persName><forename type="first">R</forename><forename type="middle">P</forename><surname>Lippmann</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">J</forename><surname>Moody</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">D</forename><forename type="middle">S</forename><surname>Towtzky</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Ed</forename><forename type="middle">San</forename><surname>Mateo</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">C</forename></persName>
		</editor>
		<editor>
			<persName><forename type="first">Morgan</forename><surname>Kaufmann</surname></persName>
		</editor>
		<imprint>
			<date type="published" when="1991">1991</date>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page" from="875" to="882" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">A</forename><surname>Teukolsky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">H</forename><surname>Press</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">P</forename><surname>Flannery</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">T</forename><surname>Wetterling</surname></persName>
		</author>
		<title level="m">Numerical Recipes in C</title>
				<meeting><address><addrLine>Cambridge, MA</addrLine></address></meeting>
		<imprint>
			<publisher>Cambridge University Press</publisher>
			<date type="published" when="1988">1988</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Discovering view-point invariant relationships that characterize objects</title>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">S</forename><surname>Zemel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neurul Information Processing Systems</title>
				<editor>
			<persName><forename type="first">R</forename><forename type="middle">P</forename><surname>Lippmann</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">J</forename><surname>Moody</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">D</forename><forename type="middle">S</forename><surname>Touretzky</surname></persName>
		</editor>
		<meeting><address><addrLine>San Mateo, CA</addrLine></address></meeting>
		<imprint>
			<publisher>Morgan Kaufmann</publisher>
			<date type="published" when="1991">1991</date>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page" from="299" to="305" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
