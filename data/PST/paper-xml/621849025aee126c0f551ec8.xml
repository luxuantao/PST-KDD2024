<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">M2I: From Factored Marginal Trajectory Prediction to Interactive Prediction</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date type="published" when="2022-02-24">24 Feb 2022</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Qiao</forename><surname>Sun</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Denotes equal contribution. ? IIIS</orgName>
								<orgName type="institution">Tsinghua University</orgName>
							</affiliation>
						</author>
						<author role="corresp">
							<persName><forename type="first">Xin</forename><surname>Huang</surname></persName>
							<email>xhuang@csail.mit.edu</email>
							<affiliation key="aff0">
								<orgName type="department">Denotes equal contribution. ? IIIS</orgName>
								<orgName type="institution">Tsinghua University</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Junru</forename><surname>Gu</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Brian</forename><surname>Williams</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Hang</forename><surname>Zhao</surname></persName>
						</author>
						<author>
							<affiliation key="aff1">
								<orgName type="laboratory">Marginal Samples Unrealistic Influencer Prediction Reactor Prediction Relation Prediction Influencer Reactor Colliding Joint Prediction using Traditional Marginal Predictors Joint Prediction using M2I</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">M2I: From Factored Marginal Trajectory Prediction to Interactive Prediction</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2022-02-24">24 Feb 2022</date>
						</imprint>
					</monogr>
					<idno type="arXiv">arXiv:2202.11884v1[cs.RO]</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-01-03T08:28+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Predicting future motions of road participants is an important task for driving autonomously in urban scenes. Existing models excel at predicting marginal trajectories for single agents, yet it remains an open question to jointly predict scene compliant trajectories over multiple agents. The challenge is due to exponentially increasing prediction space as a function of the number of agents. In this work, we exploit the underlying relations between interacting agents and decouple the joint prediction problem into marginal prediction problems. Our proposed approach M2I first classifies interacting agents as pairs of influencers and reactors, and then leverages a marginal prediction model and a conditional prediction model to predict trajectories for the influencers and reactors, respectively. The predictions from interacting agents are combined and selected according to their joint likelihoods. Experiments show that our simple but effective approach achieves state-of-the-art performance on the Waymo Open Motion Dataset interactive prediction benchmark.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Trajectory prediction is widely used by intelligent driving systems to infer future motions of nearby agents and identify risky scenarios to enable safe driving. Recent advances have shown great success in predicting accurate trajectories by learning from real-world driving examples. Many existing trajectory prediction works <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b11">12,</ref><ref type="bibr" target="#b14">15,</ref><ref type="bibr" target="#b22">23,</ref><ref type="bibr" target="#b24">25,</ref><ref type="bibr" target="#b33">34]</ref> focus on generating marginal prediction samples of future trajectories over individual agents, failing to reason about their interactions in the future. As a result, the prediction samples over multiple agents may overlap with each other and result in sub-optimal performance. We present a motivating example in Fig. <ref type="figure">1</ref>, in which a marginal predictor produces a set of prediction samples separately for two interacting agents, as visualized in the top left figure. While the predictions for each agent are reasonable without considering the presence of the other, some Figure <ref type="figure">1</ref>. A motivating example of M2I. Top: Traditional marginal predictor often produces scene inconsistent trajectory predictions that collide with each other. Even for non-colliding predictions, it ignores the potential interaction between agent futures and may predict unrealistic behaviors. Bottom: Our proposed approach M2I predicts scene compliant trajectories by first identifying an influencer reactor pair in the scene. It then predicts marginal trajectories for the influencer and reactive trajectories for the reactor. trajectory pair will collide when considering them jointly. For instance, it is unlikely that the red agent turns left while the blue agent goes forward, as indicated in the top middle example in Fig. <ref type="figure">1</ref>. Therefore, it is necessary to predict scene compliant trajectories with the existence of multiple agents to support better prediction accuracy.</p><p>To generate scene compliant trajectories, one can learn a joint predictor to predict trajectories in a joint space over multiple agents; however, it suffers from an exponentially increasing prediction space as the number of agents increases. Taking the popular goal-conditioned model <ref type="bibr" target="#b14">[15]</ref> as an example, while it is feasible to predict a set of goals for a single agent, the goal space increases exponentially with the number of agents and becomes unmanageable for even two agents with a few hundred goal candidates for each agent. A more computationally efficient alternative to producing scene compliant trajectories is to post-process marginal prediction samples by pruning colliding ones; however, such an ad-hoc approach fails to take into account potential agent interactions in the future and may ignore other conflicts which are hard to prune by heuristics. For instance, although the prediction sample in the top right figure in Fig. <ref type="figure">1</ref> is collision-free, the red agent may slow down when turning left to keep a safe distance from the blue agent. Such an interactive behavior is hard to be captured by a marginal predictor as it is unaware of the future behavior of the other agents in the scene.</p><p>In this paper, we propose M2I that leverages marginal and conditional trajectory predictors to efficiently predict scene compliant multi-agent trajectories, by approximating the joint distribution as a product of a marginal distribution and a conditional distribution. The factorization assumes two types of agents: the influencer that behaves independently without considering the other agents, and the reactor that reacts to the behavior of the influencer. This assumption is inspired by the recent study on the underlying correlations between interactive agent trajectories <ref type="bibr" target="#b34">[35]</ref>. Under the assumption, we leverage a standard marginal predictor to generate prediction samples for the influencer, and a conditional predictor to roll out future trajectories for the reactor conditioned on the future trajectory of the influencer. The advantage of our proposed approach M2I is illustrated in the bottom figures in Fig. <ref type="figure">1</ref>, in which we first predict the relations of the interactive agents. Given the relations, we predict the future trajectories of the influencer and then predict reactive behaviors of the reactor conditioned on each influencer prediction. As causality in driving interaction remains an open problem <ref type="bibr" target="#b34">[35]</ref>, we pre-label the influencerreactor relation based on a heuristic, and propose a relation predictor to classify interactive relations at inference time.</p><p>Our contributions are three-fold. First, we propose a simple but effective framework M2I that leverages marginal and conditional predictors to generate accurate and scene compliant multi-agent trajectories. The framework does not assume a specific predictor structure, allowing it to be adopted by a wide range of backbone prediction models. Second, in order to decouple the prediction space, we propose a relation predictor that helps identify high-level relations among interactive agents. Third, we demonstrate our framework through a goal-conditioned prediction model, and experiments show that it achieves state-of-the-art performance on the Waymo Open Motion Dataset interactive prediction benchmark.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Work</head><p>Trajectory prediction for traffic agents has been studied extensively in recent years. Due to uncertainty in human intent, the future trajectories are probabilistic and multimodal. To handle the multi-modality problem, <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b30">31]</ref> propose models that output behavior predictions as Gaussian mixture models (GMMs), in which each mixture component represents a single modality. Instead of parameterizing the prediction distribution, generative models, such as generative adversarial models (GANs) <ref type="bibr" target="#b15">[16,</ref><ref type="bibr" target="#b17">18,</ref><ref type="bibr" target="#b42">43]</ref> and (conditional) variational autoencoders (VAEs) <ref type="bibr" target="#b22">[23,</ref><ref type="bibr" target="#b25">26,</ref><ref type="bibr" target="#b30">31,</ref><ref type="bibr" target="#b39">40]</ref>, produce trajectory samples to approximate the distribution space. These generative models suffer from sample ineffi-ciency and require many samples to cover diverse driving scenarios <ref type="bibr" target="#b17">[18]</ref>.</p><p>More recently, a family of models are proposed to improve prediction accuracy and coverage by first predicting high-level intentions, such as goal targets <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b12">13,</ref><ref type="bibr" target="#b14">15,</ref><ref type="bibr" target="#b25">26,</ref><ref type="bibr" target="#b29">30,</ref><ref type="bibr" target="#b41">42]</ref>, lanes to follow <ref type="bibr" target="#b19">[20,</ref><ref type="bibr" target="#b32">33]</ref>, and maneuver actions <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b8">9]</ref>, before predicting low-level trajectories conditioning on each intention. Such models demonstrate great success in predicting accurate trajectories for single agents in popular trajectory prediction benchmarks, such as Argoverse <ref type="bibr" target="#b5">[6]</ref> and Waymo Open Motion Dataset <ref type="bibr" target="#b9">[10]</ref>. While our proposed approach M2I can use an arbitrary prediction model, we choose to adopt an anchor-free goal-based predictor <ref type="bibr" target="#b14">[15]</ref> because of its outstanding performance.</p><p>In the rest of the section, we introduce the literature closely related to our approach, on interactive trajectory prediction and conditional trajectory prediction.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.">Interactive Trajectory Prediction</head><p>Predicting scene compliant trajectories for multiple agents remains an open question due to its computational complexity. Early work leverages hand-crafted interaction models, such as social forces <ref type="bibr" target="#b16">[17]</ref> and energy functions <ref type="bibr" target="#b38">[39]</ref>. These hand-crafted functions require manual tuning and have difficulties modeling highly complicated and nonlinear interactions. In contrast, learning-based methods achieve better accuracy by learning interactions from realistic driving data: <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b15">16]</ref> utilize social pooling mechanisms to capture social influences from neighbor agents to predict interactive pedestrian trajectories in crowded scenes; <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b3">4,</ref><ref type="bibr" target="#b30">31]</ref> build a graph neural network (GNN) to learn the agent-toagent pairwise interactions; <ref type="bibr" target="#b23">[24,</ref><ref type="bibr" target="#b27">28,</ref><ref type="bibr" target="#b28">29,</ref><ref type="bibr" target="#b33">34]</ref> leverage attention and transformer mechanisms to learn multi-agent interaction behaviors. In this work, we focus on predicting scene compliant trajectories through marginal models to afford better computational efficiency.</p><p>Existing marginal prediction work produces scene compliant trajectories by leveraging an auxiliary collision loss <ref type="bibr" target="#b23">[24]</ref> or a critic based on an inverse reinforcement learning framework <ref type="bibr" target="#b35">[36]</ref> to discourage colliding trajectories. On the other hand, we focus on identifying agent relations explicitly as influencers and reactors in a scene to generate scene compliant predictions. Our work is relevant to <ref type="bibr" target="#b20">[21,</ref><ref type="bibr" target="#b21">22]</ref> that predicts interacting types before predicting scene consistent trajectories, but we further exploit the structure of the decoupled relations and the influence of low-level influencer trajectories as opposed to only using the high-level labels as the input to the trajectory predictor.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.">Conditional Trajectory Prediction</head><p>Conditional prediction approaches study the correlations between future agent trajectories, by predicting trajectories conditioned on the future trajectory of another agent <ref type="bibr" target="#b18">[19,</ref><ref type="bibr" target="#b30">31,</ref><ref type="bibr" target="#b34">35]</ref>. These approaches often rely on the future trajectory of the autonomous vehicle or a robot whose future plan is known to the predictor. Our work goes beyond by conditioning on the future trajectory of another agent to be predicted. Despite the prediction errors of the conditioned agent, we show that our model is able to outperform marginal predictors that do not account for the interactive correlations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Approach</head><p>In this section, we introduce a formal problem formulation and an overview of M2I, followed by detailed explanations of each model used in the approach.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Problem Formulation</head><p>Given observed states X = (M, S), including the map states M and the observed states S of all agents in a scene, the goal is to predict the future states of the interacting agents Y up to a finite horizon T . We assume the interacting agents are pre-labeled in a given scene, which is available in common interactive prediction datasets such as <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b40">41]</ref>. As the distribution over Y is a joint distribution over multiple agents, we approximate it as the factorization over a marginal distribution and a conditional distribution:</p><formula xml:id="formula_0">P (Y |X) = P (Y I , Y R |X) ? P (Y I |X)P (Y R |X, Y I ). (1)</formula><p>The factorization in Eq. ( <ref type="formula">1</ref>) first assigns the interacting agents as the influencer Y I and the reactor Y R , and decouples the joint distribution as the marginal distribution over the influencer and the conditional distribution over the reactor. This factorization allows us to reduce the complexity of learning a joint distribution to learning smaller distributions. In the case where two agents are not interacting, the factorization can be simplified as two marginal distributions:</p><formula xml:id="formula_1">P (Y |X) ? P (Y I |X)P (Y R |X),<label>(2)</label></formula><p>where there is no conditional dependence between the agents. Such independence is presumed by many marginal prediction models that predict the marginal distribution without considering other agents in the scene. We focus on two interactive agents in this paper and aim to tackle the pairwise interactive trajectory prediction problem proposed by <ref type="bibr" target="#b9">[10]</ref>. For scenarios involving more than two interactive agents, our approach can be modified by predicting the relations over extra agents and chaining multiple marginal and conditional distributions together.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Model Overview</head><p>Our proposed approach M2I is summarized in Fig. <ref type="figure">2</ref>. It includes a relation predictor to predict the influencer and the reactor in a scene, a marginal predictor to predict future trajectories of the influencer, and a conditional predictor to predict future trajectories of the reactor conditioned on the future trajectory of the influencer, and a sample selector to select a set of representative joint prediction samples. Although M2I includes three different learned models, all of them share the same encoder-decoder structure and adopt the same context encoder to learn context information, as illustrated in Fig. <ref type="figure" target="#fig_0">3</ref>. The conditional predictor takes an augmented scene context input that includes the influencer future trajectory to learn reactive behaviors for the reactor. In the rest of the section, we introduce each model with more details.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Relation Predictor</head><p>We propose a relation predictor to classify whether an interacting agent is an influencer or a reactor, based on the pass yield relation between two agents. Similar to <ref type="bibr" target="#b20">[21]</ref>, we assume three types of relations: pass, yield, and none, and determine the relation using the following heuristics. Given two agent future trajectories y 1 and y 2 with T steps, we first compute the closest spatial distance between two agents to determine whether a pass yield relation exists.  and thus we label the relation type as none. Otherwise, we obtain the time step from each agent at which they reach the closest spatial distance, such that:</p><formula xml:id="formula_2">d I = min T ?1=1 min T ?2=1 ||y ?1 1 -y ?2 2 || 2 . (<label>3</label></formula><formula xml:id="formula_3">)</formula><formula xml:id="formula_4">If d I &gt; d ,</formula><formula xml:id="formula_5">t 1 = arg min T ?1=1 min T ?2=1 ||y ?1 1 -y ?2 2 || 2 . (<label>4</label></formula><formula xml:id="formula_6">)</formula><formula xml:id="formula_7">t 2 = arg min T ?2=1 min T ?1=1 ||y ?1 1 -y ?2 2 || 2 .<label>(5)</label></formula><p>When t 1 &gt; t 2 , we define that agent 1 yields to agent 2, as it takes longer for agent 1 to reach the interaction point. Otherwise, we define that agent 1 passes agent 2.</p><p>After labelling the training data with three interaction types, we propose an encoder-decoder based model to classify an input scenario into a distribution over these types. As shown in Fig. <ref type="figure" target="#fig_0">3</ref>, the relation predictor model consists of a context encoder that extracts the context information, including the observed states of the interacting agents and nearby agents and map coordinates, into a hidden vector, as well as a relation prediction head that outputs the probability over each relation type. There is a rich set of literature on learning context information from a traffic scene, such as <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b11">12,</ref><ref type="bibr" target="#b13">14,</ref><ref type="bibr" target="#b24">25]</ref>. Our model could utilize any existing context encoder thanks to its modularized design, and we defer a detailed explanation of our choice in Sec. 4. The relation prediction head consists of one layer of multi-layer perceptron (MLP) followed by a fully connected layer to output the probability logits over each relation.</p><p>The loss to train the relation predictor is defined as:</p><formula xml:id="formula_8">L relation = L ce (R, R),<label>(6)</label></formula><p>where L ce is the cross entropy loss, R is the predicted relation distribution, and R is the ground truth relation labeled from data.</p><p>Given the predicted relation, we can assign each agent as an influencer or a reactor. If the relation is none, both agents are influencer, such that their future behaviors are independent of each other. If the relation is agent 1 yielding to agent 2, we assign agent 1 as the reactor and agent 2 as the influencer. If the relation is agent 1 passing agent 2, we flip the influencer and reactor labels.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4.">Marginal Trajectory Predictor</head><p>We propose a marginal trajectory predictor for the influencer based on an encoder-decoder structure, as shown in Fig. <ref type="figure" target="#fig_0">3</ref>, which is widely adopted in the trajectory prediction literature <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b13">14,</ref><ref type="bibr" target="#b41">42]</ref>. The predictor utilizes the same context encoder as in Sec. 3.3, and generates a set of prediction samples associated with confidence scores using a trajectory prediction head. Although our approach can take an arbitrary prediction head, we focus on an anchor-free goal-based prediction head because of its outstanding performance in trajectory prediction benchmarks, and defer a detailed explanation in Sec. 4.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.5.">Conditional Trajectory Predictor</head><p>The conditional trajectory predictor is similar to the marginal predictor, except that it takes an augmented scene context that includes the future trajectory of the influencer, as shown in Fig. <ref type="figure" target="#fig_0">3</ref>. This allows the features of the influencer future trajectory to be extracted and learned in the same way as other context features. The encoded scene feature is used by the trajectory prediction head, which shares the same model as in the marginal predictor, to produce multimodal prediction samples.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.6.">Sample Selector</head><p>Given the predicted relations of the influencer and the reactor, we predict N samples with confidence scores (or probabilities) for the influencer using the marginal predictor, and for each influencer sample, we predict N samples for the reactor using the conditional predictor. The number of joint samples is thus N 2 , and the probability of each joint sample is a product of the marginal probability and the conditional probability. We further reduce the size of the joint samples to K as evaluating each prediction sample for downstream tasks such as risk assessment can be expensive <ref type="bibr" target="#b36">[37]</ref>. In M2I, we select the K samples from N 2 candidates with the highest likelihoods.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.7.">Inference</head><p>At inference time, we generate the joint predictions following the procedure illustrated in Fig. <ref type="figure">2</ref>. First, we call the relation predictor and choose the interaction relation with the highest probability. Second, for the predicted influencer, we generate N trajectory samples using the marginal predictor. Third, for each influencer sample, we generate N samples for the predicted reactor using the conditional predictor. Fourth, we use the sample selector to select K representative samples from N 2 candidates. In the case where the predicted relation is none, we use the marginal predictor for both agents to obtain N 2 trajectory pairs, and follow the same sample selection step.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Experiments</head><p>In this section, we introduce the experimental setup, including the dataset benchmark and details of the model, followed by a series of experiments to demonstrate the effectiveness of M2I.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Dataset</head><p>We train and validate M2I in the Waymo Open Motion Dataset (WOMD), a large-scale driving dataset collected from realistic traffic scenarios. We focus on the interactive prediction task to predict the joint future trajectories of two interacting agents for the next 8 seconds, given the observations, including 1.1 seconds of agent states and the map state. The dataset includes 204,166 scenarios in the training set and 43,479 examples in the validation set. During training, we pre-label the interaction type of the interacting agents based on the heuristic defined in Sec. 3.3.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Metrics</head><p>We follow the WOMD benchmark by using the following metrics. minADE measures the average displacement error between the ground truth future joint trajectory and the closest predicted sample out of K = 6 joint samples. This metric is widely adopted since <ref type="bibr" target="#b15">[16]</ref> to measure the prediction error against a multi-modal distribution.</p><p>minFDE measures the final displacement error between the ground truth end positions in the joint trajectory and the closest predicted end positions from K joint samples.</p><p>Miss rate (MR) measures the percentage of none of the K joint prediction samples are within a given lateral and longitudinal threshold of the ground truth trajectory. The threshold depends on the initial velocity of the predicted agents. More details are described in <ref type="bibr" target="#b9">[10]</ref>.</p><p>Overlap rate (OR) measures the percentage of the predicted trajectory of any agent overlapping with the predicted trajectories of other agents. This metric only considers the most likely joint prediction sample. A lower overlap rate indicates the predictions are more scene compliant. In this paper, we slightly modify the metric definition compared to the original version of WOMD, which considers overlapping with other objects in a scene, so that we can measure directly the overlapping between predicted agents.</p><p>Mean average precision (mAP) measures the area under the precision-recall curve of the prediction samples given their confidence scores. Compared to mi-nADE/minFDE metrics that are only measured against the best sample regardless of its score, mAP measures the quality of confidence score and penalizes false positive predictions <ref type="bibr" target="#b9">[10]</ref>. It is the official ranking metric used by WOMD benchmark and we refer to <ref type="bibr" target="#b9">[10]</ref> for the implementation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Model Details</head><p>We present the detailed implementation of our model and training procedure in the following sections<ref type="foot" target="#foot_0">1</ref> .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.1">Context Encoder</head><p>The context encoder leverages both vectorized and rasterized representations to encode traffic context. Vectorized representation takes the traffic context, including observed agent states and map states, as vectors. It is efficient at covering a large spatial space. Rasterized representation draws traffic context on a single image with multiple channels and excels at capturing geometrical information. Both representations have achieved top performance in trajectory prediction benchmarks such as Argoverse and WOMD <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b9">10,</ref><ref type="bibr" target="#b11">12,</ref><ref type="bibr" target="#b13">14,</ref><ref type="bibr" target="#b14">15]</ref>.</p><p>In this work, we use the best of both worlds. First, we leverage a vector encoder based on VectorNet <ref type="bibr" target="#b11">[12]</ref> that takes observed agent trajectories and lane segments in the map as a set of polylines. Each polyline includes a set of vectors that connect neighboring points together. For each polyline, the vector encoder runs an MLP to encode the feature of vectors within the polyline and a graph neural network followed by a max-pooling layer to extract the feature of all the vectors. The polyline features, including agent polyline features and map polyline features, are processed by cross attention to obtain the final agent feature that includes information of the map and nearby agents. We refer to <ref type="bibr" target="#b11">[12]</ref> for a detailed model implementation.</p><p>In addition to encoding the vectorized feature, we utilize a second encoder to learn features from a rasterized representation. Following <ref type="bibr" target="#b13">[14]</ref>, We first rasterize the input states into an image with 60 channels, including the position of the agents at each past time frame with the map information. The size of the image is 224?224 and each pixel represents an area of 1m ? 1m. We run a pre-trained VGG16 <ref type="bibr" target="#b31">[32]</ref> model as the encoder to obtain the rasterized feature. The output of the context encoder is a concatenation of the vectorized feature and the rasterized feature.</p><p>Conditional Context Encoder The context encoder in the conditional trajectory predictor processes the additional influencer future trajectory in the following ways. First, the future trajectory is added to the vectorized representation as an extra vector and run through VectorNet. In parallel, we create extra 80 channels on the rasterzed representation and draw the (x, y) positions over the future 80 time frames. We run the pre-trained VGG16 model to encode the augmented image, and combine the output feature with the vectorized feature as the final output.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.2">Relation Prediction Head</head><p>The relation prediction head has one layer of MLP with one fully connected layer for classification. The MLP has a hidden size of 128, followed by a layer normalization layer and a ReLU activation layer. The fully connected layer outputs the logits over three types of relations, as described in Sec. 3.3.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.3">Trajectory Prediction Head</head><p>The trajectory prediction head adopts DenseTNT <ref type="bibr" target="#b14">[15]</ref> to generate multi-modal future predictions for its outstanding performance in the marginal prediction benchmarks. It first predicts the distribution of the agent goals as a heatmap, through a lane scoring module that identifies likely lanes to follow, a feature encoding module that uses the attention mechanism to extract features between goals and lanes, and a probability estimation module that predicts the likelihood of goals. Next, the prediction head regresses the full trajectory over the prediction horizon conditioned on the goal. The prediction head can be combined with the context encoder and trained end-to-end.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.4">Training Details</head><p>At training time, we train each model separately. Each model is trained on the training set from WOMD with a batch size of 64 for 30 epochs. We leverage an Adam optimizer and a learning rate scheduler that decays the learning rate by 30% every 5 epochs, with an initial value of 1e-3. The hidden size in the model is 128, if not specified.</p><p>When training the conditional predictor, we use the teacher forcing technique by providing the ground truth future trajectory of the influencer agent.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.">Quantitative Results</head><p>In Tab. 1, we compare our model with the following baselines, including the top ranked published models on the WOMD interaction prediction challenge leaderboard <ref type="bibr" target="#b0">[1]</ref>.</p><p>Waymo LSTM Baseline <ref type="bibr" target="#b9">[10]</ref> is the official baseline provided by the benchmark. It leverages an LSTM encoder to encode observed agent trajectories, and an MLP-based prediction head to generate multiple samples.</p><p>Waymo Full Baseline <ref type="bibr" target="#b9">[10]</ref> is an extended version of the Waymo LSTM Baseline, by leveraging a set of auxiliary encoders to encode context information.</p><p>SceneTransformer <ref type="bibr" target="#b27">[28]</ref> is a transformer-based model that leverages attention to combine features across road graphs and agent interactions both spatially and temporally. The model achieves state-of-the-art performance in the WOMD benchmark in both the marginal prediction task and the interactive prediction task.</p><p>HeatIRm4 <ref type="bibr" target="#b26">[27]</ref> models the agent interaction as a directed edge feature graph and leverages an attention network to extract interaction features. It was the winner of the 2021 WOMD challenge.</p><p>AIR 2 <ref type="bibr" target="#b37">[38]</ref> adopts a marginal anchor-based model using a raster representation. The model generates joint predictions by combining marginal predictions from each agent. It achieved the top performance at the WOMD challenge.</p><p>Baseline Marginal is our baseline model that leverages the same marginal predictor as M2I to generate N marginal prediction samples for both agents, without considering their future interactions. When combining the marginal predictions into joint predictions, we take the top K marginal pairs out of N 2 options given their joint probabilities as the product of marginal probabilities. This is a common practice to combine marginal predictions into joint predictions, as in <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b9">10]</ref>.</p><p>Baseline Joint is our baseline model that jointly predicts the goals and trajectories for both interacting agents, using the same context encoder and the trajectory prediction head as in M2I. As the joint goal space grows exponentially with the number of agents, we can only afford a small number of goal candidates for each agent. To ease the computational complexity, we leverage a marginal predictor to predict the top 80 goals for each agent and obtain 6400 goal pairs for joint goal and trajectory prediction. As a result, this baseline trade-offs prediction accuracy with computational feasibility by using a reduced set of goals.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.1">Validation Set</head><p>We present the results in the interactive validation set in the top half of Tab. 1, where the baseline results are reported as in <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b27">28]</ref> mAP, the official ranking metric, over vehicles, and a better miss rate over pedestrians. Although M2I has higher minFDE errors, it has improved the mAP over all agents (the most right column) by a large margin, meaning our model generates a more accurate distribution with its confidence scores and outputs fewer false positive predictions. In addition, as our proposed approach does not assume a specific prediction model, it could leverage SceneTransformer as the context encoder to achieve better minFDE, and we defer it as future work.</p><p>Comparing M2I with our own baselines that share the same context encoder and prediction head, we see that it outperforms the marginal predictor, which assumes independence between two agents and a joint predictor, which only affords a small set of goal candidates.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.2">Testing Set</head><p>We show the results in the interactive test set in the bottom half of Tab. 1. For a fair comparison, we use the numbers reported on the official benchmark website <ref type="bibr" target="#b0">[1]</ref> and only include the published models. Similar to the observations from the validation set, we observe that M2I improves mAP metrics by a large margin, compared to past WOMD interaction prediction challenge winners <ref type="bibr" target="#b26">[27,</ref><ref type="bibr" target="#b37">38]</ref> and the existing state-of-the-art model <ref type="bibr" target="#b27">[28]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5.">Ablation Study</head><p>In this section, we perform ablation studies on the conditional predictor and generalization to other predictors.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5.1">Conditional Prediction</head><p>We validate the effectiveness of our conditional predictor by comparing its performance against the marginal predic- When the conditional predictor takes the ground truth future trajectory of the influencer agent (c.f. M2I Conditional GT), it generates predictions for the reactor agent with better performance across all metrics. This validates our hypothesis that there exists a dependence between the influencer trajectory and the reactor trajectory. As the ground truth trajectories are not available at inference time, we present the prediction results when the conditional predictor takes the best predicted influencer trajectory as M2I Conditional P1.</p><p>It is not surprising to see that the performance is inferior to the marginal predictor results, due to errors in influencer prediction. However, as we show in Tab. 1, our model is able to outperform the marginal baseline model by including more than one sample from the influencer and selecting the most likely joint samples.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5.2">Generalizing to Other Predictors</head><p>We demonstrate that our proposed approach can be extended to other existing predictor models to validate its generalizibility. In this experiment, we replace the context encoder with VectorNet <ref type="bibr" target="#b11">[12]</ref> and the prediction head with TNT <ref type="bibr" target="#b41">[42]</ref>, which is an anchor-based goal-conditioned prediction model, and obtain a variant of M2I named TNT M2I.  Table <ref type="table">3</ref>. Joint metrics on the interactive validation set for vehicles at 8s. We replace the context encoder and the prediction head in M2I and baselines with a different model. We observe a similar trend in performance improvement, especially over OR and mAP, which validates the generalizibility of our proposed approach.</p><p>We compare this variant with a marginal predictor baseline (TNT Marginal) and a joint predictor baseline (TNT Joint) using the same VectorNet and TNT backbones. The results, summarized in Tab. 3, show that our approach improves all metrics, especially OR and mAP, by a large margin when using a different predictor model. The improvements in OR and mAP indicate that our proposed approach generates scene compliant trajectories and provides an accurate estimation over future trajectory distributions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.6.">Qualitative Results</head><p>We present a challenging interactive scenario 2 in Fig. <ref type="figure" target="#fig_1">4</ref>, and visualize the top prediction from a marginal baseline and M2I. In this scenario, the red agent is yielding to the blue agent who is making a U-turn. The marginal predictor on the left fails to capture the interaction and predicts overlapping trajectories. On the other hand, M2I successfully identifies the underlying interaction relation in the scene. It then predicts an accurate trajectory for the influencer and an 2 More examples can be found in Appendix B. accurate reactor trajectory that reacts to the predicted influencer trajectory. As a result, M2I achieves better prediction accuracy and scene compliance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Conclusion</head><p>In conclusion, we propose a simple but effective joint prediction framework M2I through marginal and conditional predictors, by exploiting the factorized relations between interacting agents. M2I uses a modularized encoderdecoder architecture, allowing it to choose from a variety of context encoders and prediction heads. Experiments on the interactive Waymo Open Motion Dataset benchmark show that our framework achieves state-of-the-art performance. In the ablation study, we show the generalization of our framework using a different predictor model.</p><p>Limitations We identify the following limitations. First, there exists a gap when comparing our model to the stateof-the-art in terms of the minFDE metric, indicating that our approach still has room for improvement. Thanks to its modularized design, we plan to extend M2I to use Scene-Transformer <ref type="bibr" target="#b27">[28]</ref> as the context encoder and fill the gap. Second, the performance of M2I heavily depends on the size of interactive training data, especially when training the relation predictor and the conditional trajectory predictor. Looking at Tab. 1, we see that our approach improves the mAP metrics by a large margin on vehicles because of sufficient vehicle interactions in the training data, but the improvement is more negligible over the other two types due to lack of interactive scenarios involving pedestrians and cyclists. Finally, our current ground truth interaction relations are obtained using a simple one-to-one pairwise heuristic, resulting in noisy supervision labels. We plan to leverage a more robust labelling algorithm as future work.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 3 .</head><label>3</label><figDesc>Figure 3. M2I includes three models that share the same context encoder. The relation predictor includes a relation prediction head to predict distribution over relation types. The marginal predictor adopts a trajectory prediction head to produce multi-modal prediction samples. The conditional trajectory predictor takes an augmented scene context input, including influencer future trajectory</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 4 .</head><label>4</label><figDesc>Figure 4. Example prediction using Baseline Marginal (left) and M2I (right). The marginal predictor produces overlapping and inaccurate predictions. M2I successfully identifies the influencer and reactor (the predicted relation type is annotated next to the current position of each agent) in a challenging interactive scene and achieves better prediction accuracy and scene compliance.</figDesc><graphic url="image-21.png" coords="8,15.58,24.07,303.94,291.06" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0"><head></head><label></label><figDesc></figDesc><graphic url="image-27.png" coords="12,50.11,326.61,249.83,238.54" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0"><head></head><label></label><figDesc></figDesc><graphic url="image-28.png" coords="12,302.56,326.80,249.83,238.18" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0"><head></head><label></label><figDesc></figDesc><graphic url="image-32.png" coords="13,50.11,165.45,249.83,218.95" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0"><head></head><label></label><figDesc></figDesc><graphic url="image-33.png" coords="13,302.56,165.45,249.83,218.95" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0"><head></head><label></label><figDesc></figDesc><graphic url="image-34.png" coords="13,50.11,385.56,249.83,231.22" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0"><head></head><label></label><figDesc></figDesc><graphic url="image-35.png" coords="13,302.56,385.40,249.83,231.54" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0"><head></head><label></label><figDesc></figDesc><graphic url="image-37.png" coords="14,50.11,128.80,249.83,249.83" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0"><head></head><label></label><figDesc></figDesc><graphic url="image-38.png" coords="14,302.56,128.80,249.83,249.83" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>The relation predictor predicts influencer-reactor relations for interacting agents. The marginal predictor generates marginal predictions for the influencer. The conditional predictor generates predictions for the reactor, conditioned on each influencer trajectory. The sample selector chooses a subset of representative joint samples as output.</figDesc><table><row><cell>Marginal</cell><cell></cell></row><row><cell>Predictor</cell><cell></cell></row><row><cell>I nfluencer</cell><cell></cell></row><row><cell>Relation</cell><cell>Sample</cell></row><row><cell>Predictor</cell><cell>Selector</cell></row><row><cell>R eactor</cell><cell></cell></row><row><cell>Conditional</cell><cell>K Joint Samples</cell></row><row><cell>Predictor</cell><cell>N x N Joint Samples</cell></row><row><cell>Figure 2. Overview of M2I.</cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 1 .</head><label>1</label><figDesc>. Our model M2I outperforms both Waymo baselines in terms of all metrics. Compared to the current stateof-the-art model SceneTransformer, M2I achieves a better Joint metrics on the interactive validation and test set. The best performed metrics are bolded and the grey cells indicate the ranking metric used by the WOMD benchmark. M2I outperforms both Waymo baselines and challenge winners. Compared to the current state-of-the art model SceneTransformer, it improves the mAP metric by a large margin over vehicles and all agents, demonstrating its advantage in learning a more accurate probability distribution and producing fewer false positive predictions.</figDesc><table><row><cell></cell><cell></cell><cell cols="2">Vehicle (8s)</cell><cell></cell><cell cols="2">Pedestrian (8s)</cell><cell></cell><cell></cell><cell>Cyclist (8s)</cell><cell></cell><cell>All (8s)</cell></row><row><cell>Set</cell><cell>Model</cell><cell>mFDE ?</cell><cell cols="2">MR ? mAP ?</cell><cell>mFDE ?</cell><cell cols="2">MR ? mAP ?</cell><cell>mFDE ?</cell><cell cols="2">MR ? mAP ?</cell><cell>mAP ?</cell></row><row><cell>Val.</cell><cell>Waymo LSTM Baseline [10] Waymo Full Baseline [10]</cell><cell>-6.07</cell><cell>0.88 0.66</cell><cell>0.01 0.08</cell><cell>-4.20</cell><cell>0.93 1.00</cell><cell>0.02 0.00</cell><cell>-6.46</cell><cell>0.98 0.83</cell><cell>0.00 0.01</cell><cell>0.01 0.03</cell></row><row><cell></cell><cell>SceneTransformer [28]</cell><cell>3.99</cell><cell>0.49</cell><cell>0.11</cell><cell>3.15</cell><cell>0.62</cell><cell>0.06</cell><cell>4.69</cell><cell>0.71</cell><cell>0.04</cell><cell>0.07</cell></row><row><cell></cell><cell>Baseline Marginal</cell><cell>6.26</cell><cell>0.60</cell><cell>0.16</cell><cell>3.59</cell><cell>0.63</cell><cell>0.04</cell><cell>6.47</cell><cell>0.76</cell><cell>0.03</cell><cell>0.07</cell></row><row><cell></cell><cell>Baseline Joint</cell><cell>11.31</cell><cell>0.64</cell><cell>0.14</cell><cell>3.44</cell><cell>0.93</cell><cell>0.01</cell><cell>7.16</cell><cell>0.82</cell><cell>0.01</cell><cell>0.05</cell></row><row><cell></cell><cell>M2I</cell><cell>5.49</cell><cell>0.55</cell><cell>0.18</cell><cell>3.61</cell><cell>0.60</cell><cell>0.06</cell><cell>6.26</cell><cell>0.73</cell><cell>0.04</cell><cell>0.09</cell></row><row><cell>Test</cell><cell>Waymo LSTM Baseline [10] HeatIRm4 [27]</cell><cell>12.40 7.20</cell><cell>0.87 0.80</cell><cell>0.01 0.07</cell><cell>6.85 4.06</cell><cell>0.92 0.80</cell><cell>0.00 0.05</cell><cell>10.84 6.69</cell><cell>0.97 0.85</cell><cell>0.00 0.01</cell><cell>0.00 0.04</cell></row><row><cell></cell><cell>AIR 2 [38]</cell><cell>5.00</cell><cell>0.64</cell><cell>0.10</cell><cell>3.68</cell><cell>0.71</cell><cell>0.04</cell><cell>5.47</cell><cell>0.81</cell><cell>0.04</cell><cell>0.05</cell></row><row><cell></cell><cell>SceneTransformer [28]</cell><cell>4.08</cell><cell>0.50</cell><cell>0.10</cell><cell>3.19</cell><cell>0.62</cell><cell>0.05</cell><cell>4.65</cell><cell>0.70</cell><cell>0.04</cell><cell>0.06</cell></row><row><cell></cell><cell>M2I</cell><cell>5.65</cell><cell>0.57</cell><cell>0.16</cell><cell>3.73</cell><cell>0.60</cell><cell>0.06</cell><cell>6.16</cell><cell>0.74</cell><cell>0.03</cell><cell>0.08</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 2 .</head><label>2</label><figDesc>Comparison between the marginal predictor and the conditional predictor over marginal metrics for vehicle reactors at 8s.</figDesc><table><row><cell>Model</cell><cell cols="2">minADE ? minFDE ?</cell><cell>MR ?</cell><cell>mAP ?</cell></row><row><cell>M2I Marginal</cell><cell>1.70</cell><cell>3.45</cell><cell>0.23</cell><cell>0.30</cell></row><row><cell>M2I Conditional GT</cell><cell>1.46</cell><cell>2.43</cell><cell>0.12</cell><cell>0.41</cell></row><row><cell>M2I Conditional P1</cell><cell>1.75</cell><cell>3.49</cell><cell>0.25</cell><cell>0.26</cell></row></table><note><p>tor (M2I Marginal) when predicting marginal trajectories for the reactor agents. The results are summarized in Tab. 2.</p></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0"><p>The source code and project website will be available soon.</p></note>
		</body>
		<back>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Appendix A. Additional Experiment Details</head><p>In this section, we introduce additional details on filtering interactive training data, training the baseline joint predictor, and training by agent types.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.1 Filtering Interactive Training Data</head><p>The Waymo Open Motion Dataset only provides interactive scenarios in its validation set and testing set. To filter the interactive scenario in the training set, we implement a script to identify scenarios that include 2 interacting agents based on the objects of interest mask provided in the data. The script is provided in the source code.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.2 Baseline Joint Predictor</head><p>We train the Baseline Joint predictor described in Sec. 4.4 as follows. First, we predict the distribution of goals for each interacting agent as a heatmap, according to <ref type="bibr" target="#b14">[15]</ref>. Second, we select the top 80 goals based on the predicted probability for each agent. Third, we combine the selected goals into 6400 goal pairs and run each goal pair feature, including (x, y) positions for both goals, through a 2-layer MLP with a hidden size of 128 followed by a normalization layer and a ReLU activation layer. Fourth, we run a fully connected layer to predict the probability logit for each goal pair, and train the joint goal prediction model through the following loss:</p><p>where L ce is the cross entropy loss, J is the predicted goal pair distribution, and ? is the index of the goal pair out of all candidates that is the closest to the ground truth goal pair in terms of Euclidean distance. Given the predicted goal pairs, we train the trajectory completion model to regress the full trajectories of both interacting agents following the same procedure in <ref type="bibr" target="#b14">[15]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.3 Training by Agent Types</head><p>The Waymo Open Motion Dataset consists of three types of agents to predict: vehicles, pedestrians, and cyclists. As each agent type has different behavior models and the distribution is unbalanced among types (e.g. vehicle types account for 78% of the training data), we train the marginal trajectory predictor and the conditional trajectory predictor for each agent type separately. We observe that the prediction performance over pedestrians and cyclists improves by a large margin, compared to training a single model for all agents.</p><p>For the same reason, we train four relation predictors for vehicle-vehicle interactions, vehicle-pedestrian interactions, vehicle-cyclist interactions, and interactions that cover the remaining agent pair types, including cyclistpedestrian, cyclist-cyclist, pedestrian-pedestrian.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Additional Qualitative Examples</head><p>We present additional representative examples in a variety of interaction settings to showcase the advantage of M2I over the marginal baseline.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.1 Influencer Overtakes Reactor</head><p>In Fig. <ref type="figure">5</ref>, we present three examples in which the influencer overtakes the reactor. In each example, M2I successfully predicts the correct relation type and improves prediction accuracy and scene compliance, while the marginal predictor predicts overlapping trajectories without considering the future interaction between agents.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.2 Reactor Yields to Influencer before Turning</head><p>In Fig. <ref type="figure">6</ref>, we present three examples in which the reactor waits for the influencer to pass before turning. In each example, M2I successfully predicts the correct relation type and the accurate reactive trajectories for the reactor. On the other hand, the marginal predictor ignores the interaction and results in less accurate predictions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.3 Reactor Merges behind Influencer</head><p>In Fig. <ref type="figure">7</ref>, we present two examples in which the reactor merges behind the influencer after it passes. In each example, M2I successfully predicts the correct relation type and the accurate reactor trajectories that follow the influencer, while the marginal predictor fails to account for the interaction and predicts trajectories far away from the ground truth.   </p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<ptr target="https://waymo.com/open/challenges/2021/interaction-prediction/" />
		<title level="m">Waymo open motion dataset interaction prediction</title>
		<imprint>
			<date type="published" when="2021-11-16">2021. November 16th 2021</date>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Social LSTM: Human trajectory prediction in crowded spaces</title>
		<author>
			<persName><forename type="first">Alexandre</forename><surname>Alahi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kratarth</forename><surname>Goel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vignesh</forename><surname>Ramanathan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alexandre</forename><surname>Robicquet</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Li</forename><surname>Fei-Fei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Silvio</forename><surname>Savarese</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="961" to="971" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Spatially-aware graph neural networks for relational behavior forecasting from sensor data</title>
		<author>
			<persName><forename type="first">Sergio</forename><surname>Casas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Cole</forename><surname>Gulino</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Renjie</forename><surname>Liao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Raquel</forename><surname>Urtasun</surname></persName>
		</author>
		<author>
			<persName><surname>Spagnn</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2020 IEEE International Conference on Robotics and Automation (ICRA)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="9491" to="9497" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Implicit latent variable model for scene-consistent motion forecasting</title>
		<author>
			<persName><forename type="first">Sergio</forename><surname>Casas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Cole</forename><surname>Gulino</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Simon</forename><surname>Suo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Katie</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Renjie</forename><surname>Liao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Raquel</forename><surname>Urtasun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV)</title>
		<meeting>the European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2020">2020</date>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Multipath: Multiple probabilistic anchor trajectory hypotheses for behavior prediction</title>
		<author>
			<persName><forename type="first">Yuning</forename><surname>Chai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Benjamin</forename><surname>Sapp</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mayank</forename><surname>Bansal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dragomir</forename><surname>Anguelov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conference on Robot Learning (CoRL)</title>
		<imprint>
			<date type="published" when="2019">2019. 1, 2</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Argoverse: 3d tracking and forecasting with rich maps</title>
		<author>
			<persName><forename type="first">Ming-Fang</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">John</forename><surname>Lambert</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Patsorn</forename><surname>Sangkloy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jagjeet</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Slawomir</forename><surname>Bak</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrew</forename><surname>Hartnett</surname></persName>
		</author>
		<author>
			<persName><forename type="first">De</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Peter</forename><surname>Carr</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Simon</forename><surname>Lucey</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Deva</forename><surname>Ramanan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Multimodal trajectory predictions for autonomous driving using deep convolutional networks</title>
		<author>
			<persName><forename type="first">Henggang</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vladan</forename><surname>Radosavljevic</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fang-Chieh</forename><surname>Chou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tsung-Han</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Thi</forename><surname>Nguyen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tzu-Kuo</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jeff</forename><surname>Schneider</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nemanja</forename><surname>Djuric</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2019 International Conference on Robotics and Automation (ICRA)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="2090" to="2096" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">A flexible and explainable vehicle motion prediction and inference framework combining semisupervised aog and st-lstm</title>
		<author>
			<persName><forename type="first">Shengzhe</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhiheng</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Li</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nanning</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shuofeng</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Intelligent Transportation Systems</title>
		<imprint>
			<biblScope unit="issue">2</biblScope>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Multi-modal trajectory prediction of surrounding vehicles with maneuver based lstms</title>
		<author>
			<persName><forename type="first">Nachiket</forename><surname>Deo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mohan</forename><forename type="middle">M</forename><surname>Trivedi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2018 IEEE Intelligent Vehicles Symposium (IV)</title>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="1179" to="1184" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">Large scale interactive motion forecasting for autonomous driving: The waymo open motion dataset</title>
		<author>
			<persName><forename type="first">Scott</forename><surname>Ettinger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shuyang</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Benjamin</forename><surname>Caine</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chenxi</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hang</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sabeek</forename><surname>Pradhan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuning</forename><surname>Chai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ben</forename><surname>Sapp</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Charles</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yin</forename><surname>Zhou</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2104.10133</idno>
		<imprint>
			<date type="published" when="2007">2021. 2, 3, 4, 5, 6, 7</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">TPNet: Trajectory proposal network for motion prediction</title>
		<author>
			<persName><forename type="first">Liangji</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qinhong</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jianping</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bolei</forename><surname>Zhou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="6797" to="6806" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">VectorNet: Encoding hd maps and agent dynamics from vectorized representation</title>
		<author>
			<persName><forename type="first">Jiyang</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chen</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hang</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yi</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dragomir</forename><surname>Anguelov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Congcong</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Cordelia</forename><surname>Schmid</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2007">2020. 1, 4, 5, 7</date>
			<biblScope unit="page" from="11525" to="11533" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">Gohome: Graphoriented heatmap output for future motion estimation</title>
		<author>
			<persName><forename type="first">Thomas</forename><surname>Gilles</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stefano</forename><surname>Sabatini</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dzmitry</forename><surname>Tsishkou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bogdan</forename><surname>Stanciulescu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fabien</forename><surname>Moutarde</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2109.01827</idno>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<author>
			<persName><forename type="first">Thomas</forename><surname>Gilles</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stefano</forename><surname>Sabatini</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dzmitry</forename><surname>Tsishkou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bogdan</forename><surname>Stanciulescu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fabien</forename><surname>Moutarde</surname></persName>
		</author>
		<author>
			<persName><surname>Home</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2105.10968</idno>
		<title level="m">Heatmap output for future motion estimation</title>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Densetnt: End-to-end trajectory prediction from dense goal sets</title>
		<author>
			<persName><forename type="first">Junru</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chen</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hang</forename><surname>Zhao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2021">2021. 1, 2, 5, 6, 11</date>
			<biblScope unit="page" from="15303" to="15312" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Social GAN: Socially acceptable trajectories with generative adversarial networks</title>
		<author>
			<persName><forename type="first">Agrim</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Justin</forename><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Li</forename><surname>Fei-Fei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Silvio</forename><surname>Savarese</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alexandre</forename><surname>Alahi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Social force model for pedestrian dynamics</title>
		<author>
			<persName><forename type="first">Dirk</forename><surname>Helbing</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Peter</forename><surname>Molnar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Physical review E</title>
		<imprint>
			<biblScope unit="volume">51</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page">4282</biblScope>
			<date type="published" when="1995">1995</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Diversity-aware vehicle motion prediction via latent semantic sampling</title>
		<author>
			<persName><forename type="first">Xin</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stephen</forename><forename type="middle">G</forename><surname>Mcgill</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jonathan</forename><forename type="middle">A</forename><surname>Decastro</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Luke</forename><surname>Fletcher</surname></persName>
		</author>
		<author>
			<persName><forename type="first">John</forename><forename type="middle">J</forename><surname>Leonard</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Brian</forename><forename type="middle">C</forename><surname>Williams</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Guy</forename><surname>Rosman</surname></persName>
		</author>
		<author>
			<persName><surname>Diversitygan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Robotics and Automation Letters</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="5089" to="5096" />
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">What-if motion prediction for autonomous driving</title>
		<author>
			<persName><forename type="first">Siddhesh</forename><surname>Khandelwal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">William</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jagjeet</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrew</forename><surname>Hartnett</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Deva</forename><surname>Ramanan</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2008.10587</idno>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">LaPred: Lane-aware prediction of multimodal future trajectories of dynamic agents</title>
		<author>
			<persName><forename type="first">Byeoungdo</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Seong Hyeon</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Seokhwan</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Elbek</forename><surname>Khoshimjonov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dongsuk</forename><surname>Kum</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Junsoo</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jeong</forename><surname>Soo Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jun</forename><forename type="middle">Won</forename><surname>Choi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">Interaction-based trajectory prediction over a hybrid traffic graph</title>
		<author>
			<persName><forename type="first">Sumit</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yiming</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jerrick</forename><surname>Hoang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Galen</forename><forename type="middle">Clark</forename><surname>Haynes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Micol</forename><surname>Marchetti-Bowick</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2009.12916</idno>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">Joint interaction and trajectory prediction for autonomous driving using graph neural networks</title>
		<author>
			<persName><forename type="first">Donsuk</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yiming</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jerrick</forename><surname>Hoang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Micol</forename><surname>Marchetti-Bowick</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1912.07882</idno>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Desire: Distant future prediction in dynamic scenes with interacting agents</title>
		<author>
			<persName><forename type="first">Namhoon</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wongun</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Paul</forename><surname>Vernaza</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christopher</forename><forename type="middle">B</forename><surname>Choy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Philip</forename><surname>Hs Torr</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Manmohan</forename><surname>Chandraker</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">2</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">End-to-end contextual perception and prediction with interaction transformer</title>
		<author>
			<persName><forename type="first">Luke</forename><surname>Lingyun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bin</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ming</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wenyuan</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mengye</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sean</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Raquel</forename><surname>Segal</surname></persName>
		</author>
		<author>
			<persName><surname>Urtasun</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2008.05927</idno>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Learning lane graph representa-tions for motion forecasting</title>
		<author>
			<persName><forename type="first">Ming</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bin</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rui</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yun</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Renjie</forename><surname>Liao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Song</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Raquel</forename><surname>Urtasun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2020">2020</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">It is not the journey but the destination: Endpoint conditioned trajectory prediction</title>
		<author>
			<persName><forename type="first">Karttikeya</forename><surname>Mangalam</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Harshayu</forename><surname>Girase</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shreyas</forename><surname>Agarwal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kuan-Hui</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ehsan</forename><surname>Adeli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jitendra</forename><surname>Malik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Adrien</forename><surname>Gaidon</surname></persName>
		</author>
		<idno>Au- gust 2020. 2</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV)</title>
		<meeting>the European Conference on Computer Vision (ECCV)</meeting>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Multi-modal interactive agent trajectory prediction using heterogeneous edge-enhanced graph attention network</title>
		<author>
			<persName><forename type="first">Xiaoyu</forename><surname>Mo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhiyu</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chen</forename><surname>Lv</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Workshop on Autonomous Driving</title>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<title level="m" type="main">Scene transformer: A unified architecture for predicting multiple agent trajectories</title>
		<author>
			<persName><forename type="first">Jiquan</forename><surname>Ngiam</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Benjamin</forename><surname>Caine</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vijay</forename><surname>Vasudevan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhengdong</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lewis</forename><surname>Hao-Tien</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jeffrey</forename><surname>Chiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rebecca</forename><surname>Ling</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alex</forename><surname>Roelofs</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chenxi</forename><surname>Bewley</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ashish</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><surname>Venugopal</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2106.08417</idno>
		<imprint>
			<date type="published" when="2008">2021. 2, 6, 7, 8</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Multi-agent trajectory prediction with fuzzy query attention</title>
		<author>
			<persName><forename type="first">Zhu</forename><surname>Kamra Nitin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Trivedi</forename><surname>Hao</surname></persName>
		</author>
		<author>
			<persName><surname>Dweep</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ming</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Liu</forename><surname>Yan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems (NeurIPS)</title>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Precog: Prediction conditioned on goals in visual multi-agent settings</title>
		<author>
			<persName><forename type="first">Nicholas</forename><surname>Rhinehart</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rowan</forename><surname>Mcallister</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kris</forename><surname>Kitani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sergey</forename><surname>Levine</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="2821" to="2830" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Trajectron++: Multi-agent generative trajectory forecasting with heterogeneous data for control</title>
		<author>
			<persName><forename type="first">Tim</forename><surname>Salzmann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Boris</forename><surname>Ivanovic</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Punarjay</forename><surname>Chakravarty</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Marco</forename><surname>Pavone</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV)</title>
		<meeting>the European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2020">2020</date>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
		<title level="m" type="main">Very deep convolutional networks for large-scale image recognition</title>
		<author>
			<persName><forename type="first">Karen</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrew</forename><surname>Zisserman</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1409.1556</idno>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Learning to predict vehicle trajectories with model-based planning</title>
		<author>
			<persName><forename type="first">Haoran</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Di</forename><surname>Luan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wenchao</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><forename type="middle">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qifeng</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">5th Annual Conference on Robot Learning</title>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Multiple futures prediction</title>
		<author>
			<persName><forename type="first">Charlie</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Russ</forename><forename type="middle">R</forename><surname>Salakhutdinov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">2</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<monogr>
		<title level="m" type="main">Identifying driver interactions via conditional behavior prediction</title>
		<author>
			<persName><forename type="first">Ekaterina</forename><surname>Tolstaya</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Reza</forename><surname>Mahjourian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Carlton</forename><surname>Downey</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Balakrishnan</forename><surname>Vadarajan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Benjamin</forename><surname>Sapp</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dragomir</forename><surname>Anguelov</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2104.09959</idno>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b35">
	<monogr>
		<title level="m" type="main">Safecritic: Collision-aware trajectory prediction</title>
		<author>
			<persName><forename type="first">Tessa</forename><surname>Van Der Heiden</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Naveen</forename><surname>Shankar Nagaraja</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christian</forename><surname>Weiss</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Efstratios</forename><surname>Gavves</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1910.06673</idno>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Fast risk assessment for autonomous vehicles using learned models of agent futures</title>
		<author>
			<persName><forename type="first">Allen</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xin</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ashkan</forename><surname>Jasour</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Brian</forename><surname>Williams</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Robotics: Science and Systems</title>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Air 2 for interaction prediction</title>
		<author>
			<persName><forename type="first">David</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yunan</forename><surname>Wu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Workshop on Autonomous Driving, CVPR</title>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Who are you with and where are you going?</title>
		<author>
			<persName><forename type="first">Kota</forename><surname>Yamaguchi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alexander</forename><forename type="middle">C</forename><surname>Berg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Luis</forename><forename type="middle">E</forename><surname>Ortiz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tamara</forename><forename type="middle">L</forename><surname>Berg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR 2011</title>
		<imprint>
			<date type="published" when="2011">2011</date>
			<biblScope unit="page" from="1345" to="1352" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Diverse trajectory forecasting with determinantal point processes</title>
		<author>
			<persName><forename type="first">Ye</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kris</forename><surname>Kitani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Eighth International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<monogr>
		<title level="m" type="main">Interaction dataset: An international, adversarial and cooperative motion dataset in interactive driving scenarios with semantic maps</title>
		<author>
			<persName><forename type="first">Wei</forename><surname>Zhan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Liting</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Di</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Haojie</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aubrey</forename><surname>Clausse</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Maximilian</forename><surname>Naumann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Julius</forename><surname>Kummerle</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hendrik</forename><surname>Konigshof</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christoph</forename><surname>Stiller</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Arnaud</forename><surname>De</surname></persName>
		</author>
		<author>
			<persName><forename type="first">La</forename><surname>Fortelle</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1910.03088</idno>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">TNT: Target-driven trajectory prediction</title>
		<author>
			<persName><forename type="first">Hang</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiyang</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tian</forename><surname>Lan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chen</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Benjamin</forename><surname>Sapp</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Balakrishnan</forename><surname>Varadarajan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yue</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yi</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuning</forename><surname>Chai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Cordelia</forename><surname>Schmid</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conference on Robot Learning (CoRL)</title>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Multi-agent tensor fusion for contextual trajectory prediction</title>
		<author>
			<persName><forename type="first">Tianyang</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yifei</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mathew</forename><surname>Monfort</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wongun</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chris</forename><surname>Baker</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yibiao</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yizhou</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ying</forename><forename type="middle">Nian</forename><surname>Wu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="12126" to="12134" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
