<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Revealing Event Saliency in Unconstrained Video Collection</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Dingwen</forename><surname>Zhang</surname></persName>
							<affiliation key="aff0">
								<orgName type="laboratory">II. RELATED WORKS A. Multimedia Event Detection</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Lu</forename><surname>Jiang</surname></persName>
							<affiliation key="aff0">
								<orgName type="laboratory">II. RELATED WORKS A. Multimedia Event Detection</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Senmao</forename><surname>Ye</surname></persName>
							<affiliation key="aff0">
								<orgName type="laboratory">II. RELATED WORKS A. Multimedia Event Detection</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Xiaojun</forename><surname>Chang</surname></persName>
							<affiliation key="aff0">
								<orgName type="laboratory">II. RELATED WORKS A. Multimedia Event Detection</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Revealing Event Saliency in Unconstrained Video Collection</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">D8242B0722582AAA548972E2823F0278</idno>
					<idno type="DOI">10.1109/TIP.2017.2658957</idno>
					<note type="submission">This article has been accepted for publication in a future issue of this journal, but has not been fully edited. Content may change prior to final publication. Citation information: DOI 10.1109/TIP.2017.2658957, IEEE Transactions on Image Processing</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.3" ident="GROBID" when="2023-07-27T07:57+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Recent progresses in multimedia event detection have enabled us to find videos about a predefined event from a large-scale video collection. Research towards more intrinsic unsupervised video understanding is an interesting but understudied field. Specifically, given a collection of videos sharing a common event of interest, the goal is to discover the salient fragments, i.e., the curt video fragments that can concisely portray the underlying event of interest, from each video. To explore this novel direction, this paper proposes an unsupervised event saliency revealing framework. It first extracts features from multiple modalities to represent each shot in the given video collection. Then, these shots are clustered to build the cluster-level event saliency revealing framework, which explores useful information cues (i.e., the intra-cluster prior, inter-cluster discriminability, and inter-cluster smoothness) by a concise optimization model. Compared with the existing methods, our approach could highlight the intrinsic stimulus of the unseen event within a video in an unsupervised fashion. Thus, it could potentially benefit to a wide range of multimedia tasks like video browsing, understanding and search. To quantitatively verify the proposed method, we systematically compare the method to a number of baseline methods on the TRECVID benchmarks. Experimental results have demonstrated its effectiveness and efficiency.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>I. INTRODUCTION</head><p>Multimedia event detection <ref type="bibr" target="#b0">[1]</ref>, <ref type="bibr" target="#b1">[2]</ref> aims at ranking a sequence of unconstrained videos according to their likelihood of belonging to a certain event, e.g., "wedding ceremony". It has become a popular research topic during the past few decades. Unlike the professional videos, e.g., news videos, such unconstrained videos are generally captured and edited by amateur video-uploaders with little adjustment and post-processes. Moreover, such videos often involve complex content consisting of diverse scenes, objects, actions, and the rich interactions among these factors, which intensifies the challenge of the task.</p><p>For better addressing the event detection problem, an obvious trend appeared in recent years is to explore the event-specific evidence from the videos rather than treating Manuscript received XXX; revised XXX; accepted XXX. Date of publication XXX; date of current version XXX. This work was supported in part by the National Science Foundation of China under Grants 61522207, the Excellent Doctorate Foundation of Northwestern Polytechnical University, and the China Scholarships Council under Grants 201506290113. (Junwei Han is the corresponding author.)</p><p>Dingwen Zhang, Junwei Han, and Senmao Ye are with School of Automation, Northwestern Polytechnical University, Xi'an, China. E-mail: zdw2006yyy@mail.nwpu.edu.cn; junweihan2010@gmail.com.</p><p>Lu jiang and Xiaojun Chang are with the School of Computer, Carnegie Mellon University, Pittsburgh, US. E-mail: lujiang@cs.cmu.edu; cxj273@gmail.com. each frame of the given video equally. Generally, existing approaches discover the evidence in detection-driven manner, where the evidence is discovered under a specific task of detecting pre-specified events. However, discovering evidence in such way would exhibit the following limitations. First, it usually performs in supervised fashion, where manually labeled data are needed to train taskspecific models. More importantly, the evidence discovered in this way can only be used to justify pre-specified events, i.e., the events with training labels, rather than to generically understand the common events in any given video collection. Therefore, research towards more intrinsic open-ended video understanding seems to be still missing.</p><p>To advance the techniques along this direction, we first analyze the challenges in this task and then propose an unsupervised algorithm that can reveal the underlying event saliency from the unconstrained video collection. In this paper, event saliency is defined as the capability of each video frame or shot for representing the common event appearing in a given video collection, even without knowing the exact event name. As shown in Fig. <ref type="figure" target="#fig_1">1</ref>, for the video collection of marriage proposal, the salient video shots are the red ones with the content of "Kneel with ring", which can concisely represent the event of interest. While most videos also contain large amount of un-salient fragments (e.g., the fragments of "Title words", "Get out of the car", "Laugh" shown in blue of the figure) that have weak relation with the event of interest. The goal of revealing event saliency is to predict the saliency scores for each video shot in the given video collection and highlight (assign high scores to) the salient fragments in the temporal space. As can be seen, event saliency is different from the visual saliency <ref type="bibr" target="#b2">[3]</ref>- <ref type="bibr" target="#b4">[5]</ref> as event saliency is towards higher-level semantic interpretation of the video content. Event saliency also differs from the task of video summarization <ref type="bibr" target="#b5">[6]</ref>, <ref type="bibr" target="#b6">[7]</ref> as the latter mainly aims at extracting video fragments to help users obtaining quick comprehension of the whole story (including the beginning, the key progresses of the story, and the end) of the given single video, whereas the former focuses on extracting representative fragments from a collection of videos to characterize the common event.</p><p>As the video collections are usually obtained by retrieving certain event names from the online search engines like Google or using the pre-trained predictors to cluster the collected videos rather than requiring the ground truth manual annotations from users, we need address the following challenges for revealing event saliency. 1) The events of interest having complex content may occur in various temporal Fig. <ref type="figure" target="#fig_1">1</ref>. An example to illustrate the problem of our task. Given a collection of videos sharing a common event of interest "marriage proposal"", revealing event saliency is to discover the most representative video fragments, which probably have the content of "Kneel and ring" as shown in the red cubes, to characterize the common event from the cluttered background fragments like those containing "Get out of car", "Show the sign", "Hug", et al. shown in the blue cubes. Notice that the exact event name will not be provided to our algorithm.</p><p>segments and be played back in different styles. Thus, it is helpful to utilize multi-modalies features for representing the segments in the videos. 2) The unconstrained video collections downloaded from the Internet would inevitably contain noisy videos which may not contain the common events in the same video collection. 3) Analysis of the video content in fine level, e.g., frame level or shot level, would cause the scalability issue when processing the largescale unconstraint video collections. 4) More importantly, as shown in Fig. <ref type="figure" target="#fig_1">1</ref>, the content of videos is usually so cluttered that only small video fragments, e.g., frames and shots, are characterized as the occurrence of the events of interest, whereas the remaining large proportions of the video are background and non-salient fragments which may contain the cause, the preparation, or the result of the event.</p><p>In this paper, we propose a novel and effective framework to address the aforementioned problems. Specifically, to tackle the first challenge, we explore the content in each video shot based on the multi-modality descriptors in four types of features, including the motion, vision, concept, and speech features. Fusing the multimodal descriptors creates a powerful representation for each video shot and this benefits event understanding. For addressing the second challenge, we mainly explore the event saliency in the "soft" clusterlevel rather than the "hard" shot-level so that it would not enforce every video in the collection to necessarily contain the event of interest. Experiments show that this novel strategy can effectively alleviate the noisy issue in the unconstrained video collections. In addition, by exploring the event saliency in the cluster-level, the main optimization process of the proposed framework is performed only based on the cluster centers, which can highly reduce the computational burden as compared with directly performing optimization process in shot-level. Thus, the proposed framework is designed to have good scalability that could solve the third challenge efficiently. For the forth challenge, we establish a concise and unified optimization objective which jointly considers the intra-cluster prior, inter-cluster discriminability, and inter-cluster smoothness to infer the event saliency without supervision, which is based on the observation (see Fig. <ref type="figure" target="#fig_1">1</ref>) that the salient fragments have characteristics like appearing among most videos in the collection, containing consistent contents, distinct from the un-salient fragments.</p><p>Finally, to evaluate the proposed approach, we conduct experiments on three benchmarks that have shot-level annotations, including a collection in <ref type="bibr" target="#b7">[8]</ref> and two collections recently released by the NIST TRECVID <ref type="bibr" target="#b8">[9]</ref>. We comprehensively compare our approach with a number of baseline methods. Experimental results demonstrate that our method outperforms other baseline methods. Our method could potentially benefits to a variety of multimedia applications. For example, it can facilitate users to quickly skim through the search results returned by the video retrieval systems in an unsupervised manner. In addition, given a collection of videos without any metadata, we can cluster the videos into groups and apply the proposed method to discover the salient moments so that instead of watching every video, users can have comprehensive understanding about what a video collection is about.</p><p>To sum up, this paper has three-fold contributions: 1) We advance the understanding of a meaningful yet under-studied problem, i.e., event saliency discovery, by analyzing and detailing its main challenges in this research direction.</p><p>2) We propose a novel framework for revealing event saliency, which can highlight the intrinsic stimulus of the unknown events in the unconstrained internet video collections. The saliency moments discovered by the proposed method improve the understanding about the video content.</p><p>3) Comprehensive experiments on three benchmarks have conducted to demonstrate the effectiveness and efficiency of the proposed entire framework as well as the key components considered in our approach. Notably, the proposed method is able to achieve comparable or even better results to the existing supervised method.</p><p>Multimedia event detection (MED) <ref type="bibr" target="#b0">[1]</ref>, <ref type="bibr" target="#b1">[2]</ref> is a challenging yet interesting research area that has attracted much attention during the last few years. To address this problem, early event detection research focuses on aggregating local feature descriptors from the entire video sequence to create a video-level representation. This approach works reasonably well in practice but lacks the temporal evidence that supports the decision <ref type="bibr" target="#b9">[10]</ref>, <ref type="bibr" target="#b10">[11]</ref> and cannot highlight the event of interest. More recently, the studies, e.g., <ref type="bibr" target="#b11">[12]</ref>, <ref type="bibr" target="#b12">[13]</ref>, started to understand the video content by temporally locating the evidence supporting the pre-specified event detection over the frames or shots. These studies also showed that the localized evidence can help improving the performance of multimedia event detection.</p><p>Among the proposed approaches, Lai et al. <ref type="bibr" target="#b12">[13]</ref> proposed to formulate MED as an Evidence Selective Ranking (ESR) procedure. With the labeled training videos, ESR discovered the key static-dynamic evidences in videos based on the learning objective directly enforcing positive videos to have the highest scores in the detection results. By treating the locations of characteristic segments as hidden information, Li et al. <ref type="bibr" target="#b13">[14]</ref> identified the segments most informative for detecting a given event via a multipleinstance learning paradigm. Similarly, Lai et al. <ref type="bibr" target="#b14">[15]</ref> also represented each video as a collection of instances and their goal is to learn an instance-level event detection model based on the video-level labels. More recently, Chang et al. <ref type="bibr" target="#b11">[12]</ref> defined the semantic saliency as the relevance of each shot with the event of interest. It is learned by a nearlyisotonic SVM classifier that is able to exploit the inhibit semantic ordering information.</p><p>These approaches have made a step towards video understanding in the finer frame or shot level. However, they still have limitations inherently existed in such detection-driven manner, which are the issues of heavily relying on the manually annotation and the poor generalization capability for scaling-up as discussed in the Sec. I. Compared with these approaches, this paper makes a further step towards revealing the intrinsic salient stimulus from the collected videos. In addition, the algorithm proposed in this paper can perform in an unsupervised manner and achieve promising performance as long as the noisy of the input video collection is less than 50 percent.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Co-saliency Detection</head><p>Co-saliency detection <ref type="bibr" target="#b15">[16]</ref>- <ref type="bibr" target="#b18">[19]</ref> aims at discovering the common and salient objects from two or more relevant images, while the categories, characteristics and locations of these objects are entirely unknown. Among the typical co-saliency detection methods, Li and Ngan <ref type="bibr" target="#b18">[19]</ref> proposed to combine single-image saliency and multi-image saliency to detect co-saliency from image pairs. Fu et al. <ref type="bibr" target="#b17">[18]</ref> proposed to explore the contrast cue, the spatial cue, and the corresponding cue to detect the co-salient regions in multiple related images. More recently, Zhang et al. <ref type="bibr" target="#b16">[17]</ref> further improved the detection performance by establishing the Bayesian framework to integrate the deep and wide information effectively. As can be seen, co-saliency detection task is a closely related topic to the investigated event saliency problem. However, they also have obvious differences: 1) Co-saliency detection only relies on the visual feature while revealing event saliency needs to explore the video content from more diverse feature modalities like audio, concept and motion. 2) The video collections are obtained in a unconstrained fashion, which leads to more noisiness in the collected videos. 3) The investigated event saliency can be widely used in a broader range of multimedia tasks like video search and video browsing.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Video Saliency and Video Co-segmentation</head><p>Given a video sequence, video saliency <ref type="bibr" target="#b19">[20]</ref>- <ref type="bibr" target="#b21">[22]</ref> aims at combining the spatial and temporal information to infer the salient object regions of each video frame. The key issue is how to incorporate informative motion cue and the traditional contrast cue that is widely used in the saliency detection for still images. Specifically, Fang et al. <ref type="bibr" target="#b19">[20]</ref> first generated the spatial and temporal saliency maps separately and then merged them into one by a spatiotemporally adaptive entropy-based uncertainty weighting approach. To explore the short-term continuity, Li et al. <ref type="bibr" target="#b20">[21]</ref> proposed a unified regional framework which considered the intraframe saliency and interframe saliency based on the multiple features of color and motion.</p><p>Video (foreground) co-segmentation <ref type="bibr" target="#b22">[23]</ref>- <ref type="bibr" target="#b25">[26]</ref> aims at jointly segmenting the main common objects present in a given set of videos. Unlike object segmentation in the each single video or image, the co-occurrence of common foreground objects in multiple frames of the given multiple videos needs to be explored additionally. For example, Zhang et al. <ref type="bibr" target="#b23">[24]</ref> addressed object co-segmentation in arbitrary videos via a Regulated Maximum Weight Clique (RMWC) extraction scheme, which integrated the components of sampling, tracking and matching object proposals. Recently, Fu et al. <ref type="bibr" target="#b24">[25]</ref> proposed a multiple foreground video co-segmentation technique which considered the intra-video coherence of the foreground and the foreground consistency among the different videos.</p><p>Compared with the video saliency and video cosegmentation, the event saliency problem investigated in this paper not only needs to explore the object appearance and motion information, but also needs to incorporate more diverse feature modalities like audio and concept. Essentially, event saliency aims at revealing curt video fragments that can concisely portray the underlying event of interest rather than highlighting certain objects without considering the corresponding action or event.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>III. THE PROPOSED APPROACH A. Algorithm Overview</head><p>Given a video collection containing M videos, i.e., {V m } M m=1 , sharing the same event of interest, our goal is to predict event saliency scores to reflect the probabilities of each temporal location in each video to be the event of interest. First, we decompose each video V m into multiple video shots {Sh m k } nm k=1 , where n m indicates the number of shots in the m-th video, by adopting the well-established shot boundary detection techniques <ref type="bibr" target="#b26">[27]</ref>. By using the shot boundary detection techniques <ref type="bibr" target="#b26">[27]</ref> (with the default setting), each video will be decomposed into different number of video shots, which is based on the content of the input video. The size of video shot may affect the performance but it is not a problem of this paper, because the size of video shot outputted by the shot boundary detection algorithm is constant. Summing all the shots in the video collection together leads to {Sh K } N K=1 , where N = M m=1 n m . Then, we extract the multi-modality feature f K from each video shot Sh K , leading to the multimodal understanding of the video content. Based on the extracted multi-modality features, we adopt the Kmeans algorithm <ref type="bibr" target="#b27">[28]</ref> to cluster all the video shots in the given video collection into n c clusters and thus obtain {C i } nc i=1 . Afterwards, based on the intuitions that the event of interest should appear frequently in most of the videos of the video collection, have high consistency in the feature modalities, and tend to present near the center of a video, we propose to explore the distribution prior dp, the compactness prior cp, and the location prior lp to establish the informative intra-cluster prior term E ICP (p), where p = [p 1 , p 2 , • • • p nc ] T indicates the probability of each shot cluster to be the event of interest, i.e., {P (C i |E)} nc i=1 . From the above description, we can see that the intracluster prior is mainly used to explore the helpful priors of each individual cluster. However, due to the complex content contained by the given video collection, only exploring such intra-cluster priors cannot work well in practice. Essentially, there are rich relationship among different clusters that can be used to further improve the event saliency detection results: On one hand, the representation of the salient clusters is distinct from the representation of the background clusters as the event contained by the former (e.g. "Kneel with ring" in Fig. <ref type="figure" target="#fig_0">2</ref>) is different from those contained by the latter (e.g. "Get out of car" and "Show the sign" in Fig. <ref type="figure" target="#fig_0">2</ref>). On the other hand, video shots containing the same event might be clustered into multiple clusters rather than one cluster (which is the ideal  <ref type="bibr" target="#b19">(20)</ref> and solve it to obtain the event saliency score P (C i |E) for each cluster as in Section 3.2.4; 5 Calculate P (Sh K |E) for each shot via Eq (21); case). In this scenario, exploring the relationship among different clusters could also help assigning such clusters with similar saliency scores, which better highlights the salient clusters and suppresses the background ones. Based on the above analysis, we propose to further explore two kinds of inter-cluster relationship, namely, the inter-cluster discriminability E ICD (p) and the inter-cluster smoothness E ICS (p), for detecting event saliency. To be specific, the former is used to separate the clusters containing the event of interest from other background clusters, while the latter is used to encourage the clusters containing similar contents to obtain the similar event saliency scores.</p><p>Finally, we embed the intra-cluster prior, inter-cluster discriminability and inter-cluster smoothness into a unified optimization objective and solve it as a standard quadratic programming problem. Thus, the event saliency scores of all the shots {P (Sh K |E)} N K=1 can be obtained. The overall framework is illustrated in Fig. <ref type="figure" target="#fig_0">2</ref>. The detailed algorithm is detailed in Algorithm 1.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Multi-modality Feature Extraction</head><p>For comprehensively understanding the content of each video shot, in each video shot, we extract four types of features from multiple modalities to build the shot-level feature representation. The detailed description of such features is in <ref type="bibr" target="#b28">[29]</ref>.</p><p>Deep visual feature: First, we explore the visual modality feature by using the current deep learning technique <ref type="bibr" target="#b29">[30]</ref>.</p><p>Specifically, we adopt the VGG network trained on 1.2 million ImageNet challenge images with 1,000 class labels to extract the deep visual feature. The input of the network is the center frame (resized to 224224) of each video shot and the output is the extracted 1000-dimensional feature vector. The extracted features encode mid-level visual semantics and thus can effectively reflect the video content.</p><p>Motion trajectory feature: The second type of feature explores the motion cues. Specifically, we adopt the widely used motion trajectory feature <ref type="bibr" target="#b30">[31]</ref>, which extracts feature from each video shot. This feature is mainly based on the dense trajectory between temporally consecutive frames with the video shot. In addition, the motion component with respect to the camera motion is estimated and removed, which further improves the representation capability.</p><p>Concept feature: Concept feature is an important highlevel video (shot) description that is widely explored in recent works <ref type="bibr" target="#b31">[32]</ref>, <ref type="bibr" target="#b32">[33]</ref> for extracting useful information in conception modality. By using the sharp tool developed by <ref type="bibr" target="#b28">[29]</ref>, we can extract the concept feature consisting of concepts trained on about 0.35 million shots provided by TRECVID Semantic Indexing 2012 track. Each dimension of the extracted feature corresponds to a confidence score of detecting a certain semantic concept.</p><p>Automatic speech recognition feature (ASR): For exploring the content of each video shot from speech modality, we adopt the ASR feature, which is extracted by directly inputting each video shot into the toolkit built by <ref type="bibr" target="#b28">[29]</ref>. The toolkit builds the HMM (Hidden Markov Model)/GMM (Gaussian Mixture Model) acoustic model with speaker adaptive training, where the models are trained from instructional video data <ref type="bibr" target="#b33">[34]</ref>. During the ASR feature extraction, the toolkit first utilizes Janus's algorithm <ref type="bibr" target="#b34">[35]</ref> to segment out speech segments and then generates the best hypothesis for each utterance.</p><p>After extracting the visual, motion, concept, and ASR features of each video shot, we adopt principle component analysis (PCA) to reduce the dimension of each modality feature to 250 and then splice the dimension-reduced features together to build the multi-modality feature representation for each video shot, leading to the 1000-dimensional feature vector f K . The process of building such multimodality feature representation is shown in Fig. <ref type="figure" target="#fig_2">3</ref>. Notice that even though some of the adopted features have been pre-trained on some other tasks and data, we think our approach could still be considered as unsupervised because all these features are extracted without using the groundtruth data related to the investigated task. More specifically, during the implementation, we directly use the online accessible toolkit to extract these features without using any ground-truth to fine tune them.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Event Saliency Detection</head><p>1) Intra-cluster Prior: Distribution prior: To measure how the video shots within a certain cluster distribute on the multiple videos in the given video collection, we propose to leverage a distribution histogram in this paper. Specifically, for the cluster C i , the corresponding distribution histogram</p><formula xml:id="formula_0">h i = [h i 1 , h i 2 , • • • , h i M ] T is calculated as h i m = 1 n i nm k=1 I(Sh m k ∈ C i ), m = 1 . . . M,<label>(1)</label></formula><p>where I(•) is the indicator function equaling 1 when Sh k ∈ C i and vice versa, n i indicates the number of shots in the i-th cluster. As shown in Fig. <ref type="figure">4</ref>, such distribution histogram can successfully encode the information about whether the video shots in a certain cluster frequently appear in different videos. Then, to quantitatively measure the repetitiveness of the shots in each cluster based on the distribution histogram, we calculate dp = [dp 1 , dp 2 , . . . , dp nc ] T as:</p><formula xml:id="formula_1">dp i = 1 var(h i ) + 1 ,<label>(2)</label></formula><p>where var(h i ) denotes the variance of the distribution histogram h i . As can be seen, the cluster with the high dp i would contain the video shots evenly distributed in each video in the given video collection.</p><p>Compactness prior: Based on our observation, the salient shots from different videos should have higher consistency, at least in some of feature domains than the cluttered video background (see Fig. <ref type="figure" target="#fig_4">5</ref>). Thus, we calculate the compactness of a cluster to explore the consistency of the shots within it. Specifically, as suggested in <ref type="bibr" target="#b35">[36]</ref>, we measure the compactness prior cp = [cp 1 , cp 2 , . . . , cp nc ] T by using the χ 2distance, where</p><formula xml:id="formula_2">cp i = 1 n i Sh K ∈Ci exp -χ 2 (c i , f K ) ,<label>(3)</label></formula><formula xml:id="formula_3">χ 2 (c i , f K ) = D d=1 (c d i -f d K ) 2 c d i + f d K , (<label>4</label></formula><formula xml:id="formula_4">)</formula><p>c i is the D-dimensional feature of the i-th cluster center.</p><p>Location prior: In analogy to images, where the object of interest is usually located at the center of the image, we found that the event of interest tends to present near the center of a video. We verify this hypothesis on MED data and plot examples in Fig. <ref type="figure" target="#fig_1">1</ref> where the videos containing the event of interest usually as well contain a lone story, which includes the cause, course, results of the event. Thus, it substantiates the intuition. To measure the location prior The overall intra-cluster prior: After obtaining the distribution prior dp, compactness prior cp, and location prior lp of each cluster based on Eq. 2, 4, and 5, respectively, the overall intra-cluster prior term can be formulated as:</p><formula xml:id="formula_5">E ICP (p) = p T u,<label>(6)</label></formula><p>where</p><formula xml:id="formula_6">u = [u 1 , u 2 , • • • u nc ] T and u i = exp(-dp i • cp i • lp i ).</formula><p>2) Inter-cluster Discriminability: The inter-cluster discriminability regards that the clusters containing the event of interest, e.g., surfing in the sea, are often distinctive as compared with the background clusters, e.g., walking along the sand and swimming in the sea. To formulate this property, we adopt a linear discriminative technique introduced in <ref type="bibr" target="#b36">[37]</ref>. The main idea is to leverage the squared loss function, which is usually used for linear classification, as a discriminative cost function and, intuitively, the goal is to look for solutions which have highly potential to be linearly separable.</p><p>Specifically, we represent the given n c cluster center features in a feature matrix C ∈ nc×D . Following <ref type="bibr" target="#b36">[37]</ref>- <ref type="bibr" target="#b38">[39]</ref>, in the binary case where the label is associated with the i-th cluster y i ∈ {0, 1}, the regularized linear regression problem of y = [y 1 , y 2 , ..., y nc ] T given C takes the form:</p><formula xml:id="formula_7">min w∈ d ,b∈ 1 n c nc i=1 (y i , wc i + b) + κ D ||w|| 2 2 , (<label>7</label></formula><formula xml:id="formula_8">)</formula><p>where w is the D dimensional weight vector, b is the bias, and (•) is a loss function. As suggested by <ref type="bibr" target="#b36">[37]</ref>, we adopt the square loss in this paper, i.e., (y i , wc i + b) = ||y i -wc i -b|| 2 2 , which can lead the cost function, i.e., Eq. ( <ref type="formula" target="#formula_7">7</ref>), to be simply expressed in the closed form, amenable to specific efficient convex optimization algorithms, and capable of dealing with large-scale data <ref type="bibr" target="#b36">[37]</ref>- <ref type="bibr" target="#b38">[39]</ref>. To be specific, Given the feature matrix C (which is known and fixed) and the labels y (which are unknown), we denote the optimal value E(y) as the measure of the discriminability of the classes defined by y ∈ {0, 1} nc . Then, we can compute E(y) in closed form as:</p><formula xml:id="formula_9">E(y) = y T Ay, s.t.y ∈ {0, 1} nc ,<label>(8)</label></formula><p>where</p><formula xml:id="formula_10">A = 1 n c (Π nc (I nc -C(C T Π nc C + n c κI nc ) -1 C T )Π nc ),<label>(9)</label></formula><formula xml:id="formula_11">Π nc = I nc - 1 n c 1 nc 1 T nc .<label>(10)</label></formula><p>As minimizing Eq. ( <ref type="formula" target="#formula_9">8</ref>) turns out to be a NP hard problem, we relax the boolean constraints to linear and continuous ones, which allows the optimized variables to take any value between 0 and 1. Consequently, we can relax the constraint from y ∈ {0, 1} nc to p ∈ [0, 1] nc and then obtain the objective function for the inter-cluster discriminability term:</p><formula xml:id="formula_12">E ICD (p) = p T Ap, s.t.p ∈ [0, 1] nc . (<label>11</label></formula><formula xml:id="formula_13">)</formula><p>3) Inter-cluster Smoothness: To explore the inter-cluster smoothness, we adopt the Deformed Graph Laplacian (DGL) <ref type="bibr" target="#b39">[40]</ref> to improve the smoothness of the variable p by considering the information cue of inter-cluster smoothness. Different from the traditional graph Laplacian, the DGL additionally regularizes the data which corresponds to a low degree in the established graph. As the data only having low degree in graph often confuses the learning procedure, DGL can weaken the negative influence brought by such data and thus improve the learning performance.</p><p>To implement such smoothness term, we first compute a n c × n c similarity matrix S based on the extracted shot features. Specifically, we also adopt the χ 2distance to calculate the similarity matrix S, where</p><formula xml:id="formula_14">S ij = exp -χ 2 (c i , c j ) .<label>(12)</label></formula><p>Then, the diagonal matrix D can be obtained by</p><formula xml:id="formula_15">D ij = nc j=1 S ij , i = j 0, otherwise<label>(13)</label></formula><p>Afterwards, the inter-cluster smoothness term can be formulated as:</p><formula xml:id="formula_16">E ICS (p) = p T Lp = p T Lp + p T (I -D/v)p<label>(14)</label></formula><p>where the first term, which can be reformulated as</p><formula xml:id="formula_17">p T Lp = 1 2 nc i=1 nc S ij (p i -p j ) 2 ,<label>(15)</label></formula><formula xml:id="formula_18">L = D -S,<label>(16)</label></formula><p>is the traditional graph Laplacian used to model the smoothness relationship between clusters. In addition, the second term, which can be reformulated as</p><formula xml:id="formula_19">p T (I -D/v)p = nc i=1 (1 -d ii /v)p 2 i ,<label>(17)</label></formula><formula xml:id="formula_20">d ii = nc j=1 S ij , v = nc i=1 d ii ,<label>(18)</label></formula><p>uses d ii to record the connective strength for each cluster. It enforces the clusters with large d ii to obtain a confident probability p i and the clusters with low degree d ii to receive a relatively weak probabilities. Finally, Eq. ( <ref type="formula" target="#formula_16">14</ref>) can thus provide an effective and robust way to smooth the probability values in p.</p><p>4) The Overall Objective: By combining the above described intra-cluster prior term E ICP , inter-cluster discriminability term E ICD , and inter-cluster smoothness term E ICS presented in Eq. ( <ref type="formula" target="#formula_5">6</ref>), <ref type="bibr" target="#b10">(11)</ref>, and ( <ref type="formula" target="#formula_16">14</ref>), respectively, we can obtain the overall objective of the optimizing problem in our formulation:</p><formula xml:id="formula_21">min p E ICP (p) + E ICS (p) + E ICD (p) s.t. p ∈ [0, 1] nc . (<label>19</label></formula><formula xml:id="formula_22">)</formula><p>which is equivalent to</p><formula xml:id="formula_23">min p p T ( L + A)p + p T u s.t. p ∈ [0, 1] nc .<label>(20)</label></formula><p>Since Eq. ( <ref type="formula" target="#formula_23">20</ref>) is a quadratic programming problem in its primal form, we utilize the interior-point-convex algorithm <ref type="bibr" target="#b40">[41]</ref> to solve it. It also needs to mention that even though the quadratic programming might hardly be solved efficiently in large-scale problem, it would not be an issue in the proposed framework as we have already converted the shot-level label assignment problem to cluster-level, which could largely reduce the computational cost. After obtaining the solution p, i.e., the conditional probability P (C i |E), i = 1, 2, • • • , n c , via optimizing Eq. 20, we can predict the posterior probability of each shot to contain the event of interest as:</p><formula xml:id="formula_24">P (Sh K |E) = nc i=1 P (Sh K |C i , E)P (C i |E), P (Sh K |C i , E) = exp(-||f K -c i || 2 2 ).<label>(21)</label></formula><p>As proofed in the Appendix, the probability interpretation behind the proposed optimization objective Eq. 20 is to maximize the joint conditional probability P (p, C i |E) with the assumption that P (C i |p, E) and P (p) follow the multivariate Gaussian distribution and Laplasian like distribution, respectively.</p><p>IV. EXPERIMENTS A. Experiment Setups 1) Benchmarks: We evaluate the proposed method on three public benchmarks. The first benchmark is called MED Summarize benchmark <ref type="bibr" target="#b7">[8]</ref>. Its test set, which is used in our experiments, provides annotations for 2474 shots and 416965 frames. It is originally used for the categoryspecific video summarization tasks, in which the groundhas four levels of relevancy. In order to evaluate the performance of the methods used for solving the investigated event saliency revealing problem, we relabelled the ground-truth labels of these data to obtain the binary labels to indicate whether a certain shot in a video is salient (i.e., could provide key evidence to understand the video content) or not. Specifically, we generated the ground-truth by using the video segments that are labeled (by <ref type="bibr" target="#b7">[8]</ref>) as having the capability to classify the videos to the corresponding event categories alone. Such video segments have the highest relevance to the underlying event of interest. Thus, they are consistent with the event saliency defined in this paper to a large extent.</p><p>second and third public benchmarks are from the TRECVID MED benchmark. NIST TRECVID recently released shot level annotations for some videos in the multimedia event detection task <ref type="bibr" target="#b8">[9]</ref>. The second benchmark, called MEDsal1, contains annotations over 7,699 shots and 644,225 frames. A video either belongs to one of the 10 pre-specified events or does not belong to any of these event. The third benchmark called MEDsal2 is annotated in similar setting but on a much lager collection, which has 46,355 shots over 4,537,035 frames. In our experiments, we did not use the manual annotations in training.</p><p>2) Evaluation Metrics and Implementation Detail: To conduct quantitative evaluation, we adopted three common criteria, which are the average precision (AP) score, the mean F-score (mF), and the AUC score , respectively. These criteria are widely used in evaluation of the multimedia and computer vision tasks <ref type="bibr" target="#b11">[12]</ref>, <ref type="bibr" target="#b15">[16]</ref>, <ref type="bibr" target="#b41">[42]</ref>. Basically, the ground-truth of the investigated task is the binary shotlevel value to indicate which shots are the salient ones. The prediction results of our method as well as other compared methods are the shot-level real-valued scores ranging from 0 to 1. We can then obtain the frame-level scores by assigning the shot-level score to the frames residing in this shot. Afterwards, the evaluation metrics are used to compare the prediction scores with the ground-truth to see how well the prediction matches the ground-truth.</p><p>Specifically, the PR curve and AP score are generated by binarizing the frames in a video sequence via varying the quantification threshold within the range of [0, 1]. The resulting true positive rate (or the recall rate) versus precision rate at each threshold value forms the PR curve. The under the PR curve is calculated as the AP score. Similarly, ROC curve is generated based on the false positive rate and the true positive rate, and the AUC score is obtained by calculating the area under the ROC curve. Specifically, The precision rate P RE, true positive rate T P R and false positive rate F P R values are defined as</p><formula xml:id="formula_25">P RE = |SF ∩ GF |/|SF |, (<label>22</label></formula><formula xml:id="formula_26">) T P R = |SF ∩ GF |/|GF |, (<label>23</label></formula><formula xml:id="formula_27">) F P R = |SF ∩ GB|/|GB|,<label>(24)</label></formula><p>where SF , GF and GB denote the set of frames labeled as 1 after the binarization process under a certain threshold, the set of ground truth frames containing the event of interest and the set of ground truth background frames, respectively. Under each threshold, we also calculate the corresponding F-score as:</p><formula xml:id="formula_28">F β = (1 + β 2 )P RE × T P R β 2 × P RE + T P R (<label>25</label></formula><formula xml:id="formula_29">)</formula><p>where β 2 = 0.3 as suggested in <ref type="bibr" target="#b42">[43]</ref>. We evenly choose a set of thresholds (range from 0 to 1) to calculate the corresponding F-scores. Then the mean value of the Fscores under all the thresholds is the mean F-score (mF).</p><p>During our implementation, one parameter that needs to be assigned in advance is the cluster number n c . As the entire framework of the proposed approach is performed in an unsupervised fashion, no ground-truth data can be used to tune this parameter. Consequently, we could only assign the number of clusters in an intuitive way. As we know, the parameter n c determines how many clusters would be generated to describe the content of the video collection. Intuitively, the content of the video collections containing less number of videos should be less complex than the content of the video collections containing more videos. Thus, the video collections containing less videos should be assigned with a smaller value to this parameter and vice versa. According to the above intuition, we adopt the linear relationship between the cluster number and video number, i.e., n c = M , as non-linear relationships, e.g., n c = M 2 , could dramatically increase computational cost in the following optimization procedure.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Performance Comparison With the Baselines</head><p>In this section, we compare the proposed approach with three baseline methods, i.e., the category-specific video summarization baseline (CVS), the rank constraint-based self-adaptively weighted fusion baseline (RCF), and the deep reconstruction-based common pattern mining baseline (DRM), on the event saliency revealing task. The concrete descriptions of these baselines are listed below:</p><p>Category-specific video summarization (CVS): CVS <ref type="bibr" target="#b7">[8]</ref> aims at summarizing the important video fragments among the given category-specific video collection. Different from the conventional video summarization methods, CVS tends to be more related to the investigated event saliency revealing task and can be treated as one baseline model of the proposed approach as in category-specific video collection, the important video fragments can usually help to understand why these videos belong to a certain event category. For building this baseline, given a video collection, we treat the videos in this collection as the positive samples while the videos in other collections of the corresponding dataset as the negative ones. Similar to <ref type="bibr" target="#b7">[8]</ref>, we extract the multi-modality representation of a video as if the whole video is a single shot, and then use the extracted representations of the positive and negative videos to train binary support vector machine (SVM) classifier. In the prediction phase, we adopt the learned SVM to produce the scores for each shot in the current video collection. As can be seen, this is a supervised baseline method. To avoid overfitting, we follow <ref type="bibr" target="#b43">[44]</ref> to divide each of the video collection into non-overlapped 10 parts. At each time, we use the 9 parts for training and the rest one for predicting. Thus, after 10 times training-predicting, we can obtain all the prediction scores.</p><p>Rank constraint-based self-adaptively weighted fusion (RCF): This baseline model is to use the rank constraint to generate the self-adaptive weights for integrating the intra-cluster priors, i.e., the distribution prior, compactness prior, and location prior, in the proposed method. As inspired by <ref type="bibr" target="#b44">[45]</ref>, we first construct a feature matrix to encode the features of the salient shots determined by the three intra-cluster priors on all the M videos. The idea is that the extracted representations of the salient clusters should be both similar and consistent, so the matrix rank appears low. Thus, such consistency property can be formulated as the rank constraint. Specifically, we obtain the adaptive weighs of each prior by solving a low rank matrix recovery problem, which can be relaxed as a Robust Principal Component Analysis (RPCA) <ref type="bibr" target="#b45">[46]</ref> with using the nuclear norm for the low rank matrix and the 1 -norm for the error matrix. Finally, the elements in the obtained error matrix are used to generate the weights for integrating the intra-cluster priors to obtain the final prediction.</p><p>Deep reconstruction-based common pattern mining (DRM): Deep reconstruction residual has been demonstrated to be able to extract intrinsic and general hidden patterns from the input data in an unsupervised fashion <ref type="bibr" target="#b2">[3]</ref>, <ref type="bibr" target="#b15">[16]</ref>. Thus, it can be used to discover the homogeneity of the common event among the video collection. To this end, we implement this baseline method by applying the multi-modality representations of the fragments with high intra-cluster prior u into a stacked denoising autoencoder (SDAE). SDAE is one kind of state-of-the-art unsupervised deep learning models which seeks to exploit the unknown structure in the input distribution at multiple layers to make the learned higher-level representations more abstract and informative. Thus, the SDAE models trained by using such fragments can capture the intrinsic pattern of the event saliency and thus the deep reconstruction residual generated by reconstructing the video shots using such deep model can be used to calculate the final saliency scores of the shots in each video.</p><p>The quantitative evaluation results on the MEDsal1, MEDsal2, and MED Summarize benchmarks are shown in Table <ref type="table" target="#tab_0">I</ref>, II, III, and IV. To be specific, the evaluation results of each event collection on the MEDsal1, MEDsal2, and MED Summarize benchmarks are reported in Table <ref type="table" target="#tab_0">I</ref>, II, and III, respectively. The evaluation results of the overall performance on these benchmarks are reported in Table <ref type="table" target="#tab_3">IV</ref>. As can be seen, the proposed approach can achieve superior overall performance than other baseline methods on all these three benchmarks in terms of the AP, AUC, and mF. To better analyze the performance of different approaches, we count the number of event categories where a certain approach can achieve better performance than the others under different evaluation metrics. The statistic result shows that on all the these benchmarks, the proposed Fig. <ref type="figure">6</ref>. Visualization results to show the prediction scores for some example frames in three videos from the video collection of 'Renovating Building', 'Metal Crafting', and 'Flash Mob', respectively. approach can achieve the outperforming performance on more event categories than any other baseline method, which explains the statistically significant improvement of the overall performance obtained by the proposed approach. Some specific example results are shown in Fig. <ref type="figure">6</ref>.</p><p>Based on our inspection of the experimental results, the superior performance obtained by the proposed approach is mainly due to the following reasons: compared with the RCF baseline, the proposed approach can not only leverage the useful priors in the intra-cluster prior component but also can explore another two important components, i.e., the inter-cluster discriminability and inter-cluster smoothness. Compared with the DRM baseline, the proposed approach can jointly explore the intra-cluster prior, intercluster discriminability and inter-cluster smoothness in a unified optimization framework rather than exploring them in a cascade manner. Compared with the CVS baseline, the event saliency discovered by the proposed approach is the intrinsic stimulus directly toward understanding the video content, whereas the instance discovered by CVS could only be used to explain the event detectors trained based on the video-level event labels. However, there are still some challenging cases for the proposed approach. For example, when the video frames contain complex image background or very blurred foreground motion, the features extracted from the video shots are not able to characterize the corresponding shot contents. In this case, the proposed approach can hardly reveal event saliency precisely.</p><p>Table IV also lists the average running time (without including the time for feature extraction 1 ) in processing a video collection using different approaches. All experiments were run on a 24-core Lenovo Server with an Intel Xeon CPU of 2.8-GHz and 64-GB RAM. All the compared methods were implemented using MATLAB. As can be seen, the proposed algorithm has the least running time. Compared with the RCF and DIR baseline, the less running time of the proposed approach indicates that the proposed optimization algorithm has less computational complexity compared with the rank constraint-based algorithm and the deep reconstruction-based algorithm. Compared with the CVS baseline, the less running time of the proposed approach demonstrates the efficiency of the cluster-level 1 In terms of time needed to process 1 hour video on a single CPU or GPU, the extraction of visual, motion, concept, and ASR features costs 1/20, 8, 1/10, and 5 hours, respectively. As these features were used in all the baseline methods, we did not report them in Table <ref type="table" target="#tab_3">IV</ref> for comparing the running time of different event saliency revealing methods more clearly. optimization strategy.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Feature Analysis</head><p>In this section, we analyze the contribution of the four types of multimodal feature adopted in our experiments. To be specific, we compare the proposed approach with four baseline settings which only use the ASR features, CNN features (i.e., the deep visual feature), concept features, and motion trajectory features, respectively. The experimental results on the MEDsal1 and MEDsal2 benchmarks are shown in Table . V, from which we can see that: 1) The lowlevel motion trajectory feature cannot handle the investigated event saliency revealing task well due to the complex motion environment caused by both the foreground and background objects. 2) Based on the higher-level knowledge learned from the existing large-scale datasets, the deep visual feature, ASR feature, and concept feature can work better than the motion trajectory feature. 3) Combining the features of diverse modalities can further enrich the representation capability, which leads to the consistent superior performance, especially in terms of the mF score.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D. Component Analysis</head><p>To understand the contribution of each component in the proposed framework, which includes the intra-cluster prior, inter-cluster discriminability, and inter-cluster smoothness, we conduct leave-one-component-out evaluations in this section, where the performance drop, after removing a certain component, can be used to estimate the contribution of the corresponding component to the full system. The experimental results in the MEDsal1 and MEDsal2 benchmarks are shown in Table . VI, from which we can observe that: 1) all the three components are beneficial to the full system as each of them can cause obvious performance drop, especially in terms of mF. 2) Basically, the average performance drop (i.e., the contribution) on the two benchmarks follows: inter-cluster discriminability &gt; inter-cluster smoothness &gt; intra-cluster prior, and this trend appears to be more obvious on the MEDsal2 benchmark. Actually, in this paper, we only explore several simple and intuitive prior information to construct the intra-cluster prior, which suggests that future works with exploring more and better prior information from the video collections may further improve the performance. 3) Obvious performance gain obtained by the proposed approach as compared with  the three baseline methods, i.e., the w/o ICP, the w/o ICS, and the w/o ICD, further demonstrates the rationality of the formulated optimization problem and the effectiveness of its solution to the investigated task.</p><p>For further demonstrating the effectiveness of the distribution prior (DP), the compactness prior (CP), and the location prior (LP), we also conduct the leave-one-prior-out evaluation, where the performance drop, after removing a certain prior, can be used to estimate its effectiveness in improving the final performance. The experimental results in the MEDsal1 and MEDsal2 benchmarks are shown in Table . VII, from which we can observe that all the three priors can improve the performance of the proposed approach. The location prior, to some extent, provides more helpful prior information than the other two priors.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E. Robustness Analysis</head><p>In the investigated task, one critical challenge is caused by the noisy videos collected from the internet. This is a practical issue existed in any multimedia processing task towards real-world applications. To this end, we comprehensively test and analyze the capability of the proposed approach for handling such noisy videos in this section.</p><p>The basic idea of the experiment is to analyze the performance change of the proposed approach during mixing different percentages of the noisy data progressively. For a certain video collection on an event of 90 videos, we randomly select 10 videos from other video collections as the noisy videos and add them into the current video collection. Then, we report the experimental result of this video collection (in terms of AP, AUC, and mF) as a hollow circle in each of the figures of Fig. <ref type="figure" target="#fig_5">7</ref>. After obtaining experimental results of video collections in all event categories, we report the overall performance of the benchmark under this noisy level as a solid circle in each of the figures of Fig. <ref type="figure" target="#fig_5">7</ref>. In this way, we gradually add 10 more noisy videos into each video collection in each time and totally conduct 15 times experiments, leading to the amount and percentage of the noisy videos in each video collection from 10 to 150 and 0.1 to 0.625, respectively. To alleviate the computational cost, we fix the cluster number to 10 in this experiment.</p><p>From the experimental results in Fig. <ref type="figure" target="#fig_5">7</ref>, we can observe that when the percentage of the involved noisy videos is less than 0.5, the performance of the proposed approach is weaken slightly, i.e., about 0.1, 0.03, and 0.09, in terms of AP, AUC, and mF, respectively. Whereas when the percentage of the involved noisy videos is larger than 0.5, the proposed approach would suffer from much more dramatic performance drop. Thus, this experiment demonstrates that the proposed approach can robustly handle the noisy issues existed in the multimedia systems containing less than 50 percent noisy data. This is a reasonable accuracy level which should be satisfied by recent video retrieval algorithm, e.g., <ref type="bibr" target="#b46">[47]</ref>. Essentially, the main factor considered in the proposed approach to handle the noisy issue is the "soft" cluster-level inference scheme. Under this scheme, when some noisy videos are involved, these noisy videos can only import slight influence on the salient clusters as the content in the noisy videos are more consistent with the content of the background clusters than the salient ones. Thus, the subsequent inference can still figure out the salient clusters. However, when the noisy videos are dominant in the video collection, the entire feature space would become problematic, leading to the unreasonable clusters. Furthermore, the prior knowledge used in the inference process also become to be unreliable. This explains the trend of performance shown in Fig. <ref type="figure" target="#fig_5">7</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>V. CONCLUSION AND FUTURE WORK</head><p>In this paper, we proposed to detect event saliency towards the task-free video event understanding in unconstrained video collections. By comprehensively analyzing the challenges in the event saliency revealing task, we designed a novel framework which can hopefully highlight the intrinsic stimulus of the unknown event among a certain video collection and thus it could benefit to a wide range of multimedia tasks. Comprehensive experiments on three benchmarks, including two newly established ones and the modification of an existing one, demonstrated the notable performance of the proposed approach as well as the features and components utilized in our framework.</p><p>Essentially, there are still some open problems (e.g., How to effectively integrate features from different modalities to build stronger feature representation (feature fusion)? What if combining the prediction results of using different feature modalities to obtain stronger event saliency prediction (decision fusion)? Is the former more effective than the latter? If there any other helpful prior knowledge could be used to explore the patterns of event saliency?) that need to be addressed. In the future, we will try to address these issues by conducting cross-modality analysis <ref type="bibr" target="#b47">[48]</ref> with feature selection <ref type="bibr" target="#b48">[49]</ref> and designing novel event saliency revealing frameworks and further apply them to boost the performance of the multimedia systems like video retrieval <ref type="bibr" target="#b46">[47]</ref> and video quality assignment <ref type="bibr" target="#b49">[50]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>VI. APPENDIX</head><p>Suppose P (E|C i ) ∼ N (p; 0, Σ) the multivariate Gaussian distribution, where Σ = ( L + A) -1 is the co-variance matrix. We have:</p><formula xml:id="formula_30">P (E|C i ) ∝ exp(-p T Σ -1 p).<label>(26)</label></formula><p>By assuming P (C i ) follows the Laplasian like distribution, where θ derived from the prior knowledge which equals exp{-D p • C p • L p }, we obtain:</p><formula xml:id="formula_31">P (C i ) ∝ exp(-θ p ).<label>(27)</label></formula><p>Consequently, the posterior probability P (C i |E) ∝ P (C i , E)/P (E) can be maximized by (P (E) follows a uniform distribution):  </p><formula xml:id="formula_32">P (C i</formula><p>Above equations proofs the probability interpretation behind the proposed optimization objective Eq. ( <ref type="formula" target="#formula_23">20</ref>).</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 2 .</head><label>2</label><figDesc>Fig. 2. The framework of the proposed approach.</figDesc><graphic coords="4,81.44,53.14,425.20,131.78" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Algorithm 1 :</head><label>1</label><figDesc>Overview of the proposed algorithm input : A collection of videos {V m } M m=1 output: The event saliency scores for each shot P (Sh K |E), K = 1, 2, • • • , N 1 Extract shots in each video and obtain {Sh K } N K=1 ; 2 Extract multi-modality feature {f K } for each shot; 3 Calculate u to build E ICP (p) as in Sec. III-C1; Calculate A to build E ICD (p) as in Sec. III-C2; Calculate L to build E ICS (p) as in Sec. III-C3; 4 Establish the overall objective Eq</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 3 .</head><label>3</label><figDesc>Fig. 3. Illustration of the proposed multi-modality feature representation.</figDesc><graphic coords="5,306.18,53.14,226.76,119.55" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Fig. 4 . 2 ,</head><label>42</label><figDesc>Fig. 4. Examples to show that the proposed distribution histogram can provide informative prior to reflect whether the video shots in a certain cluster frequently appear in different videos.</figDesc><graphic coords="6,67.27,53.14,453.55,109.12" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Fig. 5 .</head><label>5</label><figDesc>Fig.5. Examples to show the compactness of different clusters in the event of "marriage proposal". As can be seen, the cluster containing the event of interest tends to be more compact than the clusters containing the clustered background events.</figDesc><graphic coords="6,306.18,199.86,226.77,129.22" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Fig. 7 .</head><label>7</label><figDesc>Fig. 7. Experimental results for noise analysis on the MEDsal2 benchmark. The dashed lines are the results of each image collection of a specific event category, while the bold lines are the overall performance of the benchmark.</figDesc><graphic coords="12,81.44,53.14,425.20,144.65" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head></head><label></label><figDesc>|E) ∝ max p P (C i )P (E|C i ) = max p exp(-p T Σ -1 p} exp{-θ p ) = min p p T Σ -1 p + θ p .</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>( 28 )</head><label>28</label><figDesc>As p ∈ [0, 1] nc , we haveP (C i |E) ∝ min p p T Σ -1 p + p T u = min p p T ( L + A)p + p T u.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0"><head></head><label></label><figDesc></figDesc><graphic coords="2,81.44,53.14,425.20,179.84" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>TABLE I EVALUATION</head><label>I</label><figDesc>RESULTS OF EACH EVENT COLLECTION ON THE MEDSAL1 DATASET IN TERMS OF AP, AUC, AND MF, RESPECTIVELY.</figDesc><table><row><cell></cell><cell>AP</cell><cell>AUC</cell><cell>mF</cell><cell>AP</cell><cell>AUC</cell><cell>mF</cell><cell>AP</cell><cell>AUC</cell><cell>mF</cell><cell>AP</cell><cell>AUC</cell><cell>mF</cell><cell>AP</cell><cell>AUC</cell><cell>mF</cell></row><row><cell>Event name</cell><cell></cell><cell>Flash Mob</cell><cell></cell><cell></cell><cell>Bike Trick</cell><cell></cell><cell cols="3">Cleaning Appliance</cell><cell></cell><cell>Dog Show</cell><cell></cell><cell cols="3">Marriage Proposal</cell></row><row><cell>RCF</cell><cell>.349</cell><cell>.268</cell><cell>.473</cell><cell>.140</cell><cell>.504</cell><cell>.351</cell><cell>.209</cell><cell>.385</cell><cell>.408</cell><cell>.322</cell><cell>.284</cell><cell>.694</cell><cell>.226</cell><cell>.490</cell><cell>.287</cell></row><row><cell>DRM</cell><cell>.490</cell><cell>.376</cell><cell>.474</cell><cell>.211</cell><cell>.564</cell><cell>.362</cell><cell>.264</cell><cell>.404</cell><cell>.409</cell><cell>.489</cell><cell>.232</cell><cell>.584</cell><cell>.208</cell><cell>.495</cell><cell>.242</cell></row><row><cell>CVS</cell><cell>.475</cell><cell>.342</cell><cell>.453</cell><cell>.184</cell><cell>.482</cell><cell>.213</cell><cell>.084</cell><cell>.335</cell><cell>.450</cell><cell>.115</cell><cell>.276</cell><cell>.330</cell><cell>.093</cell><cell>.383</cell><cell>.390</cell></row><row><cell>OURS</cell><cell>.612</cell><cell>.419</cell><cell>.719</cell><cell>.241</cell><cell>.544</cell><cell>.388</cell><cell>.285</cell><cell>.415</cell><cell>.458</cell><cell>.522</cell><cell>.323</cell><cell>.730</cell><cell>.214</cell><cell>.481</cell><cell>.331</cell></row><row><cell>Event name</cell><cell cols="3">Renovating building</cell><cell cols="3">Rock Climbing</cell><cell cols="3">Hall Meeting</cell><cell cols="3">Racing w/o Vehicle</cell><cell cols="3">Metal crafting</cell></row><row><cell>RCF</cell><cell>.308</cell><cell>.347</cell><cell>.485</cell><cell>.290</cell><cell>.446</cell><cell>.504</cell><cell>.188</cell><cell>.376</cell><cell>.680</cell><cell>.256</cell><cell>.442</cell><cell>.381</cell><cell>.360</cell><cell>.348</cell><cell>.494</cell></row><row><cell>DRM</cell><cell>.191</cell><cell>.364</cell><cell>.220</cell><cell>.356</cell><cell>.395</cell><cell>.574</cell><cell>.363</cell><cell>.428</cell><cell>.688</cell><cell>.329</cell><cell>.457</cell><cell>.383</cell><cell>.323</cell><cell>.511</cell><cell>.443</cell></row><row><cell>CVS</cell><cell>.701</cell><cell>.488</cell><cell>.655</cell><cell>.366</cell><cell>.433</cell><cell>.339</cell><cell>.268</cell><cell>.398</cell><cell>.288</cell><cell>.229</cell><cell>.331</cell><cell>.282</cell><cell>.571</cell><cell>.473</cell><cell>.557</cell></row><row><cell>OURS</cell><cell>.550</cell><cell>.415</cell><cell>.625</cell><cell>.391</cell><cell>.452</cell><cell>.583</cell><cell>.418</cell><cell>.454</cell><cell>.701</cell><cell>.306</cell><cell>.418</cell><cell>.455</cell><cell>.628</cell><cell>.393</cell><cell>.722</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>TABLE II EVALUATION</head><label>II</label><figDesc>RESULTS OF EACH EVENT COLLECTION ON THE MEDSAL2 DATASET IN TERMS OF AP, AUC, AND MF, RESPECTIVELY.</figDesc><table><row><cell></cell><cell>AP</cell><cell>AUC</cell><cell>mF</cell><cell>AP</cell><cell>AUC</cell><cell>mF</cell><cell>AP</cell><cell>AUC</cell><cell>mF</cell><cell>AP</cell><cell>AUC</cell><cell>mF</cell><cell>AP</cell><cell>AUC</cell><cell>mF</cell></row><row><cell>Event name</cell><cell></cell><cell>Flash Mob</cell><cell></cell><cell></cell><cell>Bike Trick</cell><cell></cell><cell cols="3">Cleaning Appliance</cell><cell></cell><cell>Dog Show</cell><cell></cell><cell cols="3">Marriage Proposal</cell></row><row><cell>RCF</cell><cell>.698</cell><cell>.469</cell><cell>.692</cell><cell>.321</cell><cell>.481</cell><cell>.404</cell><cell>.442</cell><cell>.477</cell><cell>.521</cell><cell>.789</cell><cell>.365</cell><cell>.806</cell><cell>.225</cell><cell>.479</cell><cell>.285</cell></row><row><cell>DRM</cell><cell>.727</cell><cell>.500</cell><cell>.703</cell><cell>.328</cell><cell>.419</cell><cell>.395</cell><cell>.483</cell><cell>.452</cell><cell>.525</cell><cell>.779</cell><cell>.361</cell><cell>.787</cell><cell>.253</cell><cell>.521</cell><cell>.300</cell></row><row><cell>CVS</cell><cell>.682</cell><cell>.480</cell><cell>.748</cell><cell>.298</cell><cell>.475</cell><cell>.408</cell><cell>.435</cell><cell>.485</cell><cell>.528</cell><cell>.756</cell><cell>.409</cell><cell>.868</cell><cell>.226</cell><cell>.479</cell><cell>.296</cell></row><row><cell>OURS</cell><cell>.748</cell><cell>.585</cell><cell>.800</cell><cell>.325</cell><cell>.501</cell><cell>.428</cell><cell>.453</cell><cell>.528</cell><cell>.557</cell><cell>.815</cell><cell>.403</cell><cell>.873</cell><cell>.227</cell><cell>.535</cell><cell>.306</cell></row><row><cell>Event name</cell><cell cols="3">Renovating building</cell><cell cols="3">Rock Climbing</cell><cell cols="3">Hall Meeting</cell><cell cols="3">Racing w/o Vehicle</cell><cell cols="3">Metal crafting</cell></row><row><cell>RCF</cell><cell>.610</cell><cell>.461</cell><cell>.665</cell><cell>.630</cell><cell>.498</cell><cell>.740</cell><cell>.732</cell><cell>.401</cell><cell>.780</cell><cell>.493</cell><cell>.453</cell><cell>.626</cell><cell>.586</cell><cell>.458</cell><cell>.657</cell></row><row><cell>DRM</cell><cell>.560</cell><cell>.447</cell><cell>.678</cell><cell>.629</cell><cell>.506</cell><cell>.713</cell><cell>.754</cell><cell>.445</cell><cell>.782</cell><cell>.493</cell><cell>.521</cell><cell>.617</cell><cell>.561</cell><cell>.477</cell><cell>.627</cell></row><row><cell>CVS</cell><cell>.586</cell><cell>.487</cell><cell>.689</cell><cell>.600</cell><cell>.480</cell><cell>.607</cell><cell>.714</cell><cell>.438</cell><cell>.803</cell><cell>.495</cell><cell>.468</cell><cell>.635</cell><cell>.589</cell><cell>.470</cell><cell>.674</cell></row><row><cell>OURS</cell><cell>.636</cell><cell>.498</cell><cell>.732</cell><cell>.653</cell><cell>.537</cell><cell>.757</cell><cell>.771</cell><cell>.450</cell><cell>.848</cell><cell>.518</cell><cell>.534</cell><cell>.676</cell><cell>.614</cell><cell>.521</cell><cell>.717</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>TABLE III EVALUATION</head><label>III</label><figDesc>RESULTS OF EACH EVENT COLLECTION ON THE MED SUMMARIZE DATASET IN TERMS OF AP, AUC, AND MF, RESPECTIVELY.</figDesc><table><row><cell></cell><cell></cell><cell>AP</cell><cell>AUC</cell><cell>mF</cell><cell>AP</cell><cell cols="2">AUC</cell><cell>mF</cell><cell>AP</cell><cell cols="2">AUC</cell><cell>mF</cell><cell>AP</cell><cell cols="2">AUC</cell><cell>mF</cell><cell>AP</cell><cell>AUC</cell><cell>mF</cell></row><row><cell cols="2">Event name</cell><cell cols="3">Birthday party</cell><cell cols="4">Changing tire</cell><cell></cell><cell cols="2">Flash mob</cell><cell></cell><cell cols="4">Vehicle unstuck</cell><cell>Grooming animal</cell></row><row><cell>RCF</cell><cell></cell><cell>.375</cell><cell>.559</cell><cell>.373</cell><cell>.220</cell><cell></cell><cell>.377</cell><cell>.267</cell><cell>.152</cell><cell cols="2">.303</cell><cell>.165</cell><cell>.117</cell><cell cols="2">.301</cell><cell>.057</cell><cell>.713</cell><cell>.646</cell><cell>.593</cell></row><row><cell>DRM</cell><cell></cell><cell>.288</cell><cell>.612</cell><cell>.449</cell><cell>.210</cell><cell></cell><cell>.303</cell><cell>.129</cell><cell>.325</cell><cell cols="2">.688</cell><cell>.317</cell><cell>.133</cell><cell cols="2">.265</cell><cell>.228</cell><cell>.648</cell><cell>.326</cell><cell>.426</cell></row><row><cell>CVS</cell><cell></cell><cell>.010</cell><cell>.879</cell><cell>.515</cell><cell>.115</cell><cell></cell><cell>.449</cell><cell>.087</cell><cell>.574</cell><cell cols="2">.588</cell><cell>.472</cell><cell>.088</cell><cell cols="2">.411</cell><cell>.175</cell><cell>.140</cell><cell>.524</cell><cell>.300</cell></row><row><cell>OURS</cell><cell></cell><cell>.340</cell><cell>.479</cell><cell>.452</cell><cell>.200</cell><cell></cell><cell>.453</cell><cell>.294</cell><cell>.149</cell><cell cols="2">.269</cell><cell>.135</cell><cell>.065</cell><cell cols="2">.749</cell><cell>.248</cell><cell>.764</cell><cell>.547</cell><cell>.767</cell></row><row><cell cols="2">Event name</cell><cell cols="3">Making sandwich</cell><cell></cell><cell cols="2">Parade</cell><cell></cell><cell></cell><cell cols="2">Parkour</cell><cell></cell><cell cols="4">Repairing appliance</cell><cell>sewing project</cell></row><row><cell>RCF</cell><cell></cell><cell>.121</cell><cell>.506</cell><cell>.249</cell><cell>.209</cell><cell></cell><cell>.451</cell><cell>.214</cell><cell>.244</cell><cell cols="2">.323</cell><cell>.482</cell><cell>.268</cell><cell cols="2">.348</cell><cell>.093</cell><cell>.266</cell><cell>.552</cell><cell>.290</cell></row><row><cell>DRM</cell><cell></cell><cell>.176</cell><cell>.443</cell><cell cols="2">.266 .156</cell><cell></cell><cell>.367</cell><cell>.245</cell><cell>.439</cell><cell cols="2">.584</cell><cell>.506</cell><cell>.195</cell><cell cols="2">.685</cell><cell>.391</cell><cell>.237</cell><cell>.520</cell><cell>.217</cell></row><row><cell>CVS</cell><cell></cell><cell>.173</cell><cell>.349</cell><cell>.173</cell><cell>.750</cell><cell></cell><cell>.683</cell><cell>.671</cell><cell>.227</cell><cell cols="2">.576</cell><cell>.388</cell><cell>.143</cell><cell cols="2">.329</cell><cell>.119</cell><cell>.412</cell><cell>.436</cell><cell>.292</cell></row><row><cell>OURS</cell><cell></cell><cell>.180</cell><cell>.630</cell><cell cols="2">.205 .209</cell><cell></cell><cell>.411</cell><cell>.284</cell><cell>.530</cell><cell cols="2">.506</cell><cell>.472</cell><cell>.248</cell><cell cols="2">.740</cell><cell>.343</cell><cell>.249</cell><cell>.521</cell><cell>.313</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="3">TABLE IV</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="17">EVALUATION RESULTS OF THE OVERALL PERFORMANCES AND AVERAGE RUNNING TIME ON THE MEDSAL1, MEDSAL2, AND</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="6">MED SUMMARIZE DATASET.</cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell cols="5">Overall performance on MEDsal1</cell><cell></cell><cell cols="6">Overall performance on MEDsal2</cell><cell></cell><cell cols="3">Overall performance on MED summarize</cell></row><row><cell>Metrics</cell><cell>AP</cell><cell>AUC</cell><cell>mF</cell><cell cols="3">Running time</cell><cell>AP</cell><cell cols="2">AUC</cell><cell>mF</cell><cell cols="3">Running time</cell><cell>AP</cell><cell cols="2">AUC</cell><cell>mF</cell><cell>Running time</cell></row><row><cell>RCF</cell><cell>.265</cell><cell>.389</cell><cell>.476</cell><cell></cell><cell>5.31s</cell><cell></cell><cell>.553</cell><cell>.454</cell><cell cols="2">.618</cell><cell cols="2">100.13s</cell><cell></cell><cell>.268</cell><cell cols="2">.437</cell><cell>.278</cell><cell>2.19s</cell></row><row><cell>DRM</cell><cell>.322</cell><cell>.423</cell><cell>.438</cell><cell></cell><cell>21.62s</cell><cell></cell><cell>.557</cell><cell>.465</cell><cell cols="2">.613</cell><cell cols="2">170.41s</cell><cell></cell><cell>.281</cell><cell cols="2">.479</cell><cell>.317</cell><cell>7.87s</cell></row><row><cell>CVS</cell><cell>.309</cell><cell>.394</cell><cell>.396</cell><cell></cell><cell>17.47s</cell><cell></cell><cell>.538</cell><cell>.467</cell><cell cols="2">.626</cell><cell cols="2">304.71s</cell><cell></cell><cell>.263</cell><cell cols="2">.522</cell><cell>.319</cell><cell>22.35s</cell></row><row><cell>OURS</cell><cell>.417</cell><cell>.431</cell><cell>.571</cell><cell></cell><cell>4.03s</cell><cell></cell><cell>.576</cell><cell>.509</cell><cell cols="2">.669</cell><cell cols="2">12.93s</cell><cell></cell><cell>.293</cell><cell cols="2">.531</cell><cell>.352</cell><cell>1.59s</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>TABLE V EVALUATION</head><label>V</label><figDesc>OF THE MULTIMODAL FEATURES IN MEDSAL1 AND MEDSAL2 DATASETS, WHERE MM INDICATES THE PROPOSED FUSION OF THE FOUR TYPES OF FEATURES.</figDesc><table><row><cell></cell><cell></cell><cell>MEDsal1</cell><cell></cell><cell></cell><cell>MEDsal2</cell></row><row><cell></cell><cell>AP</cell><cell>AUC</cell><cell>mF</cell><cell>AP</cell><cell>AUC</cell><cell>mF</cell></row><row><cell>Motion</cell><cell cols="6">.363 .421 .465 .453 .460 .634</cell></row><row><cell>Visual</cell><cell cols="6">.407 .430 .568 .538 .467 .660</cell></row><row><cell>ASR</cell><cell cols="6">.439 .446 .482 .572 .480 .665</cell></row><row><cell cols="7">Concept .385 .429 .510 .492 .480 .653</cell></row><row><cell>MM</cell><cell cols="6">.417 .431 .571 .576 .509 .669</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>TABLE VII EVALUATION</head><label>VII</label><figDesc>OF THE PRIORS USED IN THE PROPOSED FRAMEWORK IN MEDSAL1 AND MEDSAL2 BENCHMARKS.</figDesc><table><row><cell></cell><cell cols="4">w/o DP w/o CP w/o LP OURS</cell></row><row><cell>AP</cell><cell>.401</cell><cell>.412</cell><cell>.406</cell><cell>.417</cell></row><row><cell>MEDsal1 AUC</cell><cell>.427</cell><cell>.421</cell><cell>.410</cell><cell>.431</cell></row><row><cell>mF</cell><cell>.556</cell><cell>.568</cell><cell>.536</cell><cell>.571</cell></row><row><cell>AP</cell><cell>.540</cell><cell>.539</cell><cell>.531</cell><cell>.576</cell></row><row><cell>MEDsal2 AUC</cell><cell>.469</cell><cell>.470</cell><cell>.474</cell><cell>.509</cell></row><row><cell>mF</cell><cell>.661</cell><cell>.657</cell><cell>.634</cell><cell>.669</cell></row></table></figure>
		</body>
		<back>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Dingwen Zhang received his B.E. degree from the Northwestern Polytechnical University, Xi'an, China, in 2012. He is currently pursuing the Ph.D. degree at Northwestern Polytechnical University and working as a visiting scholar at Carnegie Mellon University, Pittsburgh, USA. His research interests include computer vision and multimedia processing, especially on saliency detection, video understanding, and weakly supervised learning. </p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Eventnet: A large scale structured concept library for complex event detection in video</title>
		<author>
			<persName><forename type="first">G</forename><surname>Ye</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S.-F</forename><surname>Chang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">MM</title>
		<imprint>
			<biblScope unit="page" from="471" to="480" />
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Semantic pooling for complex event analysis in untrimmed videos</title>
		<author>
			<persName><forename type="first">X</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y.-L</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">P</forename><surname>Xing</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">TPAMI</title>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Background prior-based salient object detection via deep reconstruction residual</title>
		<author>
			<persName><forename type="first">J</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Wu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">TCSVT</title>
		<imprint>
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="1309" to="1321" />
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Correspondence driven saliency transfer</title>
		<author>
			<persName><forename type="first">W</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Shao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Porikli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">TIP</title>
		<imprint>
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="5025" to="5034" />
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Scalable mobile image retrieval by exploring contextual saliency</title>
		<author>
			<persName><forename type="first">X</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Qian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Xue</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">TIP</title>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="1709" to="1721" />
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Story-driven summarization for egocentric video</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Grauman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2013">2013</date>
			<biblScope unit="page" from="2714" to="2721" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">A bag-ofimportance model with locality-constrained coding based feature learning for video summarization</title>
		<author>
			<persName><forename type="first">S</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Mei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Guan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">D</forename><surname>Feng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">TMM</title>
		<imprint>
			<biblScope unit="volume">16</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="1497" to="1509" />
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Categoryspecific video summarization</title>
		<author>
			<persName><forename type="first">D</forename><surname>Potapov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Douze</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Harchaoui</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Schmid</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="540" to="555" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Trecvid 2014-an overview of the goals, tasks, data, evaluation mechanisms and metrics</title>
		<author>
			<persName><forename type="first">P</forename><surname>Over</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Fiscus</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Sanders</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Joy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Michel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Awad</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Smeaton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Kraaij</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Quénot</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of TRECVID</title>
		<meeting>TRECVID</meeting>
		<imprint>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page">52</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Learning realistic human actions from movies</title>
		<author>
			<persName><forename type="first">I</forename><surname>Laptev</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Marszałek</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Schmid</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Rozenfeld</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2008">2008</date>
			<biblScope unit="page" from="1" to="8" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Action recognition by dense trajectories</title>
		<author>
			<persName><forename type="first">H</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Kläser</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Schmid</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C.-L</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2011">2011</date>
			<biblScope unit="page" from="3169" to="3176" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Complex event detection using semantic saliency and nearly-isotonic svm</title>
		<author>
			<persName><forename type="first">X</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Xing</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Yu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="1348" to="1357" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Recognizing complex events in videos by learning key static-dynamic evidences</title>
		<author>
			<persName><forename type="first">K.-T</forename><surname>Lai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M.-S</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S.-F</forename><surname>Chang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="675" to="688" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Dynamic pooling for complex event recognition</title>
		<author>
			<persName><forename type="first">W</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Divakaran</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Vasconcelos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2013">2013</date>
			<biblScope unit="page" from="2728" to="2735" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Video event detection by inferring temporal instance labels</title>
		<author>
			<persName><forename type="first">K.-T</forename><surname>Lai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><forename type="middle">X</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M.-S</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S.-F</forename><surname>Chang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="2251" to="2258" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Cosaliency detection based on intrasaliency prior transfer and deep intersaliency mining</title>
		<author>
			<persName><forename type="first">D</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Shao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">TNNLS</title>
		<imprint>
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="1163" to="1176" />
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Detection of cosalient objects by looking deep and wide</title>
		<author>
			<persName><forename type="first">D</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IJCV</title>
		<imprint>
			<biblScope unit="volume">120</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="215" to="232" />
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Cluster-based co-saliency detection</title>
		<author>
			<persName><forename type="first">H</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Tu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE TIP</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page" from="3766" to="3778" />
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">A Co-saliency Model of Image Pairs</title>
		<author>
			<persName><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Ngan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">TIP</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page" from="3365" to="3375" />
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<author>
			<persName><forename type="first">Y</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Fang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Video saliency incorporating spatiotemporal cues and uncertainty weighting</title>
		<imprint>
			<date type="published" when="2014">2014</date>
			<biblScope unit="volume">23</biblScope>
			<biblScope unit="page" from="3910" to="3921" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Temporally coherent video saliency using regional dynamic contrast</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Sheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Xie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">TCSVT</title>
		<imprint>
			<biblScope unit="volume">23</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page" from="2067" to="2076" />
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Learning video saliency from human gaze using candidate selection</title>
		<author>
			<persName><forename type="first">D</forename><surname>Rudoy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">B</forename><surname>Goldman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Shechtman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Zelnik-Manor</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2013">2013</date>
			<biblScope unit="page" from="1147" to="1154" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Video object discovery and co-segmentation with extremely weak supervision</title>
		<author>
			<persName><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Hua</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Sukthankar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Xue</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Zheng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="640" to="655" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Video object co-segmentation by regulated maximum weight cliques</title>
		<author>
			<persName><forename type="first">D</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Javed</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Shah</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="551" to="566" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Object-based multiple foreground video co-segmentation via multi-state selection graph</title>
		<author>
			<persName><forename type="first">H</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">K</forename><surname>Ward</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">TIP</title>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="3415" to="3424" />
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Robust video object cosegmentation</title>
		<author>
			<persName><forename type="first">W</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Porikli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">TIP</title>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page" from="3137" to="3148" />
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Comparison of video shot boundary detection techniques</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">S</forename><surname>Boreczky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">A</forename><surname>Rowe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">JEI</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="122" to="128" />
			<date type="published" when="1996">1996</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">K-means clustering via principal component analysis</title>
		<author>
			<persName><forename type="first">C</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>He</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2004">2004</date>
			<biblScope unit="page">29</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<author>
			<persName><forename type="first">S.-I</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Mao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Gan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Lan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Cai</surname></persName>
		</author>
		<title level="m">NIST TRECVID Video Retrieval Evaluation Workshop</title>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
	<note>Informedia@ trecvid 2014 med and mer</note>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<title level="m" type="main">Very deep convolutional networks for large-scale image recognition</title>
		<author>
			<persName><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1409.1556</idno>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Action recognition with improved trajectories</title>
		<author>
			<persName><forename type="first">H</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Schmid</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2013">2013</date>
			<biblScope unit="page" from="3551" to="3558" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Fast and accurate content-based semantic search in 100m internet videos</title>
		<author>
			<persName><forename type="first">L</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S.-I</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Meng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Mitamura</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">G</forename><surname>Hauptmann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">MM</title>
		<imprint>
			<biblScope unit="page" from="49" to="58" />
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Searching persuasively: Joint event detection and evidence recounting with limited supervision</title>
		<author>
			<persName><forename type="first">X</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y.-L</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">G</forename><surname>Hauptmann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">MM</title>
		<imprint>
			<biblScope unit="page" from="581" to="590" />
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Instructional videos for unsupervised harvesting and learning of action examples</title>
		<author>
			<persName><forename type="first">S.-I</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Hauptmann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">MM</title>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="825" to="828" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<monogr>
		<title level="m" type="main">A one-pass decoder based on polymorphic linguistic context assignment</title>
		<author>
			<persName><forename type="first">H</forename><surname>Soltau</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Metze</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Fugen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Waibel</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2001">2001</date>
			<publisher>IEEE</publisher>
			<biblScope unit="page" from="214" to="217" />
		</imprint>
		<respStmt>
			<orgName>ASRU</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Self-paced learning with diversity</title>
		<author>
			<persName><forename type="first">L</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Meng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S.-I</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Lan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Shan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Hauptmann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="2078" to="2086" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Diffrac: a discriminative and flexible framework for clustering</title>
		<author>
			<persName><forename type="first">F</forename><forename type="middle">R</forename><surname>Bach</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Harchaoui</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2008">2008</date>
			<biblScope unit="page" from="49" to="56" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Co-localization in realworld images</title>
		<author>
			<persName><forename type="first">K</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Joulin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L.-J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Discriminative clustering for image co-segmentation</title>
		<author>
			<persName><forename type="first">A</forename><surname>Joulin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Bach</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Ponce</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Deformed graph laplacian for semisupervised learning</title>
		<author>
			<persName><forename type="first">C</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Tao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Tu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">TNNLS</title>
		<imprint>
			<biblScope unit="volume">26</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page" from="2261" to="2274" />
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<monogr>
		<title level="m" type="main">Interior-point polynomial algorithms in convex programming</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Nesterov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Nemirovskii</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Ye</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1994">1994</date>
			<biblScope unit="volume">13</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Image tag completion via dual-view linear sparse reconstructions</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">S</forename><surname>Ge</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">CVIU</title>
		<imprint>
			<biblScope unit="volume">124</biblScope>
			<biblScope unit="page" from="42" to="60" />
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Frequencytuned salient region detection</title>
		<author>
			<persName><forename type="first">R</forename><surname>Achanta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Hemami</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Estrada</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Susstrunk</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2009">2009</date>
			<biblScope unit="page" from="1597" to="1604" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Two-stage learning to predict human eye fixations via sdaes</title>
		<author>
			<persName><forename type="first">J</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Wen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">TCYB</title>
		<imprint>
			<biblScope unit="volume">46</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="487" to="498" />
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Self-adaptively weighted co-saliency detection via rank constraint</title>
		<author>
			<persName><forename type="first">X</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Tao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Feng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">TIP</title>
		<imprint>
			<biblScope unit="volume">23</biblScope>
			<biblScope unit="issue">9</biblScope>
			<biblScope unit="page" from="4175" to="4186" />
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Robust principal component analysis?</title>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">J</forename><surname>Candès</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Wright</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">JACM</title>
		<imprint>
			<biblScope unit="volume">58</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page">11</biblScope>
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Latent semantic minimal hashing for image retrieval</title>
		<author>
			<persName><forename type="first">X</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">TIP</title>
		<imprint>
			<biblScope unit="volume">26</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="355" to="368" />
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Large-scale cross-modality search via collective matrix factorization hashing</title>
		<author>
			<persName><forename type="first">G</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Gao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">TIP</title>
		<imprint>
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="5427" to="5440" />
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Semi-supervised multitask learning for scene recognition</title>
		<author>
			<persName><forename type="first">X</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Mou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">TCYB</title>
		<imprint>
			<biblScope unit="volume">45</biblScope>
			<biblScope unit="issue">9</biblScope>
			<biblScope unit="page" from="1967" to="1976" />
			<date type="published" when="2015-09">Sept 2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Spatiotemporal statistics for video quality assessment</title>
		<author>
			<persName><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Lu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">TIP</title>
		<imprint>
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="3329" to="3342" />
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
