<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">UltraGCN: Ultra Simplification of Graph Convolutional Networks for Recommendation</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date type="published" when="2021-10-28">28 Oct 2021</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Kelong</forename><surname>Mao</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Jieming</forename><surname>Zhu</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Xi</forename><surname>Xiao</surname></persName>
							<email>xiaox@sz.tsinghua.edu.cn</email>
						</author>
						<author>
							<persName><forename type="first">Biao</forename><surname>Lu</surname></persName>
							<email>lubiao4@huawei.com</email>
						</author>
						<author>
							<persName><forename type="first">Zhaowei</forename><surname>Wang</surname></persName>
							<email>wangzhaowei3@huawei.com</email>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="department">Gaoling School</orgName>
								<orgName type="institution">AI Renmin University of China</orgName>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff1">
								<orgName type="laboratory">Huawei Noah&apos;s Ark Lab Shenzhen</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff2">
								<orgName type="institution">Tsinghua University Peng Cheng Laboratory</orgName>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff3">
								<orgName type="laboratory">Huawei Noah&apos;s Ark Lab</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff4">
								<orgName type="laboratory">Huawei Noah&apos;s Ark Lab</orgName>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff5">
								<orgName type="institution">Tsinghua University</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">UltraGCN: Ultra Simplification of Graph Convolutional Networks for Recommendation</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2021-10-28">28 Oct 2021</date>
						</imprint>
					</monogr>
					<idno type="arXiv">arXiv:2110.15114v1[cs.IR]</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.2" ident="GROBID" when="2022-12-25T13:29+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>CCS CONCEPTS</term>
					<term>Information systems → Recommender systems</term>
					<term>Collaborative filtering Recommender systems</term>
					<term>collaborative filtering</term>
					<term>graph convolutional networks</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>With the recent success of graph convolutional networks (GCNs), they have been widely applied for recommendation, and achieved impressive performance gains. The core of GCNs lies in its message passing mechanism to aggregate neighborhood information. However, we observed that message passing largely slows down the convergence of GCNs during training, especially for large-scale recommender systems, which hinders their wide adoption. LightGCN makes an early attempt to simplify GCNs for collaborative filtering by omitting feature transformations and nonlinear activations. In this paper, we take one step further to propose an ultra-simplified formulation of GCNs (dubbed UltraGCN), which skips infinite layers of message passing for efficient recommendation. Instead of explicit message passing, UltraGCN resorts to directly approximate the limit of infinite-layer graph convolutions via a constraint loss. Meanwhile, UltraGCN allows for more appropriate edge weight assignments and flexible adjustment of the relative importances among different types of relationships. This finally yields a simple yet effective UltraGCN model, which is easy to implement and efficient to train. Experimental results on four benchmark datasets show that UltraGCN not only outperforms the state-of-the-art GCN models but also achieves more than 10x speedup over LightGCN.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">INTRODUCTION</head><p>Nowadays, personalized recommendation has become a prevalent way to help users find information of their interests in various applications, such as e-commerce, online news, and social media. The core of recommendation is to precisely match a user's preference with candidate items. Collaborative filtering (CF) <ref type="bibr" target="#b10">[11]</ref>, as a fundamental recommendation task, has been widely studied in both academia and industry. A common paradigm of CF is to learn vector representations (i.e., embeddings) of users and items from historical interaction data and then perform top-k recommendation based on the pairwise similarity between user and item embeddings.</p><p>As the interaction data can be naturally modelled as graphs, such as user-item bipartite graph and item-item co-occurrence graph, recent studies <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b12">13,</ref><ref type="bibr" target="#b24">24,</ref><ref type="bibr" target="#b27">27]</ref> opt for powerful graph convolutional/neural networks (GCNs, or GNNs in general) to learn user and item node representations. These GCN-based models are capable of exploiting higher-order connectivity between users and items, and therefore have achieved impressive performance gains for recommendation. PinSage <ref type="bibr" target="#b31">[31]</ref> and M2GRL <ref type="bibr" target="#b26">[26]</ref> are two successful use cases in industrial applications.</p><p>Despite the promising results obtained, we argue that current model designs are heavy and burdensome. In order to capture higher-order collaborative signals and better model the interaction process between users and items, current GNN-based CF models <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b24">24,</ref><ref type="bibr" target="#b27">27,</ref><ref type="bibr" target="#b32">32]</ref> tend to seek for more and more sophisticated network encoders. However, we observed that these GCN-based models are hard to train with large graphs, which hinders their wide adoption in industry. Industrial recommender systems usually involve massive graphs due to the large numbers of users and items. This brings efficiency and scalability challenges for model designs. Towards this end, some research efforts <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b9">10,</ref><ref type="bibr" target="#b16">17]</ref> have been made to simplify the design of GCN-based CF models, mainly by removing feature transformations and non-linear activations that are not necessary for CF. These simplified models not only obtain much better performance than those complex ones, but also brings some benefits on training efficiency.</p><p>Inspired by these pioneer studies, we performed further empirical analysis on the training process of GCN-based models and found that message passing (i.e., neighborhood aggregation) on a large graph is usually time-consuming for CF. In particular, stacking multiple layers of message passing could lead to the slow convergence of GCN-based models on CF tasks. Although the aforementioned models such as LightGCN <ref type="bibr" target="#b9">[10]</ref> have already been simplified for training, the message passing operations still dominate their training. For example, in our experiments, three-layer LightGCN takes more than 700 epochs to converge to its best result on the Amazon-Books dataset <ref type="bibr" target="#b8">[9]</ref>, which would be unacceptable in an industrial setting. How to improve the efficiency of GCN models yet retain their effectiveness on recommendation is still an open problem.</p><p>To tackle this challenge, in this work, we question the necessity of explicit message passing layers in CF, and finally propose an ultra-simplified form of GCNs (dubbed UltraGCN) without message passing for efficient recommendation. More specifically, we analyzed the message passing formula of LightGCN and identified three critical limitations: 1) The weights assigned on edges during message passing are counter-intuitive, which may not be appropriate for CF. 2) The propagation process recursively combines different types of relationship pairs (including user-item pairs, item-item pairs, and user-user pairs) into the model, but fails to capture their varying importance. This may also introduce noisy and uninformative relationships that confuse the model training.</p><p>3) The over-smoothing issue limits the use of too many layers of message passing in LightGCN. Therefore, instead of performing explicit message passing, we seek to directly approximate the limit of infinite-layer graph convolutions via a constraint loss, which leads to the ultra-simplified GCN model, UltraGCN. The loss-based design of UltraGCN is very flexible, allowing us to manually adjust the relative importances of different types of relationships and also avoid the over-smoothing problem by negative sampling. This finally yields a simple yet effective UltraGCN model, which is easy to implement and efficient to train. Furthermore, we show that Ultra-GCN achieves significant improvements over the state-of-the-art CF models. For instance, UltraGCN attains up to 76.6% improvement in NDCG@20 and more than 10x speedup in training over LightGCN on the Amazon-Books dataset.</p><p>In summary, this work makes the following main contributions:</p><p>• We empirically analyze the training inefficiency of LightGCN and further attribute its cause to the critical limitations of the message passing mechanism. • We propose an ultra simplified formulation of GCN, namely UltraGCN, which skips infinite layers of explicit message passing for efficient recommendation. • Extensive experiments have been conducted on four benchmark datasets to show the effectiveness and efficiency of UltraGCN.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">MOTIVATION</head><p>In this section, we revisit the GCN and LightGCN models, and further identify the limitations resulted from the inherent message passing mechanism, which also justify the motivation of our work.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Revisiting GCN and LightGCN</head><p>GCN <ref type="bibr" target="#b13">[14]</ref> is a representative model of graph neural networks that applies message passing to aggregate neighborhood information.</p><p>The message passing layer with self-loops is defined as follows:</p><formula xml:id="formula_0">𝐸 (𝑙+1) = 𝜎 D− 1 2 Â D− 1 2 𝐸 (𝑙) 𝑊 (𝑙)<label>(1)</label></formula><p>with Â = 𝐴 + 𝐼 and D = 𝐷 + 𝐼 . 𝐴, 𝐷, 𝐼 are the adjacency matrix, the diagonal node degree matrix, and the identity matrix, respectively.</p><p>𝐼 is used to integrate self-loop connections on nodes. 𝐸 (𝑙) and 𝑊 (𝑙)  denote the representation matrix and the weight matrix for the 𝑙-th layer. 𝜎 (•) is a non-linear activation function (e.g., ReLU). Despite the wide success of GCN in graph learning, several recent studies <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b9">10,</ref><ref type="bibr" target="#b16">17,</ref><ref type="bibr" target="#b29">29]</ref> found that simplifying GCN appropriately can further boost the performance on CF tasks. LightGCN <ref type="bibr" target="#b9">[10]</ref> is one such simplified GCN model that removes feature transformations (i.e., 𝑊 (𝑙) ) and non-linear activations (i.e., 𝜎). Its message passing layer can thus be expressed as follows:</p><formula xml:id="formula_1">𝐸 (𝑙+1) = ( D− 1 2 Â D− 1 2 )𝐸 (𝑙)<label>(2)</label></formula><p>It is worth noting that although LightGCN also removes self-loop connections on nodes, its layer combination operation has a similar effect to self-loops used in Equation <ref type="formula" target="#formula_1">2</ref>, becauase both of them output a weighted sum of the embeddings propagated at each layer as the final output representation. Given self-loop connections, we can rewrite the message passing operations for user 𝑢 and item 𝑖 as follows:</p><formula xml:id="formula_2">𝑒 (𝑙+1) 𝑢 = 1 𝑑 𝑢 + 1 𝑒 (𝑙) 𝑢 + ∑︁ 𝑘 ∈N (𝑢) 1 √ 𝑑 𝑢 + 1 √︁ 𝑑 𝑘 + 1 𝑒 (𝑙) 𝑘 ,<label>(3)</label></formula><formula xml:id="formula_3">𝑒 (𝑙+1) 𝑖 = 1 𝑑 𝑖 + 1 𝑒 (𝑙) 𝑖 + ∑︁ 𝑣 ∈N (𝑖) 1 √ 𝑑 𝑣 + 1 √ 𝑑 𝑖 + 1 𝑒 (𝑙) 𝑣<label>(4)</label></formula><p>where 𝑒</p><p>(𝑙)</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>𝑢 and 𝑒 (𝑙)</head><p>𝑢 denote the embeddings of user 𝑢 and item 𝑖 at layer 𝑙. N (𝑢) and N (𝑖) represent their neighbor node sets, respectively. 𝑑 𝑢 denotes the original degree of the node 𝑢.</p><p>As shown in the left part of Figure <ref type="figure">1</ref>, LightGCN performs a stack of message passing layers to obtain the embeddings and finally uses their dot product for training.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Limitations of Message Passing</head><p>We argue that such message passing layers have potential limitations that hinder the effective and efficient training of GCN-based models in recommendation tasks. To illustrate it, we take the 𝑙-th layer message passing of LightGCN in Equation 3 and 4 for example. Note that 𝑢 and 𝑣 denote users while 𝑖 and 𝑘 denote items. LightGCN takes the dot product of the two embedding as the final logit to capture the preference of user 𝑢 on item 𝑖. Thus we obtain:</p><formula xml:id="formula_4">𝑒 (𝑙+1) 𝑢 • 𝑒 (𝑙+1) 𝑖 = 𝛼 𝑢𝑖 (𝑒 (𝑙) 𝑢 • 𝑒 (𝑙) 𝑖 ) + ∑︁ 𝑘 ∈N (𝑢) 𝛼 𝑖𝑘 (𝑒 (𝑙) 𝑖 • 𝑒 (𝑙) 𝑘 ) + ∑︁ 𝑣 ∈N (𝑖) 𝛼 𝑢𝑣 (𝑒 (𝑙) 𝑢 • 𝑒 (𝑙) 𝑣 ) + ∑︁ 𝑘 ∈N (𝑢) ∑︁ 𝑣 ∈N (𝑖) 𝛼 𝑘𝑣 (𝑒 (𝑙) 𝑘 • 𝑒 (𝑙) 𝑣 ) ,<label>(5)</label></formula><p>where 𝛼 𝑢𝑖 , 𝛼 𝑖𝑘 , 𝛼 𝑢𝑣 , and 𝛼 𝑘𝑣 can be derived as follows:</p><formula xml:id="formula_5">𝛼 𝑢𝑖 = 1 (𝑑 𝑢 + 1)(𝑑 𝑖 + 1) , 𝛼 𝑖𝑘 = 1 √ 𝑑 𝑢 + 1 √︁ 𝑑 𝑘 + 1(𝑑 𝑖 + 1) , 𝛼 𝑢𝑣 = 1 √ 𝑑 𝑣 + 1 √ 𝑑 𝑖 + 1(𝑑 𝑢 + 1) , 𝛼 𝑘𝑣 = 1 √ 𝑑 𝑢 + 1 √︁ 𝑑 𝑘 + 1 √ 𝑑 𝑣 + 1 √ 𝑑 𝑖 + 1</formula><p>Therefore, we can observe that multiple different types of collaborative signals, including user-item relationships (𝑢-𝑖 and 𝑘-𝑣), item-item relationships (𝑘-𝑖), and user-user relationships (𝑢-𝑣), are captured when training GCN-based models with message passing layers. This also reveals why GCN-based models are effective for CF. However, we found that the edge weights assigned on various types of relationships are not justified to be appropriate for CF tasks. Based on our empirical analysis, we identify three critical limitations of the message passing layers in GCN-based models:</p><p>• Limitation I: The weight 𝛼 𝑖𝑘 is used to model the item-item relationships. However, given the user 𝑢, the factors of item 𝑖 and item 𝑘 are asymmetric ( 1</p><formula xml:id="formula_6">√ 𝑑 𝑘 +1</formula><p>for item 𝑘 while 1</p><formula xml:id="formula_7">𝑑 𝑖 +1</formula><p>for item 𝑖). This is not reasonable since it is counter-intuitive to treat the item 𝑘 and item 𝑖 unequally. Similarly, 𝛼 𝑢𝑣 that models the user-user relationships also suffer this issue. Such unreasonable weight assignments may mislead the model training and finally result in sub-optimal performance.  <ref type="bibr" target="#b9">[10]</ref>. We partially attribute it to the over-smoothing problem of message passing. As graph convolution is a special form of Laplacian smoothing <ref type="bibr" target="#b15">[16]</ref>, performing too many layers of message passing will make the nodes with the same degrees tend to have exactly the same embeddings. According to Theorem 1 in <ref type="bibr" target="#b4">[5]</ref>, we can derive the infinite powers of message passing which take the following limit:</p><formula xml:id="formula_8">lim 𝑙→∞ ( D− 1 2 Â D− 1 2 ) 𝑙 𝑖,𝑗 = √︁ (𝑑 𝑖 + 1)(𝑑 𝑗 + 1) 2𝑚 + 𝑛<label>(6)</label></formula><p>where 𝑛 and 𝑚 are the total numbers of nodes and edges in the graph, respectively.</p><p>The above limitations of message passing motivate our work. We question the necessity of explicit message passing layers in CF and further propose an ultra-simplified formulation of GCN, dubbed UltraGCN.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">UltraGCN</head><p>In this section, we present our ultra-simplified UltraGCN model and demonstrate how to incorporate different types of relationships in a flexible manner. We also elaborate on how it overcomes the above limitations and analyze its connections to other related models.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Learning on User-Item Graph</head><p>Due to the limitations of message passing, in this work, we take one step forward to question the necessity of explicit message passing in CF. Considering that the limit of infinite powers of message passing exists as shown in Equation <ref type="formula" target="#formula_8">6</ref>, we wonder whether it is possible to skip the infinite-layer message passing yet approximate the convergence state reached.</p><p>After repeating infinite layers of message passing, we express the final convergence condition as follows:</p><formula xml:id="formula_9">𝑒 𝑢 = lim 𝑙→∞ 𝑒 (𝑙+1) 𝑢 = lim 𝑙→∞ 𝑒 (𝑙) 𝑢 (7)</formula><p>That is, the representations of the last two layers keep unchanged, since the vector generated from neighborhood aggregation equals to the node representation itself. We use 𝑒 𝑢 (or 𝑒 𝑖 ) to denote the final converged representation of user 𝑢 (or item 𝑖). Then, Equation 3 can be rewritten as:</p><formula xml:id="formula_10">𝑒 𝑢 = 1 𝑑 𝑢 + 1 𝑒 𝑢 + ∑︁ 𝑖 ∈N (𝑢) 1 √ 𝑑 𝑢 + 1 √ 𝑑 𝑖 + 1 𝑒 𝑖<label>(8)</label></formula><p>After some simplifications, we derive the following convergence state:</p><formula xml:id="formula_11">𝑒 𝑢 = ∑︁ 𝑖 ∈N (𝑢) 𝛽 𝑢,𝑖 𝑒 𝑖 , 𝛽 𝑢,𝑖 = 1 𝑑 𝑢 √︄ 𝑑 𝑢 + 1 𝑑 𝑖 + 1<label>(9)</label></formula><p>In other words, if Equation 9 is satisfied for each node, it reaches the convergence state of message passing.</p><p>Instead of performing explicit message passing, we aim to directly approximate such convergence state. To this end, a straightforward way is to minimize the difference of both sides of Equation 9. In this work, we normalize the embeddings to unit vectors and then maximize the dot product of both terms:</p><formula xml:id="formula_12">max ∑︁ 𝑖 ∈N (𝑢) 𝛽 𝑢,𝑖 𝑒 ⊤ 𝑢 𝑒 𝑖 , ∀𝑢 ∈ 𝑈 ,<label>(10)</label></formula><p>which is equivalent to maximize the cosine similarity between 𝑒 𝑢 and 𝑒 𝑖 . For ease of optimization, we further incorporate sigmoid activation and negative log likelihood <ref type="bibr" target="#b1">[2]</ref>, and derive the following loss:</p><formula xml:id="formula_13">L 𝐶 = − ∑︁ 𝑢 ∈𝑈 ∑︁ 𝑖 ∈N (𝑢) 𝛽 𝑢,𝑖 log 𝜎 (𝑒 ⊤ 𝑢 𝑒 𝑖 ) , (<label>11</label></formula><formula xml:id="formula_14">)</formula><p>where 𝜎 is the sigmoid function. The loss is optimized to fulfill the structure constraint imposed by Equation <ref type="formula" target="#formula_11">9</ref>. As such, we denote L 𝐶 as the constraint loss and denote 𝛽 𝑢,𝑖 as the constraint coefficient. However, optimizing L 𝐶 could also suffer from the over-smoothing problem as L 𝐶 requires all connected pairs (𝛽 𝑢,𝑖 &gt; 0) to be similar. In this way, users and items could easily converge to the same embeddings. To alleviate the over-smoothing problem, conventional GCN-based CF models usually fix a small number of message passing layers, e.g., 2∼4 layers in LightGCN. Instead, as UltraGCN approximates the limit of infinite-layer message passing via a constraint loss, we choose to perform negative sampling during training. This is inspired from the negative sampling strategy used in Word2Vec <ref type="bibr" target="#b18">[19]</ref>, which provides a more simple and effective way to counteract the over-smoothing problem. After performing negative sampling, we finally derive the following constraint loss:</p><formula xml:id="formula_15">L 𝐶 = − ∑︁ (𝑢,𝑖) ∈𝑁 + 𝛽 𝑢,𝑖 log 𝜎 (𝑒 ⊤ 𝑢 𝑒 𝑖 ) − ∑︁ (𝑢,𝑗) ∈𝑁 − 𝛽 𝑢,𝑗 log 𝜎 (−𝑒 ⊤ 𝑢 𝑒 𝑗 )<label>(12)</label></formula><p>where 𝑁 + and 𝑁 − represent the sets of positive pairs and randomly sampled negative pairs. Note that we omit the summation over 𝑈 for ease of presentation. The constraint loss L 𝐶 enables UltraGCN to directly approximate the limit of infinite-layer message passing to capture arbitrary high-order collaborative signals in the user-item bipartite graph, while effectively avoiding the troublesome oversmoothing issue via negative sampling. Furthermore, we note that 𝛽 𝑢,𝑖 acts as the loss weight in L 𝐶 , which is inversely proportional to 𝑑 𝑢 and 𝑑 𝑖 with similar magnitudes. This is interpretable for CF. If a user interacts with many items or an item is interacted by many users, the influence of their interaction would be small, and thus the loss weight of this (𝑢, 𝑖) pair should be small.</p><p>3.1.1 Optimization. Typically, CF models perform item recommendation by applying either pairwise BPR (Bayesian personalized ranking) loss <ref type="bibr" target="#b21">[22]</ref> or pointwise BCE (binary cross-entropy) loss <ref type="bibr" target="#b10">[11]</ref> for optimization. We formulate CF as a link prediction problem in graph learning. Therefore, we choose the following BCE loss as the main optimization objective. It is also consistent with the loss format of L 𝐶 .</p><formula xml:id="formula_16">L 𝑂 = − ∑︁ (𝑢,𝑖) ∈𝑁 + log 𝜎 (𝑒 ⊤ 𝑢 𝑒 𝑖 ) − ∑︁ (𝑢,𝑗) ∈𝑁 − log 𝜎 (−𝑒 ⊤ 𝑢 𝑒 𝑗 )<label>(13)</label></formula><p>where 𝑁 + and 𝑁 − represent positive and randomly sampled negative links (i.e., 𝑢-𝑗 pairs). Note that for simplicity, we use the same sets of sample pairs with L 𝐶 , but they could also be made different conveniently.</p><p>As L 𝑂 and L 𝐶 depends only on the user-item relationships, we define it as the base version of UltraGCN, denoted as UltraGCN 𝐵𝑎𝑠𝑒 , which has the following optimization objective.</p><formula xml:id="formula_17">L = L 𝑂 + 𝜆L 𝐶 , (<label>14</label></formula><formula xml:id="formula_18">)</formula><p>where 𝜆 is the hyper-parameter to control the importance weights of two losse terms.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Learning on Item-Item Graph</head><p>As Equation <ref type="formula" target="#formula_4">5</ref>shows, except for user-item relationships, some other relationships (e.g., item-item and user-user relationships) also greatly contribute to the effectiveness of GCN-based models on CF. However, in conventional GCN-based models, these relationships are implicitly learned through the same message passing layers with user-item relationships. This not only leads to the unreasonable edge weight assignments as discussed in Section 2.2, but also fails to capture the relative importances of different types of relationships. In contrast, UltraGCN does not rely on explicit message passing so that we can separately learn other relationships in a more flexible way. This also enables us to manually adjust the relative importances of different relationships.</p><p>We emphasize that UltraGCN is flexible to extend to model many different relation graphs, such as user-user graphs, itemitem graphs, and even knowlege graphs. In this work, we mainly demonstrate its use on the item-item co-occurrence graph, which has been shown to be useful for recommendation in <ref type="bibr" target="#b26">[26]</ref>. We first build the item-item co-occurrence graph by linking items that have co-occurrences, which produces the following weighted adjacent matrix</p><formula xml:id="formula_19">𝐺 ∈ R |𝐼 |×|𝐼 | . 𝐺 = 𝐴 ⊤ 𝐴 (<label>15</label></formula><formula xml:id="formula_20">)</formula><p>where each entry denotes the co-occurrences of two items. We follow Equation <ref type="formula" target="#formula_11">9</ref>to approximate infinite-layer graph convolution on 𝐺 and derive the new coefficient 𝜔 𝑖,𝑗 :</p><formula xml:id="formula_21">𝜔 𝑖,𝑗 = 𝐺 𝑖,𝑗 𝑔 𝑖 − 𝐺 𝑖,𝑖 √︂ 𝑔 𝑖 𝑔 𝑗 , 𝑔 𝑖 = ∑︁ 𝑘 𝐺 𝑖,𝑘<label>(16)</label></formula><p>where 𝑔 𝑖 and 𝑔 𝑗 denote the degrees (sum by column) of item 𝑖 and item 𝑗 in 𝐺, respectively. Similar to Equation <ref type="formula" target="#formula_15">12</ref>, we can derive the constraint loss on the item-item graph to learn the item-item relationships in an explicit way. However, as the adjacency matrix 𝐺 of the item-item graph is usually much denser compared to the sparse adjacency matrix 𝐴 of the user-item graph, directly minimizing the constraint loss on 𝐺 would introduce too many unreliable or noisy item-item pairs into optimization, which may make UltraGCN difficult to train. This is also similar to the Limitation II of conventional GCN-based models described in Section 2.2. But thanks to the flexible design of UltraGCN, we choose to select only informative pairs for training.</p><p>Specifically, to keep sparse item connections and retain training efficiency, we first select top-𝐾 most similar items 𝑆 (𝑖) for item 𝑖 according to 𝜔 𝑖,𝑗 . Intuitively, 𝜔 𝑖,𝑗 measures the similarity of item 𝑖 and item 𝑗, since it is proportional to the co-occurrence number of item 𝑖 and item 𝑗, yet inversely proportional to the total degrees of both items. Instead of directly learning item-item pairs, we propose to augment positive user-item pairs to capture item-item relationships. This keeps the training terms of UltraGCN being unified and decrease the possible difficulty in multi-task learning. We also empirically show that such a way can achieve better performance in Section 4.4. For each positive (𝑢, 𝑖) pair, we first construct 𝐾 weighted positive (𝑢, 𝑗) pairs, for 𝑗 ∈ 𝑆 (𝑖). Then, we penalize the learning of these pairs with the more reasonable similarity score 𝜔 𝑖,𝑗 and derive the constraint loss L 𝐼 on the item-item graph as follow:</p><formula xml:id="formula_22">L 𝐼 = − ∑︁ (𝑢,𝑖) ∈𝑁 + ∑︁ 𝑗 ∈𝑆 (𝑖) 𝜔 𝑖,𝑗 log(𝜎 (𝑒 ⊤ 𝑢 𝑒 𝑗 ))<label>(17)</label></formula><p>where |𝑆 (𝑖)| = 𝐾. We omit the negative sampling here as the negative sampling in L 𝐶 and L 𝑂 has already enabled UltraGCN to counteract over-smoothing. With this constraint loss, we extend UltraGCN to better learn item-item relationships, and finally derive the following training objective of UltraGCN,</p><formula xml:id="formula_23">L = L 𝑂 + 𝜆L 𝐶 + 𝛾 L 𝐼<label>(18)</label></formula><p>where 𝜆 and 𝛾 are hyper-parameters to adjust the relative importances of user-item and item-item relationships, respectively. Figure <ref type="figure">1</ref> illustrates the simple architecture of UltraGCN in contrast to LightGCN. Similarly, in inference, we use the dot product ŷ𝑢𝑖 = 𝑒 ⊤ 𝑢 𝑒 𝑖 between user 𝑢 and item 𝑖 as the ranking score for recommendation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Discussion</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.1">Model Analysis.</head><p>We first analyze the strengths of our Ultra-GCN model: 1) The weights assigned on edges in UltraGCN, i.e., 𝛽 𝑖,𝑗 and 𝜔 𝑖,𝑗 , are more reasonable and interpretable for CF, which are helpful to better learn user-item and item-item relationships, respectively. 2) Without explicit message passing, UltraGCN is flexible to separately customize its learning with different types of relationships. It is also able to select valuable training pairs (as in Section 3.2), rather than learn from all neighbor pairs indistinguishably, which may be mislead by noise. 3) Although UltraGCN is trained with different types of relationships in a multi-task learning way, its training losses (i.e., L 𝐶 , L 𝐼 , and L 𝑂 ) are actually unified, following the form of binary cross entropy. Such unification facilitates the training of UltraGCN, which converges fast. 4) The design of UltraGCN is flexible, by setting 𝛾 to 0, it reduces to UltraGCN 𝐵𝑎𝑠𝑒 , which only learns on the user-item graph. The performance comparison between UltraGCN and UltraGCN 𝐵𝑎𝑠𝑒 is provided in Table <ref type="table" target="#tab_3">2</ref>.</p><p>Note that in the current version, we do not incorporate the modeling of user-user relationships in UltraGCN. This is mainly because that users' interests are much broader than items' attributes. We found that it is harder to capture the user-user relationships from the user-user co-occurrence graph only. In Section 4.4, we empirically show that learning on the user-user co-occurrence graph does not bring noticeable improvements to UltraGCN. In contrast, conventional GCN-based CF models indistinguishably learn over all relationships from the user-item graph (i.e., Limitation II) likely suffer from performance degradation. The user-user relationships may be better modeled from a social network graph, and we leave it for future work.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.2">Relations to Other Models.</head><p>In this part, we discuss the relations between our UltraGCN and some other existing models.</p><p>Relation to MF. UltraGCN is formally to be a new weighted MF model with BCE loss tailored for CF. In contrast to previous MF models (e.g., NeuMF <ref type="bibr" target="#b10">[11]</ref>), UltraGCN can more deeply mine the collaborative information using graphs, yet keep the same concise architecture and model efficiency as MF.</p><p>Relation to Network Embedding Methods. Qiu et al. <ref type="bibr" target="#b20">[21]</ref> have proved that many popular network embedding methods with negative sampling (e.g., DeepWalk <ref type="bibr" target="#b19">[20]</ref>, LINE <ref type="bibr" target="#b25">[25]</ref>, and Node2Vec <ref type="bibr" target="#b6">[7]</ref>) all can be unified into the MF framework. However, in contrast to these network embedding methods, the edge weights used in UltraGCN are more meaningful and reasonable for CF, and thus lead to much better performance. In addition, the random walk in many network embedding methods will also uncontrollably introduce uninformative relationships that affect the performance. We empirically show the superiority of UltraGCN over three typical network embedding methods on CF in Section 4.2.</p><p>Relation to One-Layer LightGCN. We emphasize that Ultra-GCN is also different from one-layer LightGCN with BCE loss, because LightGCN applies weight combination to embeddings aggregation while our constraint coefficients are imposed on the constraint loss function, which aims to learn the essence of infinitelayer graph convolution. On the contrary, UltraGCN can overcome the limitations of one-layer LightGCN as described in Section 3.2.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.3">Model Complexity.</head><p>Given the embedding size 𝑑, 𝐾 similar items for each 𝑆 (𝑖), 𝑅 as the number of negative samples for each positive pair, and |𝐴 + | as the number of valid non-zero entries in the user-item interaction matrix, we can derive the training time complexity of UltraGCN: O ((𝐾 + 𝑅 + 1) * |𝐴 + | * (𝑑 2 + 1)). We note that the time complexities to calculate 𝛽 and 𝜔 are O (1), since we can pre-calculate them offline before training. As we usually limit 𝐾 to be small (e.g., 10 in our experiments) in practice, the time complexity of UltraGCN lies in the same level with MF, which is O ((𝑅 + 1) * |𝐴 + | * 𝑑 2 ). Besides, the only trainable parameters in UltraGCN are the embeddings of users and items, which is also the same with MF and LightGCN. As a result, our low-complexity UltraGCN brings great efficiency for model training and should be more practically applicable to large-scale recommender systems.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">EXPERIMENTS</head><p>We first compare UltraGCN with various state-of-the-art CF methods to demonstrate its effectiveness and high efficiency. We also perform detailed ablation studies to justify the rationality and effectiveness of the design choices of UltraGCN.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Experimental Setup</head><p>Datasets and Evaluation Protocol. We use four publicly available datasets, including Amazon-Book, Yelp2018, Gowalla, and MovieLens-1M to conduct our experiments, as many recent GCNbased CF models <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b27">27,</ref><ref type="bibr" target="#b28">28,</ref><ref type="bibr" target="#b32">32]</ref> are evaluated on these four datasets. We closely follow these GCN-based CF studies and use the same data split as them. Table <ref type="table" target="#tab_2">1</ref> shows the statistics of the used datasets.</p><p>For the evaluation protocol, Recall@20 and NDCG@20 are chosen as the evaluation metrics as they are popular in the evaluation of GCN-based CF models. We treat all items not interacted by a user as the candidates, and report the average results over all users. Baselines. In total, we compare UltraGCN with various types of the state-of-the-art models, covering MF-based (MF-BPR <ref type="bibr" target="#b14">[15]</ref>, ENMF <ref type="bibr" target="#b2">[3]</ref>), metric learing-based (CML <ref type="bibr" target="#b11">[12]</ref>), network embedding methods (DeepWalk <ref type="bibr" target="#b19">[20]</ref>, LINE <ref type="bibr" target="#b25">[25]</ref>, and Node2Vec <ref type="bibr" target="#b6">[7]</ref>), and GCNbased (NGCF <ref type="bibr" target="#b27">[27]</ref>, NIA-GCN <ref type="bibr" target="#b24">[24]</ref>, LR-GCCF <ref type="bibr" target="#b3">[4]</ref>, LightGCN <ref type="bibr" target="#b9">[10]</ref>, and DGCF <ref type="bibr" target="#b28">[28]</ref>).</p><p>Parameter Settings. Generally, we adopt Gaussian distribution with 0 mean and 10 −4 standard deviation to initialize embeddings. In many cases, we adopt 𝐿 2 regularization with 10 −4 weight and we set the learning rate to 10 −4 , the batch size to 1024, the negative sampling ratio 𝑅 to 300, and the size of the neighbor set 𝐾 to 10. In particular, we fix the embedding size to 64 which is identical to recent GCN-based work <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b24">24,</ref><ref type="bibr" target="#b27">27,</ref><ref type="bibr" target="#b28">28]</ref> to keep the same level of the number of parameters for fair comparison. We tune 𝜆 in [0.2, 0.4, 0.6, 0.8, 1.0, 1.2, 1.4], and 𝛾 in [0.1, 0.5, 1.0, 1.5, 2.0, 2.5, 3, 3.5]. For some baselines, we report the results from their papers to keep consistency. They also comparable since we use the exactly same datasets and experimental settings provided by them. For other baselines, we mainly use their official open-source code and carefully tune the parameters to achieve the best performance for fair comparisons. To allow for reproduciblility, we have released the source code and benchmark settings of UltraGCN at Github 1 . in graphs. These advantages together lead to the superiority of UltraGCN than compared state-of-the-art models. • Overall, network embedding models perform worse than GCN-based models, especially on Gowalla. The reason might be that the powerful graph convolution is more effective than traditional random walk or heuristic mining strategies in many network embedding methods, to capture collaborative information for recommendation. • Since UltraGCN is a special MF which only needs the dot product operation for embeddings, its architecture is orthogonal to some state-of-the-art models (e.g., DGCF). Therefore, similar to MF, UltraGCN can be deemed as an effective and efficient CF framework which is possible to be incorporated with other methods, such as enabling disentangled representation for users and items as DGCF, to achieve better performance. We leave such study in future work.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Performance Comparison</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Efficiency Comparison</head><p>As highlighted in Section 3.3, UltraGCN is endowed with high training efficiency for CF thanks to its concise and unified designs.</p><p>We have also theoretically demonstrated that the training time complexity of UltraGCN is on the same level as MF in Section 3.3.3.</p><p>In this section, we further empirically demonstrate the superiority of UltraGCN on training efficiency compared with other CF models, especially GCN-based models. To be specific, we select MF-BPR, ENMF, LightGCN, and LR-GCCF as the competitors, which are relatively efficient models in their respective categories. To be more convincing, we compare their training efficiency from two views:</p><p>• The total training time and epochs for achieving their best performance.</p><p>• Training them with the same epochs to see what performance they can achieve.</p><p>Note that the validation time is not included in the training time.</p><p>Considering the fact that the official implementations of the compared models can be optimized to be more efficient, we use a uniform code framework implemented by ourselves for all models for fair comparison. In particular, our implementations refer to their official versions and optimize them with uniform acceleration methods (e.g, parallel sampling) to ensure the fairness of comparison. We will release all of our code. Experiments are conducted on Amazon-Book with the same Intel(R) Xeon(R) Silver 4210 CPU @ 2.20GHz machine with one GeForce RTX 2080 GPU for all compared models. Results of the two experiments are shown in Table <ref type="table" target="#tab_4">3</ref> and Table <ref type="table" target="#tab_5">4</ref> respectively. We have the following conclusions:</p><p>(1) Table <ref type="table" target="#tab_4">3</ref> shows that the training speed (i.e., Time/Epoch) of Ul-traGCN is close to MF-BPR, which empirically justifies our analysis that the time complexities of UltraGCN and MF are on the same level. UltraGCN needs 75 epochs to converge which is much less than LR-GCCF and LightGCN, leading to only 45 minutes for total training. Finally, UltraGCN has around 14x, 4x, 4x speedup compared with LightGCN, LR-GCCF, and ENMF respectively, demonstrating the big efficiency superiority of UltraGCN.</p><p>(2) Table <ref type="table" target="#tab_5">4</ref> shows that when UltraGCN converges (i.e., train the fixed 75 epochs), the performances of all the other compared models are much worse than UltraGCN. That is to say, UltraGCN can achieve much better performance with less time, which further Table <ref type="table" target="#tab_3">2</ref>: Overall performance comparison. Improv. denotes the relative improvements over the best GNN-based baselines.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Model</head><p>Amazon-Books Yelp2018 Gowalla Movielens-1M Recall@20 NDCG@20 Recall@20 NDCG@20 F1@20 NDCG@20 Recall@20 NDCG@20 </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Ablation Study</head><p>We perform ablation studies on Amazon-Book to justify the following opinions: (i) The designs of UltraGCN is effective, which can flexibly and separately learn the user-item relationships and item-item relationships to improve recommendation performance;</p><p>(ii) Augmenting positive user-item pairs for training to learn itemitem relationships can achieve better performance than optimizing between item-item pairs; (iii) User-user co-occurrence information is probably not very informative to help recommendation.</p><p>For opinion (i), we compare UltraGCN with the following variants to show the effectiveness of our designs in UltraGCN:</p><p>• UltraGCN(𝜆 = 0, 𝛾 = 0): when setting 𝜆 and 𝛾 to 0, UltraGCN is simply reduced to MF training with BCE loss function, which does not leverage graph information and cannot capture higher-order collaborative signals.</p><p>• UltraGCN(𝛾 = 0): this variant is identical to UltraGCN 𝐵𝑎𝑠𝑒 , which only learns on the user-item graph and lacks more effective learning for item-item relationships. • UltraGCN(𝜆 = 0): this variant lacks the graph convolution ability for learning on the user-item graph to more deeply mine the collaborative information. Results are shown in Figure <ref type="figure" target="#fig_1">2</ref>. We have the following observations:</p><p>(1) UltraGCN(𝛾 = 0) and UltraGCN(𝜆 = 0) all perform better than UltraGCN(𝜆 = 0, 𝛾 = 0), demonstrating that the designs of UltraGCN can effectively learn on both the user-item graph and item-item graph to improve recommendation; (2) Relatively, UltraGCN(𝜆 = 0) is inferior to UltraGCN(𝛾 = 0), indicating that user-item relationships may be better modeled than item-item relationships in UltraGCN; (3) UltraGCN performs much better than all the other three variants, demonstrating that our idea to disassemble various relationships, eliminate uninformative things which may disturb the model learning, and finally conduct multi-task learning in a clearer way, is effective to overcome the limitations (see Section 2.2) of previous GCN-based CF models.</p><p>For opinion (ii), we change L 𝐼 to L ′ 𝐼 :</p><formula xml:id="formula_24">L ′ 𝐼 = − ∑︁ (𝑢,𝑖) ∈𝑁 + ∑︁ 𝑗 ∈𝑆 (𝑖) 𝜔 𝑖,𝑗 log(𝜎 (𝑒 ⊤ 𝑖 𝑒 𝑗 ))<label>(19)</label></formula><p>which is instead to optimize between the target positive item and its most 𝐾 similar items. We compare the performance of UltraGCN using L 𝐼 and L ′ 𝐼 respectively with careful parameters tuning. Results are shown in Figure <ref type="figure" target="#fig_2">3</ref>. It is clear that no matter incorporating L 𝐶 or Table <ref type="table">5</ref>: Performance comparison of whether learning on the user-user co-occurrence graph.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Model</head><p>UltraGCN(𝛾 = 0) UltraGCN(𝜆 = 0) UltraGCN Recall@20 NDCG@20 Recall@20 NDCG@20 Recall@20 NDCG@20    not, using L 𝐼 can achieves obvious better performance than using L ′ 𝐼 , which proves that our designed strategy to learn on item-item graph is more effective. Furthermore, the performance gap between using L 𝐼 and using L ′ 𝐼 becomes large when incorporating L 𝐶 , indicating that our strategy which makes the objective of UltraGCN unified can thus facilitate training and improve performance.</p><p>For opinion (iii), we derive the user-user constraint loss L 𝑈 with the similar method of Section 3.2 and combine it to the final objective. We carefully re-tune the parameters and show the comparison results of whether using L 𝑈 in Table <ref type="table">5</ref>. As can be seen, incorporating L 𝑈 to learn user-user relationships does not bring obvious benefits to UltraGCN. We attribute this phenomenon to the fact that the users' interests are broader than items' properties, and thus it is much harder to capture user-user relationships just from the user-user co-occurrence graph. Therefore, we do not introduce the modeling of user-user relationships into UltraGCN in this paper, and we will continue to study it in the future.   </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5">Parameter Analysis</head><p>We investigate the influence of the number of selected neighbors 𝐾 and the weights of the two constraint losses (i.e., 𝜆 and 𝛾) on the performance for a better understanding of UltraGCN. 4.5.1 Impact of 𝐾. We test the performance of UltraGCN with different 𝐾 in <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b9">10,</ref><ref type="bibr" target="#b19">20,</ref><ref type="bibr">50]</ref> on Amazon-Book and Yelp2018. Figure <ref type="figure" target="#fig_4">4</ref> shows the experimental results. We can find that when 𝐾 increases from 5 to 50, the performance shows a trend of increasing first and then decreasing. This is because that when 𝐾 is 5, the item-item relationships are not sufficiently exploited. While when 𝐾 becomes large, there may introduce some less similar or less confident item-item relationships into the learning process that affect model performance. Such phenomenon also confirms that conventional GCN-based CF models inevitably take into account too many low-confidence relationships, thus hurting performance. 4.5.2 Impact of 𝜆 and 𝛾. We first set 𝜆 = 0 and show the performance of different 𝜆 from 0.2 to 1.4 (0.2 as the interval). Then we test with different 𝛾 in [0.1, 0.5, 1, 1.5, 2, 2.5, 3, 3.5] based on the best 𝜆. Experiments are conducted on Amazon-Book, and we show results in Figure <ref type="figure" target="#fig_5">5</ref>. For 𝜆, we find that the small value limits the exertion of the user-item constraint loss, and a value of 1 or so would be suitable for 𝜆. For the impact of 𝛾, its trend is similar to 𝜆 but is more significant, and 2.5 is a suitable choice for 𝛾. In general, our investigations for 𝜆 and 𝛾 show that these two parameters are important to UltraGCN, which can flexibly adjust the learning weights for different relationships and should be carefully set.</p><p>In this section, we briefly review some representative GNN-based methods and their efforts for model simplification toward recommendation tasks.</p><p>With the development and success of GNN in various machine learning areas, there appears a lot of excellent work in recommendation community since the interaction of users and items could be naturally formed to a user-item bipartite graph. Rianne van den Berg et al. <ref type="bibr" target="#b0">[1]</ref> propose graph convolutional matrix completion (GC-MC), a graph-based auto-encoder framework for explicit matrix completion. The encoder of GC-MC aggregates the information from neighbors based on the types of ratings, and then combine it to the new embeddings of the next layer. It is the first work using graph convolutional neural networks for recommendation. Ying et al. <ref type="bibr" target="#b31">[31]</ref> first applys GCN on web-scale recommender systems and propose an efficient GCN-based method named Pinsage, which combines efficient random walks and graph convolutions to generate embeddings of items that incorporate both graph structure as well as item feature information. Then, Wang et al. <ref type="bibr" target="#b27">[27]</ref> design NGCF which is a new graph-based framework for collaborative filtering. NGCF has a crafted interaction encoder to capture the collaborative signals among users and items. Although NGCF achieves good performance compared with previous non-GNN based methods, its heavy designs limit its efficiency and full exertion of GCN. To model the diversity of user intents on items, Wang et al. <ref type="bibr" target="#b28">[28]</ref> devise Disentangled Graph Collaborative Filtering (DGCF), which considers user-item relationships at the finer granularity of user intents and generates disentangled user and item representations to get better recommendation performance.</p><p>Although GNN-based recommendation models have achieved impressive performance, their efficiencies are still unsatisfactory when facing large-scale recommendation scenarios. How to improve the efficiency of GNNs and reserve their high performance for recommendation becomes a hot research problem. Recently, Dai et al. <ref type="bibr" target="#b5">[6]</ref> and Gu et al. <ref type="bibr" target="#b7">[8]</ref> extend fixed-point theory on GNN for better representation learning. Liu et al. <ref type="bibr" target="#b17">[18]</ref> propose UCMF that simplifies GCN for the node classification task. Wu et al. <ref type="bibr" target="#b29">[29]</ref> find the non-necessity of nonlinear activation and feature transformation in GCN, proposing a simplified GCN (SGCN) model by removing these two parts. Inspired by SGC, He et al. <ref type="bibr" target="#b9">[10]</ref> devise LightGCN for recommendation by removing nonlinear activation and feature transformation too. However, its efficiency is still limited by the time-consuming message passing. Qiu et al. <ref type="bibr" target="#b20">[21]</ref> demonstrate that many network embedding algorithms with negative sampling can be unified into the MF framework which may be efficient, however, their performances still have a gap between that of GCNs. We are inspired by these instructive studies, and propose UltraGCN for both efficient and effective recommendation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">CONCLUSION</head><p>In this work, we propose an ultra-simplified formulation of GCN, dubbed UltraGCN. UltraGCN skips explicit message passing and directly approximate the limit of infinite message passing layers. Extensive experimental results demonstrate that UltraGCN achieves impressive improvements over the state-of-the-art CF models in terms of both accuracy and efficiency. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8">APPENDIX</head><p>To further demonstrate the effectiveness of UltraGCN, we additionally provide the results compared to some more recent stateof-the-art CF models, including NBPO <ref type="bibr" target="#b33">[33]</ref>, BGCF <ref type="bibr" target="#b23">[23]</ref>, SCF <ref type="bibr" target="#b34">[34]</ref>, LCFN <ref type="bibr" target="#b32">[32]</ref>, and SGL-ED <ref type="bibr" target="#b30">[30]</ref>. For simplicity and fairness of comparison, we use the same dataset and evaluation protocol provided by each paper. We also duplicate the results reported in their papers to keep consistency. The results in Table <ref type="table" target="#tab_7">6</ref> again validate the effectiveness of UltraGCN, which outperforms the most recent CF models by a large margin. </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: Performance comparison of variants of UltraGCN.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: Performance comparison of using L ′ 𝐼 and L 𝐼 .</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 4 :</head><label>4</label><figDesc>Figure 4: Performance comparison of setting different 𝐾.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 5 :</head><label>5</label><figDesc>Figure 5: Performance comparison with different 𝜆 and 𝛾.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>7 ACKNOWLEDGEMENT</head><label>7</label><figDesc>This work was supported in part by the National Natural Science Foundation of China (61972219), the Research and Development Program of Shenzhen (JCYJ20190813174403598, SGDX20190918101201696), the National Key Research and Development Program of China (2018YFB1800601), and the Overseas Research Cooperation Fund of Tsinghua Shenzhen International Graduate School (HW2021013).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 1 :</head><label>1</label><figDesc>Statistics of the datasets.</figDesc><table><row><cell>Dataset</cell><cell cols="3">#Users #Items #Interactions Density</cell></row><row><cell cols="3">Amazon-Book 52, 643 91, 599</cell><cell>2, 984, 108 0.062%</cell></row><row><cell>Yelp2018</cell><cell cols="2">31, 668 38, 048</cell><cell>1, 561, 406 0.130%</cell></row><row><cell>Gowalla</cell><cell cols="2">29, 858 40, 981</cell><cell>1, 027, 370 0.084%</cell></row><row><cell>Movielens-1M</cell><cell>6,022</cell><cell>3,043</cell><cell>995, 154 5.431%</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 2</head><label>2</label><figDesc></figDesc><table><row><cell>reports the performance comparison results. We have the</cell></row><row><cell>following observations:</cell></row><row><cell>• UltraGCN consistently yields the best performance across all</cell></row><row><cell>four datasets. In particular, UltraGCN hugely improves over</cell></row><row><cell>the strongest GCN-based baseline (i.e., DGCF) on Amazon-</cell></row><row><cell>Book by 61.4% and 71.6% w.r.t. Recall@20 and NDCG@20</cell></row><row><cell>respectively. The results of significance testing indicates that</cell></row><row><cell>our improvements over the current strongest GCN-based</cell></row><row><cell>baseline are statistically significant (𝑝-value &lt; 0.05). With ad-</cell></row><row><cell>ditional learning on the item-item graph, UltraGCN performs</cell></row><row><cell>consistently better than its simpler variant UltraGCN 𝐵𝑎𝑠𝑒 .</cell></row><row><cell>We attribute such good performance of UltraGCN to the</cell></row><row><cell>following reasons: 1) Compared with network embedding</cell></row><row><cell>models and the other GCN-based models, UltraGCN can</cell></row><row><cell>respectively filter uninformative user-item and item-item re-</cell></row><row><cell>lationships in a soft way (i.e., optimize with 𝛽) and a hard way</cell></row><row><cell>(i.e., only select 𝐾 most similar item pairs). The edge weights</cell></row><row><cell>for the learning of user-item and item-item relationships in</cell></row><row><cell>UltraGCN are also more reasonable; 2) Compared with other</cell></row><row><cell>baselines, UltraGCN can leverage powerful graph convolu-</cell></row><row><cell>tion to exploit useful and deeper collaborative information</cell></row></table><note>1 https://github.com/xue-pai/UltraGCN</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 3 :</head><label>3</label><figDesc>Efficiency comparison from the first view.</figDesc><table><row><cell cols="2">MF-BPR</cell><cell>0.0338</cell><cell>0.0261</cell><cell>0.0549</cell><cell>0.0445</cell><cell>0.1616</cell><cell>0.1366</cell><cell>0.2153</cell><cell>0.2175</cell></row><row><cell>CML</cell><cell></cell><cell>0.0522</cell><cell>0.0428</cell><cell>0.0622</cell><cell>0.0536</cell><cell>0.1670</cell><cell>0.1292</cell><cell>0.1730</cell><cell>0.1563</cell></row><row><cell>ENMF</cell><cell></cell><cell>0.0359</cell><cell>0.0281</cell><cell>0.0624</cell><cell>0.0515</cell><cell>0.1523</cell><cell>0.1315</cell><cell>0.2315</cell><cell>0.2069</cell></row><row><cell cols="2">DeepWalk</cell><cell>0.0346</cell><cell>0.0264</cell><cell>0.0476</cell><cell>0.0378</cell><cell>0.1034</cell><cell>0.0740</cell><cell>0.1348</cell><cell>0.1057</cell></row><row><cell>LINE</cell><cell></cell><cell>0.0410</cell><cell>0.0318</cell><cell>0.0549</cell><cell>0.0446</cell><cell>0.1335</cell><cell>0.1056</cell><cell>0.2336</cell><cell>0.2226</cell></row><row><cell cols="2">Node2Vec</cell><cell>0.0402</cell><cell>0.0309</cell><cell>0.0452</cell><cell>0.0360</cell><cell>0.1019</cell><cell>0.0709</cell><cell>0.1475</cell><cell>0.1186</cell></row><row><cell>NGCF</cell><cell></cell><cell>0.0344</cell><cell>0.0263</cell><cell></cell><cell>0.0477</cell><cell>0.1570</cell><cell>0.1327</cell><cell>0.2513</cell><cell>0.2511</cell></row><row><cell cols="2">NIA-GCN</cell><cell>0.0369</cell><cell>0.0287</cell><cell>0.0599</cell><cell>0.0491</cell><cell>0.1359</cell><cell>0.1106</cell><cell>0.2359</cell><cell>0.2242</cell></row><row><cell cols="2">LR-GCCF</cell><cell>0.0335</cell><cell>0.0265</cell><cell>0.0561</cell><cell>0.0343</cell><cell>0.1519</cell><cell>0.1285</cell><cell>0.2231</cell><cell>0.2124</cell></row><row><cell cols="2">LightGCN</cell><cell>0.0411</cell><cell>0.0315</cell><cell>0.0649</cell><cell>0.0530</cell><cell>0.1830</cell><cell>0.1554</cell><cell>0.2576</cell><cell>0.2427</cell></row><row><cell>DGCF</cell><cell></cell><cell>0.0422</cell><cell>0.0324</cell><cell>0.0654</cell><cell>0.0534</cell><cell>0.1842</cell><cell>0.1561</cell><cell>0.2640</cell><cell>0.2504</cell></row><row><cell cols="2">UltraGCN 𝐵𝑎𝑠𝑒</cell><cell>0.0504</cell><cell>0.0393</cell><cell>0.0667</cell><cell>0.0552</cell><cell>0.1845</cell><cell>0.1566</cell><cell>0.2671</cell><cell>0.2539</cell></row><row><cell cols="2">UltraGCN</cell><cell>0.0681</cell><cell>0.0556</cell><cell>0.0683</cell><cell>0.0561</cell><cell>0.1862</cell><cell>0.1580</cell><cell>0.2787</cell><cell>0.2642</cell></row><row><cell cols="2">Improv.</cell><cell>61.37%</cell><cell>71.60%</cell><cell>4.43%</cell><cell>5.06%</cell><cell>1.09%</cell><cell>1.22%</cell><cell>5.57%</cell><cell>5.51%</cell></row><row><cell cols="2">𝑝-value</cell><cell>4.03e-8</cell><cell>5.64e-8</cell><cell>1.61e-4</cell><cell>1.24e-4</cell><cell>7.21e-3</cell><cell>3.44e-4</cell><cell>4.19e-5</cell><cell>2.23e-5</cell></row><row><cell>Model</cell><cell cols="4">Time/Epoch #Epoch Training Time</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>MF-BPR</cell><cell>30s</cell><cell>23</cell><cell>12m</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>ENMF</cell><cell>129s</cell><cell>81</cell><cell cols="2">2h54m</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>LR-GCCF</cell><cell>67s</cell><cell>165</cell><cell>3h5m</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>LightGCN</cell><cell>51s</cell><cell>780</cell><cell cols="2">11h3m</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>UltraGCN</cell><cell>36s</cell><cell>75</cell><cell>45m</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 4 :</head><label>4</label><figDesc>Efficiency comparison from the second view. All models are trained with the fixed 75 epochs except MF-BPR. Since MF-BPR needs less than 75 epochs to converge, we report its actual training time.</figDesc><table><row><cell>Model</cell><cell cols="3">Training Time Recall@20 NDCG@20</cell></row><row><cell>MF-BPR</cell><cell>12m</cell><cell>0.0338</cell><cell>0.0261</cell></row><row><cell>ENMF</cell><cell>2h41m</cell><cell>0.0355</cell><cell>0.0277</cell></row><row><cell>LR-GCCF</cell><cell>1h13m</cell><cell>0.0304</cell><cell>0.0185</cell></row><row><cell>LightGCN</cell><cell>1h4m</cell><cell>0.0342</cell><cell>0.0262</cell></row><row><cell>UltraGCN</cell><cell>45m</cell><cell>0.0681</cell><cell>0.0556</cell></row><row><cell cols="4">demonstrates the higher efficiency of UltraGCN than the other</cell></row><row><cell cols="2">GCN-based CF models.</cell><cell></cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 6 :</head><label>6</label><figDesc>Performance comparison with some more models, including SCF, LCFN, NBPO, BGCF, and SGL-ED.</figDesc><table><row><cell></cell><cell cols="2">Movielens-1M</cell></row><row><cell>Model</cell><cell>F1@20</cell><cell>NDCG@20</cell></row><row><cell>NGCF</cell><cell>0.1582</cell><cell>0.2511</cell></row><row><cell>SCF</cell><cell>0.1600</cell><cell>0.2560</cell></row><row><cell>LCFN</cell><cell>0.1625</cell><cell>0.2603</cell></row><row><cell>UltraGCN</cell><cell>0.2004</cell><cell>0.2642</cell></row><row><cell>Improv.</cell><cell>23.3%</cell><cell>1.5%</cell></row><row><cell cols="3">Amazon-Electronics</cell></row><row><cell>Model</cell><cell>F1@20</cell><cell>NDCG@20</cell></row><row><cell>MF-BPR</cell><cell>0.0275</cell><cell>0.0680</cell></row><row><cell>ENMF</cell><cell>0.0314</cell><cell>0.0823</cell></row><row><cell>NBPO</cell><cell>0.0313</cell><cell>0.0810</cell></row><row><cell>UltraGCN</cell><cell>0.0330</cell><cell>0.0829</cell></row><row><cell>Improv.</cell><cell>5.1%</cell><cell>0.7%</cell></row><row><cell></cell><cell cols="2">Amazon-CDs</cell></row><row><cell>Model</cell><cell cols="2">Recall@20 NDCG@20</cell></row><row><cell>NGCF</cell><cell>0.1258</cell><cell>0.0792</cell></row><row><cell>NIA-GCN</cell><cell>0.1487</cell><cell>0.0932</cell></row><row><cell>BGCF</cell><cell>0.1506</cell><cell>0.0948</cell></row><row><cell>UltraGCN</cell><cell>0.1622</cell><cell>0.1043</cell></row><row><cell>Improv.</cell><cell>7.7%</cell><cell>10.0%</cell></row><row><cell></cell><cell cols="2">Amazon-Books</cell></row><row><cell>Model</cell><cell cols="2">Recall@20 NDCG@20</cell></row><row><cell>NGCF</cell><cell>0.0344</cell><cell>0.0263</cell></row><row><cell>LightGCN</cell><cell>0.0411</cell><cell>0.0315</cell></row><row><cell>SGL-ED</cell><cell>0.0478</cell><cell>0.0379</cell></row><row><cell>UltraGCN</cell><cell>0.0681</cell><cell>0.0556</cell></row><row><cell>Improv.</cell><cell>42.5%</cell><cell>46.7%</cell></row></table></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Graph Convolutional Matrix Completion</title>
		<author>
			<persName><forename type="first">Rianne</forename><surname>Van Den</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Thomas</forename><forename type="middle">N</forename><surname>Berg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Max</forename><surname>Kipf</surname></persName>
		</author>
		<author>
			<persName><surname>Welling</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">KDD&apos;18 Deep Learning Day</title>
				<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">An Analysis of the Softmax Cross Entropy Loss for Learning-to-Rank with Binary Relevance</title>
		<author>
			<persName><forename type="first">Sebastian</forename><surname>Bruch</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xuanhui</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><surname>Bendersky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Marc</forename><surname>Najork</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2019 ACM SIGIR International Conference on Theory of Information Retrieval (SIGIR)</title>
				<meeting>the 2019 ACM SIGIR International Conference on Theory of Information Retrieval (SIGIR)</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="75" to="78" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Efficient Neural Matrix Factorization without Sampling for Recommendation</title>
		<author>
			<persName><forename type="first">Chong</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Min</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yongfeng</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yiqun</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shaoping</forename><surname>Ma</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Transactions on Information Systems (TOIS)</title>
		<imprint>
			<biblScope unit="volume">38</biblScope>
			<biblScope unit="page" from="1" to="28" />
			<date type="published" when="2020">2020. 2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Revisiting Graph Based Collaborative Filtering: A Linear Residual Graph Convolutional Network Approach</title>
		<author>
			<persName><forename type="first">Lei</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Le</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Richang</forename><surname>Hong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kun</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Meng</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the AAAI Conference on Artificial Intelligence (AAAI)</title>
				<meeting>the AAAI Conference on Artificial Intelligence (AAAI)</meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="27" to="34" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Simple and Deep Graph Convolutional Networks</title>
		<author>
			<persName><forename type="first">Ming</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhewei</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zengfeng</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bolin</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yaliang</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning (ICML)</title>
				<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="1725" to="1735" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Learning Steady-states of Iterative over Graphs</title>
		<author>
			<persName><forename type="first">Hanjun</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zornitsa</forename><surname>Kozareva</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bo</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alex</forename><surname>Smola</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Le</forename><surname>Song</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning (ICML)</title>
				<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="1106" to="1114" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Node2vec: Scalable Feature Learning for Networks</title>
		<author>
			<persName><forename type="first">Aditya</forename><surname>Grover</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jure</forename><surname>Leskovec</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 22nd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining (SIGKDD)</title>
				<meeting>the 22nd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining (SIGKDD)</meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="855" to="864" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Implicit Graph Neural Networks</title>
		<author>
			<persName><forename type="first">Fangda</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Heng</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wenwu</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Somayeh</forename><surname>Sojoudi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Laurent</forename><forename type="middle">El</forename><surname>Ghaoui</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems (NeurIPS)</title>
				<imprint>
			<date type="published" when="2020">2020. 2020</date>
			<biblScope unit="page" from="11984" to="11995" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Ups and Downs: Modeling the Visual Evolution of Fashion Trends with One-Class Collaborative Filtering</title>
		<author>
			<persName><forename type="first">Ruining</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Julian</forename><surname>Mcauley</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 25th International Conference on World Wide Web (WWW)</title>
				<meeting>the 25th International Conference on World Wide Web (WWW)</meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="507" to="517" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">LightGCN: Simplifying and Powering Graph Convolution Network for Recommendation</title>
		<author>
			<persName><forename type="first">Xiangnan</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kuan</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yan</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yong-Dong</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Meng</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 43rd International ACM SIGIR conference on research and development in Information Retrieval (SIGIR)</title>
				<meeting>the 43rd International ACM SIGIR conference on research and development in Information Retrieval (SIGIR)</meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="639" to="648" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Neural Collaborative Filtering</title>
		<author>
			<persName><forename type="first">Xiangnan</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lizi</forename><surname>Liao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hanwang</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Liqiang</forename><surname>Nie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xia</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tat-Seng</forename><surname>Chua</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 26th International Conference on World Wide Web (WWW)</title>
				<meeting>the 26th International Conference on World Wide Web (WWW)</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="173" to="182" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Collaborative Metric Learning</title>
		<author>
			<persName><forename type="first">Cheng-Kang</forename><surname>Hsieh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Longqi</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yin</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tsung-Yi</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Serge</forename><surname>Belongie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Deborah</forename><surname>Estrin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 26th International Conference on World Wide Web (WWW)</title>
				<meeting>the 26th International Conference on World Wide Web (WWW)</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="193" to="201" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Dual Channel Hypergraph Collaborative Filtering</title>
		<author>
			<persName><forename type="first">Shuyi</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yifan</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rongrong</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xibin</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wanwan</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yue</forename><surname>Gao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 26th ACM SIGKDD International Conference on Knowledge Discovery &amp; Data Mining (SIGKDD)</title>
				<meeting>the 26th ACM SIGKDD International Conference on Knowledge Discovery &amp; Data Mining (SIGKDD)</meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="2020" to="2029" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Semi-Supervised Classification with Graph Convolutional Networks</title>
		<author>
			<persName><forename type="first">Thomas</forename><forename type="middle">N</forename><surname>Kipf</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Max</forename><surname>Welling</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations (ICLR)</title>
				<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Matrix Factorization Techniques for Recommender Systems</title>
		<author>
			<persName><forename type="first">Yehuda</forename><surname>Koren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Robert</forename><surname>Bell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chris</forename><surname>Volinsky</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computer</title>
		<imprint>
			<biblScope unit="volume">42</biblScope>
			<biblScope unit="page" from="30" to="37" />
			<date type="published" when="2009">2009. 2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Deeper Insights into Graph Convolutional Networks for Semi-Supervised Learning</title>
		<author>
			<persName><forename type="first">Qimai</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhichao</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiao-Ming</forename><surname>Wu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the AAAI Conference on Artificial Intelligence (AAAI)</title>
				<meeting>the AAAI Conference on Artificial Intelligence (AAAI)</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="page" from="3538" to="3545" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">RGCF: Refined Graph Convolution Collaborative Filtering with Concise and Expressive embedding</title>
		<author>
			<persName><forename type="first">Kang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Feng</forename><surname>Xue</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Richang</forename><surname>Hong</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2007.03383</idno>
		<imprint>
			<date type="published" when="2020">2020. 2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<author>
			<persName><forename type="first">Qiang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Haoli</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhaocheng</forename><surname>Liu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2007.09036</idno>
		<title level="m">Simplification of Graph Convolutional Networks: A Matrix Factorization-based Perspective</title>
				<imprint>
			<date type="published" when="2020">2020. 2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Distributed Representations of Words and Phrases and their Compositionality</title>
		<author>
			<persName><forename type="first">Tomas</forename><surname>Mikolov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kai</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Greg</forename><forename type="middle">S</forename><surname>Corrado</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jeff</forename><surname>Dean</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems (NeurIPS)</title>
				<imprint>
			<date type="published" when="2013">2013</date>
			<biblScope unit="page" from="3111" to="3119" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Deepwalk: Online Learning of Social Representations</title>
		<author>
			<persName><forename type="first">Bryan</forename><surname>Perozzi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rami</forename><surname>Al-Rfou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Steven</forename><surname>Skiena</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 20th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining (SIGKDD)</title>
				<meeting>the 20th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining (SIGKDD)</meeting>
		<imprint>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="701" to="710" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Network Embedding as Matrix Factorization: Unifying Deepwalk, LINE, PTE, and Node2vec</title>
		<author>
			<persName><forename type="first">Jiezhong</forename><surname>Qiu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuxiao</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hao</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jian</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kuansan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jie</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Eleventh ACM International Conference on Web Search and Data Mining (WSDM)</title>
				<meeting>the Eleventh ACM International Conference on Web Search and Data Mining (WSDM)</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="459" to="467" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title/>
		<author>
			<persName><forename type="first">Steffen</forename><surname>Rendle</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christoph</forename><surname>Freudenthaler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zeno</forename><surname>Gantner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lars</forename><surname>Schmidt-Thieme</surname></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">BPR: Bayesian Personalized Ranking from Implicit Feedback</title>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Twenty-Fifth Conference on Uncertainty in Artificial Intelligence (UAI)</title>
				<meeting>the Twenty-Fifth Conference on Uncertainty in Artificial Intelligence (UAI)</meeting>
		<imprint>
			<biblScope unit="page" from="452" to="461" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">A Framework for Recommending Accurate and Diverse Items Using Bayesian Graph Convolutional Neural Networks</title>
		<author>
			<persName><forename type="first">Jianing</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wei</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dengcheng</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yingxue</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Florence</forename><surname>Regol</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yaochen</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Huifeng</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ruiming</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Han</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiuqiang</forename><surname>He</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 26th ACM SIGKDD International Conference on Knowledge Discovery &amp; Data Mining (SIGKDD)</title>
				<meeting>the 26th ACM SIGKDD International Conference on Knowledge Discovery &amp; Data Mining (SIGKDD)</meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="2030" to="2039" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Neighbor Interaction Aware Graph Convolution Networks for Recommendation</title>
		<author>
			<persName><forename type="first">Jianing</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yingxue</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wei</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Huifeng</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ruiming</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiuqiang</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chen</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mark</forename><surname>Coates</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 43rd International ACM SIGIR Conference on Research and Development in Information Retrieval (SIGIR)</title>
				<meeting>the 43rd International ACM SIGIR Conference on Research and Development in Information Retrieval (SIGIR)</meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="1289" to="1298" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Line: Large-scale Information Network Embedding</title>
		<author>
			<persName><forename type="first">Jian</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Meng</forename><surname>Qu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mingzhe</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ming</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jun</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qiaozhu</forename><surname>Mei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 24th International Conference on World Wide Web (WWW)</title>
				<meeting>the 24th International Conference on World Wide Web (WWW)</meeting>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="1067" to="1077" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">M2GRL: A Multi-task Multi-view Graph Representation Learning Framework for Web-scale Recommender Systems</title>
		<author>
			<persName><forename type="first">Menghan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yujie</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Guli</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Keping</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiao-Ming</forename><surname>Wu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 26th ACM SIGKDD International Conference on Knowledge Discovery &amp; Data Mining (SIGKDD)</title>
				<meeting>the 26th ACM SIGKDD International Conference on Knowledge Discovery &amp; Data Mining (SIGKDD)</meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="2349" to="2358" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Neural Graph Collaborative Filtering</title>
		<author>
			<persName><forename type="first">Xiang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiangnan</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Meng</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fuli</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tat-Seng</forename><surname>Chua</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 42nd International ACM SIGIR Conference on Research and Development in Information Retrieval (SIGIR)</title>
				<meeting>the 42nd International ACM SIGIR Conference on Research and Development in Information Retrieval (SIGIR)</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="165" to="174" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Disentangled Graph Collaborative Filtering</title>
		<author>
			<persName><forename type="first">Xiang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hongye</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">An</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiangnan</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tong</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tat-Seng</forename><surname>Chua</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 43rd International ACM SIGIR Conference on Research and Development in Information Retrieval (SIGIR)</title>
				<meeting>the 43rd International ACM SIGIR Conference on Research and Development in Information Retrieval (SIGIR)</meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="1001" to="1010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Simplifying Graph Convolutional Networks</title>
		<author>
			<persName><forename type="first">Felix</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Amauri</forename><surname>Souza</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tianyi</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christopher</forename><surname>Fifty</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tao</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kilian</forename><surname>Weinberger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning (ICML)</title>
				<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="6861" to="6871" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Self-supervised Graph Learning for Recommendation</title>
		<author>
			<persName><forename type="first">Jiancan</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fuli</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiangnan</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Liang</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jianxun</forename><surname>Lian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xing</forename><surname>Xie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 44th International ACM SIGIR Conference on Research and Development in Information Retrieval (SIGIR)</title>
				<meeting>the 44th International ACM SIGIR Conference on Research and Development in Information Retrieval (SIGIR)</meeting>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="726" to="735" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Graph Convolutional Neural Networks for Web-scale Recommender Systems</title>
		<author>
			<persName><forename type="first">Rex</forename><surname>Ying</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ruining</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kaifeng</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pong</forename><surname>Eksombatchai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">William</forename><forename type="middle">L</forename><surname>Hamilton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jure</forename><surname>Leskovec</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 24th ACM SIGKDD International Conference on Knowledge Discovery &amp; Data Mining (SIGKDD)</title>
				<meeting>the 24th ACM SIGKDD International Conference on Knowledge Discovery &amp; Data Mining (SIGKDD)</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="974" to="983" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Graph Convolutional Network for Recommendation with Low-pass Collaborative Filters</title>
		<author>
			<persName><forename type="first">Wenhui</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zheng</forename><surname>Qin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning (ICML)</title>
				<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="10936" to="10945" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Sampler Design for Implicit Feedback Data by Noisy-label Robust Learning</title>
		<author>
			<persName><forename type="first">Wenhui</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zheng</forename><surname>Qin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 43rd International ACM SIGIR Conference on Research and Development in Information Retrieval (SIGIR)</title>
				<meeting>the 43rd International ACM SIGIR Conference on Research and Development in Information Retrieval (SIGIR)</meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="861" to="870" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Spectral Collaborative Filtering</title>
		<author>
			<persName><forename type="first">Lei</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chun-Ta</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fei</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiawei</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Philip</forename><forename type="middle">S</forename><surname>Yu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 12th ACM Conference on Recommender Systems (RecSys)</title>
				<meeting>the 12th ACM Conference on Recommender Systems (RecSys)</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="311" to="319" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
