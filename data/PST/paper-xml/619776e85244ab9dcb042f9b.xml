<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">SINGLE LAYERS OF ATTENTION SUFFICE TO PREDICT PROTEIN CONTACTS</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Nicholas</forename><surname>Bhattacharya</surname></persName>
							<affiliation key="aff6">
								<orgName type="department">Equal Contribution</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">U</forename><forename type="middle">C</forename><surname>Berkeley</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Neil</forename><surname>Thomas</surname></persName>
							<email>nthomas@berkeley.edu</email>
							<affiliation key="aff6">
								<orgName type="department">Equal Contribution</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Roshan</forename><surname>Rao</surname></persName>
							<email>rmrao@berkeley.edu</email>
						</author>
						<author>
							<persName><forename type="first">Justas</forename><surname>Dauparas</surname></persName>
							<email>justas@uw.edu</email>
						</author>
						<author>
							<persName><forename type="first">Peter</forename><forename type="middle">K</forename><surname>Koo</surname></persName>
							<email>koo@cshl.edu</email>
						</author>
						<author>
							<persName><forename type="first">David</forename><surname>Baker</surname></persName>
							<email>dabaker@uw.edu</email>
						</author>
						<author>
							<persName><forename type="first">Yun</forename><forename type="middle">S</forename><surname>Song</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Sergey</forename><surname>Ovchinnikov</surname></persName>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="institution">UC Berkeley</orgName>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff1">
								<orgName type="institution">UC Berkeley</orgName>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff2">
								<orgName type="institution">University of Washington</orgName>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff3">
								<orgName type="laboratory">Cold Spring Harbor Laboratory</orgName>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff4">
								<orgName type="institution">University of Washington</orgName>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff5">
								<orgName type="institution">Harvard University</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">SINGLE LAYERS OF ATTENTION SUFFICE TO PREDICT PROTEIN CONTACTS</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="DOI">10.1101/2020.12.21.423882</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.2" ident="GROBID" when="2023-01-01T13:36+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>The established approach to unsupervised protein contact prediction estimates coevolving positions using undirected graphical models. This approach trains a Potts model on a Multiple Sequence Alignment, then predicts that the edges with highest weight correspond to contacts in the 3D structure. On the other hand, increasingly large Transformers are being pretrained on protein sequence databases but have demonstrated mixed results for downstream tasks, including contact prediction. This has sparked discussion about the role of scale and attention-based models in unsupervised protein representation learning. We argue that attention is a principled model of protein interactions, grounded in real properties of protein family data. We introduce a simplified attention layer, factored attention, and show that it achieves comparable performance to Potts models, while sharing parameters both within and across families. Further, we extract contacts from the attention maps of a pretrained Transformer and show they perform competitively with the other two approaches. This provides evidence that large-scale pretraining can learn meaningful protein features when presented with unlabeled and unaligned data. We contrast factored attention with the Transformer to indicate that the Transformer leverages hierarchical signal in protein family databases not captured by our single-layer models. This raises the exciting possibility for the development of powerful structured models of protein family databases. 1</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">INTRODUCTION</head><p>Inferring protein structure from sequence is a longstanding problem in computational biochemistry. Potts models, a particular kind of Markov Random Field (MRF), are the predominant unsupervised method for modeling interactions between amino acids. Potts models are trained to maximize pseudolikelihood on alignments of evolutionarily related proteins <ref type="bibr" target="#b2">(Balakrishnan et al., 2011;</ref><ref type="bibr" target="#b12">Ekeberg et al., 2013;</ref><ref type="bibr" target="#b39">Seemayer et al., 2014)</ref>. Features derived from Potts models were the main drivers of performance at the CASP11 competition <ref type="bibr" target="#b31">(Monastyrskyy et al., 2016)</ref> and have become standard in state-of-the-art supervised models <ref type="bibr">(Wang et al., 2017;</ref><ref type="bibr" target="#b55">Yang et al., 2019;</ref><ref type="bibr" target="#b40">Senior et al., 2020)</ref>.</p><p>Inspired by the success of BERT <ref type="bibr" target="#b8">(Devlin et al., 2018)</ref>, <ref type="bibr">GPT (Brown et al., 2020)</ref> and related unsupervised models in NLP, a line of work has emerged that learns features of proteins through self-supervised pretraining <ref type="bibr" target="#b37">(Rives et al., 2020;</ref><ref type="bibr" target="#b13">Elnaggar et al., 2020;</ref><ref type="bibr" target="#b35">Rao et al., 2019;</ref><ref type="bibr" target="#b29">Madani et al., 2020;</ref><ref type="bibr" target="#b32">Nambiar et al., 2020)</ref>. This new approach trains Transformer <ref type="bibr" target="#b49">(Vaswani et al., 2017</ref>) models 2 BACKGROUND Proteins are polymers composed of amino acids and are commonly represented as strings. Along with this 1D sequence representation, each protein folds into a 3D physical structure. Physical distance between positions in 3D is often a much better indicator of functional interaction than proximity in sequence. One representation of physical distance is a contact map C, a symmetric matrix in which entry C ij = 1 if the β carbons of i and j are within 8 Å of one another, and 0 otherwise.</p><p>Multiple Sequence Alignments. To understand structure and function of a protein sequence, one typically assembles a set of its evolutionary relatives and looks for patterns within the set. A set of related sequences is referred to as a protein family, commonly represented by a Multiple Sequence Alignment (MSA). Gaps in aligned sequences correspond to insertions from an alignment algorithm <ref type="bibr" target="#b21">(Johnson et al., 2010;</ref><ref type="bibr" target="#b36">Remmert et al., 2012)</ref>, ensuring that positions with similar structure and function line up for all members of the family. After aligning, sequence position carries significant evolutionary, structural, and functional information. See Appendix A.1 for more information.</p><p>Coevolutionary Analysis of Protein Families. The observation that statistical patterns in MSAs can be used to predict couplings has been widely used to infer structure and function from protein families <ref type="bibr" target="#b24">(Korber et al., 1993;</ref><ref type="bibr" target="#b17">Göbel et al., 1994;</ref><ref type="bibr" target="#b25">Lapedes et al., 1999;</ref><ref type="bibr" target="#b27">Lockless &amp; Ranganathan, 1999;</ref><ref type="bibr" target="#b16">Fodor &amp; Aldrich, 2004;</ref><ref type="bibr" target="#b48">Thomas et al., 2008;</ref><ref type="bibr" target="#b52">Weigt et al., 2009;</ref><ref type="bibr" target="#b15">Fatakia et al., 2009)</ref>. Let X i be the amino acid at position i sampled from a particular family. High mutual information between  The proteins are all loops formed by one blue and one yellow amino acid locking together. The MSA for this family aligns these critical yellow and blue amino acids. For the trained MRF on this MSA, the weight matrix W 4,1 has the highest values due to the evolutionary constraint that blue and yellow covary for those positions. In this case, the highest predict contact recapitulates a true contact.</p><p>X i and X j suggests an interaction between positions i and j. The main challenge in estimating contacts from mutual information is to disentangle "direct couplings" corresponding to functional interactions from interactions induced by non-functional patterns <ref type="bibr" target="#b25">(Lapedes et al., 1999;</ref><ref type="bibr" target="#b52">Weigt et al., 2009)</ref>. State-of-the-art estimates interactions from MRF parameters, as described below.</p><p>Supervised Structure Prediction. Modern structure prediction methods take a supervised approach, taking MSAs as inputs and outputting predicted structural features. Deep Learning has greatly advanced state of the art for supervised contact prediction <ref type="bibr">(Wang et al., 2017;</ref><ref type="bibr" target="#b22">Jones &amp; Kandathil, 2018;</ref><ref type="bibr" target="#b40">Senior et al., 2020;</ref><ref type="bibr" target="#b0">Adhikari, 2020)</ref>. These methods train deep residual networks that take covariance statistics or coevolutionary parameters as inputs and output contact maps or distance matrices. Extraction of contacts without supervised structural signal has not seen competitive performance from neural networks until the recent introduction of Transformers pretrained on protein databases.</p><p>Attention-Based Protein Models. <ref type="bibr" target="#b13">Elnaggar et al. (2020)</ref>; <ref type="bibr" target="#b37">Rives et al. (2020)</ref>; <ref type="bibr" target="#b35">Rao et al. (2019)</ref>; <ref type="bibr" target="#b29">Madani et al. (2020)</ref> train unsupervised attention-based models on millions to billions of protein sequences and show that pretraining can produce good representations. <ref type="bibr" target="#b35">Rao et al. (2019)</ref> and <ref type="bibr" target="#b37">Rives et al. (2020)</ref> assess embeddings as inputs to supervised structure prediction methods. When protein structure is known, <ref type="bibr" target="#b19">Ingraham et al. (2019)</ref> and <ref type="bibr" target="#b9">Du et al. (2019)</ref> have used sparse attention based on the physical structure of the protein to directly encode priors about interaction. <ref type="bibr" target="#b53">Widrich et al. (2020)</ref> and <ref type="bibr" target="#b34">Ramsauer et al. (2020)</ref> connect Hopfield networks to attention, then apply them to immune repertoire classification.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">METHODS</head><p>Throughout this section, x = (x 1 , . . . , x L ) is a sequence of length L from an alphabet of size A. This sequence is part of an MSA of length L with N total sequences. Recall that a fully-connected Pairwise MRF over p variables X 1 , . . . , X p specifies a distribution</p><formula xml:id="formula_0">p θ (x 1 , . . . , x p ) = 1 Z exp i&lt;j E θ (x i , x j ) ,</formula><p>where Z is the partition function and E θ (x i , x j ) is an arbitrary function of i, j, x i and x j . For all models below, we can introduce an explicit functional E θ (x i ) to capture the marginal distribution of X i . When introduced, we parametrize the marginal with</p><formula xml:id="formula_1">E θ (x i ) = b i,xi for b ∈ R L×A .</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">POTTS MODELS</head><p>A Potts model is a fully-connected pairwise MRF with L variables, each representing a position in the MSA. An edge (i, j) is parametrized with a matrix W ij ∈ R A×A . These matrices are organized into an order-4 tensor which form the parameters of a Potts model, see Figure <ref type="figure" target="#fig_1">1</ref>. Note that W ij = W ji . The energy functional of a Potts model is given through lookups, namely</p><formula xml:id="formula_2">E θ (x i , x j ) = W ij (x i , x j ).</formula><p>(1)</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">FACTORED ATTENTION</head><p>Like Potts, factored attention is a fully-connected pairwise MRF with L variables. The parameters of this model consist of</p><formula xml:id="formula_3">H triples (W Q , W K , W V ), where W Q , W K ∈ R L×d ; W V ∈ R A×A ;</formula><p>and d is a hyperparameter. Each such triple is called a head and d is the head size. Unlike a Potts model, the parameters for each edge (i, j) are tied through the use of heads. The energy functional is</p><formula xml:id="formula_4">E θ (x i , x j ) = H h=1 symm softmax W h Q W h K T ij W h V (x i , x j ),<label>(2)</label></formula><p>where symm(M ) = (M + M T )/2 ensures the positional interactions are symmetric.</p><p>Factored attention has two advantages over Potts for modeling protein families: it shares a pool of amino acid feature matrices across all positions and it estimates O(L) parameters instead of O(L 2 ).</p><p>Sharing amino acid features. Many contacts in a protein are driven by similar interactions between amino acids, such as many types of weakly polar interactions <ref type="bibr" target="#b6">(Burley &amp; Petsko, 1988;</ref><ref type="bibr" target="#b20">Jaenicke, 2000)</ref>. If two pairs of positions (i, j) and (l, m) are both in contact due to the same interaction, a Potts model must estimate completely separate amino acid features W ij and W lm . In order to share amino acid features, we want to compute all energies from one pool of A × A feature matrices. The simplest way to accomplish this is by associating an L × L matrix A to every A × A feature matrix W V . For H such pairs (A, W V ), we could introduce a factorized MRF:</p><formula xml:id="formula_5">E θ (x i , x j ) = H h=1 symm softmax A h ij W h V (x i , x j ).<label>(3)</label></formula><p>A row-wise softmax is taken to encourage sparse interactions and aid in normalization. This model allows the pairs (i, j) and (l, m) to reuse a single feature W h V , assuming A h ij and A h lm are both large. Scaling linearly in length. Both Potts and the factorized model in Equation <ref type="formula" target="#formula_5">3</ref>have O(L 2 ) parameters. However, contacts are observed to grow linearly over the wide range of protein structures currently available <ref type="bibr" target="#b47">(Taylor &amp; Sadowski, 2011;</ref><ref type="bibr" target="#b23">Kamisetty et al., 2013)</ref>, which we examine in Figure <ref type="figure" target="#fig_14">11</ref>. Given that the number of interactions we wish to estimate grows linearly in length, the quadratic scaling of these models can be greatly improved. One way to fix this is by introducing the factorization</p><formula xml:id="formula_6">A = W Q W T K , where W Q , W K ∈ R L×d .</formula><p>As before, we employ a row-wise softmax for sparsity and normalization.</p><p>Combining feature sharing with linear length scaling leads to Equation <ref type="formula" target="#formula_4">2</ref>.</p><p>Recovering attention with sequence-dependent interactions. All models introduced so far estimate a single undirected graphical model from the training data. While a single graph can be a good approximation for the structure associated with a protein family, many families have subfamilies with different functional specializations and even different underlying contacts <ref type="bibr" target="#b4">(Brown et al., 2007;</ref><ref type="bibr" target="#b30">Malinverni &amp; Barducci, 2019)</ref>. Since subfamily identity is rarely known, allowing edge weights to be a function of sequence could enable the estimation of a family of highly related graphs.</p><p>The attention mechanism of the Transformer implements sequence-dependent edge weights by allowing positional interactions to be a function A(x). In the language of the Transformer, factored attention estimates a single graph because it computes queries and keys using only the positional encoding. More precise explanations are in Section A.2.</p><p>While data-dependent interactions significantly increase model capacity, this comes with multiple tradeoffs. Demands on GPU memory increase significantly with data-dependent interactions, as we detail in Section A.3. Further, attention-based models can be very challenging to train <ref type="bibr" target="#b56">(Zhang et al., 2019;</ref><ref type="bibr" target="#b26">Liu et al., 2019)</ref>; we found both Potts and factored attention trained more stably in all experiments. Lastly, the expressivity conferred by attention creates a risk of overfitting, especially when the pretraining dataset is small. Effective training on small MSAs is particularly important, since MSA depth correlates strongly with contact quality <ref type="bibr" target="#b38">(Schaarschmidt et al., 2018)</ref>.</p><p>A single Transformer encoder layer consists of an attention layer followed by a dense layer, with residual connections <ref type="bibr" target="#b18">(He et al., 2016)</ref> and layer normalization <ref type="bibr" target="#b1">(Ba et al., 2016)</ref> to aid in optimization. Transformer implementations typically use a sine/cosine positional encoding <ref type="bibr" target="#b49">(Vaswani et al., 2017)</ref> or learned Gaussian positional encoding <ref type="bibr" target="#b54">(Wolf et al., 2020)</ref>, rather than the one-hot positional encoding used in our single-layer models.</p><p>Self-Supervised Losses. Given an MSA, state-of-the-art methods estimate Potts model parameters through pseudolikelihood maximization <ref type="bibr" target="#b23">(Kamisetty et al., 2013;</ref><ref type="bibr" target="#b12">Ekeberg et al., 2013)</ref>. On the other hand, BERT-like attention-based models are typically trained with variants of masked language modeling <ref type="bibr" target="#b8">(Devlin et al., 2018)</ref>. Pseudolikelihood is challenging to compute efficiently for generic models, unlike masked language modeling. Both of these losses require computing conditionals of the form p θ (x i |x \M ), where M is a subset of {1, . . . , L} \ {i}. The losses L P L and L M LM for pseudolikelihood and masked language modeling, respectively, are</p><formula xml:id="formula_7">L P L (θ; x) = L i=1 log p θ (x i |x \i ), L M LM (θ; x, M ) = i∈M log p θ (x i |x \M ).</formula><p>Regularization for Potts and factored attention are both based on MRF edge parameters, while single-layer attention is penalized using weight decay. More details can be found in Section A.4.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">PRETRAINING ON SEQUENCE DATABASES</head><p>All single-layer models are trained on a set of evolutionarily related sequences. Given a large database of protein sequences such as UniRef100 <ref type="bibr" target="#b45">(Suzek et al., 2007)</ref> or BFD <ref type="bibr" target="#b43">(Steinegger &amp; Söding, 2018;</ref><ref type="bibr" target="#b44">Steinegger et al., 2019)</ref>, these models cannot be trained until significant preprocessing has been done: clustering, dereplication of highly related sequences, and alignment to generate an MSA for each cluster. In contrast, the self-supervised approach taken by works such as <ref type="bibr" target="#b13">(Elnaggar et al., 2020;</ref><ref type="bibr" target="#b37">Rives et al., 2020;</ref><ref type="bibr" target="#b35">Rao et al., 2019;</ref><ref type="bibr" target="#b29">Madani et al., 2020)</ref> applies BERT-style pretraining directly on the database of proteins with minimal preprocessing.</p><p>Difference in positional encoding. For attention-based models, the contrast between single-family and database-level pretraining shows up entirely in the choice of positional encoding. A singlefamily model, such as factored attention, uses a one-hot encoding of position in the MSA for that family. As such, the positional encoding of factored attention relies on all preprocessing steps mentioned above. On the other hand, the positional encoding used by pretrained Transformers is computed on the raw input sequence, simplifying pretraining due to lack of clustering or alignment.</p><p>While it is easier to implement positional encoding on raw sequence position, an ablation study in Section 4 shows that single-layer models fail on unaligned sequences. This suggests that there is a tradeoff: clustering and alignment enables small, easily trained models to learn effectively, while large models can learn directly from the database if trained carefully.</p><p>Simpler inference with pretrained Transformers. Given a new sequence of interest and a database of sequences, single-family models require more steps for inference than pretrained Transformers.</p><p>To apply a single-family model, one must query the database for related sequences, dereplicate the set, align sequences into an MSA, then train a model to learn contacts. On the other hand, a Transformer pretrained on the database simply computes a forward pass for the sequence of interest and its attention activations are used to predict contacts. No explicit querying or aligning is performed.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">EXTRACTING CONTACTS</head><p>Potts. We follow standard practice and extract a contact map C ∈ R L×L from the order-4 interaction tensor W by setting C ij = W ij F . Factored Attention. Since factored attention is a pairwise MRF, we can compute its order-4 interaction tensor W and use the same procedure as Potts. See Equation <ref type="formula" target="#formula_11">4</ref>.</p><p>Single-Layer Attention. To produce contacts for an MSA, we compute attention maps from only the positional encoding (without sequence) and average attention maps from all heads. Each singlelayer attention model is trained on one MSA, so the positional encoding is a feature shared by all sequences in the MSA. ProtBERT-BFD. We extract contacts from ProtBERT by averaging a subset of attention maps for an input sequence x. Of the 16 heads in 30 layers, we selected six whose attention maps had the top individual contact precisions over 500 families randomly selected from the Yang et al. ( <ref type="formula">2019</ref>) dataset. Predicted contacts for x are given by averaging the L × L attention maps from these six heads, then symmetrizing additively. See Table <ref type="table" target="#tab_1">2</ref>.</p><p>Average Product Correction (APC). Empirically, Potts models trained with Frobenius norm regularization have artifacts in the outputs C. These are removed with the Average Product Correction (APC) <ref type="bibr" target="#b10">(Dunn et al., 2008)</ref>. Unless otherwise stated, we apply APC to all extracted contacts.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">RESULTS</head><p>Experimental Setup. We use a set of 748 protein families from <ref type="bibr" target="#b55">Yang et al. (2019)</ref> to evaluate all models. For Potts models and single attention layers, we train separate models on each individual MSA. ProtBERT-BFD is frozen for all experiments. We train models using PyTorchLightning (Falcon, 2019) and Weights and Biases <ref type="bibr" target="#b3">(Biewald, 2020)</ref>. We compare predicted contact maps C to true contact maps C using standard metrics based on precision. A particularly important metric is precision at L, where L is the length of the sequence <ref type="bibr" target="#b38">(Schaarschmidt et al., 2018;</ref><ref type="bibr" target="#b42">Shrestha et al., 2019)</ref>. This is computed by masking C to only consider positions ≥ 6 apart, predicting the top L entries to be contacts, and computing precision. We provide more information on data and metrics in Appendix A.6 and more information on model hyperparameters in Section A.7.</p><p>Attention assumptions reflected in 15,051 protein structures. We examine all 15,051 structures in the <ref type="bibr" target="#b55">Yang et al. (2019)</ref> dataset for evidence of two key properties useful for single-layer attention models: few contacts per residue and the number of contacts scaling linearly in length. Figure <ref type="figure" target="#fig_14">11</ref> shows a linear trend of number of contacts versus length. In Figure <ref type="figure" target="#fig_2">12</ref>, we see that 80% of the 3,747,101 million residues in these structures have 4 or fewer contacts. Only 1.8% of residues have more than ten contacts. This shows that the row-wise softmax, which encourages each residue to attend to only a few other residues per-head, reflects structure found in the data.</p><p>Factored attention matches Potts performance on 748 families. Figure <ref type="figure" target="#fig_2">2</ref> shows a representative sample of good quality contact maps extracted from all models. Figure <ref type="figure" target="#fig_3">3a</ref> summarizes the performance of all models over the set of 748 protein families. Factored attention, Potts, and ProtBERT-BFD have comparable overall performance, with median precision at L of 0.46, 0.47, and 0.48, respectively. Stratifying by number of sequences reveals that ProtBERT-BFD has higher precision on MSAs with fewer than 256 sequences. For MSAs with greater than 1024 sequences, Potts, factored attention, and ProtBERT-BFD have comparable performance. Single-layer attention is uniformly worse over all MSA depths. Next, we evaluate the impact of sequence length on performance. Figure <ref type="figure" target="#fig_3">3b</ref> shows that factored attention and Potts achieve similar precision at L over the whole range of family lengths, despite factored attention having far fewer parameters for long families. This shows that factored attention can successfully leverage sparsity assumptions where they are most useful.   Long-range contacts are particularly important for downstream structure-prediction algorithmslong-range precision at L/5 is reported in both CASP12 and CASP13 <ref type="bibr" target="#b38">(Schaarschmidt et al., 2018;</ref><ref type="bibr" target="#b42">Shrestha et al., 2019)</ref>. Figure <ref type="figure" target="#fig_4">4</ref> breaks down contact precisions based on position separation into short (6 ≤ sep &lt; 12), medium (12 ≤ sep &lt; 24), and long (24 ≤ sep). We see that ProtBERT-BFD performs best on short-range contacts, with a median increase of 0.068 precision at L/5. On long-range ProtBERT-BFD, there is no appreciable difference in performance to Potts and factored attention. Across the range of contact bins, factored attention and Potts perform very similarly.</p><p>Fewer heads can match Potts on L/5 contacts. We probe the limits of parameter sharing by lowering the number of heads in factored attention and evaluating whether fewer heads can be used to precisely estimate contacts. Figure <ref type="figure" target="#fig_5">5a</ref> shows that 128 heads can be used to estimate L/5 contacts as precisely as Potts over the full set of 748 families. In Figure <ref type="figure" target="#fig_5">5b</ref>, we see that factored attention with 32 and 64 heads is still able to achieve reasonable overall performance compared to Potts. 32 and 64 heads have precision at L/5 at least as high as Potts for 329 and 348 families, respectively. If   we wish to recover the top L contacts, 256 heads are required to match Potts across all families, as seen in Figure <ref type="figure" target="#fig_3">13</ref>. Having more heads than 256 does not further increase performance. Intriguingly, Figure <ref type="figure" target="#fig_16">14</ref> demonstrates that both Spearman and Pearson correlation between the order-4 interaction tensors of factored attention and Potts improve even when increasing to 512 heads. We do not observe the same trends for increasing head size, as shown in Figure <ref type="figure" target="#fig_5">15</ref>.</p><p>For some families, the number of heads can be reduced even further. We show an example on the MSA built for PDB entry 3n2a. In Figure <ref type="figure" target="#fig_8">6a</ref>, we see that merely 4 heads are required to recover L/5 contacts nearly identical to those recovered by Potts. This shows that shared amino acid features and interaction parameters can enable identical performance with a 300x reduction in parameters. The training dynamics of these models are shown in Figure <ref type="figure" target="#fig_8">6b</ref>. Both factored attention with 256 heads and Potts converge after roughly 100 gradient steps, whereas factored attention with 4 heads requires nearly 10,000 steps to converge. In Figure <ref type="figure" target="#fig_8">16</ref>, we show that the top L contacts are significantly worse for 4 heads compared to Potts.</p><p>One set of amino acid features can be used for all families. Thus far we have only examined models that share parameters within single protein families. Since ProtBERT is trained on an entire database, it can leverage feature sharing across families to attain greater parameter efficiency and improved performance on small MSAs.  To explore the possibility that attention can share parameters across families, we train factored attention using a single set of frozen value matrices. We first train factored attention normally on 3n2a with 256 heads, then freeze the learned value matrices for the remaining 747 families. The query and key parameters are trained normally.</p><p>In Figure <ref type="figure" target="#fig_10">7</ref>, we compare the precision at L of factored attention with frozen 3n2a features to that of factored attention trained normally. Using a single frozen set of features results in only 6 families seeing precision at L decrease by more than 0.05, with a maximum drop of 0.11.</p><p>Frozen values are also comparable to Potts performance, as expected -see Figure <ref type="figure" target="#fig_10">17</ref>. This suggests that, even for a single-layer model, a single set of value matrices can capture amino acid features across functionally and structurally distinct protein families.</p><p>Factored attention reduces total parameters estimated. For an MSA of length L with alphabet size A, Potts models require L 2 A 2 parameters. Factored attention with H heads and head size d requires H(2Ld + A 2 ) parameters. In Figure <ref type="figure" target="#fig_1">18</ref>, we plot number of parameters versus length for various values of H and d = 32. Potts requires a total of 12 billion parameters to model all 748 families. Factored attention with 256 heads and head size 32 has 3.2 billion parameters; lowering to 128 heads reduces this to 790 million. Half of this reduction comes from 107 families of length greater than 400. ProtBERT-BFD is the most efficient, with 420 million parameters.</p><p>Ablations APC has a considerable impact on both Potts and factored attention, creating a median increase in precision at L of 0.1 and 0.07, respectively. The effect of APC is negligible for singlelayer attention and ProtBERT. Replacing pseudolikelihood maximization with Masked Language Modeling did not appreciably change performance for either Potts or factored attention. Addition of the single-site potential b i increases performance slightly for attention layers, but not enough to change overall trends. To compare to ProtBERT-BFD, we train our single-layer attention models on unaligned families and found that performance degrades significantly. See Figures <ref type="figure" target="#fig_19">19-22</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">DISCUSSION</head><p>We have shown that attention-based models achieve state-of-the-art performance on unsupervised contact extraction and that the assumptions encoded by attention reflect important properties of protein families. These results suggest that attention has a natural role in protein representation learning, without analogy to attention's success in the domain of NLP.</p><p>The motivating principles of factored attention were 1) sharing amino acid features across positions and 2) reducing parameters to O(L). The additional assumption of sequence-dependent contacts led to attention. The parametrizations of these properties presented in this paper were chosen to match those of the Transformer, but many alternatives exist. We believe broader exploration of this space will be fruitful. For example, the importance of APC for factored attention suggests including it directly into the attention mechanism, rather than as a correction after training. The success of Potts and factored attention demonstrates that estimating a single graphical model for a single protein family is often a reasonable approximation. Adapting attention to better model a set of highly related graphs could prove useful for capturing subfamily structure within deep MSAs, or for transferring information across related families. There remains, unexplored, a rich space of protein-specific architectures that potentially have the capacity to learn powerful protein features.</p><p>Many mysteries surround the success of Transformers at extracting protein contacts through masked language modeling. Most surprising is the apparent ability of ProtBERT to model many protein families at once, despite not being presented any information about protein family identity or even the existence of such a classification. Beyond family information, ProtBERT appears to learn parsimonious representations which share a vast amount of signal across protein families. Even including a number of within-family hierarchical assumptions and sharing amino acid features across all families, factored attention falls far short of ProtBERT in its potential to efficiently represent tens of thousands of protein families. Despite its impressive performance, ProtBERT does not improve long-range contact extraction in our evaluations. Much work remains to understand how pretraining can be most useful for downstream applications.</p><p>Our results show that hierarchical structure, both within and across families, is a source of signal available to attention models. Understanding how such structure can be learned without the use of protein family labels could lead to the development of widely applicable modeling components for protein representation learning. We believe that unsupervised attention-based models have the potential to impact structure prediction as broadly as existing coevolutionary methods.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Evolutionary Tree Multiple Sequence Alignment</head><p>Figure <ref type="figure">8</ref>: The tree on the right depicts evolution of a protein family. The protein at the root is the ancestral protein, and the five proteins at the leaves are its present-day descendants. The alignment on the left is the corresponding Multiple Sequence Alignment of observed sequences. The problem of multiplying aligning a set of sequences has been extensively studied in Bioinformatics and Computational <ref type="bibr" target="#b11">(Durbin et al., 1998)</ref>. Given a set of N sequences of average L, a full multiple alignment can be generated by performing dynamic programming on an array of size O( LN ). This is computationally intractable; the common workaround is to perform iterative pairwise alignment to a fixed reference sequence. The MSAs we use were generated with the HHSuite <ref type="bibr" target="#b44">(Steinegger et al., 2019)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Multiple Sequence Alignment Padded Sequences</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.2 RECOVERING FACTORED ATTENTION FROM STANDARD ATTENTION</head><p>We show more precisely that factored attention can be recovered from standard attention one of two ways 1. Computing queries and keys from one-hot positional encodings and values from one-hot sequence embeddings.</p><p>2. Using a learned positional embedding and sequence embedding with identity query, key, and value matrices.</p><p>The symmetrization operator symm is not applied to Transformer multihead attention, so we present a slight variation of factored attention without symm in this section.</p><p>Single attention layer. Given a sequence of dense vectors x = (x 1 , . . . , x L ) with x i ∈ R p , the attention mechanism of the Transformer encoder (multihead scaled dot product self-attention) produces a continuous representation y ∈ R L×p . If head size is d, this representation is computed using</p><formula xml:id="formula_8">H heads (W Q , W K , W V ), where W Q , W K , W V ∈ R p×d .</formula><p>Queries, keys, and values are defined as</p><formula xml:id="formula_9">Q = xW Q , K = xW K , V = xW V . For a single head (W Q , W K , W V ), the output is given by y = softmax QK T √ d V.</formula><p>The full output in R dH is produced by concatenating all head outputs. A single Transformer encoder layer passes the output through a dense layer, applying layer-norms and residual connection to aid optimization.</p><p>For the first layer, the input x is a sequence of discrete tokens. To produce a dense vector combining sequence and position information, positional encodings and sequence embeddings are combined.</p><p>The positional encoding E pos ∈ R L×e produces a dense vector of dimension e for each position i. The sequence embedding E seq ∈ R A×e maps each element of the vocabulary to a dense vector of dimension e. Typically these are combined through summation to produce a dense vector xi = E seq (x i ) + E pos (i), which is input to the Transformer as described above.</p><p>For this paper, we use only multihead self-attention without the dense layer, layernorm, or residual connections, as these drastically hurt performance when employed for one layer.</p><p>Factored attention from standard attention. Written explicitly, the input Transformer layer computes queries for a single head with Q = (E pos + E seq (x)) W Q . Keys and values are computed similarly. To recover factored attention, we instead compute queries and keys via Q = E pos W Q and K = E pos W K , while values are given by V = E seq (x)W V . For simplicity, we one-hot encode both position and sequence, which corresponds using identity matrices E pos = I L ∈ R L×L and E seq = I A ∈ R A×A . Equivalently, one can view the positional encoding and sequence embedding as being learned while fixing W Q , W K , and W V to be identity matrices.</p><p>Implicit term in attention. For a single layer of attention, the product E pos W V is a matrix in R L×A . This matrix does not depend on sequence inputs, thus allowing it to act as a single-site term. This suggests why inclusion of an explicit single-site term in Figure <ref type="figure" target="#fig_19">22</ref> had no effect for single-layer attention.</p><p>A In Table <ref type="table" target="#tab_0">1</ref>, we show the number of gradient steps per second for Potts, factored attention, and singlelayer attention. We fix a batch size of 32 and report numbers for families of lengths 120, 440, and 904 in order to explore the impact of length on computational efficiency for all models. Metrics are collected on a node with a single NVIDIA RTX 2080Ti, an Intel i7-7800x with 6 physical cores and 12 virtual cores, and 32 GB of system RAM. To get steady state batches per second, we run each model for 5000 steps and report the mean batches per second between steps 500 and 4500. Potts and factored attention have similar throughput, while standard attention scales much worse in length due to its memory requirements necessitating the use of gradient accumulation. This poor memory performance is because standard attention must compute L×L attention maps for each batch element separately, whereas the L × L component of other models does not depend on sequence.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.4 LOSSES</head><p>The loss for all three models is of the form</p><formula xml:id="formula_10">(θ; x) = L(θ; x) + cR(θ),</formula><p>where L is either pseudolikelihood or masked language modeling and R is a regularizer.</p><p>Potts regularization. Consider the order-4 interaction tensor W , where W ij ∈ R A×A gives the parameters associated to edge (i, j). We regularize W by setting R(θ) = i&lt;j W ij 2 F . This term is multiplied by λ • L • A, following <ref type="bibr" target="#b33">Ovchinnikov et al. (2014)</ref>.</p><p>Factored attention regularization. Since factored attention is also a fully connected pairwise MRF, we use identical regularization to that of Potts. The order-4 tensor W is given by</p><formula xml:id="formula_11">W ij ab = H h=1 symm softmax W h Q W h K T ij W h V (a, b).<label>(4)</label></formula><p>Single-layer attention regularization. Due the lack of an MRF interpretation for single-layer attention, we chose to use weight decay as is typically done for attention models. This corresponds to setting R(θ) to be the sum of Frobenius norm squared for all weights W Q , W K and W V .</p><p>Single-site term. When any model has a single-site term, we follow standard practice and regularize its Frobenius norm squared.</p><p>A.5 PROTBERT-BFD  <ref type="bibr">et al., 2007)</ref>, as well as metagenomic datasets. Our random sample is representative of the range of MSA depths and protein lengths, see Figure <ref type="figure" target="#fig_12">10</ref>. 2. A set of six families from the 748 was chosen to choose hyperparameters for single-layer attention. They were chosen to span a range of MSA depth (large and small), as well as three different regimes of Potts performance (Good, Ok, Poor). These families were used to tune hyperparameters as described in Section A.7.1. See Table <ref type="table">3</ref>.</p><p>3. A set of ten families from the 748 was chosen where factored attention performed very poorly in our initial experiments. Half were chosen to be long proteins and the other half to be short. This set was used to optimize learning rate and regularization for factored attention to ensure reasonable model performance. See Table <ref type="table" target="#tab_3">4</ref>.</p><p>4. 500 entirely disjoint families were further selected randomly from <ref type="bibr" target="#b55">Yang et al. (2019)</ref> and used to compute average precision for each head in ProtBERT-BFD <ref type="bibr" target="#b13">(Elnaggar et al., 2020)</ref>.</p><p>Performance on these families was used for selecting the top 6 heads, see  A PDB structure gives 3D coordinates for every atom in a structure. We use Euclidean distance between the beta carbons to define distance between any pair of positions. A pair of positions where this distance is less than 8 Å is declared to be a contact.</p><p>A.6.3 SCORING CONTACT PREDICTIONS Given a predicted contact map C ∈ R L×L and a true contact map C ∈ {0, 1} L×L , we describe metrics for scoring C.</p><p>A sequence x = (x 1 , . . . , x L ) of length L has L 2 potential contacts. Since we see O(L) contacts, contact prediction a sparse prediction task. Accordingly, we focus on precision-recall based quantitative analyses of Ĉ. Common practice in the field is to sort all L 2 entries of Ĉ in decreasing order and evaluate precision at various length thresholds, such as the top L or L/10 predictions <ref type="bibr" target="#b42">(Shrestha et al., 2019)</ref>. Note that this analysis is similar to choosing recall cutoffs along a precision-recall curve, where sorted length index plays the role of recall on the x axis. Unlike recall, length-based cutoffs do not rely on knowledge of the actual number of contacts. In addition to the precision at various length (recall) cutoffs, we also computed Area Under the Precision-Recall Curve (AUC). AUC is a widely used metric for comparing classifiers when the positive class is rare.     <ref type="formula" target="#formula_11">4</ref>) and Potts (see Section 3).   </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: Training and interpretation of a Potts model on a single protein family.The proteins are all loops formed by one blue and one yellow amino acid locking together. The MSA for this family aligns these critical yellow and blue amino acids. For the trained MRF on this MSA, the weight matrix W 4,1 has the highest values due to the evolutionary constraint that blue and yellow covary for those positions. In this case, the highest predict contact recapitulates a true contact.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: Predicted contact maps and Precision at L for each model on PDB entry 2BFW. Blue indicates a true positive, red indicates a false positive, and grey indicates a false negative.</figDesc><graphic url="image-1.png" coords="6,149.57,101.76,72.84,72.84" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: Model performance evaluated on MSA depth and reference length.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 4 :</head><label>4</label><figDesc>Figure 4: Contact precision for all models stratified by the range of the interaction.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 5 :</head><label>5</label><figDesc>Figure 5: Examining impact of number of heads on precision at L/5. Left: Comparing performance of Potts and 128 heads over each family shows comparable performance. Right: Precision at L/5 drops off slowly until 32 heads, then steeply declines beyond that.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head></head><label></label><figDesc>Top L/5 contacts on 3n2a.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head></head><label></label><figDesc>Training dynamics of models on 3n2a.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Figure 6 :</head><label>6</label><figDesc>Figure 6: Factored attention with 4 heads can learn the top L/5 contacts on 3n2a.</figDesc><graphic url="image-6.png" coords="8,138.47,100.76,72.57,72.57" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head>Figure 7 :</head><label>7</label><figDesc>Figure 7: A single set of frozen value matrices can be used for all families.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_11"><head>Figure 9 :</head><label>9</label><figDesc>Figure 9: MSA for sequences from Figure 8 compared to a padded batch of the same sequences.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_12"><head>Figure 10 :</head><label>10</label><figDesc>Figure 10: The length and MSA size distribution for our 748 family subset (red) compared to the full 15,051 families in the trRosetta dataset selected for training</figDesc><graphic url="image-9.png" coords="18,108.00,81.86,460.80,345.60" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_14"><head>Figure 11 :</head><label>11</label><figDesc>Figure 11: The total number of contacts for a structure as a function of protein length follows a linear trend. (slope = 2.64, R 2 = 0.929)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_15"><head>Figure 12 :Figure 13 :</head><label>1213</label><figDesc>Figure 12: The empirical CDF of number of per-residue contacts for 3,747,101 residues in 15,051 structures in the trRosetta dataset.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_16"><head>Figure 14 :</head><label>14</label><figDesc>Figure14: Effect of number of heads on correlation between the order-4 weight tensors for factored attention (see Equation4) and Potts (see Section 3).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_17"><head>Figure 15 :Figure 16</head><label>1516</label><figDesc>Figure 15: Effect of head size on factored attention precision at L and L/5 over 748 families.Increasing head size has a small effect on precision, though not nearly as pronounced as the number of heads.</figDesc><graphic url="image-10.png" coords="22,149.57,524.09,152.28,152.28" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_18"><head>Figure 17 :Figure 18 :</head><label>1718</label><figDesc>Figure 17: Factored attention trained with a single set of frozen value matrices performs comparably to Potts, evaluated on precision at L across 748 families.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_19"><head>FFigure 22 :</head><label>22</label><figDesc>Figure 22: The addition of a single-site term to either factored or standard attention produces little additional benefit.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 :</head><label>1</label><figDesc>.3 COMPUTATIONAL EFFICIENCY OF SINGLE LAYER MODELS Throughput of various models for batches of size 32. Stars (*) indicate usage of gradient accumulation due to GPU memory constraints. For sequences of length 440, batches of size 16 were used for standard attention. For sequences of length 904, batches of size 4 were used.</figDesc><table><row><cell>Model</cell><cell cols="2">Length Batches/s</cell></row><row><cell>Potts Factored attention (H = 256) Single-layer attention (H = 128)</cell><cell>120 440 904 120 440 904 120 440* 904*</cell><cell>36.59 5.21 1.04 32.89 3.91 1.03 17.15 1.685* 0.46*</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 :</head><label>2</label><figDesc>The top 16 heads in ProtBERT-BFD whose attention maps gave the most precise contact maps across 500 validation families. Most of the top performing heads are found in the last layer. The top six heads were selected for our contact extraction in all results.</figDesc><table><row><cell>HEAD SELECTION</cell></row><row><cell>layer head P@L 29 7 0.517 29 8 0.396 29 4 0.394 29 2 0.353 29 11 0.333 29 0 0.299 28 3 0.275 29 15 0.177 29 6 0.167 29 12 0.158 28 4 0.141 29 9 0.139 28 6 0.125 28 5 0.125 3 4 0.115 28 11 0.106</cell></row><row><cell>A.6 DATA AND METRICS</cell></row><row><cell>A.6.1 SELECTION OF PROTEIN FAMILIES</cell></row><row><cell>We used the following sets of families for model development:</cell></row><row><cell>1. A set of 748 families was chosen for performance evaluation. All metrics reported in the paper are on this set, with a single choice of hyperparameters for Potts models, factored attention, and standard attention. The 748 families were chosen randomly from the Yang et al. (2019) dataset, which consists of 15,051 MSAs generated from the databases Uni-Clust30 and UniRef100 (Suzek</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2</head><label>2</label><figDesc></figDesc><table><row><cell>.</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 4 :</head><label>4</label><figDesc></figDesc><table /><note>10 families chosen for hyperparameter optimization for factored attention</note></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>ACKNOWLEDGMENTS</head><p>We would like to thank Jeffrey Chan and Nilesh Tripuraneni for helpful comments on presentation and experiments. NB, NT, and YSS were supported in part by NIH Grant R35-GM134922. RR was supported by Berkeley Deep Drive. PKK was supported in part by the Simons Center for Quantitative Biology at Cold Spring Harbor Laboratory and the Developmental Funds from the Cancer Center Support Grant 5P30CA045508. JD was supported by The Open Philanthropy Project Improving Protein Design Fund. DB was supported by Howard Hughes Medical Institute and The Audacious Project at the Institute for Protein Design. YSS is a Chan-Zuckerberg Biohub Investigator. SO was supported by NIH Grant DP5OD026389.</p></div>
			</div>

			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A APPENDIX A.1 MULTIPLE SEQUENCE ALIGNMENTS</head><p>We demonstrate key concepts of protein evolution in Figure <ref type="figure">8</ref>. On the left is a phylogenetic tree. The leaves represent five observed proteins in a family, while the root represents their most recent common ancestor. The ancestral protein was a loop and evolution preserved this loop structure. Thus every observed sequence has one yellow and one blue amino acid on its ends, which lock together to pinch off the loop. The amino acids in the middle of the loop exhibit considerable differences within the family, presumably leading to variations in function. This variation within the protein family is captured by the MSA on the right through its placement of gap characters (white squares). Compared to standard padding, shown in Figure <ref type="figure">9</ref>, the placement of gap characters in the MSA ensures all blue and yellow amino acids lie in a single column. This signal is obfuscated by standard padding.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.7 HYPERPARAMETERS</head><p>Potts. We used λ = 0.5, learning rate of 0.5, and batch size 4096. Pad, gap, and mask were all encoded with the same token.</p><p>The Potts model is trained using a modified version of Adam presented in <ref type="bibr" target="#b7">Dauparas et al. (2019)</ref>. This modification was made to improve performance of Adam to match that of L-BFGS.</p><p>Factored attention. We AdamW with a learning rate of 5e-3 and set λ = 0.01. The default head size was set to 32 unless stated otherwise.</p><p>Single-layer attention. We set embedding size of 256, head size of 64, and number of heads 128. The model is trained with AdamW using a learning rate of 5e-3 and weight decay of 2e-3. Attention dropout of 0.05 is also applied. The batch size is 32 and mask prob for masked language modeling is 0.15. We use a separate mask token and pad,gap token. Potts. The Potts model implementation using psuedolikelihood has been optimized by others, so we did not tune performance. Since performance with MLM was comparable to pseudolikelihood, we did not sweep for MLM either.</p><p>Single-layer attention. Standard attention is by far the most sensitive model to hyperparameters. To find a reasonable set of hyperparameters, we first swept over the six families in Table <ref type="table">3</ref>, performing a grid search over Factored attention. We swept factored attention over the families in Table <ref type="table">4</ref>, performing a grid search over</p><p>We found that learning rate of 5e-3 and regularization of 0.01 were effective, but that other configurations such as regularization of 5e-3 also performed well. Both H and d are evaluated extensively in our results.   </p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Deepcon: protein contact prediction using dilated convolutional neural networks with dropout</title>
		<author>
			<persName><forename type="first">Badri</forename><surname>Adhikari</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Bioinformatics</title>
		<imprint>
			<biblScope unit="volume">36</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="470" to="477" />
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title/>
		<author>
			<persName><forename type="first">Jimmy</forename><surname>Lei Ba</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jamie</forename><surname>Ryan Kiros</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Geoffrey</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1607.06450</idno>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
	<note type="report_type">Layer normalization. arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Learning generative models for protein fold families</title>
		<author>
			<persName><forename type="first">Sivaraman</forename><surname>Balakrishnan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hetunandan</forename><surname>Kamisetty</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jaime</forename><forename type="middle">G</forename><surname>Carbonell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Su-In</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christopher</forename><surname>James</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Langmead</forename></persName>
		</author>
		<idno type="DOI">10.1002/prot.22934</idno>
		<ptr target="http://doi.wiley.com/10.1002/prot.22934" />
	</analytic>
	<monogr>
		<title level="j">Proteins: Structure, Function, and Bioinformatics</title>
		<imprint>
			<biblScope unit="volume">79</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="1061" to="1078" />
			<date type="published" when="2011-04">apr 2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">Experiment tracking with weights and biases</title>
		<author>
			<persName><forename type="first">Lukas</forename><surname>Biewald</surname></persName>
		</author>
		<ptr target="https://www.wandb.com/.Softwareavailablefromwandb.com" />
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Automated protein subfamily identification and classification</title>
		<author>
			<persName><forename type="first">Nandini</forename><surname>Duncan P Brown</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kimmen</forename><surname>Krishnamurthy</surname></persName>
		</author>
		<author>
			<persName><surname>Sjölander</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">PLoS Comput Biol</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page">e160</biblScope>
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Language models are Few-Shot learners</title>
		<author>
			<persName><forename type="first">Benjamin</forename><surname>Tom B Brown</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nick</forename><surname>Mann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Melanie</forename><surname>Ryder</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jared</forename><surname>Subbiah</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Prafulla</forename><surname>Kaplan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Arvind</forename><surname>Dhariwal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pranav</forename><surname>Neelakantan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Girish</forename><surname>Shyam</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Amanda</forename><surname>Sastry</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sandhini</forename><surname>Askell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ariel</forename><surname>Agarwal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gretchen</forename><surname>Herbert-Voss</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tom</forename><surname>Krueger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rewon</forename><surname>Henighan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aditya</forename><surname>Child</surname></persName>
		</author>
		<author>
			<persName><surname>Ramesh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Daniel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jeffrey</forename><surname>Ziegler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Clemens</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christopher</forename><surname>Winter</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mark</forename><surname>Hesse</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Eric</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mateusz</forename><surname>Sigler</surname></persName>
		</author>
		<author>
			<persName><surname>Litwin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Ilya Sutskever, and Dario Amodei</title>
				<meeting><address><addrLine>Scott Gray, Benjamin Chess, Jack Clark, Christopher Berner, Sam McCandlish, Alec Radford</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2020-05">May 2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Weakly polar interactions in proteins</title>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">K</forename><surname>Burley</surname></persName>
		</author>
		<author>
			<persName><surname>Petsko</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in protein chemistry</title>
		<imprint>
			<biblScope unit="volume">39</biblScope>
			<biblScope unit="page" from="125" to="189" />
			<date type="published" when="1988">1988</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">Unified framework for modeling multivariate distributions in biological sequences</title>
		<author>
			<persName><forename type="first">Justas</forename><surname>Dauparas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Haobo</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Avi</forename><surname>Swartz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Peter</forename><surname>Koo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mor</forename><surname>Nitzan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sergey</forename><surname>Ovchinnikov</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019-06">June 2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">BERT: Pre-training of deep bidirectional transformers for language understanding</title>
		<author>
			<persName><forename type="first">Jacob</forename><surname>Devlin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ming-Wei</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kenton</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kristina</forename><surname>Toutanova</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018-10">October 2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">Energy-based Models for Atomic-Resolution Protein Conformations</title>
		<author>
			<persName><forename type="first">Yilun</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Joshua</forename><surname>Meier</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jerry</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rob</forename><surname>Fergus</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alexander</forename><surname>Rives</surname></persName>
		</author>
		<ptr target="https://github.com/facebookresearch/" />
		<imprint>
			<date type="published" when="2019-09">sep 2019</date>
		</imprint>
	</monogr>
	<note type="report_type">Technical report</note>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Mutual information without the influence of phylogeny or entropy dramatically improves residue contact prediction</title>
		<author>
			<persName><forename type="first">L M</forename><surname>S D Dunn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Wahl</surname></persName>
		</author>
		<author>
			<persName><surname>Gloor</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Bioinformatics</title>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="333" to="340" />
			<date type="published" when="2008-02">February 2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">Biological sequence analysis: probabilistic models of proteins and nucleic acids</title>
		<author>
			<persName><forename type="first">Richard</forename><surname>Durbin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sean</forename><forename type="middle">R</forename><surname>Eddy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Anders</forename><surname>Krogh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Graeme</forename><surname>Mitchison</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1998">1998</date>
			<publisher>Cambridge university press</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Improved contact prediction in proteins: using pseudolikelihoods to infer potts models</title>
		<author>
			<persName><forename type="first">Magnus</forename><surname>Ekeberg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Cecilia</forename><surname>Lövkvist</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yueheng</forename><surname>Lan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Martin</forename><surname>Weigt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Erik</forename><surname>Aurell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Phys. Rev. E Stat. Nonlin. Soft Matter Phys</title>
		<imprint>
			<biblScope unit="volume">87</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page">12707</biblScope>
			<date type="published" when="2013-01">January 2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">Prot-Trans: Towards cracking the language of life&apos;s code through Self-Supervised deep learning and high performance computing</title>
		<author>
			<persName><forename type="first">Ahmed</forename><surname>Elnaggar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><surname>Heinzinger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christian</forename><surname>Dallago</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ghalia</forename><surname>Rihawi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yu</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Llion</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tom</forename><surname>Gibbs</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tamas</forename><surname>Feher</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christoph</forename><surname>Angerer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Debsindhu</forename><surname>Bhowmik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Burkhard</forename><surname>Rost</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020-07">July 2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">Pytorch lightning. GitHub</title>
		<author>
			<persName><surname>Wa Falcon</surname></persName>
		</author>
		<ptr target="https://github.com/PyTorchLightning/pytorch-lightning" />
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="volume">3</biblScope>
			<pubPlace>Note</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Computing Highly Correlated Positions Using Mutual Information and Graph Theory for G Protein-Coupled Receptors</title>
		<author>
			<persName><forename type="first">N</forename><surname>Sarosh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stefano</forename><surname>Fatakia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Carson</forename><forename type="middle">C</forename><surname>Costanzi</surname></persName>
		</author>
		<author>
			<persName><surname>Chow</surname></persName>
		</author>
		<idno type="DOI">10.1371/journal.pone.0004681</idno>
		<ptr target="https://dx.plos.org/10.1371/journal.pone.0004681" />
	</analytic>
	<monogr>
		<title level="j">PLoS ONE</title>
		<idno type="ISSN">1932-6203</idno>
		<imprint>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page">e4681</biblScope>
			<date type="published" when="2009-03">mar 2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">On Evolutionary Conservation of Thermodynamic Coupling in Proteins</title>
		<author>
			<persName><forename type="first">Anthony</forename><forename type="middle">A</forename><surname>Fodor</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Richard</forename><forename type="middle">W</forename><surname>Aldrich</surname></persName>
		</author>
		<idno type="DOI">10.1074/jbc.M402560200</idno>
		<ptr target="https://pubmed.ncbi.nlm.nih.gov/15023994/" />
	</analytic>
	<monogr>
		<title level="j">Journal of Biological Chemistry</title>
		<imprint>
			<biblScope unit="volume">279</biblScope>
			<biblScope unit="issue">18</biblScope>
			<date type="published" when="2004">19046-19050, apr 2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Correlated mutations and residue contacts in proteins</title>
		<author>
			<persName><forename type="first">Ulrike</forename><surname>Göbel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chris</forename><surname>Sander</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Reinhard</forename><surname>Schneider</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alfonso</forename><surname>Valencia</surname></persName>
		</author>
		<idno type="DOI">10.1002/prot.340180402</idno>
		<ptr target="https://pubmed.ncbi.nlm.nih.gov/8208723/" />
	</analytic>
	<monogr>
		<title level="j">Proteins: Structure, Function, and Bioinformatics</title>
		<imprint>
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="309" to="317" />
			<date type="published" when="1994">1994</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shaoqing</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
				<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="770" to="778" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Generative models for graph-based protein design</title>
		<author>
			<persName><forename type="first">John</forename><surname>Ingraham</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Vikas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Regina</forename><surname>Garg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tommi</forename><surname>Barzilay</surname></persName>
		</author>
		<author>
			<persName><surname>Jaakkola</surname></persName>
		</author>
		<ptr target="https://papers.nips.cc/paper/9711-generative-models-for-graph-based-protein-design" />
	</analytic>
	<monogr>
		<title level="m">Neural Information Processing Systems. MIT</title>
				<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Stability and stabilization of globular proteins in solution</title>
		<author>
			<persName><forename type="first">Rainer</forename><surname>Jaenicke</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Biotechnology</title>
		<imprint>
			<biblScope unit="volume">79</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="193" to="203" />
			<date type="published" when="2000">2000</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Hidden Markov model speed heuristic and iterative HMM search procedure</title>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">Steven</forename><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sean</forename><forename type="middle">R</forename><surname>Eddy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Elon</forename><surname>Portugaly</surname></persName>
		</author>
		<idno type="DOI">10.1186/1471-2105-11-431</idno>
		<ptr target="https://bmcbioinformatics.biomedcentral.com/articles/10.1186/1471-2105-11-431" />
	</analytic>
	<monogr>
		<title level="j">BMC Bioinformatics</title>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page">431</biblScope>
			<date type="published" when="2010-08">aug 2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">High precision in protein contact prediction using fully convolutional neural networks and minimal sequence features</title>
		<author>
			<persName><forename type="first">T</forename><surname>David</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shaun</forename><forename type="middle">M</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName><surname>Kandathil</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Bioinformatics</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="issue">19</biblScope>
			<biblScope unit="page" from="3308" to="3315" />
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Assessing the utility of coevolutionbased residue-residue contact predictions in a sequence-and structure-rich era</title>
		<author>
			<persName><forename type="first">Hetunandan</forename><surname>Kamisetty</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sergey</forename><surname>Ovchinnikov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><surname>Baker</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Proc. Natl. Acad. Sci. U. S. A</title>
		<imprint>
			<biblScope unit="volume">110</biblScope>
			<biblScope unit="issue">39</biblScope>
			<biblScope unit="page" from="15674" to="15679" />
			<date type="published" when="2013-09">September 2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Covariation of mutations in the V3 loop of human immunodeficiency virus type 1 envelope protein: an information theoretic analysis</title>
		<author>
			<persName><forename type="first">R M</forename><surname>B T Korber</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D H</forename><surname>Farber</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Wolpert</surname></persName>
		</author>
		<author>
			<persName><surname>Lapedes</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Proc. Natl. Acad. Sci. U. S. A</title>
		<imprint>
			<biblScope unit="volume">90</biblScope>
			<biblScope unit="issue">15</biblScope>
			<biblScope unit="page" from="7176" to="7180" />
			<date type="published" when="1993-08">August 1993</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Correlated mutations in models of protein sequences: Phylogenetic and structural effects</title>
		<author>
			<persName><forename type="first">Alan</forename><forename type="middle">S</forename><surname>Lapedes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bertrand</forename><forename type="middle">G</forename><surname>Giraud</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lonchang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gary</forename><forename type="middle">D</forename><surname>Stormo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Lect. Notes Monogr. Ser</title>
		<imprint>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page" from="236" to="256" />
			<date type="published" when="1999">1999</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<author>
			<persName><forename type="first">Liyuan</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Haoming</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pengcheng</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Weizhu</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaodong</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jianfeng</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiawei</forename><surname>Han</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1908.03265</idno>
		<title level="m">On the variance of the adaptive learning rate and beyond</title>
				<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Evolutionarily conserved pathways of energetic connectivity in protein families</title>
		<author>
			<persName><forename type="first">Steve</forename><forename type="middle">W</forename><surname>Lockless</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rama</forename><surname>Ranganathan</surname></persName>
		</author>
		<idno type="DOI">10.1126/science.286.5438.295</idno>
		<ptr target="https://science.sciencemag.org/content/286/5438/295https://science.sciencemag.org/content/286/5438/295.abstract" />
	</analytic>
	<monogr>
		<title level="j">Science</title>
		<imprint>
			<biblScope unit="volume">286</biblScope>
			<biblScope unit="issue">5438</biblScope>
			<biblScope unit="page" from="295" to="299" />
			<date type="published" when="1999-10">oct 1999</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Self-supervised contrastive learning of protein representations by mutual information maximization</title>
		<author>
			<persName><forename type="first">Amy</forename><forename type="middle">X</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Haoran</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Marzyeh</forename><surname>Ghassemi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alan</forename><surname>Moses</surname></persName>
		</author>
		<idno type="DOI">10.1101/2020.09.04.283929</idno>
		<ptr target="https://www.biorxiv.org/content/early/2020/11/10/2020.09.04.283929" />
	</analytic>
	<monogr>
		<title level="j">bioRxiv</title>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">ProGen: Language modeling for protein generation</title>
		<author>
			<persName><surname>Madani</surname></persName>
		</author>
		<author>
			<persName><surname>Mccann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N S</forename><surname>Naik</surname></persName>
		</author>
		<author>
			<persName><surname>Keskar</surname></persName>
		</author>
		<author>
			<persName><surname>Others</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">arXiv preprint arXiv</title>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Coevolutionary analysis of protein subfamilies by sequence reweighting</title>
		<author>
			<persName><forename type="first">Duccio</forename><surname>Malinverni</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alessandro</forename><surname>Barducci</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Entropy</title>
		<imprint>
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page">1127</biblScope>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">New encouraging developments in contact prediction: Assessment of the casp 11 results</title>
		<author>
			<persName><forename type="first">Bohdan</forename><surname>Monastyrskyy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D'</forename><surname>Daniel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Krzysztof</forename><surname>Andrea</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Anna</forename><surname>Fidelis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andriy</forename><surname>Tramontano</surname></persName>
		</author>
		<author>
			<persName><surname>Kryshtafovych</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Proteins: Structure, Function, and Bioinformatics</title>
		<imprint>
			<biblScope unit="volume">84</biblScope>
			<biblScope unit="page" from="131" to="144" />
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
		<title level="m" type="main">Transforming the language of life: Transformer neural networks for protein prediction tasks</title>
		<author>
			<persName><forename type="first">Ananthan</forename><surname>Nambiar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Simon</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mark</forename><surname>Hopkins</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Maeve</forename><surname>Heflin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sergei</forename><surname>Maslov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Anna</forename><surname>Ritz</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020-06">June 2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
		<title level="m" type="main">Robust and accurate prediction of residue-residue interactions across protein interfaces using evolutionary information</title>
		<author>
			<persName><forename type="first">Sergey</forename><surname>Ovchinnikov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hetunandan</forename><surname>Kamisetty</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><surname>Baker</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014-05">May 2014</date>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">e02030</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<monogr>
		<title level="m" type="main">Hopfield networks is all you need</title>
		<author>
			<persName><forename type="first">Hubert</forename><surname>Ramsauer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bernhard</forename><surname>Schäfl</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Johannes</forename><surname>Lehner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Philipp</forename><surname>Seidl</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><surname>Widrich</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lukas</forename><surname>Gruber</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Markus</forename><surname>Holzleitner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Milena</forename><surname>Pavlović</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Geir</forename><surname>Kjetil Sandve</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Victor</forename><surname>Greiff</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2008.02217</idno>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Evaluating protein transfer learning with TAPE</title>
		<author>
			<persName><forename type="first">Roshan</forename><surname>Rao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nicholas</forename><surname>Bhattacharya</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Neil</forename><surname>Thomas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yan</forename><surname>Duan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Peter</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">John</forename><surname>Canny</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pieter</forename><surname>Abbeel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yun</forename><surname>Song</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
				<editor>
			<persName><surname>Wallach</surname></persName>
		</editor>
		<editor>
			<persName><surname>Larochelle</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">F</forename><surname>Beygelzimer</surname></persName>
		</editor>
		<editor>
			<persName><surname>Dalché-Buc</surname></persName>
		</editor>
		<editor>
			<persName><surname>Fox</surname></persName>
		</editor>
		<editor>
			<persName><surname>Garnett</surname></persName>
		</editor>
		<imprint>
			<publisher>Curran Associates, Inc</publisher>
			<date type="published" when="2019">2019</date>
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="page" from="9689" to="9701" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">HHblits: Lightning-fast iterative protein sequence searching by HMM-HMM alignment</title>
		<author>
			<persName><forename type="first">Michael</forename><surname>Remmert</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andreas</forename><surname>Biegert</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andreas</forename><surname>Hauser</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Johannes</forename><surname>Söding</surname></persName>
		</author>
		<idno type="DOI">10.1038/nmeth.1818</idno>
		<ptr target="http://predictioncenter.org/casp9/groups{_}analysis.cgi?type=server{&amp;}tbm=on/" />
	</analytic>
	<monogr>
		<title level="j">Nature Methods</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="173" to="175" />
			<date type="published" when="2012-02">feb 2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<monogr>
		<title level="m" type="main">Biological structure and function emerge from scaling unsupervised learning to 250 million protein sequences</title>
		<author>
			<persName><forename type="first">Alexander</forename><surname>Rives</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Joshua</forename><surname>Meier</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tom</forename><surname>Sercu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Siddharth</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zeming</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Demi</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Myle</forename><surname>Ott</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lawrence</forename><surname>Zitnick</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jerry</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rob</forename><surname>Fergus</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020-08">August 2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Assessment of contact predictions in casp12: co-evolution and deep learning coming of age</title>
		<author>
			<persName><forename type="first">Joerg</forename><surname>Schaarschmidt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bohdan</forename><surname>Monastyrskyy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andriy</forename><surname>Kryshtafovych</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alexandre Mjj</forename><surname>Bonvin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Proteins: Structure, Function, and Bioinformatics</title>
		<imprint>
			<biblScope unit="volume">86</biblScope>
			<biblScope unit="page" from="51" to="66" />
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">CCMpred -Fast and precise prediction of protein residue-residue contacts from correlated mutations</title>
		<author>
			<persName><forename type="first">Stefan</forename><surname>Seemayer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Markus</forename><surname>Gruber</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Johannes</forename><surname>Söding</surname></persName>
		</author>
		<idno type="DOI">10.1093/bioinformatics/btu500</idno>
		<ptr target="https://pubmed.ncbi.nlm.nih.gov/25064567/" />
	</analytic>
	<monogr>
		<title level="j">Bioinformatics</title>
		<imprint>
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="issue">21</biblScope>
			<biblScope unit="page" from="3128" to="3130" />
			<date type="published" when="2014-05">may 2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Improved protein structure prediction using potentials from deep learning</title>
		<author>
			<persName><forename type="first">Andrew</forename><forename type="middle">W</forename><surname>Senior</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Richard</forename><surname>Evans</surname></persName>
		</author>
		<author>
			<persName><forename type="first">John</forename><surname>Jumper</surname></persName>
		</author>
		<author>
			<persName><forename type="first">James</forename><surname>Kirkpatrick</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Laurent</forename><surname>Sifre</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tim</forename><surname>Green</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chongli</forename><surname>Qin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Augustin</forename><surname>Žídek</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alexander</forename><forename type="middle">W R</forename><surname>Nelson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alex</forename><surname>Bridgland</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hugo</forename><surname>Penedones</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stig</forename><surname>Petersen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Karen</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Steve</forename><surname>Crossan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pushmeet</forename><surname>Kohli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><forename type="middle">T</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><surname>Silver</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Koray</forename><surname>Kavukcuoglu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Demis</forename><surname>Hassabis</surname></persName>
		</author>
		<idno type="DOI">10.1038/s41586-019-1923-7</idno>
		<ptr target="https://doi.org/10.1038/s41586-019-1923-7" />
	</analytic>
	<monogr>
		<title level="j">Nature</title>
		<imprint>
			<biblScope unit="volume">577</biblScope>
			<biblScope unit="issue">7792</biblScope>
			<biblScope unit="page" from="706" to="710" />
			<date type="published" when="2020-01">jan 2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<monogr>
		<title level="m" type="main">Is transfer learning necessary for protein landscape prediction?</title>
		<author>
			<persName><forename type="first">Amir</forename><surname>Shanehsazzadeh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><surname>Belanger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><surname>Dohan</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Assessing the accuracy of contact predictions in casp13</title>
		<author>
			<persName><forename type="first">Rojan</forename><surname>Shrestha</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Eduardo</forename><surname>Fajardo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nelson</forename><surname>Gil</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Krzysztof</forename><surname>Fidelis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andriy</forename><surname>Kryshtafovych</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bohdan</forename><surname>Monastyrskyy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andras</forename><surname>Fiser</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proteins: Structure, Function, and Bioinformatics</title>
				<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="volume">87</biblScope>
			<biblScope unit="page" from="1058" to="1068" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Clustering huge protein sequence sets in linear time</title>
		<author>
			<persName><forename type="first">Martin</forename><surname>Steinegger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Johannes</forename><surname>Söding</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nature communications</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1" to="8" />
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">HH-suite3 for fast remote homology detection and deep protein annotation</title>
		<author>
			<persName><forename type="first">Martin</forename><surname>Steinegger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Markus</forename><surname>Meier</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Milot</forename><surname>Mirdita</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Harald</forename><surname>Vöhringer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stephan</forename><forename type="middle">J</forename><surname>Haunsberger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Johannes</forename><surname>Söding</surname></persName>
		</author>
		<idno type="DOI">10.1186/s12859-019-3019-7</idno>
		<ptr target="https://bmcbioinformatics.biomedcentral.com/articles/10.1186/s12859-019-3019-7" />
	</analytic>
	<monogr>
		<title level="j">BMC Bioinformatics</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page">473</biblScope>
			<date type="published" when="2019-09">sep 2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">UniRef: comprehensive and non-redundant UniProt reference clusters</title>
		<author>
			<persName><forename type="first">Hongzhan</forename><surname>Baris E Suzek</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Peter</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Raja</forename><surname>Mcgarvey</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Cathy</forename><forename type="middle">H</forename><surname>Mazumder</surname></persName>
		</author>
		<author>
			<persName><surname>Wu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Bioinformatics</title>
		<imprint>
			<biblScope unit="volume">23</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page" from="1282" to="1288" />
			<date type="published" when="2007-05">May 2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Uniref clusters: a comprehensive and scalable alternative for improving sequence similarity searches</title>
		<author>
			<persName><forename type="first">Yuqi</forename><surname>Baris E Suzek</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hongzhan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Peter</forename><forename type="middle">B</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Cathy</forename><forename type="middle">H</forename><surname>Mcgarvey</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Uniprot</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><surname>Consortium</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Bioinformatics</title>
		<imprint>
			<biblScope unit="volume">31</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="926" to="932" />
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Structural constraints on the covariance matrix derived from multiple aligned protein sequences</title>
		<author>
			<persName><forename type="first">R</forename><surname>William</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><forename type="middle">I</forename><surname>Taylor</surname></persName>
		</author>
		<author>
			<persName><surname>Sadowski</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">PLoS One</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page">e28265</biblScope>
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Graphical models of residue coupling in protein families</title>
		<author>
			<persName><forename type="first">John</forename><surname>Thomas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Naren</forename><surname>Ramakrishnan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chris</forename><surname>Bailey-Kellogg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE/ACM Transactions on Computational Biology and Bioinformatics</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="183" to="197" />
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Attention is all you need</title>
		<author>
			<persName><forename type="first">Ashish</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Noam</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Niki</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jakob</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Llion</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aidan</forename><forename type="middle">N</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ł Ukasz</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Illia</forename><surname>Polosukhin ; I Guyon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">U V</forename><surname>Luxburg</surname></persName>
		</author>
		<author>
			<persName><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName><surname>Wallach</surname></persName>
		</author>
		<author>
			<persName><surname>Fergus</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
				<editor>
			<persName><surname>Vishwanathan</surname></persName>
		</editor>
		<editor>
			<persName><surname>Garnett</surname></persName>
		</editor>
		<imprint>
			<publisher>Curran Associates, Inc</publisher>
			<date type="published" when="2017">2017</date>
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="page" from="5998" to="6008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<monogr>
		<author>
			<persName><surname>Vig</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L R</forename><surname>Madani</surname></persName>
		</author>
		<author>
			<persName><surname>Varshney</surname></persName>
		</author>
		<author>
			<persName><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName><surname>Others</surname></persName>
		</author>
		<title level="m">Bertology meets biology: Interpreting attention in protein language models. arXiv preprint arXiv</title>
				<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Accurate de novo prediction of protein contact map by ultra-deep learning model</title>
		<author>
			<persName><forename type="first">Sheng</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Siqi</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhen</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Renyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jinbo</forename><surname>Xu</surname></persName>
		</author>
		<idno type="DOI">10.1371/journal.pcbi.1005324</idno>
		<ptr target="https://doi.org/10.1371/journal.pcbi.1005324" />
	</analytic>
	<monogr>
		<title level="j">PLOS Computational Biology</title>
		<imprint>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page">2017</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">Identification of direct residue contacts in proteinprotein interaction by message passing</title>
		<author>
			<persName><forename type="first">R A</forename><surname>Weigt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>White</surname></persName>
		</author>
		<author>
			<persName><surname>Szurmant</surname></persName>
		</author>
		<author>
			<persName><surname>Others</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="s">Proceedings of the</title>
		<imprint>
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">Modern hopfield networks and attention for immune repertoire classification</title>
		<author>
			<persName><forename type="first">Michael</forename><surname>Widrich</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bernhard</forename><surname>Schäfl</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Milena</forename><surname>Pavlović</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hubert</forename><surname>Ramsauer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lukas</forename><surname>Gruber</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Markus</forename><surname>Holzleitner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Johannes</forename><surname>Brandstetter</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Geir</forename><surname>Kjetil Sandve</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Victor</forename><surname>Greiff</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sepp</forename><surname>Hochreiter</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
				<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="volume">33</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">Transformers: State-of-the-art natural language processing</title>
		<author>
			<persName><forename type="first">Thomas</forename><surname>Wolf</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lysandre</forename><surname>Debut</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Victor</forename><surname>Sanh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Julien</forename><surname>Chaumond</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Clement</forename><surname>Delangue</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Anthony</forename><surname>Moi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pierric</forename><surname>Cistac</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tim</forename><surname>Rault</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rémi</forename><surname>Louf</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Morgan</forename><surname>Funtowicz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Joe</forename><surname>Davison</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sam</forename><surname>Shleifer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Clara</forename><surname>Patrick Von Platen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yacine</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Julien</forename><surname>Jernite</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Canwen</forename><surname>Plu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Teven</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sylvain</forename><surname>Le Scao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mariama</forename><surname>Gugger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Quentin</forename><surname>Drame</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alexander</forename><forename type="middle">M</forename><surname>Lhoest</surname></persName>
		</author>
		<author>
			<persName><surname>Rush</surname></persName>
		</author>
		<ptr target="https://www.aclweb.org/anthology/2020.emnlp-demos.6" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing: System Demonstrations</title>
				<meeting>the 2020 Conference on Empirical Methods in Natural Language Processing: System Demonstrations</meeting>
		<imprint>
			<publisher>Online</publisher>
			<date type="published" when="2020-10">October 2020</date>
			<biblScope unit="page" from="38" to="45" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b55">
	<analytic>
		<title level="a" type="main">Improved protein structure prediction using predicted inter-residue orientations</title>
		<author>
			<persName><forename type="first">Jianyi</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ivan</forename><surname>Anishchenko</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hahnbeom</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhenling</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sergey</forename><surname>Ovchinnikov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><surname>Baker</surname></persName>
		</author>
		<idno type="DOI">10.1101/846279v1</idno>
		<ptr target="https://www.biorxiv.org/content/10.1101/846279v1" />
	</analytic>
	<monogr>
		<title level="j">bioRxiv</title>
		<imprint>
			<biblScope unit="page">846279</biblScope>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<monogr>
		<title level="m" type="main">Why adam beats sgd for attention models</title>
		<author>
			<persName><forename type="first">Jingzhao</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sai</forename><surname>Praneeth Karimireddy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andreas</forename><surname>Veit</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Seungyeon</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Sashank</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sanjiv</forename><surname>Reddi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Suvrit</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName><surname>Sra</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1912.03194</idno>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
