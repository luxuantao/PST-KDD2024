<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Direct Access, High-Performance Memory Disaggregation with DIRECTCXL</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Donghyun</forename><surname>Gouk</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Advanced Institute of Science and Technology (KAIST)</orgName>
								<orgName type="laboratory">Computer Architecture and Memory Systems Laboratory</orgName>
								<address>
									<country key="KR">Korea</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Sangwon</forename><surname>Lee</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Advanced Institute of Science and Technology (KAIST)</orgName>
								<orgName type="laboratory">Computer Architecture and Memory Systems Laboratory</orgName>
								<address>
									<country key="KR">Korea</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Miryeong</forename><surname>Kwon</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Advanced Institute of Science and Technology (KAIST)</orgName>
								<orgName type="laboratory">Computer Architecture and Memory Systems Laboratory</orgName>
								<address>
									<country key="KR">Korea</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Myoungsoo</forename><surname>Jung</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Advanced Institute of Science and Technology (KAIST)</orgName>
								<orgName type="laboratory">Computer Architecture and Memory Systems Laboratory</orgName>
								<address>
									<country key="KR">Korea</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Direct Access, High-Performance Memory Disaggregation with DIRECTCXL</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.2" ident="GROBID" when="2023-01-01T13:38+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>New cache coherent interconnects such as CXL have recently attracted great attention thanks to their excellent hardware heterogeneity management and resource disaggregation capabilities. Even though there is yet no real product or platform integrating CXL into memory disaggregation, it is expected to make memory resources practically and efficiently disaggregated much better than ever before.</p><p>In this paper, we propose directly accessible memory disaggregation, DIRECTCXL that straight connects a host processor complex and remote memory resources over CXL's memory protocol (CXL.mem). To this end, we explore a practical design for CXL-based memory disaggregation and make it real. As there is no operating system that supports CXL, we also offer CXL software runtime that allows users to utilize the underlying disaggregated memory resources via sheer load/store instructions. Since DIRECTCXL does not require any data copies between the host memory and remote memory, it can expose the true performance of remote-side disaggregated memory resources to the users.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Memory disaggregation has attracted great attention thanks to its high memory utilization, transparent elasticity, and resource management efficiency <ref type="bibr" target="#b0">[1]</ref><ref type="bibr" target="#b1">[2]</ref><ref type="bibr" target="#b2">[3]</ref>. Many studies have explored various software and hardware approaches to realize memory disaggregation and put significant efforts into making it practical in large-scale systems <ref type="bibr" target="#b3">[4]</ref><ref type="bibr" target="#b4">[5]</ref><ref type="bibr" target="#b5">[6]</ref><ref type="bibr" target="#b6">[7]</ref><ref type="bibr" target="#b7">[8]</ref><ref type="bibr" target="#b8">[9]</ref><ref type="bibr" target="#b9">[10]</ref><ref type="bibr" target="#b10">[11]</ref><ref type="bibr" target="#b11">[12]</ref><ref type="bibr" target="#b12">[13]</ref><ref type="bibr" target="#b13">[14]</ref><ref type="bibr" target="#b14">[15]</ref><ref type="bibr" target="#b15">[16]</ref>.</p><p>We can broadly classify the existing memory disaggregation runtimes into two different approaches based on how they manage data between a host and memory server(s): i) pagebased and ii) object-based. The page-based approach <ref type="bibr" target="#b3">[4]</ref><ref type="bibr" target="#b4">[5]</ref><ref type="bibr" target="#b5">[6]</ref><ref type="bibr" target="#b6">[7]</ref><ref type="bibr" target="#b7">[8]</ref><ref type="bibr" target="#b8">[9]</ref><ref type="bibr" target="#b9">[10]</ref> utilizes virtual memory techniques to use disaggregated memory without a code change. It swaps page cache data residing on the host's local DRAMs from/to the remote memory systems over a network in cases of a page fault. On the other hand, the object-based approach handles disaggregated memory from a remote using their own database such as a key-value store instead of leveraging the virtual memory systems <ref type="bibr" target="#b10">[11]</ref><ref type="bibr" target="#b11">[12]</ref><ref type="bibr" target="#b12">[13]</ref><ref type="bibr" target="#b13">[14]</ref><ref type="bibr" target="#b14">[15]</ref><ref type="bibr" target="#b15">[16]</ref>. This approach can address the challenges imposed by address translation (e.g., page faults, context switching, and write amplification), but it requires significant sourcelevel modifications and interface changes.</p><p>While there are many variants, all the existing approaches need to move data from the remote memory to the host memory over remote direct memory access (RDMA) <ref type="bibr">[4, 5, 11-13, 15, 16]</ref> (or similar fine-grain network interfaces <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b8">9,</ref><ref type="bibr" target="#b9">10,</ref><ref type="bibr" target="#b16">17]</ref>). In addition, they even require managing locally cached data in either the host or memory nodes. Unfortunately, the data movement and its accompanying operations (e.g., page cache management) introduce redundant memory copies and software fabric intervention, which makes the latency of disaggregated memory longer than that of local DRAM accesses by multiple orders of magnitude. In this work, we advocate compute express link (CXL <ref type="bibr" target="#b17">[18]</ref>), which is a new concept of open industry standard interconnects offering high-performance connectivity among multiple host processors, hardware accelerators, and I/O devices <ref type="bibr" target="#b18">[19]</ref>. CXL is originally designed to achieve the excellency of heterogeneity management across different processor complexes, but both industry and academia anticipate its cache coherence ability can help improve memory utilization and alleviate memory over-provisioning with low latency <ref type="bibr" target="#b19">[20]</ref><ref type="bibr" target="#b20">[21]</ref><ref type="bibr" target="#b21">[22]</ref>. Even though CXL exhibits a great potential to realize memory disaggregation with low monetary cost and high performance, it has not been yet made for production, and there is no platform to integrate memory into a memory pooling network.</p><p>We demonstrate DIRECTCXL, direct accessible disaggregated memory that connects host processor complex and remote memory resources over CXL's memory protocol (CXL.mem). To this end, we explore a practical design for CXL-based memory disaggregation and make it real. Specifically, we first show how to disaggregate memory over CXL and integrate the disaggregated memory into processor-side system memory. This includes implementing CXL controller that employs multiple DRAM modules on a remote side. We then prototype a set of network infrastructure components such as a CXL switch in order to make the disaggregated memory connected to the host in a scalable manner. As there is no operating system that support CXL, we also offer CXL software runtime that allows users to utilize the underlying disaggregated memory resources through sheer load/store instructions. DIRECTCXL does not require any data copies between the host memory and remote memory, and therefore, it can expose the true performance of remote-side disaggregated memory resources to the users.</p><p>In this work, we prototype DIRECTCXL using many cus-tomized memory add-in-cards, 16nm FPGA-based processor nodes, a switch, and a PCIe backplane. On the other hand, DI-RECTCXL software runtime is implemented based on Linux 5.13. To the best of our knowledge, this is the first work that brings CXL 2.0 into a real system and analyzes the performance characteristics of CXL-enabled disaggregated memory design. The results of our real system evaluation show that the disaggregated memory resources of DIRECTCXL can exhibit DRAM-like performance when the workload can enjoy the host processor's cache. When the load/store instructions go through the CXL network and are served from the disaggregated memory, DIRECTCXL's latency is shorter than the best latency of RDMA by 6.2×, on average. For real-world applications, DIRECTCXL exhibits 3× better performance than RDMA-based memory disaggregation, on average.</p><p>2 Memory Disaggregation and Related Work</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Remote Direct Memory Access</head><p>The basic idea of memory disaggregation is to connect a host with one or more memory nodes, such that it does not restrict a given job execution because of limited local memory space.</p><p>For the backend network control, most disaggregation work employ remote direct memory access (RDMA) <ref type="bibr">[4, 5, 11-13, 15,16]</ref> or similar customized DMA protocols <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b8">9,</ref><ref type="bibr" target="#b9">10]</ref>. Figure <ref type="figure">1</ref> shows how RDMA-style data transfers (one-sided RDMA) work. For both the host and memory node sides, RDMA needs hardware support such as RDMA NIC (RNIC <ref type="bibr" target="#b22">[23]</ref>), which is designed toward removing the intervention of the network software stack as much as possible. To move data between them, processes on each side first require defining one or more memory regions (MRs) and letting the MR(s) to the underlying RNIC. During this time, the RNIC driver checks all physical addresses associated with the MR's pages and registers them to RNIC's memory translation table (MTT). Since those two RNICs also exchange their MR's virtual address at the initialization, the host can simply send the memory node's destination virtual address with data for a write. The remote node then translates the address by referring to its MTT and copies the incoming data to the target location of MR. Reads over RDMA can also be performed in a similar manner. Note that, in addition to the memory copy operations (for DMA), each side's application needs to prepare or retrieve the data into/from MRs for the data transfers, introducing additional data copies within their local DRAM <ref type="bibr" target="#b23">[24]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Swap: Page-based Memory Pool</head><p>Page-based memory disaggregation <ref type="bibr" target="#b3">[4]</ref><ref type="bibr" target="#b4">[5]</ref><ref type="bibr" target="#b5">[6]</ref><ref type="bibr" target="#b6">[7]</ref><ref type="bibr" target="#b7">[8]</ref><ref type="bibr" target="#b8">[9]</ref><ref type="bibr" target="#b9">[10]</ref> achieves memory elasticity by relying on virtual memory systems. Specifically, this approach intercepts paging requests when there is a page fault, and then it swaps the data to a remote memory node instead of the underlying storage. To this end, a disaggregation driver underneath the host's kernel swap daemon (kswapd) converts the incoming block address to the memory node's virtual address. It then copies the target page to RNIC's MR and issues the corresponding RDMA request to the memory node. Since all operations for memory disaggregation is managed under kswapd, it is easy-to-adopt and transparent to all user applications. However, page-based systems suffer from performance degradation due to the overhead of page fault handling, I/O amplifications, and context switching when there are excessive requests for the remote memory <ref type="bibr" target="#b15">[16]</ref>.</p><p>Note that there are several studies that migrate locally cached data in a finer granular manner <ref type="bibr" target="#b3">[4]</ref><ref type="bibr" target="#b4">[5]</ref><ref type="bibr" target="#b5">[6]</ref><ref type="bibr" target="#b6">[7]</ref> or reduce the page fault overhead by offloading memory management (including page cache coherence) to the network <ref type="bibr" target="#b7">[8]</ref> or memory nodes <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b9">10]</ref>. However, all these approaches use RDMA (or a similar network protocol), which is essential to cache the data and pay the cost of memory operations for network handling.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">KVS: Object-based Memory Pool</head><p>In contrast, object-based memory disaggregation systems <ref type="bibr" target="#b10">[11]</ref><ref type="bibr" target="#b11">[12]</ref><ref type="bibr" target="#b12">[13]</ref><ref type="bibr" target="#b13">[14]</ref><ref type="bibr" target="#b14">[15]</ref><ref type="bibr" target="#b15">[16]</ref> directly intervene in RDMA data transfers using their own database such as key-value store (KVS). Object-based systems create two MRs for both host and memory node sides, each dealing with buffer data and submission/completion queues (SQ/CQ). Generally, they employ a KV hash-table whose entries point to corresponding (remote) memory objects. Whenever there is a request of Put (or Get) from an application, the systems place the corresponding value into the host's buffer MR and submit it by writing the remote side of SQ MR over RDMA. Since the memory node keeps polling SQ MR, it can recognize the request. The memory node then reads the host's buffer MR, copies the value to its buffer MR over RDMA, and completes the request by writing the host's CQ MR. As it does not lean on virtual memory systems, object-based systems can address the overhead imposed by page swap. However, the performance of objectbased systems varies based on the semantics of applications compared to page-based systems; kswapd fully utilizes local page caches, but KVS does not for remote accesses. In addition, this approach is unfortunately limited because it requires significant source-level modifications for legacy applications.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Direct Accessible Memory Aggregation</head><p>While caching pages and network-based data exchange are essential in the current technologies, they can unfortunately significantly deteriorate the performance of memory disaggregation. DIRECTCXL instead directly connects remote memory resources to the host's computing complex and allows users to access them through sheer load/store instructions.    </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Connecting Host and Memory over CXL</head><p>CXL devices and controllers. In practice, existing memory disaggregation techniques still require computing resources at the remote memory node side. This is because all DRAM modules and their interfaces are designed as passive peripherals, which require the control computing resources. CXL.mem in contrast allows the host computing resources directly access the underlying memory through PCIe buses (FlexBus); it works similar to local DRAM, connected to their system buses. Thus, we design and implement CXL devices as pure passive modules, each being able to have many DRAM DIMMs with its own hardware controllers. Our CXL device employs multiple DRAM controllers, connecting DRAM DIMMs over the conventional DDR interfaces. Its CXL controller then exposes the internal DRAM modules to FlexBus through many PCIe lanes. In the current architecture, the device's CXL controller parses incoming PCIe-based CXL packets, called CXL flits, converts their information (address and length) to DRAM requests, and serves them from the underlying DRAMs using the DRAM controllers. Integrating devices into system memory. Figure <ref type="figure" target="#fig_0">2</ref> shows how CXL devices' internal DRAMs are mapped (exposed) to a host's memory space over CXL. The host CPU's system bus contains one or more CXL root ports (RPs), which connect one or more CXL devices as endpoint (EP) devices. Our host-side kernel driver first enumerates CXL devices by querying the size of their base address register (BAR) and their internal memory, called host-managed device memory (HDM), through PCIe transactions. Based on the retrieved sizes, the kernel driver maps BAR and HDM in the host's reserved system memory space and lets the underlying CXL devices know where their BAR and HDM (base addresses) are mapped in the host's system memory. When the host CPU accesses an HDM system memory through load/store instruction, the request is delivered to the corresponding RP, and the RP converts the requests to a CXL flit. Since HDM is mapped to a different location of the system memory, the memory address space of HDM is different from that of EP's internal DRAMs. Thus, the CXL controller translates the incoming addresses by simply deducting HDM's base address from them and issues the translated request to the underlying DRAM controllers. The results are returned to the host via a CXL switch and FlexBus. Note that, since HDM accesses have no software intervention or memory data copies, DIRECTCXL can expose the CXL device's memory resources to the host with low access latency.</p><p>Designing CXL network switch. Figure <ref type="figure" target="#fig_2">3a</ref> illustrates how DIRECTCXL can disaggregate memory resources from a host using one or more and CXL devices, and Figure <ref type="figure" target="#fig_2">3b</ref> shows our CXL switch organization therein. The host's CXL RP is connected to upstream port (USP) of either a CXL switch or the CXL device directly. The CXL switch's downstream port (DSP) also connects either another CXL switch's USP or the CXL device. Note that our CXL switch employs multiple USPs and DSPs. By setting an internal routing table, our CXL switch's fabric manager (FM) reconfigures the switch's crossbar to connect each USP to a different DSP, which creates a virtual hierarchy from a root (host) to a terminal (CXL device). Since a CXL device can employ one or more controllers and many DRAMs, it can also define multiple logical devices, each exposing its own HDM to a host. Thus, different hosts can be connected to a CXL switch and a CXL device. Note that each CXL virtual hierarchy only offers the path from one to another to ensure that no host is sharing an HDM.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Software Runtime for DirectCXL</head><p>In contrast to RDMA, once a virtual hierarchy is established between a host and CXL device(s), applications running on the host can directly access the CXL device by referring to HDM's memory space. However, it requires software runtime/driver to manage the underlying CXL devices and expose their HDM in the application's memory space. We thus support DIRECTCXL runtime that simply splits the address space of HDM into multiple segments, called cxl-namespace. DIRECTCXL runtime then allows the applications to access each CXL-namespace as memory-mapped files (mmap).</p><p>Figure <ref type="figure" target="#fig_3">4</ref> shows the software stack of our runtime and how the application can use the disaggregated memory through cxl-namespaces. When a CXL device is detected (at a PCIe enumeration time), DIRECTCXL driver creates an entry device (e.g., /dev/directcxl) to allow users to manage a cxl-namespace via ioctl. If users ask a cxl-namespace to /dev/directcxl, the driver checks a (physically) contiguous address space on an HDM by referring to its HDM segment table whose entry includes a segment's offset, size, and reference count (recording how many cxl-namespaces that indicate this segment). Since multiple processes can access this table, its header also keeps necessary information such as spinlock, read/write locks, and a summary of table entries (e.g., valid entry numbers). Once DIRECTCXL driver allocates a segment based on the user request, it creates a device for mmap (e.g., /dev/cxl-ns0) and updates the segment table. The user  application can then map the cxl-namespace to its process virtual memory space using mmap with vm_area_struct.</p><p>Note that DIRECTCXL software runtime is designed for direct access of CXL devices, which is a similar concept to the memory-mapped file management of persistent memory development toolkit (PMDK <ref type="bibr" target="#b24">[25]</ref>). However, it is much simpler and more flexible for namespace management than PMDK. For example, PMDK's namespace is very much the same idea as NVMe namespace, managed by file systems or DAX with a fixed size <ref type="bibr" target="#b25">[26]</ref>. In contrast, our cxl-namespace is more similar to the conventional memory segment, which is directly exposed to the application without a file system employment.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Prototype Implementation</head><p>Figure <ref type="figure" target="#fig_5">5a</ref> illustrates our design of a CXL network topology to disaggregate memory resources, and the corresponding implementation in a real system is shown in Figure <ref type="figure" target="#fig_5">5b</ref>. There are n numbers of compute hosts connected to m number of CXL devices through a CXL switch; in our prototype, n and m are four, but those numbers can scale by having more CXL switches. Specifically, each CXL device prototype is built on our customized add-in-card (AIC) CXL memory blade that employs 16nm FPGA and 8 different DDR4 DRAM modules (64GB). In the FPGA, we fabricate a CXL controller and eight DRAM controllers, each managing the CXL endpoint and internal DRAM channels. As yet there is no processor architecture supporting CXL, we also build our own in-house host processor using RISC-V ISAs, which employs four out-oforder cores whose last-level cache (LLC) implements CXL RP. Each CXL-enabled host processor is implemented in a highperformance datacenter accelerator card, taking a role of a host, which can individually run Linux 5.13 and DIRECTCXL software runtime. We expose four CXL devices (32 DRAM modules) to the four hosts through our PCIe backplane. We extended the backplane with one more accelerator card that implements DIRECTCXL's CXL switch. This switch implements FM that can create multiple virtual hierarchies, each connecting a host and a CXL device in a flexible manner.</p><p>To the best of our knowledge, there are no commercialized CXL 2.0 IPs for the processor side's CXL engines and CXL switch. Thus, we built all DIRECTCXL IPs from the ground. The host-side processors require advanced configuration and power interface (ACPI <ref type="bibr" target="#b26">[27]</ref>) for CXL 2.0 enumeration (e.g., RP location and RP's reserved address space). Since RISC-V does not support ACPI yet, we enable the CXL enumeration by adding such information into the device tree <ref type="bibr" target="#b27">[28]</ref>. Specifically, we update an MMIO register designated as a property of the tree's node to let the processor know where CXL RP exists. On the other hand, we add a new field (cxl-reserved-area) in the node to indicate where an HDM can be mapped. Our in-house softcore processors work at 100MHz while CXL and PCIe IPs (RP, EP, and Switch) operate at 250MHz.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Evaluation</head><p>Testbed prototypes for memory disaggregation. In addition to the CXL environment that we implemented in Section 3.3 (DirectCXL), we set up the same configuration with it for our RDMA-enabled hardware system (RDMA). For RDMA, we use Mellanox ConnectX-3 VPI InfiniBand RNIC (56Gbps, <ref type="bibr" target="#b28">[29]</ref>) instead of our CXL switch as RDMA network interface card (RNIC). In addition, we port Mellanox OpenFabric Enterprise Distribution (OFED) v4.9 <ref type="bibr" target="#b29">[30]</ref> as an RDMA driver to enable RNIC in our evaluation testbed. Lastly, we port FastSwap <ref type="bibr" target="#b0">[1]</ref> and HERD <ref type="bibr" target="#b11">[12]</ref> into RISC-V Linux 5.13.19 computing environment atop RDMA, each realizing page-based disaggregation (Swap) and object-based disaggregation (KVS).</p><p>For better comparison, we also configure the host processors to use only their local DRAM (Local) by disabling all the CXL memory nodes. Note that we used the same testbed hardware mentioned above for both CXL experiments and non-CXL experiments but differently configured the testbed for each reference. For example, our testbed's FPGA chips for the host (in-house) processors and CXL devices use all the same architecture/technology and product line-up. Benchmark and workloads. Since there is no microbenchmark that we can compare different memory pooling technologies (RDMA vs. DirectCXL), we also build an in-house memory benchmark for in-depth analysis of those two technologies (Section 4.1). For RDMA, this benchmark allocates a large size of the memory pool at the remote side in advance. This benchmark allows a host processor to send random memory requests to a remote node with varying lengths; the remote node serves the requests using the pre-allocated memory pool. For DirectCXL and Local, the benchmark maps cxl namespace or anonymous mmap to user spaces, respectively. The benchmark then generates a group of RISC-V memory instructions, which can cover a given address length in a random pattern and directly issues them without software intervention. For the real workloads, we use Facebook's deep learning recommendation model (DLRM <ref type="bibr" target="#b30">[31]</ref>), an in-memory database used for the HERD evaluation (MemDB <ref type="bibr" target="#b11">[12]</ref>), and four graph analysis workloads (MIS <ref type="bibr" target="#b31">[32]</ref>, BFS <ref type="bibr" target="#b32">[33]</ref>, CC <ref type="bibr" target="#b33">[34]</ref>, and BC <ref type="bibr" target="#b34">[35]</ref>) coming from Ligra <ref type="bibr" target="#b35">[36]</ref>. All their tables and data structures are stored in the remote node, while each host's local memory handles the execution code and static data. Table <ref type="table" target="#tab_1">1</ref> summarizes the per-node memory usage and total data sizes for each workload that we tested.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">In-depth Analysis of RDMA and CXL</head><p>In this subsection, we compare the performance of RDMA and CXL technologies when the host and memory nodes are   configured through a 1:1 connection. Figure <ref type="figure" target="#fig_6">6</ref> shows latency breakdown of RDMA and DirectCXL when reading 64 bytes of data. One can observe from the figure that RDMA requires two DMA operations, which doubles the PCIe transfer and memory access latency. In addition, the communication overhead of InfiniBand (Network) takes 78.7% (2129 cycles) of the total latency (2705 cycles). In contrast, DirectCXL only takes 328 cycles for memory load request, which is 8.3× faster than RDMA. There are two reasons behind this performance difference. First, DirectCXL straight connects the compute nodes and memory nodes using PCIe while RDMA requires protocol/interface changes between InfiniBand and PCIe. Second, DirectCXL can translate memory load/store request from LLC into the CXL flits whereas RDMA must use DMA to read/write data from/to memory. Sensitivity tests. Figure <ref type="figure" target="#fig_7">7a</ref> decomposes RDMA latency into essential hardware (Memory and Network), software (Library), and data transfer latencies (Copy). In this evaluation, we instrument two user-level InfiniBand libraries, libibverbs and libmlx4 to measure the software side latency. Library is the primary performance bottleneck in RDMA when the size of payloads is smaller than 1KB (4158 cycles, on average). As the payloads increase, Copy gets longer and reaches 28.9% of total execution time. This is because users must copy all their data into RNIC's MR, which takes extra overhead in RDMA. On the other hand, Memory and Network shows a performance trend similar to RDMA analyzed in Figure <ref type="figure" target="#fig_6">6</ref>. Note that the actual times of Network (Figure <ref type="figure" target="#fig_7">7a</ref>) do not decrease as the payload increases; while Memory increases to handle large size of data, RNIC can simultaneously transmit the data to the underlying network. These overlapped cycles are counted by Memory in our analysis. As shown in Figure <ref type="figure" target="#fig_7">7b</ref>, the breakdown analysis for DirectCXL shows a completely different story; there is neither software nor data copy overhead. As the payloads increase, the dominant component of DirectCXL's latency is LLC (CPU Cache). This is because LLC can handle 16 concurrent misses through miss status holding registers (MSHR) in our custom CPU. Thus, many memory requests (64B) composing a large payload data can be stalled at CPU, which takes 67% of the total latency to handle 4KB payloads. PCIe shown in Figure <ref type="figure" target="#fig_7">7a</ref> does not decrease as the payloads increase because of a similar reason of RDMA's Network. However, it</p><p>Per-node usage Total usage Data stored in remote memory Local Remote DLRM <ref type="bibr" target="#b30">[31]</ref> Less than 100MB 17GB 68GB Embedding tables. MemDB <ref type="bibr" target="#b11">[12]</ref> 4GB 16GB Key-value pairs and tree structure. Ligra <ref type="bibr" target="#b35">[36]</ref> 7GB 28GB Deserialized graph structure. ). This can be accelerated by increasing the working frequency, which will be discussed shortly.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Latency Distribution and Scaling Study</head><p>Latency distribution. In addition to the latency trend (average) we reported above, we also analyze complete latency behaviors of Local, RDMA, and DirectCXL. Figure <ref type="figure" target="#fig_9">9</ref> shows the latency CDF of memory accesses (64B) for the different pooling methods. RDMA shows the performance curve, which ranges from 1790 cycles to 4006 cycles. The reason why there is a difference between the minimum and maximum latency of RDMA is RNIC's MTT memory buffer and CPU caches for data transfers. While RDMA cannot take the benefits from direct load/store instruction with CPU caches, its data transfers themselves utilize CPU caches. Nevertheless, RDMA cannot avoid the network accesses for remote memory accesses, making its latency worse than Local by 36.8×, on average. In contrast, the latency behaviors of DirectCXL are similar to Local. Even though the latency of DirectCXL (reported in Figures <ref type="figure" target="#fig_7">6 and 7b</ref>) is the average value, its best performance is the same as Local (4∼24 cycles). This is because, as we showed in the previous section, DirectCXL can take the benefits of CPU caches directly. The tail latency is 2.8× worse than Local, but its latency curve is similar to that of Local. This is because both DirectCXL and Local use the same DRAM (and there is no network access overhead). Speed scaling estimation. The cycle numbers that we reported here are measured at each host's CPU using registerlevel instrumentation. We believe it is sufficient and better     than a cross-time-domain analysis to decompose the system latency. Nevertheless, we estimate a time delay in cases where the target system accelerates the frequency of its processor complex and CXL IPs (RP, EP, and Switch) by 1.2GHz and 1GHz, respectively. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Performance of Real Workloads</head><p>Figure <ref type="figure" target="#fig_11">10a</ref> shows the execution latency of Swap, KVS, and DirectCXL when running DLRM, MemDB, and four workloads from Ligra. For better understanding, all the results in this subsection are normalized to those of Swap. For Ligra, we only compare DirectCXL with Swap because Ligra's graph processing engines (handling in-/out-edges and vertices) is not compatible with a key-value structure. KVS can reduce the latency of Swap as it addresses the overhead imposed by pagebased I/O granularity to access the remote memory. However, it has two major issues behind KVS. First, it requires significant modification of the application's source codes, which is often unable to service (e.g., MIS, BFS, CC, BC). Second, KVS requires heavy computation such as hashing at the memory node, which increases monetary costs. In contrast, DirectCXL without having a source modification and remoteside resource exhibits 3× and 2.2× better performance than Swap and even KVS, respectively. To better understand this performance improvement of DirectCXL, we also decompose the execution times into RDMA, network library intervention (Software), and application execution itself (Workload) latencies, and the results are shown in Figure <ref type="figure" target="#fig_11">10b</ref>. This figure demonstrates where Swap degrades the overall performance from its execution; 51.8% of the execution time is consumed by kernel swap daemon (kswapd) and FastSwap driver, on average. This is because Swap just expands memory with the local and remote based on LRU, which makes its page exchange frequent. The reason why KVS shows performance better than Swap in the cases of DLRM and MemDB is mainly related to workload characteristics and its service optimization. For DLRM, KVS loads the exact size of embeddings rather than a page, which reduces Swap's data transfer overhead as high as 6.9×. While KVS shows the low overhead in our evaluation, RDMA and Software can linearly increase as the number of inferences increases; in our case, we only used 13.5MB (0.0008%) of embeddings for single inference. For MemDB, as KVS stores all key-value pairs into local DRAM, it only accesses remoteside DRAM to inquiry values. However, it spends 55.3% and 24.9% of the execution time for RDMA and Software to handle the remote DRAMs, respectively. In contrast, DirectCXL removes such hardware and software overhead, which exhibits much better performance than Swap and KVS. Note that MemDB contains 2M key-value pairs whose value size is 2KB, and its host queries 8M Get requests by randomly generating their keys. This workload characteristic roughly makes DirectCXL's memory accesses be faced with a cache miss for every four queries. Note that Workload of DirectCXL is longer than that of KVS, because DirectCXL places all hash table and tree for key-value pairs whereas KVS has it in local DRAM. Lastly, all the four graph workloads show similar trends; Swap is always slower than DirectCXL. They require multiple graph traverses, which frequently generate random memory access patterns. As Swap requires exchanging 4KB pages to read 8B pointers for graph traversing, it shows 2.2× worse performance than DirectCXL.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusion</head><p>In this paper, we propose DIRECTCXL that connects host processor complex and remote memory resources over CXL's memory protocol (CXL.mem). The results of our real system evaluation show that the disaggregated memory resources of DIRECTCXL can exhibit DRAM-like performance when the workload can enjoy the host-processor's cache. For realworld applications, it exhibits 3× better performance than RDMA-based memory disaggregation, on average.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Future Work and Acknowledgement</head><p>The authors are extending the kernel for efficient CXL memory management and consider having an SoC silicon as future work of DirectCXL. This work is protected by one or more patents. The authors would like to thank the anonymous reviewers for their comments, and Myoungsoo Jung is the corresponding author (mj@camelab.org).</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: DIRECTCXL's connection method.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head></head><label></label><figDesc>(a) CXL virtual hierarchy.(b) CXL switch.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: DIRECTCXL's network and switch.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 4 :</head><label>4</label><figDesc>Figure 4: DIRECTCXL software runtime.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 5 :</head><label>5</label><figDesc>Figure 5: CXL-enabled cluster.</figDesc><graphic url="image-3.png" coords="5,197.85,76.13,85.18,58.67" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 6 :</head><label>6</label><figDesc>Figure 6: RDMA vs. CXL.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure 7 :</head><label>7</label><figDesc>Figure 7: Sensitivity tests.Figure8: Memory hierarchy performance.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Figure 8 :</head><label>8</label><figDesc>Figure 7: Sensitivity tests.Figure8: Memory hierarchy performance.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>Figure 9 :</head><label>9</label><figDesc>Figure 9: Memory-level latency CDF (64B).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_11"><head>Figure 10 :</head><label>10</label><figDesc>Figure 10: Real workload performance.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0"><head></head><label></label><figDesc></figDesc><graphic url="image-1.png" coords="1,-9.00,-10.01,630.00,255.87" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0"><head></head><label></label><figDesc></figDesc><graphic url="image-2.png" coords="1,-9.00,543.00,630.00,259.01" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 1 :</head><label>1</label><figDesc>Memory usage characteristic of each workload. is not as much as what Network did as only 16 concurrent misses can be overlapped. ote that PCIe shown in Figures 6 and 7b includes the latency of CXL IPs (RP, EP, and Switch), which is different from the pure cycles of PCIe physical bus. The pure cycles of PCIe physical bus (FlexBus) account for 28% of DirectCXL latency. The detailed latency decomposition will be analyzed in Section 4.2. Memory hierarchy performance. Figure 8 shows latency cycles of different components in the system's memory hierarchy. While Local and DirectCXL exhibits CPU cache by lowering the memory access latency to 4 cycles, RDMA has negligible impacts on CPU cache as their network overhead is much higher than that of Local. The best-case performance of RDMA was 2027 cycles, which is 6.2× and 510.5× slower than that of DirectCXL and L1 cache, respectively. DirectCXL requires 328 cycles whereas Local requires only 60 cycles in the case of L2 misses. Note that the performance bottleneck of DirectCXL is PCIe including CXL IPs (77.8% of the total latency</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 2 :</head><label>2</label><figDesc>Latency breakdown and estimated 64B load latency.</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 2 decomposes</head><label>2</label><figDesc>DirectCXL's latency of a 64B memory load and compares it with the estimated time delay. The cycle counts of L1/L2 cache misses are not different as they work in all the same clock domain of CPU. While other components (FlexBus, CXL IPs, and DRAM controller) speed up by 4× (250MHz → 1GHz), the number of cycles increases since CPU gets faster by 12×. Note that, as the version of PCIe is changed and the number of lanes for PCIe increases by double, FlexBus's cycles decrease. The table includes the time delays corresponding to the estimated system from the CPU's viewpoint. While the time delay of FlexBus is pretty good (∼60ns), the corresponding CXL IPs have room to improve further with a higher working frequency.</figDesc><table /></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Can far memory improve job throughput?</title>
		<author>
			<persName><forename type="first">Emmanuel</forename><surname>Amaro</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christopher</forename><surname>Branner-Augmon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhihong</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Amy</forename><surname>Ousterhout</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Marcos</forename><forename type="middle">K</forename><surname>Aguilera</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aurojit</forename><surname>Panda</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sylvia</forename><surname>Ratnasamy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Scott</forename><surname>Shenker</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Fifteenth European Conference on Computer Systems</title>
				<meeting>the Fifteenth European Conference on Computer Systems</meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="1" to="16" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Memory disaggregation: Research problems and opportunities</title>
		<author>
			<persName><forename type="first">Ling</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wenqi</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Semih</forename><surname>Sahin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qi</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Juhyun</forename><surname>Bae</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yanzhao</forename><surname>Wu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2019 IEEE 39th International Conference on Distributed Computing Systems (ICDCS)</title>
				<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="1664" to="1673" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">System-level implications of disaggregated memory</title>
		<author>
			<persName><forename type="first">Kevin</forename><surname>Lim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yoshio</forename><surname>Turner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jose</forename><forename type="middle">Renato</forename><surname>Santos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alvin</forename><surname>Auyoung</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jichuan</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Thomas</forename><forename type="middle">F</forename><surname>Parthasarathy Ranganathan</surname></persName>
		</author>
		<author>
			<persName><surname>Wenisch</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Symposium on High-Performance Comp Architecture</title>
				<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2012">2012</date>
			<biblScope unit="page" from="1" to="12" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Efficient memory disaggregation with infiniswap</title>
		<author>
			<persName><forename type="first">Juncheng</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Youngmoon</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yiwen</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mosharaf</forename><surname>Chowdhury</surname></persName>
		</author>
		<author>
			<persName><surname>Kang G Shin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">14th USENIX Symposium on Networked Systems Design and Implementation (NSDI 17)</title>
				<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="649" to="667" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Remote regions: a simple abstraction for remote memory</title>
		<author>
			<persName><forename type="first">Marcos</forename><forename type="middle">K</forename><surname>Aguilera</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nadav</forename><surname>Amit</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Irina</forename><surname>Calciu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xavier</forename><surname>Deguillard</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jayneel</forename><surname>Gandhi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stanko</forename><surname>Novakovic</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Arun</forename><surname>Ramanathan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pratap</forename><surname>Subrahmanyam</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lalith</forename><surname>Suresh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kiran</forename><surname>Tati</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">USENIX Annual Technical Conference (USENIX ATC 18)</title>
				<imprint>
			<date type="published" when="2018">2018. 2018</date>
			<biblScope unit="page" from="775" to="787" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Semeru: A memory-disaggregated managed runtime</title>
		<author>
			<persName><forename type="first">Chenxi</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Haoran</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shi</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuanqi</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhenyuan</forename><surname>Ruan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Khanh</forename><surname>Nguyen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><forename type="middle">D</forename><surname>Bond</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ravi</forename><surname>Netravali</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Miryung</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Guoqing</forename><forename type="middle">Harry</forename><surname>Xu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">14th USENIX Symposium on Operating Systems Design and Implementation</title>
				<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="page" from="261" to="280" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Thymesisflow: a softwaredefined, hw/sw co-designed interconnect stack for rackscale memory disaggregation</title>
		<author>
			<persName><forename type="first">Christian</forename><surname>Pinto</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dimitris</forename><surname>Syrivelis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michele</forename><surname>Gazzetti</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Panos</forename><surname>Koutsovasilis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrea</forename><surname>Reale</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kostas</forename><surname>Katrinis</surname></persName>
		</author>
		<author>
			<persName><surname>Peter Hofstee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2020 53rd Annual IEEE/ACM International Symposium on Microarchitecture (MICRO)</title>
				<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="868" to="880" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Mind: In-network memory management for disaggregated data centers</title>
		<author>
			<persName><forename type="first">Seung-Seob</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yanpeng</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yupeng</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Anurag</forename><surname>Khandelwal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lin</forename><surname>Zhong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Abhishek</forename><surname>Bhattacharjee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the ACM SIGOPS 28th Symposium on Operating Systems Principles</title>
				<meeting>the ACM SIGOPS 28th Symposium on Operating Systems Principles</meeting>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="488" to="504" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Clio: A hardware-software codesigned disaggregated memory system</title>
		<author>
			<persName><forename type="first">Zhiyuan</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yizhou</forename><surname>Shan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xuhao</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yutong</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yiying</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 27th ACM International Conference on Architectural Support for Programming Languages and Operating Systems</title>
				<meeting>the 27th ACM International Conference on Architectural Support for Programming Languages and Operating Systems</meeting>
		<imprint>
			<date type="published" when="2022">2022</date>
			<biblScope unit="page" from="417" to="433" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Rethinking software runtimes for disaggregated memory</title>
		<author>
			<persName><forename type="first">Irina</forename><surname>Calciu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ivan</forename><surname>Talha Imran</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sanidhya</forename><surname>Puddu</surname></persName>
		</author>
		<author>
			<persName><surname>Kashyap</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Al</forename><surname>Hasan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Onur</forename><surname>Maruf</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aasheesh</forename><surname>Mutlu</surname></persName>
		</author>
		<author>
			<persName><surname>Kolli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 26th ACM International Conference on Architectural Support for Programming Languages and Operating Systems</title>
				<meeting>the 26th ACM International Conference on Architectural Support for Programming Languages and Operating Systems</meeting>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="79" to="92" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Disaggregating persistent memory and controlling them remotely: An exploration of passive disaggregated keyvalue stores</title>
		<author>
			<persName><forename type="first">Shin-Yeh</forename><surname>Tsai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yizhou</forename><surname>Shan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yiying</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2020 USENIX Annual Technical Conference (USENIX ATC 20)</title>
				<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="33" to="48" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Using rdma efficiently for key-value services</title>
		<author>
			<persName><forename type="first">Anuj</forename><surname>Kalia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><surname>Kaminsky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><forename type="middle">G</forename><surname>Andersen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2014 ACM Conference on SIGCOMM</title>
				<meeting>the 2014 ACM Conference on SIGCOMM</meeting>
		<imprint>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="295" to="306" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Farm: Fast remote memory</title>
		<author>
			<persName><forename type="first">Aleksandar</forename><surname>Dragojević</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dushyanth</forename><surname>Narayanan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Miguel</forename><surname>Castro</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Orion</forename><surname>Hodson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">11th USENIX Symposium on Networked Systems Design and Implementation (NSDI 14)</title>
				<imprint>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="401" to="414" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">No compromises: Distributed transactions with consistency, availability, and performance</title>
		<author>
			<persName><forename type="first">Aleksandar</forename><surname>Dragojević</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dushyanth</forename><surname>Narayanan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Edmund</forename><forename type="middle">B</forename><surname>Nightingale</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Matthew</forename><surname>Renzelmann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alex</forename><surname>Shamis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Anirudh</forename><surname>Badam</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Miguel</forename><surname>Castro</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 25th symposium on operating systems principles</title>
				<meeting>the 25th symposium on operating systems principles</meeting>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="54" to="70" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Latency-tolerant software distributed shared memory</title>
		<author>
			<persName><forename type="first">Jacob</forename><surname>Nelson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Brandon</forename><surname>Holt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Brandon</forename><surname>Myers</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Preston</forename><surname>Briggs</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Luis</forename><surname>Ceze</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Simon</forename><surname>Kahan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mark</forename><surname>Oskin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2015 USENIX Annual Technical Conference (USENIX ATC 15)</title>
				<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="291" to="305" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Aifm: High-performance, application-integrated far memory</title>
		<author>
			<persName><forename type="first">Zhenyuan</forename><surname>Ruan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Malte</forename><surname>Schwarzkopf</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Marcos</forename><forename type="middle">K</forename><surname>Aguilera</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Adam</forename><surname>Belay</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">14th USENIX Symposium on Operating Systems Design and Implementation (OSDI 20)</title>
				<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="315" to="332" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title/>
		<author>
			<persName><forename type="first">Gen-Z</forename><surname>Consortium</surname></persName>
		</author>
		<ptr target="https://genzconsortium.org/specifications/" />
	</analytic>
	<monogr>
		<title level="j">Gen-Z Final Specifications</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">Compute Express Link Specification Revision 2</title>
		<ptr target="https://www.computeexpresslink.org/download-the-specification" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<ptr target="https://www.computeexpresslink.org/_files/ugd/0c1418_14c5283e7f3e40f9b2955c7d0f60bebe.pdf" />
		<title level="m">Compute Express Link™ 2.0 White Paper</title>
				<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<author>
			<persName><forename type="first">Navin</forename><surname>Shenoy</surname></persName>
		</author>
		<ptr target="https://newsroom.intel.com/editorials/milestone-moving-data" />
		<title level="m">Milestone in Moving Data</title>
				<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<author>
			<persName><forename type="first">Debendra</forename><surname>Das</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sharma</forename></persName>
		</author>
		<ptr target="https://www.electronicdesign.com/technologies/embedded-revolution/article/21162617/cxl-coherency-memory-and-io-semantics-on-pcie-infrastructure" />
		<title level="m">CXL: Coherency, Memory, and I/O Semantics on PCIe Infrastructure</title>
				<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">Compute Express Link or CXL What it is and Examples</title>
		<author>
			<persName><forename type="first">Patrick</forename><surname>Kennedy</surname></persName>
		</author>
		<ptr target="https://www.servethehome.com/compute-express-link-or-cxl-what-it-is-and-examples/" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Rdma over ethernet-a preliminary study</title>
		<author>
			<persName><forename type="first">Hari</forename><surname>Subramoni</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ping</forename><surname>Lai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Miao</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dhabaleswar</forename><forename type="middle">K</forename><surname>Panda</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2009 IEEE International Conference on Cluster Computing and Workshops</title>
				<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2009">2009</date>
			<biblScope unit="page" from="1" to="9" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Minimizing the hidden cost of rdma</title>
		<author>
			<persName><forename type="first">Philip</forename><surname>Werner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Frey</forename></persName>
		</author>
		<author>
			<persName><forename type="first">Gustavo</forename><surname>Alonso</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">29th IEEE International Conference on Distributed Computing Systems</title>
				<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2009">2009. 2009</date>
			<biblScope unit="page" from="553" to="560" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title level="m" type="main">Persistent Memory Developer Kit Version v1</title>
		<author>
			<persName><surname>Intel</surname></persName>
		</author>
		<idno>11.0</idno>
		<ptr target="https://pmem.io/" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<author>
			<persName><surname>Intel</surname></persName>
		</author>
		<ptr target="https://pmem.io/documents/NVDIMM_Namespace_Spec.pdf" />
		<title level="m">NVDIMM Namespace Specification</title>
				<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<title level="m" type="main">Advanced Configuration and Power Interface (ACPI) Specification Version 6</title>
		<ptr target="https://uefi.org/specs/ACPI/6.4/" />
		<imprint>
			<date type="published" when="2021">2021</date>
			<publisher>UEFI Forum, Inc</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<title level="m" type="main">The devicetree specification</title>
		<author>
			<persName><surname>Linaro</surname></persName>
		</author>
		<ptr target="https://www.devicetree.org/" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<author>
			<persName><surname>Mellanox</surname></persName>
		</author>
		<author>
			<persName><surname>Mellanox</surname></persName>
		</author>
		<ptr target="https://www.mellanox.com/related-docs/prod_adapter_cards/PB_ConnectX3_VPI_Card_Dell.pdf" />
		<title level="m">ConnectX-3 FDR (56Gbps) Infini-Band VPI</title>
				<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<author>
			<persName><surname>Xilinx</surname></persName>
		</author>
		<author>
			<persName><surname>Mellanox</surname></persName>
		</author>
		<ptr target="https://www.mellanox.com/products/infiniband-drivers/linux/mlnx_ofed" />
		<title level="m">OpenFabrics Enterprise Distribution</title>
				<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Liang Xiong, and Misha Smelyanskiy. Deep learning recommendation model for personalization and recommendation systems</title>
		<author>
			<persName><forename type="first">Maxim</forename><surname>Naumov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dheevatsa</forename><surname>Mudigere</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><surname>Hao-Jun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jianyu</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Narayanan</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jongsoo</forename><surname>Sundaraman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaodong</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Udit</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Carole-Jean</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alisson</forename><forename type="middle">G</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dmytro</forename><surname>Azzolini</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrey</forename><surname>Dzhulgakov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ilia</forename><surname>Mallevich</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yinghai</forename><surname>Cherniavskii</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Raghuraman</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ansha</forename><surname>Krishnamoorthi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Volodymyr</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stephanie</forename><surname>Kondratenko</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xianjie</forename><surname>Pereira</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wenlin</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vijay</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><surname>Rao</surname></persName>
		</author>
		<idno>CoRR, abs/1906.00091</idno>
	</analytic>
	<monogr>
		<title level="j">Bill Jia</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">A simple parallel algorithm for the maximal independent set problem</title>
		<author>
			<persName><forename type="first">Michael</forename><surname>Luby</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">SIAM journal on computing</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="1036" to="1053" />
			<date type="published" when="1986">1986</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Breadth-first search</title>
		<author>
			<persName><forename type="first">Alan</forename><surname>Bundy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lincoln</forename><surname>Wallen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Catalogue of artificial intelligence tools</title>
				<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="1984">1984</date>
			<biblScope unit="page" from="13" to="13" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Connected components in random graphs with given expected degree sequences</title>
		<author>
			<persName><forename type="first">Fan</forename><surname>Chung</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Linyuan</forename><surname>Lu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Annals of combinatorics</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="125" to="145" />
			<date type="published" when="2002">2002</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">A faster algorithm for betweenness centrality</title>
		<author>
			<persName><forename type="first">Ulrik</forename><surname>Brandes</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of mathematical sociology</title>
		<imprint>
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="163" to="177" />
			<date type="published" when="2001">2001</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Ligra: a lightweight graph processing framework for shared memory</title>
		<author>
			<persName><forename type="first">Julian</forename><surname>Shun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Guy</forename><forename type="middle">E</forename><surname>Blelloch</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 18th ACM SIGPLAN symposium on Principles and practice of parallel programming</title>
				<meeting>the 18th ACM SIGPLAN symposium on Principles and practice of parallel programming</meeting>
		<imprint>
			<date type="published" when="2013">2013</date>
			<biblScope unit="page" from="135" to="146" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
