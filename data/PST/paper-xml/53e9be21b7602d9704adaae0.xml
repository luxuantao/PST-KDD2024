<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Surfels: Surface Elements as Rendering Primitives</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Hanspeter</forename><surname>Pfister</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">MERL</orgName>
								<address>
									<settlement>Cambridge</settlement>
									<region>MA</region>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Matthias</forename><surname>Zwicker</surname></persName>
							<email>[zwicker@inf.ethz.ch</email>
							<affiliation key="aff0">
								<orgName type="institution">MERL</orgName>
								<address>
									<settlement>Cambridge</settlement>
									<region>MA</region>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Jeroen</forename><surname>Van Baar</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">MERL</orgName>
								<address>
									<settlement>Cambridge</settlement>
									<region>MA</region>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Markus</forename><surname>Gross</surname></persName>
							<email>gross]@inf.ethz.ch</email>
							<affiliation key="aff0">
								<orgName type="institution">MERL</orgName>
								<address>
									<settlement>Cambridge</settlement>
									<region>MA</region>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Surfels: Surface Elements as Rendering Primitives</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">E5E5BC0D7237E14CAE8D38016293CEE7</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.3" ident="GROBID" when="2023-07-28T13:45+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Surfel rendering examples CR Categories: I</term>
					<term>3</term>
					<term>3 [Computer Graphics]: Picture/Image Generation -Viewing Algorithms; I</term>
					<term>3</term>
					<term>6 [Computer Graphics]: Methodology and Techniques -Graphics Data Structures and Data Types Rendering Systems, Texture Mapping</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Surface elements (surfels) are a powerful paradigm to efficiently render complex geometric objects at interactive frame rates. Unlike classical surface discretizations, i.e., triangles or quadrilateral meshes, surfels are point primitives without explicit connectivity. Surfel attributes comprise depth, texture color, normal, and others. As a pre-process, an octree-based surfel representation of a geometric object is computed. During sampling, surfel positions and normals are optionally perturbed, and different levels of texture colors are prefiltered and stored per surfel. During rendering, a hierarchical forward warping algorithm projects surfels to a z-buffer. A novel method called visibility splatting determines visible surfels and holes in the z-buffer. Visible surfels are shaded using texture filtering, Phong illumination, and environment mapping using per-surfel normals. Several methods of image reconstruction, including supersampling, offer flexible speed-quality tradeoffs. Due to the simplicity of the operations, the surfel rendering pipeline is amenable for hardware implementation. Surfel objects offer complex shape, low rendering cost and high image quality, which makes them specifically suited for low-cost, real-time graphics, such as games.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>3D computer graphics has finally become ubiquitous at the consumer level. There is a proliferation of affordable 3D graphics hardware accelerators, from high-end PC workstations to low-priced gamestations. Undoubtedly, key to this success is interactive computer games that have emerged as the "killer application" for 3D graphics. However, interactive computer graphics has still not reached the level of realism that allows a true immersion into a virtual world. For example, typical foreground characters in realtime games are extremely minimalistic polygon models that often exhibit faceting artifacts, such as angular silhouettes.</p><p>Various sophisticated modeling techniques, such as implicit surfaces, NURBS, or subdivision surfaces, allow the creation of 3D graphics models with increasingly complex shapes. Higher order modeling primitives, however, are eventually decomposed into triangles before being rendered by the graphics subsystem. The triangle as a rendering primitive seems to meet the right balance between descriptive power and computational burden <ref type="bibr" target="#b6">[7]</ref>. To render realistic, organic-looking models requires highly complex shapes with ever more triangles, or, as Alvy Ray Smith puts it: "Reality is 80 million polygons" <ref type="bibr" target="#b25">[26]</ref>. Processing many small triangles leads to bandwidth bottlenecks and excessive floating point and rasterization requirements <ref type="bibr" target="#b6">[7]</ref>.</p><p>To increase the apparent visual complexity of objects, texture mapping was introduced by Catmull <ref type="bibr" target="#b2">[3]</ref> and successfully applied by others <ref type="bibr" target="#b12">[13]</ref>. Textures convey more detail inside a polygon, thereby allowing larger and fewer triangles to be used. Today's graphics engines are highly tailored for high texture mapping performance. However, texture maps have to follow the underlying geometry of the polygon model and work best on flat or slightly curved surfaces. Realistic surfaces frequently require a large number of textures that have to be applied in multiple passes during rasterization. And phenomena such as smoke, fire, or water are difficult to render using textured triangles.</p><p>In this paper we propose a new method of rendering objects with rich shapes and textures at interactive frame rates. Our rendering architecture is based on simple surface elements (surfels) as rendering primitives. Surfels are point samples of a graphics model. In a preprocessing step, we sample the surfaces of complex geometric models along three orthographic views. At the same time, we perform computation-intensive calculations such as texture, bump, or displacement mapping. By moving rasterization and texturing from the core rendering pipeline to the preprocessing step, we dramatically reduce the rendering cost.</p><p>From a modeling point of view, the surfel representation provides a mere discretization of the geometry and hence reduces the object representation to the essentials needed for rendering. By contrast, triangle primitives implicitly store connectivity information, such as vertex valence or adjacency -data not necessarily available or needed for rendering. In a sense, a surfel relates to what Levoy and Whitted call the lingua franca of rendering in their pioneering report from 1985 <ref type="bibr" target="#b17">[18]</ref>.</p><p>Storing normals, prefiltered textures, and other per surfel data enables us to build high quality rendering algorithms. Shading and transformations applied per surfel result in Phong illumination, bump, and displacement mapping, as well as other advanced rendering features. Our data structure provides a multiresolution object representation, and a hierarchical forward warping algorithm allows us to estimate the surfel density in the output image for speed-quality tradeoffs.</p><p>The surfel rendering pipeline complements the existing graphics pipeline and does not intend to replace it. It is positioned between conventional geometry-based approaches and image-based rendering and trades memory overhead for rendering performance and quality. The focus of this work has been interactive 3D applications, not high-end applications such as feature films or CAD/CAM. Surfels are not well suited to represent flat surfaces, such as walls or scene backgrounds, where large, textured polygons provide better image quality at lower rendering cost. However, surfels work well for models with rich, organic shapes or high surface details and for applications where preprocessing is not an issue. These qualities make them ideal for interactive games.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Work</head><p>The use of points as rendering primitives has a long history in computer graphics. As far back as 1974, Catmull <ref type="bibr" target="#b2">[3]</ref> observed that geometric subdivision may ultimately lead to points. Particles were subsequently used for objects that could not be rendered with geometry, such as clouds, explosions, and fire <ref type="bibr" target="#b22">[23]</ref>. More recently, image-based rendering has become popular because its rendering time is proportional to the number of pixels in the source and output images and not the scene complexity.</p><p>Visually complex objects have been represented by dynamically generated image sprites <ref type="bibr" target="#b24">[25]</ref>, which are quick to draw and largely retain the visual characteristics of the object. A similar approach was used in the Talisman rendering system <ref type="bibr" target="#b26">[27]</ref> to maintain high and approximately constant frame rates. However, mapping objects onto planar polygons leads to visibility errors and does not allow for parallax and disocclusion effects. To address these problems, several methods add per-pixel depth information to images, variously called layered impostors <ref type="bibr" target="#b23">[24]</ref>, sprites with depth, or layered depth images <ref type="bibr" target="#b24">[25]</ref>, just to name a few. Still, none of these techniques provide a complete object model that can be illuminated and rendered from arbitrary points of view.</p><p>Some image-based approaches represent objects without explicitly storing any geometry or depth. Methods such as view interpolation and Quicktime VR <ref type="bibr" target="#b4">[5]</ref> or plenoptic modeling <ref type="bibr" target="#b20">[21]</ref> create new views from a collection of 2D images. Lightfield <ref type="bibr" target="#b16">[17]</ref> or lumigraph <ref type="bibr" target="#b8">[9]</ref> techniques describe the radiance of a scene or object as a function of position and direction in a four-or higher-dimensional space, but at the price of considerable storage overhead. All these methods use view-dependent samples to represent an object or scene. However, view-dependent samples are ineffective for dynamic scenes with motion of objects, changes in material properties, and changes in position and intensities of light sources.</p><p>The main idea of representing objects with surfels is to describe them in a view-independent, object-centered rather than imagecentered fashion. As such, surfel rendering is positioned between geometry rendering and image-based rendering. In volume graphics <ref type="bibr" target="#b15">[16]</ref>, synthetic objects are implicitly represented with surface voxels, typically stored on a regular grid. However, the extra third dimension of volumes comes at the price of higher storage requirements and longer rendering times. In <ref type="bibr" target="#b7">[8]</ref>, Perlin studies "surflets," a flavor of wavelets that can be used to describe free-form implicit surfaces. Surflets have less storage overhead than volumes, but rendering them requires lengthy ray casting.</p><p>Our research was inspired by the following work: Animatek's Caviar player <ref type="bibr" target="#b0">[1]</ref> provides interactive frame rates for surface voxel models on a Pentium class PC, but uses simplistic projection and illumination methods. Levoy and Whitted <ref type="bibr" target="#b17">[18]</ref> use points to model objects for the special case of continuous, differentiable surfaces. They address the problem of texture filtering in detail. Max uses point samples obtained from orthographic views to model and render trees <ref type="bibr" target="#b19">[20]</ref>. Dally et al. <ref type="bibr" target="#b5">[6]</ref> introduced the delta tree as an objectcentered approach to image-based rendering. The movement of the viewpoint in their method, however, is still confined to particular locations. More recently, Grossman and Dally <ref type="bibr" target="#b11">[12]</ref> describe a point sample representation for fast rendering of complex objects. Chang et al. <ref type="bibr" target="#b3">[4]</ref> presented the LDI tree, a hierarchical space-partitioning data structure for image-based rendering.</p><p>We extend and integrate these ideas and present a complete point sample rendering system comprising an efficient hierarchical representation, high quality texture filtering, accurate visibility calculations, and image reconstruction with flexible speed-quality tradeoffs. Our surfel rendering pipeline provides high quality rendering of exceedingly complex models and is amenable for hardware implementation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Conceptual Overview</head><p>Similar to the method proposed by Levoy and Whitted <ref type="bibr" target="#b17">[18]</ref>, our surfel approach consists of two main steps: sampling and surfel rendering. Sampling of geometry and texture is done during preprocessing, which may include other view-independent methods such as bump and displacement mapping. Figure <ref type="figure" target="#fig_0">2</ref> gives a conceptual overview of the algorithm.</p><p>The sampling process (Section 5) converts geometric objects and their textures to surfels. We use ray casting to create three orthogonal layered depth images (LDIs) <ref type="bibr" target="#b24">[25]</ref>. The LDIs store multiple surfels along each ray, one for each ray-surface intersection point. Lischinski and Rappaport <ref type="bibr" target="#b18">[19]</ref> call this arrangement of three orthogonal LDIs a layered depth cube (LDC). An important and novel aspect of our sampling method is the distinction between sampling of shape, or geometry, and shade, or texture color. A surfel stores both shape, such as surface position and orientation, and shade, such as multiple levels of prefiltered texture colors. Because of the similarities to traditional texture mipmaps we call this hierarchical color information a surfel mipmap.</p><p>From the LDC we create an efficient hierarchical data structure for rendering. Chang et al. <ref type="bibr" target="#b3">[4]</ref> introduce the LDI tree, an octree with an LDI attached to each octree node. We use the same hierarchical space-partitioning structure, but store an LDC at each node of the octree (Section 6). Each LDC node in the octree is called a block. We call the resulting data structure the LDC tree. In a step called 3-to-1 reduction we optionally reduce the LDCs to single LDIs on a block-by-block basis for faster rendering.</p><p>The rendering pipeline (Section 7) hierarchically projects blocks to screen space using perspective projection. The rendering is accelerated by block culling <ref type="bibr" target="#b11">[12]</ref> and fast incremental forward warping. We estimate the projected surfel density in the output image to control rendering speed and quality of the image reconstruction. A conventional z-buffer together with a novel method called visibility splatting solves the visibility problem. Texture colors of visible surfels are filtered using linear interpolation between appropriate levels of the surfel mipmap. Each visible surfel is shaded using, for example, Phong illumination and reflection mapping. The final stage performs image reconstruction from visible surfels, including hole filling and antialiasing. In general, the resolution of the output image and the resolution of the z-buffer do not have to be the same.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Definition of a Surfel</head><p>We found the term surfel as an abbreviation for surface element or surface voxel in the volume rendering and discrete topology literature. Herman <ref type="bibr" target="#b14">[15]</ref> defines a surfel as an oriented ´Ò ½µ- dimensional object in Ê Ò . For Ò ¿ , this corresponds to an oriented unit square (voxel face) and is consistent with thinking of voxels as little cubes. However, for our discussion we find it more useful to define surfels as follows:</p><p>A surfel is a zero-dimensional n-tuple with shape and shade attributes that locally approximate an object surface.</p><p>We consider the alternative term, point sample, to be too general, since voxels and pixels are point samples as well.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Sampling</head><p>The goal during sampling is to find an optimal surfel representation of the geometry with minimum redundancy. Most sampling methods perform object discretization as a function of geometric parameters of the surface, such as curvature or silhouettes. This object space discretization typically leads to too many or too few primitives for rendering. In a surfel representation, object sampling is aligned to image space and matches the expected output resolution of the image.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">LDC Sampling</head><p>We sample geometric models from three sides of a cube into three orthogonal LDIs, called a layered depth cube (LDC) <ref type="bibr" target="#b18">[19]</ref> or block.</p><p>Figure <ref type="figure">3</ref> shows an LDC and two LDIs using a 2D drawing. Ray casting records all intersections, including intersections with backfacing surfaces. At each intersection point, a surfel is created with floating point depth and other shape and shade properties. Perturbation of the surface normal or of the geometry for bump and displacement mapping can be performed on the geometry before sampling or during ray casting using procedural shaders. Alternatively, we could sample an object from predetermined directions on a surrounding convex hull using orthographic depth images <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b11">12]</ref>. However, combining multiple reference images and eliminating the redundant information is a difficult problem <ref type="bibr" target="#b20">[21]</ref>, and sampling geometry with reference images works best for smooth and convex objects. In addition, LDC sampling allows us to easily build a hierarchical data structure, which would be difficult to do from dozens of depth images.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Adequate Sampling Resolution</head><p>Given a pixel spacing of ¼ for the full resolution LDC used for sampling, we can determine the resulting sampling density on the surface. Suppose we construct a Delaunay triangulation on the object surface using the generated surfels as triangle vertices. As was observed in <ref type="bibr" target="#b18">[19]</ref>, the imaginary triangle mesh generated by this sampling process has a maximum sidelength ×Ñ Ü of Ô ¿ ¼. The minimum sidelength ×Ñ Ò is 0 when two or three sampling rays intersect at the same surface position.</p><p>Similarly to <ref type="bibr" target="#b11">[12]</ref>, we call the object adequately sampled if we can guarantee that at least one surfel is projected into the support of each ouptut pixel filter for orthographic projection and unit magnification. That condition is met if ×Ñ Ü, the maximum distance between adjacent surfels in object space, is less than the radius Ö ¼ Ö of the desired pixel reconstruction filter. Typically, we choose the LDI resolution to be slightly higher than this because of the effects of magnification and perspective projection. We will revisit these observations when estimating the number of projected surfels per pixel in Section 7.2.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3">Texture Prefiltering</head><p>A feature of surfel rendering is that textures are prefiltered and mapped to object space during preprocessing. We use viewindependent texture filtering as in <ref type="bibr" target="#b11">[12]</ref>. To prevent view-dependent texture aliasing we also apply per-surfel texture filtering during rendering (see Sections 7.4 and 7.6).</p><p>To determine the extent of the filter footprint in texture space, we center a circle at each surfel on its tangent plane, as shown in Figure <ref type="figure" target="#fig_2">4a</ref>. We call these circles tangent disks. The tangent disks are mapped to ellipses in texture space (see Figure <ref type="figure" target="#fig_2">4b</ref>) using the predefined texture parameterization of the surface. An EWA filter <ref type="bibr" target="#b13">[14]</ref> is applied to filter the texture and the resulting color is assigned to the surfel. To enable adequate texture reconstruction, the elliptical filter footprints in texture space must overlap each other. Consequently, we choose Ö ¼ ÔÖ ×Ñ Ü, the maximum distance between adjacent surfels in object space, as the radius for the tangent disks. This usually guarantees that the tangent disks intersect each other in object space and that their projections in texture space overlap.</p><p>Grossman and Dally <ref type="bibr" target="#b11">[12]</ref> also use view-independent texture filtering and store one texture sample per surfel. Since we use a modified z-buffer algorithm to resolve visibility (Section 7.3), not all surfels may be available for image reconstruction, which leads to texture aliasing artifacts. Consequently, we store several (typically three or four) prefiltered texture samples per surfel. Tangent disks with dyadically larger radii Ö ÔÖ ×Ñ Ü¾ are mapped to texture space and used to compute the prefiltered colors. Because of its similarity to mipmapping <ref type="bibr" target="#b12">[13]</ref>, we call this a surfel mipmap. Figure <ref type="figure" target="#fig_2">4b</ref> shows the elliptical footprints in texture space of consecutively larger tangent disks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Data Structure</head><p>We use the LDC tree, an efficient hierarchical data structure, to store the LDCs acquired during sampling. It allows us to quickly estimate the number of projected surfels per pixel and to trade rendering speed for higher image quality.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.1">The LDC Tree</head><p>Chang et al. <ref type="bibr" target="#b3">[4]</ref> use several reference depth images of a scene to construct the LDI tree. The depth image pixels are resampled onto multiple LDI tree nodes using splatting <ref type="bibr" target="#b28">[29]</ref>. We avoid these interpolation steps by storing LDCs at each node in the octree that are subsampled versions of the highest resolution LDC.</p><p>The octree is recursively constructed bottom up, and its height is selected by the user. The highest resolution LDC -acquired during geometry sampling -is stored at the lowest level Ò ¼ . If the highest resolution LDC has a pixel spacing of ¼, then the LDC at level Ò has a pixel spacing of Ò ¼¾ Ò . The LDC is subdivided into blocks with user-specified dimension , i.e., the LDIs in a block have ¾ layered depth pixels. is the same for all levels of the tree. and empty blocks are white. Blocks on higher levels of the octree are constructed by subsampling their children by a factor of two. Figure <ref type="figure" target="#fig_3">5b</ref> shows level Ò ½ of the LDC tree. Note that surfels at higher levels of the octree reference surfels in the LDC of level 0, i.e., surfels that appear in several blocks of the hierarchy are stored only once and shared between blocks.</p><p>Empty blocks (shown as white squares in the figure) are not stored. Consequently, the block dimension is not related to the dimension of the highest resolution LDC and can be selected arbitrarily. Choosing ½ makes the LDC tree a fully volumetric octree representation. For a comparison between LDCs and volumes see <ref type="bibr" target="#b18">[19]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.2">3-to-1 Reduction</head><p>To reduce storage and rendering time it is often useful to optionally reduce the LDCs to one LDI on a block-by-block basis. Because this typically corresponds to a three-fold increase in warping speed, we call this step 3-to-1 reduction. First, surfels are resampled to integer grid locations of ray intersections as shown in Figure <ref type="figure" target="#fig_4">6</ref>. Currently we use nearest neighbor interpolation, although a more resampled surfels on grid locations LDI 1 surfels LDI 2 surfels sophisticated filter, e.g., splatting as in <ref type="bibr" target="#b3">[4]</ref>, could easily be implemented. The resampled surfels of the block are then stored in a single LDI.</p><p>The reduction and resampling process degrades the quality of the surfel representation, both for shape and for shade. Resampled surfels from the same surface may have very different texture colors and normals. This may cause color and shading artifacts that are worsened during object motion. In practice, however, we did not encounter severe artifacts due to 3-to-1 reduction. Because our rendering pipeline handles LDCs and LDIs the same way, we could store blocks with thin structures as LDCs, while all other blocks could be reduced to single LDIs.</p><p>As in Section 5.2, we can determine bounds on the surfel density on the surface after 3-to-1 reduction. Given a sampling LDI with pixel spacing ¼, the maximum distance between adjacent surfels on the object surface is ×Ñ Ü Ô ¿ ¼, as in the original LDC tree.</p><p>The minimum distance between surfels increases to ×Ñ Ò ¼ due to the elimination of redundant surfels, making the imaginary Delaunay triangulation on the surface more uniform.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">The Rendering Pipeline</head><p>The rendering pipeline takes the surfel LDC tree and renders it using hierarchical visibility culling and forward warping of blocks. Hierarchical rendering also allows us to estimate the number of projected surfels per output pixel. For maximum rendering efficiency, we project approximately one surfel per pixel and use the same resolution for the z-buffer as in the output image. For maximum image quality, we project multiple surfels per pixel, use a finer resolution of the z-buffer, and high quality image reconstruction.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.1">Block Culling</head><p>We traverse the LDC tree from top (the lowest resolution blocks) to bottom (the highest resolution blocks). For each block, we first perform view-frustum culling using the block bounding box. Next, we use visibility cones, as described in <ref type="bibr" target="#b10">[11]</ref>, to perform the equivalent of backface culling of blocks. Using the surfel normals, we precompute a visibility cone per block, which gives a fast, conservative visibility test: no surfel in the block is visible from any viewpoint within the cone. In contrast to <ref type="bibr" target="#b10">[11]</ref>, we perform all visibility tests hierarchically in the LDC tree, which makes them more efficient.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.2">Block Warping</head><p>During rendering, the LDC tree is traversed top to bottom <ref type="bibr" target="#b3">[4]</ref>. To choose the octree level to be projected, we conservatively estimate for each block the number of surfels per pixel. We can choose one surfel per pixel for fast rendering or multiple surfels per pixel for supersampling. For each block at tree level Ò, the number of surfels per pixel is determined by Ò Ñ Ü , the maximum distance between adjacent surfels in image space. We estimate Ò Ñ Ü by dividing the maximum length of the projected four major diagonals of the block bounding box by the block dimension . This is correct for orthographic projection. However, the error introduced by using perspective projection is small because a block typically projects to a small number of pixels.</p><p>For each block, Ò Ñ Ü is compared to the radius Ö ¼ Ö of the desired pixel reconstruction filter. Ö ¼ Ö is typically</p><formula xml:id="formula_0">Ô ¾ ¾ ×Ó,</formula><p>where ×Ó is the sidelength of an output pixel. If Ò Ñ Ü of the current block is larger than Ö ¼ Ö then its children are traversed. We project the block whose Ò Ñ Ü is smaller than Ö ¼ Ö , rendering approximately one surfel per pixel. Note that the number of surfels per pixel can be increased by requiring that Ò Ñ Ü is a fraction of Ö ¼ Ö . The resulting Ò Ñ Ü is stored as Ñ Ü with each projected surfel for subsequent use in the visibility testing and the image reconstruction stages. The radius of the actual reconstruction filter is ÖÖ Ñ Ü´Ö ¼ Ö Ñ Üµ (see Section 7.6).</p><p>To warp a block to screen space we use the optimized incremental block warping by Grossman and Dally, presented in detail in <ref type="bibr" target="#b10">[11]</ref>. Its high efficiency is achieved due to the regularity of LDCs. It uses only 6 additions, 3 multiplications, and 1 reciprocal per sample. The LDIs in each LDC block are warped independently, which allows us to render an LDC tree where some or all blocks have been reduced to single LDIs after 3-to-1 reduction.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.3">Visibility Testing</head><p>Perspective projection, high z-buffer resolution, and magnification may lead to undersampling or holes in the z-buffer. A z-buffer pixel is a hole if it does not contain a visible surfel or background pixel after projection. Holes have to be marked for image reconstruction. Each pixel of the z-buffer stores a pointer to the closest surfel and the current minimum depth. Surfel depths are projected to the zbuffer using nearest neighbor interpolation.</p><p>To correctly resolve visibility in light of holes, we scan-convert the orthographic projection of the surfel tangent disks into the zbuffer. The tangent disks have a radius of Ö Ò Ø ×Ñ Ü¾ Ò , where ×Ñ Ü is the maximum distance between adjacent surfels in object space and Ò is the level of the block. We call this approach visibility splatting, shown in Figure <ref type="figure" target="#fig_5">7</ref>. Visibility splatting effectively sepa- rates visibility calculations and reconstruction of the image, which produces high quality images and is amenable to hardware implementation <ref type="bibr" target="#b21">[22]</ref>.</p><p>After orthographic projection, the tangent disks form an ellipse around the surfel, as shown in Figure <ref type="figure" target="#fig_5">7b</ref>. We approximate the ellipse with a partially axis-aligned bounding box, shown in red. The bounding box parallelogram can be easily scan-converted, and each z-buffer pixel is filled with the appropriate depth (indicated by the shaded squares in the figure), depending on the surfel normal AE . This scan conversion requires only simple setup calculations, no interpolation of colors, and no perspective divide.</p><p>The direction of the minor axis Ñ Ò of the projected ellipse is parallel to the projection of the surfel normal AE . The major axis Ñ Ü is orthogonal to Ñ Ò. The length of Ñ Ü is the projection of the tangent disk radius Ö Ò Ø , which is approximated by Ñ Ü. This approximation takes the orientation and magnification of the LDC tree during projection into account. Next, we calculate the coordinate axis that is most parallel to Ñ Ò (the y-axis in Figure <ref type="figure" target="#fig_5">7</ref>). The short side of the bounding box is axis aligned with this coordinate axis to simplify scan conversion. Its height is computed by intersecting the ellipse with the coordinate axis. The width Û of the bounding box is determined by projecting the vertex at the intersection of the major axis and the ellipse onto the second axis (the Ü-axis in Figure <ref type="figure" target="#fig_5">7</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Þ Ü and Þ</head><p>Ý are the partial derivatives of the surfel depth Þ with respect to the screen Ü and Ý direction. They are constant because of the orthographic projection and can be calculated from the unit normal AE . During scan conversion, the depth at each pixel inside the bounding box is calculated using Þ Ü and Þ Ý . In addition, we add a small threshold to each projected Þ value. The threshold prevents surfels that lie on the foreground surface to be accidentally discarded. Pixels that have a larger Þ than the Þ values of the splatted tangent disk are marked as holes.</p><p>If the surface is extremely bent, the tangential planes do not cover it completely, potentially leaving tears and holes. In addition, extreme perspective projection makes orthographic projection a bad approximation to the actual projected tangent disk. In practice, however, we did not see this as a major problem. If the projected tangent disk is a circle, i.e., if AE is almost parallel to the viewing direction, the bounding box parallelogram is a bad approximation. In this case, we use a square bounding box instead.</p><p>Using a somewhat related approach, Grossman and Dally <ref type="bibr" target="#b11">[12]</ref> use a hierarchical z-buffer for visibility testing. Each surfel is projected and the hole size around the surfel is estimated. The radius of the hole determines the level of the hierarchical z-buffer where the z-depth of the surfel will be set. This can be regarded as visibility splatting using a hierarchical z-buffer. The advantage is that the visibility splat is performed with a single depth test in the hierarchical z-buffer. However, the visibility splat is always square, essentially representing a tangential disk that is parallel to the image plane. In addition, it is not necessarily centered around the projected surfel. To recover from those drawbacks, <ref type="bibr" target="#b11">[12]</ref> introduces weights indicating coverage of surfels. But this makes the reconstruction process more expensive and does not guarantee complete coverage of hidden surfaces.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.4">Texture Filtering</head><p>As explained in Section 5.3, each surfel in the LDC tree stores several prefiltered texture colors of the surfel mipmap. During rendering, the surfel color is linearly interpolated from the surfel mipmap colors depending on the object minification and surface orientation. Figure <ref type="figure" target="#fig_6">8a</ref> shows all visible surfels of a sampled surface projected to the z-buffer. The ellipses around the centers of the surfels mark the projection of the footprints of the highest resolution texture prefilter (Section 5.3). Note that during prefiltering, we try to guarantee that the footprints cover the surface completely. In figure <ref type="figure" target="#fig_6">8b</ref>  the number of samples per z-buffer pixel is limited to one by applying z-buffer depth tests. In order to fill the gaps appearing in the coverage of the surface with texture footprints, the footprints of the remaining surfels have to be enlarged. If surfels are discarded in a given z-buffer pixel, we can assume that the z-buffer pixels in the 3x3 neighborhood around it are not holes. Thus the gaps can be filled if the texture footprint of each surfel covers at least the area of a z-buffer pixel. Consequently, the ellipse of the projected footprint has to have a minor radius of Ô ¾×Þ in the worst case, where ×Þ is the z-buffer pixel spacing. But we ignore that worst case and use Ô ¾ ¾ ×Þ , implying that surfels are projected to z-buffer pixel centers.</p><p>Figure <ref type="figure" target="#fig_6">8b</ref> shows the scaled texture footprints as ellipses around projected surfels.</p><p>To select the appropriate surfel mipmap level, we use traditional view-dependent texture filtering, as shown in Figure <ref type="figure" target="#fig_7">9</ref> </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>radius</head><p>Ô ¾ ¾ ×Þ is projected through a pixel onto the tangent plane of the surface from the direction of the view, producing an ellipse in the tangent plane. In this calculation, the projection of the circle is approximated with an orthographic projection. Similar to isotropic texture mapping, the major axis of the projected tangent space ellipse is used to determine the surfel mipmap level. The surfel color is computed by linear interpolation between the closest two mipmap levels with prefilter radii Ö ÔÖ and Ö •½ ÔÖ , respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.5">Shading</head><p>The illumination model is usually applied before visibility testing. However, deferred shading after visibility testing avoids unnecessary work. Grossman and Dally <ref type="bibr" target="#b11">[12]</ref> perform shading calculations in object space to avoid transformation of normals to camera space. However, we already transform the normals to camera space during visibility splatting (Section 7.3). With the transformed normals at hand, we use cube reflectance and environment maps <ref type="bibr" target="#b27">[28]</ref> to calculate a per-surfel Phong illumination model. Shading with per-surfel normals results in high quality specular highlights.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.6">Image Reconstruction and Antialiasing</head><p>Reconstructing a continuous surface from projected surfels is fundamentally a scattered data interpolation problem. In contrast to other approaches, such as splatting <ref type="bibr" target="#b28">[29]</ref>, we separate visibility calculations from image reconstruction <ref type="bibr" target="#b21">[22]</ref>. Z-buffer pixels with holes are marked during visibility splatting. These hole pixels are not used during image reconstruction because they do not contain any visible samples. Figure <ref type="figure" target="#fig_8">10</ref> shows the z-buffer after rendering of an object and the image reconstruction process. The simplest and fastest approach, shown in Figure <ref type="figure" target="#fig_8">10a</ref>, is to choose the size of an output pixel ×Ó to be the same as the zbuffer pixel size ×Þ . Surfels are mapped to pixel centers using nearest neighbor interpolation, shown with color squares in the figure. Holes are marked with a black X. Recall that during forward warping each surfel stores Ñ Ü, an estimate of the maximum distance between adjacent projected surfels of a block. Ñ Ü is a good estimate for the minimum radius of a pixel filter that contains at least one surfel. To interpolate the holes, we use a radially symmetric Gauss filter with a radius ÖÖ slightly larger than Ñ Ü positioned at hole pixel centers. Alternatively, to fill the holes we implemented the pull-push algorithm used by Grossman and Dally <ref type="bibr" target="#b11">[12]</ref> and described by Gortler et al. <ref type="bibr" target="#b8">[9]</ref>.</p><p>A high quality alternative is to use supersampling, shown in Figure <ref type="figure" target="#fig_8">10b</ref>. The output image pixel size ×Ó is any multiple of the zbuffer pixel size ×Þ . Dotted lines in the figure indicate image-buffer subpixels. Rendering for supersampling proceeds exactly the same as before. During image reconstruction we put a Gaussian filter at the centers of all output pixels to filter the subpixel colors. The radius of the filter is ÖÖ Ñ Ü´Ö ¼ Ö Ñ Üµ. Thus ÖÖ is at least as large as Ö ¼ Ö Ô ¾ ¾ ×Ó, but it can be increased if Ñ Ü indicates a low density of surfels in the output image.</p><p>It is instructive to see how the color of an output pixel is determined for regular rendering and for supersampling in the absence of holes. For regular rendering, the pixel color is found by nearest neighbor interpolation from the closest visible surfel in the z-buffer. The color of that surfel is computed by linear interpolation between two surfel mipmap levels. Thus the output pixel color is calculated from two prefiltered texture samples. In the case of supersampling, one output pixel contains the filtered colors of one surfel per zbuffer subpixel. Thus, up to eight prefiltered texture samples may contribute to an output pixel for ¾¢¾ supersampling. This produces image quality similar to trilinear mipmapping.</p><p>Levoy and Whitted <ref type="bibr" target="#b17">[18]</ref> and Chang et al. <ref type="bibr" target="#b3">[4]</ref> use an algorithm very similar to Carpenter's A-Buffer <ref type="bibr" target="#b1">[2]</ref> with per-pixel bins and compositing of surfel colors. However, to compute the correct per pixel coverage in the A-buffer requires projecting all visible surfels. Max <ref type="bibr" target="#b19">[20]</ref> uses an output LDI and an A-buffer for high quality anti-aliasing, but he reports rendering times of 5 minutes per frame. Our method with hierarchical density estimation, visibility splatting, and surfel mipmap texture filtering offers more flexible speed-quality tradeoffs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8">Implementation and Results</head><p>We implemented sampling using the Blue Moon Rendering Tools (BMRT) ray tracer <ref type="bibr" target="#b9">[10]</ref>. We use a sampling resolution of ½¾ ¾ for the LDC for ¼ ¾ expected output resolution. At each intersec- tion point, a Renderman shader performs view-independent calculations, such as texture filtering, displacement mapping, and bump mapping, and prints the resulting surfels to a file. Pre-processing for a typical object with 6 LOD surfel mipmaps takes about one hour.</p><p>A fundamental limitation of LDC sampling is that thin structures that are smaller than the sampling grid cannot be correctly represented and rendered. For example, spokes, thin wires, or hair are hard to capture. The rendering artifacts are more pronounced after 3-to-1 reduction because additional surfels are deleted. However, we had no problems sampling geometry as thin as the legs and wings of the wasp shown in Figure <ref type="figure">1</ref> and Figure <ref type="figure" target="#fig_11">12</ref>.</p><p>The surfel attributes acquired during sampling include a surface normal, specular color, shininess, and three texture mipmap levels. Material properties are stored as an index into a table. Our system does currently not support transparency. Instead of storing a normal we store an index to a quantized normal table for reflection and environment map shading <ref type="bibr" target="#b27">[28]</ref>. Table <ref type="table" target="#tab_0">1</ref> shows the minimum storage requirements per surfel. We currently store RGB colors as 32-bit integers for a total of 20 Bytes per surfel.    overhead is due to the octree data structure, mainly because of the pointers from the lower resolution blocks to surfels of the sampled LDC. We currently do not optimize or compress the LDC tree.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Data</head><p>Figure <ref type="figure">1</ref> shows different renderings of surfel objects, including environment mapping and displacement mapping. Figure <ref type="figure" target="#fig_11">12</ref> shows an example of hole detection and image reconstruction. Visibility splatting performs remarkably well in detecting holes. However, holes start to appear in the output image for extreme closeups when there are less than approximately one surfel per 30 square pixels. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>b) Hole detection (hole pixels in green). c) Image reconstruction with a Gaussian filter.</head><p>To compare image quality of different reconstruction filters, we rendered the surfel checker plane shown in Figure <ref type="figure" target="#fig_9">11</ref>. There is an increasing number of surfels per pixel towards the top of the image, while holes appear towards the bottom for nearest neighbor reconstruction. However, a checker plane also demonstrates limitations of the surfel representation. Because textures are applied during sampling, periodic texture patterns are stored explicitly with the object instead of by reference. In addition, flat surfaces are much more efficiently rendered using image space rasterization, where attributes can be interpolated across triangle spans.</p><p>Table <ref type="table" target="#tab_2">3</ref> shows rendering performance broken down into percentages per major rendering tasks. The frame rates were measured on a 700 MHz Pentium III system with 256 MB of SDRAM using an unoptimized C version of our program. All performance numbers are averaged over one minute of an animation that arbitrarily rotates the object centered at the origin. The animation was run at three different image resolutions to measure the effects of magnification and holes. Similar to image-based rendering, the performance drops almost linearly with increasing output resolution. For ¾ ¾ or object mini- fication, the rendering is dominated by warping, especially for objects with many surfels. For ½¼¾ ¾ , or large object magnification, visibility splatting and reconstruction dominate due to the increasing number of surface holes. The performance difference between a full LDC tree (Wasp 3LDI) and a reduced LDC tree (Wasp 3to1) is mainly in the warping stage because fewer surfels have to be projected. Performance decreases linearly with supersampling, as shown for 2x2 supersampling at ¼ ¾ resolution (Wasp SS). The same object at ½¼¾ ¾ output resolution with no supersampling per- forms almost identically, except for slower image reconstruction due to the increased number of hole pixels.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Data</head><p>To compare our performance to standard polygon rendering, we rendered the wasp with 128k polygons and 2.3 MB for nine textures using a software-only Windows NT OpenGL viewing program. We used GL LINEAR MIPMAP NEAREST for texture filtering to achieve similar quality as with our renderer. The average performance was 3 fps using the Microsoft OpenGL implementation (opengl32.lib) and 1.7 fps using Mesa OpenGL. Our unoptimized surfel renderer achieves 2.7 fps for the same model, which compares favorably with Mesa OpenGL. We believe that further optimization will greatly improve our performance.</p><p>Choosing the block size for the LDC tree nodes has an influence on block culling and warping performance. We found that a block size of ½ is optimal for a wide range of objects. However, the frame rates remain practically the same for different choices of due to the fact that warping accounts for only a fraction of the overall rendering time.</p><p>Because we use a z-buffer we can render overlapping surfel objects and integrate them with traditional polygon graphics, such as OpenGL. However, the current system supports only rigid body animations. Deformable objects are difficult to represent with surfels and the current LDC tree data structure. In addition, if the surfels do not approximate the object surface well, for example after 3-to-1 reduction or in areas of high curvature, some surface holes may appear during rendering.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="9">Future Extensions</head><p>A major strength of surfel rendering is that in principal we can convert any kind of synthetic or scanned object to surfels. We would like to extend our sampling approach to include volume data, point clouds, and LDIs of non-synthetic objects. We believe that substantial compression of the LDC tree can be achieved using run length encoding or wavelet-based compression techniques. The performance of our software renderer can be substantially improved by using Pentium III SSE instructions. Using an occlusion compatible traversal of the LDC tree <ref type="bibr" target="#b20">[21]</ref>, one could implement orderindependent transparency and true volume rendering.</p><p>Our major goal is the design of a hardware architecture for surfel rendering. Block warping is very simple, involving only two conditionals for z-buffer tests <ref type="bibr" target="#b10">[11]</ref>. There are no clipping calculations. All framebuffer operations, such as visibility splatting and image reconstruction, can be implemented using standard rasterization and framebuffer techniques. The rendering pipeline uses no inverse calculations, such as looking up textures from texture maps, and runtime texture filtering is very simple. There is a high degree of data locality because the system loads shape and shade simultaneously and we expect high cache performance. It is also possible to enhance an existing OpenGL rendering pipeline to efficiently support surfel rendering.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="10">Conclusions</head><p>Surfel rendering is ideal for models with very high shape and shade complexity. As we move rasterization and texturing from the core rendering pipeline to the preprocessing step, the rendering cost per pixel is dramatically reduced. Rendering performance is essentially determined by warping, shading, and image reconstruction -operations that can easily exploit vectorization, parallelism, and pipelining.</p><p>Our surfel rendering pipeline offers several speed-quality tradeoffs. By decoupling image reconstruction and texture filtering we achieve much higher image quality than comparable point sample approaches. We introduce visibility splatting, which is very effective at detecting holes and increases image reconstruction performance. Antialiasing with supersampling is naturally integrated in our system. Our results demonstrate that surfel rendering is capable of high image quality at interactive frame rates. Increasing processor performance and possible hardware support will bring it into the realm of real-time performance.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: Algorithm overview: a) Preprocessing. b) Rendering of the hierarchical LDC tree.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>2 Figure 3 :</head><label>23</label><figDesc>Figure 3: Layered depth cube sampling (shown in 2D).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 4 :</head><label>4</label><figDesc>Figure 4: Texture prefiltering with tangent disks.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>FigureFigure 5 :</head><label>5</label><figDesc>Figure5ashows two levels of an LDC tree with using a 2D drawing. In the figure, neighboring blocks are differently shaded,</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 6 :</head><label>6</label><figDesc>Figure 6: 3-to-1 reduction example.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 7 :</head><label>7</label><figDesc>Figure 7: Visibility splatting.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 8 :</head><label>8</label><figDesc>Figure 8: Projected surfel mipmaps.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure 9 :</head><label>9</label><figDesc>Figure8bshows the scaled texture footprints as ellipses around projected surfels.To select the appropriate surfel mipmap level, we use traditional view-dependent texture filtering, as shown in Figure9. A circle with Object Tangent Space Image Space</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Figure 10 :</head><label>10</label><figDesc>Figure 10: Image reconstruction.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>Figure 11 :</head><label>11</label><figDesc>Figure 11: Tilted checker plane. Reconstruction filter: a) Nearest neighbor. b) Gaussian filter. c) Supersampling.</figDesc><graphic coords="7,55.44,50.00,166.30,166.30" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head></head><label></label><figDesc>before and after 3-to-1 reduction. All models use three LODs and three surfel mipmap levels. The size of the LDC tree is about a factor of 1.3 larger than the LDC acquired during sampling. This 15 MB 204 k / 8 MB Cab 155 k 744 k / 28 MB 539 k / 20 MB Table 2: Geometric model sizes and storage requirements (# surfels / file size) for full and 3-to-1 reduced LDC trees.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_11"><head>Figure 12 :</head><label>12</label><figDesc>Figure 12: Hole detection and image reconstruction. a) Surfel object with holes. b) Hole detection (hole pixels in green). c) Image reconstruction with a Gaussian filter.</figDesc><graphic coords="7,59.76,454.70,75.40,75.40" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 :</head><label>1</label><figDesc>Typical storage requirements per surfel.</figDesc><table><row><cell>Storage</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2</head><label>2</label><figDesc>lists the surfel objects that we used for performance analysis with their geometric model size, number of surfels, and file size</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 3 :</head><label>3</label><figDesc>Rendering</figDesc><table><row><cell></cell><cell cols="6">WRP VIS Output image: ¾ ¢ ¾ SHD REC CLR fps</cell></row><row><cell cols="2">Salamander 39%</cell><cell>3%</cell><cell>28%</cell><cell>17%</cell><cell>13%</cell><cell>11.2</cell></row><row><cell>Wasp</cell><cell>61%</cell><cell>4%</cell><cell>21%</cell><cell>8%</cell><cell>8%</cell><cell>6.0</cell></row><row><cell>Cab</cell><cell cols="4">91% Output image: ¼ ¢ ¼ 2% 5% 1%</cell><cell>1%</cell><cell>2.5</cell></row><row><cell cols="2">Salamander 14%</cell><cell cols="2">18% 31%</cell><cell>22%</cell><cell>16%</cell><cell>4.6</cell></row><row><cell>Wasp 3to1</cell><cell>29%</cell><cell cols="2">17% 29%</cell><cell>15%</cell><cell>9%</cell><cell>2.7</cell></row><row><cell cols="2">Wasp 3LDI 48%</cell><cell cols="2">13% 22%</cell><cell>11%</cell><cell>6%</cell><cell>2.0</cell></row><row><cell>Wasp SS</cell><cell>15%</cell><cell cols="2">22% 28%</cell><cell>18%</cell><cell>16%</cell><cell>1.3</cell></row><row><cell>Cab</cell><cell cols="4">74% Output image: ½¼¾ ¢ ½¼¾ 7% 11% 5%</cell><cell>3%</cell><cell>1.4</cell></row><row><cell cols="2">Salamander 5%</cell><cell cols="2">14% 26%</cell><cell>32%</cell><cell>23%</cell><cell>1.3</cell></row><row><cell>Wasp</cell><cell>13%</cell><cell cols="2">19% 25%</cell><cell>26%</cell><cell>17%</cell><cell>1.0</cell></row><row><cell>Cab</cell><cell>16%</cell><cell cols="2">36% 24%</cell><cell>16%</cell><cell>8%</cell><cell>0.6</cell></row></table><note><p>times with breakdown for warping (WRP), visibility splatting (VIS), Phong shading (SHD), image reconstruction (REC), and framebuffer clear (CLR). Reconstruction with pull-push filter. All models, except Wasp 3LDI, are 3-to-1 reduced. Wasp SS indicates 2x2 supersampling.</p></note></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="11">Acknowledgments</head><p>We would like to thank Ron Perry and Ray Jones for many helpful discussions, Collin Oosterbaan and Frits Post for their contributions to an earlier version of the system, and Adam Moravanszky and Simon Schirm for developing a surfel demo application. Thanks also to Matt Brown, Mark Callahan, and Klaus Müller for contributing code, and to Larry Gritz for his help with BMRT <ref type="bibr" target="#b9">[10]</ref>. Finally, thanks to Alyn Rockwood, Sarah Frisken, and the reviewers for their constructive comments, and to Jennifer Roderick for proofreading the paper.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">Caviar Technology</title>
		<author>
			<persName><surname>Animatek</surname></persName>
		</author>
		<ptr target="http://www.animatek.com/" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">The A-buffer, an Antialiased Hidden Surface Method</title>
		<author>
			<persName><forename type="first">L</forename><surname>Carpenter</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SIGGRAPH &apos;84 Proceedings</title>
		<imprint>
			<date type="published" when="1984-07">July 1984</date>
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="page" from="103" to="108" />
		</imprint>
	</monogr>
	<note>Computer Graphics</note>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">A Subdivision Algorithm for Computer Display of Curved Surfaces</title>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">E</forename><surname>Catmull</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1974-12">December 1974</date>
		</imprint>
		<respStmt>
			<orgName>University of Utah, Salt Lake City</orgName>
		</respStmt>
	</monogr>
	<note>Ph.D. thesis</note>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">LDI Tree: A Hierarchical Representation for Image-Based Rendering</title>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">F</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Bishop</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Lastra</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Graphics, SIGGRAPH &apos;99 Proceedings</title>
		<meeting><address><addrLine>Los Angeles, CA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="1999-08">August 1999</date>
			<biblScope unit="page" from="291" to="298" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Quicktime VR -An Image-Based Approach to Virtual Environment Navigation</title>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">E</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Graphics, SIGGRAPH &apos;95 Proceedings</title>
		<meeting><address><addrLine>Los Angeles, CA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="1995-08">August 1995</date>
			<biblScope unit="page" from="29" to="38" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">The Delta Tree: An Object-Centered Approach to Image-Based Rendering</title>
		<author>
			<persName><forename type="first">W</forename><surname>Dally</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Mcmillan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Bishop</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Fuchs</surname></persName>
		</author>
		<idno>AIM-1604</idno>
		<imprint>
			<date type="published" when="1996-05">May 1996</date>
		</imprint>
		<respStmt>
			<orgName>AI Lab, MIT</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">Technical Report</note>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Data Complexity for Virtual Reality: Where do all the Triangles Go?</title>
		<author>
			<persName><forename type="first">M</forename><surname>Deering</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Virtual Reality Annual International Symposium (VRAIS)</title>
		<meeting><address><addrLine>Seattle, WA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="1993-09">September 1993</date>
			<biblScope unit="page" from="357" to="363" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Texturing &amp; Modeling -A Procedural Approach</title>
		<author>
			<persName><forename type="first">D</forename><surname>Ebert</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Musgrave</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Peachey</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Perlin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Worley</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">AP Professional</title>
		<imprint>
			<date type="published" when="1994">1994</date>
		</imprint>
	</monogr>
	<note>second edition</note>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<author>
			<persName><forename type="first">S</forename><surname>Gortler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Grzeszczuk</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Szeliski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Cohen</surname></persName>
		</author>
		<author>
			<persName><surname>Lumigraph</surname></persName>
		</author>
		<title level="m">Computer Graphics, SIGGRAPH &apos;96 Proceedings</title>
		<meeting><address><addrLine>New Orleans, LS</addrLine></address></meeting>
		<imprint>
			<date type="published" when="1996-08">August 1996</date>
			<biblScope unit="page" from="43" to="54" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">Blue Moon Rendering Tools</title>
		<author>
			<persName><forename type="first">L</forename><surname>Gritz</surname></persName>
		</author>
		<ptr target="http://www.bmrt.org/" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">Point Sample Rendering. Master&apos;s thesis</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">P</forename><surname>Grossman</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1998-08">August 1998</date>
		</imprint>
		<respStmt>
			<orgName>Department of Electrical Engineering and Computer Science, MIT</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Point Sample Rendering</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">P</forename><surname>Grossman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Dally</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Rendering Techniques &apos;98</title>
		<meeting><address><addrLine>Wien, Vienna, Austria</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="1998-07">July 1998</date>
			<biblScope unit="page" from="181" to="192" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Survey of Texture Mapping</title>
		<author>
			<persName><forename type="first">P</forename><surname>Heckbert</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Computer Graphics &amp; Applications</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="56" to="67" />
			<date type="published" when="1986-11">November 1986</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">Fundamentals of Texture Mapping and Image Warping</title>
		<author>
			<persName><forename type="first">P</forename><surname>Heckbert</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1989-06-17">June 17 1989</date>
		</imprint>
		<respStmt>
			<orgName>University of California at Berkeley, Department of Electrical Engineering and Computer Science</orgName>
		</respStmt>
	</monogr>
	<note>Master&apos;s thesis</note>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">T</forename><surname>Herman</surname></persName>
		</author>
		<title level="m">Discrete Multidimensional Jordan Surfaces. CVGIP: Graphical Modeling and Image Processing</title>
		<imprint>
			<date type="published" when="1992-11">November 1992</date>
			<biblScope unit="volume">54</biblScope>
			<biblScope unit="page" from="507" to="515" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Volume Graphics</title>
		<author>
			<persName><forename type="first">A</forename><surname>Kaufman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Cohen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Yagel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computer</title>
		<imprint>
			<biblScope unit="volume">26</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="51" to="64" />
			<date type="published" when="1993-07">July 1993</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Light Field Rendering</title>
		<author>
			<persName><forename type="first">M</forename><surname>Levoy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Hanrahan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Graphics, SIGGRAPH &apos;96 Proceedings</title>
		<meeting><address><addrLine>New Orleans, LS</addrLine></address></meeting>
		<imprint>
			<date type="published" when="1996-08">August 1996</date>
			<biblScope unit="page" from="31" to="42" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">The Use of Points as Display Primitives</title>
		<author>
			<persName><forename type="first">M</forename><surname>Levoy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Whitted</surname></persName>
		</author>
		<idno>TR 85-022</idno>
		<imprint>
			<date type="published" when="1985">1985</date>
		</imprint>
		<respStmt>
			<orgName>The University of North Carolina at Chapel Hill, Department of Computer Science</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">Technical Report</note>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Image-Based Rendering for Non-Diffuse Synthetic Scenes</title>
		<author>
			<persName><forename type="first">D</forename><surname>Lischinski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Rappoport</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Rendering Techniques &apos;98</title>
		<meeting><address><addrLine>Wien, Vienna, Austria</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="1998-06">June 1998</date>
			<biblScope unit="page" from="301" to="314" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Hierarchical Rendering of Trees from Precomputed Multi-Layer Z-Buffers</title>
		<author>
			<persName><forename type="first">N</forename><surname>Max</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Rendering Techniques &apos;96</title>
		<meeting><address><addrLine>Wien, Porto, Portugal</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="1996-06">June 1996</date>
			<biblScope unit="page" from="165" to="174" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Plenoptic Modeling: An Image-Based Rendering System</title>
		<author>
			<persName><forename type="first">L</forename><surname>Mcmillan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Bishop</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Graphics, SIGGRAPH &apos;95 Proceedings</title>
		<meeting><address><addrLine>Los Angeles, CA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="1995-08">August 1995</date>
			<biblScope unit="page" from="39" to="46" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">High Quality 3D Image Warping by Separating Visibility from Reconstruction</title>
		<author>
			<persName><forename type="first">V</forename><surname>Popescu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Lastra</surname></persName>
		</author>
		<idno>TR99-002</idno>
		<imprint>
			<date type="published" when="1999-01-15">January 15 1999</date>
		</imprint>
		<respStmt>
			<orgName>University of North Carolina</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">Technical Report</note>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Particle Systems -A Technique for Modeling a Class of Fuzzy Objects</title>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">T</forename><surname>Reeves</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SIGGRAPH &apos;83 Proceedings</title>
		<imprint>
			<date type="published" when="1983-07">July 1983</date>
			<biblScope unit="volume">17</biblScope>
			<biblScope unit="page" from="359" to="376" />
		</imprint>
	</monogr>
	<note>Computer Graphics</note>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Per-Object Image Warping with Layered Impostors</title>
		<author>
			<persName><forename type="first">G</forename><surname>Schaufler</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Rendering Techniques &apos;98</title>
		<meeting><address><addrLine>Wien, Vienna, Austria</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="1998-06">June 1998</date>
			<biblScope unit="page" from="145" to="156" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Layered Depth Images</title>
		<author>
			<persName><forename type="first">J</forename><surname>Shade</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">J</forename><surname>Gortler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Szeliski</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Graphics, SIGGRAPH &apos;98 Proceedings</title>
		<meeting><address><addrLine>Orlando, FL</addrLine></address></meeting>
		<imprint>
			<date type="published" when="1998-07">July 1998</date>
			<biblScope unit="page" from="231" to="242" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">R</forename><surname>Smith</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Smooth Operator. The Economist</title>
		<imprint>
			<date type="published" when="1999-03-06">March 6 1999</date>
			<biblScope unit="page" from="73" to="74" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Talisman: Commodity Real-Time 3D Graphics for the PC</title>
		<author>
			<persName><forename type="first">J</forename><surname>Torborg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Kajiya</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Graphics, SIGGRAPH &apos;96 Proceedings</title>
		<meeting><address><addrLine>New Orleans, LS</addrLine></address></meeting>
		<imprint>
			<date type="published" when="1996-08">August 1996</date>
			<biblScope unit="page" from="353" to="364" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Reflection Vector Shading Hardware</title>
		<author>
			<persName><forename type="first">D</forename><surname>Voorhies</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Foran</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Graphics, Proceedings of SIGGRAPH 94</title>
		<imprint>
			<date type="published" when="1994-07">July 1994</date>
			<biblScope unit="page" from="163" to="166" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Footprint Evaluation for Volume Rendering</title>
		<author>
			<persName><forename type="first">L</forename><surname>Westover</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Graphics, Proceedings of SIGGRAPH 90</title>
		<imprint>
			<date type="published" when="1990-08">August 1990</date>
			<biblScope unit="page" from="367" to="376" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
