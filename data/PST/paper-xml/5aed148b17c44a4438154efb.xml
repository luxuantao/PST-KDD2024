<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Higher-order Network Representation Learning</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Ryan</forename><forename type="middle">A</forename><surname>Rossi</surname></persName>
							<email>rrossi@adobe.com</email>
							<affiliation key="aff0">
								<orgName type="institution">Intel Labs</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Nesreen</forename><forename type="middle">K</forename><surname>Ahmed</surname></persName>
							<email>nesreen.k.ahmed@intel.com</email>
							<affiliation key="aff0">
								<orgName type="institution">Intel Labs</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Higher-order Network Representation Learning</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="DOI">10.1145/3184558.3186900</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-01-03T08:52+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>This paper describes a general framework for learning Higher-Order Network Embeddings (HONE) from graph data based on network motifs. The HONE framework is highly expressive and flexible with many interchangeable components. The experimental results demonstrate the effectiveness of learning higher-order network representations. In all cases, HONE outperforms recent embedding methods that are unable to capture higher-order structures with a mean relative gain in AUC of 19% (and up to 75% gain) across a wide variety of networks and embedding methods.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">INTRODUCTION</head><p>Roles represent node (or edge <ref type="bibr" target="#b1">[2]</ref>) connectivity patterns such as hubs, star-centers, star-edge nodes, near-cliques or nodes that act as bridges to different regions of the graph. Intuitively, two nodes belong to the same role if they are structurally similar <ref type="bibr" target="#b7">[8]</ref>. Many network representation learning methods (including random-walk based methods such as node2vec [4]) seek to capture the notion of structural similarity (roles) [8] by defining node similarity locally based on neighborhood properties and/or proximity (e.g., near one another in the graph). However, such methods are insufficient for roles <ref type="bibr" target="#b7">[8]</ref> as they fail to capture the higher-order connectivity patterns of a node. For instance, instead of representing hub nodes in a similar fashion, methods using random-walks (proximity/distancebased) would represent a hub node and its neighbors similarly despite them having fundamentally different connectivity patterns.</p><p>In this work, we propose higher-order network representation learning and describe a general framework called Higher-Order Network Embeddings (HONE) for learning such higher-order embeddings based on network motifs. The term motif is used generally and may refer to graphlets or orbits (graphlet automorphisms) <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b5">6]</ref>. The HONE framework expresses a general family of embedding methods based on a set of motif-based matrices and their powers. In this work, we investigate HONE variants based on the weighted motif graph, motif transition matrix, motif Laplacian matrix, as well as other motif-based matrix formulations. The experiments demonstrate the effectiveness of higher-order network embeddings.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>(weighted) adjacency matrices: W = W 1 , W 2 , . . . , W T where (W t ) i j = number of instances of motif H t ? H that contain nodes i and j. To generalize HONE for any motif-based matrix formulation, we define ? as a function ? : R N ?N ? R N ?N over a (k-step) weighted motif adjacency matrix W k t . For convenience, we use W below to denote a weighted adjacency matrix for an arbitrary motif. We summarize the motif matrix functions ? investigated below.</p><p>? Motif Weighted Graph: In the case of using HONE directly with a weighted motif adjacency matrix W, then</p><formula xml:id="formula_0">? : W ? IW<label>(1)</label></formula><p>The number of paths weighted by motif counts from node i to node j in k-steps is given by</p><formula xml:id="formula_1">(W k ) i j = W ? ? ? W k i j<label>(2)</label></formula><p>? Motif Transition Matrix: The random walk on a graph W weighted by motif counts has transition probabilities</p><formula xml:id="formula_2">P i j = W i j w i</formula><p>where w i = j W i j is the motif degree of node i. The random walk motif transition matrix P for an arbitrary weighted motif graph W is defined as:</p><formula xml:id="formula_3">P = D -1 W<label>(3)</label></formula><p>where D = diag(We) is a N ? N diagonal motif degree matrix with the motif degree w i = j W i j of each node on the diagonal and e = [ 1 1</p><formula xml:id="formula_4">? ? ? 1 ] T ? R N</formula><p>is the vector of all ones. The motif transition matrix P represents the transition probabilities of a non-uniform random walk on the graph that selects subsequent nodes with probability proportional to the connecting edge's motif count. Therefore, the probability of transitioning from node i to node j depends on the motif degree of j relative to the total sum of motif degrees of all neighbors of i. The probability of transitioning from node i to node j in k-steps is given by (P k ) i j . ? Motif Laplacian: The motif Laplacian for a weighted motif graph W is defined as:</p><formula xml:id="formula_5">L = D -W<label>(4)</label></formula><p>where D = diag(We) is the diagonal matrix of motif degrees. ? Normalized Motif Laplacian: Given a graph W weighted by the counts of an arbitrary network motif H t ? H , the normalized motif Laplacian is defined as</p><formula xml:id="formula_6">L = I -D -1/2 WD -1/2 (<label>5</label></formula><formula xml:id="formula_7">)</formula><p>where I is the identity matrix and D = diag(We).</p><p>Notice that all variants are easily formulated as functions ? in terms of an arbitrary motif weighted graph W. Next, we derive all k-step motif-based matrices for all T motifs and K steps:</p><formula xml:id="formula_8">S (k ) t = ?(W k t ), for k = 1, 2, . . . , K and t = 1, . . . ,T<label>(6)</label></formula><p>These k-step motif-based matrices can densify quickly and therefore we recommend using K ? 4. Given a k-step motif-based matrix S (k ) t</p><p>for an arbitrary network motif H t ? H , we learn node embeddings by solving the following objective function:</p><formula xml:id="formula_9">arg min U (k ) t ,V (k ) t ? C D S (k ) t ? ??U (k ) t V (k ) t ?<label>(7)</label></formula><p>where D is a generalized Bregman divergence with matching linear or non-linear function ? and C denotes constraints (e.g., U T U = I, V T V = I). We use Eq. 7 to learn a N ? D ? local embedding</p><formula xml:id="formula_10">U (k ) t from S (k )</formula><p>t for all t = 1, . . . ,T and k = 1, . . . , K. 1 Afterwards, we scale each column of U (k) t using the Euclidean norm. Next, we concatenate the k-step embedding matrices for all T motifs and all K steps:</p><formula xml:id="formula_11">Y = U (1) 1 ? ? ? U (1) T 1-step ? ? ? U (K ) 1 ? ? ? U (K ) T K -steps (8)</formula><p>where Y is a N ? T KD ? matrix. Given Y, we learn a global higherorder network embedding by solving the following:</p><formula xml:id="formula_12">arg min Z,H? C D Y ? ??ZH? (<label>9</label></formula><formula xml:id="formula_13">)</formula><p>where Z is a N ? D matrix of node embeddings. In Eq. 9 we use Frobenius norm which leads to the following minimization problem:</p><formula xml:id="formula_14">min Z,H 1 2 Y -ZH 2 F = 1 2 i j Y i j -(ZH) i j 2<label>(10)</label></formula><p>A similar minimization problem is solved for Eq. 7.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">EXPERIMENTS</head><p>We compare the proposed HONE variants to five recent state-ofthe-art methods (see Table <ref type="table" target="#tab_0">1</ref>). All methods output</p><formula xml:id="formula_15">(D = 128)- dimensional node embeddings Z = z 1 ? ? ? z N T</formula><p>where z i ? R D . For node2vec, we perform a grid search over p, q ? {0.25, 0.5, 1, 2, 4} as mentioned in <ref type="bibr" target="#b3">[4]</ref>. All other hyperparameters for node2vec <ref type="bibr" target="#b3">[4]</ref>, DeepWalk <ref type="bibr" target="#b4">[5]</ref>, and LINE <ref type="bibr" target="#b8">[9]</ref> correspond to those mentioned in <ref type="bibr" target="#b3">[4]</ref>. In contrast, the HONE variants have only one hyperparameter, namely, the number of steps K which is selected automatically via a grid search over K ? {1, 2, 3, 4} using 10% of the labeled data. We use all 2-4 node connected orbits <ref type="bibr" target="#b5">[6]</ref> and set D ? = 16 for the local motif embeddings. All methods use logistic regression (LR) with an L2 penalty. The model is selected using 10-fold cross-validation on 10% of the labeled data. Experiments are repeated for 10 random seed initializations. Data was obtained from <ref type="bibr" target="#b6">[7]</ref>.</p><p>We evaluate the HONE variants for link prediction. Given a partially observed graph G with a fraction of missing edges, the link prediction task is to predict these missing edges. We generate a labeled dataset of edges. Positive examples are obtained by removing 50% of edges randomly, whereas negative examples are generated by randomly sampling an equal number of node pairs (i, j) E. For each method, we learn embeddings using the remaining graph. Using the embeddings from each method, we then learn a model to predict whether a given edge in the test set exists in E or not. 1 For the motif Laplacian matrix formulations proposed above, we also investigated using the eigenvectors of the D ? smallest eigenvalues of ?(W k t ) as node embeddings. To construct edge features from the node embeddings, we use the mean operator defined as (z i + z j ) 2. The AUC results are provided in Table <ref type="table" target="#tab_0">1</ref>. In all cases, the HONE methods outperform the other embedding methods with an overall mean gain of 19.24% (and up to 75.21% gain) across a wide variety of graphs with different characteristics. Overall, the HONE variants achieve an average gain of 10.68% over node2vec, 12.56% over DeepWalk, 13.79% over LINE, 17.17% over GraRep, and 41.99% over Spectral clustering across all networks. We also derive a total ranking of the embedding methods over all graph problems based on mean relative gain (1-vs-all). Results are provided in the last column of Table <ref type="table" target="#tab_0">1</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">CONCLUSION</head><p>In this work, we introduced higher-order network representation learning and proposed a general framework called higher-order network embedding (HONE) for learning such embeddings based on higher-order connectivity patterns. The experimental results demonstrate the effectiveness of learning higher-order network representations. Future work will investigate the framework using other useful motif-based matrices.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 :</head><label>1</label><figDesc>AUC results comparing HONE to recent embedding methods. See text for discussion.</figDesc><table><row><cell>so c-ha m st er</cell><cell>rt-tw itt er -c op</cell><cell>so c-wi ki -V ot e</cell><cell>te ch -ro ut er s-rf</cell><cell>fa ce bo ok -P U</cell><cell>in f-o pe nf lig ht s</cell><cell>so c-bi tc oi nA</cell><cell>Rank</cell></row><row><cell cols="7">HONE-W (Eq. 1) 0.841 0.843 0.811 0.862 0.726 0.910 0.979</cell><cell>1</cell></row><row><cell cols="7">HONE-P (Eq. 3) 0.840 0.840 0.812 0.863 0.724 0.913 0.980</cell><cell>2</cell></row><row><cell cols="7">HONE-L (Eq. 4) 0.829 0.841 0.808 0.858 0.722 0.906 0.975</cell><cell>3</cell></row><row><cell cols="7">HONE-L (Eq. 5) 0.829 0.836 0.803 0.862 0.722 0.908 0.976</cell><cell>4</cell></row><row><cell cols="7">Node2Vec [4] 0.810 0.635 0.721 0.804 0.701 0.844 0.894</cell><cell>5</cell></row><row><cell cols="7">DeepWalk [5] 0.796 0.621 0.710 0.796 0.696 0.837 0.863</cell><cell>6</cell></row><row><cell cols="7">LINE [9] 0.752 0.706 0.734 0.800 0.630 0.837 0.780</cell><cell>7</cell></row><row><cell cols="7">GraRep [3] 0.805 0.672 0.743 0.829 0.702 0.898 0.559</cell><cell>8</cell></row><row><cell cols="7">Spectral [10] 0.561 0.699 0.593 0.602 0.516 0.606 0.629</cell><cell>9</cell></row></table></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">Efficient Graphlet Counting for Large Networks</title>
		<author>
			<persName><forename type="first">K</forename><surname>Nesreen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jennifer</forename><surname>Ahmed</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ryan</forename><forename type="middle">A</forename><surname>Neville</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nick</forename><surname>Rossi</surname></persName>
		</author>
		<author>
			<persName><surname>Duffield</surname></persName>
		</author>
		<idno>ICDM. 10</idno>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">Edge Role Discovery via Higher-Order Structures</title>
		<author>
			<persName><forename type="first">K</forename><surname>Nesreen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ryan</forename><forename type="middle">A</forename><surname>Ahmed</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Theodore</forename><forename type="middle">L</forename><surname>Rossi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rong</forename><surname>Willke</surname></persName>
		</author>
		<author>
			<persName><surname>Zhou</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="291" to="303" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">GraRep: Learning graph representations with global structural information</title>
		<author>
			<persName><forename type="first">Shaosheng</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wei</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qiongkai</forename><surname>Xu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CIKM</title>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="891" to="900" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">node2vec: Scalable feature learning for networks</title>
		<author>
			<persName><forename type="first">Aditya</forename><surname>Grover</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jure</forename><surname>Leskovec</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SIGKDD</title>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="855" to="864" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Deepwalk: Online learning of social representations</title>
		<author>
			<persName><forename type="first">Bryan</forename><surname>Perozzi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rami</forename><surname>Al-Rfou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Steven</forename><surname>Skiena</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SIGKDD</title>
		<imprint>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="701" to="710" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Biological network comparison using graphlet degree distribution</title>
		<author>
			<persName><forename type="first">Nata?a</forename><surname>Pr?ulj</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Bioinfo</title>
		<imprint>
			<biblScope unit="volume">23</biblScope>
			<biblScope unit="page" from="177" to="e183" />
			<date type="published" when="2007">2007. 2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">The Network Data Repository with Interactive Graph Analytics and Visualization</title>
		<author>
			<persName><forename type="first">Ryan</forename><forename type="middle">A</forename><surname>Rossi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nesreen</forename><forename type="middle">K</forename><surname>Ahmed</surname></persName>
		</author>
		<ptr target="http://networkrepository.com" />
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="4292" to="4293" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Role Discovery in Networks</title>
		<author>
			<persName><forename type="first">Ryan</forename><forename type="middle">A</forename><surname>Rossi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nesreen</forename><forename type="middle">K</forename><surname>Ahmed</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Transactions on Knowledge and Data Engineering</title>
		<imprint>
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="page" from="1112" to="1131" />
			<date type="published" when="2015-04">2015. April 2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">LINE: Large-scale Information Network Embedding</title>
		<author>
			<persName><forename type="first">Jian</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Meng</forename><surname>Qu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mingzhe</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ming</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jun</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qiaozhu</forename><surname>Mei</surname></persName>
		</author>
		<idno>WWW. 1067-1077</idno>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Leveraging social media networks for classification</title>
		<author>
			<persName><forename type="first">Lei</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Huan</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Data Mining and Knowledge Discovery</title>
		<imprint>
			<biblScope unit="volume">23</biblScope>
			<biblScope unit="page" from="447" to="478" />
			<date type="published" when="2011">2011. 2011</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
