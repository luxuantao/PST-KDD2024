<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Modelling Domain Relationships for Transfer Learning on Retrieval-based Question Answering Systems in E-commerce</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Jianfei</forename><surname>Yu</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Minghui</forename><surname>Qiu</surname></persName>
							<email>minghui.qmh@alibaba-inc.com</email>
						</author>
						<author>
							<persName><forename type="first">Jing</forename><surname>Jiang</surname></persName>
							<email>jingjiang@smu.edu.sg</email>
						</author>
						<author>
							<persName><forename type="first">Wei</forename><surname>Chu</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Haiqing</forename><surname>Chen</surname></persName>
							<email>haiqing.chenhq@alibaba-inc.com</email>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="institution">Singapore Management University</orgName>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff1">
								<orgName type="laboratory">Alibaba Group</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff2">
								<orgName type="institution">Singapore Management University</orgName>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff3">
								<orgName type="laboratory">Shuangyong Song Alibaba Group</orgName>
								<orgName type="institution">Jun Huang</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff4">
								<orgName type="laboratory">Alibaba Group</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff5">
								<orgName type="laboratory">Alibaba Group</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff6">
								<address>
									<addrLine>5-9, Marina Del Rey</addrLine>
									<postCode>2018</postCode>
									<region>CA</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Modelling Domain Relationships for Transfer Learning on Retrieval-based Question Answering Systems in E-commerce</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">404099F72F445EF07ECF1D4BFE12640E</idno>
					<idno type="DOI">10.1145/3159652.3159685</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.3" ident="GROBID" when="2023-07-28T12:40+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Nowadays, it is a heated topic for many industries to build automatic question-answering (QA) systems. A key solution to these QA systems is to retrieve from a QA knowledge base the most similar question of a given question, which can be reformulated as a paraphrase identification (PI) or a natural language inference (NLI) problem. However, most existing models for PI and NLI have at least two problems: They rely on a large amount of labeled data, which is not always available in real scenarios, and they may not be efficient for industrial applications.</p><p>In this paper, we study transfer learning for the PI and NLI problems, aiming to propose a general framework, which can effectively and efficiently adapt the shared knowledge learned from a resource-rich source domain to a resource-poor target domain. Specifically, since most existing transfer learning methods only focus on learning a shared feature space across domains while ignoring the relationship between the source and target domains, we propose to simultaneously learn shared representations and domain relationships in a unified framework. Furthermore, we propose an efficient and effective hybrid model by combining a sentence encoding-based method and a sentence interaction-based method as our base model. Extensive experiments on both paraphrase identification and natural language inference demonstrate that our base model is efficient and has promising performance compared to the competing models, and our transfer learning method can help to significantly boost the performance. Further analysis shows that the inter-domain and intra-domain relationship captured by our model are insightful. Last but not least, we deploy our transfer learning model for PI into our online chatbot system, which can bring in significant improvements over our existing system.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">INTRODUCTION</head><p>Question Answering (QA) systems have been widely developed and used in many domains. Examples of industry applications include Alibaba's AliME <ref type="bibr" target="#b25">[26]</ref>, Microsoft's SuperAgent <ref type="bibr" target="#b6">[7]</ref>, Apple's Siri and Google's Google Assistant. Generally speaking, there are two kinds of commonly-used techniques behind most QA systems: Information Retrieval (IR)-based models <ref type="bibr" target="#b38">[39]</ref> and generation-based models <ref type="bibr" target="#b30">[31]</ref>. In this work, we focus on building up an IR-based QA system for automatically answering frequently asked questions (FAQs) in the E-commerce industry.  Fig. <ref type="figure">1</ref> illustrates the workflow of IR-based QA systems, where a key component is the Question Rerank module which reranks candidate questions in a question-answering knowledge base (KB) to find the best matching question given a question from a user. This task can be reduced to a paraphrase identification or a natural language inference problem. Take the query question and knowledge base shown in Fig. <ref type="figure">1</ref> for example. If we can detect that question C1 in the KB is a paraphrase of the query question, then we can take its answer as the answer for the query. In some cases, if we allow the matching question to be more general than the query question (i.e., entailed by the query question), we can also take the answer for question C2 in the KB as the query question's answer.</p><p>In the literature, paraphrase identification (PI) and natural language inference (NLI) have been extensively studied in the last decade <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b3">4,</ref><ref type="bibr" target="#b5">6,</ref><ref type="bibr" target="#b28">29,</ref><ref type="bibr" target="#b41">42]</ref>. However, when applying existing solutions to PI and NLI in IR-based QA systems in the E-commerce industry, there are at least two major challenges we face: (1) Lack of rich training data: All these solutions rely on a large amount of labeled data. However, it is generally time-consuming and costly to manually annotate sufficient labeled data for each domain. For example, different product categories might need different training data. <ref type="bibr" target="#b1">(2)</ref> Hard to reach a high QPS 3 . Most of the existing methods focus on improving the effectiveness or accuracy without paying much attention to efficiency. For real industry applications, when real-time responses are expected and a large number of customers are being served simultaneously, we need an efficient method to support a high QPS.</p><p>In this paper, we try to address the two challenges above. Specifically, we first make an empirical comparison of both the effectiveness and efficiency of several representative methods for modeling sentence pairs and propose an effective and efficient hybrid model as our base model. This ensures that we can achieve a high QPS. On top of the base model, we further design a new transfer learning (TL) framework, which is able to efficiently improve the performance on a resource-poor target domain by leveraging knowledge from a resource-rich source domain.</p><p>A Hybrid Base Model. Observing that LSTM-based methods <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b5">6]</ref> are much more time-consuming than CNN-based methods [20,   3 Queries Per Second 42], we focus on CNN-based methods in this study. Meanwhile, there are typically two types of CNN-based methods for the task, namely sentence encoding (SE)-based methods <ref type="bibr" target="#b20">[21,</ref><ref type="bibr" target="#b40">41]</ref> and sentence interaction (SI)-based methods <ref type="bibr" target="#b12">[13,</ref><ref type="bibr" target="#b22">23]</ref>. We argue that these two types of methods may highly complement each other, and thus we propose a hybrid CNN model by combining an SE-based method <ref type="bibr" target="#b41">[42]</ref> and an SI-based method <ref type="bibr" target="#b22">[23]</ref>. Specifically, we modify the SE-based method using two element-wise comparison functions inspired by <ref type="bibr" target="#b19">[20,</ref><ref type="bibr" target="#b34">35]</ref> to match the two sentence embeddings, and then concatenate them together with sentence embeddings from the SI-based method.</p><p>Transfer Learning Framework. Transfer learning aims to apply knowledge gained in a source domain to help a target domain <ref type="bibr" target="#b21">[22]</ref>. The key issue is how to transfer the shared knowledge from the source domain to the target domain while exclude the specific knowledge in the source domain based on the domain relationship. Most recent studies for TL in NLP perform multi-task feature learning by exploiting different NN models to capture a shared feature space across domains. As illustrated in Fig. <ref type="figure" target="#fig_0">2a</ref> and Fig. <ref type="figure" target="#fig_0">2b</ref>, one line of work employs a fully-shared framework to learn a shared representation followed by using two different fully connected layers for each domain <ref type="bibr" target="#b20">[21,</ref><ref type="bibr" target="#b39">40]</ref>, while another line of work uses a specific-shared framework to learn not only a shared representation for both domains but also a domain-specific representation for each domain <ref type="bibr" target="#b17">[18]</ref>.</p><p>However, the first line of work simply assumes that two domains share the same feature space but ignore the domain-specific feature space. Although the latter one is capable of capturing both the shared and the domain-specific representations, it does not consider any relationships between the weights of the final output layer. Generally speaking, the weights on the output layer should capture both the inter-domain and the intra-domain relationships: (1) For the shared feature space across domains, since it is expected to be domain-independent, the weights corresponding to this feature space in the two domains should be positively related to each other;</p><p>(2) For the shared and the domain-specific feature spaces in each domain, since they are expected to respectively capture domainindependent and domain-dependent features, their corresponding weights should be irrelevant to each other. Motivated by such an intuition, in this paper, we propose a new transfer learning method by explicitly modeling the domain relationships via a covariance matrix, which imposes a regularization term on the weights of the output layer to uncover both the inter-domain and the intradomain relationships. Besides, to make the shared representation more invariant across domains, we follow some recent work on adversarial networks <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b17">18]</ref> and introduce an adversarial loss on the shared feature space in our method. Fig. <ref type="figure" target="#fig_2">3</ref> gives an outline of our full model.</p><p>To evaluate our proposed method, we conduct both intrinsic evaluation and extrinsic evaluation.</p><p>Intrinsic Evaluation. We conduct extensive experiments on both a benchmark dataset and our own dataset. <ref type="bibr" target="#b0">(1)</ref> The hybrid CNN model is shown to be not only efficient but also effective, in comparison with several representative methods; (2) Our proposed transfer learning method can bring significant improvements over the base model without transfer learning, and outperform existing Technical Presentation WSDM'18, February 5-9, 2018, Marina Del Rey, CA, USA TL frameworks including the widely used fully-shared model and the recently proposed specific-shared model; (3) Further analysis on our learned correlation matrix shows that our method is able to capture the inter-domain and intra-domain relationships. Extrinsic Evaluation. We deploy our proposed hybrid CNNbased transfer learning model into our online chatbot system, which is deployed on a real E-commerce site AliExpress. Both the offline and the online evaluations show that our new system can significantly outperform the existing online chatbot system. Finally, we launch our new system on Eva<ref type="foot" target="#foot_0">4</ref> , a chatbot platform in AliExpress.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">MODEL</head><p>In this section, we present our general model for paraphrase identification and natural language inference, which will be used for question reranking in our chatbot-based QA system.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Problem Formulation and Notation</head><p>Our model is designed to address the following general problem. Given a pair of sentences, we would like to identify their semantic relation. For paraphrase identification (PI), the semantic relation indicates whether or not the two sentences express the same meaning <ref type="bibr" target="#b40">[41]</ref>; for natural language inference (NLI), it indicates whether a hypothesis sentence can be inferred from a premise sentence <ref type="bibr" target="#b2">[3]</ref>.</p><p>Formally, assume there are two sentences</p><formula xml:id="formula_0">X 1 = (x 1 1 , x 1 2 , . . . , x 1 m ) and X 2 = (x 2 1 , x 2 2 , . . . , x 2 n )</formula><p>, where x j i denotes an l-dimensional dense embedding vector retrieved from a lookup table E ∈ R l × | V | for all the words in the vocabulary V. Our task is to predict the semantic label y which indicates the relation between X 1 and X 2 . For PI, we assume the label y to be either paraphrase or not paraphrase; for NLI, we assume y to be either neutral, entailment or contradiction.</p><p>We consider a transfer learning setting, where we have a set of labeled sentence pairs from a source domain and a target domain, respectively, denoted by D s and D t . Note that |D s | is assumed to be much larger than |D t |. We seek to use both D s and D t to train a good model so that it can work well in the target domain.</p><p>To solve such a problem, a widely used transfer learning method (as illustrated in Fig. <ref type="figure" target="#fig_0">2a</ref>) is to use the same NN model to transform every pair of input sentences in both domains into a hidden representation z c ∈ R q , where q is the size of the hidden representations. To facilitate our discussion, let us assume z c = f Θ c (X 1 , X 2 ), where f Θ c denotes the transformation function parameterized by Θ c . Next, for the source and the target domains, we assume that two fully connected layers are separately learned to map z c to label y. Besides, another transfer learning approach <ref type="bibr" target="#b17">[18]</ref> was recently proposed to use a domain-shared NN model and two domainspecific NN models to obtain a shared embedding z c and two domain-specific embeddings</p><formula xml:id="formula_1">p(y | z c ) = softmax(W sc z c + b s ) if y is source label, softmax(W tc z c + b t ) if y is target label,</formula><formula xml:id="formula_2">z s = f Θ s (X 1 , X 2 ) and z t = f Θ t (X 1 , X 2 ).</formula><p>The main limitation of the fully-shared framework is that it ignores source-specific or target-specific features. While for the specific-shared framework, it fails to consider any inherent correlations between the weights on the output layers. Therefore, we will introduce our proposed method that explicitly incorporates such correlations into the specific-shared framework in the next session.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Proposed Transfer Learning Method</head><p>Our goal is to model the inter-domain relationship between W sc and W tc , and the intra-domain relationship between W s and W sc as well as W t and W tc . Hence, we first reshape each weight matrix into a vector w d ∈ R q |Y | , followed by concatenating all the four reshaped vectors to form a new matrix W ∈ R q |Y |×4 , where each column corresponds to one weight matrix of the output layer.</p><p>Next, to capture the domain relationships mentioned above, we introduce a covariance matrix Ω ∈ R 4×4 . Note that each element Ω i,j indicates the correlation between W i and W j , where i and j are one of s, sc, t and tc. Inspired by a general multi-task relationship learning framework as introduced in <ref type="bibr" target="#b44">[45]</ref>, we consider confining the output layer's weights with Ω by using tr(WΩ -1 W T ), where tr(•) is the trace of a square matrix. This means that if Ω i,j is a large positive/negative value, W i and W j will be positively/negatively related to each other; otherwise if Ω i,j is close to zero, W i and W j will be irrelevant to each other. Note that to the best of our knowledge, we are the first to apply the multi-task relationship learning framework into NN-based transfer learning methods.</p><p>In order to simultaneously learn our model parameters and the domain relationships in a unified framework, we formulate our loss function as follows:</p><formula xml:id="formula_3">L = k∈s,t - 1 n k n k i=1 log p y i | X i 1 , X i 2 + λ 1 2 tr(WΩ -1 W T ) + λ 2 2 ||W|| 2 F + λ 3 2 ||Θ c || 2 F + λ 4 2 ||Θ s || 2 F + λ 5 2 ||Θ t || 2 F s.t. Ω ≥ 0, tr(Ω) = 1.<label>(1)</label></formula><p>where λ 1 , λ 2 , λ 3 , λ 4 and λ 5 are regularization parameters, Ω is required to be positive semi-definite, and tr(Ω) is required to be 1 without losing generality. In the above formulation, the first term refers to the cross-entropy loss for both domains, and the second term serves as a domain-relationship regularizer to constrain the weights on the output layer. The remaining terms are standard L2-regularization terms.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Adversarial Loss</head><p>Our above transfer learning method is based on the specific-shared framework, which is assumed to well capture the shared and domainspecific feature spaces. However, as suggested by <ref type="bibr" target="#b17">[18]</ref>, the shared representation learned in this framework may still contain noisy domain-specific features. Therefore, to eliminate the noisy features, here we also consider incorporating an adversarial loss on the shared feature space so that the trained model can not distinguish between the source and target domains on it <ref type="bibr" target="#b10">[11]</ref>.</p><p>First, we assume that the shared layer z c is mapped to a binary domain label d, which indicates whether z c comes from the source or the target domain:</p><formula xml:id="formula_4">p(d | z c ) = softmax(W d z c + b d ).</formula><p>Since the goal of adversarial training is to encourage the shared feature space indiscriminate across two domains, we define the adversarial loss as minimizing the negative entropy of the predicted domain distribution, which is different from maximizing the negative cross-entropy as in <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b17">18]</ref>:</p><formula xml:id="formula_5">ℓ = k∈s,t 1 n k n k i=1 1 j=0 p(d i j | X i 1 , X i 2 ) log p(d i j | X i 1 , X i 2 ) .<label>(2)</label></formula><p>Finally, we obtain a combined objective function as follows:</p><p>min</p><formula xml:id="formula_6">Ω,W,W d ,Θ c , Θ s ,Θ t ,b s ,b t ,b d L + λ 0 ℓ s.t. Ω ≥ 0, tr(Ω) = 1,</formula><p>where λ 0 is a hyper-parameter for tuning the importance of the adversarial loss. As suggested by <ref type="bibr" target="#b44">[45]</ref>, it is not easy to optimize such a semi-definite programming problem. We will present an alternating training approach in Section 2.5 for solving it efficiently.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.4">Base Model</head><p>Although the proposed transfer learning method is general and any neural networks for modeling a pair of sentences can be applied to it, we further target at proposing an efficient and effective base model for encoding a pair of sentences. On one hand, although various attention-based LSTM architectures have been proposed to achieve a superior performance on both PI and NLI <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b23">24,</ref><ref type="bibr" target="#b27">28,</ref><ref type="bibr" target="#b34">35]</ref>, these models are very time-consuming due to the computation of memory cells and attention weights in each time step, which may not satisfy the industry demand, especially when QPS is high. On the other hand, CNN-based models are proven to be efficient, hence are the focus of our study. Most existing CNN-based models can be categorized into two groups: sentence encoding (SE)-based methods and sentence interaction (SI)-based methods. The former aims to first learn good representations for each sentence, followed by using a comparison function to transform them into a single representation <ref type="bibr" target="#b20">[21,</ref><ref type="bibr" target="#b40">41]</ref>, while the latter tries to directly model the interaction between two sentences at the beginning and then makes abstractions on top of the interaction output <ref type="bibr" target="#b12">[13,</ref><ref type="bibr" target="#b22">23]</ref>. Observing that the two lines of methods focus on different perspectives to model sentence pairs, we expect that a combination of them can capture both good sentence representations and rich interaction structures.</p><p>Hence, we propose a hybrid CNN (hCNN) model, which are based on some minor modifications of two existing models: a SEbased BCNN model <ref type="bibr" target="#b41">[42]</ref> and a SI-based Pyramid model <ref type="bibr" target="#b22">[23]</ref>. Fig. <ref type="figure" target="#fig_2">3</ref> depicts our full transfer learning framework, which contains one shared hCNN and two domain-specific hCNNs. Below we briefly go through the architecture of hCNN. Note that in our implementation and the model description below, we pad the two input sentences to the same length m.</p><p>Modified BCNN: Following the original BCNN <ref type="bibr" target="#b41">[42]</ref>, we first use two separate 1-D convolutional (conv) and 1-D max-pooling layers to encode the two input sentences into two sentence embeddings:</p><formula xml:id="formula_7">h 1 = CNN(X 1 ); h 2 = CNN(X 2 ).</formula><p>Furthermore, as suggested by <ref type="bibr" target="#b19">[20,</ref><ref type="bibr" target="#b34">35]</ref> that element-wise comparison can work well on the problem, we use two comparison functions to match the two sentence embeddings, and then concatenate them together with the sentence embeddings as the sentence pair representation:</p><formula xml:id="formula_8">H b = h 1 ⊕ h 2 ⊕ (h 1 -h 2 ) ⊕ (h 1 • h 2 )</formula><p>, whereand • refer to element-wise subtraction and element-wise multiplication, and ⊕ refers to concatenation. Note that this setting is different from the original BCNN, which yields better performance in our empirical experiments.</p><p>Pyramid: As shown in the rightmost part of Fig <ref type="figure" target="#fig_2">3</ref>, we first produce an interaction matrix M ∈ R m×m , where M i,j denotes the similarity score between the i th word in X 1 and the j th word in X 2 . Following <ref type="bibr" target="#b22">[23]</ref>, we use dot-product to compute the similarity score.</p><p>Next, by viewing the interaction matrix as an image, we stack two 2-D convolutional layers and two 2-D max-pooling layers on it to obtain the hidden representation H p .</p><p>Finally, we concatenate the two hidden representations as the final representation for each input sentence pair: z = H b ⊕ H p .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.5">Inference</head><p>In our combined objective function, we have nine parameters Ω, W, W d , b s , b t , b d , Θ c , Θ s and Θ t , and it is not easy to optimize them at the same time. Following the practice in <ref type="bibr" target="#b44">[45]</ref>, we employ an alternating stochastic method, i.e., first optimizing the other eight parameters by fixing Ω, and then alternatively optimizing Ω by fixing the others in each iteration. The details are given as below:</p><p>Updating W, W d , b s , b t , b d , Θ c , Θ s and Θ t . While fixing Ω, the optimization problem becomes: min</p><formula xml:id="formula_9">W,W d ,Θ c ,Θ s , Θ t ,b s ,b t ,b d k∈s,t 1 n k n k i=1 -log p(y i | X i 1 , X i 2 ) + λ 0 1 j=0 p(d i j | X i 1 , X i 2 ) log p(d i j | X i 1 , X i 2 ) + λ 1 2 tr(WΩ -1 W T ) + λ 2 2 ||W|| 2 F + λ 3 2 ||Θ c || 2 F + λ 4 2 ||Θ s || 2 F + λ 5 2 ||Θ t || 2 F</formula><p>Since it is a smooth function, we can easily compute its partial derivatives with respect to the eight parameters.</p><p>Updating Ω. After fixing the eight parameters, the optimization problem is as follows:</p><formula xml:id="formula_10">min Ω tr(WΩ -1 W T ) s.t. Ω ≥ 0, tr(Ω) = 1.</formula><p>As proved by <ref type="bibr" target="#b44">[45]</ref>, the above optimization problem has an analytical</p><formula xml:id="formula_11">solution Ω = (W T W) 1 2 tr (W T W) 1 2</formula><p>. Finally, we present the whole procedure for training our full model as in Algorithm 1. Note that we only update Ω when we scan all the target training instances once.   if c t == tgt_batches then 13:</p><formula xml:id="formula_12">D t (|D s | ≫ |D t |) . 2: Output: Ω, W, W d , b s , b t , b d , Θ c , Θ s , Θ t 3: Initialize W, W d , b s , b t , b d , Θ c , Θ s , Θ t with</formula><formula xml:id="formula_13">c t = 0 14: Update Ω = (W T W) 1 2</formula><p>tr (W T W) </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.6">Implementation Details</head><p>In our full transfer learning model, we initialize the lookup table E with the pre-trained vectors from GloVe <ref type="bibr" target="#b24">[25]</ref> by setting l as 300. For BCNN, the window size and activation function are set to be 4 and ReLU, and the feature map sizes are set to be 50 and 100 for PI and NLI; for the two convolution layers of Pyramid in both PI and NLI, the feature map sizes are set to be 8 and 16, the strides are set to be 1 and 3, and the kernel sizes are set to be 6 × 6 and 4 × 4; for the two max-pooling layers of Pyramid, the strides are set to be 4 and 2, and the pooling sizes are set to be 4 × 4 and 2 × 2. Besides, for λ 0 and λ 1 , we set them as 0.05 and 0.0008; while for λ 2 , λ 3 , λ 4 and λ 5 , we set them as 0.0004. AdaGrad <ref type="bibr" target="#b9">[10]</ref> is used to train our model with an initial learning rate of 0.08.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">ONLINE SYSTEM</head><p>As introduced in Section 1, our online chatbot system is based on traditional information retrieval techniques, where the goal is to obtain the nearest question in the knowledge base for a given customer question <ref type="bibr" target="#b13">[14]</ref>. Fig. <ref type="figure">1</ref> depicts the whole system architecture.</p><p>Specifically, we first build an indexing for all the questions in our knowledge base (KB) using Apache Lucene<ref type="foot" target="#foot_1">5</ref> . Next, given a query question, we employ TF-IDF ranking algorithm <ref type="bibr" target="#b37">[38]</ref> in Lucene to compute its similarities to all the questions in the KB, and call back the top-K candidate questions. We then use a reranking algorithm to compute the similarities between the query and the K candidates, and obtain the most similar candidate. Finally we return the answer of the selected candidate to answer the query question. Note that in this paper, we only consider formulating our question rerank module as a PI task, but one can also model it as an NLI task.</p><p>Our existing reranking method is based on this ensemble method for the Answer Selection task <ref type="bibr" target="#b32">[33]</ref>. But instead of using the output of the time-consuming LSTM model, we feed another three features, namely, Word Mover's Distance <ref type="bibr" target="#b15">[16]</ref>, keywords features <ref type="bibr" target="#b32">[33]</ref> and the cosine distance of sentence embeddings <ref type="bibr" target="#b35">[36]</ref> to a gradient boosted regression tree (GBDT).</p><p>To combine our model with the existing ranking method, we treat the probability of being paraphrases predicted by our model as an additional feature, and feed all features to GBDT for reranking.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">EXPERIMENTS</head><p>In this section, we describe a qualitative evaluation of our proposed methods from the following perspectives: (1) From Section 4.1 to Section 4.4, we perform an intrinsic evaluation by utilizing a benchmark dataset and our own dataset to show the efficiency and effectiveness of our proposed base model and transfer learning framework; (2) In Section 4.5, we deploy our full model into our chatbot system, and conduct an extrinsic evaluation to show that our full model can bring in significant improvements to our existing online chatbots.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Experiment Settings</head><p>Datasets: In this section, we evaluate our methods on both Paraphrase Identification (PI) and Natural Language Inference (NLI).</p><p>For PI, we used a recently released large-scale dateset <ref type="foot" target="#foot_2">6</ref> by Quora as the source domain, and our E-commerce dataset as the target domain. Based on our historical data, we constructed a question answering KB, which consists of around 15,000 frequently asked QA pairs. To create labeled question pairs, we first collected all the query questions from the chat log of conversations between clients and our staff from May 22 to May 28, 2017. For each query question, we then used Lucene indexing to retrieve several of its similar questions, and obtained 45,075 question pairs. Finally, we asked a business analyst to annotate all the question pairs.</p><p>For NLI, we employed a large-scale multi-genre corpus <ref type="bibr" target="#b36">[37]</ref>, which contains an image captioning domain (SNLI) and another five distinct genres/domains about written and spoken English (MultiNLI).Since the number of sentence pairs in SNLI is much larger than that in the other five domains, we took SNLI as the source domain, and the others as the target domains.</p><p>Table <ref type="table" target="#tab_3">1</ref> and Table <ref type="table" target="#tab_4">2</ref> summarize the statistics of our datasets. Note that in Table <ref type="table" target="#tab_3">1</ref>, the number before and after the slash for Q-Q pairs denote respectively the total number of question pairs and the number of positive question pairs (i.e., paraphrases), while the two numbers for #Query-Q respectively denote the total number of query questions and the number of questions with paraphrasing candidates. Besides, for #Candi-Q, we refer to the average number of candidate questions for each query. Compared Methods: For base models, we compared our hCNN model with the following models:</p><p>• BCNN is the left component of our hCNN model, which incorporates element-wise comparisons on top of the base model proposed in <ref type="bibr" target="#b41">[42]</ref>. • Pyramid is the right component of our hCNN model based on sentence interactions as in <ref type="bibr" target="#b22">[23]</ref>. • ABCNN is the attention-based CNN model by <ref type="bibr" target="#b41">[42]</ref>.</p><p>• BiLSTM is similar to BCNN, but uses LSTM instead of CNN to encode each sentence as in <ref type="bibr" target="#b2">[3]</ref>. • ESIM is one of the state-of-the-art attention-based LSTM models on SNLI proposed by <ref type="bibr" target="#b5">[6]</ref>. • hCNN is our hybrid CNN model as introduced in Section 2.4.</p><p>For evaluating the proposed transfer learning framework, we employed the following compared systems:</p><p>• Tgt-Only is the baseline trained in the target domain.</p><p>• Src-Only is another baseline trained in the source domain.</p><p>• Mixed is to simply combine the labeled data in the two domains to train the hCNN model. • Fine-Tune is a widely used TL method, where we first train a model on the source data, and then use the learned parameters to initialize the model parameters for training another model on the target data.</p><p>• FS and SS are the fully-shared and specific-shared frameworks as detailed in Section 2.1. • DRSS is our proposed model of learning domain relationships based on SS as in Section 2.2. • SS-Adv and DRSS-Adv denote adding the adversarial loss into SS and DRSS as in Section 2.3.</p><p>All the methods in this paper are implemented with Tensorflow and are trained on machines with NVIDIA Tesla K40m GPU. Evaluation Metrics: For PI, since our goal is to retrieve the most similar candidate for each query question, we use our model to predict each candidate's probability of being paraphrase as its similarity score, and then rank all the candidates. To evaluate the ranking performance, we use Precision@1, Recall@1, F 1 @1 as metrics; to evaluate the classification performance for all question pairs, we employ two metrics: the Area under the Receiver Operating Characteristic curve (AUC) score <ref type="bibr" target="#b4">[5]</ref> and the classification accuracy (ACC). For NLI, we only use ACC as the evaluation metric.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Comparisons Between Base Models</head><p>In Table <ref type="table" target="#tab_5">3</ref>, we compared different models for classifying sentence pairs with hCNN in both efficiency and effectiveness. Note that to fairly evaluate the efficiency of each model, we compute the total time of predicting all the test sentence pairs on CPU by setting the mini-batch size to 1, and report the average time. Also, for feature map sizes in BCNN, ABCNN and the BCNN component in hCNN, we set them as 50 for PI and 300 for NLI.</p><p>First, we can find that LSTM-based methods are generally much slower than CNN-based methods. Especially for ESIM, although it can outperform all CNN-based models, its computational time for each sentence pair is 32.2ms for our dataset and 79.5ms for SNLI, which is 6-11 times of CNN-based models. This means that most existing state-of-the-art models can only support low QPS, and therefore hard to be applied to industry. Second, clearly for both tasks, hCNN performs better than the other CNN-based methods, which indicates that BCNN and Pyramid are complementary to each other, and can work better when combined. Moreover, we verified that the improvements of hCNN over the other methods are significant with p &lt; 0.05 based on McNemar's paired significance test <ref type="bibr" target="#b11">[12]</ref>. Finally, while the computational cost of hCNN is slightly higher than BCNN and Pyramid, it can still serve 233 question pairs per second, which is able to satisfy the current demand of our industrial bot.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Comparisons Between TL Methods</head><p>We further evaluated the performance of our transfer learning method in Table <ref type="table" target="#tab_6">4</ref> and Table <ref type="table" target="#tab_7">5</ref>.</p><p>We can observe from Table <ref type="table" target="#tab_7">5</ref> that for all the five target domains, Src-Only perform much worse than Tgt-Only, and the average performance of Mixed is even worse than Tgt-Only. This implies that the source domain is quite different from all the target domains, and simply mixing the training data in two domains may lead to the model overfitting the source data since |D s | is much larger than |D t |. In addition, it is observed that the widely used Fine-Tune method can perform slightly better than Tgt-Only in most cases, which shows that pre-training the model parameters on a related source domain is better than randomly initializing them. Moreover, Technical Presentation WSDM'18, February 5-9, 2018, Marina Del Rey, CA, USA  in all the five domains, the performance of two existing transfer learning frameworks FS and SS are both 1.9% better than that of Tgt-Only, which proves their usefulness. Furthermore, our proposed method DRSS improves the average performance of SS to 0.665, and the improvements are significant over all the tasks with p &lt; 0.05 based on McNemar's paired significance test. This suggests that capturing the relationship between domains is generally useful for transfer learning. Finally, we can see that the incorporation of adversarial loss into SS and DRSS further boosts their performance, and DRSS-Adv can achieve the best accuracy across all the methods. Similar trends can be also observed for the PI task from Table <ref type="table" target="#tab_6">4</ref>. Interestingly, by comparing Table <ref type="table" target="#tab_5">3</ref> and Table <ref type="table" target="#tab_6">4</ref>, we find that with the help of training data from the source domain, the performance of DRSS-Adv is even better than that of ESIM. These observations demonstrate the effectiveness of our transfer learning method. Apart from the effectiveness, we also measure the efficiency of each method. Since the first five methods only use a single hCNN model for prediction, the computational time is the same as hCNN. As for SS, DRSS and their adversarial extensions, the computational time is 6.9ms, which is slightly longer than the other five methods but still much shorter than LSTM-based methods as in Table <ref type="table" target="#tab_5">3</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Domain Relationships</head><p>After obtaining the covariance matrix Ω for each source/target pair, we can derive their corresponding correlation matrices. For better comparison, here we show the square root of the correlation matrices for DRSS.</p><p>As shown in Fig. <ref type="figure" target="#fig_5">4</ref> that across all the six source/target pairs, W sc and W tc are positively related with each other. This is intuitive as the shared network is supposed to learn shared features between the source and the target domains, thus the learned W sc and W tc should be close to each other. This also shows the learned correlation matrix helps to capture the inter-domain relationship between W sc and W tc .</p><p>In Fig. <ref type="figure" target="#fig_5">4</ref>, we can also see that for most source/target pairs except SNLI→Fict, the correlation between W s and W sc and that  between W t and W tc learnt by our model are with small values. This indicates that in most cases, the shared feature space and the domain-specific feature space learnt by SS tend to be different from each other, and our model can help to reveal such intra-domain relationships.</p><p>Finally, to help us get a deeper insight on the helpfulness of the adversarial training, we perform comparisons on the correlation matrices learnt by DRSS and DRSS-Adv. We first show the result of SNLI→Fict. in Table <ref type="table" target="#tab_8">6</ref>. As we can see, for DRSS, the correlation between W s (or W t ) and W sc (or W tc ) is relatively large, while for DRSS-Adv, the correlation is relatively small. For the other subtasks, we find that the learnt matrices of DRSS-Adv are similar to those of DRSS, but we still observe that the intra-domain correlations of DRSS-Adv are generally smaller than those of DRSS. This shows that adding the adversarial loss can encourage the shared feature space to capture more domain-independent features, and further make the shared and domain-specific feature spaces more different. Therefore, the adversarial training can lead our model to better satisfy our assumption on the domain relationships, and finally improve the performance. All the above observations demonstrate that our model can capture the inter-domain and intra-domain relationship as mentioned in Section 2.2.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5">Extrinsic Evaluations</head><p>As mentioned in Section 3, for the online reranking algorithm, we propose to train GBDT by treating the prediction score of our DRSS model as another feature. To achieve this, we first took out the prediction scores of our DRSS model on the validation set. Then, we combined them together the other features as introduced in Section 3, and trained GBDT on the validation set. The model performance on the test set is reported in Table <ref type="table" target="#tab_9">7</ref>. Note that the test time here denotes the average serving time (including the Response Time), which is different from the reported test time in Table <ref type="table" target="#tab_5">3</ref>.  As we can see from the offline test, the GBDT model with the feature derived from our DRSS model (referred to as GBDT-DRSS) is respectively 26.3% and 7.1% better than our existing online model (referred to as GBDT) and the GBDT model with the feature derived from hCNN (refered to as GBDT-hCNN) in F 1 @1. Although adding our DRSS feature leads to more computational time, the total prediction time is 80.7ms for each query question (i.e., QPS of 12), which is acceptable for our chatbots.</p><p>For online serving, to accelerate the computation, we set the number of candidates returned by Lucene as 30, and bundle the 30 candidates into a mini-batch to feed into our model for prediction. For online evaluation, we randomly sampled 2750 questions, where 1317 questions are answered by GBDT and 1433 questions are answered by GBDT-DRSS. Then, we asked one business analyst to annotate if the nearest question returned by models expresses the same meaning as the query question, and compared their precision at top-1. As shown in Table <ref type="table" target="#tab_9">7</ref>, the Prec@1 of GBDT-DRSS is 18.8% higher than that of GBDT.  <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b28">29,</ref><ref type="bibr" target="#b41">42]</ref>. Although all these models have been shown</p><p>to significantly outperform the traditional methods without deep learning, most of them only focus on improving the performance of standard in-domain setting, and therefore require a large amount of labeled data to train a robust model. However, in practice, it will be time-consuming and costly to manually annotate much labeled data for each target domain we are interested in. Hence, in this paper, our focus is to apply an efficient and effective NN model into a transfer learning framework so that we can leverage the large amount of labeled data from a related source domain to train a robust model for a resource-poor target domain, which can benefit our chatbot-based question answering system. Transfer Learning: Transfer learning (TL) has been extensively studied in the last decade <ref type="bibr" target="#b18">[19]</ref>. Most existing studies for TL can be generally categorized into two groups. The first line of work assumes that we have enough labeled data from a source domain and also a little labeled data from a target domain <ref type="bibr" target="#b8">[9]</ref>, and the second line assumes that we only have labeled data from source domain but may also have some unlabeled data from a target domain <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b43">44]</ref>. Our study belongs to the first line of work, which is also referred to as supervised domain adaptation.</p><p>For supervised domain adaptation, a majority of previous work belong to two clusters: instance-based and feature-based transfer learning. The former focuses on mining from the source labeled data to find those instances that are similar to the distribution of the target domain, and combine them together with the target labeled data <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b14">15]</ref>. The core idea of the latter line of work is to find a shared feature space, which can reduce the divergence between the distribution of the source and the target domains <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b16">17,</ref><ref type="bibr" target="#b26">27,</ref><ref type="bibr" target="#b31">32]</ref>. Our work follows the latter one, and tries to leverage NN models to learn a shared hidden representation for sentence pairs across domains.</p><p>Deep Transfer Learning: With the recent advances of deep learning, different NN-based TL frameworks have been proposed for image processing <ref type="bibr" target="#b42">[43]</ref> and speech recognition <ref type="bibr" target="#b33">[34]</ref> as well as NLP <ref type="bibr" target="#b17">[18,</ref><ref type="bibr" target="#b20">21,</ref><ref type="bibr" target="#b39">40]</ref>. A simple but widely used framework is referred to as fine-tuning approaches, which first use the parameters of the well trained models on the source domain to initialize the model parameters of the target domain, and then fine tune the parameters based on labeled data in the target domain <ref type="bibr" target="#b20">[21,</ref><ref type="bibr" target="#b42">43]</ref>. Another line of work can be referred to as multi-task feature learning approaches, which bears the same intuition behind the feature-transfer methods as mentioned above. Among this line of work, one typical framework is to simply use a shared NN to learn a shared feature Technical Presentation WSDM'18, February 5-9, 2018, Marina Del Rey, CA, USA space <ref type="bibr" target="#b20">[21,</ref><ref type="bibr" target="#b39">40]</ref>, while another representative framework is to employ a shared NN and two domain-specific NNs to respectively derive a shared feature space and two domain-specific feature space <ref type="bibr" target="#b17">[18]</ref>. Motivated by the observation that both methods fail to consider the domain relationship, in this paper, we propose to jointly learn the shared feature representations and domain relationships in a unified model. Moreover, inspired by the recent success of applying adversarial networks into unsupervised domain adaptation <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b29">30]</ref> and multi-task learning <ref type="bibr" target="#b17">[18]</ref>, we also incorporate the adversarial training into our transfer learning model in order to learn a more robust shared feature space across domains.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">CONCLUSIONS</head><p>In this paper, we systematically evaluated different base methods and transfer learning techniques for modelling sentence pairs, with the goal of proposing an effective and efficient transfer learning framework for PI and NLI. Specifically, we first proposed a hybrid CNN model on the basis of two existing and then further proposed a general transfer learning framework, which can simultaneously perform the shared feature learning and domain relationship learning in an end-to-end mode. Evaluations on both a benchmark dataset and our own dataset showed that (1) our hybrid CNN model is both effective and efficient in comparison with several representative models; (2) our transfer learning framework can outperform all the existing frameworks across six source/target pairs. We further deployed our transfer learning model in our online chatbot system, and showed that it can improve the performance of the existing system by a large margin.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: Existing Transfer Learning Frameworks.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head></head><label></label><figDesc>where W sc ∈ R |Y |×q and W tc ∈ R |Y |×q are weight matrices and b s ∈ R |Y | and b t ∈ R |Y | are bias vectors.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: Our Full Transfer Learning Model for Paraphrase Identification and Natural Language Inference. Algorithm 1 Training Procedure for our Full Model 1: Input: source training instances D s , target training instancesD t (|D s | ≫ |D t |) . 2: Output: Ω, W, W d , b s , b t , b d , Θ c , Θ s , Θ t 3: Initialize W, W d , b s , b t , b d , Θ c , Θ s , Θ t withrandom values 4: Initialize Ω = 1 4 I 4 , where I is an identity matrix 5: epoch = 0 6: while epoch ≤ MaxEpoch do</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>random values 4 : 8 :</head><label>48</label><figDesc>Initialize Ω = 1 4 I 4 , where I is an identity matrix 5: epoch = 0 6: while epoch ≤ MaxEpoch do 7: c s , c t = 0, 0 while c s ≤ src_batches do 9: read the c s th mini-batch from the source domain 10: Update Θ c , Θ s , W, W d , b s and b d</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 4 :</head><label>4</label><figDesc>Figure 4: Learnt Correlation Matrix. A darker color means a larger entry value. S:Source, T:Target, SC:Source-shared, TC: Target-shared.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 1 :</head><label>1</label><figDesc>Statistics of Paraphrase Identification Datasets</figDesc><table><row><cell></cell><cell></cell><cell>Train</cell><cell>Dev</cell><cell>Test</cell></row><row><cell></cell><cell>Q-Q Pairs</cell><cell>29,884/8,624</cell><cell cols="2">7,622/1,968 7,569/2,133</cell></row><row><cell></cell><cell>#Query-Q</cell><cell>3202/2414</cell><cell>781/578</cell><cell>777/584</cell></row><row><cell>AliExpress</cell><cell>#Candi-Q</cell><cell>9.33</cell><cell>9.76</cell><cell>9.74</cell></row><row><cell></cell><cell cols="2">#words per Query-Q 11.71</cell><cell>11.73</cell><cell>12.04</cell></row><row><cell></cell><cell cols="2">#words per Candi-Q 8.37</cell><cell>8.37</cell><cell>8.54</cell></row><row><cell>Quora</cell><cell>Q-Q Pairs</cell><cell cols="2">404,290/149,265 N.A.</cell><cell>N.A.</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 2 :</head><label>2</label><figDesc>Statistics of Natural Language Inference Datesets</figDesc><table><row><cell>SNLI</cell><cell cols="2">Fiction Travel Slate Telephone Government</cell></row><row><cell cols="2">Train 550,125 77,348 77,350 77,306 83,348</cell><cell>77,350</cell></row><row><cell cols="2">Dev 10,000 2,000 2,000 2,000 2,000</cell><cell>2,000</cell></row><row><cell cols="2">Test 10,000 2,000 2,000 2,000 2,000</cell><cell>2,000</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 3 :</head><label>3</label><figDesc>A Comparison Between Different Base Models</figDesc><table><row><cell></cell><cell></cell><cell cols="2">E-Commerce</cell><cell></cell><cell>SNLI</cell></row><row><cell></cell><cell cols="5">AUC ACC Test Time(ms) ACC Test Time(ms)</cell></row><row><cell>BCNN</cell><cell>81.0</cell><cell>77.5</cell><cell>2.8</cell><cell>81.0</cell><cell>3.3</cell></row><row><cell cols="2">Pyramid 77.8</cell><cell>77.0</cell><cell>3.8</cell><cell>77.7</cell><cell>5.3</cell></row><row><cell cols="2">ABCNN 81.0</cell><cell>78.2</cell><cell>5.2</cell><cell>81.8</cell><cell>12.3</cell></row><row><cell>hCNN</cell><cell cols="3">82.2 † 79.2 † 4.3</cell><cell cols="2">83.2 † 6.4</cell></row><row><cell cols="2">BiLSTM 79.9</cell><cell>77.8</cell><cell>7.1</cell><cell>80.6</cell><cell>19.6</cell></row><row><cell>ESIM</cell><cell>84.2</cell><cell>79.8</cell><cell>32.2</cell><cell>86.7</cell><cell>79.5</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 4 :</head><label>4</label><figDesc>The Result of Paraphrase Identification Task</figDesc><table><row><cell></cell><cell cols="3">Prec@1 Rec@1 F1@1</cell><cell>ACC</cell><cell>AUC</cell></row><row><cell>Tgt-Only</cell><cell>0.717</cell><cell>0.551</cell><cell>0.623</cell><cell>0.792</cell><cell>0.822</cell></row><row><cell>Src-Only</cell><cell>0.619</cell><cell>0.368</cell><cell>0.461</cell><cell>0.719</cell><cell>0.686</cell></row><row><cell>Mixed</cell><cell>0.735</cell><cell>0.532</cell><cell>0.618</cell><cell>0.788</cell><cell>0.810</cell></row><row><cell>Fine-Tune</cell><cell>0.713</cell><cell>0.567</cell><cell>0.632</cell><cell>0.790</cell><cell>0.825</cell></row><row><cell>FS</cell><cell>0.734</cell><cell>0.595</cell><cell>0.657</cell><cell>0.797</cell><cell>0.831</cell></row><row><cell>SS</cell><cell>0.744</cell><cell>0.601</cell><cell>0.665</cell><cell>0.800</cell><cell>0.837</cell></row><row><cell>SS-Adv</cell><cell>0.743</cell><cell>0.603</cell><cell>0.666</cell><cell>0.808</cell><cell>0.842</cell></row><row><cell>DRSS</cell><cell>0.757</cell><cell>0.608</cell><cell>0.674 †</cell><cell cols="2">0.812 † 0.847</cell></row><row><cell cols="2">DRSS-Adv 0.753</cell><cell>0.620</cell><cell cols="2">0.680 † 0.809</cell><cell>0.849</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 5 :</head><label>5</label><figDesc>The Classification Result of NLI Task</figDesc><table><row><cell></cell><cell>Fict.</cell><cell>Trav.</cell><cell>Gov.</cell><cell>Tele.</cell><cell>Slate</cell><cell>AVG</cell></row><row><cell>Tgt-Only</cell><cell>0.647</cell><cell>0.658</cell><cell>0.692</cell><cell>0.644</cell><cell>0.579</cell><cell>0.644</cell></row><row><cell>Src-Only</cell><cell>0.520</cell><cell>0.516</cell><cell>0.540</cell><cell>0.520</cell><cell>0.488</cell><cell>0.517</cell></row><row><cell>Mixed</cell><cell>0.647</cell><cell>0.647</cell><cell>0.675</cell><cell>0.648</cell><cell>0.580</cell><cell>0.639</cell></row><row><cell>Fine-Tune</cell><cell>0.653</cell><cell>0.652</cell><cell>0.684</cell><cell>0.651</cell><cell>0.591</cell><cell>0.646</cell></row><row><cell>FS</cell><cell>0.662</cell><cell>0.671</cell><cell>0.704</cell><cell>0.657</cell><cell>0.588</cell><cell>0.656</cell></row><row><cell>SS</cell><cell>0.653</cell><cell>0.668</cell><cell>0.700</cell><cell>0.668</cell><cell>0.592</cell><cell>0.656</cell></row><row><cell>SS-Adv</cell><cell>0.666</cell><cell>0.666</cell><cell>0.701</cell><cell>0.664</cell><cell>0.597</cell><cell>0.659</cell></row><row><cell>DRSS</cell><cell>0.665</cell><cell cols="2">0.674 † 0.706 †</cell><cell>0.673 †</cell><cell>0.605 †</cell><cell>0.665</cell></row><row><cell cols="3">DRSS-Adv 0.676 † 0.673</cell><cell cols="4">0.707 † 0.675 † 0.607 † 0.668</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>Table 6 :</head><label>6</label><figDesc>Correlation Matrices on SNLI→Fict.</figDesc><table><row><cell></cell><cell></cell><cell cols="2">DRSS</cell><cell></cell><cell></cell><cell cols="2">DRSS-Adv</cell></row><row><cell></cell><cell>W s</cell><cell>W sc</cell><cell>W t</cell><cell>W tc</cell><cell>W s</cell><cell>W sc</cell><cell>W t</cell><cell>W tc</cell></row><row><cell>W s</cell><cell cols="8">1.000 0.242 0.101 0.205 1.000 0.090 0.055 0.062</cell></row><row><cell cols="9">W sc 0.242 1.000 0.008 0.247 0.090 1.000 0.024 0.221</cell></row><row><cell>W t</cell><cell cols="8">0.101 0.008 1.000 0.127 0.055 0.024 1.000 0.043</cell></row><row><cell cols="9">W tc 0.205 0.247 0.127 1.000 0.062 0.221 0.043 1.000</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head>Table 7 :</head><label>7</label><figDesc>The Performance of Online Serving</figDesc><table><row><cell></cell><cell cols="2">Offline</cell><cell>Online Evaluation</cell></row><row><cell></cell><cell cols="2">F 1 @1 Time(ms per query)</cell><cell>Prec@1</cell></row><row><cell>GBDT</cell><cell>0.539</cell><cell>20.1</cell><cell>0.614</cell></row><row><cell>GBDT-hCNN</cell><cell>0.636</cell><cell>69.9</cell><cell>-</cell></row><row><cell>GBDT-DRSS</cell><cell>0.681</cell><cell>80.7</cell><cell>0.729</cell></row><row><cell cols="2">5 RELATED WORK</cell><cell></cell><cell></cell></row><row><cell cols="4">Paraphrase Identification and Natural Language Inference:</cell></row><row><cell cols="4">Recent have witnessed great successes of applying different</cell></row><row><cell cols="4">neural networks, including Recursive Neural Networks (ReNN),</cell></row><row><cell cols="4">Reccurrent Neural Networks (RNN) and Convolutional Neural Net-</cell></row><row><cell cols="4">works (CNN), into Paraphrase Identification and Natural Language</cell></row><row><cell>Inference</cell><cell></cell><cell></cell><cell></cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4" xml:id="foot_0"><p>Eva can be accessed via the following link: https://gcx.aliexpress.com/ae/evaenglish/ portal.htm?pageId=195440 Therefore, the output layers are defined as:p(y | z c , z s ) = softmax(W sc z c + W s z s + b s ) if y is source label, softmax(W tc z c + W t z t + b t ) if y is target label.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="5" xml:id="foot_1"><p>https://lucene.apache.org/core/</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="6" xml:id="foot_2"><p>https://www.kaggle.com/c/quora-question-pairs</p></note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>ACKNOWLEDGEMENTS</head><p>The authors would like to give great thanks to Feng Ji, Wei Zhou, Weipeng Zhao and Xu Hu for their helpfulness during the project, and the anonymous reviewers for their constructive comments.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Multitask feature learning</title>
		<author>
			<persName><forename type="first">Andreas</forename><surname>Argyriou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Theodoros</forename><surname>Evgeniou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Massimiliano</forename><surname>Pontil</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Domain Adaptation with Structural Correspondence Learning</title>
		<author>
			<persName><forename type="first">John</forename><surname>Blitzer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ryan</forename><surname>Mcdonald</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fernando</forename><surname>Pereira</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP</title>
		<imprint>
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">A large annotated corpus for learning natural language inference</title>
		<author>
			<persName><forename type="first">R</forename><surname>Samuel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gabor</forename><surname>Bowman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christopher</forename><surname>Angeli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christopher</forename><forename type="middle">D</forename><surname>Potts</surname></persName>
		</author>
		<author>
			<persName><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP</title>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">A Fast Unified Model for Parsing and Sentence Understanding</title>
		<author>
			<persName><forename type="first">R</forename><surname>Samuel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jon</forename><surname>Bowman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Abhinav</forename><surname>Gauthier</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Raghav</forename><surname>Rastogi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christopher</forename><forename type="middle">D</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christopher</forename><surname>Manning</surname></persName>
		</author>
		<author>
			<persName><surname>Potts</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL</title>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">The use of the area under the ROC curve in the evaluation of machine learning algorithms</title>
		<author>
			<persName><forename type="first">P</forename><surname>Andrew</surname></persName>
		</author>
		<author>
			<persName><surname>Bradley</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pattern recognition</title>
		<imprint>
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="page" from="1145" to="1159" />
			<date type="published" when="1997">1997. 1997</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Enhanced LSTM for Natural Language Inference</title>
		<author>
			<persName><forename type="first">Qian</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaodan</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhen-Hua</forename><surname>Ling</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Si</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hui</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Diana</forename><surname>Inkpen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL</title>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">SuperAgent: A Customer Service Chatbot for E-commerce Websites</title>
		<author>
			<persName><forename type="first">Lei</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shaohan</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Furu</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chuanqi</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chaoqun</forename><surname>Duan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ming</forename><surname>Zhou</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017">2017. 2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Boosting for transfer learning</title>
		<author>
			<persName><forename type="first">Wenyuan</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qiang</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gui-Rong</forename><surname>Xue</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yong</forename><surname>Yu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Frustratingly Easy Domain Adaptation</title>
		<author>
			<persName><forename type="first">Hal</forename><surname>Daume</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Iii</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL</title>
		<imprint>
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Adaptive subgradient methods for online learning and stochastic optimization</title>
		<author>
			<persName><forename type="first">John</forename><surname>Duchi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Elad</forename><surname>Hazan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yoram</forename><surname>Singer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="page" from="2121" to="2159" />
			<date type="published" when="2011-07">2011. Jul (2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Domain-adversarial training of neural networks</title>
		<author>
			<persName><forename type="first">Yaroslav</forename><surname>Ganin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Evgeniya</forename><surname>Ustinova</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hana</forename><surname>Ajakan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pascal</forename><surname>Germain</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hugo</forename><surname>Larochelle</surname></persName>
		</author>
		<author>
			<persName><forename type="first">François</forename><surname>Laviolette</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mario</forename><surname>Marchand</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Victor</forename><surname>Lempitsky</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">17</biblScope>
			<biblScope unit="page" from="1" to="35" />
			<date type="published" when="2016">2016. 2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">Some statistical issues in the comparison of speech recognition algorithms</title>
		<author>
			<persName><forename type="first">Laurence</forename><surname>Gillick</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stephen</forename><forename type="middle">J</forename><surname>Cox</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1989">1989</date>
		</imprint>
	</monogr>
	<note>In ICASSP</note>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Convolutional neural network architectures for matching natural language sentences</title>
		<author>
			<persName><forename type="first">Baotian</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhengdong</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hang</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qingcai</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Finding similar questions in large question and answer archives</title>
		<author>
			<persName><forename type="first">Jiwoon</forename><surname>Jeon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bruce</forename><surname>Croft</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Joon</forename><surname>Ho</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lee</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CIKM</title>
		<imprint>
			<date type="published" when="2005">2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Instance Weighting for Domain Adaptation in NLP</title>
		<author>
			<persName><forename type="first">Jing</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chengxiang</forename><surname>Zhai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL</title>
		<imprint>
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">From word embeddings to document distances</title>
		<author>
			<persName><forename type="first">Matt</forename><surname>Kusner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yu</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nicholas</forename><surname>Kolkin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kilian</forename><surname>Weinberger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Learning a meta-level prior for feature relevance from multiple related tasks</title>
		<author>
			<persName><forename type="first">Su-In</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vassil</forename><surname>Chatalbashev</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><surname>Vickrey</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Daphne</forename><surname>Koller</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Adversarial Multi-task Learning for Text Classification</title>
		<author>
			<persName><forename type="first">Pengfei</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xipeng</forename><surname>Qiu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xuanjing</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL</title>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Adaptation regularization: A general framework for transfer learning</title>
		<author>
			<persName><forename type="first">Mingsheng</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jianmin</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Guiguang</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sinno</forename><surname>Jialin Pan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Philip</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Knowledge and Data Engineering</title>
		<imprint>
			<biblScope unit="volume">26</biblScope>
			<biblScope unit="page" from="1076" to="1089" />
			<date type="published" when="2014">2014. 2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Natural Language Inference by Tree-Based Convolution and Heuristic Matching</title>
		<author>
			<persName><forename type="first">Lili</forename><surname>Mou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rui</forename><surname>Men</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ge</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yan</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rui</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhi</forename><surname>Jin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL</title>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">How Transferable are Neural Networks in NLP Applications?</title>
		<author>
			<persName><forename type="first">Lili</forename><surname>Mou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhao</forename><surname>Meng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rui</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ge</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yan</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhi</forename><surname>Jin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP</title>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">A survey on transfer learning</title>
		<author>
			<persName><forename type="first">Jialin</forename><surname>Sinno</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qiang</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on knowledge and data engineering</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="page" from="1345" to="1359" />
			<date type="published" when="2010">2010. 2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Text Matching as Image Recognition</title>
		<author>
			<persName><forename type="first">Liang</forename><surname>Pang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yanyan</forename><surname>Lan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiafeng</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jun</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shengxian</forename><surname>Wan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xueqi</forename><surname>Cheng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">A Decomposable Attention Model for Natural Language Inference</title>
		<author>
			<persName><forename type="first">Ankur</forename><surname>Parikh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Oscar</forename><surname>Täckström</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dipanjan</forename><surname>Das</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jakob</forename><surname>Uszkoreit</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP</title>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">GloVe: Global Vectors for Word Representation</title>
		<author>
			<persName><forename type="first">Jeffrey</forename><surname>Pennington</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christopher</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP</title>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">AliMe Chat: A Sequence to Sequence and Rerank based Chatbot Engine</title>
		<author>
			<persName><forename type="first">Minghui</forename><surname>Qiu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Feng-Lin</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Siyu</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xing</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yan</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Weipeng</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Haiqing</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jun</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wei</forename><surname>Chu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL</title>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">A Short-Term Rainfall Prediction Model using Multi-Task Convolutional Neural Networks</title>
		<author>
			<persName><forename type="first">Minghui</forename><surname>Qiu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Peilin</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ke</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jun</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xing</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaoguang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wei</forename><surname>Chu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICDM</title>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Reasoning about entailment with neural attention</title>
		<author>
			<persName><forename type="first">Tim</forename><surname>Rocktäschel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Edward</forename><surname>Grefenstette</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Karl</forename><forename type="middle">Moritz</forename><surname>Hermann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tomáš</forename><surname>Kočiskỳ</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Phil</forename><surname>Blunsom</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ICLR</title>
		<imprint>
			<date type="published" when="2015">2015. 2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Dynamic pooling and unfolding recursive autoencoders for paraphrase detection</title>
		<author>
			<persName><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Eric</forename><forename type="middle">H</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jeffrey</forename><surname>Pennin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christopher</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrew</forename><forename type="middle">Y</forename><surname>Ng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Unsupervised cross-domain image generation</title>
		<author>
			<persName><forename type="first">Yaniv</forename><surname>Taigman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Adam</forename><surname>Polyak</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lior</forename><surname>Wolf</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ICLR</title>
		<imprint>
			<date type="published" when="2017">2017. 2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
		<author>
			<persName><forename type="first">Oriol</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Quoc</forename><surname>Le</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1506.05869</idno>
		<title level="m">A neural conversational model</title>
		<imprint>
			<date type="published" when="2015">2015. 2015</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Manifold alignment using procrustes analysis</title>
		<author>
			<persName><forename type="first">Chang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sridhar</forename><surname>Mahadevan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">A Long Short-Term Memory Model for Answer Sentence Selection in Question Answering</title>
		<author>
			<persName><forename type="first">Di</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Eric</forename><surname>Nyberg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL-IJCNLP</title>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Transfer learning for speech and language processing</title>
		<author>
			<persName><forename type="first">Dong</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Thomas</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zheng</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Signal and Information Processing Association Annual Summit and Conference (APSIPA)</title>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">A Compare-Aggregate Model for Matching Text Sequences</title>
		<author>
			<persName><forename type="first">Shuohang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jing</forename><surname>Jiang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ICLR</title>
		<imprint>
			<date type="published" when="2017">2017. 2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Towards universal paraphrastic sentence embeddings</title>
		<author>
			<persName><forename type="first">John</forename><surname>Wieting</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mohit</forename><surname>Bansal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kevin</forename><surname>Gimpel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Karen</forename><surname>Livescu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ICLR</title>
		<imprint>
			<date type="published" when="2016">2016. 2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<monogr>
		<author>
			<persName><forename type="first">Adina</forename><surname>Williams</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nikita</forename><surname>Nangia</surname></persName>
		</author>
		<author>
			<persName><surname>Samuel R Bowman</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1704.05426</idno>
		<title level="m">A Broad-Coverage Challenge Corpus for Sentence Understanding through Inference</title>
		<imprint>
			<date type="published" when="2017">2017. 2017</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Interpreting tf-idf term weights as making relevance decisions</title>
		<author>
			<persName><forename type="first">Chung</forename><surname>Ho</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Robert</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pong</forename><surname>Wing</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kam</forename><forename type="middle">Fai</forename><surname>Luk</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kui</forename><forename type="middle">Lam</forename><surname>Wong</surname></persName>
		</author>
		<author>
			<persName><surname>Kwok</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Transactions on Information Systems (TOIS)</title>
		<imprint>
			<biblScope unit="volume">26</biblScope>
			<biblScope unit="page">13</biblScope>
			<date type="published" when="2008">2008. 2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Learning to respond with deep neural networks for retrieval-based human-computer conversation system</title>
		<author>
			<persName><forename type="first">Rui</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yiping</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hua</forename><surname>Wu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SIGIR</title>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Transfer learning for sequence tagging with hierarchical recurrent networks</title>
		<author>
			<persName><forename type="first">Zhilin</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ruslan</forename><surname>Salakhutdinov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">William</forename><forename type="middle">W</forename><surname>Cohen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ICLR</title>
		<imprint>
			<date type="published" when="2017">2017. 2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Convolutional Neural Network for Paraphrase Identification</title>
		<author>
			<persName><forename type="first">Wenpeng</forename><surname>Yin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hinrich</forename><surname>Schütze</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NAACL-HLT</title>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">ABCNN: Attention-Based Convolutional Neural Network for Modeling Sentence Pairs</title>
		<author>
			<persName><forename type="first">Wenpeng</forename><surname>Yin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hinrich</forename><surname>Schütze</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bing</forename><surname>Xiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bowen</forename><surname>Zhou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Transactions of ACL</title>
		<imprint>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page" from="259" to="272" />
			<date type="published" when="2016">2016. 2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">How transferable are features in deep neural networks?</title>
		<author>
			<persName><forename type="first">Jason</forename><surname>Yosinski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jeff</forename><surname>Clune</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hod</forename><surname>Lipson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Learning Sentence Embeddings with Auxiliary Tasks for Cross-Domain Sentiment Classification</title>
		<author>
			<persName><forename type="first">Jianfei</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jing</forename><surname>Jiang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP</title>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">A convex formulation for learning task relationships in multi-task learning</title>
		<author>
			<persName><forename type="first">Yu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dit-Yan</forename><surname>Yeung</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">UAI</title>
		<imprint>
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
