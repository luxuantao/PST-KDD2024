<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">A Prediction System for Web Requests using N-gram Sequence Models</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Zhong</forename><surname>Su</surname></persName>
							<email>suzhong-bj@hotmail.com</email>
						</author>
						<author>
							<persName><forename type="first">Qiang</forename><surname>Yang</surname></persName>
							<email>qyang@cs.sfu.ca</email>
						</author>
						<author>
							<persName><forename type="first">Ye</forename><surname>Lu</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Hongjang</forename><surname>Zhang</surname></persName>
							<email>hjzhang@microsoft.com</email>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="department">Department of Computing Science and Technology</orgName>
								<orgName type="institution">Tsinghua University</orgName>
								<address>
									<postCode>100084</postCode>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff1">
								<orgName type="department">School of Computing Science</orgName>
								<orgName type="laboratory">Microsoft Research China 5F</orgName>
								<orgName type="institution">Simon Fraser University Burnaby BC</orgName>
								<address>
									<addrLine>Beijing Sigma Center #49, Zhichun Road</addrLine>
									<postCode>V5A 1S6</postCode>
									<settlement>Haidian</settlement>
									<country>Canada District</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff2">
								<address>
									<postCode>100080</postCode>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">A Prediction System for Web Requests using N-gram Sequence Models</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">ADBC8C3A0D977AC5EB82DE28BCDD4F8F</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.3" ident="GROBID" when="2023-07-27T09:16+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>As an increasing number of users access information on the web, there is a great opportunity to learn from the server logs to learn about the users' probable actions in the future. In this paper, we present an n-gram based model to utilize path profiles of users from very large data sets to predict the users' future requests. Since this is a prediction system, we cannot measure the recall in a traditional sense. We, therefore, present the notion of applicability to give a measure of the ability to predict the next document. Our model is based on a simple extension of existing pointbased models for such predictions, but our results show for n-gram based prediction when n is greater than three, we can increase precision by 20% or more for two realistic web lops. Also we present an efficient method that can compress our model to 30% of its original size so that the model can be loaded in main memory. Our result can potentially be applied to a wide range of applications on the web, including pre-sending, pre-fetching, enhancement of recommendation systems as well as web caching policies. Our tests are based on three realistic web logs. Our algorithm is implemented in a prediction system called WhatNext, which shows a marked improvement in precision and applicability over previous approaches.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Ke yw or&amp;</head><p>N-gram model, Web data mining, prediction * This work was performed while the author was visiting Microsoft Research China in Beijing.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">INTRODUCTION</head><p>The Internet provides a rich environment for users to retrieve information. At the same time, it also makes it easy for a user to get lost in the sea of information. One way to assist the user with their informational need is to predict a user's future request and use the prediction for pre-fetching, pre-sending, caching and recommendation. Prediction is increasingly feasible to do as more information is tracked through search engines and web servers. The purpose of this paper is to explore ways to exploit the information from web logs for predicting users' actions on the web.</p><p>There are generally two types of information source available: query log and server log. A query log tracks users' queries while server log tracks a user's browsing activities on a server. In this paper, we discuss how to exploit server logs of users for the purpose of prediction.</p><p>There has been an increasing amount of work on prediction models on the web. In the past, web-log based inference has been focused on prediction models that make best guesses on the user's next actions based on their previous ones. Much work has been done on recommendation systems and pre-sending systems.</p><p>Recommendation systems rely on a prediction model to make inferences on users <ref type="bibr">'</ref> interests based upon which to make recommendations.</p><p>Examples are the WebWatcher <ref type="bibr" target="#b4">[6]</ref> system and Letzia[lO] system. Pre-sending systems go a step further --they focus on making use of the predictions to send documents ahead of time. Accurate prediction can potentially shorten the users' access times and reduce network traffic when pre-sending is handled correctly.</p><p>Prediction models can be either point based or path based. Point-based prediction models are built on actions that are indexed on time instants and are used to predict the user's next action based on the currently observed action. These models draw on relatively small amount of information from each session and therefore the prediction can potentially be rather inaccurate. For examples, the best model[ I][ 161 predicts, for a confidence measure of over 50%, future documents with an accuracy of only around 30%. There has been relatively little work on path-based models in the past. These models are built based on the user's previous path data, and can potentially be more accurate. But the general belief is that they may suffer from much lower recall because sequences with long length are rare. The aim of this paper is to dispel this myth and show that with large enough web access logs one can build an accurate enough prediction models that also come with high recall.</p><p>In this paper, we present a simple probabilistic path-based prediction model that is inspired by n-gram prediction models commonly used in speech-processing communities <ref type="bibr" target="#b7">[9]</ref>. We have found that when using 3-gram and above the prediction accuracy is increased substantially whereas there is only a moderate decrease in applicability. We present a combined approach where multiple high-order n-gram models are organized in a step-wise manner. Experiments show that this approach achieves a reasonable balance between precision and applicability. Our work assumes very little knowledge about users and target pages while providing high accuracy and maintaining relatively high applicability. The system assumes no knowledge of user profiles as the ones required by Syskill and Webert [12], and no knowledge about linkage structures of web sites as required by Webwatcher. It's only requirement is that user sessions in web access can be logged successfully, a requirement realistic enough to apply to a wide range of domains.</p><p>Our assumptions, hypothesis and goal may be summarized as follows: 0 We assume that we are given a list of sequences of user clicks that correspond to visits to URLs' on the web;</p><p>We assume that the log resides on the server side and that we can observe users' requests made on the server;</p><p>We assume that we have methods of identifying users such that each sequence in the server log corresponds to a unique ID (which may not be a user ID); 0 0 0 We hypothesize that the users' short request sequences generally correspond to unpredictable requests. We thus only make predictions on users' next actions based on long enough sequences of user requests;</p><p>Given the above, our task is to predict the next user request when the users have made long enough sequences of requests.</p><p>This paper is organized as follows. Section 2 presents the algorithms for the construction of path-based models. Section 3 presents the prediction algorithms. Section 4 evaluates the performance of the proposed algorithms.</p><p>Section 5 discusses related work. Finally, section 6</p><p>provides a summary of this work.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Path Based Model</head><p>Our path-based model is built on a web-server log file L.</p><p>We consider L to be preprocessed into a collection of user sessions, such that each session is indexed by a unique user id and starting time. Each session is a sequence of requests where each request corresponds to a visit to a web page (an U=).</p><p>For brevity, we represent each request as an alphabet. The log L then consists of a set of sessions.</p><p>Our algorithm builds an n-gram prediction model based on the occurrence frequency. Each sub-string of length n is an n-gram. These sub-strings serve as the indices of a count table T. During its operation, the algorithm scans through all sub-strings exactly once, recording occurrence frequencies of the next click immediately after the substring in all sessions. The maximum occurred request is used as the prediction for the sub-string. The algorithm for building a path-based model on sub-strings is described below.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Algorithm PathModelConstruction (n: length of n-gram: L: log file of sessions: T: table indexed by all n-grams of all sessions in L)</head><p>Begin L := Filter(L); // we will explain how to filter log file L later: </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>T[] := 0; //Initialize Table T to zero for all n-grams</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>HI] := 0; // result of the model is stored in hash table H(), indexed on n-grams</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Max[] := 0 // Max[] records the maximum count for each n-gram</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Prediction Algorithm N-Gram</head><p>Based on the n-gram prediction model constructed out of the log data, we can then make predictions on a user's clicks in real time. Let Hi() be the prediction model built on i-gram model. Our algorithm is as follows:</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Prediction</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>End</head><p>In this algorithm, Filter(L) removes all requests with low visiting frequency according to a certain threshold 8.  As an example, assume that we have built up 3-gram and 2gram models as H3 and H2 in the last section. Suppose that we observe that the current clicking sequence consists of only one click "DBC'. In this case, the prediction algorithm checks H3 first to see if an index "DBC" exists. It finds out that the index does not exist. Therefore, it checks the 2-gram model H2 for the index "BC', which exists, thus the predicted next click is "D', according to H2.</p><p>In the evaluation of the algorithm, we use the following measures. Let S(m)=( SI, Sz, .. ., Sn) be the set of sessions in a log file that have sequence length greater than m. We build models on a subset of these sequences, known as the training sequences, which are separated from the remaining or the testing sequences. When applying the trained model on the testing sequences, let P+ be the correct predictions and Pbe the incorrect predictions. Because we remove the infrequent requests, the union of P' and P-is a subset of S(m). Let I R I be the set of all requests. We define the following measures for each learned prediction model Hn[]:</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>P +</head><p>(1)</p><p>-1</p><formula xml:id="formula_0">precision = P + + P - applicability = -<label>(2)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>IRI</head><p>In the above equations, precision has its similar meaning as often used in information retrieval literature, whereas applicability is a new measure that is different from recall.</p><p>In particular, the notion of applicability is measures, out of all requests in the original log, the number of requests can be predicted (correctly or wrongly) by our model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Domain Analysis and Evaluation</head><p>We first analyze the data set under consideration. One important piece of information about the server data is revealed in Figure <ref type="figure">1</ref>. In this figure, the horizontal axis shows integer in log scale, designating the number of user visits (to pages). There are two curves in the figure. The upper curve "Page Ratio" depicts, for each value X on the X-axis, the percentage of pages that are visited X times or less by all users. For the same X value, the lower curve "Request Ratio" depicts, shows the percentage of accumulated visits out of all visits in the log on the pages which are visited X times or less. Thus, for example, X=10</p><p>represents a visit count of ten times. The upper curve for X=10 shows that around 60% of pages are each visited 10 times or less, and the total number of such visits represents around 15% of all visits there are.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A I S ; I</head><p>PageRatio(X ) = i=l We have also repeated the page vs. request ratio for two more data sets, NASA and M!jN.com data sets, as shown in Figures <ref type="figure">2</ref> and<ref type="figure">3</ref>. The NASA data set contains two months worth of all HTTP requests to the NASA Kennedy Space Center WWW server in Florida. The log was collected from 0O:OO:OO August 1, 1995 through 2 3 5 9 5 9 August 31, 1995. In this period there were 1,569,898 requests. As can be seen from both figures, the data in all three domains follow the same pattem: a large proportion of web pages corresponds to low access ones (less than10 visits per page in the entire log), and together these visits count for a small percentage of total requests as well. Therefore removing them from training set will only decrease precision by a small amount. This further justifies our filtering operation in the first step of the PathModelConstruction algorithm. After applying the model construction algorithm, we have built different hash tables for storing the leamed models. These models are stored in memory. To give an indication of their sizes, for the Monash University log our n-gram table sizes are shown in Table <ref type="table">1</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>I IO 100 1000 10000</head><p>Request Times server. These charts tell us that a significant number of sessions are for one or two requests in a row. However, there are still a sizable number of requests for sessions with lengths greater than three. In fact, our prediction algorithms are aimed at just these sessions. There are several reasons for this choice. We hypothesize that for sessions with lengths less than or equal to three it is difficult to make any predictions with significant accuracy based purely on the statistical information. This hypothesis is supported by our individual n-gram precision experiments in all three domains, as shown in Figures <ref type="figure">6,</ref><ref type="figure">8</ref> and IO, respectively. It can be seen that for all three domains, the prediction errors are reduced significantly when one predicts based on 3-gram sequences as compared to I-gram data.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Tablel. Hash table sizes for implementing n-gram models</head><p>Hash I 1 gram 1 2 g r a m I 3 g r a m I 4 g r a m I  For all data, we took 4/5 of the log as training data set and the remaining 1/5 as testing log. For each test, we recorded the precision and applicability information as described by Equations (1) and (2). We have recorded the prediction precision as a function of n where n is the path length of the sequence used for n-gram prediction. Figure <ref type="figure">6</ref> shows the precision for the Monash University data and Figure <ref type="figure">7</ref> shows the applicability on the same scale. In these and subsequent figures, the x-axes are marked with the length of n-grams (n=l, 2, 3, 4), and the corresponding y-axes represent precision obtained when applying algorithm ngram(n). The remaining mark on x-axes is "3-gram+", which represent experiments on data consisting of sessions having length greater than or equal to three for both training and testing (that is, setting m to 3 in n-gram+(m) algorithm). As can be seen, our prediction using the combined 3 and 4-gram models achieved a much higher precision than using I-gram prediction only. We have also applied the same training and testing to NASA log data as shown in Figures <ref type="figure">8</ref> and<ref type="figure">9</ref>. The results confirm similar conclusions.</p><p>Finally, we took 10% of the NASA log and run a comparison of n-gram models (n-gram(n)) for n between one and seven. The results of precision and applicability are shown in Figure <ref type="figure">12</ref>. As we can see, as the order of ngram model increases, the precision is increasing linearly while applicability decreases linearly as well. However, we can see that the decrease in applicability is faster than the increase of precision, indicating an upper limit in which ngram models should be applied.</p><p>I "ni" I </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">RELATED WORK</head><p>and Curewitz et a1 <ref type="bibr" target="#b1">[4]</ref> were the first to examine the use of compression modeling techniques to track events and prefetch items. They prove that such techniques converge to an optimal online algorithm. They go on to test this work for memory access pattems in an object oriented database and a CAD system. Kroeger et a1 <ref type="bibr" target="#b5">[7]</ref> adapts Prediction by Partial Match in a different manner. The problem domain they examine was the file systems access pattems. The hit ratio of 4M caches using PPM is even higher than 90M caches using LRU. Compared to their work, we focused on the comparison of n-gram models for different n. We also applied a cascading n-gram model (n-gram+) over three realistic web server logs and show that the prediction techniques hold valid for the web domain.</p><p>The availability of web related information has inspired an increasing amount of work in user action prediction. Much work has been done in recommendation systems, which provide suggestions for user's future visits on the web based on machine learning or data mining algorithms. An example is the Webwatcher system <ref type="bibr" target="#b4">[6]</ref>, which makes recommendations on the future hyperlinks that the user might like to visit based on a model obtained through reinforcement learning. Other recommendation systems include the Letizia system that anticipates a user's browsing actions by making forward explorations and the Syskill &amp; Webert system that learns to rate pages on the World Wide Web.</p><p>Compared to these systems, our path-based prediction model is obtained by building sequences of user requests of long enough length from all user actions in a user log and predicts the next action based on statistical analysis of sequence information.</p><p>Due to bandwidth limitations, users on the Intemet are experiencing increasing delays in obtaining the desired documents. In response, many researchers have designed action systems that make use of predictions from a leamed model to pre-fetch or pre-send documents. The work by Zukerman et al. and Albrecht et al. belong to this class. In this work, a Markov model is leamed through training on a web server log based on both time interval information and document sequence information. The predicted documents are then sent to a cache of a certain size on the client side ahead of time. Similarly, Lau and Horvitz <ref type="bibr" target="#b6">[8]</ref> have classified user queries and built a Bayesian model to predict users' next query goal or intention based on previous queries and time interval. Our work is also related to that of [ 111 who studied users' complete web search sequences and the work of Silverstein[l3] who provided a detailed statistical analysis of log data. Compared to these systems, our work focuses on utilizing the server log that contains the users.' browsing actions rather than queries submitted to a search service. In addition, our algorithm only makes prediction on users' actions when it gathers enough information regarding the users' actions on a long enough sequence of such requests. When the users are observed to make short sequence visits, we do not make any predictions since such users may be making random visits on the web, and thus the next action may not be predictable. The work of [14][15] used similar techniques but did not compare the effectiveness of different n-gram models as we did in this work.</p><p>Compared to previous research, our main contributions are: I . We compared the effectiveness of n-gram prediction for different sequence length n, and found that with an increase in sequence length, there is an increase in precision and decrease in applicability.</p><p>In response, we formulated a cascading model n-gram+ by including successively lower order models in prediction, and obtained a good balance between predictability and applicability.</p><p>We preformed experiments on three very different server logs (commercial, university and govemmental), and found that our prediction algorithm indeed performed well.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>2.</head><p>3.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Conclusions and Future Work</head><p>Our work is aimed at showing that using simple n-gram models for n greater than two will result in significant gain in prediction accuracy while maintaining reasonable applicability. Our results show that for n-gram based prediction when n is greater than three gives a precision gain on the order of 10% or more for the three realistic web logs. Our combined algorithm n-gram+(3) shows a higher precision than individual 3-gram model and slightly lower than 4-gram model, while at the same time having a applicability equal to that of the 3-gram model and higher than that of the 4-gram model. This shows that the n-gram+(3) algorithm applies to a significant portion of the web logs for it to be useful. Our results also show that both the training and prediction algorithms can be applied in a real time setting.</p><p>Our algorithm has immediate applications in web server caching, pre-sending and recommendation systems. In our future work, we wish to apply this algorithm to these domains.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head></head><label></label><figDesc>Fori:= I to ILI Do s := L[I]: //jind a sub-string of length n starting at alphabet j P : = sub-string CS, j , n ) ; C := sub-string(S,j+l,I); //find the next click T[P, C ] : = T[P, C ] + 1;</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>FP</head><label></label><figDesc>:= the same sequence with the first element A,B,C,D removed; A,B,C,F A,B,C,F B,C,D,G B,C,D,G B,C,D,F If we were to construct a 3-gram model, we have two 3grams to build our prediction model on. These are End For Rerum( "No Prediction"); End For comparison purposes, in our experiment we also test individual n-gram algorithms as defined below: Algorithm n-gram (P: user's current clicking sequence) A,B,C; B , C B Our application of the algorithm retums the following hash table H30: Begin If((lPI &gt;= n) and ( P is an index of Hn[])) Then Prediction : = Hn[P]; I Return( "No Prediction"); G EndHowever, if we were to build a 2-gram model, then we have the following 2-grams to contend with:</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 2 .Figure 4 .Figure 3 .Figure 5 .</head><label>2435</label><figDesc>Figure 2. Page vs. request percentage for NASA data set</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 6 .Figure 7 .Figure 8 .Figure 9 .Figure 10 .</head><label>678910</label><figDesc>Figure 6. Precision and Applicability as a function of session lengths for Monarsh University data log. n-grams (n=1, 2, 3, 4) represent precision as recorded for sessions having length greater than n</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table Table</head><label>Table</label><figDesc></figDesc><table><row><cell>17,434</cell><cell>23.763</cell><cell>22,804</cell><cell>20,958</cell></row><row><cell>Size</cell><cell></cell><cell></cell><cell></cell></row></table></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgments</head><p>We thank Jing Han for her initial involvement in this work, Susan Dumais and Eric Horvitz for their timely feedback on this work. We also thank Steven Johnson of MSR Web Support Group and David Abrecht and Ingrid Zukerman</p></div>
			</div>

			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>from Monash University for sharing their web log data with US.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.">References</head><p>[ I ] Albrecht, D. W., Zukerman, I., and Nicholson, A. E. <ref type="bibr">(1999)</ref>. Pre-sending documents on the WWW: A comparative study. IJCAI99 -Proceedings of' the Sixteenth International Joint Conference on Artificial Intelligence.</p><p>[2] Balabanovic, M. (1998). Exploring versus exploiting when learning user models for text recommendation. User Modeling and User-adapted Interaction 8(1-2):71-102.</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Empirical Analysis of Predictive Algorithms for Collaborative Filtering</title>
		<author>
			<persName><forename type="first">J</forename><surname>Breese</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Heckerman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Kadie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Fourteenth Conference on Uncertainty in Artificial Intelligence</title>
		<meeting>the Fourteenth Conference on Uncertainty in Artificial Intelligence<address><addrLine>Madison, WI</addrLine></address></meeting>
		<imprint>
			<date type="published" when="1998-07">1998. July, 1998</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">Practical Prefetching via Data Compression. SIGMOD</title>
		<author>
			<persName><forename type="first">M</forename><surname>Curewitz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Krishnan</forename><forename type="middle">J S</forename><surname>Vitter</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1993">1993</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title/>
	</analytic>
	<monogr>
		<title level="j">Record</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="257" to="266" />
			<date>Jun. I993</date>
			<publisher>ACM</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Continual Computation Policies for Utility-Directed Prefetching</title>
		<author>
			<persName><forename type="first">E</forename><surname>Horvitz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Seventh ACM Conference on Information and Knowledge Management</title>
		<meeting>the Seventh ACM Conference on Information and Knowledge Management</meeting>
		<imprint>
			<date type="published" when="1998-11">1998. November 1998</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Webwatch: A tour guild for the World Wide Web</title>
		<author>
			<persName><forename type="first">T</forename><surname>Joachims</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Freitag</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Mitchell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IJCAI 97 -Proceedings of the Fvteenth Intemational Joint Conference on Artificial Intelligence</title>
		<imprint>
			<date type="published" when="1997">1997</date>
			<biblScope unit="page" from="770" to="775" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Predicting Future file-System Actions From Prior Events</title>
		<author>
			<persName><forename type="first">T</forename><surname>Kroeger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Darrell</forename><forename type="middle">D E</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the USENIX I996 Annual Technical Conference</title>
		<meeting>the USENIX I996 Annual Technical Conference</meeting>
		<imprint>
			<date type="published" when="1996-01">1996. Jan I996</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Patterns of search: analyzing and modeling web query refinement</title>
		<author>
			<persName><forename type="first">T</forename><surname>Lau</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Horvitz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Uxer Modeling &apos;99, ~~1</title>
		<imprint>
			<date type="published" when="1999">1999</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="9" to="10" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">Automatic Speech Recognition: The Development of the SPHINX System</title>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">F</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Mahajan</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1989">1989</date>
			<publisher>Kluwer</publisher>
			<pubPlace>Dordrecht, The Netherlands</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Mining Longest Repeating Subsequences to Predict WWW Surfing</title>
		<author>
			<persName><forename type="first">H</forename><surname>Lieberman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">P</forename><surname>I I] Maglio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Barrett</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Henzinger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Marais</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Moricz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename></persName>
		</author>
		<idno>1998-014</idno>
	</analytic>
	<monogr>
		<title level="m">IJCAI95 -Proceedings of&apos;the Fourteenth International Joint Conference on Artqicial Intelligence</title>
		<editor>
			<persName><forename type="first">J</forename><surname>Pitkow</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">P</forename><surname>Pirolli</surname></persName>
		</editor>
		<meeting><address><addrLine>Portland, OR., ~~5; Palo Alto, CA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="1995">1995. 1997. 1996. 1998. 1999</date>
			<biblScope unit="volume">97</biblScope>
			<biblScope unit="page" from="5" to="16" />
		</imprint>
	</monogr>
	<note type="report_type">Technical Report</note>
	<note>Proceedings of the 1999 USENIX Annual Technical Conference</note>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Using path profiles to predict HTTP requests</title>
		<author>
			<persName><forename type="first">S</forename><surname>Schechter</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Krishnan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">D</forename><surname>Smith</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Predicting user&apos;s request on the WWW. UM99 -Proceedings of the Seventh International Conference on User Modeling</title>
		<editor>
			<persName><forename type="first">I</forename><surname>I61 Zukerman</surname></persName>
		</editor>
		<editor>
			<persName><forename type="middle">W</forename><surname>Albrecht</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">A</forename><surname>Nicholson</surname></persName>
		</editor>
		<meeting><address><addrLine>Brisbane, Australia</addrLine></address></meeting>
		<imprint>
			<date type="published" when="1998">1998. 1999</date>
		</imprint>
	</monogr>
	<note>Proceedings ofthe Seventh International World Wide Web Conference</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
