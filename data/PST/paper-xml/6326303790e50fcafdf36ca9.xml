<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">A Cross-Prefetcher Schedule Optimization Methodology</title>
				<funder ref="#_srxNdfv #_QDnmQdU">
					<orgName type="full">Singapore MoE AcRF</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">R?zvan</forename><surname>Ni?u</surname></persName>
							<idno type="ORCID">0000-0001-9713-2760</idno>
							<affiliation key="aff0">
								<orgName type="department">Automatic Control and Engineering Faculty</orgName>
								<orgName type="institution">University POLITEHNICA of Bucharest</orgName>
								<address>
									<postCode>060042</postCode>
									<settlement>Bucharest</settlement>
									<country key="RO">Romania</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Lingfeng</forename><surname>Pei</surname></persName>
							<idno type="ORCID">0000-0002-2953-4650</idno>
							<affiliation key="aff1">
								<orgName type="department">Department of Computer Science</orgName>
								<orgName type="institution">National University of Singapore</orgName>
								<address>
									<postCode>119077</postCode>
									<country key="SG">Singapore</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><roleName>AND</roleName><forename type="first">Trevor</forename><forename type="middle">E</forename><surname>Carlson</surname></persName>
							<idno type="ORCID">0000-0001-8742-134X</idno>
							<affiliation key="aff1">
								<orgName type="department">Department of Computer Science</orgName>
								<orgName type="institution">National University of Singapore</orgName>
								<address>
									<postCode>119077</postCode>
									<country key="SG">Singapore</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">A Cross-Prefetcher Schedule Optimization Methodology</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="DOI">10.1109/ACCESS.2022.3195046</idno>
					<note type="submission">Received 30 May 2022, accepted 21 July 2022, date of publication 29 July 2022, date of current version 25 August 2022.</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-01-03T09:38+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Analytical model</term>
					<term>computer architecture</term>
					<term>FPGA</term>
					<term>optimization</term>
					<term>prefetching</term>
					<term>program analysis</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Prefetching offers the potential to significantly improve performance by speculatively loading application data so that it is available before it is needed. By their very nature, prefetching techniques are application behavior dependant. This implies that no universal prefetching solution exists. A combination of prefetching strategies need to be used to target a diverse set of applications. In this work, we develop the first comprehensive mathematical framework that allows a designer to better understand the prefetching opportunities of an application. We first use dynamic analysis to study the memory access behavior of an application and measure a series of metrics to both identify the optimized schedule, and estimate its achievable performance. To validate our model, we implement and evaluate three different prefetching strategies: helper threads, software prefetching and FPGA prefetching. We show that, for each individual scenario, our framework correctly generates the optimized schedule of prefetches and predicts the performance improvement with an accuracy of more than 95%. Using our framework, developers can choose the best prefetching strategy and parameters for their specific workload and use case.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>I. INTRODUCTION</head><p>As the speed gap between modern processors and the memory system is ever increasing <ref type="bibr" target="#b25">[26]</ref>, <ref type="bibr" target="#b55">[56]</ref>, the bottleneck of memory accessing in today's Von-Neumann machines becomes the pain-point that inspires various optimizing techniques such as caching <ref type="bibr" target="#b21">[22]</ref> and prefetching <ref type="bibr" target="#b6">[7]</ref>, <ref type="bibr" target="#b7">[8]</ref>, <ref type="bibr" target="#b37">[38]</ref>, <ref type="bibr" target="#b45">[46]</ref>.</p><p>Prefetching is a fundamental technology of most highperformance systems today <ref type="bibr" target="#b23">[24]</ref>, <ref type="bibr" target="#b49">[50]</ref>, <ref type="bibr" target="#b52">[53]</ref>- <ref type="bibr" target="#b54">[55]</ref>. The goal of the prefetching is to retrieve, in a timely manner, data from a high latency memory, typically DRAM, and place it in fast-toaccess cache memory. One key feature of a prefetcher is that it aims to fetch the data that is needed before the computation unit accesses and uses it. Prefetching can significantly reduce the time a CPU needs to wait when accessing data.</p><p>Existing prefetchers implemented in hardware <ref type="bibr" target="#b8">[9]</ref>, <ref type="bibr" target="#b13">[14]</ref>, <ref type="bibr" target="#b17">[18]</ref>- <ref type="bibr" target="#b19">[20]</ref>, <ref type="bibr" target="#b27">[28]</ref>, <ref type="bibr" target="#b29">[30]</ref>, <ref type="bibr" target="#b30">[31]</ref>, <ref type="bibr" target="#b32">[33]</ref>, <ref type="bibr" target="#b37">[38]</ref>, <ref type="bibr" target="#b45">[46]</ref>, <ref type="bibr" target="#b46">[47]</ref>, <ref type="bibr" target="#b48">[49]</ref>, <ref type="bibr" target="#b50">[51]</ref>, <ref type="bibr" target="#b57">[58]</ref> provide fixed-function operation and can not</p><p>The associate editor coordinating the review of this manuscript and approving it for publication was Christian Pilato .</p><p>fundamentally change to adapt to the application, limiting attainable performance.</p><p>We argue that future prefetchers need to be configurable to support different strategies, possibly to the extent that they are configured by software. Memory access patterns are well known to be application dependent, which makes it hard to prefetch in an accurate and timely manner. For example, different prefetching distances, i.e. how far ahead the prefetcher sends requests, can lead to up to 10? variation in performance <ref type="bibr" target="#b31">[32]</ref>. Therefore, we argue that prefetching needs to be driven by dynamic application behavior <ref type="bibr" target="#b2">[3]</ref>, <ref type="bibr" target="#b4">[5]</ref>, <ref type="bibr" target="#b31">[32]</ref>.</p><p>This opens up a large design space for the developer: Which prefetching strategy should be selected? When should a prefetch request be sent out? Is it worthwhile to keep the current strategy or is there a benefit to switch to a new one (while considering the potential overhead of this change)? Which parameters should one select if the prefetch strategy is parameterizable? Until now, such questions have not been possible to address in a systematic way.</p><p>In this work, we propose a novel analytical framework that, based on measurements of application execution, can suggest close-to-optimal prefetcher strategies. Our framework provides two results. First, for a given prefetching strategy, the framework outputs an optimized schedule of prefetches that is both accurate and timely. The prefetching plan can be used to improve application performance. In our experiments, we show that the speed-up obtained while using the generated prefetch schedule is at or near optimal, seeing speed-ups between 1.16? and 2.05?.</p><p>Second, a performance estimate of the application that uses the mentioned prefetching schedule is computed. This estimate can be used to select between different prefetching strategies. In our experiments, the difference between the estimated and the measured speed-up is less than 5%.</p><p>Prior to our work, prefetching has been viewed as a black box. Developers have been using trial-and-error techniques for developing prefetchers hoping to meet their performance targets. In contrast, our framework brings transparency to prefetching by providing the analytical tools for a developer to understand the prefetching capabilities, or limitations, of an application that runs on a given system. Additionally, it offers the possibility to obtain the information required to select the best solution from a basket of options.</p><p>Below, we list the main contributions of this work.</p><p>? We propose a mathematical framework to both understand and predict potential prefetcher performance. The framework abstracts the technique of prefetching and is general enough to cover most prefetching scenarios.</p><p>? We develop a methodology of evaluating the prefetching capabilities of an application to allow developers to evaluate its suitability for a given hardware configuration.</p><p>? We describe how memory-level parallelism (MLP) for prefetching can close the gap to optimal performance.</p><p>? We evaluate the accuracy of our framework in the context of helper threads, software prefetching and FPGA prefetching. The remainder of the paper is organized as follows: Section II details the motivation of this work and provides a high-level overview of our methodology, Section III extensively describes the analytical model, and Section IV details the methodology to apply our work. Section V presents our experimental results, Section VI discusses relevant work, and we present our conclusions in Section VIII.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>II. MOTIVATION AND OVERVIEW</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. MOTIVATION</head><p>Though a plethora of prefetching techniques exist, no single solution can outperform all others in every situation <ref type="bibr" target="#b4">[5]</ref>, <ref type="bibr" target="#b7">[8]</ref>. As such, understanding the applicability of a prefetching technique in a given scenario is fundamental to choosing the right solution. To that end we propose an analytical model that aids the programmer in understanding the prefetching capabilities of the application on a given system.</p><p>Software controlled prefetching techniques, such as software prefetching and helper threads, have a better view of the program in general; thus, they are more flexible and may adapt to a larger spectrum of applications than traditional hardware-based solutions <ref type="bibr" target="#b20">[21]</ref>. However, up until this point, the proposed solutions for software controlled prefetching provide a trial-and-error mechanism of identifying the relevant values for parameters such as prefetch distance. In this work, we advance the state of the art by developing a mathematical framework that computes an optimized schedule of prefetches and estimates its performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. METHODOLOGY OVERVIEW</head><p>Our work is based on the idea that prefetcher timeliness and the amount of work done between prefetch events are the two key metrics needed when developing an optimal software prefetching strategy.</p><p>To that end, we measure these characteristics to allow us to understand the potential prefetching benefits available to an application on a specific hardware platform (See Section III for details). These metrics are: (a) the time it takes the CPU and the prefetcher to access non-cache memory, (b) the time it takes the CPU to access the cache, (c) the available computation that is present between two consecutive cache misses triggered by the same load instruction and (d) the communication latency between the CPU and the prefetcher. The cache is typically the first level, but can refer to any level.</p><p>Figure <ref type="figure" target="#fig_0">1</ref> presents a high level overview of our proposed methodology. We first analyze the application to identify problematic load instructions and collect the mentioned metrics. We then input the designated prefetch technique and, if necessary, implement the prefetch kernel. A prefetch kernel is represented by the code that is run to compute the prefetch addresses and issue the prefetch requests.</p><p>We also analyze the data prefetch latency to compute the time it takes to prefetch a data item. Finally, we apply our mathematical formula to identify the optimized schedule and compute a performance estimate. Steps (2) to (4) may be repeated for any number of prefetching techniques to understand which strategy is best for the given application.</p><p>Previous work <ref type="bibr" target="#b2">[3]</ref>, <ref type="bibr" target="#b4">[5]</ref> has used dynamic analysis to identify problematic loads, however, to the best of our knowledge, we are the first to push this analysis further by examining application runtime latencies. The benefit of this proposal is that we can use this information to both timely and accurately prefetch the necessary application data. By analyzing these values, we show that it is possible to understand what percentage of the total data accesses can be prefetched in a timely manner, and what it the best prefetching strategy for a given application.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>III. ANATOMY OF PREFETCHING A. RELEVANT METRICS</head><p>We utilize a set of metrics to describe prefetching and build our mathematical framework. For simplicity, we will consider only single program, single threaded workloads. However, the analytical framework can be extended to multi-program, multi-threaded applications. T entity action is the time for the entity (cpu or pf) to perform an action (initialization (init), accessing the cache, or mem, and the latency of compute). T cpu mem denotes the time it takes the CPU to read one data element from the high latency memory. Similarly, T pf mem represents the time it takes the prefetching implementation to perform the same operation. T cpu cache represents the time it takes the CPU to read one element of data from cache. Both T cache and T mem include the time that is necessary to compute the address for the read request. T pf init is the start up time for the prefetching implementation. It represents the time between the moment when the computation unit issues a prefetch request and the moment when the prefetching implementation actually starts running. T cpu compute is used to indicate the time between 2 consecutive read requests issued by the CPU that have the same instruction pointer and that tend to miss in the cache.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. FORMALIZING PREFETCHING</head><p>Given a specific application, we would like to determine an optimized strategy for prefetching considering T  In this situation the ability to prefetch is severely limited, however benefits can still be obtained. We start by assuming an ideal scenario and incrementally close the gap between it and real world situations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>1) INFINITE CACHE</head><p>We start by assuming that the system has an infinite amount of fully-associative cache memory, therefore once cache lines are allocated, they are never evicted. This enables us to first determine the maximum amount of data that can be prefetched in a timely manner. We assume that the working set is known. Next we consider N miss , the number of cache accesses that miss. As the cache is infinitely sized, in order to avoid paying the communication latency between the prefetching implementation and the CPU, the CPU will issue a single request. In turn, the prefetching implementation will fetch the entire data set required for the application.</p><p>Scenario 1:</p><formula xml:id="formula_0">T pf mem &lt; T cpu cache + T cpu compute .</formula><p>If it takes the prefetching implementation less time to access one noncached data item than it takes the computation unit to access a cached data item, then all of the N miss data elements that are missing from the cache can be fetched in one pass. Assuming the prefetching implementation and the computation unit are launched at the same time, we pay a minor delay of T pf init that can be translated into N lost number of accesses that are still going to miss before the CPU starts accessing the prefetched region of data:</p><formula xml:id="formula_1">N lost = T pf init T cpu mem . (<label>1</label></formula><formula xml:id="formula_2">)</formula><p>VOLUME 10, 2022</p><p>Scenario 2:</p><formula xml:id="formula_3">T cpu cache + T cpu compute &lt; T pf mem &lt; T cpu mem + T cpu compute .</formula><p>In this scenario, the prefetching implementation is able to fetch data ahead of the computation unit when the latter reads the data from non-cache memory. However, when the CPU starts accessing cached data it will eventually catch up. Without any prefetching involved, we can express the application total run time as:</p><formula xml:id="formula_4">N miss * (T cpu mem + T cpu compute )<label>(2)</label></formula><p>But if prefetching is going to be used, then some of these accesses are going to be turned in cache hits, therefore the new application total run time is going to be:</p><formula xml:id="formula_5">Y * (T cpu mem + T cpu compute ) + (N miss -Y ) * (T cpu cache + T cpu compute )<label>(3)</label></formula><p>where Y represents the number of cache misses that the CPU will still produce. During that time the prefetching implementation should be able to fetch the N miss -Y data items, therefore, Equation 3 can also be expressed as:</p><formula xml:id="formula_6">(N miss -Y ) * T pf mem + T pf init (4)</formula><p>Equalising Equations 3 and 4 we are able to deduce Y:</p><formula xml:id="formula_7">Y = N miss * (T pf mem -T cpu cache -T cpu compute ) + T pf init T pf mem -T cpu cache + T cpu mem .<label>(5)</label></formula><p>The value of Y is optimal for prefetching. Since the cache is infinite, the optimal procedure is to start prefetching at the same time as the CPU starts its processing and fetch N miss -Y data items starting with the Yth missing access. Scenario 3:</p><formula xml:id="formula_8">T pf mem &gt; T cpu mem + T cpu compute .</formula><p>This situation is similar to the preceding one because there is still the need to sacrifice some accesses in order to prefetch others. However, in this case the number of sacrificed accesses is going to be very large because of the slow access time.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>2) FINITE CACHE AND EVICTIONS</head><p>In this scenario we take a step closer to the real world. We assume that the cache has a fixed size, Cache size , that is known both to the prefetcher and the computation unit, and therefore multiple requests should be issued to the prefetching implementation if the entire prefetchable data does not fit into the cache. In this situation, we may apply Equation 5, however, N miss is replaced with a divisor of Cache size . The occurrence of cache evictions cannot be identified in a deterministic manner because they depend on the overall system load, however, in practice, N miss can be evaluated with different values until an optimized one is identified. In our experiments using Zynq hardware, we have seen that the ideal value for N miss occurs when the prefetched data size for one prefetch request occupies 1  4 * Cache size .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>3) SPEED-UP</head><p>Given the above formulas we are also able to compute the maximum expected speed-up. The total run time of an application without any prefetching is computed using Equation <ref type="formula" target="#formula_4">2</ref>. The run time of the application with prefetching involved is computed using Equation <ref type="formula" target="#formula_5">3</ref>. Therefore, the speed-up is computed as the division of the two:</p><formula xml:id="formula_9">S = N miss * (T cpu mem + T cpu compute ) Y * (T cpu mem -T cpu cache ) + N miss * (T cpu cache + T cpu compute ) . (<label>6</label></formula><formula xml:id="formula_10">)</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>IV. METHODOLOGY</head><p>To identify the values for the parameters discussed in section III-A we take the following steps. We dynamically analyze the application to identify the number of missing loads that occur and the responsible loops that cause them. We collect this information by performing test runs on real hardware (although, simulation techniques can also be employed). Next, we work to understand the application source code to identify the memory access patterns of the application. We ask the following questions: how many iterations does the loop have? How many cache misses occur per iteration? This step can be done manually or automatically as discussed by Ayers et. al <ref type="bibr" target="#b4">[5]</ref>. By combining the knowledge obtained in the previous steps, we differentiate between loads that are part of the address generation of another load and loads that fetch actual data needed for the computation. After this step, we can divide the total execution time of the loop to the number of data loads to obtain the approximate value of T cpu mem + T cpu compute . To identify the value of T cpu cache + T cpu compute we apply the same procedure, except that we populate the cache, in advance, with the otherwise missing data. An alternative is to use simulation to obtain this information. Dividing the resulting runtime by the number of cache misses that we obtained earlier, we arrive at the value of T cpu cache + T cpu compute . Note that it is not necessary to separate the compute value from the access latency because they are both present together in all formulas. Next, we implement the prefetch kernel, according to the chosen prefetching strategy, that performs prefetch requests for the faulting loads. We measure the runtime of the prefetching implementation and we divide it by the same number of cache misses that we used earlier. This operation will result in identifying the value of T pf mem . The value of T pf init is identified by measuring the runtime of a prefetcher implementation that does not perform any operation. In the case of software prefetching and helper threads, we consider this value to be negligible. After we identify the corresponding values of the relevant metrics, we update the original program to issue prefetch requests using the derived parameters. In this work, we tackle single process, single program workloads that have a deterministic memory access behavior (i.e. the work set is known in advance).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>V. EVALUATION</head><p>To apply our analysis, we use a Xilinx Zynq Z1 board <ref type="bibr" target="#b28">[29]</ref>. Figure <ref type="figure" target="#fig_3">2</ref> highlights the architecture of the board and Table <ref type="table" target="#tab_0">1</ref> presents component details. The Application Processing Unit (APU) consists of two cores, each with its private L1 cache. The processors share the L2 cache and the On Chip Memory (OCM). The FPGA has a direct link to the L2 cache memory through the accelerator coherency port (ACP).</p><p>This architecture is flexible enough to support multiple prefetching strategies: (1) helper threads by using a core to prefetch data for the other core, (2) software prefetching by using the processors' preload instruction and (3) FPGA prefetching by using the FPGA to prefetch data for one (or both) of the CPUs.</p><p>We analyze and optimize three micro-benchmarks and two realistic applications. The selection of applications covers the most common memory access patterns and the relevant aspects presented in Section III-B. LBM is part of the SPEC CPU2006 <ref type="bibr" target="#b26">[27]</ref> suite and features stride array accesses with large amounts of compute. In this situation, even if prefetching is perfect, the speed-up is limited by the amount of compute present in the loop.</p><p>Linked list. To increase the memory level parallelism in this benchmark, we have implemented a linked list by using jump pointers similar to previous work <ref type="bibr" target="#b46">[47]</ref>. By using jump pointers, we are effectively introducing MLP.</p><p>To obtain the baseline performance measurements, we simply measure the runtime of the mentioned applications without performing any type of prefetching.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. HELPER THREADS</head><p>For each of the benchmarks, we have implemented an additional prefetch helper thread using the pthreads library (the Zynq processor has two cores). In this scenario, prefetching is done into the L1 cache of the other processor. The main thread occasionally sends prefetching requests to the prefetcher thread by specifying the start address and the chunk size.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. SOFTWARE PREFETCHING</head><p>We perform software prefetching for each of the benchmarks by introducing a preload instruction in the loop bodies of our experimental applications. The data is prefetched into the L1 cache of the CPU. We have determined the baseline prefetch distance by using a trial-and-error strategy. We use software prefetching only to synthetically add compute -see Section V-F -because the nature of software prefetching does not permit the issuance of multiple prefetch requests per iteration. This limitation arises from the fact that address computation instructions are still executed by the processor, consuming pipeline slots. For tight loops, these instructions may require more time to compute than the original work performed inside the loop. However, our framework may still accurately predict the speed-up for this scenario, as seen in Figure <ref type="figure" target="#fig_6">4</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D. FPGA PREFETCHING</head><p>We implement FPGA prefetchers using Vivado HLS and Design 2019.1 for each of the mentioned benchmarks. The FPGA prefetcher is able to fetch the data into the shared L2 cache of the Zynq processor. Alternatively, the OCM could be used for prefetching, however, the OCM exhibits the same access latency as the L2 cache <ref type="bibr" target="#b42">[43]</ref>, <ref type="bibr" target="#b47">[48]</ref>, but is non-coherent. Similar to the helper thread implementation, the application running on the CPU triggers the FPGA prefetcher.  <ref type="table" target="#tab_2">3</ref> present the values of the observed metrics. It can be seen that the FPGA has a high latency link to DRAM through the L2 cache controller of the CPU. Although we have utilized the full parallelism potential in the FPGA, the architectural constraints, such as total number of outstanding memory requests, severely limit the prefetching capabilities. As a result, only a small percentage of the total data items can be prefetched. In contrast, the helper thread implementation benefits from a shorter DRAM access latency and therefore is able to prefetch a larger portion of the data accesses.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E. PERFORMANCE RESULTS</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Table 2 and Table</head><p>Figure <ref type="figure" target="#fig_5">3</ref> highlights the obtained speed-up by applying the optimized schedule of prefetches obtained from our framework and compares it to the expected value that is computed by using our formulas. Since the helper thread implementation has a smaller time to access the non-cached memory, it performs better in all of the tested scenarios. It can also be observed that in all situations the difference between the expected speed-up and the measured one is less that 5%.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>F. COMPUTE</head><p>To test our assumption that the best scenario for prefetching is obtained when T pf mem &lt;= T cpu cache + T cpu compute we have synthetically added compute to the benchmarks and observed the effect on performance.</p><p>Figure <ref type="figure" target="#fig_6">4</ref> highlights the impact of synthetically adding compute to the applications on ideal and measured speedup. The ideal speed-up is computed by using our framework and represents the speed-up that would be obtained if all of the accessed data items would be present in the cache when the processor needs them. While the amount of compute increases per iteration, the ideal speed-up decreases because the benefits of prefetching are overshadowed by the added compute. However, there is more time for the prefetching implementation to bring the needed data into the cache. Therefore, we observe that in order to be able to prefetch all of the data elements, it is necessary that T pf mem &lt;= T cpu cache + T cpu compute . Theoretically, this can be achieved either by increasing the amount of compute per loop iteration or by issuing additional prefetch requests in parallel. For example, by adding another prefetcher helper thread, it is expected that the T pf mem will decrease, ideally, by a factor of 2. The implication for systems is that, by improving the memory-level parallelism (MLP) available to prefetchers, one can reduce the gap between achievable and optimal prefetching performance. Our experiments show that by looking at the runtime latencies of data accesses, it is possible to understand the amount of prefetching available in a given scenario and how to schedule the prefetch requests. Moreover, using this information it is possible to compute a performance estimate of the prefetch-enhanced application.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>VI. RELATED WORK</head><p>Prefetching is a standard technique that has been used in many different ways and in many situations. We briefly outline relevant work and elaborate, further, our approach.</p><p>For prefetcher performance it is important to understand the memory access patterns of algorithms and applications. Ayers et al. <ref type="bibr" target="#b4">[5]</ref> have recently reinforced this and they developed a classification of memory access patterns, highlighted in Table <ref type="table" target="#tab_3">4</ref>, that can be used to express most memory access types. This classification offers insights into whether a specific type of prefetching is suitable for a specific workload. We note that the same algorithm and indeed application can exhibit different access patterns in different phases of execution.</p><p>Prefetching can be implemented in hardware, software, as well as a combination of hardware and software. Table <ref type="table" target="#tab_4">5</ref> groups similar prefetching techniques into categories and highlights the relevant attributes of each technique.</p><p>Hardware prefetching techniques require a specialized physical unit that handles the monitoring of memory accesses and automatically generates prefetch requests. This unit is commonly tightly coupled to the execution unit, normally a processor core. This allows for low latency communication between the core and the prefetch hardware unit. The hardware units tend not to support anything but a general prefetch method which may not be optimal for all algorithms or applications. In our work and in this paper, we show that latency is not crucial for performance allowing a looselycoupled and program controlled accelerator to carry out prefetching effectively. This also allows the prefetchers in our approach to implement specialized and more complicated prefetch methods.</p><p>There exist several different types of methods commonly implemented by hardware prefetch units. This includes stride, history based and irregular prefetchers.</p><p>Stride prefetchers <ref type="bibr" target="#b13">[14]</ref>, <ref type="bibr" target="#b19">[20]</ref>, <ref type="bibr" target="#b27">[28]</ref>, <ref type="bibr" target="#b29">[30]</ref>, <ref type="bibr" target="#b37">[38]</ref>, <ref type="bibr" target="#b48">[49]</ref> represent the most common form of hardware prefetcher employed in current systems. Simple and easy to implement, stride prefetchers benefit a subset of memory access patterns <ref type="bibr" target="#b34">[35]</ref>, namely, regular streaming access patterns. For other types, the stride prefetcher may actually worsen performance by replacing useful data with prefetched data that is not used. History based prefetchers <ref type="bibr" target="#b30">[31]</ref>, <ref type="bibr" target="#b32">[33]</ref>, <ref type="bibr" target="#b45">[46]</ref>, <ref type="bibr" target="#b50">[51]</ref> have the ability to prefetch more complex access patterns by storing a sequence of prior accesses and predict future accesses based on it. However, to achieve good performance, history based prefetchers require a large amount of memory, up to megabytes, to store the necessary information. In addition, pointer-chasing and indirect memory patterns are not supported because of their irregular nature.</p><p>Irregular prefetchers target complex access patterns (pointer chasing and indirect) and can be divided into 2 categories: specialized and general.</p><p>Specialized irregular hardware prefetching units target a single access pattern. Multiple solutions have been proposed for both pointer-fetching prefetchers <ref type="bibr" target="#b17">[18]</ref>, <ref type="bibr" target="#b18">[19]</ref>, <ref type="bibr" target="#b46">[47]</ref> and indirect access prefetchers <ref type="bibr" target="#b8">[9]</ref>, <ref type="bibr" target="#b57">[58]</ref>. Although these units provide significant performance benefits, they lack generality.</p><p>Run-ahead execution prefetchers <ref type="bibr" target="#b24">[25]</ref>, <ref type="bibr" target="#b43">[44]</ref>, <ref type="bibr" target="#b44">[45]</ref> may prefetch many types of memory access patterns by speculatively pre-executing the program's own code. By closely mimicking the access patterns of the application, this technique is highly general, supporting many types of memory access patterns. However, this approach requires prohibitive amounts of analysis hardware to identify the instruction streams that cause cache misses. Once identified, the instruction stream is executed ahead of time on a separate core. This leads to the inability of prefetching data for loads that contain a long latency load in their address computation.</p><p>Although hardware prefetching techniques may prove beneficial in certain scenarios, they lack the flexibility required to adapt to any kind of access pattern. We overcome this limitation by dynamically analyzing the application before execution and specifically targeting the long latency loads.</p><p>Software prefetching techniques rely on prefetch hints or instructions that are inserted in the source code. These generate pre-load instructions that are executed before the actual load. These instructions are committed immediately and therefore do not stall the pipeline. This approach has the advantage that it does not require extra hardware since most architectures implement a form of prefetch instruction. However, software prefetching techniques suffer from two major shortcomings: (1) inserting prefetch instructions that accurately target long latency loads is difficult and (2) accesses that involve multiple long latency loads will continue to stall the pipeline and therefore require extra computation that masks the prefetch.  Static analysis prefetching relies on the compiler to (1) identify memory accesses that will cause a cache miss and (2) automatically insert prefetch instructions. Due to the static nature of the analysis, the set of patterns identified is limited to simple stride <ref type="bibr" target="#b22">[23]</ref>, <ref type="bibr" target="#b51">[52]</ref> and indirect <ref type="bibr" target="#b1">[2]</ref> accesses. However, in most cases, the speed-up resulted from static analysis prefetching is inferior to manually inserted prefetching code. Dynamic analysis prefetching <ref type="bibr" target="#b39">[40]</ref>, <ref type="bibr" target="#b41">[42]</ref> [5] leverages runtime information with regard to the last level cache miss of each load instruction request to appropriately target them for software prefetching. The prefetch instructions are then manually inserted in the source code. This approach offers the benefit of accurately identifying the problematic loads at the cost of extra upfront dynamic analysis.</p><p>In this work, we adopt dynamic analysis and manually insert prefetch triggers and allow for multiple accesses to occur in parallel. Our model can be used to identify the roofline speed-up for ideal software prefetching.</p><p>Helper threads <ref type="bibr" target="#b10">[11]</ref>- <ref type="bibr" target="#b12">[13]</ref>, <ref type="bibr" target="#b15">[16]</ref>, <ref type="bibr" target="#b16">[17]</ref>, <ref type="bibr" target="#b33">[34]</ref>, <ref type="bibr" target="#b35">[36]</ref>, <ref type="bibr" target="#b36">[37]</ref>, <ref type="bibr" target="#b40">[41]</ref> tackle prefetching by statically extracting the code for delinquent loads and running it on a spare thread context. This approach can optimally target any access pattern by increasing the number of helper threads. Furthermore, it is flexible enough to be implemented both in hardware <ref type="bibr" target="#b11">[12]</ref>, <ref type="bibr" target="#b12">[13]</ref>, <ref type="bibr" target="#b15">[16]</ref>, <ref type="bibr" target="#b16">[17]</ref>, <ref type="bibr" target="#b40">[41]</ref> and software <ref type="bibr" target="#b10">[11]</ref>, <ref type="bibr" target="#b33">[34]</ref>, <ref type="bibr" target="#b35">[36]</ref>, <ref type="bibr" target="#b36">[37]</ref>. However, even using a single extra thread comes at an increased energy penalty on high performance cores. Moreover, accesses that require loads in their address computation will stall, and in the absence of a hardware event queue, the synchronization of loads becomes costly in terms of both implementation and performance.</p><p>Programmable hardware techniques employ specialized hardware units that are able to run specific address computation instructions. Jones et al. have proposed a programmable prefetcher specifically designed for graph workloads that targets specific traversals <ref type="bibr" target="#b0">[1]</ref>. Yi et al. have designed a hybrid prefetcher that targets indirect memory accesses <ref type="bibr" target="#b9">[10]</ref>. Several approaches have targeted linked list data structures <ref type="bibr" target="#b3">[4]</ref>, <ref type="bibr" target="#b14">[15]</ref>, <ref type="bibr" target="#b38">[39]</ref>, <ref type="bibr" target="#b56">[57]</ref>. A more general approach has been developed by Ainsworth and Jones <ref type="bibr" target="#b2">[3]</ref> that uses multiple small in-order cores to run prefetch kernels that are indicated in software. This work has shown significant speed-ups for load-intensive applications, however, the design is not able to deal with the pointer chasing pattern and the prefetch kernel size is limited to only a few instructions, whereas previous work <ref type="bibr" target="#b4">[5]</ref> reports prefetch kernels that require up to 80 instructions.</p><p>Summary. Prefetching is a well studied technique and a large range of solutions have been proposed, both general and pattern specific. Each technique has its strengths and weaknesses as highlighted in Table <ref type="table" target="#tab_4">5</ref>.</p><p>To better the prefetching potential of an application, we have devised an analytical framework that evaluates prefetching in a given scenario and helps computer architects understand what are the optimal conditions for prefetching.</p><p>Our approach uses dynamic analysis to (1) identify the instructions that cause long latency loads and <ref type="bibr" target="#b1">(2)</ref> to determine what are the access latencies for both cached and non-cache accesses. Using this information, we determine the optimal schedule of prefetches for a given prefetching technique, taking into account hardware limitations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>VII. DISCUSSION</head><p>In this work we have demonstrated that prefetching performance can be predicted and we have tested our model for single program, single processor applications. We have performed our measurements both on top of an operating system and on the baremetal hardware. However, we have not experimented with various degrees of system utilization. Since an application's optimal prefetch schedule depends on the load of the system at a specific point, this aspect remains to be investigated in future work. One idea that could be used to improve the current status is to simulate the application in a maximally utilized system and deduce the worst case scenario values for the metrics. By using these worst case values, it is possible to tune our prefetch schedule so that it performs optimally irrespective of the system load.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>VIII. CONCLUSION</head><p>Some might see prefetching as a black-box, where one attempts to optimize the strategy in a trial-and-error fashion. As an alternative, this work has taken the first steps toward a rigorous analysis of prefetching, opening the door to new possibilities for both hardware and software systems.</p><p>In this work, we propose a novel mathematical framework to abstract prefetching into its fundamental components. With this understanding, one can now, in an up-front manner, determine how much prefetching can improve the performance of key workloads. Our methodology applies to specific hardware/software pairs under study to present a variety of potential prefetching solutions.</p><p>In addition to presenting a new analytical understanding of prefetching, in this work we present how one can optimize FPGA, helper-thread and software-prefetching-based systems to maximize performance. The result is a significant speed-up for a set of applications that are among the most difficult to optimize (those without a significant amount of compute that can be used to hide the memory latency). Understanding the system requirements with prefetching can also lead to improved hardware designs that can take advantage of the level of optimization provided by our methodology.</p><p>This work presents a hardware-validated model and methodology that can accurately predict high-performing prefetching schedules.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>FIGURE 1 .</head><label>1</label><figDesc>FIGURE 1. Methodology to evaluate an application for prefetching capabilities. (1) Analyze binary, (2) Generate prefetch kernel, (3) Analyze prefetcher, (4) Obtain the performance estimate and the optimized prefetch schedule.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head></head><label></label><figDesc>.e., accessing data from cache is faster than accessing data from memory, there are 3 possible scenarios for the prefetching implementation:1) T pf mem &lt; T cpu cache + T cpu computeThis case may seem counter intuitive, however it is possible to achieve this by having a large amount of parallelism in the prefetching implementation or a large amount of computation time on the CPU side.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>FIGURE 2 .</head><label>2</label><figDesc>FIGURE 2. Zynq architecture. The FPGA can perform L2 cache coherent requests.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head></head><label></label><figDesc>A. BENCHMARK DESCRIPTION Simple stride and Complex stride represent two demonstration micro-benchmarks developed by us. Both have T cpu compute close to 0 and compute the sum of a number of array elements. Simple stride exhibits the a[i] pattern, whereas complex stride is of the form a[4*i*(i+1)]. While hardware stride prefetching can handle simple stride, it has difficulty with complex stride.IntSort is part of the NAS Parallel Benchmark suite<ref type="bibr" target="#b5">[6]</ref>. The main computation path is formed by a loop containing an indirect memory access and almost no extra computation besides the address calculation.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>FIGURE 3 .</head><label>3</label><figDesc>FIGURE 3. Helper threads and FPGA prefetching speed-up over no prefetching. The difference between the predicted and the obtained speed-ups is less than 5%.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>FIGURE 4 .</head><label>4</label><figDesc>FIGURE 4. The effect of adding synthetic compute on the ideal speed-up vs. the measured speed-up. The x-axis represents the added compute, measured in microseconds. The y-axis represents the speed-up obtained. The best performing configuration for prefetching is represented as the convergence point of the ideal (orange) and real speed-up (blue).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>TABLE 1 .</head><label>1</label><figDesc>FPGA platform.</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>TABLE 2 .</head><label>2</label><figDesc>Helper thread metrics (latencies in microseconds).</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>TABLE 3 .</head><label>3</label><figDesc>FPGA prefetching -metrics for each benchmark (latencies are measured in microseconds).</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>TABLE 4 .</head><label>4</label><figDesc>Memory access patterns.</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>TABLE 5 .</head><label>5</label><figDesc>Prefetching techniques.</figDesc><table /></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" xml:id="foot_0"><p>VOLUME 10, 2022   </p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="87420" xml:id="foot_1"><p>  VOLUME 10, 2022   </p></note>
		</body>
		<back>

			<div type="acknowledgement">
<div><head>ACKNOWLEDGMENT</head><p>The authors would like to express their gratitude to <rs type="person">Dr. Sven Karlsson</rs> and <rs type="person">Udaree Kanewala</rs> for their valuable and constructive suggestions during the planning and construction of this work. This work was supported by <rs type="funder">Singapore MoE AcRF</rs> grants <rs type="grantNumber">T1-251RES1705</rs> and <rs type="grantNumber">T1-251RES2102</rs>.</p></div>
			</div>
			<listOrg type="funding">
				<org type="funding" xml:id="_srxNdfv">
					<idno type="grant-number">T1-251RES1705</idno>
				</org>
				<org type="funding" xml:id="_QDnmQdU">
					<idno type="grant-number">T1-251RES2102</idno>
				</org>
			</listOrg>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Graph prefetching using data structure knowledge</title>
		<author>
			<persName><forename type="first">S</forename><surname>Ainsworth</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">M</forename><surname>Jones</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Int. Conf. Supercomputing</title>
		<meeting>Int. Conf. Supercomputing</meeting>
		<imprint>
			<date type="published" when="2016-06">Jun. 2016</date>
			<biblScope unit="page" from="1" to="11" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Software prefetching for indirect memory accesses,&apos;&apos; in Proc</title>
		<author>
			<persName><forename type="first">S</forename><surname>Ainsworth</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">M</forename><surname>Jones</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE/ACM Int. Symp. Code Gener. Optim</title>
		<imprint>
			<biblScope unit="page" from="305" to="317" />
			<date type="published" when="2017-02">Feb. 2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">An event-triggered programmable prefetcher for irregular workloads</title>
		<author>
			<persName><forename type="first">S</forename><surname>Ainsworth</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">M</forename><surname>Jones</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. 23rd Int. Conf. Architectural Support Program. Lang. Operating Syst. (ASPLOS)</title>
		<meeting>23rd Int. Conf. Architectural Support Program. Lang. Operating Syst. (ASPLOS)</meeting>
		<imprint>
			<date type="published" when="2018-03">Mar. 2018</date>
			<biblScope unit="page" from="578" to="592" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Compiler-directed contentaware prefetching for dynamic data structures</title>
		<author>
			<persName><forename type="first">H</forename><surname>Al-Sukhni</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Bratt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">A</forename><surname>Connors</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Oceans Conf. Exhib. Conf</title>
		<meeting>Oceans Conf. Exhib. Conf</meeting>
		<imprint>
			<date type="published" when="2003-10">Sep./Oct. 2003</date>
			<biblScope unit="page" from="91" to="100" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Classifying memory access patterns for prefetching</title>
		<author>
			<persName><forename type="first">G</forename><surname>Ayers</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Litz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Kozyrakis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Ranganathan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. 25th Int. Conf. Architectural Support Program. Lang. Operating Syst. (ASPLOS)</title>
		<meeting>25th Int. Conf. Architectural Support Program. Lang. Operating Syst. (ASPLOS)</meeting>
		<imprint>
			<date type="published" when="2020-03">Mar. 2020</date>
			<biblScope unit="page" from="513" to="526" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">The NAS parallel benchmarks</title>
		<author>
			<persName><forename type="first">D</forename><surname>Bailey</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Barszcz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Barton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Browning</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Carter</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Dagum</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Fatoohi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Frederickson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Lasinski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Schreiber</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Simon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Venkatakrishnan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Weeratunga</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Int. J. High Perform. Comput. Appl</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="63" to="73" />
			<date type="published" when="1991">1991</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Bingo spatial data prefetcher</title>
		<author>
			<persName><forename type="first">M</forename><surname>Bakhshalipour</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Shakerinava</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Lotfi-Kamran</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Sarbazi-Azad</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Int. Symp. High Perform. Comput. Archit. (HPCA)</title>
		<meeting>IEEE Int. Symp. High Perform. Comput. Archit. (HPCA)</meeting>
		<imprint>
			<date type="published" when="2019-02">Feb. 2019</date>
			<biblScope unit="page" from="399" to="411" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Evaluation of hardware data prefetchers on server processors</title>
		<author>
			<persName><forename type="first">M</forename><surname>Bakhshalipour</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Tabaeiaghdaei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Lotfi-Kamran</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Sarbazi-Azad</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Comput. Surveys</title>
		<imprint>
			<biblScope unit="volume">52</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="1" to="29" />
			<date type="published" when="2020-05">May 2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Array tracking prefetcher for indirect accesses</title>
		<author>
			<persName><forename type="first">M</forename><surname>Cavus</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Sendag</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">J</forename><surname>Yi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE 36th Int. Conf. Comput. Design (ICCD)</title>
		<meeting>IEEE 36th Int. Conf. Comput. Design (ICCD)</meeting>
		<imprint>
			<date type="published" when="2018-10">Oct. 2018</date>
			<biblScope unit="page" from="132" to="139" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Informed prefetching for indirect memory accesses</title>
		<author>
			<persName><forename type="first">M</forename><surname>Cavus</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Sendag</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">J</forename><surname>Yi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Trans. Archit. Code Optim</title>
		<imprint>
			<biblScope unit="volume">17</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1" to="29" />
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Helper thread prefetching for loosely-coupled multiprocessor systems</title>
		<author>
			<persName><forename type="first">C</forename><surname>Jung</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Lim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Solihin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. 20th IEEE Int. Parallel Distrib. Process. Symp. (PDPS)</title>
		<meeting>20th IEEE Int. Parallel Distrib. ess. Symp. (PDPS)</meeting>
		<imprint>
			<date type="published" when="2006">2006</date>
			<biblScope unit="page" from="1" to="10" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Microarchitectural support for precomputation microthreads</title>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">S</forename><surname>Chappell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Tseng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Yoaz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><forename type="middle">N</forename><surname>Patt</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. 35th Annu</title>
		<meeting>35th Annu</meeting>
		<imprint>
			<date type="published" when="2002">2002</date>
			<biblScope unit="page" from="74" to="84" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Simultaneous subordinate microthreading (SSMT)</title>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">S</forename><surname>Chappell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Stark</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">P</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">K</forename><surname>Reinhardt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><forename type="middle">N</forename><surname>Patt</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. 26th Int. Symp. Comput. Archit. (ISCA)</title>
		<meeting>26th Int. Symp. Comput. Archit. (ISCA)</meeting>
		<imprint>
			<date type="published" when="1999">1999</date>
			<biblScope unit="page" from="186" to="195" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Reducing memory latency via non-blocking and prefetching caches</title>
		<author>
			<persName><forename type="first">T.-F</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J.-L</forename><surname>Baer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. 5th Int. Conf. Architectural Support Program</title>
		<meeting>5th Int. Conf. Architectural Support Program</meeting>
		<imprint>
			<date type="published" when="1992">1992</date>
			<biblScope unit="page" from="51" to="61" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">A general framework for prefetch scheduling in linked data structures and its application to multi-chain prefetching</title>
		<author>
			<persName><forename type="first">S</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Kohout</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Pamnani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Yeung</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Trans. Comput. Syst</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="214" to="280" />
			<date type="published" when="2004-05">May 2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Speculative precomputation: Long-range prefetching of delinquent loads</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">D</forename><surname>Collins</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">M</forename><surname>Tullsen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Hughes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y.-F</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Lavery</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">P</forename><surname>Shen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. 28th Annu. Int. Symp. Comput. Archit. (ISCA)</title>
		<meeting>28th Annu. Int. Symp. Comput. Archit. (ISCA)</meeting>
		<imprint>
			<date type="published" when="2001">2001</date>
			<biblScope unit="page" from="14" to="25" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Dynamic speculative precomputation</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">D</forename><surname>Collins</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">M</forename><surname>Tullsen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">P</forename><surname>Shen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. 34th ACM/IEEE Int. Symp. Microarchitecture (MICRO)</title>
		<meeting>34th ACM/IEEE Int. Symp. Microarchitecture (MICRO)</meeting>
		<imprint>
			<date type="published" when="2001">2001</date>
			<biblScope unit="page" from="306" to="317" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Pointer cache assisted prefetching</title>
		<author>
			<persName><forename type="first">J</forename><surname>Collins</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Sair</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Calder</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">M</forename><surname>Tullsen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. 35th Annu</title>
		<meeting>35th Annu</meeting>
		<imprint>
			<date type="published" when="2002">2002</date>
			<biblScope unit="page" from="62" to="73" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">A stateless, content-directed data prefetching mechanism</title>
		<author>
			<persName><forename type="first">R</forename><surname>Cooksey</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Jourdan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Grunwald</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. 10th Int. Conf. Architectural Support Program</title>
		<meeting>10th Int. Conf. Architectural Support Program</meeting>
		<imprint>
			<date type="published" when="2002">2002</date>
			<biblScope unit="page" from="279" to="290" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Effectiveness of hardware-based stride and sequential prefetching in shared-memory multiprocessors</title>
		<author>
			<persName><forename type="first">F</forename><surname>Dahlgren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Stenstrom</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. 1st IEEE Symp. High Perform. Comput. Archit. (HPCA)</title>
		<meeting>1st IEEE Symp. High Perform. Comput. Archit. (HPCA)</meeting>
		<imprint>
			<date type="published" when="1995-01">Jan. 1995</date>
			<biblScope unit="page" from="68" to="77" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">A primer on hardware prefetching</title>
		<author>
			<persName><forename type="first">B</forename><surname>Falsafi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">F</forename><surname>Wenisch</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Synth. Lectures Comput. Archit</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1" to="67" />
			<date type="published" when="2014-05">May 2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Make the most out of last level cache in Intel processors</title>
		<author>
			<persName><forename type="first">A</forename><surname>Farshin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Roozbeh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">Q</forename><surname>Maguire</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Kosti?</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. 14th EuroSys Conf</title>
		<meeting>14th EuroSys Conf</meeting>
		<imprint>
			<date type="published" when="2019-03">Mar. 2019</date>
			<biblScope unit="page" from="1" to="17" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Compiler-directed data prefetching in multiprocessors with memory hierarchies</title>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">H</forename><surname>Gornish</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">D</forename><surname>Granston</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">V</forename><surname>Veidenbaum</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Int. Conf. Supercomput. 25th Anniversary</title>
		<meeting>Int. Conf. Supercomput. 25th Anniversary</meeting>
		<imprint>
			<date type="published" when="1990">1990</date>
			<biblScope unit="page" from="128" to="142" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">The IBM Blue Gene/Q compute chip</title>
		<author>
			<persName><forename type="first">R</forename><surname>Haring</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Ohmacht</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Fox</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Gschwind</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Satterfield</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Sugavanam</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Coteus</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Heidelberger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Blumrich</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Wisniewski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Gara</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Chiu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Boyle</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Chist</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Kim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Micro</title>
		<imprint>
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="48" to="60" />
			<date type="published" when="2012-04">Mar./Apr. 2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Continuous runahead: Transparent hardware acceleration for memory intensive workloads</title>
		<author>
			<persName><forename type="first">M</forename><surname>Hashemi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Mutlu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><forename type="middle">N</forename><surname>Patt</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. 49th Annu</title>
		<meeting>49th Annu</meeting>
		<imprint>
			<date type="published" when="2016-10">Oct. 2016</date>
			<biblScope unit="page" from="1" to="12" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title level="m" type="main">Computer Architecture: A Quantitative Approach</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">L</forename><surname>Hennessy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">A</forename><surname>Patterson</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2011">2011</date>
			<publisher>Elsevier</publisher>
			<pubPlace>Amsterdam, The Netherlands</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">SPEC CPU2006 benchmark descriptions</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">L</forename><surname>Henning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM SIGARCH Comput. Archit. News</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="1" to="17" />
			<date type="published" when="2006-09">Sep. 2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Memory prefetching using adaptive stream detection</title>
		<author>
			<persName><forename type="first">I</forename><surname>Hur</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. 39th Annu</title>
		<meeting>39th Annu</meeting>
		<imprint>
			<date type="published" when="2006-12">Dec. 2006</date>
			<biblScope unit="page" from="397" to="408" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<title level="m" type="main">PYNQ-Z1</title>
		<author>
			<persName><surname>Digilent</surname></persName>
		</author>
		<ptr target="https://reference.digilentinc.com/reference/programmable-logic/pynq-z1/start" />
		<imprint>
			<date type="published" when="2022-02-29">Feb. 29, 2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Access map pattern matching for data cache prefetch</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Ishii</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Inaba</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Hiraki</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. 23rd Int. Conf. Supercomput. (ICS)</title>
		<meeting>23rd Int. Conf. Supercomput. (ICS)</meeting>
		<imprint>
			<date type="published" when="2009">2009</date>
			<biblScope unit="page" from="499" to="500" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Linearizing irregular memory accesses for improved correlated prefetching</title>
		<author>
			<persName><forename type="first">A</forename><surname>Jain</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. 46th Annu</title>
		<meeting>46th Annu</meeting>
		<imprint>
			<date type="published" when="2013-12">Dec. 2013</date>
			<biblScope unit="page" from="247" to="259" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">APT-GET: Profile-guided timely software prefetching</title>
		<author>
			<persName><forename type="first">S</forename><surname>Jamilan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">A</forename><surname>Khan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Ayers</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Kasikci</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Litz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="s">Proc. 17th Eur. Conf. Comput. Syst.</title>
		<imprint>
			<biblScope unit="page" from="747" to="764" />
			<date type="published" when="2022-03">Mar. 2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Prefetching using Markov predictors</title>
		<author>
			<persName><forename type="first">D</forename><surname>Joseph</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Grunwald</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. 24th Annu. Int. Symp. Comput. Archit. (ISCA)</title>
		<meeting>24th Annu. Int. Symp. Comput. Archit. (ISCA)</meeting>
		<imprint>
			<date type="published" when="1997">1997</date>
			<biblScope unit="page" from="252" to="263" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Inter-core prefetching for multicore processors using migrating helper threads</title>
		<author>
			<persName><forename type="first">M</forename><surname>Kamruzzaman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Swanson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">M</forename><surname>Tullsen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. 16th Int. Conf. Architectural Support Program</title>
		<meeting>16th Int. Conf. Architectural Support Program</meeting>
		<imprint>
			<date type="published" when="2011">2011</date>
			<biblScope unit="page" from="393" to="404" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Profiling a warehouse-scale computer</title>
		<author>
			<persName><forename type="first">S</forename><surname>Kanev</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">P</forename><surname>Darago</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Hazelwood</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Ranganathan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Moseley</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G.-Y</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Brooks</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. 42nd Annu. Int. Symp. Comput. Archit. (ISCA)</title>
		<meeting>42nd Annu. Int. Symp. Comput. Archit. (ISCA)</meeting>
		<imprint>
			<date type="published" when="2015-06">Jun. 2015</date>
			<biblScope unit="page" from="158" to="169" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Design and evaluation of compiler algorithms for pre-execution</title>
		<author>
			<persName><forename type="first">D</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Yeung</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. 10th Int. Conf. Architectural Support Program</title>
		<meeting>10th Int. Conf. Architectural Support Program</meeting>
		<imprint>
			<date type="published" when="2002">2002</date>
			<biblScope unit="page" from="159" to="170" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">A study of source-level compiler algorithms for automatic construction of pre-execution code</title>
		<author>
			<persName><forename type="first">D</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Yeung</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Trans. Comput. Syst</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="326" to="379" />
			<date type="published" when="2004-08">Aug. 2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Path confidence based lookahead prefetching</title>
		<author>
			<persName><forename type="first">J</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">H</forename><surname>Pugsley</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">V</forename><surname>Gratz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">L N</forename><surname>Reddy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Wilkerson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Chishti</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. 49th Annu</title>
		<meeting>49th Annu</meeting>
		<imprint>
			<date type="published" when="2016-10">Oct. 2016</date>
			<biblScope unit="page" from="1" to="12" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Multi-chain prefetching: Effective exploitation of inter-chain memory parallelism for pointerchasing codes</title>
		<author>
			<persName><forename type="first">N</forename><surname>Kohout</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Yeung</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Int. Conf. Parallel Architectures Compilation Techn</title>
		<meeting>Int. Conf. Parallel Architectures Compilation Techn</meeting>
		<imprint>
			<date type="published" when="2001">2001</date>
			<biblScope unit="page" from="268" to="279" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">The performance of runtime data cache prefetching in a dynamic optimization system</title>
		<author>
			<persName><forename type="first">J</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W.-C</forename><surname>Hsu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Othmer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P.-C</forename><surname>Yew</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D.-Y</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. 36th Annu</title>
		<meeting>36th Annu</meeting>
		<imprint>
			<date type="published" when="2003-12">Dec. 2003</date>
			<biblScope unit="page" from="180" to="190" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Dynamic helper threaded prefetching on the sun UltraSPARC CMP processor,&apos;&apos; in Proc. 38th Annu</title>
		<author>
			<persName><forename type="first">J</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Das</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W.-C</forename><surname>Hsu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Nguyen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">G</forename><surname>Abraham</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE/ACM Int. Symp. Microarchitecture (MICRO)</title>
		<imprint>
			<biblScope unit="page" from="1" to="12" />
			<date type="published" when="2005-11">Nov. 2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Profile-guided post-link stride prefetching</title>
		<author>
			<persName><forename type="first">C.-K</forename><surname>Luk</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Muth</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Patil</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Weiss</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">G</forename><surname>Lowney</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Cohn</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. 16th Int. Conf. Supercomput. (ICS)</title>
		<meeting>16th Int. Conf. Supercomput. (ICS)</meeting>
		<imprint>
			<date type="published" when="2002">2002</date>
			<biblScope unit="page" from="167" to="178" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Analysis and optimization of I/O cache coherency strategies for SoC-FPGA device</title>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">W</forename><surname>Min</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>El-Hadedy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W.-M</forename><surname>Hwu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. 29th Int. Conf. Field Program</title>
		<meeting>29th Int. Conf. Field Program</meeting>
		<imprint>
			<date type="published" when="2019-09">Sep. 2019</date>
			<biblScope unit="page" from="301" to="306" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Runahead execution: An alternative to very large instruction Windows for out-of-order processors</title>
		<author>
			<persName><forename type="first">O</forename><surname>Mutlu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Stark</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Wilkerson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><forename type="middle">N</forename><surname>Patt</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. 9th Int. Symp. High-Perform</title>
		<meeting>9th Int. Symp. High-Perform</meeting>
		<imprint>
			<date type="published" when="2003">2003</date>
			<biblScope unit="page" from="129" to="140" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Techniques for efficient processing in runahead execution engines</title>
		<author>
			<persName><forename type="first">O</forename><surname>Mutlu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><forename type="middle">N</forename><surname>Patt</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. 32nd Int. Symp. Comput. Archit. (ISCA)</title>
		<meeting>32nd Int. Symp. Comput. Archit. (ISCA)</meeting>
		<imprint>
			<date type="published" when="2005">2005</date>
			<biblScope unit="page" from="370" to="381" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Bouquet of instruction pointers: Instruction pointer classifier-based spatial hardware prefetching</title>
		<author>
			<persName><forename type="first">S</forename><surname>Pakalapati</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Panda</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ACM/IEEE 47th Annu. Int. Symp. Comput. Archit. (ISCA)</title>
		<meeting>ACM/IEEE 47th Annu. Int. Symp. Comput. Archit. (ISCA)</meeting>
		<imprint>
			<date type="published" when="2020-05">May 2020</date>
			<biblScope unit="page" from="118" to="131" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Effective jump-pointer prefetching for linked data structures</title>
		<author>
			<persName><forename type="first">A</forename><surname>Roth</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">S</forename><surname>Sohi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. 26th Int. Symp. Comput. Archit. (ISCA)</title>
		<meeting>26th Int. Symp. Comput. Archit. (ISCA)</meeting>
		<imprint>
			<date type="published" when="1999">1999</date>
			<biblScope unit="page" from="111" to="121" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Energy and performance exploration of accelerator coherency port using Xilinx ZYNQ</title>
		<author>
			<persName><forename type="first">M</forename><surname>Sadri</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Weis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Wehn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Benini</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. 10th FPGAworld Conf. (FPGAworld)</title>
		<meeting>10th FPGAworld Conf. (FPGAworld)</meeting>
		<imprint>
			<date type="published" when="2013">2013</date>
			<biblScope unit="page" from="1" to="8" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Efficiently prefetching complex address patterns</title>
		<author>
			<persName><forename type="first">M</forename><surname>Shevgoor</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Koladiya</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Balasubramonian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Wilkerson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">H</forename><surname>Pugsley</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Chishti</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. 48th Int. Symp. Microarchitecture (MICRO)</title>
		<meeting>48th Int. Symp. Microarchitecture (MICRO)</meeting>
		<imprint>
			<date type="published" when="2015-12">Dec. 2015</date>
			<biblScope unit="page" from="1" to="12" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Knights landing: Second-generation Intel Xeon Phi product</title>
		<author>
			<persName><forename type="first">A</forename><surname>Sodani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Gramunt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Corbal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Vinod</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Chinthamani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Hutsell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Agarwal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Micro</title>
		<imprint>
			<biblScope unit="volume">36</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="34" to="46" />
			<date type="published" when="2016-04">Mar./Apr. 2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Using a user-level memory thread for correlation prefetching</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Solihin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Torrellas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. 29th Annu. Int. Symp. Comput. Archit. (ISCA)</title>
		<meeting>29th Annu. Int. Symp. Comput. Archit. (ISCA)</meeting>
		<imprint>
			<date type="published" when="2002-05">May 2002</date>
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="page" from="171" to="182" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">A compilerdirected data prefetching scheme for chip multiprocessors</title>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">W</forename><surname>Son</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Kandemir</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Karakoy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Chakrabarti</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. 14th ACM SIGPLAN Symp. Princ. Pract. Parallel Program</title>
		<meeting>14th ACM SIGPLAN Symp. Princ. Pract. Parallel Program</meeting>
		<imprint>
			<date type="published" when="2008">2008</date>
			<biblScope unit="page" from="209" to="218" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">The AMD &apos;Zen 2&apos; processor</title>
		<author>
			<persName><forename type="first">D</forename><surname>Suggs</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Subramony</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Bouvier</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Micro</title>
		<imprint>
			<biblScope unit="volume">40</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="45" to="52" />
			<date type="published" when="2020-04">Mar./Apr. 2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">POWER4 system microarchitecture</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">M</forename><surname>Tendler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">S</forename><surname>Dodson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">S</forename><surname>Fields</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Sinharoy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IBM J. Res. Develop</title>
		<imprint>
			<biblScope unit="volume">46</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="5" to="25" />
			<date type="published" when="2002-01">Jan. 2002</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<monogr>
		<author>
			<persName><forename type="first">K</forename><surname>Viswanathan</surname></persName>
		</author>
		<ptr target="https://software.intel.com/content/www/us/en/develop/articles/disclosure-of-hw-prefetcher-control-on-some-intel-processors.html" />
		<title level="m">Disclosure of Hardware Prefetcher Control on Some Intel Processors</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<analytic>
		<title level="a" type="main">Hitting the memory wall: Implications of the obvious</title>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">A</forename><surname>Wulf</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">A</forename><surname>Mckee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM SIGARCH Comput. Archit. News</title>
		<imprint>
			<biblScope unit="volume">23</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="20" to="24" />
			<date type="published" when="1995-03">Mar. 1995</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<analytic>
		<title level="a" type="main">A programmable memory hierarchy for prefetching linked data structures</title>
		<author>
			<persName><forename type="first">C.-L</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Lebeck</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="s">High Performance Computing (Lecture Notes in Computer Science</title>
		<imprint>
			<date type="published" when="2002">2002</date>
			<publisher>Springer</publisher>
			<pubPlace>Berlin, Germany</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<analytic>
		<title level="a" type="main">IMP: Indirect memory prefetcher</title>
		<author>
			<persName><forename type="first">X</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">J</forename><surname>Hughes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Satish</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Devadas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. 48th Int. Symp. Microarchitecture (MICRO)</title>
		<meeting>48th Int. Symp. Microarchitecture (MICRO)</meeting>
		<imprint>
			<date type="published" when="2015-12">Dec. 2015</date>
			<biblScope unit="page" from="178" to="190" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b58">
	<analytic>
		<title level="a" type="main">where he is currently pursuing the Ph.D. degree in programming languages and security. His research interests include programming languages, security, computer architecture, and education techniques</title>
		<author>
			<persName><forename type="first">B</forename><surname>Sc</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Sc</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">the IoT engineering, and the M.Sc. degree in computer science from the National University of</title>
		<meeting><address><addrLine>Bucharest, Romania; Singapore (NUS</addrLine></address></meeting>
		<imprint/>
		<respStmt>
			<orgName>University POLITEHNICA of Bucharest (UPB)</orgName>
		</respStmt>
	</monogr>
	<note>LINGFENG PEI received the B.S. degree from the Huazhong University of Science and Technology. where she is currently pursuing the Ph.D. degree in computer science. Her current research interests include prefetching, hardware security, and FPGAs</note>
</biblStruct>

<biblStruct xml:id="b59">
	<monogr>
		<title level="m" type="main">IEEE) is currently an Assistant Professor at the National University of Singapore working to develop high-efficiency microarchitectures that can meet the performance and needs of the future IoT and server applications</title>
		<author>
			<persName><forename type="first">Trevor</forename><forename type="middle">E</forename><surname>Carlson (senior</surname></persName>
		</author>
		<author>
			<persName><surname>Member</surname></persName>
		</author>
		<imprint/>
	</monogr>
	<note>He also co-develops the Sniper Multi-Core Simulator</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
