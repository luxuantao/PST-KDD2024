<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Deep CTR Prediction in Display Advertising</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Junxuan</forename><surname>Chen</surname></persName>
							<email>chenjunxuan@sjtu.edu.cn</email>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science and Engineering</orgName>
								<orgName type="institution">Shanghai Jiao Tong University</orgName>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="laboratory">Alibaba Group</orgName>
								<address>
									<settlement>Hangzhou</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Baigui</forename><surname>Sun</surname></persName>
							<email>baigui.sbg@alibaba-inc.com</email>
							<affiliation key="aff1">
								<orgName type="laboratory">Alibaba Group</orgName>
								<address>
									<settlement>Hangzhou</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Hao</forename><surname>Li</surname></persName>
							<affiliation key="aff1">
								<orgName type="laboratory">Alibaba Group</orgName>
								<address>
									<settlement>Hangzhou</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Hongtao</forename><surname>Lu</surname></persName>
							<email>htlu@sjtu.edu.cn</email>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science and Engineering</orgName>
								<orgName type="institution">Shanghai Jiao Tong University</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Xian-Sheng</forename><surname>Hua</surname></persName>
							<email>xiansheng.hxs@alibaba-inc.com</email>
							<affiliation key="aff1">
								<orgName type="laboratory">Alibaba Group</orgName>
								<address>
									<settlement>Hangzhou</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Deep CTR Prediction in Display Advertising</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">DA0F6207A4825EB197466C314F3AF851</idno>
					<idno type="DOI">10.1145/2964284.2964325</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.3" ident="GROBID" when="2023-07-28T02:43+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>DNN</term>
					<term>CNN</term>
					<term>Click through rate</term>
					<term>Image Ads</term>
					<term>Display Advertising</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Click through rate (CTR) prediction of image ads is the core task of online display advertising systems, and logistic regression (LR) has been frequently applied as the prediction model. However, LR model lacks the ability of extracting complex and intrinsic nonlinear features from handcrafted high-dimensional image features, which limits its effectiveness. To solve this issue, in this paper, we introduce a novel deep neural network (DNN) based model that directly predicts the CTR of an image ad based on raw image pixels and other basic features in one step. The DNN model employs convolution layers to automatically extract representative visual features from images, and nonlinear CTR features are then learned from visual features and other contextual features by using fully-connected layers. Empirical evaluations on a real world dataset with over 50 million records demonstrate the effectiveness and efficiency of this method.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">INTRODUCTION</head><p>Online display advertising generates a significant amount of revenue by showing textual or image ads on various web pages <ref type="bibr" target="#b3">[3]</ref>. The ad publishers like Google and Yahoo sell ad zones on different web pages to advertisers who want to show their ads to users. And then Publishers get paid by advertisers every time the ad display leads to some desired action such as clicking or purchasing according to the payment options such as cost-per-click (CPC) or cost-perconversion (CPA) <ref type="bibr" target="#b15">[15]</ref>. The expected revenue for publishers is the product of the bid price and click-through rate (CTR) or conversion rate (CVR).</p><p>Recently, more and more advertisers prefer displaying image ads <ref type="bibr" target="#b17">[17]</ref> (Figure <ref type="figure" target="#fig_0">1</ref>) because they are more attractive and comprehensible compared with textual ads. To maximize the revenue of publishers, this has led to a huge demand on approaches that are able to choose the most proper image ad to show for a particular user when he or she is visiting a web page so that to maximize the CTR or CVR.</p><p>Therefore, in most online advertising systems, predicting the CTR or CVR is the core task of ads allocation. In this paper, we focus on CPC and predict the CTR of display ads. Typically an ads system predicts and ranks the CTR of available ads based on contextual information, and then shows the top K ads to the users. In general, prediction models are learned from past click data based on machine learning techniques <ref type="bibr" target="#b3">[3,</ref><ref type="bibr" target="#b24">24,</ref><ref type="bibr">9,</ref><ref type="bibr" target="#b5">5,</ref><ref type="bibr" target="#b32">32,</ref><ref type="bibr" target="#b16">16]</ref>.</p><p>Features that are used to represent an ad are extremely important in a machine learning model. In recent years, to make the CTR prediction model more accurate, many researchers use millions of features to describe a user's response record (we call it an ad impression). Typically, an image ad impression has basic features and visual features. The basic features are information about users, products and ad positions in a web page, etc. Visual features describe the visual appearance of an image ad at different levels. For example, color and texture are low level features, while face and other contextual objects are high level features. Low level and high level features may both have the power to influence the CTR of an image ad (Figure <ref type="figure" target="#fig_1">2</ref>). Traditionally, researchers lack effective method to extract high-level visual features. The importance of visual features is also usually under estimated. However, as we can see from Figure <ref type="figure" target="#fig_1">2</ref>, ads with same basic features may have largely different CTRs due to different ad images. As a consequence, How to use the visual features in machine learning models effectively becomes an urgent task.</p><p>Among different machine learning models that have been applied to predict ads CTR using the above features, Logistic regression (LR) is the mostly well-known and widelyused one due to its simplicity and effectiveness. Also, LR is easy to be parallelized on a distributed computing system thus it is not challenging to make it work on billions of samples <ref type="bibr" target="#b3">[3]</ref>. Being able to handle big data efficiently is necessary for a typical advertising system especially when the prediction model needs to be updated frequently to deal with new ads. However, LR is a linear model which is inferior in extracting complex and effective nonlinear features from handcrafted feature pools. Though one can mitigate this issue by computing the second-order conjunctions of the features, it still can not extract higher-order nonlinear representative features and may cause feature explosion if we continue increasing the conjunction order.</p><p>To address these problems, other methods such as factorization machine <ref type="bibr" target="#b22">[22]</ref>, decision tree <ref type="bibr">[9]</ref>, neural network <ref type="bibr" target="#b32">[32]</ref> are widely used. Though these methods can extract non-linear features, they only deal with basic features and handcrafted visual features, which are inferior in describing images. In this paper, we propose a deep neural network (DNN) to directly predict the CTR of an image ad from raw pixels and other basic features. Our DNN model contains convolution layers to extract representative visual features and then fully-connected layers that can learn the complex and effective nonlinear features among basic and visual features. The main contributions of this work can be summarized as follows:</p><p>1. This paper proposed a DNN model which not only directly takes both high-dimensional sparse feature and image as input, but also can be trained from end to end. The paper is organized as follows. Section 2 introduces the related work, followed by an overview of our scheme in Section 3. In Section 4, we describe the proposed DNN model in detail, and we show the challenges in the training stage as well as our solutions in Section 5. Section 6 presents the experimental results and discussion, and then Section 7 is the conclusion.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">RELATED WORK</head><p>We consider display advertising CTR prediction and deep neural network are two mostly related areas to our work. We find too many subjects in an men's clothing ad may bring negative effect. (the number of impressions of each ad is sufficiently high to make the CTR meaningful).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Display Advertising CTR prediction</head><p>Since the display advertising has taken a large share of online advertising market, many works addressing the CTR prediction problem have been published. In <ref type="bibr" target="#b24">[24,</ref><ref type="bibr" target="#b2">2]</ref>, authors handcraft many features from raw data and use logistic regression (LR) to predict the click-through rate. <ref type="bibr" target="#b3">[3]</ref> also uses LR to deal with the CTR problem and scales it to billions of samples and millions of parameters on a distributed learning system. In <ref type="bibr" target="#b20">[20]</ref>, a Hierarchical Importance-aware Factorization Machine (FM) <ref type="bibr" target="#b23">[23]</ref> is introduced, which provides a generic latent factor framework that incorporates importance weights and hierarchical learning. In <ref type="bibr" target="#b5">[5]</ref>, boosted decision trees have been used to build a prediction model. In <ref type="bibr">[9]</ref>, a model which combines decision trees with logistic regression has been proposed, and outperforms either of the above two models. <ref type="bibr" target="#b32">[32]</ref> combines deep neural networks with FM and also brings an improvement. All of these methods are very effective when deal with ads without images. However, when it comes to the image ads, they can only use pre-extracted image features, which is less flexible to take account of the unique properties of different datasets.</p><p>Therefore, the image features in display advertisement have received more and more attention. In <ref type="bibr" target="#b1">[1,</ref><ref type="bibr" target="#b4">4]</ref>, the impact of visual appearance on user's response in online display advertising is considered for the first time. They extract over 30 handcrafted features from ad images and build a CTR prediction model using image and basic features. The experiment result shows that their method achieves better performance than models without visual features. <ref type="bibr" target="#b18">[18]</ref> is the most related work in literature with us, in which a decapitated convolutional neural network (CNN) is used to extract image features from ads. However, there are two important differences between their method and ours. First, they do not consider basic features when extracting image features using CNN. Second, when predicting the CTR they use logistic regression which lacks the ability in exploring the complex relations between image and basic features. Most of the information in their image features is redundant such as product category which is included in basic features. As a result, their model only achieves limited improvements when combining both kinds of features. Worse still, when the dataset contains too many categories of products, it can hardly converge when training. Our model uses an end to end model to predict the CTR of image ads using basic features and raw images in one step, in which image features can be seen as supplementary to the basic features.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Deep Neural Network</head><p>In recent years, deep neural network has achieved big breakthroughs in many fields. In computer vision field, convolutional neural network (CNN) <ref type="bibr" target="#b13">[13]</ref> is one of the most efficient tools to extract effective image features from raw image pixels. In speech recognition, deep belief network (DBN) <ref type="bibr" target="#b10">[10]</ref> is used and much better performance is obtained comparing with Gaussian mixture models. Comparing with traditional models that have shallow structure, deep learning can model the underlying patterns from massive and complex data. With such learning ability, deep learning can be used as a good feature extractor and applied into many other applications <ref type="bibr" target="#b21">[21,</ref><ref type="bibr" target="#b26">26]</ref>.</p><p>In CTR prediction field, besides <ref type="bibr" target="#b32">[32]</ref> that is mentioned in Section 2.1, DNN has also been used in some public CTR prediction competitions 12 recently. In these two competitions, only basic features are available for participants. An ensemble of four-layer DNNs which use fully-connected layers and different kinds of non-linear activations achieves better or comparable performance than LR with feature conjunction, factorization machines, decision trees, etc. Comparing with this method, our model can extract more powerful features by taking consideration of the visual features in image ads.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">METHOD OVERVIEW</head><p>As aforementioned, in this paper, each record of user's behavior on an ad is called an impression. denoted by x. Each impression has an image u with a resolution of around 120 × 200. Besides the image, the basic feature vector is denoted by v ∈ R d such as the user's gender, product's category, ad position in the web page, and usually d can be very large, say, from a few thousand to many million. Our goal is to predict the probability that a user clicks on an image ad given these features. We will still use logistic regression to map our predicted CTR value ŷ to 0 to 1, thus the CTR prediction problem can be written as:</p><formula xml:id="formula_0">ŷ = 1 1 + e -z<label>(1)</label></formula><formula xml:id="formula_1">z = f (x)<label>(2)</label></formula><p>where f (.) is what we are going to learn from training data, that is, the embedding function that maps an impression to a real value z. Suppose we have N impressions X = [x1, x2...xN ] and each with a label yi ∈ {0, 1} depends on the user's feedback, 0 means not clicked while 1 means clicked.</p><p>Then the learning problem is defined as minimizing a Loga- rithmic Loss (Logloss):</p><formula xml:id="formula_2">L(W) = - 1 N i (yi log ŷi + (1 -yi) log(1 -ŷi)) + λ||W|| 2</formula><p>(3) where W is the parameters of the embedding function f (.) and λ is a regularization parameter that controls the model complexity.</p><p>In this model, what we need to learn is the embedding function f (.). Conventional methods extract handcrafted visual features from raw image u and concatenate them with basic features v, then learn linear or nonlinear transformations to obtain the embedding function. In this paper we learn this function directly from raw pixels of an image ad and the basic features using one integrated deep neural network.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">NETWORK ARCHITECTURE</head><p>Considering basic features and raw images come from two different domains, we cannot simply concatenate them together directly in the network. Training two separate networks is also inferior since it cannot take into account the correlations between the two features. As a result, our network adopts two different sub-networks to deal with basic features and raw images, respectively, and then uses multiple fully-connected layers to capture their correlations.</p><p>As illustrated in Figure <ref type="figure" target="#fig_2">3</ref>, a deep neural network called DeepCTR is designed which contains three parts. One part, Convnet, takes raw image u as input and follows with a convolutional network. The output of the Convnet is a feature vector of the raw image. The second part which is called Basicnet, takes basic features v as input and applies a fullyconnected layer to reduce the dimensionality. Subsequently, outputs of Convnet and Basicnet are concatenated into one  vector and fed to two fully-connected layers. The output of the last fully-connected layer is a real value z. This part is called Combnet. On the top of the whole network, Logloss is computed as described in Section 3.</p><p>The design of Convnet is inspired by the network in <ref type="bibr" target="#b7">[7,</ref><ref type="bibr" target="#b27">27]</ref>, as shown in Figure <ref type="figure" target="#fig_3">4</ref>. The network consists of 17 convolution layers. The first convolution layer uses 5 × 5 convolution kernels. Following first layer, there are four groups and each has four layers with 3 × 3 kernels. We do not build a very deep network such as more than 50 layers in consideration of the trade off between performance and training time. We pre-train the Convnet on the images in training dataset with category labels. We use two fully-connected layers with 1024 hidden nodes (we call them fc18 and fc19 ), a fullyconnected layer with 96-way outputs (we call it fc20 ) and a softmax after the Convnet in pre-training period. Since our unique images set is smaller (to be detailed in Section 6) than ImageNet <ref type="bibr" target="#b6">[6]</ref>, we use half the number of outputs in each group comparing with <ref type="bibr" target="#b7">[7]</ref>. After pre-training, a 128way fully-connected layer is connected behind the last convolution layer. Then we train the whole DeepCTR using Logloss from end to end.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">SPEED UP TRAINING</head><p>An online advertising system has a large number of new user response records everyday. It is necessary for ad systems to update as frequently as possible to adapt new tendency. An LR model with distributed system requires several hours to train with billions of samples, which makes it popular in industry.</p><p>Typically a deep neural network has millions of parameters which makes it impossible to train quickly. With the development of GPUs, one can train a deep CNN with 1 million training images in two days on a single machine. However, it is not time feasible for our network since we have more than 50 million samples. Moreover, the dimensionality of basic features is nearly 200,000 which leads to much more parameters in our network than a normal deep neural network. Directly training our network on a single machine may take hundreds of days to converge according to a rough estimation. Even using multi-machine can hardly resolve the training problem. We must largely speed up the training if we want to deploy our DeepCTR on a real online system.</p><p>To make it feasible to train our model with less than one day, we adopt two techniques: using sparse fully-connected layer and a new data sampling scheme. The use of these two techniques makes the training time of our DeepCTR suitable for a real online system.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Sparse Fully-Connected Layer</head><p>In CTR prediction, the basic feature of an ad impression includes user information like gender, age, purchasing power, and ad information like ad ID, ad category, ad zone, etc. This information is usually encoded by one-hot encoding or feature hashing <ref type="bibr" target="#b29">[29]</ref> which makes the feature dimension very large. For example, it is nearly 200,000 in our dataset. Consequently, in Basicnet, the first fully-connected layer using the basic feature as input has around 60 million parameters, which is similar to the number of all the parameters in AlexNet <ref type="bibr" target="#b13">[13]</ref>. However, the basic feature is extremely sparse due to the one-hot encoding. Using sparse matrix in first fully-connected layer can largely reduce the computing complexity and GPU memory usage.</p><p>In our model, we use compressed sparse row (CSR) format to represent a batch of basic features V . When computing network forward</p><formula xml:id="formula_3">Y f c1 = V W,<label>(4)</label></formula><p>sparse matrix operations can be used in the first fully-connected layer. When backward pass, we only need to update the weights that link to a small number of nonzero dimensions according to the gradient</p><formula xml:id="formula_4">∇(W ) = V.<label>(5)</label></formula><p>Both of the forward pass and backward pass only need a time complexity of O(nd ) where d is the number of nonzero elements in basic features and d d. An experiment result that compares the usages of time and GPU memory with/out sparse fully-connected layer can be found in Section 6.2.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Data Sampling</head><p>Another crucial issue in training is that Convnet limits the batch-size of Stochastic Gradient Descent (SGD). To train a robust CTR prediction model, we usually need millions of ad samples. However, the Convnet requires lots of GPU memory, which makes our batch-size very small, say, a few hundred. For a smaller batch-size, the parallel computing of GPU can not maximize the effect in the multiplication of large matrix. And the number of iterations of each epoch will be very large. We need much more time to run over an epoch of iterations. Though the sparse fully-connected</p><formula xml:id="formula_5">0 0 1 0 • • • • • • 0 0 1 0 1 0 0 • • • • • • 1 0 0 • • • 1 0 0 1 • • • • • • 0 1 0 0 0 0 0 • • • • • • 1 1 0 0 1 0 1 • • • • • • 0 0 0 • • • 0 1 0 0 • • • • • • 0 0 1 Convent Basicnet { { Figure 5:</formula><p>Originally, the image and the basic feature vector are one to one correspondence. In our data sampling method, we group basic features of an image together, so that we can deal with much more basic features per batch.</p><p>layer can largely reduce the forward-backward time in Basicnet, training the whole net on such a large dataset still requires infeasible time. Also, the gradient of each iteration is unstable in the case of smaller batch-size, which makes the convergence harder. In CTR prediction, this problem is even more serious because the training data is full of noise.</p><p>In this paper, we propose a simple but effective training method based on an intrinsic property of the image ads click records, that is, many impressions share a same image ad. Though the total size of the dataset is very large, the number of unique images is relatively smaller. Since a good many of basic features can be processed quickly by sparse fullyconnected layer, we can set a larger batch-size for Basicnet and a smaller one for Convnet. In this paper we employ a data sampling method that groups basic features of a same image ad together to achieve that (Figure <ref type="figure">5</ref>), which is detailed as follows.</p><p>Suppose the unique images set in our dataset is U, the set of impressions related to an image u are Xu and basic features are Vu. At each iteration, suppose the training batch size is n, we sample n different images U from U. Together with each image u ∈ U , we sample k basic features Vu from Vu with replacement. Thus we have n images and kn basic features in each batch. After Convnet, we have n image features. For each feature vector convu we copy it k times to have Cu and send them forward to Combnet along with Vu. In backward time, the gradient of each image feature vector can be computed as:</p><formula xml:id="formula_6">∇(convu) = 1 k c∈Cu ∇(c)<label>(6)</label></formula><p>The training method is summarized in Alg. 1 and Alg. 2. In fact, this strategy makes us able to deal with kn samples in a batch. Since the sparse fully-connected layer requires very small GPU memory, we can set k a very big value according to the overall average number of basic feature vectors of A larger batch-size also makes the gradient of each batch much more stable which leads to the model easy to converge. We also conduct an experiment to evaluate whether this sampling method influences the performance of DeepCTR comparing a throughly shuffle strategy in Section 6.2.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">EXPERIMENT</head><p>In this section, a series of experiments are conducted to verify the superiority of our DeepCTR model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.1">Experimental Setting</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.1.1">Dataset</head><p>The experiment data comes from a commercial advertising platform (will expose it in the final version) in an arbitrary week of year 2015. We use the data from first six days as our training data and the data from last day (which is a Friday) as testing data. As described in Section 3, each impression consists of an ad x and a label y. An impression has an image u (Figure <ref type="figure" target="#fig_1">2</ref>) and a basic feature vector v. The size of training data is 50 million while testing set is 9 million. The ratio of positive samples and negative samples is around 1:30. We do not perform any sub-sampling of negative events on the dataset. We have 101,232 unique images in training data and 17,728 unique images in testing data. 3,090 images in testing set are never shown in training set. Though the image data of training set and test data are highly overlapped, they follow the distribution of the real-world data. To make our experiment more convincing, we also conduct a experiment on a sub test set that only contains new images data that never been used in training. The basic feature v is onehot encoded and has a dimensionality of 153,231. Following information is consisted by basic features:</p><p>1. Ad zone. The display zone of an ad on the web page.</p><p>We have around 700 different ad zones in web pages.</p><p>2. Ad group. The ad group is a small set of ads. The ads in an ad group share almost same products but different ad images (in Figure <ref type="figure" target="#fig_1">2</ref>, (a) and (b) belong to an ad group while (c) and (d) belong to another group). We have over 150,000 different ad groups in our dataset. Each ad group consists less than 20 different ads.</p><p>3. Ad target. The groups of target people of the ad. We have 10 target groups in total.</p><p>4. Ad category. The category of the product in ads. We have 96 different categories, like clothing, food, household appliances.</p><p>5. User. The user information includes user's gender, age, purchasing power, etc.</p><p>Besides above basic features, we do not use any handcrafted conjunction features. We hope that our model can learn effective non-linear features automatically from feature pools.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.1.2">Baselines</head><p>We use LR only with basic features as our first baseline. We call this method lr basic in following experiments. To verify that our DNN model has the ability of extracting effective high-order features, a Factorization Machine implemented by LibFM <ref type="bibr" target="#b23">[23]</ref> only with basic features is our second baseline. We call it FM basic. We use 8 factors for 2-way interactions and MCMC for parameter learning in FM basic. Then we evaluate a two hidden layers DNN model only using basic features. The numbers of outputs of two hidden layers are 128 and 256 respectively. The model can be seen as our DeepCTR net without the Convnet part. This method is called dnn basic. We further replace the Convnet in our DeepCTR net with pre-extracted features, SIFT <ref type="bibr" target="#b14">[14]</ref> with bag of words and the outputs of different layers of the pre-trained Convnet. We call these two methods dnn sift and dnn layername (for example dnn conv17 ).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.1.3">Evaluation Metric</head><p>We use two popular metrics to evaluate the experiment result, Logloss and the area under receiver operator curve (AUC). Logloss can quantify the accuracy of the predicted click probability. AUC measures the ranking quality of the prediction. Our dataset comes from a real commercial platform, so both of these metrics use relative numbers comparing with lr basic.</p><p>Since the AUC value is always larger than 0.5, we remove this constant part (0.5) from the AUC value and then compute the relative numbers as in <ref type="bibr" target="#b30">[30]</ref>: relative AUC = ( AU C(method) -0.5 AU C(lr basic) -0.5 -1) × 100% (8)</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.1.4">Network Configuration</head><p>In our Convnet, a 112 × 112 random crop and horizontal mirror for the input image are used for data augmentation.  Each group has four convolution layers followed by a batch normalization <ref type="bibr" target="#b11">[11]</ref> and a ReLU <ref type="bibr" target="#b19">[19]</ref> activation. The stride of the first convolution layer is 2 if the output size of a group halves. We initialize the layer weights as in <ref type="bibr" target="#b8">[8]</ref>. When pretraining the Convnet on our image dataset with category labels, we use SGD with a mini-batch size of 128. The learning rate starts from 0.01 and is divided by 10 when test loss plateaus. The pre-trained model converges after around 120 epochs. The weight decay of the net is set as 0.0001 and momentum is 0.9.</p><p>After pre-training Convnet, we train our DeepCTR model from end to end. Other parts of our net use the same initialization method as Convnet. We choose the size of mini-batch n as 20, and k = 500. That is to say, we deal with 10,000 impressions per batch. We start with the learning rate 0.1, and divided it by 10 after 6×10 4 , 1×10 5 and 1.4×10 5 iterations. The Convnet uses a smaller initial learning rate 0.001 in case of destroying the pre-trained model. The weight decay of the whole net is set as 5 × 10 -5 . The dnn basic, dnn sift and dnn layername use the same learning strategy.</p><p>We implement our deep network on C++ Caffe toolbox <ref type="bibr" target="#b12">[12]</ref> with some modifications like sparse fully-connected layer.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.2">Results and Discussion</head><p>In this section we compare the results of various methods and the effects of some network structures. First we compare the results of models with deep features in different levels. We plot the two metrics of dnn conv13, dnn conv17, dnn fc18, dnn fc19, and dnn fc20 in the middle of Table <ref type="table" target="#tab_2">1</ref>. From the results, we find that dnn conv17 and dnn fc18 achieve best performance. Image features in these layers are of relatively high level but not highly group invariant <ref type="bibr" target="#b31">[31]</ref>. Comparing with following fully-connected layers, they have more discriminations in same category. Comparing with previous layers, they contain features in a sufficiently high-level which are superior in describing the objects in images. Consequently, we connect conv17 layer in our DeepCTR model. We do not choose fc18 because it needs higher computations. We have also tried to compare our DeepCTR with the approach in <ref type="bibr" target="#b18">[18]</ref>. However the model in <ref type="bibr" target="#b18">[18]</ref> does not  converge on our dataset. We think the reason is that our dataset consists of too many categories of products while in their datasets only 5 different categories are available.</p><p>Comparison between other baselines is shown in Table <ref type="table" target="#tab_2">1</ref> too. From the result, it can be seen that a deep neural network and image features can both improve the CTR prediction accuracy. FM basic and dnn basic achieve almost same improvements comparing with lr basic, which indicates that these two models both have strong power in extracting effective non-linear basic features. For the image part, comparing with handcrafted features, like SIFT, deep features have stronger power in describing the image, which leads to a significant improvement in the prediction accuracy. Our DeepCTR model goes one step further by using an end to end learning scheme. Ensemble of multiple deep networks usually brings better performance, so we train 3 DeepCTR models and average their predictions, and it gives the best AUC and Logloss. Compared with lr basic, the AUC increase will bring us 1∼2 percent CTR increase in the advertising system (according to online experiments), which will lead to over 1 million earnings growth per day for an 100 million ads business.</p><p>To make our results more convincing, we also conduct an experiment on the sub test set that only contains 3,090 images that are never shown in training set. The relative AUC and Logloss of three representative methods dnn basic, dnn sift and a single DeepCTR are in Table <ref type="table" target="#tab_3">2</ref>. Clearly, our DeepCTR wins by a large margin consistently. We also notice that while the AUC of dnn basic decreases, dnn sift and our DeepCTR have an even higher relative AUC than the result on the full test set. Ad images in 3K sub test For different real-world problems, different techniques may be needed due to the intrinsic characteristics of the problem, network design and data distribution. Therefore, solutions based on deep learning typically will compare and analyze the impact and effectiveness of those techniques to find the best practices for a particular problem. Therefore, we further explore the influence of different deep learning techniques in our DeepCTR model empirically.</p><p>First we find that the batch normalization in the Combnet can speed up training and largely improve performance (Figure <ref type="figure" target="#fig_6">6</ref>). To investigate the reason, we show the histogram (Figure <ref type="figure" target="#fig_7">7</ref>) of the outputs of Convnet and BasicN et. We can see from the histogram that two outputs have significant difference in scale and variance. Simply concatenating these two different kinds of data stream makes the following fully-connected layer hard to converge.</p><p>Dropout <ref type="bibr" target="#b28">[28]</ref> is an efficient way to prevent over-fitting problem in deep neural network. Most deep convolution networks remove the dropout because batch normalization can regularize the models <ref type="bibr" target="#b7">[7,</ref><ref type="bibr" target="#b11">11]</ref>. However, in our DeepCTR model, we find it still suffers from over-fitting without dropout. We compare the loss curves of the model with/without dropout in the last two fully-connected layers. We can see that the model with dropout achieves lower testing Logloss, though we need more time to reach the lowest test loss.</p><p>We also evaluate the performance of the sparse fully-connected layer and our data sampling method. We plot computing time and memory overhead (Table <ref type="table" target="#tab_4">3</ref>) of the sparse fullyconnected layer comparing with dense layer. Loss curves of training and testing are exactly the same since sparse fully-   Finally, we investigate whether the performance of our model descends using our data sampling method comparing a throughly shuffle. We only evaluate the sampling method on dnn conv17 model, that is, we conduct experiments on a model where the Convnet is frozen. Ideally, we should use an unfrozen Convnet without data sampling as the contrast experiment. However, as mentioned in Section 5.2, training an unfrozen Convnet limits our batch-size less than 200 because the Convnet needs much more GPU memory, while a model with frozen Convnet can deal with more than 10000 samples in a batch. It will takes too much time to training our model on such a small batch-size. Also, the main difference between with/out sampling is whether the samples were thoroughly shuffled, while freezing the Convnet or not does not influence the order of samples. Therefore, we believe that our DeepCTR model performs similarly with dnn conv17 model. From Table <ref type="table" target="#tab_5">4</ref> we can see the performance of the model is not influenced by data sampling method. At the same time, our method costs far less training time comparing with the approach without data sampling. Using this data sampling method, training our DeepCTR model from end to end only takes around 12 hours to converge on a NVIDIA TESLA k20m GPU with 5 GB memory, which is acceptable for an online advertising system requiring daily update. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.3">Visualizing the Convnet</head><p>Visualizing the CNN can help us better understand exactly what we have learned. In this section, we follow the saliency map visualization method used in <ref type="bibr" target="#b25">[25]</ref>. We use a linear score model to approximate our DeepCTR for clicked or not clicked :</p><formula xml:id="formula_7">z(U ) ≈ w T U + b,<label>(9)</label></formula><p>where image U is in the vectorized (one-dimension) form, and w and b are weight and bias of the model. Indeed, Eq 9 can be seen as the first order Taylor expansion of our DeepCTR model. We use the magnitude of elements of w to show the importance of the corresponding pixels of U for the clicked probability. where w is the derivative of z with respect to the image U at the point (image) U0:</p><formula xml:id="formula_8">w = ∂z ∂U U 0<label>(10)</label></formula><p>In our case, the image U is in RGB format and has three channels at pixel Ui,j. To derive a single class saliency value Mi,j of each pixel, we take the maximum absolute value of wi,j across RGB channels c:</p><p>Mi,j = maxc|wi,j(c)| (11) Some of the typical visualization examples are shown as heat maps in Figure <ref type="figure" target="#fig_10">10</ref>. In these examples, brighter area plays a more important role in impacting the CTR of the ad. We can see main objects in ads are generally more important. However, some low level features like texture, characters, and even background can have effect on the CTR of the ad. In another example (Figure <ref type="figure">9</ref>), it is more clearly to see that visual features in both high level and low level have effectiveness. From the visualization of the Convnet, we can find that the task of display ads CTR prediction is quite different from object classification where high level features dominate in the top layers. It also gives an explanation why an end-to-end training can improve the model. Apparently, the Convnet can be trained to extract features that are more particularly useful for CTR prediction.</p><p>The visualization provides us an intuitive understanding of the impact of visual features in ad images which may be useful for designers to make design choices. For example, we can decide whether add another model or not in an ad according to the saliency map of this model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.">CONCLUSIONS</head><p>CTR prediction plays an important role in online display advertising business. Accurate prediction of the CTR of ads not only increases the revenue of web publishers, also improves the user experience. In this paper we propose an end to end integrated deep network to predict the CTR of image ads. It consists of Convnet, Basicnet and Combnet. Convnet is used to extract image features automatically while Basicnet is used to reduce the dimensionality of basic features. Combnet can learn complex and effective non-linear features from these two kinds of features. The usage of sparse fully-connected layer and data sampling techniques speeds up the training process significantly. We evaluate DeepCTR model on a 50 million real world dataset. The empirical result demonstrates the effectiveness and efficiency of our DeepCTR model.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: Display ads on an e-commerce web page.</figDesc><graphic coords="1,323.39,206.22,225.95,98.01" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: Two groups of image ads in each row. The two ads in each group have completely same ad group id, ad zone and target people. CTRs of image ads (a) and (b) are 1.27% and 0.83%. (b) suffers from low contrast between product and background obviously. CTRs of (c) and (d) are 2.40% and 2.23%.We find too many subjects in an men's clothing ad may bring negative effect. (the number of impressions of each ad is sufficiently high to make the CTR meaningful).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>1Figure 3 :</head><label>3</label><figDesc>Figure 3: The overall architecture of the network. The output of each fully-connected layer is then pass through a ReLU nonlinear activation function.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 4 :</head><label>4</label><figDesc>Figure 4: The architecture of the 17-layer Convnet in our model.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Algorithm 1 4 :</head><label>14</label><figDesc>Training a DeepCTR network Input: : Network N et with parameter W, unique images set U, basic features set V, labels Y, batch size n, basic feature sample number k. Output: : Network for CTR prediction, N et 1: Initialize N et. 2: Compute the sample probability p(u) of each image u, Sample n images U according to p(u). 5: For each u in U , sample k basic features Vu from Vu with labels Yu uniformly with replacement. 6: f orward backward(N et, U, V, Y ). 7: until N et converges Algorithm 2 f orward backward Input: : Network N et with parameters W which contains a Convnet, Basicnet and Combnet, image samples U , basic features V , labels Y , basic feature sample number k. 1: Compute the feature vector convu of each image u: conv = net f oward(Convnet, U ) 2: Copy each feature vector k times so we have C. 3: loss = net f orward(Basicnet and Combnet, V, C). 4: ∇(C) = net backward(Combnet and Basicnet, loss). 5: Compute ∇(convu) of each image u according to Eq. 6. 6: net backward(Convnet, ∇(conv)). 7: Update network N et. image ads. This strategy reduces the number of iterations of an epoch to several thousand and largely speeds up training.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 6 :</head><label>6</label><figDesc>Figure 6: Test Logloss of the DeepCTR net with/without batch normalization in Combnet.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure 7 :</head><label>7</label><figDesc>Figure 7: (a) and (b) are the histograms of outputs of Basicnet and Convnet without batch normalization while (c) and (d) with batch normalization.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>Figure 8 :</head><label>8</label><figDesc>Figure 8: Logloss of the DeepCTR net with/without dropout in Combnet. Dashed lines denote training loss, and bold lines denote test loss.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head>Figure 10 :</head><label>10</label><figDesc>Figure 10: Saliency map of the image ads. Brighter area plays a more important role in effecting the CTR of ads.</figDesc><graphic coords="9,71.46,229.55,73.01,74.32" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 1 :</head><label>1</label><figDesc>relative AUC and Logloss. All the numbers are best resuts achieved in three repeated experiments. We omit dnn of methods using deep neural network with pre-extracted features.</figDesc><table><row><cell cols="2">method</cell><cell></cell><cell cols="4">lr basic FM basic basic</cell><cell>sift</cell><cell cols="3">conv13 conv17 fc18</cell><cell>fc19</cell><cell>fc20 DeepCTR 3 DeepCTRs</cell></row><row><cell cols="3">AUC(%)</cell><cell>-</cell><cell></cell><cell>1.47</cell><cell>1.41</cell><cell>1.69</cell><cell>3.92</cell><cell>4.13</cell><cell>4.10</cell><cell>3.48</cell><cell>3.04</cell><cell>5.07</cell><cell>5.92</cell></row><row><cell cols="3">Logloss(%)</cell><cell>-</cell><cell></cell><cell>-0.40</cell><cell cols="2">-0.39 -0.45</cell><cell>-0.79</cell><cell>-0.86</cell><cell cols="3">-0.86 -0.74 -0.69</cell><cell>-1.11</cell><cell>-1.30</cell></row><row><cell>-0.4 0</cell><cell>-0.2</cell><cell>0</cell><cell>0.2</cell><cell>0.4</cell><cell>0.6</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 2 :</head><label>2</label><figDesc>new ads added into the ad system and the ad groups have not appeared in the training set. This lead to the prediction worse in 3K sub test set because the ad group feature does not exist in the training set. However, though we lack some basic features, visual features bring much more supplementary information. This is the reason that dnn sift and DeepCTR have more improvements over the baseline methods (which only have basic features) in 3K sub test set comparing with the full test set. This experiment shows that visual features can be used to identify ads with similar characteristics and thus to predict the CTR of new image ads more accurately. It also verifies that our model indeed has strong generalization ability but not memories the image id rigidly.</figDesc><table><row><cell cols="3">relative AUC and Logloss of the sub test</cell></row><row><cell cols="3">set that only contains images never shown in the</cell></row><row><cell>training set.</cell><cell></cell><cell></cell></row><row><cell></cell><cell cols="2">AUC (%) Logloss (%)</cell></row><row><cell>lr basic</cell><cell>-</cell><cell>-</cell></row><row><cell>dnn basic</cell><cell>1.07</cell><cell>-0.21</cell></row><row><cell>dnn sift</cell><cell>2.14</cell><cell>-0.48</cell></row><row><cell>DeepCTR</cell><cell>5.54</cell><cell>-0.85</cell></row><row><cell>set are all</cell><cell></cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 3 :</head><label>3</label><figDesc></figDesc><table><row><cell>sparse layer</cell><cell>6.67</cell><cell>397</cell></row><row><cell>dense layer</cell><cell>189.56</cell><cell>4667</cell></row><row><cell cols="3">connected layer does not change any computing results in</cell></row><row><cell cols="3">the net, so we do not plot them. From this table we can find</cell></row><row><cell cols="3">dense layer requires much more computing time and mem-</cell></row><row><cell cols="3">ory than sparse one. Using sparse layer allows a lager batch</cell></row><row><cell cols="3">size when training, which speeds up the training and makes</cell></row><row><cell cols="2">the net much easier to converge.</cell><cell></cell></row></table><note><p><p>forward-backward time and GPU memory overhead of first fully-connected layer with a batch size of 1,000.</p>time (ms) memory (MB)</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 4 :</head><label>4</label><figDesc>AUC and Logloss of dnn conv17 model with our data sampling and a throughly shuffle.Figure9: Saliency map of an image ad. We can see that cats, texture, and characters all have effect on the CTR.</figDesc><table><row><cell></cell><cell cols="2">AUC(%) Logloss(%)</cell></row><row><cell>data sampling</cell><cell>4.13</cell><cell>-0.86</cell></row><row><cell>throughly shuffle</cell><cell>4.12</cell><cell>-0.86</cell></row></table></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8.">ACKNOWLEDGMENTS</head><p>This paper is partially supported by NSFC (No. 61272247, 61533012, 61472075), the 863 National High Technology Research and Development Program of China (SS2015AA020501), the Basic Research Project of Innovation Action Plan (16JC1402800) and the Major Basic Research Program (15JC1400103) of Shanghai Science and Technology Committee.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title/>
		<author>
			<persName><surname>References</surname></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">The impact of visual appearance on user response in online display advertising</title>
		<author>
			<persName><forename type="first">J</forename><surname>Azimi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Navalpakkam</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Mao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Fern</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 21st international conference companion on World Wide Web</title>
		<meeting>the 21st international conference companion on World Wide Web</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2012">2012</date>
			<biblScope unit="page" from="457" to="458" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Contextual advertising by combining relevance with click feedback</title>
		<author>
			<persName><forename type="first">D</forename><surname>Chakrabarti</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Agarwal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Josifovski</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 17th international conference on World Wide Web</title>
		<meeting>the 17th international conference on World Wide Web</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2008">2008</date>
			<biblScope unit="page" from="417" to="426" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Simple and scalable response prediction for display advertising</title>
		<author>
			<persName><forename type="first">O</forename><surname>Chapelle</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Manavoglu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Rosales</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Transactions on Intelligent Systems and Technology</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page">61</biblScope>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Multimedia features for click prediction of new ads in display advertising</title>
		<author>
			<persName><forename type="first">H</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">V</forename><surname>Zwol</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Azimi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Manavoglu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Navalpakkam</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 18th ACM SIGKDD international conference on Knowledge discovery and data mining</title>
		<meeting>the 18th ACM SIGKDD international conference on Knowledge discovery and data mining</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2012">2012</date>
			<biblScope unit="page" from="777" to="785" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Learning the click-through rate for rare/new ads from similar ads</title>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">S</forename><surname>Dave</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Varma</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 33rd international ACM SIGIR conference on Research and development in information retrieval</title>
		<meeting>the 33rd international ACM SIGIR conference on Research and development in information retrieval</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2010">2010</date>
			<biblScope unit="page" from="897" to="898" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Imagenet: A large-scale hierarchical image database</title>
		<author>
			<persName><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L.-J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision and Pattern Recognition</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2009">2009. 2009</date>
			<biblScope unit="page" from="248" to="255" />
		</imprint>
	</monogr>
	<note>CVPR 2009. IEEE Conference on</note>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1512.03385</idno>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">Delving deep into rectifiers: Surpassing human-level performance on imagenet classification</title>
		<author>
			<persName><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1502.01852</idno>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Practical lessons from predicting clicks on ads at facebook</title>
		<author>
			<persName><forename type="first">X</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName><surname>Xu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of 20th ACM SIGKDD Conference on Knowledge Discovery and Data Mining</title>
		<meeting>20th ACM SIGKDD Conference on Knowledge Discovery and Data Mining</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="1" to="9" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Deep neural networks for acoustic modeling in speech recognition: The shared views of four research groups</title>
		<author>
			<persName><forename type="first">G</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">E</forename><surname>Dahl</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mohamed</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Signal Processing Magazine</title>
		<imprint>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="82" to="97" />
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
	<note>IEEE</note>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<author>
			<persName><forename type="first">S</forename><surname>Ioffe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1502.03167</idno>
		<title level="m">Batch normalization: Accelerating deep network training by reducing internal covariate shift</title>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Caffe: Convolutional architecture for fast feature embedding</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Shelhamer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Donahue</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Karayev</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Guadarrama</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the ACM International Conference on Multimedia</title>
		<meeting>the ACM International Conference on Multimedia</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="675" to="678" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Imagenet classification with deep convolutional neural networks</title>
		<author>
			<persName><forename type="first">A</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<editor>
			<persName><forename type="first">F</forename><surname>Pereira</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">C</forename><surname>Burges</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">L</forename><surname>Bottou</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">K</forename><surname>Weinberger</surname></persName>
		</editor>
		<imprint>
			<publisher>Curran Associates, Inc</publisher>
			<date type="published" when="2012">2012</date>
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="page" from="1097" to="1105" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Object recognition from local scale-invariant features</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">G</forename><surname>Lowe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The proceedings of the seventh IEEE international conference on</title>
		<imprint>
			<publisher>Ieee</publisher>
			<date type="published" when="1999">1999. 1999</date>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="1150" to="1157" />
		</imprint>
	</monogr>
	<note>Computer vision</note>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Pay-per-action model for online advertising</title>
		<author>
			<persName><forename type="first">M</forename><surname>Mahdian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Tomak</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 1st international workshop on Data mining and audience intelligence for advertising</title>
		<meeting>the 1st international workshop on Data mining and audience intelligence for advertising</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2007">2007</date>
			<biblScope unit="page" from="1" to="6" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Ad click prediction: a view from the trenches</title>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">B</forename><surname>Mcmahan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Holt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Sculley</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Young</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Ebner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Grady</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Nie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Phillips</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Davydov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Golovin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 19th ACM SIGKDD international conference on Knowledge discovery and data mining</title>
		<meeting>the 19th ACM SIGKDD international conference on Knowledge discovery and data mining</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2013">2013</date>
			<biblScope unit="page" from="1222" to="1230" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Internet multimedia advertising: techniques and technologies</title>
		<author>
			<persName><forename type="first">T</forename><surname>Mei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X.-S</forename><surname>Hua</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 19th ACM international conference on Multimedia</title>
		<meeting>the 19th ACM international conference on Multimedia</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2011">2011</date>
			<biblScope unit="page" from="627" to="628" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Image feature learning for cold start problem in display advertising</title>
		<author>
			<persName><forename type="first">K</forename><surname>Mo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Jiang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 24th International Conference on Artificial Intelligence, IJCAI&apos;15</title>
		<meeting>the 24th International Conference on Artificial Intelligence, IJCAI&apos;15</meeting>
		<imprint>
			<publisher>AAAI Press</publisher>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="3728" to="3734" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Rectified linear units improve restricted boltzmann machines</title>
		<author>
			<persName><forename type="first">V</forename><surname>Nair</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 27th International Conference on Machine Learning (ICML-10)</title>
		<meeting>the 27th International Conference on Machine Learning (ICML-10)</meeting>
		<imprint>
			<date type="published" when="2010">2010</date>
			<biblScope unit="page" from="807" to="814" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Predicting response in mobile advertising with hierarchical importance-aware factorization machine</title>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">J</forename><surname>Oentaryo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E.-P</forename><surname>Lim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J.-W</forename><surname>Low</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Lo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Finegold</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 7th ACM international conference on Web search and data mining</title>
		<meeting>the 7th ACM international conference on Web search and data mining</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="123" to="132" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Faster r-cnn: Towards real-time object detection with region proposal networks</title>
		<author>
			<persName><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="91" to="99" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Factorization machines</title>
		<author>
			<persName><forename type="first">S</forename><surname>Rendle</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Data Mining (ICDM), 2010 IEEE 10th International Conference on</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2010">2010</date>
			<biblScope unit="page" from="995" to="1000" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Factorization machines with libfm</title>
		<author>
			<persName><forename type="first">S</forename><surname>Rendle</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Transactions on Intelligent Systems and Technology (TIST)</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page">57</biblScope>
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Predicting clicks: estimating the click-through rate for new ads</title>
		<author>
			<persName><forename type="first">M</forename><surname>Richardson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Dominowska</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Ragno</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 16th international conference on World Wide Web</title>
		<meeting>the 16th international conference on World Wide Web</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2007">2007</date>
			<biblScope unit="page" from="521" to="530" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<author>
			<persName><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Vedaldi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1312.6034</idno>
		<title level="m">Deep inside convolutional networks: Visualising image classification models and saliency maps</title>
		<imprint>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Two-stream convolutional networks for action recognition in videos</title>
		<author>
			<persName><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="568" to="576" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<title level="m" type="main">Very deep convolutional networks for large-scale image recognition</title>
		<author>
			<persName><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1409.1556</idno>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Dropout: A simple way to prevent neural networks from overfitting</title>
		<author>
			<persName><forename type="first">N</forename><surname>Srivastava</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Salakhutdinov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1929" to="1958" />
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Feature hashing for large scale multitask learning</title>
		<author>
			<persName><forename type="first">K</forename><surname>Weinberger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Dasgupta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Langford</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Smola</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Attenberg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 26th Annual International Conference on Machine Learning</title>
		<meeting>the 26th Annual International Conference on Machine Learning</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2009">2009</date>
			<biblScope unit="page" from="1113" to="1120" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Coupled group lasso for web-scale ctr prediction in display advertising</title>
		<author>
			<persName><forename type="first">L</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W.-J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G.-R</forename><surname>Xue</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Han</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 31st International Conference on Machine Learning (ICML-14)</title>
		<meeting>the 31st International Conference on Machine Learning (ICML-14)</meeting>
		<imprint>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="802" to="810" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Visualizing and understanding convolutional networks</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">D</forename><surname>Zeiler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Fergus</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision-ECCV 2014</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="818" to="833" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
		<title level="m" type="main">Deep learning over multi-field categorical data: A case study on user response prediction</title>
		<author>
			<persName><forename type="first">W</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1601.02376</idno>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
