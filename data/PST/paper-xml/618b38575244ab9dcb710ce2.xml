<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">COLD BREW: DISTILLING GRAPH NODE REPRESEN-TATIONS WITH INCOMPLETE OR MISSING NEIGHBOR-HOODS</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date type="published" when="2021-11-08">8 Nov 2021</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Wenqing</forename><surname>Zheng</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">The University of Texas at Austin</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Edward</forename><forename type="middle">W</forename><surname>Huang</surname></persName>
							<email>ewhuang@amazon.com</email>
							<affiliation key="aff0">
								<orgName type="institution">The University of Texas at Austin</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Nikhil</forename><surname>Rao</surname></persName>
							<email>nikhilsr@amazon.com</email>
							<affiliation key="aff0">
								<orgName type="institution">The University of Texas at Austin</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Sumeet</forename><surname>Katariya</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">The University of Texas at Austin</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Zhangyang</forename><surname>Wang</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">The University of Texas at Austin</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Karthik</forename><surname>Subbian</surname></persName>
							<email>ksubbian@amazon.com</email>
							<affiliation key="aff0">
								<orgName type="institution">The University of Texas at Austin</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">COLD BREW: DISTILLING GRAPH NODE REPRESEN-TATIONS WITH INCOMPLETE OR MISSING NEIGHBOR-HOODS</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2021-11-08">8 Nov 2021</date>
						</imprint>
					</monogr>
					<idno type="arXiv">arXiv:2111.04840v1[cs.LG]</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.2" ident="GROBID" when="2022-12-25T13:01+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Graph Neural Networks (GNNs) have achieved state of the art performance in node classification, regression, and recommendation tasks. GNNs work well when high-quality and rich connectivity structure is available. However, this requirement is not satisfied in many real world graphs where the node degrees have power-law distributions as many nodes have either fewer or noisy connections. The extreme case of this situation is a node may have no neighbors at all, called Strict Cold Start (SCS) scenario. This forces the prediction models to rely completely on the node's input features. We propose Cold Brew to address the SCS and noisy neighbor setting compared to pointwise and other graph-based models via a distillation approach. We introduce feature-contribution ratio (FCR), a metric to study the viability of using inductive GNNs to solve the SCS problem and to select the best architecture for SCS generalization. We experimentally show FCR disentangles the contributions of various components of graph datasets and demonstrate the superior performance of Cold Brew on several public benchmarks and proprietary e-commerce datasets. The source code for our approach is available at: https: //github.com/amazon-research/gnn-tail-generalization.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">INTRODUCTION</head><p>Graph Neural Networks (GNNs) achieve state-of-the-art results across a wide range of tasks such as graph classification, node-classification, -regression, link prediction, and recommendation <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b1">2,</ref><ref type="bibr" target="#b2">3,</ref><ref type="bibr" target="#b3">4,</ref><ref type="bibr" target="#b4">5,</ref><ref type="bibr" target="#b5">6]</ref>. GNNs rely on the principle of message passing to aggregate node features across multi-hop neighborhoods in order to learn aggregated representations. The success of modern GNN models relies on the presence of dense connections and high-quality neighborhoods. State-of-the-art GNNs <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b7">8,</ref><ref type="bibr" target="#b8">9,</ref><ref type="bibr" target="#b9">10,</ref><ref type="bibr" target="#b10">11,</ref><ref type="bibr" target="#b11">12]</ref> benefit from efficient message-passing frameworks developed to best leverage (deep) neighborhood information to learn node representations. Other methods like the spectral diffusion <ref type="bibr" target="#b12">[13,</ref><ref type="bibr" target="#b13">14,</ref><ref type="bibr" target="#b14">15,</ref><ref type="bibr" target="#b15">16]</ref> and label propagation <ref type="bibr" target="#b16">[17,</ref><ref type="bibr" target="#b17">18]</ref> also require a non-empty and high-quality neighborhood per node to perform message passing. Even the inductive GNNs (e.g., <ref type="bibr" target="#b8">[9]</ref>) learn a function of the node feature and the node neighborhood, requiring the neighborhood to be present during the inference time.</p><p>Most of the large-scale real-world graphs are power-law in nature, with a majority of the nodes having very few connections <ref type="bibr" target="#b18">[19,</ref><ref type="bibr" target="#b19">20,</ref><ref type="bibr" target="#b20">21,</ref><ref type="bibr" target="#b21">22]</ref>. In Figure <ref type="figure">1</ref> we conceptually show the long-tail distribution of such a graph and also empirically show this characteristics on several public datasets. In many information retrieval and recommender systems applications, there is another ubiquitous challenge: the Strict Cold Start (SCS) <ref type="bibr" target="#b22">[23,</ref><ref type="bibr" target="#b19">20]</ref>, wherein some "nodes" will have no edges in the graph. For instance, in most of the public and proprietary datasets we used in the experimentation we have anywhere between 3% to 6% isolated nodes with no neighbors. In these cases, existing GNNs fail to perform due to sparsity or absence of neighborhood.</p><p>In this paper, we develop GNN models that can achieve truly inductive capabilities: one can learn effective node embeddings for "orphaned" nodes in a graph. This capability is Preprint important to fully realize the potential of large-scale GNN models on modern, industrialsized datasets with very long tails and nodes with no neighbors in the graph. To this end, Normalized Number of Nodes Figure <ref type="figure">1</ref>: Top: Graphs may have a power-law ("long-tail") distribution, with a large fraction of nodes (yellow) having few to no neighbors. Bottom: The long-tail distribution is often the case in real-world datasets, and a GNN depending on the node neighborhood will not generalize to the tail/cold-start nodes.</p><p>we adopt the teacher-student knowledge distillation procedure <ref type="bibr" target="#b23">[24,</ref><ref type="bibr" target="#b24">25]</ref> and propose Cold Brew to distill the knowledge of a GNN teacher into a multilayer perceptron (MLP) student. The Cold Brew framework boils down to addressing two key questions: (1) how we efficiently distill the teacher's knowledge for the sake of tail and coldstart generalization, and (2) how a student can make use of this knowledge. We answer these two question by learning a latent node-wise embedding using knowledge distillation, which both avoid "over-smoothness" <ref type="bibr" target="#b25">[26,</ref><ref type="bibr" target="#b26">27,</ref><ref type="bibr" target="#b27">28]</ref> and could discover the the neighborhoods upon their absence. We discuss everything else in detail in the following sections. Note that in contrast to traditional knowledge distillation <ref type="bibr" target="#b28">[29]</ref>, our aim is not to train a simpler student model to perform as well as the more complex teacher. Instead, we aim to train a student model that is better than the teacher, and generalizes to samples in the same graph where the teacher will be ineffective.</p><p>To help select the cold-start-friendly model architectures, we develop a metric called Feature-Contribution Ratio (FCR) that disentangles the graph data into node features and the neighborhood structure. We then build submodules for each part and search for the best architecture. Besides selecting the optimal model architecture, we leverage FCR to provide a measure of importance of the node feature compared to the neighborhood structure, quantifying the difficulty of learning truly inductive GNNs. We summarize our key contributions as follows:</p><p>• To generalize better to tail and SCS nodes, we design the Cold Brew framework to distill a GNN teacher into an MLP student. We improve over GNN models by adding the node-wise Structural Embedding (SE) to the Cold Brew's teacher GNN to strengthen the expressiveness of the teacher GNN model. We designed a novel mechanism for the MLP student to discover the "latent/virtual neighborhoods" when they are missing, and perform message passing as its teacher model does.</p><p>• We propose Feature-Contribution Ratio (FCR): a new metric for graph datasets that quantifies the contribution of node features w.r.t. the adjacency structure in the dataset for a specific downstream task. FCR indicates the difficulty in generalizing to tail and cold-start nodes in a given dataset. We also leverage FCR as a principled "screen process" to select the best model architecture for both the GNN teacher and the MLP student in Cold Brew.</p><p>• Through extensive experiments on several public datasets as well as a proprietary ecommerce graph dataset, we validate the effectiveness of Cold Brew in cold-start generalization. We further uncover the contribution ratio of node features in node prediction tasks with FCR.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.1">PROBLEM SETUP</head><p>GNNs effectively learn the representations of two components in graph data: they process node features by distributed node-wise transformations and process adjacency structure by localized neighborhood aggregations. For the first component, GNNs apply shared feature transformations to all nodes regardless of the neighborhoods, while for the second component, GNNs use permutationinvariant aggregators to collect neighborhood information.</p><p>We take the node classification problem in this paper for ease of exposition, and the proposed method can be trivially adapted to semi-supervised/unsupervised settings. We denote the graph data of interest by G with node set V, |V| = N . Each node possesses a d in −dimensional feature and a d out −dimensional label (either d out classes or a continuous vector in the case of regression). Let X 0 ∈ R N ×din and Y ∈ R N ×dout be the matrices of node features and labels, respectively. Let N i be the neighborhood of node i ∈ [N ]. As is common with modern, large-scale graphs, |N i | is small for several nodes in the graph, which we refer to as tail nodes. For a number of nodes, |N i | = 0, and we refer to these nodes as cold-start nodes.</p><p>A traditional GNN learns representations for the i th node at the l th layer as a function of the node representation itself and its neighborhoods' representations, at the (l − 1) th layer:</p><formula xml:id="formula_0">x l i := f {x l−1 i }, {x l−1 j } j∈Ni<label>(1)</label></formula><p>where f (•) is a general function that applies node-wise transformation on node x l−<ref type="foot" target="#foot_0">1</ref> i and aggregates information of its neighborhood {x l−1 j } j∈Ni to obtain the final node representation. Given i's input features x 0 i and its neighborhood N i , one can use (1) to obtain its representation and predict y i , making these models inductive.</p><p>We are interested in improving the performance of these GNNs on a set of tail and cold-start nodes, where N i for node i is either unreliable 1 or absent. In these cases, applying (1) will yield a suboptimal node representation, since {x l−1 j } j∈Ni will be unreliable or empty at inference time.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">RELATED WORK</head><p>GNNs learn by aggregating neighborhood information to learn node representations <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b7">8,</ref><ref type="bibr" target="#b8">9,</ref><ref type="bibr" target="#b9">10,</ref><ref type="bibr" target="#b10">11,</ref><ref type="bibr" target="#b11">12]</ref>. Inductive variants of GNNs such as GraphSAGE <ref type="bibr" target="#b8">[9]</ref> require initial node features as well as the neighborhood information of each node to learn the representation. Most works on improving GNNs have focused on learning better aggregation functions, and methods that can work when the neighborhood is absent or noisy have not been sufficiently exploited, except two very recent or concurrent works <ref type="bibr" target="#b29">[30,</ref><ref type="bibr" target="#b30">31]</ref>.</p><p>In the context of cold start, <ref type="bibr" target="#b18">[19]</ref> and <ref type="bibr" target="#b19">[20]</ref> employ a transfer learning approach. <ref type="bibr" target="#b23">[24]</ref> proposes a knowledge distillation for GNN, while <ref type="bibr" target="#b24">[25]</ref> proposes a self-distillation approach. In all the above cases, the models need full knowledge of the neighbors of the cold-start nodes in question and do not address the case of noisy or missing neighborhoods. Another possible solution is to directly train an MLP that only take node features. <ref type="bibr" target="#b29">[30]</ref> proposes to learn graph embeddings with only node-wise MLP, while using contrastive loss to regularize the graph structure. However, in order to train it with contrastive loss, the training process still relies on neighbor information, and we show through experiments that such approach does not generalize well to tail and cold start nodes.</p><p>Some previous works have studied the relation between node feature similarity and edge connections and studied their influence on the selection of appropriate graph models. <ref type="bibr" target="#b31">[32]</ref> proposed the homophily metric that categorizes the graphs into assortative and disassortative classes. <ref type="bibr" target="#b15">[16]</ref> dissected the feature propagation steps of linear GCNs from a perspective of continuous graph diffusion and analyzed why linear GCNs fail to benefit from more propagation steps. <ref type="bibr" target="#b32">[33]</ref> further studied the influence of homophily on model selection and proposed a non-local GNN. Compared with the homophily metric, our proposed FCR quantifies the contribution ratio of node features w.r.t. the adjacency structure. With FCR, one can not only assess the loss of performance for orphaned nodes (through the difference of MLP and GNN) but also obtain the most suitable configuration of the GNN model through a parameter-searching procedure when computing FCR.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">STRICT COLD START GENERALIZATION</head><p>We now address the problem of generalization to the tail and cold-start nodes, where the neighborhood information is missing/noisy (Section 1). A naive baseline is to train an MLP to map node features to labels. However, such method disregards all graph information, and we show via our Feature- (a) The teacher-student knowledge distillation of the Cold Brew framework under the cold start setting.  Contribution Ratio and other experimental results that for most assortative graph datasets, the node-wise MLP approach is suboptimal.</p><formula xml:id="formula_1">x l+1 i = σ(Σ j∈𝒩 i a ij x l j W l ) GCN layers L 3. Self-</formula><p>The key idea of our framework is the following: the GNN maps node features into a d-dimensional embedding space, and since the number of nodes N is usually much bigger than the embedding dimensionality d, we end up with an overcomplete set for this space using the embeddings as the basis. This implies the possibility that any node representation can be cast as a linear combination of K N existing node representations. Our aim will be to train a student model that can accurately discover the combination of the best K existing node embeddings of a target isolated node. We call this procedure latent/virtual neighborhood discovery, which is equivalent to using MLPs to "mimic" the node representations learned by the teacher GNN.</p><p>We adopt the knowledge distillation procedure <ref type="bibr" target="#b23">[24,</ref><ref type="bibr" target="#b24">25]</ref> to improve the quality of the learned embeddings for tail and cold-start nodes. We use a teacher GNN model to embed the nodes onto a low-dimensional manifold by utilizing the graph structure. Then, the job of the student is to learn a mapping from the node features to this manifold without knowledge of the graph that the teacher has. We further aim to let the student model generalize to SCS cases where the teacher model fails, beyond just mimicking the teacher as standard knowledge distillation does.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">THE TEACHER MODEL OF COLD BREW: STRUCTURAL EMBEDDING GNN</head><p>Consider a graph G. For a Graph Convolutional Network with L layers, the l-th layer transformation can be written as<ref type="foot" target="#foot_1">2</ref> : X (l+1) = σ( ÃX (l) W (l) ), where Ã is the normalized adjacency matrix, Ã = D −1/2 AD −1/2 , D is the diagonal degree matrix and A is the adjacency matrix. X (l) ∈ R N ×d1 is the node representations in the l-th layer, W (l) ∈ R d1×d2 is the feature transformation matrix, where the values of d 1 /d 2 depend on layer l:</p><formula xml:id="formula_2">(d 1 , d 2 ) = (d in , d hidden ) for l = 0, (d hidden , d hidden ) for 1 ≤ l ≤ L − 2,</formula><p>and (d hidden , n classes ) for l = L − 1. σ(•) = is the nonlinear functions applied to each layer, (e.g. ReLU). N orm(•) refers to an optional batch or layer normalization.</p><p>GNNs typically suffer from oversmoothing <ref type="bibr" target="#b25">[26,</ref><ref type="bibr" target="#b26">27,</ref><ref type="bibr" target="#b27">28]</ref>, making all the node representations similar to each other. This is harmful not only for the GNN itself, but also to the student model that looks to mimic the GNN. Inspired by the positional encoding in Transformers <ref type="bibr" target="#b33">[34]</ref>, we train the teacher GNN to additionally learn a set of node embeddings which we term the Structural Embedding (SE).</p><p>SE learns to incorporate additional information (such as node labels in the case of semi supervised learning) through gradient backpropagation. The existence of SE avoids the oversmoothing issue in GNNs: the transformations applied to different nodes are no longer the same for each nodes, since the SE of each node is different, which is part of the feature transformation function. Note that this could be of independent interest to practitioners who train and study GNNs. Specifically, for each layer l, 0 ≤ l ≤ L − 1, the Structural Embedding is a learnable matrix E (l) , and the SE-GNN layer forward pass can be written as:</p><formula xml:id="formula_3">X (l+1) = σ Ã X (l) W (l) + E (l) , X (l) ∈ R N ×d1 , W (l) ∈ R d1×d2 , E (l) ∈ R N ×d2 (2)</formula><p>Remark 1: Note that SE is not the same as the bias term in traditional feature transformation l) , where for the bias b ∈ R N ×d2 the rows are copied/shared across all nodes. In contrast, we have a different structural embedding for every node.</p><formula xml:id="formula_4">X (l+1) = σ Ã X (l) W (l) + b (</formula><p>Remark 2: SE is also unlike traditional label propagation (LP) <ref type="bibr" target="#b34">[35,</ref><ref type="bibr" target="#b35">36,</ref><ref type="bibr" target="#b36">37]</ref>. LP encodes label information through iterating</p><formula xml:id="formula_5">E (t+1) = (1 − α)G + α ÃE (t)</formula><p>, where G is a one-hot encoding of ground truth for training node classes and zeros for test nodes, and 0 &lt; α &lt; 1 is the portion of mixture at each iteration. SE-GNN enables node i to learn to encode the self and neighbors' label information<ref type="foot" target="#foot_2">3</ref> into its own node embedding through Ã. We use the Graph Convolutional Networks <ref type="bibr" target="#b6">[7]</ref>, combined with other building blocks proposed in recent literature including: (1) initial/dense/jumping connections; and (2) batch/pair/node/group normalization, as the backbone of Cold Brew's teacher GNN. More details are described in Appendix A. We also apply a regularization term to the loss function, yielding the following loss function:</p><formula xml:id="formula_6">loss = CE(X (L) train , Y train ) + η E 2 2 (3)</formula><p>where</p><formula xml:id="formula_7">X (L)</formula><p>train is the model's embedding at the L-th layer, which is exactly the model's output, CE(X (L) train , Y train ) is the Cross Entropy between the model output X (L) train and the ground truth Y on the training set, and η is the coefficient (grid-searched for different datasets in practice). The Cross-Entropy loss can be replaced by any other appropriate loss depending on the task.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">THE STUDENT MLP MODEL OF COLD BREW</head><p>We design the student to be composed of two MLP modules. Given a target node, the first MLP module imitates the node embeddings generated by the GNN teacher. Next, given any node, we find a set of virtual neighbors of that node from the graph. Finally, the second MLP attends to both the target node and the virtual neighborhood and transforms them into the embeddings of interest.</p><p>Suppose we would like to obtain the embedding of an potentially isolated target node i, given only its feature x i . From the teacher GNN, at each layer l, we have access to two sets of node embeddings: X (l) W (l) and E (l) . Denote Ē as the desired embeddings that the teacher GNN pass over to the student MLP. We offer two options for Ē: it can be the final output of the teacher GNN, in this case, Ē ∈ R N ×dout := X (L) , or optionally, it can be the concatenation of all intermediate results of the teacher GNN, similar to <ref type="bibr" target="#b37">[38]</ref>:</p><formula xml:id="formula_8">Ē ∈ R N ×(d hidden * (L−1)+dout) := X (L) L−1 l=0 (E (l) + X (l) W (l)</formula><p>), where is the concatenation of matrices at the feature dimension (second dimension). Ē acts as the target for the first MLP module and the input to the second MLP module.</p><p>The first MLP learns a mapping from the input node features X (0) to Ē, that is ξ 1 : x (0) i → e i for node i, e i = Ē[i, :]. Then, we discover the virtual neighborhood by applying an attention-based aggregation of the existing embeddings in the graph, before linearly combining them:</p><formula xml:id="formula_9">ẽi = sof tmax(Θ K (e i Ē )) Ē<label>(4)</label></formula><p>where Θ K (•) is the top-K hard thresholding operator: for z ∈ R 1×N : [Θ K (z)] j = z j if z j is among the top-K largest elements of z, and Θ K (z) j = −∞ otherwise. Finally, the second MLP learns a mapping ξ 2 : [x i , e i ] → y i , where y i = Y[i, :] is the ground truth for node i.</p><p>Equation (4) first selects K nodes from the N nodes that the teacher GNN was trained on via the hard thresholding operator. ẽj is then a linear combination of K node<ref type="foot" target="#foot_3">4</ref> embeddings. Thus, every sample whether or not seen previously while training the GNN can be represented as a linear combination of these representations. The MLP ξ 2 (•) maps this representation to the final target of interest. Thus, we decompose every node embedding as a linear combination of an (overcomplete) basis.</p><p>The training of ξ 1 (•) is by minimizing the mean squared error over the non-isolated node in the graph (mimicing the teacher's embeddings), and the training of ξ 2 (•) is by minimizing the cross entropy (for node classification task) or mean squared error (for node regression task) on the training split of the tail and isolated part of the graph. An illustration of SE-MLP's inference procedure for the isolated nodes is shown in Fig. <ref type="figure" target="#fig_2">2</ref>. When the number of nodes is large, the ranking procedure involved in Θ K (•) can be pre-computed after training the first part and before training the second part.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">MODEL INTERPRETATION FROM A LABEL SMOOTHING PERSPECTIVE</head><p>We cite Theorem 1 in <ref type="bibr" target="#b35">[36]</ref>: Suppose that the latent ground-truth mapping from node features to node labels is differentiable and L-Lipschitz. If the edge weights a ij approximately smooth x i over its immediate neighbors with error i , i.e., x i = 1 dii Σ j∈N a ij x j + i , then the a ij also approximately smooth y i to bound within error</p><formula xml:id="formula_10">|y i − 1 dii Σ j∈Ni a ij y j | ≤ L|| || 2 + o(max j∈Ni (||x j − x i || 2 ))</formula><p>, where o(•) denotes a higher order infinitesimal.</p><p>This theorem indicates that the errors of the label predictions are determined by the difference of the features after neighborhood aggregation: if i is large, then the error in the label prediction is also large, and vice versa. However, with structural embedding, each node i also learns an independent embedding Ē[:, i] during the aggregation, which changes 1  dii Σ j∈N a ij x j + i into 1 dii Σ j∈N a ij x j + Ē[:, i] + i . Deduced from this theorem, the structural embedding Ē is important for the teacher model: it allows higher flexibility and expressiveness in learning the residual difference between nodes, and hence the error i can be are lowered if Ē is properly learned.</p><p>From this theorem, one can also see the necessity of introducing neighborhood aggregations like the Cold Brew's student model. If one directly applies MLP models without neighborhood aggregation, the i turns out to be non-negligible, leading to higher losses in the label predictions. However, Cold Brew introduced the neighborhood aggregation mechanism, so that the second part of the student MLP takes over the aggregation of neighborhood generated by the first MLP. Therefore, Cold Brew eliminates the above residual error even in the absence of the actual neighborhood.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">MODEL SELECTION AND GRAPH COMPONENT DISENTANGLEMENT WITH FEATURE-CONTRIBUTION RATIO</head><p>We now discuss Feature Contribution Ratio (FCR): a metric to quantify the difficulty of learning representations under the truly inductive cold start case, and a hyperparameter optimization approach to select the best suitable model architecture that helps tail and cold start generalization.</p><p>As conceptually illustrated in Figure <ref type="figure" target="#fig_2">2</ref>, there are four atomic components contributing to the learned embedding of node i in the graph: 1. the label of i (self-label); 2. the label of neighbors of i (neighborlabels); 3. the features of i (self-feature); 4. the features of neighbors of i (neighbor-features). To quantize the SCS generalization difficulty, we divide these four components into two submodules to disentangle the contributions of the node features with respect to the adjacency structure of the graph dataset, and quantize it based on the assumption that the SCS generalization difficulty is propotional to the contribution ratio of the node features.</p><p>We posit that a submodule that learns accurate node representations must include the node's (self) label, so that training can be performed via backpropagation. What remains is to use the label Preprint with other atomic components to construct two specialized models that each make use of only the node features or the adjacency structure. For the first submodule, we build an MLP that maps the self-features to self-labels, ignoring any neighborhood information present in the dataset. For the second submodule, we adopt a Label Propagation (LP) method <ref type="bibr" target="#b16">[17]</ref> <ref type="foot" target="#foot_4">5</ref> to learn representations from self-and neighbor-labels. This model ignores any node feature information.</p><p>With the above two submodules, we introduce the Feature-Contribution Ratio (FCR) that characterizes the relative importance of the node features and the graph structure. Specifically, for graph dataset G, we define the contribution of a submodule to be the residual performance of the submodule compared to a full-fledged GNN (e.g., Equation ( <ref type="formula" target="#formula_0">1</ref>)) using both the node feature as well as the adjacency structure. Denote z M LP , z LP , and z GN N as the performance of the MLP submodule, LP submodule, and the full GNN on the test set, respectively. If z M LP z GN N , then F CR(G) is small and the graph structure is important, and noisy or missing neighborhood information will hurt model performance. Based on this intuition, we build SCR as:</p><formula xml:id="formula_11">δMLP =zGNN − zMLP , δLP = zGNN − zLP (5a) F CR(G) = δ LP δ M LP +δ LP × 100% zMLP ≤ zGNN 1 + |δ M LP | |δ M LP |+δ LP × 100% zMLP &gt; zGNN (5b)</formula><p>Interpreting FCR values. For a particular graph G, if 0% ≤ F CR(G) &lt; 50%, it means z GN N &gt; z LP &gt; z M LP , and the neighborhood information in G is mainly responsible for the GNN achieving good performance. If 50% ≤ F CR(G) &lt; 100%, then z GN N &gt; z M LP &gt; z LP , the node features contribute more to the GNN's performance. If F CR(G) ≥ 100%, then z M LP &gt; z GN N &gt; z LP , and the node aggregation in GNNs can actually lead to reduced performance compared to pointwise models. This case usually happens for some disassortative graphs (the majority of neighborhoods hold different labels than the center node), e.g. as observed by <ref type="bibr" target="#b32">[33]</ref> .</p><p>Integrate FCR as a tool to design teacher and student models. For some graph datasets and models, the SCS generalization can be challenging without neighborhood information (i.e, z GN N &gt; z LP &gt; z M LP ). We hence consider FCR as a principled "screening process" to select model architectures for both teacher and student, that own the best inductive bias for SCS generalization.</p><p>To achieve this, during the computation of FCR, we perform exhaustive grid search of the architectures (residual connection types, normalization, hidden layers, etc.) for the MLP, LP, and GNN modules, and pick the best-performing variant. Detailed definition of the search space can be found in Appendix A. We treat this grid search procedure as a special case of architecture selection and hyperparameter optimization for Cold Brew. We observe that FCR is able to identify the GNN and MLP architectures that are particularly friendly for SCS generalization, improving our method design.</p><p>In experiments, we observe that different model configurations are favored by different datasets, and we use the found optimal teacher GNN and student MLP architectures to perform Cold Brew. More detailed discussions will be presented in section 5.3</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">EXPERIMENTS AND DISCUSSION</head><p>In this section, we first evaluate the FCR for several commonly used graph datasets to ascertain how well GNNs trained on them can generalize to tail and cold-start nodes. We also compare it to the the graph homophiliy metric β proposed in <ref type="bibr" target="#b31">[32]</ref>. Next, we apply Cold Brew to these datasets and compare its generalization ability to baseline graph-based as well as purely pointwise models on these datasets. We also show results on proprietary industry datasets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">DATASETS AND SPLITS</head><p>We first detail the datasets used in this paper. We perform studies on several open-source datasets, together with four proprietary datasets. The proprietary E-commerce datasets, E-comm 1/2/3/4, refers to graphs subsampled from anonymized logs of an e-commerce store. They are sampled so as to not reflect the actual raw traffic distributions, and results are provided with respect to a baseline model for these datasets. The different datasets refer to different product subsets, and the labels indicate Preprint product categories that we wish to determine. Node features are text embeddings obtained from a fine-tuned BERT model. We show FCR values for the public datasets and exhaustively evaluate the proposed Cold Brew as well as several baselines for five public datasets and the proprietary datasets. The statistics of the datasets are summarized in Table <ref type="table">1</ref>.</p><p>We create specific training and test splits of the datasets in order to specifically study the generalization ability of Cold Brew to tail and cold-start nodes. In the following tables, the head data corresponds to the top 10% highest degree nodes in the graph, and the subgraph that they induce. The tail data corresponds to the 10% nodes in the graph with lowest (non-zero) degree, and the subgraph that they induce. All the zero degree nodes are in the isolation data. For datasets that don't have isolated nodes, we manually remove edges from 10% lowest degree nodes. The Overall data refers to the training/test splits without distinguishing head/tail/isolation. Table <ref type="table">1</ref>: The statistics of datasets selected for evaluations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">FCR EVALUATION</head><p>In Table <ref type="table">2</ref>, the top half presented the FCR results together with the homophily metric β in <ref type="bibr" target="#b31">[32]</ref> (Equation <ref type="formula">6</ref>). The below half uncovered the different prediction quality for the head and the tail nodes. As can be seen from the table, as an indicator of the tail generalization difficulty, FCR differs among datasets, and is negatively correlated with the homophily metric (with pearson correlation coefficient -0.76). The evaluations on more datasets (including the datasets where FCR &gt; 100%) are presented in Appendix B. Table <ref type="table">2</ref>: Above half: FCR and its components, β metric are added as a reference; below half: the performance difference of GNN on the head/tail and head/isolation splits. Here the "head/tail/isolation" means the 10% most connected, fewest connected, and isolated nodes in the graph.</p><formula xml:id="formula_12">β(G) = 1</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3">EXPERIMENTAL RESULTS ON TAIL GENERALIZATION WITH COLD-BREW</head><p>In Table <ref type="table" target="#tab_4">3</ref>, we present the performances of the Cold Brew together with baselines on the tail and the isolation splits, across several different datasets. All models in the table are trained on the overall training subset, and are evaluated on the hold-out test subsets of tail and isolation splits (discussed in section 5.1). For the isolation split, the GNNs are evaluated with only self-loop (all other edges are non-existent/removed). The GCN refers to the the best configuration found through FCR-guided grid search (the search is performed over a range of hyperparameters, check Appendix A for details), without Structural Embedding. Correspondingly, GCN + SE refers to the best FCRguided configuration with Structural Embedding, which is the default teacher model of Cold Brew. GraphSAGE refers to <ref type="bibr" target="#b8">[9]</ref>, Simple MLP refers to a simple node-wise MLP that has two hidden layers with 128 hidden dimensions, and GraphMLP refers to <ref type="bibr" target="#b29">[30]</ref>. The results for E-commerce datasets are presented as relative improvements to the baseline (each value is the difference w.r.   There are other interesting observations. First, if the FCR is high (such as Pubmed), then for this dataset the MLP-type models tend to outperform GNN-type models in all splits. Regardless of FCR, for almost all datasets, the MLP-type models outperform the GNN-type models on the isolation split, and a few on the tail split, while the GNN-type models are superior in other splits. Second, the proposed structural embeddings imply a tantalizing potential to alleviate the over-smoothness <ref type="bibr" target="#b25">[26,</ref><ref type="bibr" target="#b26">27,</ref><ref type="bibr" target="#b27">28]</ref> and bottleneck <ref type="bibr" target="#b38">[39]</ref> issues observed in deep GCN models. As shown in table <ref type="table" target="#tab_5">Table 4</ref>, Cold Brew's GCN (GCN + SE) significantly outperformed the traditional GCN on 64 layers: the former has 34% test accuracy higher on Cora, 23% higher on Citeseer and similar on others.</p><p>Indeed, the improvement over isolation and tail splits (especially the isolation split) comes with a cost: we observed a performance drop for the student MLP model on the head and several other datasets' tail splits, compared with the naive GCN model. The full performance on other splits are listed in the appendix as a reference. However, the cold-brew specifically targets at the challenging strict cold start issues, as a new compelling alternative for in these cases. Meanwhile in the non-cold-start cases, the traditional GCN models can still be used to obtain good performance. Note that, even on the head splits, the proposed GNN teacher model of Cold Brew still outperformed traditional GNN models.</p><p>We hence consider as promising future work to adaptively switch between using Cold Brew teacher and student models, based on the current node connectivity degree and tailedness.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">CONCLUSION</head><p>In this paper, we studied the problem of generalizing GNNs to the tail and strict cold start nodes, whose neighborhood information is noisy or missing. We proposed a teacher-student knowledge distillation procedure to better generalize to the isolated nodes. We added an independent set of structural embeddings in GNN layers to to alleviate node over-smoothness, and also proposed a virtual neighbor discovery step for the student model to attend to estimated neighborhood nodes. We additionally established FCR, a metric for quantifying the difficulty of truly inductive representation learning, to optimize our method design. Experiments demonstrated the consistently superior performance of our proposed framework, on several common benchmarks as well as proprietary datasets. </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Feature of Node i 4 . 1 .</head><label>41</label><figDesc>Neighbor-Features of Node i Self-Label of Node i 2. Neighbor-Labels of Node i (b) Four GNN Atomic Components in deciding GNN's output, which are used for FCR analysis.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: (a): The proposed Cold Brew framework under the cold start setting: when the adjacency structure is missing (no explicit neighborhood), Cold Brew's student model learns to discover the latent neighborhood, and infer the target embedding from the node feature and the estimated neighbors. The "SE" in blue color is the structural embedding, learned by Cold Brew's teacher GNN. (b): Four atomic components of node i in deciding its embeddings learned by the GNN. Our proposed FCR metric disentangles them into two models: the MLP that only considers Part 1 and Part 3, and label propagation that only considers Part 1 and Part 2.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>5. 4 VISUALIZATIONFigure 3</head><label>43</label><figDesc>Figure3visualizes the last-layer embeddings of different models after t-SNE dimensionality reduction. In the figure, colors denotes node labels and all nodes are marked as dots, with isolation subset</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: The top two subfigures: the last layer embeddings of GCN and Simple MLP, the bottom two subfigures: the last layer embeddings of GraphMLP and Cold Brew's student MLP. All embeddings are projected to 2D with t-SNE. Cold Brew's MLP has the fewest isolated nodes that are misplaced into wrong clusters.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head></head><label></label><figDesc>t. the value of the</figDesc><table><row><cell>Preprint</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Splits</cell><cell cols="2">Metrics/Models</cell><cell></cell><cell cols="3">Open-Source Datasets</cell><cell></cell><cell></cell><cell cols="2">Proprietary Datasets</cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell cols="9">Cora Citeseer Pubmed Arxiv Chameleon E-comm1 E-comm2 E-comm3 E-comm4</cell></row><row><cell></cell><cell>GNNs</cell><cell>GCN 2 layers GraphSAGE</cell><cell>58.02 66.02</cell><cell>47.09 51.46</cell><cell>71.50 69.87</cell><cell>44.51 47.32</cell><cell>57.28 59.83</cell><cell>− +3.89</cell><cell>− +4.81</cell><cell>− +5.24</cell><cell>− +0.52</cell></row><row><cell>Isolation</cell><cell>MLPs</cell><cell>Simple MLP GraphMLP</cell><cell>68.40 65.00</cell><cell>53.26 52.82</cell><cell>65.84 71.22</cell><cell>51.03 51.10</cell><cell>60.76 63.54</cell><cell>+5.89 +6.27</cell><cell>+9.85 +9.46</cell><cell>+5.83 +5.99</cell><cell>+6.42 +7.37</cell></row><row><cell></cell><cell>Cold Brew</cell><cell cols="2">GCN + SE 2 layers 58.37 Student MLP 69.62</cell><cell>47.78 53.17</cell><cell>73.85 72.33</cell><cell>45.20 52.36</cell><cell>60.13 62.28</cell><cell>+0.27 +7.56</cell><cell>+0.76 +11.09</cell><cell>-0.50 +5.64</cell><cell>+1.22 +9.05</cell></row><row><cell></cell><cell>GNNs</cell><cell>GCN 2 layers GraphSAGE</cell><cell>84.54 82.82</cell><cell>56.51 52.77</cell><cell>74.95 73.07</cell><cell>67.74 63.23</cell><cell>58.33 61.26</cell><cell>− -3.82</cell><cell>− -3.07</cell><cell>− -2.87</cell><cell>− -6.42</cell></row><row><cell>Tail</cell><cell>MLPs</cell><cell>Simple MLP GraphMLP</cell><cell>70.76 70.09</cell><cell>54.85 55.56</cell><cell>67.21 71.45</cell><cell>52.14 52.40</cell><cell>50.12 52.84</cell><cell>-0.37 -0.33</cell><cell>+1.74 +1.64</cell><cell>-0.13 +1.27</cell><cell>-0.45 +0.80</cell></row><row><cell></cell><cell>Cold Brew</cell><cell cols="2">GCN + SE 2 layers 84.66 Student MLP 71.80</cell><cell>56.32 54.88</cell><cell>75.33 72.54</cell><cell>68.11 53.24</cell><cell>60.80 51.36</cell><cell>+0.85 +0.32</cell><cell>+0.44 +3.09</cell><cell>-0.60 -0.18</cell><cell>+1.10 +2.09</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 3 :</head><label>3</label><figDesc>The performance comparisons on the isolation and tail splits of different datasets. The full comparisons on head/tail/isolation/overall data are in the Appendix B. GCN+SE 2 layers is Cold Brew's teacher model. The Cold Brew outperforms GNN and other MLP baselines, and achieves the best performance on the isolation splits as well as some tail splits.</figDesc><table><row><cell>Splits</cell><cell>Metrics/Models</cell><cell cols="3">Open-Source Datasets</cell><cell></cell><cell></cell><cell cols="2">Proprietary Datasets</cell><cell></cell></row><row><cell></cell><cell cols="9">Cora Citeseer Pubmed Arxiv Chameleon E-comm1 E-comm2 E-comm3 E-comm4</cell></row><row><cell>Overall</cell><cell>GCN 64 layers GCN + SE 64 layers 74.23 40.04</cell><cell>23.66 46.80</cell><cell>75.65 78.12</cell><cell>65.53 69.28</cell><cell>58.14 59.88</cell><cell>-5.49 -1.71</cell><cell>-6.59 -2.92</cell><cell>-6.13 -3.29</cell><cell>-3.57 -0.06</cell></row><row><cell>Head</cell><cell>GCN 64 layers GCN + SE 64 layers 87.38 46.46</cell><cell>49.84 71.18</cell><cell>85.89 86.81</cell><cell>67.53 71.35</cell><cell>67.16 69.63</cell><cell>-5.60 -1.78</cell><cell>-6.24 -2.17</cell><cell>-6.05 -2.79</cell><cell>-3.16 -0.35</cell></row><row><cell>Tail</cell><cell>GCN 64 layers GCN + SE 64 layers 79.56 45.14</cell><cell>24.42 36.52</cell><cell>71.89 74.88</cell><cell>63.91 65.19</cell><cell>56.48 61.73</cell><cell>-3.85 -2.42</cell><cell>-3.62 -2.52</cell><cell>-3.84 -3.68</cell><cell>-1.14 -1.23</cell></row><row><cell>Isolation</cell><cell>GCN 64 layers GCN + SE 64 layers 40.33 39.97</cell><cell>22.12 24.53</cell><cell>68.57 71.22</cell><cell>40.03 41.18</cell><cell>57.60 60.13</cell><cell>-4.66 -3.08</cell><cell>-4.63 -3.02</cell><cell>-4.93 -4.00</cell><cell>-1.89 -2.32</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 4 :</head><label>4</label><figDesc>The comparisons of Cold Brew's GCN and the traditional GCN for deep layers. When the number of layers is large, Cold Brew's GCN retained good performance while traditional GCN without SE suffers from the "over-smoothess" and degrades. Even for shallow layer, Cold Brew's GCN is better than traditional GCN.GCN 2 layers on same dataset of the same split), and we do not disclose the absolute numbers due to proprietary reasons.As shown in Table3, Cold Brew's student MLP improves accuracy on isolated nodes by up to +11% on the E-commerce datasets and +2.4% on open source datasets. Cold Brew's student model handles isolated nodes much better, and it teacher GNN also achieves better performance in the tail split, compared to all other models. Especially, when compared with GraphMLP, Cold Brew's student MLP shows consistently better performance in most datasets. This can be explained from their different mechanisms: GraphMLP encodes graph knowledge implicitly in the learned weights, while Cold Brew explicitly attends to neighborhoods, even when it is absent. More detailed comparisons can be found in Appendix B.</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 5 :</head><label>5</label><figDesc>Best GCN configurations    </figDesc><table><row><cell>Preprint</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Dataset</cell><cell></cell><cell cols="2">Best GCN</cell><cell></cell></row><row><cell></cell><cell>Num layers</cell><cell>whether has SE</cell><cell>residual type</cell><cell>normalization type</cell></row><row><cell>Cora</cell><cell>2 layer</cell><cell>has structural embedding</cell><cell>no residual</cell><cell>PairNorm</cell></row><row><cell>Citeseer</cell><cell>2 layer</cell><cell>has structural embedding</cell><cell>no residual</cell><cell>PairNorm</cell></row><row><cell>Pubmed</cell><cell>16 layer</cell><cell cols="2">has structural embedding initial connection</cell><cell>GroupNorm</cell></row><row><cell>Arxiv</cell><cell>4 layer</cell><cell cols="2">has structural embedding initial connection</cell><cell>GroupNorm</cell></row><row><cell>Chameleon</cell><cell>2 layer</cell><cell cols="2">has structural embedding initial connection</cell><cell>BatchNorm</cell></row><row><cell>Dataset</cell><cell></cell><cell>Best MLP</cell><cell></cell><cell></cell></row><row><cell></cell><cell cols="3">hidden layers residual connection hidden dimensions</cell><cell>optimizer</cell></row><row><cell>Cora</cell><cell>2 layer</cell><cell>no residual</cell><cell>128</cell><cell>Adam(lr=0.001)</cell></row><row><cell>Citeseer</cell><cell>4 layer</cell><cell>no residual</cell><cell>128</cell><cell>Adam(lr=0.001)</cell></row><row><cell>Pubmed</cell><cell>2 layer</cell><cell>no residual</cell><cell>256</cell><cell>Adam(lr=0.02)</cell></row><row><cell>Arxiv</cell><cell>2 layer</cell><cell>no residual</cell><cell>256</cell><cell>Adam(lr=0.001)</cell></row><row><cell>Chameleon</cell><cell>2 layer</cell><cell>no residual</cell><cell>256</cell><cell>Adam(lr=0.001)</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 6 :</head><label>6</label><figDesc>Best MLP configurations</figDesc><table><row><cell>Dataset</cell><cell></cell><cell>Best LP</cell><cell></cell></row><row><cell></cell><cell cols="3">number of propagation propagation matrix type mixing coefficient</cell></row><row><cell>Cora</cell><cell>50</cell><cell>Laplacian matrix</cell><cell>0.1</cell></row><row><cell>Citeseer</cell><cell>100</cell><cell>Laplacian matrix</cell><cell>0.01</cell></row><row><cell>Pubmed</cell><cell>50</cell><cell>Adjacency matrix</cell><cell>0.5</cell></row><row><cell>Arxiv</cell><cell>100</cell><cell>Laplacian matrix</cell><cell>0.5</cell></row><row><cell>Chameleon</cell><cell>50</cell><cell>Laplacian matrix</cell><cell>0.1</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>Table 7 :</head><label>7</label><figDesc>Best Label Propagation configurations</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head>Table 8 :</head><label>8</label><figDesc>The performance comparisons on different splits of different datasets.</figDesc><table /></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0">For example, a user with only one movie watched or an item with too few purchases.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_1">Compared to Equation (1), multiplication by Ã plays the role of aggregating both {xi} and {xj}i∈N i .</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3" xml:id="foot_2">This will be inferred in the case of labels missing.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4" xml:id="foot_3">We abuse terminology here since E contains node and structural embeddings from multiple layers</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="5" xml:id="foot_4">We ignore the node features and use the label logits as explained in<ref type="bibr" target="#b16">[17]</ref>.</note>
		</body>
		<back>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A SEARCH SPACE DETAILS</head><p>In the computation of FCR, we include a search space of model hyperparameters for GNN, MLP and LP, in order to find the best suitable configurations for distillation.</p><p>For GNN model, we take GCN as a backbone and performed grid search over the number of hidden layers, whether it has structural embedding or not, the type of residual connection, and the type of normalization. For the number of hidden layers, we considered 2, 4, 8, 16, 32, and 64. For the types of residual connections, we include: (1) connecting to the last layer <ref type="bibr" target="#b39">[40,</ref><ref type="bibr" target="#b26">27]</ref>, (2) initial connection connecting to the initial layer <ref type="bibr" target="#b40">[41,</ref><ref type="bibr" target="#b11">12,</ref><ref type="bibr" target="#b41">42]</ref>, (3) dense connection connecting to all the preceding layers <ref type="bibr" target="#b39">[40,</ref><ref type="bibr" target="#b26">27,</ref><ref type="bibr" target="#b42">43,</ref><ref type="bibr" target="#b43">44]</ref>, and (4) jumping connection combining all all the preceding layers only at the final graph convolutional layer <ref type="bibr" target="#b44">[45,</ref><ref type="bibr" target="#b45">46]</ref>. For the types of normalizations, we grid search over: batch normalization (BatchNorm) <ref type="bibr" target="#b46">[47]</ref>, pair normalization (PairNorm) <ref type="bibr" target="#b47">[48]</ref>, node normalization (NodeNorm) <ref type="bibr" target="#b48">[49]</ref>, mean normalization (MeanNorm) <ref type="bibr" target="#b49">[50]</ref>, and differentiable group normalization (GroupNorm) <ref type="bibr" target="#b50">[51]</ref>.</p><p>For the architecture design for Cold Brew's MLP, we conducted hyperparameter search over the number of hidden layers, the existence of residual connection, the hidden dimensions, and the optimizers. The number of hidden layers is searched over <ref type="bibr" target="#b1">(2,</ref><ref type="bibr" target="#b7">8,</ref><ref type="bibr" target="#b15">16,</ref><ref type="bibr" target="#b31">32)</ref>. The number of hidden dimensions is searched over (128, 256). The optimizer is searched over (Adam(lr=0.001) Adam(lr=0.005), Adam(lr=0.02), SGD(lr=0.005))</p><p>For the Label Propagation, we conducted hyperparameter search over number of propagations, propagation matrix type, and the mixing coefficient α <ref type="bibr" target="#b16">[17]</ref>. The number of propagations is searched over <ref type="bibr" target="#b9">(10,</ref><ref type="bibr" target="#b19">20,</ref><ref type="bibr" target="#b49">50,</ref><ref type="bibr">100,</ref><ref type="bibr">200)</ref>. The propagation matrix type is searched over adjacency matrix or normalized Laplacian matrix. The mixing coefficient α is searched over (0.01, 0.1, 0.5, 0.9, 0.99).</p><p>The best GCN, MLP, and LP configurations are reported in Tables 5, 6, and 7, respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B THE PERFORMANCE ON ALL SPLITS OF THE DATA</head><p>The performance evaluations on all splits are listed in </p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">A comprehensive survey on graph neural networks</title>
		<author>
			<persName><forename type="first">Zonghan</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shirui</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fengwen</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Guodong</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chengqi</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Philip</forename><surname>Yu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE transactions on neural networks and learning systems</title>
				<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="page" from="4" to="24" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">Graph embedding techniques, applications, and performance: A survey. Knowledge-Based Systems</title>
		<author>
			<persName><forename type="first">Palash</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Emilio</forename><surname>Ferrara</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="volume">151</biblScope>
			<biblScope unit="page" from="78" to="94" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">Recommendation system using a deep learning and graph analysis approach</title>
		<author>
			<persName><forename type="first">Mahdi</forename><surname>Kherad</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Amir</forename><surname>Jalaly</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bidgoly</forename></persName>
		</author>
		<idno type="arXiv">arXiv:2004.08100</idno>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Recommendation system in e-commerce websites: A graph based approached</title>
		<author>
			<persName><forename type="first">Shakila</forename><surname>Shaikh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sheetal</forename><surname>Rathi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Prachi</forename><surname>Janrao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2017 IEEE 7th International Advance Computing Conference (IACC)</title>
				<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="931" to="934" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">A graph-based friend recommendation system using genetic algorithm</title>
		<author>
			<persName><forename type="first">Ren</forename><surname>Nitai B Silva</surname></persName>
		</author>
		<author>
			<persName><forename type="first">George</forename><forename type="middle">Dc</forename><surname>Tsang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jyh</forename><surname>Cavalcanti</surname></persName>
		</author>
		<author>
			<persName><surname>Tsang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE congress on evolutionary computation</title>
				<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2010">2010</date>
			<biblScope unit="page" from="1" to="7" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">Star-gcn: Stacked and reconstructed graph convolutional networks for recommender systems</title>
		<author>
			<persName><forename type="first">Jiani</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xingjian</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shenglin</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Irwin</forename><surname>King</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1905.13129</idno>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">Semi-supervised classification with graph convolutional networks</title>
		<author>
			<persName><forename type="first">N</forename><surname>Thomas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Max</forename><surname>Kipf</surname></persName>
		</author>
		<author>
			<persName><surname>Welling</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1609.02907</idno>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">Graph attention networks</title>
		<author>
			<persName><forename type="first">Petar</forename><surname>Veličković</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Guillem</forename><surname>Cucurull</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Arantxa</forename><surname>Casanova</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Adriana</forename><surname>Romero</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pietro</forename><surname>Lio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1710.10903</idno>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">Inductive representation learning on large graphs</title>
		<author>
			<persName><forename type="first">Rex</forename><surname>William L Hamilton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jure</forename><surname>Ying</surname></persName>
		</author>
		<author>
			<persName><surname>Leskovec</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1706.02216</idno>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<author>
			<persName><forename type="first">Keyulu</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Weihua</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jure</forename><surname>Leskovec</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stefanie</forename><surname>Jegelka</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1810.00826</idno>
		<title level="m">How powerful are graph neural networks?</title>
				<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Simplifying graph convolutional networks</title>
		<author>
			<persName><forename type="first">Felix</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Amauri</forename><surname>Souza</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tianyi</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christopher</forename><surname>Fifty</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tao</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kilian</forename><surname>Weinberger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International conference on machine learning</title>
				<imprint>
			<publisher>PMLR</publisher>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="6861" to="6871" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">Predict then propagate: Graph neural networks meet personalized pagerank</title>
		<author>
			<persName><forename type="first">Johannes</forename><surname>Klicpera</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aleksandar</forename><surname>Bojchevski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stephan</forename><surname>Günnemann</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1810.05997</idno>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Transductive learning via spectral graph partitioning</title>
		<author>
			<persName><forename type="first">Thorsten</forename><surname>Joachims</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 20th International Conference on Machine Learning (ICML-03)</title>
				<meeting>the 20th International Conference on Machine Learning (ICML-03)</meeting>
		<imprint>
			<date type="published" when="2003">2003</date>
			<biblScope unit="page" from="290" to="297" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Spectral clustering of graphs with general degrees in the extended planted partition model</title>
		<author>
			<persName><forename type="first">Kamalika</forename><surname>Chaudhuri</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fan</forename><surname>Chung</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alexander</forename><surname>Tsiatas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">JMLR Workshop and Conference Proceedings</title>
				<imprint>
			<date type="published" when="2012">2012</date>
			<biblScope unit="page" from="35" to="36" />
		</imprint>
	</monogr>
	<note>Conference on Learning Theory</note>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">Understanding regularized spectral clustering via graph conductance</title>
		<author>
			<persName><forename type="first">Yilin</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Karl</forename><surname>Rohe</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1806.01468</idno>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">Dissecting the diffusion process in linear graph convolutional networks</title>
		<author>
			<persName><forename type="first">Yifei</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yisen</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiansheng</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhouchen</forename><surname>Lin</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2102.10739</idno>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">Combining label propagation and simple models out-performs graph neural networks</title>
		<author>
			<persName><forename type="first">Qian</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Horace</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Abhay</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ser-Nam</forename><surname>Lim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Austin</forename><forename type="middle">R</forename><surname>Benson</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2010.13993</idno>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">Semi-supervised learning with graphs</title>
		<author>
			<persName><forename type="first">Xiaojin</forename><surname>Zhu</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2005">2005</date>
		</imprint>
		<respStmt>
			<orgName>Carnegie Mellon University</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Pre-training graph neural networks for cold-start users and items representation</title>
		<author>
			<persName><forename type="first">Bowen</forename><surname>Hao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jing</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hongzhi</forename><surname>Yin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Cuiping</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hong</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 14th ACM International Conference on Web Search and Data Mining</title>
				<meeting>the 14th ACM International Conference on Web Search and Data Mining</meeting>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="265" to="273" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">Zero-shot recommender systems</title>
		<author>
			<persName><forename type="first">Yifei</forename><surname>Hao Ding</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Anoop</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuyang</forename><surname>Deoras</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hao</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><surname>Wang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2105.08318</idno>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Addressing cold-start problem in recommendation systems</title>
		<author>
			<persName><forename type="first">Xuan Nhat</forename><surname>Lam</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Thuc</forename><surname>Vu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Trong</forename><surname>Duc Le</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Anh</forename><forename type="middle">Duc</forename><surname>Duong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2nd international conference on Ubiquitous information management and communication</title>
				<meeting>the 2nd international conference on Ubiquitous information management and communication</meeting>
		<imprint>
			<date type="published" when="2008">2008</date>
			<biblScope unit="page" from="208" to="211" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Meta-learning on heterogeneous information networks for cold-start recommendation</title>
		<author>
			<persName><forename type="first">Yuanfu</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuan</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chuan</forename><surname>Shi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 26th ACM SIGKDD International Conference on Knowledge Discovery &amp; Data Mining</title>
				<meeting>the 26th ACM SIGKDD International Conference on Knowledge Discovery &amp; Data Mining</meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="1563" to="1573" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">From zero-shot learning to cold-start recommendation</title>
		<author>
			<persName><forename type="first">Jingjing</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mengmeng</forename><surname>Jing</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ke</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lei</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yang</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zi</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the AAAI Conference on Artificial Intelligence</title>
				<meeting>the AAAI Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page" from="4189" to="4196" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Extract the knowledge of graph neural networks and go beyond it: An effective knowledge distillation framework</title>
		<author>
			<persName><forename type="first">Cheng</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiawei</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chuan</forename><surname>Shi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Web Conference 2021</title>
				<meeting>the Web Conference 2021</meeting>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="1227" to="1237" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title level="m" type="main">On selfdistilling graph neural network</title>
		<author>
			<persName><forename type="first">Yuzhao</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yatao</forename><surname>Bian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xi</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yu</forename><surname>Rong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tingyang</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Junzhou</forename><surname>Huang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2011.02255</idno>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Graph neural networks exponentially lose expressive power for node classification</title>
		<author>
			<persName><forename type="first">Kenta</forename><surname>Oono</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Taiji</forename><surname>Suzuki</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
				<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Deeper insights into graph convolutional networks for semi-supervised learning</title>
		<author>
			<persName><forename type="first">Qimai</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhichao</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiao-Ming</forename><surname>Wu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Thirty-Second AAAI Conference on Artificial Intelligence</title>
				<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<author>
			<persName><forename type="first">N</forename><forename type="middle">T</forename><surname>Hoang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Takanori</forename><surname>Maehara</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1905.09550</idno>
		<title level="m">Revisiting graph neural networks: All we have is low-pass filters</title>
				<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<author>
			<persName><forename type="first">Geoffrey</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Oriol</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jeff</forename><surname>Dean</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1503.02531</idno>
		<title level="m">Distilling the knowledge in a neural network</title>
				<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<title level="m" type="main">Graph-mlp: Node classification without message passing in graph</title>
		<author>
			<persName><forename type="first">Yang</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Haoxuan</forename><surname>You</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhecan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhicheng</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Erjin</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yue</forename><surname>Gao</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2106.04051</idno>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
		<title level="m" type="main">Graph-less neural networks: Teaching old mlps new tricks via distillation</title>
		<author>
			<persName><forename type="first">Shichang</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yozen</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yizhou</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Neil</forename><surname>Shah</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2110.08727</idno>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
		<title level="m" type="main">Geom-gcn: Geometric graph convolutional networks</title>
		<author>
			<persName><forename type="first">Hongbin</forename><surname>Pei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bingzhe</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kevin</forename><surname>Chen-Chuan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yu</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bo</forename><surname>Lei</surname></persName>
		</author>
		<author>
			<persName><surname>Yang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2002.05287</idno>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
		<author>
			<persName><forename type="first">Meng</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhengyang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shuiwang</forename><surname>Ji</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2005.14612</idno>
		<title level="m">Non-local graph neural networks</title>
				<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Attention is all you need</title>
		<author>
			<persName><forename type="first">Ashish</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Noam</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Niki</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jakob</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Llion</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aidan</forename><forename type="middle">N</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Łukasz</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Illia</forename><surname>Polosukhin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
				<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="5998" to="6008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Yannis Avrithis, and Ondrej Chum. Label propagation for deep semi-supervised learning</title>
		<author>
			<persName><forename type="first">Ahmet</forename><surname>Iscen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Giorgos</forename><surname>Tolias</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
				<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="5070" to="5079" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<monogr>
		<title level="m" type="main">Unifying graph convolutional neural networks and label propagation</title>
		<author>
			<persName><forename type="first">Hongwei</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jure</forename><surname>Leskovec</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2002.06755</idno>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b36">
	<monogr>
		<title level="m" type="main">Combining label propagation and simple models out-performs graph neural networks</title>
		<author>
			<persName><forename type="first">Qian</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Horace</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Abhay</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ser-Nam</forename><surname>Lim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Austin</forename><forename type="middle">R</forename><surname>Benson</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2010.13993</idno>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b37">
	<monogr>
		<author>
			<persName><forename type="first">Adriana</forename><surname>Romero</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nicolas</forename><surname>Ballas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Samira</forename><forename type="middle">Ebrahimi</forename><surname>Kahou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Antoine</forename><surname>Chassang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Carlo</forename><surname>Gatta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1412.6550</idno>
		<title level="m">Fitnets: Hints for thin deep nets</title>
				<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b38">
	<monogr>
		<title level="m" type="main">On the bottleneck of graph neural networks and its practical implications</title>
		<author>
			<persName><forename type="first">Uri</forename><surname>Alon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Eran</forename><surname>Yahav</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2006.05205</idno>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Deepgcns: Can gcns go as deep as cnns?</title>
		<author>
			<persName><forename type="first">Guohao</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Matthias</forename><surname>Muller</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ali</forename><surname>Thabet</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bernard</forename><surname>Ghanem</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision</title>
				<meeting>the IEEE/CVF International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="9267" to="9276" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<monogr>
		<title level="m" type="main">Simple and deep graph convolutional networks</title>
		<author>
			<persName><forename type="first">Ming</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhewei</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zengfeng</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bolin</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yaliang</forename><surname>Li</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2007.02133</idno>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b41">
	<monogr>
		<title level="m" type="main">Revisiting graph convolutional network on semi-supervised node classification from an optimization perspective</title>
		<author>
			<persName><forename type="first">Hongwei</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tijin</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zenjun</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuanqing</forename><surname>Xia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuan</forename><surname>Zhang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2009.11469</idno>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b42">
	<monogr>
		<title level="m" type="main">Deepergcn: All you need to train deeper gcns</title>
		<author>
			<persName><forename type="first">Guohao</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chenxin</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ali</forename><surname>Thabet</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bernard</forename><surname>Ghanem</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2006.07739</idno>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b43">
	<monogr>
		<title level="m" type="main">Break the ceiling: Stronger multi-scale deep graph convolutional networks</title>
		<author>
			<persName><forename type="first">Sitao</forename><surname>Luan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mingde</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiao-Wen</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Doina</forename><surname>Precup</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1906.02174</idno>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Representation learning on graphs with jumping knowledge networks</title>
		<author>
			<persName><forename type="first">Keyulu</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chengtao</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yonglong</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tomohiro</forename><surname>Sonobe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ken-Ichi</forename><surname>Kawarabayashi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stefanie</forename><surname>Jegelka</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
				<imprint>
			<publisher>PMLR</publisher>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="5453" to="5462" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Towards deeper graph neural networks</title>
		<author>
			<persName><forename type="first">Meng</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hongyang</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shuiwang</forename><surname>Ji</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 26th ACM SIGKDD International Conference on Knowledge Discovery &amp; Data Mining</title>
				<meeting>the 26th ACM SIGKDD International Conference on Knowledge Discovery &amp; Data Mining</meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="338" to="348" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Batch normalization: Accelerating deep network training by reducing internal covariate shift</title>
		<author>
			<persName><forename type="first">Sergey</forename><surname>Ioffe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christian</forename><surname>Szegedy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ICML</title>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<monogr>
		<author>
			<persName><forename type="first">Lingxiao</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Leman</forename><surname>Akoglu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1909.12223</idno>
		<title level="m">Pairnorm: Tackling oversmoothing in gnns</title>
				<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b48">
	<monogr>
		<title level="m" type="main">Understanding and resolving performance degradation in graph convolutional networks</title>
		<author>
			<persName><forename type="first">Kuangqi</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yanfei</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kaixin</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wee</forename><surname>Sun Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bryan</forename><surname>Hooi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Huan</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiashi</forename><surname>Feng</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2006.07107</idno>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b49">
	<monogr>
		<author>
			<persName><forename type="first">Chaoqi</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ruijie</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shuochao</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shengzhong</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tarek</forename><surname>Abdelzaher</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2003.13663</idno>
		<title level="m">Revisiting &quot;over-smoothing&quot; in deep gcns</title>
				<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Towards deeper graph neural networks with differentiable group normalization</title>
		<author>
			<persName><forename type="first">Kaixiong</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiao</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuening</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Daochen</forename><surname>Zha</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rui</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xia</forename><surname>Hu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
				<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="volume">33</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title/>
	</analytic>
	<monogr>
		<title level="j">Preprint Cora Citeseer Pubmed Arxiv Cham. Squ. Actor Cornell Texas Wisconsin GNN</title>
		<imprint>
			<biblScope unit="volume">86</biblScope>
			<biblScope unit="page">96</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<monogr>
		<idno>FCR % 32.86 % 63.39 % 76.91% 16.45% 73.61% 141.91% 57.93% 139.04% 171.2 % 108.48</idno>
		<title level="m">% β(G) % 83% 71% 79% 68% 25% 22% 24% 11% 6%</title>
				<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<monogr>
		<title level="m" type="main">Above half: FCR and its components, β metric are added as a reference</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
		</imprint>
	</monogr>
	<note>below half: the performance difference of GNN on the head/tail and head/isolation splits</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
