<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">PIUMA: Programmable Integrated Unified Memory Architecture</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date type="published" when="2020-10-13">13 Oct 2020</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Sriram</forename><surname>Aananthakrishnan</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Nesreen</forename><forename type="middle">K</forename><surname>Ahmed</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Vincent</forename><surname>Cavé</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Marcelo</forename><surname>Cintra</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Yigit</forename><surname>Demir</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Du</forename><surname>Kristof</surname></persName>
						</author>
						<author>
							<persName><surname>Bois</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Stijn</forename><surname>Eyerman</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Joshua</forename><forename type="middle">B</forename><surname>Fryman</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Ivan</forename><surname>Ganev</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Wim</forename><surname>Heirman</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Hans-Christian</forename><surname>Hoppe</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Jason</forename><surname>Howard</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Ibrahim</forename><surname>Hur</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Midhunchandra</forename><surname>Kodiyath</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Samkit</forename><surname>Jain</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Daniel</forename><forename type="middle">S</forename><surname>Klowden</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Marek</forename><forename type="middle">M</forename><surname>Landowski</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Laurent</forename><surname>Montigny</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Ankit</forename><surname>More</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Przemyslaw</forename><surname>Ossowski</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Robert</forename><surname>Pawlowski</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Nick</forename><surname>Pepperling</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Fabrizio</forename><surname>Petrini</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Mariusz</forename><surname>Sikora</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Balasubramanian</forename><surname>Seshasayee</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Shaden</forename><surname>Smith</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Sebastian</forename><surname>Szkoda</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Sanjaya</forename><surname>Tayal</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Jesmin</forename><forename type="middle">Jahan</forename><surname>Tithi</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Yves</forename><surname>Vandriessche</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Izajasz</forename><forename type="middle">P</forename><surname>Wrosz</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Intel</forename><surname>Corporation</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Microsoft</forename><surname>Corporation</surname></persName>
						</author>
						<title level="a" type="main">PIUMA: Programmable Integrated Unified Memory Architecture</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2020-10-13">13 Oct 2020</date>
						</imprint>
					</monogr>
					<idno type="arXiv">arXiv:2010.06277v1[cs.AR]</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.2" ident="GROBID" when="2022-12-25T13:09+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>High performance large scale graph analytics is essential to timely analyze relationships in big data sets. Conventional processor architectures suffer from inefficient resource usage and bad scaling on graph workloads. To enable efficient and scalable graph analysis, Intel developed the Programmable Integrated Unified Memory Architecture (PIUMA). PIUMA consists of many multi-threaded cores, fine-grained memory and network accesses, a globally shared address space and powerful offload engines. This paper presents the PIUMA architecture, and provides initial performance estimations, projecting that a PIUMA node will outperform a conventional compute node by one to two orders of magnitude. Furthermore, PIUMA continues to scale across multiple nodes, which is a challenge in conventional multinode setups.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>I. INTRODUCTION</head><p>Current practices in data analytics and artificial intelligence perform tasks such as object classification on unending streams of data. Computing infrastructure for classification is predominantly oriented toward "dense" compute, such as matrix computations. The continuing exponential growth in generated data <ref type="bibr" target="#b0">[1]</ref> has shifted compute to offload to GPUs and other focused accelerators across multiple domains that are densecompute dominated.</p><p>The next step in both AI and data analytics is reasoning about the relationships between these classified objects, typically represented as a graph. Determining the relationships between entities in a graph is the basis of graph analytics <ref type="bibr" target="#b1">[2]</ref>. Graph analytics poses important challenges on existing processor architectures due to its sparse structure. This sparseness leads to scattered and irregular memory accesses and communication, challenging the optimizations implemented for decades that have gone into traditional dense compute solutions. Consider the common case of pushing data along the graph edges, see the example graph in Figure <ref type="figure" target="#fig_1">1</ref>. All vertices initially store a value locally and then proceed to add their value to all neighbors along outgoing edges. This basic computation is ubiquitous in graph algorithms such as PageRank <ref type="bibr" target="#b2">[3]</ref>. The resulting access stream (Figure <ref type="figure" target="#fig_1">1b</ref>) is § Ankit More and Shaden Smith were with Intel when working on this project.  irregular and has no locality, making conventional prefetching and caching useless. The combination of low performance and very large graph sizes limits the usability of graph analytics. Recognizing both the increasing importance of graph analytics and the need for vastly improved sparse computation performance compared to traditional approaches, DARPA launched their HIVE program to achieve at least 1000× Performance/Watt breakthrough on such large problems before the end of 2022 <ref type="bibr" target="#b3">[4]</ref>.</p><p>This paper introduces the Programmable Integrated Unified Memory Architecture (PIUMA) developed for the DARPA HIVE program. The PIUMA machine is designed for graph analytics at massive scales. PIUMA enables high-performance graph processing by addressing constraints across the network, memory, and compute architectures that typically limit performance on graph workloads.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>II. GRAPH WORKLOAD CHALLENGES</head><p>Graph algorithms face several major scalability challenges on existing architectures, because of their irregularity and sparsity.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Challenge 1: Cache and Bandwidth Utilization</head><p>Figure <ref type="figure">2</ref> shows the cache line utilization for a variety of graph analysis applications when executed on a conventional cache based processor with prefetcher. For every 64 byte cache line fetched from memory, it shows how many bytes are actually used by the CPU. For most cache lines, either 0, 8, or the full 64 bytes are used.</p><p>The zero usage fraction stems from cache lines that were prefetched but never used. Cache lines with 8 or fewer bytes used are caused by sparse accesses with no spatial locality. A typical pattern in graph application is a chain of indirect loads, similar to a pointer chasing pattern: a vertex's neighbors are stored in a list, which are used to index the data array. Because neighbor lists do not show regularity or locality, accesses to the data array are intrinsically sparse. Other memory accesses have limited locality (e.g., fetching the neighbor list itself), as shown by the fraction where all 64 bytes are used, but they are limited in size, explaining the useless prefetches that fetch past the end of the list.</p><p>As a result, the execution of graph applications suffers from inefficient cache and bandwidth utilization: caches are thrashed with single-use sparse accesses and useless prefetches, and most of the 64 byte memory fetches contain only one 8-byte useful data element. Overprovisioning memory bandwidth and/or cache space to cope with sparsity is inefficient in terms of power consumption, chip area and I/O pin count. Instead, PIUMA uses limited caching and small granularity memory accesses to efficiently deal with the memory behavior of graph applications.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Challenge 2: Irregular Computation and Memory Intensity</head><p>Further analysis of graph algorithms shows additional problems in optimizing performance. The computations are irregular: they exhibit skewed compute time distributions, encounter frequent control flow instructions, and perform many memory accesses. The compute time for a vertex in the PageRank example is proportional to the number of outgoing edges (degree) of that vertex. Graphs such as the one illustrated in Figure <ref type="figure" target="#fig_1">1</ref> have skewed degree distributions, and thus the work per vertex has a high variance, leading to significant load imbalance.</p><p>Our analysis reveals that graph applications are heavy on branches and memory operations. Furthermore, conditional branches are often data dependent, e.g., checking the degree or certain properties of vertices, leading to irregular and therefore hard to predict branch outcomes. Together with the high cache miss rates caused by the sparse accesses, conventional performance oriented out-of-order processors are largely underutilized: most of the time they are stalled on cache misses, while a large part of the speculative resources is wasted due to branch mispredictions. In PIUMA, this observation was the incentive to use single issue in-order pipelines with many threads to hide memory latency and avoid speculation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Challenge 3: Fine-and Coarse-Grained synchronization</head><p>Graph algorithms require frequent fine-and coarse-grained synchronization. For example, PageRank requires fine-grained synchronizations (e.g., atomics) to prevent race conditions when pushing values along edges. Synchronization instructions that resolve in the cache hierarchy place a large stress on the cache coherency mechanisms for multi-socket systems, and all synchronizations incur long round-trip latencies on multinode systems. Additionally, the sparse memory accesses result in even more memory traffic for synchronizations due to false sharing in the cache coherency system.</p><p>Coarse-grained synchronizations (e.g., system-wide barriers and prefix scans) fence the already-challenging computations in graph algorithms. These synchronizations have diverse uses including resource coordination, dynamic load balancing, and the aggregation of partial results. These synchronizations can dominate execution time on large-scale systems due to high network latencies and imbalanced computation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D. Challenge 4: Massive Datasets</head><p>Current commercial graph databases exceed 20 TB as an in-memory representation. Such large problems exceed the capabilities of even a rack of computational nodes of any type, which requires a large-scale multi-node platform to even house the graph's working set. When combined with the prior observations -poor memory hierarchy utilization, high control flow changes, frequent memory references, and abundant synchronizations -any architecture that targets graph workloads must focus on reducing latency to access remote data, combined with latency hiding techniques in the processing elements.</p><p>Although the analysis in this section focuses on CPUs, the same challenges apply for GPUs: sparse accesses prevent memory coalescing, branches cause thread divergence and synchronization limits thread progress. Nevertheless, GPUs usually perform better on graph algorithms than CPUs for small graphs <ref type="bibr" target="#b4">[5]</ref>, because they have more threads, which hides memory latency, and much higher memory bandwidth, brute-forcing the inefficient bandwidth utilization. However, GPUs have limited memory capacity and limited scale-out capabilities, which means that they are unable to process large, multi-TB graphs. Furthermore, graphs are extremely sparse ( 1% non-zeros), so the typical GPU trick to densify the connectivity matrix for a more efficient GPU execution leads to another few orders of magnitude increase in memory usage, restricting it to small graphs only. PIUMA directly operates on sparse data (e.g., CSR).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>III. PIUMA ARCHITECTURE</head><p>The observations on graph analysis workloads guided the PIUMA design, targeting breakthrough performance per Watt for graph analytics. We discuss how each component of the UMA architecture is designed to cope with the challenges imposed by graph workloads.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. PIUMA Cores</head><p>The design of PIUMA cores builds on the observations that most graph workloads have abundant parallelism, are memory bound and are not compute intensive. These observations call for many simple pipelines, with multi-threading to hide memory latency, see Figure <ref type="figure" target="#fig_2">3</ref>. PIUMA multi-threaded cores (MTC) are round-robin multi-threaded in-order pipelines <ref type="bibr" target="#b5">[6]</ref>. At any moment, each thread can only have one in-flight instruction, which considerably simplifies the core design for better energy efficiency. Single-threaded cores (STC) are used for singlethread performance sensitive tasks, such as memory and thread management threads (e.g., from the operating system). These are in-order stall-on-use cores that are able to exploit some instruction and memory-level parallelism, while avoiding the high power consumption of aggressive out-or-order pipelines. Both core types implement the same custom RISC instruction set.</p><p>Each core has a small data and instruction cache (D$ and I$), and a register file (RF) to support its thread count. Because of the low locality in graph workloads, no higher cache levels are included, avoiding useless chip area and power consumption of large caches. For scalability, caches are not coherent across the whole system. It is the responsability of the programmer to avoid modifying shared data that is cached, or to flush caches if required for correctness. MTCs and STCs are grouped into blocks, each of which has a large local scratchpad (SPAD) for low latency storage. Programmers are responsible for selecting which memory accesses to cache (e.g., local stack), which to put on SPAD (e.g., often reused data structures or the result of a DMA gather operation) and which not to store locally. There are no prefetchers to avoid useless data fetches and to limit power consumption. Instead, the offload engines described below can be used to efficiently fetch large chunks of useful data.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Offload Engines</head><p>Although the MTCs hide some of the memory latency by supporting multiple concurrent threads, their in-order design limits the number of outstanding memory accesses to one per thread. To increase memory-level parallelism and to free more compute cycles to the cores, a memory offload engine is added to each block. The offload engine performs memory operations typically found in many graph applications in the background, while the cores continue with their computations. The direct memory access (DMA) engine performs operations such as (strided) copy, scatter and gather. Queue engines are responsible for maintaining queues allocated in shared memory, alleviating the core from atomic inserts and removals. They can be used for work stealing algorithms and dynamically partitioning the workload. Collective engines implement efficient system-wide reductions and barriers. Remote atomics perform atomic operations at the memory controller where the data is located, instead of burdening the pipeline with first locking the data, moving the data to the core, updating it, writing back and unlocking. They enable efficient and scalable synchronization, which is indispensable for the high thread count in PIUMA.</p><p>The engines are directed by the PIUMA cores using specific PIUMA instructions. These instructions are non-blocking, enabling the cores to perform other work while the operation is done in the background. Custom polling and waiting instructions are used to synchronize the threads and offloaded computations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Memory Organization</head><p>Sparse and irregular accesses to a large data structure are typical for graph analysis applications. Therefore, accesses to remote memory should be done with minimal overhead. PIUMA implements a hardware distributed global address space (DGAS), which enables each core to uniformly access memory across the full system (multiple PIUMA nodes) with one address space. Besides avoiding the overhead of setting up communication for remote accesses, a DGAS also greatly simplifies programming, because there is no implementation difference between accessing local and remote memory. Address translation tables (ATT) contain programmable rules to translate application memory addresses to physical locations, to arrange the address space to the need of the application (e.g., address interleaved, block partitioned, etc.).</p><p>The memory controllers (one per block) are redesigned to support native 8-byte acesses, while supporting standard cache line accesses as well. Fetching only the data that is actually needed reduces memory bandwidth pressure and utilizes the available bandwidth more efficiently.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D. Network</head><p>The network connecting the blocks is responsible for sending memory requests to remote memory controllers. Similar to the memory controller, it is optimized for small 8 byte messages. Furthermore, due to the high fraction of remote accesses, network bandwidth exceeds local DRAM bandwidth, which is different from conventional architectures that assume higher local traffic than remote traffic.</p><p>To obtain high bandwidth and low latency to remote blocks, the network needs to have a high radix and a low diameter. This is achieved with a HyperX topology <ref type="bibr" target="#b6">[7]</ref>, with all-toall connections on each level. Links on the highest levels are optical to ensure power-efficient, high-bandwidth communication. The hierarchical topology and optical links enable PIUMA to efficiently scale out to many nodes, maintaining easy and fast remote access.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E. Comparison to other Graph Processors</head><p>The Cray Urika-GD graph processor <ref type="bibr" target="#b7">[8]</ref> was one of the first commercial graph-oriented big data processors. Similar to PIUMA, it consists of multiple many-threaded cores with no large caches and a memory-coherent network. It does not support fine-grained 8 byte accesses, wasting bandwidth on loading full cache lines. Furthermore, it has no offload memory engines, such as the DMA, QMA and remote atomics as in PIUMA, leading to more memory stalls in the pipelines. As we will show in the results section, 8 byte accesses and offload memory engines are important contributors to PUMA's performance and energy efficiency.</p><p>The Emu architecture <ref type="bibr" target="#b8">[9]</ref> is a recently proposed architecture for big data analysis, including graph analysis workloads. Similar to PIUMA and Urika-GD, it consists of many small cores with many hardware threads per core to hide memory latency. It also features 8 byte DRAM accesses and is completely cacheless. Unique is its low-overhead thread migration scheme, which enables moving threads to a core near to the memory controller that owns the required data instead of moving the data to the current core. Moving threads to data is beneficial if the overhead of moving the thread is compensated by the amount of locally consumed data. Young et al. <ref type="bibr" target="#b9">[10]</ref> report that migrating a thread involves moving 200 bytes, which means that at least 25 local 8 byte accesses are needed to compensate for the thread migration. Therefore, optimizing data locality is crucial for obtaining good performance on Emu <ref type="bibr" target="#b9">[10]</ref>, which is often hard to obtain for graph analysis applications. In contrast, PIUMA does not rely on any locality. Instead, it uses the offload engines to perform complex systemwide memory operations in parallel, and only moves the data that is eventually needed to the core that requests it. For example, a DMA gather will not move the memory stored indices or addresses of the data elements to gather to the requesting core, only the requested elements from the data array.</p><p>Song et al. <ref type="bibr" target="#b10">[11]</ref> propose a graph processor based on sparse matrix algebra, building on the observation that many graph applications can be represented as operations on sparse matrices. Their architecture has overlaps with PIUMA, such as the absence of caches, and fine-grained communication and memory accesses. Graphicionado <ref type="bibr" target="#b11">[12]</ref> is a graph analysis accelerator, implementing a vertex-centric compute paradigm. While these accelerators are likely more energy efficient for analyzing small graphs, PIUMA's goal is to provide a flexible instruction set architecture, optimized for typical graph analysis operations, and is not limited to algorithms that use sparse matrix algebra or vertex-centric operations. Furthermore, none of these proposals scale out to multi-TB graphs with trillions of vertices.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>IV. HARDWARE/SOFTWARE CO-DESIGN</head><p>Crucial for the pathfinding and development of PIUMA was the hardware/software co-design process. This process requires the involvement of multiple multi-disciplinary teams: architects, system software developers, workload analysis teams, and performance simulation and analysis teams.</p><p>We developed a C/C++ compiler based on LLVM that supports the RISC ISA of PIUMA, including basic library functions. PIUMA specific operations, such as the offload engines and remote atomics, are accessible using intrinsics. Additionally, we constructed a runtime environment that implements basic memory and thread management, supporting common programming models, such as gather-apply-scatter, task-based and SPMD-style parallelism.</p><p>In parallel, we developed an architectural simulator for PIUMA, simulating the timing of all instructions in the pipelines, engines, memory and network, based on the hardware specifications. Next to performance estimations of running a workload on PIUMA, it provides an extensive set of performance analysis reports, such as CPI stacks and detailed performance information on each memory structure and each instruction. This enables workload owners to quickly detect bottleneck causes, and to use these insights to optimize the workload for PIUMA and report hardware bottlenecks to the hardware design team. The hardware team then responds with an updated design, feeding a continuous cycle of gradual improvements to hardware and software.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>V. PERFORMANCE RESULTS</head><p>We implemented a variety of graph analysis applications for PIUMA and evaluated their performance using our detailed timing simulator. The applications are selected from a list of high priority workloads suggested by DARPA for the HIVE project, as well as common sparse applications. We compare the estimated PIUMA performance with timing measurements of the same applications on a high-end Intel Xeon server (Intel Xeon Gold 6140), containing 4 sockets of 18 cores each. Initial power estimates show that a PIUMA node, containing 256 PIUMA blocks, consumes about the same amount of power as the Xeon server. Therefore, a performance comparison between 1 PIUMA node and the Xeon server is also an energy efficiency comparison.</p><p>Because most applications are basic kernels, the baseline Xeon implementations are hand-crafted and optimized, avoiding potential library overhead. The PIUMA implementations are also written from scratch. For Xeon, graph applications do not scale well beyond a single node, with even worse performance for small node counts, due to the overhead of the fine-grained communication <ref type="bibr" target="#b12">[13]</ref>. For PIUMA, the application code does not need to change for multinode execution, thanks to the system-wide shared memory. Our simulator can practically simulate systems up to a few dozen blocks, we use an analytical model to extrapolate performance for larger systems. The analytical model takes into account increased network latencies and network bandwidth restrictions as the system scales out.</p><p>PIUMA specifically targets scale-out and tera-to-peta-scale workloads, which is why we do not compare against GPUs and other graph accelerators, because these do not support such large workloads.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. SpMV analysis</head><p>To provide insight into how the PIUMA design choices have an impact on performance, we first perform a detailed analysis for sparse matrix dense vector multiplication (SpMV). The basic operation of SpMV is very similar to that of the PageRank algorithm: a multiply-accumulate of sparse matrix elements (the (weighted) graph edges) and a dense vector (the pagerank values). We implement multiple PIUMA versions for SpMV, gradually adding more PIUMA specific operations, to show the impact of each optimization individually. We apply SpMV on an RMAT-30 synthetic matrix, stored in compressed sparse row (CSR) format, both on Xeon and PIUMA. Table <ref type="table" target="#tab_1">I</ref> shows the speedup of each of the implementations versus Xeon.</p><p>The first PIUMA version is a straightforward implementation of SpMV, with each thread calculating one or more elements of the result vector. The rows are partitioned across the threads based on the number of non-zeros for a balanced execution. It does not make use of DMA operations, and all accesses are non-cached 8-byte, which is the default for PIUMA (except for thread local stack accesses, these are cached by default). This basic implementation already outperforms Xeon by a factor 10 through using a higher thread count and 8 byte memory accesses, avoiding memory bandwidth saturation.</p><p>The next implementation uses selective caching: accesses to the matrix values are cached, while the sparse accesses to the vector bypass caches. In the compressed sparse row (CSR) representation of a sparse matrix, all non-zero elements on a row are stored consecutively and accessed sequentially, resulting in spatial locality. The dense vector, on the other hand, is accessed sparsely, because only a few of its elements are needed for the multiply-accumulate, i.e., the indices of the non-zeros in the row of the matrix. So we cache the accesses to the matrix, while the vector accesses remain uncached 8 byte accesses. This leads to another 2× speedup. We also simulated caching all accesses, which led to lower performance compared to the base version. Caching the sparse accesses to the vector causes an increase in memory traffic, because 64 bytes are fetched for every access. The extra traffic saturates the bandwidth, leading to lower overall performance. This experiment shows why uncached 8-byte accesses are required to achieve higher performance and better efficiency for sparse graph applications.</p><p>Lastly, we use a DMA gather operation to fetch the elements of the dense vector that are needed for the current row from memory, and put them on local scratchpad. The multiplyaccumulate reduction is then done by the core, fetching the matrix elements from cache and the vector elements from scratchpad. Not only does this significantly reduce the number of load instructions, it also reduces data movement: the index list does not need to be transferred to the requesting core, only the final gathered vector elements. While data is gathered, the thread is stalled, allowing other threads that have already fetched their data to compute a result vector element. Using the DMA gather offload improves performance by 47%, leading to a total 29× speedup versus Xeon. This implementation uses more than 95% of the available memory bandwidth, while not wasting bandwidth on useless and sparse accesses.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. All applications</head><p>Table <ref type="table" target="#tab_2">II</ref> shows the speedup of a single PIUMA node versus the Xeon server for all evaluated applications. It also shows the speedup of a PIUMA system with 16 nodes, which is the first proof-of-concept system that will be built for the HIVE project. The base is still the single node Xeon performance, we expect that the performance of a 16-node Xeon will not be much higher than that of a single node for graph applications.</p><p>Single node PUMA performance exceeds Xeon performance by one to two orders of magnitude. The performance benefit of low-compute low-locality applications, such as Random Walks, is the highest, while more compute-intensive applications, such as Application Classification, benefit less, but still outperform Xeon. The main reasons are the much higher thread count support (144 threads for Xeon, more than 16K threads for PIUMA), enabling threads to progress while others are stalled on memory operations, efficient small size local and remote memory operations, and powerful offload engines that allow for more memory/compute overlap. The speedups for 16 nodes show that PIUMA scales out well. Scaling is not perfectly linear, due to the larger latencies and bandwidth restrictions, but it significantly outperforms conventional multinode Xeon configurations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>VI. CONCLUSION</head><p>PIUMA is a graph analysis oriented architecture developed by Intel in response to the DARPA HIVE project. Based on the observation that graph workloads are dominated by irregular sparse accesses, it features many highly-threaded simple cores to hide the latency of remote memory accesses. Combined with small access granularity to memory and network, and powerful offload engines, PIUMA outperforms current high-end processors for typical graph workloads. Furthermore, it is designed to scale out efficiently thanks to the high bandwidth network and shared address space, increasing the performance gap with current multinode computers, which perform poorly on distributed graph applications. The effective hardware/software co-design process of PIUMA guarantees highly optimized hardware, and ensures that system and development tools are available by the time PIUMA will be released.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 1 .</head><label>1</label><figDesc>Fig. 1. (a) A sparse graph with directed edges. (b) Memory access patterns observed when moving data along the edges of (a).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 3 .</head><label>3</label><figDesc>Fig. 3. High-level diagram of PIUMA architecture.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>TABLE I SINGLE</head><label>I</label><figDesc>NODE PIUMA SPEEDUP FOR SPMV VERSIONS.</figDesc><table><row><cell></cell><cell>Base</cell><cell cols="2">Selective caching With DMA</cell></row><row><cell cols="2">Versus Xeon 10.0×</cell><cell>19.8×</cell><cell>29.2×</cell></row><row><cell>Versus Base</cell><cell>1×</cell><cell>2.0×</cell><cell>2.9×</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>TABLE II PIUMA</head><label>II</label><figDesc>SPEEDUP VERSUS ONE 4-SOCKET XEON NODE.</figDesc><table><row><cell>Application</cell><cell cols="2">1 node 16 nodes</cell></row><row><cell>Application Classification</cell><cell>6.9×</cell><cell>111×</cell></row><row><cell>Random Walks</cell><cell>279×</cell><cell>2,606×</cell></row><row><cell>Graph Search</cell><cell>34×</cell><cell>544×</cell></row><row><cell>Louvain Community</cell><cell>41×</cell><cell>555×</cell></row><row><cell>TIES Sampler</cell><cell>93×</cell><cell>419×</cell></row><row><cell>Graph2Vec</cell><cell>42×</cell><cell>178×</cell></row><row><cell>Graph Sage</cell><cell>3.1×</cell><cell>46×</cell></row><row><cell>Graph Wave</cell><cell>8.0×</cell><cell>125×</cell></row><row><cell>Parallel Decoding FST</cell><cell>6.8×</cell><cell>109×</cell></row><row><cell>Geolocation</cell><cell>15×</cell><cell>243×</cell></row><row><cell>SpMV</cell><cell>29×</cell><cell>467×</cell></row><row><cell>SpMSpV</cell><cell>111×</cell><cell>1,387×</cell></row><row><cell>Breadth-first Search</cell><cell>7.5×</cell><cell>117×</cell></row></table></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>ACKNOWLEDGEMENT</head><p>This research was, in part, funded by the U.S. Government. The views and conclusions contained in this document are those of the authors and should not be interpreted as representing the official policies, either expressed or implied, of the U.S. Government.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">The Digitization of the World -From Edge to Core</title>
		<author>
			<persName><forename type="first">D</forename><surname>Reinsel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Gantz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Rydning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ser. An IDC White Paper -#US44413318</title>
				<imprint>
			<date type="published" when="2018-11">Nov. 2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Techniques for graph analytics on big data</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">U</forename><surname>Nisar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Fard</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">A</forename><surname>Miller</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2013 IEEE International Congress on Big Data</title>
				<imprint>
			<date type="published" when="2013-06">June 2013</date>
			<biblScope unit="page" from="255" to="262" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">The PageRank citation ranking: Bringing order to the web</title>
		<author>
			<persName><forename type="first">L</forename><surname>Page</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Brin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Motwani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Winograd</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1999">1999</date>
			<publisher>Stanford InfoLab, Tech. Rep</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">Hierarchical identify verify exploit (HIVE)</title>
		<author>
			<persName><forename type="first">W</forename><surname>Shen</surname></persName>
		</author>
		<ptr target="https://www.darpa.mil/program/hierarchical-identify-verify-exploit" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Simd-x: Programming and processing of graph algorithms on gpus</title>
		<author>
			<persName><forename type="first">H</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">H</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2019 USENIX Annual Technical Conference (USENIX ATC 19)</title>
				<imprint>
			<date type="published" when="2019-07">Jul. 2019</date>
			<biblScope unit="page" from="411" to="428" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Multi-processor performance on the Tera MTA</title>
		<author>
			<persName><forename type="first">A</forename><surname>Snavely</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Carter</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Boisseau</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Majumdar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">S</forename><surname>Gatlin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Mitchell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Feo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Koblenz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SC&apos;98: Proceedings of the 1998 ACM/IEEE Conference on Supercomputing</title>
				<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="1998">1998</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">HyperX: topology, routing, and packaging of efficient large-scale networks</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">H</forename><surname>Ahn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Binkert</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Davis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Mclaren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">S</forename><surname>Schreiber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Conference on High Performance Computing Networking, Storage and Analysis</title>
				<meeting>the Conference on High Performance Computing Networking, Storage and Analysis</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2009">2009</date>
			<biblScope unit="page">41</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Overview of the next generation Cray XMT</title>
		<author>
			<persName><forename type="first">A</forename><surname>Kopser</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Vollrath</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Cray User Group Proceedings</title>
				<imprint>
			<date type="published" when="2011">2011</date>
			<biblScope unit="page" from="1" to="10" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Highly scalable near memory processing with migrating threads on the Emu system architecture</title>
		<author>
			<persName><forename type="first">T</forename><surname>Dysart</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Kogge</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Deneroff</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Bovell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Briggs</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Brockman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Jacobsen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Juan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Kuntz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Lethin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Mcmahon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Pawar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Perrigo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Rucker</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Ruttenberg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Ruttenberg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Stein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Sixth Workshop on Irregular Applications: Architectures and Algorithms</title>
				<meeting>the Sixth Workshop on Irregular Applications: Architectures and Algorithms</meeting>
		<imprint>
			<biblScope unit="page" from="2" to="9" />
		</imprint>
	</monogr>
	<note>ser. IA 3 &apos;16, 2016</note>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">A microbenchmark characterization of the Emu chick</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">S</forename><surname>Young</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Hein</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Eswar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Lavin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Riedy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Vuduc</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Conte</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Parallel Computing</title>
		<imprint>
			<biblScope unit="volume">87</biblScope>
			<biblScope unit="page" from="60" to="69" />
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Novel graph processor architecture, prototype system, and results</title>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">S</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Gleyzer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Lomakin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Kepner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2016 IEEE High Performance Extreme Computing Conference</title>
				<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="1" to="7" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Graphicionado: A high-performance and energy-efficient accelerator for graph analytics</title>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">J</forename><surname>Ham</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Sundaram</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Satish</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Martonosi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2016 49th Annual IEEE/ACM International Symposium on Microarchitecture</title>
				<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="1" to="13" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Scalability of hybrid sparse matrix dense vector (SpMV) multiplication</title>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">A</forename><surname>Page</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">M</forename><surname>Kogge</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2018 International Conference on High Performance Computing Simulation (HPCS)</title>
				<imprint>
			<date type="published" when="2018-07">July 2018</date>
			<biblScope unit="page" from="406" to="414" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
