<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Hybrid Methods Using Genetic Algorithms for Global Optimization</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Jean-Michel</forename><surname>Renders</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Stcphane</forename><forename type="middle">P</forename><surname>Flasse</surname></persName>
						</author>
						<title level="a" type="main">Hybrid Methods Using Genetic Algorithms for Global Optimization</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">ABCB1357F9B49BC23D4DFAB4EFC4F8A4</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.3" ident="GROBID" when="2023-07-28T16:11+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>This paper discusses the trade-off between accuracy, reliability and computing time in global optimization. Particular compromises provided by traditional methods (Quasi-Newton and Nelder-Mead's Simplex methods) and Genetic Algorithms are addressed and illustrated by a particular application in the field of nonlinear system identification. Subsequently, new hybrid methods are designed, combining principles from Genetic Algorithms and "hill-climbing" methods in order to find a better compromise to the trade-off. Inspired by biology and especially by the manner in which living beings adapt themselves to their environment, these hybrid methods involve two interwoven levels of optimization, namely Evolution (Genetic Algorithms) and Individual Learning (Quasi-Newton), which cooperate in a global process of optimization. One of these hybrid methods appears to join the group of state-of-the-art global optimization methods: it combines the reliability properties of the Genetic Algorithms with the accuracy of Quasi-Newton method, while requiring a computation time only slightly higher than the latter. * Decrease rate of mutation: 5. Probability of cross-over (1 point cross-over): 0.8 (per individual). Type of cross-over: a) continuous crossover: 1270, b) discrete crossover: 88%.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>I. INTRODUCTION ENETIC Algorithms have recently emerged as an in-</head><p>G creasingly popular family of methods for global optimization [l], [ 2 ] . These methods perform a search by evolving a population of candidate solutions through the use of nondeterministic operators and by improving incrementally the individuals forming the population by mechanisms inspired from those of genetics (e.g., crossover and mutation). They are known to offer significant advantages over traditional methods by using simultaneously several search principles and heuristics whose most important ones are: a population-wide search, a continuous balance between exploitation (convergence) and exploration (maintained diversity) and the principle of building-block combination. In certain cases, particularly when facing complex optimization problems with numerous local optima, where traditional optimization methods fail to provide efficiently reliable results, Genetic Algorithms can constitute an interesting alternative. Nevertheless, Genetic Algorithms can suffer from excessively slow convergence before providing an accurate solution because of their fundamental requirement of using minimal a priori knowledge and not exploiting local information. This kind of "blindness" may prevent them from being really of practical interest for a lot of applications.</p><p>On the other hand, classical "hill-climbing'' methods, such as the Quasi-Newton method, are well known to exploit all local information in an efficient way, provided that certain conditions are fulfilled and, in particular, that the function to be minimized is "well-conditioned" in the neighborhood of the unique optimum <ref type="bibr" target="#b2">[3]</ref>. Such a high level of exploitation requires a lot of local information to be known (gradient and, sometimes, Hessian matrix): the more intensive the exploitation, the stronger the need of specialized information about the function to be minimized. Moreover, if the basic requirements are not satisfied, the reliability of the "hill-climbing'' method is greatly jeopardized.</p><p>At this stage of consideration, it is useful to keep in mind the fundamental conflict between accuracy, reliability and computation time when searching for the global optimum of complex problems, especially for problems with many local optima: it is generally impossible to reach accurately and reliably the global optimum in a short computation time. This conflict is closely related to the "exploitation-exploration'' trade-off. Each optimization method represents a particular compromise or, in other words, a particular way to manage the fundamental conflict. As far as Genetic Algorithms are concerned, despite incontestable advantages and original principles that they implement (especially, the ability to "rough out" a problem reliably by finding the most promising regions of the entire search space), they often represent an unsatisfactory compromise, because they suffer from a certain inefficiency, characterized by a slow convergence and a lack of accuracy when an exact solution is required. Complementary, "hill-climbing'' methods appear to realize another "extreme compromise" to solve the conflict: they focus solely on accuracy and computation time (exploitation) to the detriment of reliability. The aim of this work is to investigate how to overcome the limitations inherent to both classes of methods and to design hybrid methods bringing together the benefits of the single ones: the hybrid methods we will propose combine the reliability properties of the Genetic Algorithms and their original search heuristics with the accuracy of "specialized" hill-climbing methods, while requiring a computation time only slightly higher than the latter (particularly when parallel computers are used).</p><p>This paper discusses the trade-off between accuracy, reliability and computing time, examines the particular compromises provided by traditional methods (Quasi-Newton [4] and Nelder-Mead' s Simplex [5] methods) and Genetic Algorithms, and offers new hybrid methods combining principles from Genetic Algorithms and "hill-climbing" methods in order to 1083-4419/96$05.00 0 1996 IEEE find a better compromise to the trade-off. Inspired by biology and especially by the manner in which living beings adapt themselves to their environment, these hybrid methods involve two interwoven levels of optimization, namely Evolution (Genetic Algorithms) and Individual Learning (Quasi-Newton), which cooperate in a global process of optimization.</p><p>To illustrate these theoretical considerations, a particular application is chosen in the field of nonlinear system identification. System identification with Genetic Algorithms alone has already been proposed by <ref type="bibr" target="#b5">[6]</ref>, focusing on linear dynamic systems in view of their control. Genetic Algorithms have also been developed for structural identification by <ref type="bibr" target="#b6">[7]</ref>, in combination with specific parameter estimation methods. The system considered in this paper is the Earth's surface as seen by satellite; the identification problem consists of retrieving surface properties from satellite measurements [XI, <ref type="bibr" target="#b8">[9]</ref>. Such a problem, which can be seen as an "inversion problem," usually requires the minimization of functions with complex landscapes. This system is flexible enough to exhibit a large class of behaviors (local minima, nearly-flat valley, . . .) by modifying several parameters (number of measurement data, physical parameters of the system, . . .), while being sufficiently representative of a large class of problems in nonlinear identification. In the first part of the paper, we apply traditional methods and Genetic Algorithms to this problem and we compare the methods in terms of accuracy, reliability and computation time. In the second part, we design hybrid methods to offer an optimal compromise between these three criteria in solving the identification problem. Finally, one of the new hybrid methods is compared to the "Multi-Level Single Linkage" method <ref type="bibr">[lo]</ref>, considered as a powerful approach for global optimization, and whose principles have some similarity with those of our hybridized "biologicallyinspired" methods.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="11.">TRADITIONAL OPTIMIZATION METHODS AND GENETIC ALGORITHMS</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A, Description of the Optimization Methods</head><p>Minima of a function f can be formally defined as follows: Let M be the set of feasible points (i.e., possible solutions which satisfy the constraints).</p><formula xml:id="formula_0">f ( z * ) is a local minimum $ [3â‚¬ &gt; 01vz E M : llz -z*ll &lt; E + f ( x ) 2 f ( z * ) ] and z* E M (1) f ( z * ) is a global minimum $ V z E M : f ( x ) 2 f ( z * ) and x* E A4 (2)</formula><p>in other words, the global minimum is the smallest of the local minima. Henceforth, we shall use the term "local minimum" only to mean a local minimum which is not a global minimum.</p><p>At least three main strategies of search can be distinguished, according to their ways of exploring and exploiting the function to be optimized. The first one (e.g., Quasi-Newton) explores the search space using a siingle point and exploits all the local information (in particular the gradient) to find a better next point. The second (e.g., Simplex) uses a family of points to explore the search space and exploits the relative order of the various candidate solutions to drive the future search in a better direction. Both strategies are usually known as "hillclimbing," because they use only local information to find a better solution. When they converge to a stationary point, there is no guarantee that this is in fact the global optimum. By contrast, the third strategy (e.g., Genetic Algorithms) uses a population of candidate solutions initially distributed over the whole function space and quickly identifies the subdomain in which the global minimum function is located, while maintaining constant exploration of the search space. In accordance with the convention in the literature of Genetic Algorithms, this ability is called the "explorative" property, while the ability to exploit all the local information for refining progressively and efficiently the solution is called the "exploitative" property. Based on the characteristics of each method, our prior expectation would be that the "hillclimbing" methods would show good exploitative properties, and the Genetic Algorithms good explorative properties.</p><p>1 ) Quasi-Newton (QN): The Quasi-Newton method, also called the variable metric method, is based on Newton's method, which consists, at each iteration, of the minimization of a quadratic approximation to the function. This requires that the function be twice differentiable, and that both gradient vector and Hessian matrix can be calculated at all points. The Quasi-Newton does not require the Hessian matrix to be calculated, but builds it up iteratively; it therefore only requires the first derivatives of the function [in fact, the routine adopted in this study (routine E04JAF from the NAG library [ 111) approximates the first derivatives with finite differences]. The Quasi-Newton rnethod here used carries out line minimizations, their results being used to update the current estimate of the Hessian matrix and to find further promising directions of search <ref type="bibr" target="#b3">[4]</ref>.</p><p>The termination criteria are based on the following principles. The search is halted when the relative change occurring between two successive iterations is less than some prescribed quantity for the sequence of candidate solutions and function values, or when a maximum number of function evaluations is reached.</p><p>2) Simplex or Flexible Polyhedron (S): The Simplex method [5] is a robust nonlinear multi-dimensional optimization technique. The method does not require the derivatives of the function to be optimized. A simplex is a geometrical figure consisting, in N dimensions, of ( N + 1) vertices.</p><p>The Simplex method starts, not with a single point, but with an initial simplex ( N + 1 points); then, through a sequence of elementary geometric transformations (reflection, contraction and extension), the initial simplex moves, expands and contracts, in such a way that it adapts itself to the function landscape and finally surrounds the optimum. For determining the appropriate transformation, the method uses only the relative order between the performances (values of the function to be optimized) of the point considered. After each transformation, the current worst point is replaced by a better one; in this way, providing that certain precautions are taken, the algorithm always forces convergence of the sequence of iterates.</p><p>The termination criterion generally depends on the magnitude of the difference between the performances of the best point and the worst one: as soon as this difference is less than a pre-defined threshold, the search is halted. When the termination criterion is reached, it is customary to restart the entire procedure with a new simplex "exploded" (the point claimed to be the optimum is kept as one of the vertices of the new simplex, while the others are chosen in arbitrary orthogonal directions). Such explosions are carried out until the simplex repeatedly collapses onto the same solution. This iterative procedure allows certain anomalous terminations to be avoided and can increase the reliability of finding the global optimum.</p><p>3) Genetic Algorithms (GA): The Genetic Algorithms method [l] is an iterative search algorithm based on an analogy with the process of natural selection (Darwinism) and evolutionary genetics. The search aims to optimize a user-defined function (the function to be optimized) called the fitness function. To perform this task, GA maintains a "population" of candidate points, called "individuals," over the entire search space. At each iteration, called a "generation," a new population is created. This new generation generally consists of individuals which fit better than the previous ones into the external environment as represented by the fitness function. As the population iterates through successive generations, the individuals will in general tend toward the optimum of the fitness function. To generate a new population on the basis of a previous one, GA performs three steps: a) it evaluates the fitness score of each individual of the old population, b) it selects individuals on the basis of their fitness score, and c) it recombines these selected individuals using "genetic operators" such as mutation and crossover, which, from an algorithmic point of view, can be considered respectively as means to change locally the current solutions and to combine them.</p><p>What makes GA attractive is its ability to accumulate information about an initially unknown search space and to exploit this knowledge to guide subsequent search into useful sub-spaces. The fundamental implicit mechanism underlying this search consists of the combination of high-performance "building blocks" discovered during past trials.</p><p>Three important features distinguish the GA approach: a)</p><p>GA works in parallel on a number of search points (potential solutions) and not on a unique solution, which means that the search method is not local in scope but rather global over the search space; b) GA requires from the environment only an objective function measuring the fitness score of each individual and no other information nor assumptions such as derivatives and differentiability; and c) both selection and recombination steps are performed by using probability rules rather than deterministic ones; this aims to maintain the global explorative properties of the search.</p><p>In classical GA, a fourth difference can be distinguished: GA encodes the parameters to be optimized and operates on the codes and not on the parameters themselves (cf., the genotype/phenotype distinction of genetics). The aim of coding the parameters is to transform the original optimization problem into a combinatorial one, because GA is essentially a mechanism for combinatorial search. For example, in the case of continuous numerical optimization problems, the coding of the parameter value is usually chosen as a binary coding into a chain of limited length. In this way, continuous problems are handled as discrete ones. Coding the parameters implies the design of decoders and repair algorithms, in order to ensure that the solution found by GA can be translated into a feasible solution of the original problem. In several cases, this design is not an easy task, as described in <ref type="bibr">[IZ]</ref>.</p><p>Recently, a modified version of the classical GA ("discrete GA") has been developed [2], <ref type="bibr" target="#b11">[13]</ref> to meet the needs of industrial GA practitioners in solving real-world problems; this new version is called "real-coded GA" or "continuous GA" and is characterized by a direct floating-point representation of the parameters to be optimized, whereas a binary representation is usually adopted with classical GA, resulting in a loss of precision. This real-coded GA generally offers the advantages of being better adapted to numerical optimization for continuous problems, of speeding up the search and of making easier the development of approaches "hybridized" with other methods; but it requires the development of new "genetics-inspired" operators, and to date it lacks strong mathematical foundations, currently established only for discrete GA (the "Schema theorem" [ 11, deception problem related to Walsh analysis <ref type="bibr" target="#b12">[14]</ref>, and convergence theorems <ref type="bibr" target="#b13">[15]</ref>). In our particular case study, we have adopted a real-coded CA. Appendix B provides a comparison between the two kinds of GA, describes the genetic operators used for real-coded GA (mutation and crossover), and shows that the continuous version is more convenient and more efficient for solving the inversion problem.</p><p>Several termination criteria for the search have been proposed. One simple criterion is to stop the procedure when almost all individuals are identical or nearly so; another possible criterion is to test the improvement in the best fitness score over successive generations. However, the first criterion can lead to excessive search time (too many generations before halting), while the second one is not satisfactory for functions characterized by a landscape presenting "plateau-type'' regions. In this work, we decided simply to stop the search after a fixed number of generations; this number was experimentally chosen as representing a reasonable compromise between the constraints of population convergence, computing time and accuracy.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Case Study Description</head><p>In order to test the applicability and the accuracy of the different methods, a particular application is chosen in the field of nonlinear system identification.</p><p>The system envisaged is the Earth's surface as seen by satellite: satellite sensors measure the surface reflectance. This reflectance is directly related to surface properties that influence the scattering of the light, and to the geometry of illumination and observation (since the Earth's surface is anisotropic). In order to describe the surface by parameters that correspond to the physics used in the surface description of climate models, this study case uses the physically-based model of <ref type="bibr" target="#b7">[8]</ref>, which can be formally represented as follows:</p><p>where p represents the modeled measurement (reflectance), CY the characteristics of the measurements, and x* the properties of the surface (independent variables of the model)-bold letters represent set of parameters (vectors). In the model used in this study (see description in Appendix A), x* E R4 and c\c E R3.</p><p>The interpretation of satellite data then consists in the retrieval of intrinsic parameters of the system (x*) from a set of measurements and a pre-determined structure of the model. This implies the "inversion" of (3) against the measurements (problem of "model inversion" or "parameter identification"). Because of the complexity of the model, no analytical solution is generally known for the inversion of models, and in particular for the inversion of the model adopted in this study. Therefore, some form of numerical search technique is needed. When transforming the inversion problem in an optimization problem, the choice of the criterion to be optimized depends on the characteristics of the measurement noise, as well as the modeling errors. As it may be difficult to know accurately these characteristics and, in particular, the probability distributions of these errors, choosing the correct criterion can be a delicate problem in practice. For the sake of simplicity, we suppose, in this work, that real measurements and output of the model are related by where p k corresponds to the measurement k , p to the modeled value of that measurement, using its characteristics f f k and the true surface properties x*; Ek is a Gaussian white noise with zero mean and constant variance, uncorrelated with the other E , and representing measurements and modeling errors. The criterion to be minimized is then (least-square criterion)</p><formula xml:id="formula_1">n S2(x) = [ P k -P ( W ; X)l2 (5) k=l</formula><p>where n is the number of measurements; a z , k the characteristics of the kth measurement (IC = I, ... , n); pk is the measurement k in itself and p is the modeled measurement calculated according to (3) using estimates x of the surface parameters. The domain of this function is bounded and included in R4. Several inversion experiments were performed using measurements generated by (3) over three different surfaces (A, B, C). The results of the inversions will be compared to the values of the parameters representing the surfaces used to generate these measurements. These sets of parameters were chosen after study of the partial derivatives of the bidirectional function, to represent between them most of the significant features of the entire function. Of course the properties of a four-dimension domain cannot be fully expressed with 3 sets of surfaces; however, the conclusions drawn from these three surfaces are believed to hold for a wide range of surfaces.</p><p>For each surface, three types (1, '2, 3) of data sets were built, with n = 4, 16, and 64 measurement values respectively, in order to test the sensitivity of the inversion to the number of measurements. The angles were chosen in order to insure the observability of the parameters.</p><p>The number N of parameters to retrieve being 4, the minimum number of measurements required is also 4. When only 4 measurement values are available, the problem is not, strictly speaking, an optimization problem; it is rather the search for the solution of 4 nonlinear simultaneous equations in 4 unknowns, treated as an optimization problem.</p><p>These experiments were performed 50 times, with "initial guess" values taken randomly in the whole domain of the function.</p><p>The values of the parameters representing the surfaces, the angles used to create the data sets and the technical characteristics of each method are (described in Appendix C.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Results and Discussions</head><p>1) Description of the Function 6': It is interesting to try to represent the different shapes of the function to be minimized and to visualize its global and local minima. Since S2 is a 4dimensional function, it is not possible to have a global view of the function in a single graph. VVe attempt to represent the most interesting facts within 2-dimensional figures, keeping in mind that two of the four parameters are fixed and that the full 4-dimensional function could exhibit unexpected features, difficult to visualize.</p><p>A careful analysis of the inversion results and of the graphs representing the function S2 for two parameters-the others being fixed-leads to the observation of three phenomena: 1) The global minimum is always located in a kind of quasilinear gutter or valley, nearly parallel to 'one of the parameter axes. This kind of gutter presents a bottom nearly "flat" in the axial direction. If no special precautions are taken, the flatter the gutter, the more significant the effects of roundoff and truncation errors. that a small variation in 2 4 induces a large variation in Z. This "cross-sensitivity'' occurs for the other parameters as well: Fig. <ref type="figure">3</ref>, for example, shows that the function S2, for data set C1 and with 2 1 and 2 2 fixed at a value near (but not exactly on) the optimal value, presents a gutter [Fig. <ref type="figure">3</ref> Since the function S2 appears to be complex and "illbehaved," the optimal solution will not be easy to find and, with standard termination criteria, optimization procedures can result in solutions either stuck far from the real solution for one or two parameters [phenomena (1) and ( <ref type="formula">2</ref>)], or located a long way from it [phenomenon <ref type="bibr" target="#b2">(3)</ref>]. Therefore, it is necessary to use a strong and powerful method in order to ensure the correct retrieval of the parameters.</p><p>We observed that the number of data values influences very much the shape of the function 6'. A larger number of values always increases the gradient in all directions around the global minimum and decreases the "cross-sensitivity" in its neighborhood; therefore the effects of phenomena (1) and ( <ref type="formula">2</ref>) are less significant. With a higher number of values, the function J2 "curls up," placing the minimum in a kind of "bowl" rather than in a gutter, as seen in Fig. <ref type="figure" target="#fig_3">4</ref> for data sets A. As far as phenomenon ( <ref type="formula">3</ref>) is concerned, we observed that with a larger number of values local minima present less of a problem. Different types of surface generally produce a function S2 of the same global shape. However, they may display different sensitivity to the above phenomena. For example, the gutter, as seen in case A l , becomes larger and flatter in width in case B 1, while case C1 shows more "cross-sensitivity.''</p><formula xml:id="formula_2">IEEE TRANSACTIONS ON SYSTEMS, M o x l ~-4 / x , = 0 2 , x. = 0 0 ---- (a) 0.004 r -- x, = 0.2, x1 = 0.0 -=.---<label>(b)</label></formula><p>2) Comparison Criteria: We compare the methods under four different headings: convergence, reliability, accuracy, and computation time.</p><p>Convergence: Convergence is defined in theory as the existence of a finite limit for the sequence of solutions (or iterates). In practice, this is assessed by observing the magnitude of the change over the last few terms of the sequence. This does not guarantee convergence to a stationary point. The problem of insuring real convergence on an optimum before stopping a numerical search is very tricky, particularly in the problem discussed here. The criterion of stopping the search when the difference between successive values of the function is sufficiently small is not always satisfactory, because the search halts prematurely for "gently-sloping" functions, such as the function S2 [phenomenon (l)]. On the other hand, the alternative criterion proposed-stopping the search when the distance moved in the parameter space during the last few iterations is less than the location accuracy required-can give rise to similar difficulties <ref type="bibr" target="#b2">[3]</ref>. We mentioned above the usual termination criteria associated with each of the methods (QN, S, and GA). In this study, we decided to adopt these usual criteria without modification or special precaution.</p><p>Reliability: The reliability of a method is its ability to find the global minimum and therefore to avoid local minima. In our study, only the solutions found outside the gutter including the global minimum [phenomenon (3)] were considered as failure for the method. This is easily assessed using the error on the first two parameters, since these errors are very small when the solution is in the gutter, compared to those for solutions outside the gutter. The reliability will be characterized by the percentage of searches successful out of the 50 experiments.</p><p>Accuracy: The accuracy corresponds to the distance from the solution to the global minimum, for cases not considered as failure. In particular, this accuracy involves the effects of phenomena (1) and <ref type="bibr" target="#b1">(2)</ref>. It is assessed by the error defined as: where x: and x,, i = 1, ... , 4 are the values of the four parameters for the real minimum and the inversion solution respectively, wz is the weight given for each parameter, corresponding to the range of the domain of each parameter (w, = 1, 2, 1, 10 for parameters 2 1 , 2 2 , 2 3 , 2 4 , respectively), in such a way that an identical relative error on each parameter gives an identical absolute contribution to the global ERROR. The accuracy of the method will be characterized by the mean of the error over the successful experiments.</p><p>Computation Time: Evaluating the function to be minimized constitutes the greatest part of the computation time. Therefore, we assess the computation time by the number of calls of the function p (3). Since we perform the search over several experiments which differ only in their "initial guess," we consider the mean number of calls over the number of successful experiments. Moreover, in order to make the results comparable between the different data sets (n = 4, 16, and 64), we have normalized the mean computation time by the number of measurements. In this way, the normalized computation time corresponds to the average number of calls of the function 6'.</p><p>3) Methods Comparison: We discuss here only the experiments with "clean data sets" and initial guesses taken at random over the whole domain of the function.</p><p>Convergence: Although it is clear that convergence is not a very good indicator for comparing methods, it is important to ensure that the methods converge. In the implementation adopted, GA, S, and QN guarantee that the value of the function 62 will not increase because, in the iterations, the new "best point" is always chosen to be strictly better than or equal to the previous one. As a consequence the methods necessarily converge. However, premature termination is not necessarily avoided.</p><p>The usual termination criteria turn out to be unsatisfactory for the inversion with small data sets (4 values), i.e., when phenomena (1) and ( <ref type="formula">2</ref>) are the most significant. For these cases, premature termination in the gutter brings poor accuracy. Refinements in the criteria, for example by re-scaling the function on the gutter region ("zoom"), should be studied in order to ensure a satisfactory behavior of the search for the problematic cases. This point is currently under investigation.</p><p>Reliability: Fig. <ref type="figure" target="#fig_4">5</ref>(a) shows that, as expected, GA offers a very good exploration of the search space and is 100% secure in finding the global minimum. The "hill-climbers'' (QN and S) have more difficulties in the search for the global minimum. For example, the reliability for data sets 1 varies between 45% and 95%. However, the larger the number of data values, the more reliable the result. Moreover, it seems that S is more reliable than QN for a small number of data values (4 values), whereas QN is better than S with a larger number <ref type="bibr">(16 and 64)</ref>.</p><p>Accuracy: Fig. <ref type="figure" target="#fig_4">5</ref>(b) shows that, whatever the method, the more data values available the higher the accuracy. The error never reaches a value better than 0.12 with data sets 1 but can reach an error of lop6 with data sets 3. For a small number of data values, no method shows systematic advantages over the others. However, when the number of data values increases, GA never reaches a high level of accuracy, whereas QN and S do.</p><p>We observed that most of the error is carried by the parameter z3. This is the gutter phenomenon. When the number of data values increases, the error on x 3 diminishes and the parameter z 4 becomes the least well defined. This behavior is equally visible in all methods, the error on a single parameter being inherent in the function landscape [cf., phenomenon (1) in 5.11.</p><p>Computation Time: QN is generally at least twice as fast as S, while GA takes far more computation time [Fig. <ref type="figure" target="#fig_4">5(c)</ref>]; as discussed above, the computation time allowed for CA was chosen as a reasonable compromise between the constraints of population convergence, computing time and accuracy. The differences between the computation time of the different methods are basically related to the use of the information acquired during the search. QN uses quantitative information about the neighborhood of the current (single) point, particularly by computing the derivatives, whereas S only uses the order relations between the points of the current simplex. GA uses also the order relations between the points of individuals of the current population; however, its search is performed partially randomly and may include bad intermediate solutions that a "hill-climbing" method would not have explored.</p><p>The convergence of GA occurs during the first hundred generations. Afterwards, the S2 value (fitness) of the solution continues to decrease slightly, but this does not necessarily imply that the accuracy of the solution improves; this is related to the shape of the function landscape. Conclusion: CA requires a lot of computation time and yields poor accuracy, but seems to be absolutely reliable. As expected, CA turns out to be very good for exploration. QN on the other hand has the advantage of being very accurate provided it is not stuck in a trap, and this accuracy is reached in a relatively short time. QN shows good exploitative properties. S falls in between GA and QN, but its performances remain nearer those of QN because both use the "hill-climbing'' principle. All in all, it turns out that the results of the optimization procedures agree with our prior expectations, taking into account the function landscape.</p><p>Finally, tests have shown that adding noise to the data only moves the global minimum from the expected one, and does not interfere with the search for a minimum in itself. Since measurements are contaminated by noise due to the physical characteristics of instruments and to the conditions of Initial guess f Solution Fig. <ref type="figure">6</ref>. Schematic representation of hybrid methods principles observation, further investigation should address the question of what is the maximum amount of noise that can be tolerated in the data before the global minimum becomes unacceptably different from the expected one, rendering the inversion procedure meaningless.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="111.">HYBRID METHODS</head><p>The idea is to combine two of the previous methods in order to merge their advantages and reduce their disadvantages. Similar ideas have already been introduced in neural network learning [ 161 and in engineering design optimization; for this last application, Powell has developed an "interdigitized" technique combining Expert Systems, numerical optimization and Genetic Algorithms to provide an answer to engineering design problems [ 171.</p><p>We investigated a coupling of GA and QN, using both the exploration capabilities, parallelism and combination properties of GA, and the exploitation power of QN. For this purpose, we "interweaved" the two methods, relating the use of CA to the concept of "evolution" of a population of individuals and that of QN to the concept of "life" for each individual. In general, GA takes a population and makes it evolve in such a way that most of the population reaches the global minimum. With pure GA, the transition from one generation to another is based on the selection of individuals evaluated at their "birth" by instantaneous fitness. With hybrid methods, the principle is to let GA make its selections based on the fitness at the end of the individual life, the life being performed by QN (Fig. <ref type="figure">6</ref>). Other couplings would have been possible (e.g., t t t = t , /1 A Fig. <ref type="figure">7</ref>. Typical evolution to the population on the fitness landscape for the Darwin-inspired combination of real-coded Genetic Algorithms and Quasi-Newton (GQD). Symbol "X' indicates the nondeveloped individual (at birth), while the winding array represents the life of this individual (optimization with QN). Individuals selected for reproduction are marked by a dashed circle.</p><p>GA and S), but we chose to restrict our study to the most promising coupling, i.e., the one consisting of 2 methods with very different principles and complementary properties.</p><p>We tried two hybrid methods inspired by the evolutionary theories [ 181 of Darwin and Lamarck, respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Darwin-Inspired Combination of Real-Coded Genetic Algorithms and Quasi-Newton (GQD)</head><p>Genetics-Inspired Description (Fig. <ref type="figure">7</ref>): The natural selection of individuals depends on their fitness after living, maturing and adapting to their environment. The genetic material used during reproduction is the genetic code of the individual which is inborn and never changes during its life. The population gradually comes to be composed of genetic material producing optimal adult individuals.</p><p>Algorithmic Description: Iteratively, GA gradually produces better initial guesses for QN algorithms (these could work in parallel), in the sense that the QN starting point comes to be one which, being located squarely within the basin of attraction of the global minimum, will easily and reliably evolve to this optimum. When evaluating the fitness of each individual, GA uses the results of QN methods working with an initial guess corresponding to this individual. During reproduction and genetic transformations (cross-over, mutation) for the production of the individuals of the next generation, GA works on the initial guesses previously given to QN and not on the solutions provided by it. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Lamarck-lnspired Combination of Real-Coded Genetic Algorithms and Quasi-Newton (GQL)</head><p>Genetics-Inspired Description (Fig. <ref type="figure" target="#fig_5">8</ref>): Here again, the natural selection of individuals depends on their fitness after living, maturing and adapting in their environment. But in this case, the features acquired by the organism during its life can be hereditary transmitted by sexual reproduction (Lamarckism). This is as if the genetic code of the individual changed during its life, with the genetic material used in reproduction being the modified genetic code of the mature organism.</p><p>Algorithmic Description: Iteratively, GA gradually produces better solutions using QN as an "accelerator" mechanism thanks to its exploitative properties. When evaluating the fitness of each individual, GA uses the results of QN methods working with an initial guess corresponding to this individual. During reproduction and genetic transformation (cross-over, mutation) for the production of the individuals of the next generation, GA works on the new solution (termination point) found by QN (in contrast to the GQD method, in which GA works on the initial points provided to QN).</p><p>Recently, other researchers have investigated the benefits of adding "Lamarckian" principles to GA for solving graph partitioning problems [ 191 and competitive, multi-agent control problems <ref type="bibr" target="#b19">[20]</ref>. They have found that, in some cases, it can result in a significant enhancement of performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Termination of QN "Life"</head><p>For both GQD and GQL methods, when evaluating the individuals with QN, it is not necessary to reach one of the usual termination criteria of the method: individual optimization (life) can be performed over a limited number of steps, because the main part of the information given by the search with QN is acquired during the first few steps and because the search is pursued and refined over the next generations anyway, especially for the GQL method. GQD, however, needs a longer "QN-life" time than GQL because, at each generation, the information acquired during the individual's life is not directly used when generating new individuals (lower level of exploitation); in other words, GQD will require a more accurate fitness evaluation and consequently a longer "QNlife" time so that the selection mechanism remains meaningful.</p><p>In practice, the combined methods terminated with an "extended-life,'' in which the best individual of the last GA generation was exploited by QN using the normal termination criteria in order to give the best accuracy for the final solution. We found experimentally that limiting the maximum number of function evaluations of the QN procedure to 20 for GQL and 115 for GQD (instead of 1615 under the normal criterion) gave good results for the function S2.'</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D. Comparison of Interweaving Methods with Darwin and Lamarck Principles</head><p>Technical details concerning the implementation of both methods are given in Appendix C. The interweaving methods always converge because they use a combination of methods that converge separately. The reliability is generally excellent for both methods, even if GQL presents a 6% failure rate in the most difficult case [Fig. <ref type="figure" target="#fig_7">9(a)</ref>]. The accuracy of both methods is of the same order of magnitude, linked to the QN accuracy [Fig. <ref type="figure" target="#fig_7">9(b)</ref>]. As expected because of the construction of the methods, GQL works faster than GQD by a factor of 6 [Fig. <ref type="figure" target="#fig_7">9(c)</ref>].</p><p>In the GQD method, a shorter QN-life would not preserve the reliability, even though the computing time would be globally reduced. This can be explained by the fact that the evaluation of an individual with a shorter QN-life is no longer completely meaningful. Indeed, if a too short life time is adopted, individuals which offer a faster decrease of the S2 function during their life are favored for further selection, regardless of the minimum value that they would have reached if a complete local convergence was allowed. As GA works on the initial points provided to QN, and not on the termination point, no further refinement or correction of the information used for the individual evaluation can be expected and successive selection are more likely to produce results corresponding to some nonglobal minimum. In the GQL method, a longer QN-life would of course improve the quality of the function evaluation and therefore increase the reliability, but to the detriment of the computation time. We believe that the current choice of GA and QN parameters in the GQD ana GQL methods, for these case studies, seems to result in the more satisfactory compromise.</p><p>All in all, GQD and GQL give nearly the same results, the only significant difference being the computation time. The principal advantage of GQL is that the convergence can be made very fast, because each generation considers individuals which have already evolved toward the solution.</p><p>' These numbers include an initialization cost of 15 function evaluations involved in starting up the QN method with a finite difference approximation to the initial Hessian. Since a QN-step requires a minimum of N + 1 function evaluations, the 20 function evaluations of GQL correspond to the initialization stage and one QN-step, while the 115 function evaluations of GQD correspond to the initialization stage and at most 20 QN-steps. </p><formula xml:id="formula_3">A i 81 C i A 2 82 C2 A 3 83 Data Sets (b) QN ~G A I I G Q D 1 G Q L I Q N - -G A I G Q D G Q L i L c3</formula><p>-e r n 6 2 2 S : Z I : For the function S2, it was found possible to combine this rapid convergence (strong exploitative mechanisms) with a high reliability; such a result might not hold for a different function landscape, e.g., one with many local minima. As noted above GQD needed much longer "QN lives" for its individuals than GQL. It may be that for certain function landscapes, the slower convergence of GQD (considering the total number of function evaluations) would be accompanied by greater reliability. Nevertheless, GQL seems to be quite adequate for the function 6'.</p><formula xml:id="formula_4">Data Sets (c)</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E. Conzparison of Interweaving Methods with Non Hybrid Methods</head><p>The comparison of the interweaving methods with nonhybrid methods shows the power of the former. They offer a success rate very close to loo%, and always greater than or equal to that of other methods. The accuracy of the results is generally very similar to that of QN. In terms of computation time, interweaving methods are faster than GA; they are slower than QN and S, but offer the security of finding the absolute minimum. GQL offers such a security using only 4 times more computation time than the "hill-climbing'' methods. In most cases, these levels of security-and accuracy cannot be attained simultaneously by taking the best results of 4 parallel "hillclimbing" methods. It must be taken into account that with the use of massively-parallel computers for this application, the computation time could easily be reduced by a factor corresponding to the population size of GA (15 in our case); in these conditions, GQL could be nearly as fast as QN.</p><p>For the completeness of the comparison, we investigated also a direct combination between GA and QN: in the first stage, GA would explore the search space in order to discover useful subspaces and to provide a solution, not located precisely at the global optimum, but at least within its basin of attraction; in the second stage, QN would use the candidate solution provided by GA as an initial guess, and pursue the search in its own exploitative way. Such a combination was demonstrated to have some merit, in that a finer initial guess provided to QN improves both the reliability and the accuracy of the method. However, in order to guarantee that GA will provide a well-located starting point, a substantial number of generations (comparable with that of GA alone) is necessary and, therefore, a substantial computation time is required.</p><p>It can be concluded that the direct combination method is feasible, but does not compete with the GQL method for this last reason.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>F. Comparison of GQL with a Multi-Level Approach for Global Optimization</head><p>Finally, we compared our best interweaving method (GQL) with the "Multi-level single linkalge" method (MLSL) [ 101 (see <ref type="bibr" target="#b20">[21]</ref> for a parallel implementation of the algorithm). This method, briefly described hereafter, is considered as a powerful approaches for global optimization, offering excellent computational performance and providing probabilistic guarantees to find the global minimum based on strong theoretical properties.</p><p>The MLSL-method is a stochastic iterative algorithm, combining random search with local minimization; it presents therefore some similarities with our hybridized methods. Basically, as in GQL and GQD, each iteration of the MLSL method consists of three steps: generation of new individuals, selection, and local search (cf., IQM-life). However, the way to generate and to select new individuals is different. MLSL generates new individuals randomly following a uniform distribution over the search space, without trying to accumulate and to exploit information to guide subsequent search in the most promising subspaces. MLSL then selects individuals (i.e., starting points for local search) using a geometrical Criterion: an individual z is chosen for local minimization if it has n6t been already chosen during a previous iteration and if there is no individual within a critical distance ~( k ) of 5 with a lower function value [ r ( k ) is a decreasing function of the iteration number k ] . The local minimizations can be performed by any standard "hill-climbing'' method and, in particular, by a QN method. The termination criterion is a Bayesian stopping rule, based on an estimate of the total number of local minima and the portion of the search space covered by the regions of attraction of the local minima found so far. Moreover, under conditions usually satisfied, both the accuracy and the efficiency of the algorithm are guaranteed in a strong probabilistic sense.</p><p>We applied MLSL as follows: 1) The selection of starting points was performed without sample reduction. 2) The local minimizations were performed with the QN method. For the reasons discussed in Section 111-C, we limited the maximum number of function evaluations in order to save computation time. The precise characteristics of the implementation are given in Appendix C.</p><p>In some of our case studies, where the number of local minima is too large (possibly infinite), the Bayesian stopping rule may require an unrealistic number of sample points and iterations. Therefore, we chose not to adopt the standard stopping rule, but to stop the search when the number of evaluations of the S2 function is approximately the same as for GQL. It must be borne in mind that there is no longer a probabilistic guarantee of finding the global minimum, as it is no longer possible to reach the theoretical number of sample points required. However, this allows us to compare the reliability and the accuracy of both methods for comparable amounts of computing timc. Given the difference in the numbers of sample points (individuals) involved, and the need to operate in complete generations, the computation times required are not identical-see Fig. show that both methods present equivalent performances in terms of reliability as well as accuracy, but their optimal performance is obtained for different situations (surfaces and number of measurements). Our experiments confirm that MLSL may fail in finding the global minimum [Fig. <ref type="figure">10(a)</ref>], all the more so since the standard stopping rule is not applied. It must be kept in mind that there is no theoretical guarantee that GQL will find the global minimum.</p><p>We expected GQL to produce better results than MLSL because GQL offers the advantage to exploit the information accumulated during previous generation to guide subsequent search into promising regions, while MLSL is, to some extent, blind. However, experimental results show that this expected superiority is not systematically obtained. We believe that this may be due to the limited number of generations which does not allow to take full advantage of the "guided search" property of the Genetic Algorithms in GQL, principally obtained by the successive application of the cross-over and selection mechanisms.</p><p>MLSL would require many more sample points to improve the reliability. In this case, the selection phase would require a significant amount of computing time, no longer negligible compared to the time needed for performing function evaluations. It would follow that our definition of computing time, based on the number of function evaluations, would have to be refined, in order to reflect this extra computing load. This computing load increases with the square of the total number of points already generated up to iteration k , if a straightforward sequential implementation of the algorithm is used-although it is theoretically possible to render it linear in the number of sample points by certain search techniques. On the other hand, the selection procedure of GQL is independent of the total number of individuals generated up to generation k , and is far faster. These arguments suggest that GQL might show significant advantage over MLSL when a large number of sample points are needed; however, such theoretical considerations need to be confirmed experimentally.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Iv. CONCLUSIONS AND FURTHER INVESTIGATIONS</head><p>This study emphasized the trade-off between accuracy, reliability, and computation time in global optimization. This can be seen as the impossibility of reaching accurately and reliably the global minimum in a short time. This trade-off is closely related to the balance "exploration-exploitation.'' Experimental comparisons showed how traditional methods (Quasi-Newton, Simplex) and Genetic Algorithms can be seen as particular instances of this trade-off.</p><p>Subsequently, the analysis of results of comparisons gave natural insights on how to produce an optimal compromise by interweaving the methods, taking the best advantage of both exploration and exploitation techniques, with a reasonable computation time. Several interweaving combinations were examined. One of these (the combination between Genetic Algorithms and Quasi-Newton method, with a selection scheme based on Lamarckian theory, GQL) appeared to join the group of state-of-the-art global optimization methods: GQL combines the reliability properties of the Genetic Algorithms with the accuracy of Quasi-Newton method, while requiring a computation time only slightly higher than the latter (particularly when parallel computers are used).</p><p>There are still difficulties which are not overcome by the proposed methods. In particular, ways for improving the accuracy of the optimization by avoiding premature termination and by adapting termination criteria must be investigated.</p><p>Beyond the particular problem tackled in this study (optimization of the function S'), we believe that the structure of the Hybrid Algorithms presented in this paper, designed using a combination of Quasi-Newton methods and Genetic Algorithms, could be applied to solving a wider range of nonlinear least-squares identification problems and to optimizing a broader class of functions, by providing a more efficient tool for finding the global optimum. For the particular function landscape, only the parameters of the Hybrid Algorithms (crossover and mutation rates, population size, "Quasi-Newton life" time, etc.) would of course have to be tuned in order to give optimal performances.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>APPENDIX A</head><p>This study case uses the physically-based model of <ref type="bibr" target="#b7">[8]</ref> which represents the satellite measurements in terms of surface properties. They have derived an analytical expression for the bidirectional reflectance from physical and geometrical considerations of the transfer of radiation through a porous medium. Their model represents the bidirectional reflectance p of a homogeneous and semi-infinite canopy illuminated by the Sun from a direction (01, 41), observed from a direction (02, 4'). In order to facilitate the retrieval of parameters through an inversion procedure, [9] have developed a simpler and parametric version where the reflectance is normalized with respect to the reflectance of a perfectly reflecting Lambertian surface under the same conditions of illumination and observation, using the significant parameters of the full theoretical model described in <ref type="bibr" target="#b7">[8]</ref>. This model is given </p><formula xml:id="formula_5">+ Q2p2 i = 1, 2 Q1 10.5 -0.6333~2 -0 . 3 3 ~; Q2 = 0.877(1 -2Q1) G = [tan' 01 + tan' e2 -2 tan Q1 \ . tan 02 cos (41 -42)11/2 1 -0 2 p(g) = [1+ 0 2 -2 0 cos (7r -g)]"/"</formula><p>In these equations, w is the average single scattering albedo of the particles making up the surface; 61 and 6 2 describe the scatterer orientation distribution for the illumination and viewing angles, respectively; x2 is a function of scatterer angle distribution; P, (G) is the parametric phase function that accounts for the "hotspot" effect; T is the radius of the sun flecks on the inclined leaf and A is the scatterer area density; P(g) is the average phase function; 0 is the asymmetry factor; and the term H ( p l / ~l ) H ( p l / ~~) -1 approximates the contribution from multiple scattering.</p><p>For each pair of angles in the illumination and viewing directions, the model represents the way the incoming radiation is scattered by the surface. Therefore the bidirectional reflectance model defines the surface characteristics only by the characteristics that influence the transmission of light, and the model can formally be represented by the following relation:</p><formula xml:id="formula_6">P = p(ct.1, QZ, ~3 , ~4 i 21, 2 2 , 23, 2 4 )<label>(8)</label></formula><p>where p represents the measurement, a, the characteristics (61, 0 2 ; 41, $2) of the measurements and x3 the properties of the surface (w, 0 , XI, r h , the independent variables of the model).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>APPENDIX B</head><p>COMPARISON OF DISCRETE (BINARY-CODED) AND CONTINUOUS (REAL-CODED) GENETIC ALGORITHMS Details about the implementation of both algorithms can be found in [l] and <ref type="bibr" target="#b11">[13]</ref>. We illustrate here the mutation and crossover operators used with our "real-coded version" of GA.</p><p>, 2 , ) be the individual about to be mutated; for real-coded GA, each gene represents one particular (un-coded) parameter of the problem and n is the number of parameters to be optimized. Each gene will undergo a modification, that will be significant during the first generations, and then gradually decreasing as the search is going further. For a generation t, two numbers are randomly generated: the first one ( p ) is either -1 (negative change), or $1 (positive change) with equal probability; the second one ( T ) , which determines indirectly the amplitude of the modification, is a number taken randomly in the interval [O; 11 with a uniform distribution. The mutated gene is given by <ref type="bibr" target="#b11">[13]</ref>: <ref type="bibr" target="#b8">(9)</ref> i f p = -1(10) where x,,, and xmax designate, respectively, the upper and lower bounds of the value of Xk; b is the decrease rate of mutation (b = 5 in all our experiments); T is the index of the generation for which the mutation amplitude goes to zero (the convenience of the correspondence "one genelone variable,"</p><formula xml:id="formula_7">Mutation: Let x = (51, 5 2 , 5 3 , xi = xk + (zmax -x k ) { l -T [ l -( t / * ) l b 1 xi = x k -(zk -xmln){l -r[l-(t/T)lb) if p = +I</formula><p>2) avoidance of problems related to the artificial character of the binary coding (e.g., Hammingclifs), 3) higher accuracy (especially with large domains), and 4) faster convergence.</p><p>We have compared the two techniques for solving the inversion problem, with the same "clean" data sets as in the experiments described in the body of this report. Fig. <ref type="figure">13</ref> shows that the use of binary-coded GA does not offer the same level of reliability and accuracy as the real-coded GA, while requiring a higher computation time. One explanation of these poor performances is the effect of the discretization of the search space combined with the phenomena described in 5.1; to avoid such a problem, the binary coding would excessive computation time.</p><p>All things considered, the preference given to the real-coded version of GA seems to be justified in the case of this inversion later generations will no longer be mutated). Fig. <ref type="figure" target="#fig_11">11</ref> gives the mutation amplitude with respect to the random number T for different values of the ratio ( t / T ) and for b = 5. Of course, the gradual decreasing of the mutation amplitude, which is similar to the temperature decrease schedule in the "Simulated Annealing" method, has a poor biological plausibility.</p><p>Crossover: We define here 2 types of crossover operators. The first one, applied with probability p l , builds a child by randomly taking the gene of one or the other parent: for instance, ( u l , u2, a3, u4, u5, u6) and ( b l , b 2 , u3, b4, u5, b ~) as children. This kind of crossover is called "discrete crossover," because it is similar the operator used in classical discrete GA (uniform crossover). The second type of crossover, called "continuous crossover," is applied to a pair of parents with probability p 2 and performs an "averaging" operation on the genes of both parents; for instance, the above-mentioned parents could give birth to the following two children: [ a l , u2, ( a 3 + b3)/2, u4, (a5 + b5)/2, (a6 +bG) <ref type="bibr">/2]</ref>. Of course, it is possible (and often useful) to design other kinds of crossover operators: for example, a complete continuous crossover, which would produce a child by (convex) linear combination of both parents, or even a "biased" continuous crossover which would result in a child situated preferentially in the neighborhood of the best parent. Fig. <ref type="figure" target="#fig_13">12</ref> illustrates the 4 types of crossover, showing for a given pair of parents the potential children (without taking into account the mutation effect). After all, we could imagine yet other combination mechanisms: for instance, one could take much more than 2 parents to give birth to children following the rules of the Simplex method, therefore using the local relative fitness values to direct the recombination. This could speed up the search efficiently, because operators based on local exploitation are used during the reproduction. But, once again, one faces the exploration/exploitation trade-off and the optimal compromise depends strongly on the particular problem to be solved.</p><p>There already are several studies [13] recommending the floating point representation, for the following reasons: </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>APPENDIX C EXPERIMENTATION CHARACTERISTICS</head><p>A. Technical Characteristics of the Optimization Methods 1) Quasi-Newton: The Quasi-Newton algorithm used is the routine E04JAF from the Numerical Algorithm Group (NAG) Library. This routine approximates the first derivatives with finite differences.</p><p>The maximum number of function evaluations is fixed at 1615 in the routine EO4JAF. Explosion termination criterion: ( z k + lz k l &lt; lop4, where z k is the best candidate solution (normalized) found after a complete run of the Simplex following the kth explosion. Constraint handling; a new objective function is introduced, defined by: f ( z ) = f ( z ) f K d ( z ) , where K is an arbitrary high positive number, f ( z ) is the function to be optimized and d is the distance from the set of feasible solutions (d = 0 if the point is feasible).</p><p>Candidate solution normalized (0 -1)</p><p>3) Binary-Coded Genetic Algorithms: The meaning of the parameters and details about the implementation are given in HI.   </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head></head><label></label><figDesc>By varying the surface parameters (x*) and the number of measurements, it turns out that the corresponding landscape of the function S2 exhibits different degrees of complexity (local minima, nearly flat valley, . . .).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>6 x1Fig. 2 .Fig. 3 .</head><label>623</label><figDesc>Fig. 1. x1 and x2 fixed. (b) Contour representation around the global minimum. Representation of the function 6' for data set A l . (a) Landscape with</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head></head><label></label><figDesc>(a)] where the bottom slightly slopes toward the boundary [Fig. 3(b)]. 3) Local minima can occur. They either form a "horizontal gutter" along a bound of the function domain, or are isolated points inside it.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Fig. 4 .</head><label>4</label><figDesc>Fig. 4. Representation of the function 6' for data set A1 (A), A2 (B), A3 (C).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Fig. 5 .</head><label>5</label><figDesc>Fig. 5. Accuracy. (c) Normalized computation time. Comparison of the methods QN, S, and GA. (a) Reliability. (b)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Fig. 8 .</head><label>8</label><figDesc>Fig.8. Typical evolution to the population on the fitness landscape for the Lamarck-inspired combination of real-coded Genetic Algorithms and Quasi-Newton (GQL). Symbol "X' indicates the nondeveloped individual (at birth), while the winding array represents the life of this individual (optimization with QN). Individuals selected for reproduction are marked by a dashed circle.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Fig 9 (</head><label>9</label><figDesc>Fig 9 (b) Accuracy (c) Normalized computation time Comparisqn of the methods QN, GA, GQD, and GQL. (a) Reliability</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head></head><label></label><figDesc>lO(c). Fig. lO(a) and (b)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head></head><label></label><figDesc>Fig. 10. Accuracy. (c) Normalized computation time. Comparison of the methods GQL and MLSL. (a) Reliability. (b)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head></head><label></label><figDesc>DESCRIPTION OF THE BIDIRECTIONAL REKECTANCE MODEL by where p1 = cos 01 p' = cos 0' cos g = cos 01 cos 02 + sin 01 sin 02 cos (41 -4 2 ) n,(p,) =Q1</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_11"><head>Fig. 11 .</head><label>11</label><figDesc>Fig. 11. Distribution of the mutation amplitude (t/T = relative generation index)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_12"><head></head><label></label><figDesc>1) the ( b l , b2r b3, b4, b 5 , b 6 ) could give (air a2, b3, a4, b 5 , a6) and b5)/2, (a6 f b 6 d 2 i and [bl, b 2 , (a3 + b3)/2, b4, (a5 f problem.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_13"><head>2 )and</head><label>2</label><figDesc>Simplex: The meaning of the parameters and details about the implementation are given in Number of vertices of the Simplex: 5. Simplex termination criterion (before reinitializing search by exploding the Simplex): IS, , , -Sminl &lt; lop7, Smin are, respectively, the minimum and maximum value of the S function over all vertices of the current Simplex. Explosion length (normalized): 0.1 in each direction.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_14"><head></head><label></label><figDesc>Chromosome length: 40 (bits). Population size: 80. Probability of mutation: 0.01 (per bit).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_15"><head>Fig 12 Fig. 13 .00</head><label>1213</label><figDesc>Fig 12 Types of crossover mechanisms for "real-coded'' GA (a) Original palr of parents (white circles) (b) Children generated by discrete crossover (white and black circles) (c) Children generated by continuous crossover (white and black circles) (d) Children generated by complete continuous crossover (hatched area) (e) Children generated by biased continuous crossover (area in the neighborhood of the best parent)</figDesc></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" xml:id="foot_0"><p>Manuscript received June23, 1993; revised December 10, 1994. J.-M. Renders is with the Facult6 des Sciences Appliqukes, Universitk Libre de Bruxelles, Brussels, Belgium.S. P. Flasse is with the Environmental Sciences Group, Natural Resources Institute, Kent ME4 4TB, UK (e-mail: nri@ukc.ac.uk).Publisher Item Identifier S 1083-4419(96)02306-0.</p></note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>The authors are grateful to M. M. Verstraete and M. N. Mitchison for extensive review of the paper and very helpful discussions.</p></div>
			</div>

			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Angles</head><p>Candidate solution normalized (0 -1) (See also Tables <ref type="table">I</ref> and<ref type="table">11</ref>). He is currently working at Tractebel Energy Engineering, Artificial Intelligence Section, Brussels, Belgium. His research interest include neural networks, genetic algorithms, and artificial intelligence techniques applied to process control and power systems. </p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">E</forename><surname>Goldberg</surname></persName>
		</author>
		<title level="m">Genetic Algorithms in Search, Optimization, and Machine Learning</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">Handbook of Genetic Algorithms</title>
		<author>
			<persName><forename type="first">L</forename><surname>Davis</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1991">1991</date>
			<pubPlace>New York Van Nostsand Reinhold</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">Optimization: Theory and Practice</title>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">S</forename><surname>Beveridge</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">S</forename><surname>Schechter</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1970">1970</date>
			<publisher>McGraw-Hill</publisher>
			<pubPlace>New York</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Quasi-Newton methods for unconstrained optimization</title>
		<author>
			<persName><forename type="first">P</forename><surname>Gill</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Murray</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Inst. Math. Applicat</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="page" from="91" to="108" />
			<date type="published" when="1972">1972</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">A simplex method for function optimization</title>
		<author>
			<persName><forename type="first">Si</forename><forename type="middle">A</forename><surname>Nelder</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Mead</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computation J</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page" from="308" to="313" />
			<date type="published" when="1965">1965</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">System identification and control using genetic algorithm</title>
		<author>
			<persName><forename type="first">K</forename><surname>Kristinsson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">A</forename><surname>Dumont</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Syst., Man, Cybern</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Biological learning metaphors for adaptive process control: A general strategy</title>
		<author>
			<persName><forename type="first">J.-M</forename><surname>Renders</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J.-P</forename><surname>Nordvik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Hanus</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Int. Symp. Intelligent Control</title>
		<meeting>IEEE Int. Symp. Intelligent Control<address><addrLine>Glasgow</addrLine></address></meeting>
		<imprint>
			<date type="published" when="1992-08">Aug. 1992</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Bidirectional reflectance of vegetation canopies. Part I: Theory</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">M</forename><surname>Verstraete</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Pinty</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">E</forename><surname>Dickinson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Geophys. Res</title>
		<imprint>
			<biblScope unit="volume">95</biblScope>
			<biblScope unit="page">765</biblScope>
			<date type="published" when="1990">1990</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Bidirectional reflectance of vegetation canopies. Part 11: Inversion and validation</title>
		<author>
			<persName><forename type="first">B</forename><surname>Pinty</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">M</forename><surname>Verstraete</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">E</forename><surname>Dickinson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Geophys. Res</title>
		<imprint>
			<biblScope unit="volume">95</biblScope>
			<biblScope unit="page">775</biblScope>
			<date type="published" when="1990">1990</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Stochastic methods for global optimization</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">H G</forename><surname>Rinnooy Kan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">T</forename><surname>Timmer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Amer. J. Mathemat. Manag. Sci</title>
		<editor>
			<persName><forename type="first">N</forename><forename type="middle">A G</forename></persName>
		</editor>
		<editor>
			<persName><forename type="first">N</forename><forename type="middle">A</forename><surname>Fortran Libra? Manual</surname></persName>
		</editor>
		<editor>
			<persName><surname>Mark 14</surname></persName>
		</editor>
		<imprint>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="issue">7 4 0</biblScope>
			<date type="published" when="1984">1984. 1990</date>
			<pubPlace>Oxford</pubPlace>
		</imprint>
		<respStmt>
			<orgName>The Numerical Algorithm Group</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Ahandling constraints in genetic algorithms</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Michalewicz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">Z</forename><surname>Janikow</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Fourth Inc. Conf Genetic Algorithms</title>
		<editor>
			<persName><forename type="first">R</forename><forename type="middle">K</forename><surname>Belew</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">L</forename><forename type="middle">B</forename><surname>Booker</surname></persName>
		</editor>
		<meeting>Fourth Inc. Conf Genetic Algorithms<address><addrLine>San Francisco</addrLine></address></meeting>
		<imprint>
			<publisher>Morgan Kanfman</publisher>
			<date type="published" when="1991">1991</date>
			<biblScope unit="page" from="151" to="157" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">An experimental comparison of binary and floating point representation in genetic algorithms</title>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">2</forename><surname>Janikow</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Michalewicz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Fourth lnt. Con$ Genetic Algorithms</title>
		<meeting>Fourth lnt. Con$ Genetic Algorithms<address><addrLine>San Francisco</addrLine></address></meeting>
		<imprint>
			<publisher>Morgan Kaufman</publisher>
			<date type="published" when="1991">1991</date>
			<biblScope unit="page" from="31" to="36" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Genetic algorithms and Walsh functions: Part 11, deception and its analysis</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">E</forename><surname>Goldberg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Complex Syst</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page" from="153" to="171" />
			<date type="published" when="1989">1989</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">A simulated annealing like convergence theory for the simple genetic algorithm</title>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">E</forename><surname>Davis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">C</forename><surname>Principe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Fourth Int. Con$ Genetic Algorithms</title>
		<editor>
			<persName><forename type="first">R</forename><forename type="middle">K</forename><surname>Belew</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">L</forename><forename type="middle">B</forename><surname>Booker</surname></persName>
		</editor>
		<meeting>Fourth Int. Con$ Genetic Algorithms<address><addrLine>San Francisco</addrLine></address></meeting>
		<imprint>
			<publisher>Morgan Kaufman</publisher>
			<date type="published" when="1991">1991</date>
			<biblScope unit="page" from="174" to="181" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Evolving networks: Using the genetic algorithms with connectionist learning</title>
		<author>
			<persName><forename type="first">R</forename><surname>Belew</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Mcinerney</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><forename type="middle">N</forename><surname>Schraudoph</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Santa Fe Institute Studies in the Sciences of Complexity</title>
		<editor>
			<persName><forename type="first">Artijicial</forename><surname>Life</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">I</forename><forename type="middle">I</forename></persName>
		</editor>
		<editor>
			<persName><forename type="first">C</forename><forename type="middle">G</forename><surname>Langton</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">C</forename><surname>Taylor</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">J</forename><forename type="middle">D</forename><surname>Farmer</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">S</forename><surname>Rasmussen</surname></persName>
		</editor>
		<meeting><address><addrLine>Redwood City, CA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="1991">1991</date>
			<biblScope unit="page" from="511" to="547" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Interdigitation: A hybrid technique for engineering design optimization employing genetic algorithms, expert systems, and numerical optimization</title>
		<author>
			<persName><forename type="first">D</forename><surname>Powell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Skolnock</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">S</forename><surname>Tong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Handbook of</title>
		<meeting><address><addrLine>New York</addrLine></address></meeting>
		<imprint>
			<publisher>Addison-Wesley</publisher>
			<date type="published" when="1989">1989</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title/>
		<author>
			<persName><forename type="first">L</forename><surname>Genetic Algorithms</surname></persName>
		</author>
		<author>
			<persName><surname>Davis</surname></persName>
		</author>
		<editor>New York Van Nostrand Reinhold</editor>
		<imprint>
			<date type="published" when="1991">1991</date>
			<biblScope unit="page" from="2" to="331" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<author>
			<persName><forename type="first">T</forename><surname>Dobzhanskv</surname></persName>
		</author>
		<title level="m">Genetics of the Evolutionant Process</title>
		<meeting><address><addrLine>New York</addrLine></address></meeting>
		<imprint>
			<publisher>L 1 Columbia University Press</publisher>
			<date type="published" when="1970">1970</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">A case for Lamarckian evolution</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">H</forename><surname>Ackley</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">L</forename><surname>Littman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Santa Fe Institute Studies in the Sciences of Complexity</title>
		<editor>
			<persName><forename type="first">L</forename><surname>Artificial</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">C</forename><forename type="middle">G</forename><surname>$e Iii</surname></persName>
		</editor>
		<editor>
			<persName><surname>Langton</surname></persName>
		</editor>
		<meeting><address><addrLine>Ed. Redwood City, CA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="1994">1994</date>
			<biblScope unit="page" from="3" to="10" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Lamarckian learning in multi-agent environments</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">J</forename><surname>Gefenstette</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Fourth Int. Con$ on Genetic Algorithms</title>
		<editor>
			<persName><forename type="first">Morgan</forename><surname>Kaufman</surname></persName>
		</editor>
		<meeting>Fourth Int. Con$ on Genetic Algorithms</meeting>
		<imprint>
			<date type="published" when="1991">1991</date>
			<biblScope unit="page" from="303" to="310" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Concurrent stochastic methods for global optimization</title>
		<author>
			<persName><forename type="first">R</forename><surname>Byrd</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Dert</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">Rinnooy</forename><surname>Kan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Schnabel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Muthemat. Prog</title>
		<imprint>
			<biblScope unit="volume">46</biblScope>
			<biblScope unit="page" from="1" to="29" />
			<date type="published" when="1990">1990</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
