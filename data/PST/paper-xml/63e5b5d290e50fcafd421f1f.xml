<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Toolformer: Language Models Can Teach Themselves to Use Tools</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Timo</forename><surname>Schick</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Jane</forename><surname>Dwivedi-Yu</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Roberto</forename><surname>Dess?</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution">Universitat Pompeu Fabra</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Roberta</forename><surname>Raileanu</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Maria</forename><surname>Lomeli</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Luke</forename><surname>Zettlemoyer</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Nicola</forename><surname>Cancedda</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Thomas</forename><surname>Scialom</surname></persName>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="institution">Meta AI Research</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Toolformer: Language Models Can Teach Themselves to Use Tools</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-01-03T09:20+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Language models (LMs) exhibit remarkable abilities to solve new tasks from just a few examples or textual instructions, especially at scale. They also, paradoxically, struggle with basic functionality, such as arithmetic or factual lookup, where much simpler and smaller models excel. In this paper, we show that LMs can teach themselves to use external tools via simple APIs and achieve the best of both worlds. We introduce Toolformer, a model trained to decide which APIs to call, when to call them, what arguments to pass, and how to best incorporate the results into future token prediction. This is done in a self-supervised way, requiring nothing more than a handful of demonstrations for each API. We incorporate a range of tools, including a calculator, a Q&amp;A system, a search engine, a translation system, and a calendar. Toolformer achieves substantially improved zero-shot performance across a variety of downstream tasks, often competitive with much larger models, without sacrificing its core language modeling abilities.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Large language models achieve impressive zeroand few-shot results on a variety of natural language processing tasks <ref type="bibr" target="#b6">(Brown et al., 2020;</ref><ref type="bibr">Chowdhery et al., 2022, i.a.)</ref> and show several emergent capabilities <ref type="bibr" target="#b50">(Wei et al., 2022)</ref>. However, all of these models have several inherent limitations that can at best be partially addressed by further scaling. These limitations include an inability to access up-to-date information on recent events <ref type="bibr" target="#b25">(Komeili et al., 2022)</ref> and the related tendency to hallucinate facts <ref type="bibr" target="#b31">(Maynez et al., 2020;</ref><ref type="bibr" target="#b19">Ji et al., 2022)</ref>, difficulties in understanding low-resource languages <ref type="bibr" target="#b30">(Lin et al., 2021)</ref>, a lack of mathematical skills to perform precise calculations <ref type="bibr" target="#b37">(Patel et al., 2021)</ref> and an unawareness of the progression of time <ref type="bibr" target="#b11">(Dhingra et al., 2022)</ref>.  The model autonomously decides to call different APIs (from top to bottom: a question answering system, a calculator, a machine translation system, and a Wikipedia search engine) to obtain information that is useful for completing a piece of text.</p><p>A simple way to overcome these limitations of today's language models is to give them the ability to use external tools such as search engines, calculators, or calendars. However, existing approaches either rely on large amounts of human annotations <ref type="bibr" target="#b25">(Komeili et al., 2022;</ref><ref type="bibr">Thoppilan et al., 2022)</ref> or limit tool use to task-specific settings only (e.g., <ref type="bibr">Gao et al., 2022;</ref><ref type="bibr" target="#b36">Parisi et al., 2022)</ref>, hindering a more widespread adoption of tool use in LMs. Therefore, we propose Toolformer, a model that learns to use tools in a novel way, which fulfills the following desiderata:</p><p>? The use of tools should be learned in a self-supervised way without requiring large amounts of human annotations. This is impor-arXiv:2302.04761v1 [cs.CL] 9 Feb 2023</p><p>x 1:i-1 = Pittsburgh is also known as  Figure <ref type="figure">2</ref>: Key steps in our approach, illustrated for a question answering tool: Given an input text x, we first sample a position i and corresponding API call candidates c<ref type="foot" target="#foot_0">1</ref> i , c 2 i , . . . , c k i . We then execute these API calls and filter out all calls which do not reduce the loss L i over the next tokens. All remaining API calls are interleaved with the original text, resulting in a new text x * .</p><formula xml:id="formula_0">x i:n = the</formula><formula xml:id="formula_1">r i 2 = United States L i (c i 1 ? Steel City) &lt; min(L i (c i 1 ? ?), L i (?)) L i (c i 2 ? United States) &gt; min(L i (c i 2 ? ?), L i (?))</formula><p>tant not only because of the costs associated with such annotations, but also because what humans find useful may be different from what a model finds useful.</p><p>? The LM should not lose any of its generality and should be able to decide for itself when and how to use which tool. In contrast to existing approaches, this enables a much more comprehensive use of tools that is not tied to specific tasks.</p><p>Our approach for achieving these goals is based on the recent idea of using large LMs with incontext learning <ref type="bibr" target="#b6">(Brown et al., 2020)</ref> to generate entire datasets from scratch <ref type="bibr">(Schick and Sch?tze, 2021b;</ref><ref type="bibr" target="#b16">Honovich et al., 2022;</ref><ref type="bibr" target="#b49">Wang et al., 2022)</ref>: Given just a handful of human-written examples of how an API can be used, we let a LM annotate a huge language modeling dataset with potential API calls. We then use a self-supervised loss to determine which of these API calls actually help the model in predicting future tokens. Finally, we finetune the LM itself on the API calls that it considers useful. As illustrated in Figure <ref type="figure" target="#fig_1">1</ref>, through this simple approach, LMs can learn to control a variety of tools, and to choose for themselves which tool to use when and how.</p><p>As our approach is agnostic of the dataset being used, we can apply it to the exact same dataset that was used to pretrain a model in the first place. This ensures that the model does not lose any of its generality and language modeling abilities. We conduct experiments on a variety of different downstream tasks, demonstrating that after learning to use tools, Toolformer, which is based on a pretrained GPT-J model <ref type="bibr" target="#b48">(Wang and Komatsuzaki, 2021</ref>) with 6.7B parameters, achieves much stronger zero-shot results, clearly outperforming a much larger GPT-3 model <ref type="bibr" target="#b6">(Brown et al., 2020)</ref> and several other baselines on various tasks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Approach</head><p>Our aim is to equip a language model M with the ability to use different tools by means of API calls. We require that inputs and outputs for each API can be represented as text sequences. This allows seamless insertion of API calls into any given text, using special tokens to mark the start and end of each such call.</p><p>We represent each API call as a tuple c = (a c , i c ) where a c is the name of the API and i c is the corresponding input. Given an API call c with a corresponding result r, we denote the linearized sequences of the API call not including and including its result, respectively, as:</p><formula xml:id="formula_2">e(c) = &lt;API&gt; a c (i c ) &lt;/API&gt; e(c, r) = &lt;API&gt; a c (i c ) ? r &lt;/API&gt;</formula><p>where "&lt;API&gt;", "&lt;/API&gt;" and "?" are special tokens. 1 Some examples of linearized API calls inserted into text sequences are shown in Figure <ref type="figure" target="#fig_1">1</ref>.</p><p>Given a dataset C = {x 1 , . . . , x |C| } of plain texts, we first convert this dataset into a dataset C * augmented with API calls. This is done in three steps, illustrated in Figure <ref type="figure">2</ref>: First, we exploit the in-context learning ability of M to sample a large number of potential API calls. We then execute these API calls and finally check whether the obtained responses are helpful for predicting future tokens; this is used as a filtering criterion. After filtering, we merge API calls for different tools, resulting in the augmented dataset C * , and finetune M itself on this dataset. Each of these steps is described in more detail below.</p><p>Sampling API Calls For each API, we write a prompt P (x) that encourages the LM to annotate an example x = x 1 , . . . , x n with API calls. An example of such a prompt for a question answering tool is shown in Figure <ref type="figure" target="#fig_2">3</ref>; all prompts used are shown in Appendix A.2. Let p M (z n+1 | z 1 , . . . , z n ) be the probability that M assigns to token z n+1 as a continuation for the sequence z 1 , . . . , z n . We first sample up to k candidate positions for doing API calls by computing, for each i ? {1, . . . , n}, the probability</p><formula xml:id="formula_3">p i = p M (&lt;API&gt; | P (x), x 1:i-1 )</formula><p>that M assigns to starting an API call at position i. Given a sampling threshold ? s , we keep all positions I = {i | p i &gt; ? s }; if there are more than k such positions, we only keep the top k.</p><p>For each position i ? I, we then obtain up to m API calls c 1 i , . . . , c m i by sampling from M given the sequence [P (x), x 1 , . . . , x i-1 , &lt;API&gt;] as a prefix and &lt;/API&gt; as an end-of-sequence token. <ref type="foot" target="#foot_1">2</ref>Executing API Calls As a next step, we execute all API calls generated by M to obtain the corresponding results. How this is done depends entirely on the API itself -for example, it can involve calling another neural network, executing a Python script or using a retrieval system to perform search over a large corpus. The response for each API call c i needs to be a single text sequence r i .</p><p>Filtering API Calls Let i be the position of the API call c i in the sequence x = x 1 , . . . , x n , and let r i be the response from the API. Further, given a sequence</p><formula xml:id="formula_4">(w i | i ? N) of weights, let L i (z) = - n j=i w j-i ? log p M (x j | z, x 1:j-1 )</formula><p>be the weighted cross entropy loss for M over the tokens x i , . . . , x n if the model is prefixed with z. We compare two different instantiations of this loss:</p><formula xml:id="formula_5">L + i = L i (e(c i , r i )) L - i = min (L i (?), L i (e(c i , ?)))</formula><p>where ? denotes an empty sequence. The former is the weighted loss over all tokens x i , . . . , x n if the API call and its result are given to M as a prefix;<ref type="foot" target="#foot_2">3</ref> the latter is the minimum of the losses obtained from (i) doing no API call at all and (ii) doing an API call, but not providing the response. Intuitively, an API call is helpful to M if providing it with both the input and the output of this call makes it easier for the model to predict future tokens, compared to not receiving the API call at all, or receiving only its input. Given a filtering threshold ? f , we thus only keep API calls for which</p><formula xml:id="formula_6">L - i -L + i ? ? f</formula><p>holds, i.e., adding the API call and its result reduces the loss by at least ? f , compared to not doing any API call or obtaining no result from it.</p><p>Model Finetuning After sampling and filtering calls for all APIs, we finally merge the remaining API calls and interleave them with the original inputs. That is, for an input text x = x 1 , . . . , x n with a corresponding API call and result (c i , r i ) at position i, we construct the new sequence x * =</p><p>x 1:i-1 , e(c i , r i ), x i:n ; we proceed analogously for texts with multiple API calls. Doing this for all x ? C results in the new dataset C * augmented with API calls. We use this new dataset to finetune M , using a standard language modeling objective. Crucially, apart from inserted API calls the augmented dataset C * contains the exact same texts as C, the original dataset. As a consequence, finetuning M on C * exposes it to the same content as finetuning on C. Moreover, as API calls are inserted in exactly those positions and with exactly those inputs that help M predict future tokens, finetuning on C * enables the language model to decide when and how to use which tool, based purely on its own feedback.</p><p>Inference When generating text with M after finetuning with our approach, we perform regular decoding until M produces the "?" token, indicating that it next expects the response for an API call. At this point, we interrupt the decoding process, call the appropriate API to get a response, and continue the decoding process after inserting both the response and the &lt;/API&gt; token.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Tools</head><p>We explore a variety of tools to address different shortcomings of regular LMs. The only constraints we impose on these tools is that (i) both their inputs and outputs can be represented as text sequences, and (ii) we can obtain a few demonstrations of their intended use. Concretely, we explore the following five tools: a question answering system, a Wikipedia search engine, a calculator, a calendar, and a machine translation system. Some examples of potential calls and return strings for the APIs associated with each of these tools are shown in Table <ref type="table" target="#tab_2">1</ref>. We briefly discuss all tools below; further details can be found in Appendix A.</p><p>Question Answering Our first tool is a question answering system based on another LM that can answer simple factoid questions. Specifically, we use Atlas <ref type="bibr">(Izacard et al., 2022)</ref>, a retrieval-augmented LM finetuned on Natural Questions <ref type="bibr" target="#b27">(Kwiatkowski et al., 2019)</ref>.</p><p>Calculator As a second tool, we use a calculator that can perform simple numeric calculations; we only support the four basic arithmetic operations.</p><p>Results are always rounded to two decimal places.</p><p>Wikipedia Search Our third tool is a search engine that, given a search term, returns short text snippets from Wikipedia. Compared to our question answering tool, this search enables a model to get more comprehensive information on a subject, but requires it to extract the relevant parts by itself. As our search engine, we use a BM25 retriever <ref type="bibr" target="#b42">(Robertson et al., 1995;</ref><ref type="bibr" target="#b2">Baeza-Yates et al., 1999)</ref> that indexes the Wikipedia dump from KILT <ref type="bibr" target="#b38">(Petroni et al., 2021)</ref>.</p><p>Machine Translation System Our fourth tool is a machine translation system based on a LM that can translate a phrase from any language into English. More concretely, we use the 600M parameter NLLB <ref type="bibr" target="#b9">(Costa-juss? et al., 2022)</ref> as our multilingual machine translation model that works for 200 languages (including low-resource ones). The source language is automatically detected using the fast-Text classifier <ref type="bibr" target="#b22">(Joulin et al., 2016)</ref>, while the target language is always set to English.</p><p>Calendar Our final tool is a calendar API that, when queried, returns the current date without taking any input. This provides temporal context for predictions that require some awareness of time.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experiments</head><p>We investigate whether our approach enables a model to use tools without any further supervision and to decide for itself when and how to call which of the available tools. To test this, we select a variety of downstream tasks where we assume at least one of the considered tools to be useful, and evaluate performance in zero-shot settings (Section 4.2).</p><p>Beyond that, we also ensure that our approach does not hurt the model's core language modeling abilities; we verify this by looking at perplexity on two language modeling datasets (Section 4.3). Finally, we investigate how the ability to learn using tools is affected by model size (Section 4.4).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Experimental Setup</head><p>Dataset Generation Throughout all of our experiments, we use a subset of CCNet <ref type="bibr" target="#b51">(Wenzek et al., 2020)</ref> as our language modeling dataset C and GPT-J <ref type="bibr" target="#b48">(Wang and Komatsuzaki, 2021)</ref> as our language model M . To reduce the computational cost of annotating C with API calls, we define heuristics for some APIs to get a subset of C for which API calls are more likely to be helpful than for an average text. For example, we only consider texts for the calculator tool if they contain at least three numbers. Details of the heuristics used are given in Appendix A. For obtaining C * from C, we perform all steps described in Section 2 and additionally filter out all examples for which all API calls were eliminated in the filtering step. <ref type="foot" target="#foot_3">4</ref> For the weighting function, we use</p><formula xml:id="formula_7">w t = wt s?N ws with wt = max(0, 1 -0.2 ? t)</formula><p>to make sure that API calls happen close to where the information provided by the API is actually helpful for the model. The thresholds ? s and ? f are chosen individually for each tool to ensure a sufficiently larger number of examples; see Appendix A for details. Table <ref type="table" target="#tab_3">2</ref> shows relevant statistics of our final dataset augmented with API calls.</p><p>Model Finetuning We finetune M on C * using a batch size of 128 and a learning rate of 1 ? 10 -<ref type="foot" target="#foot_4">5</ref> with linear warmup for the first 10% of training. Details of our finetuning procedure are given in Appendix B.</p><p>Baseline Models Throughout the remainder of this section, we mainly compare the following models:</p><p>? GPT-J: A regular GPT-J model without any finetuning.</p><p>? GPT-J + CC: GPT-J finetuned on C, our subset of CCNet without any API calls.</p><p>? Toolformer: GPT-J finetuned on C * , our subset of CCNet augmented with API calls.</p><p>? Toolformer (disabled): The same model as Toolformer, but API calls are disabled during decoding. 5</p><p>For most tasks, we additionally compare to OPT (66B) <ref type="bibr" target="#b55">(Zhang et al., 2022)</ref> and GPT-3<ref type="foot" target="#foot_5">6</ref> (175B) <ref type="bibr" target="#b6">(Brown et al., 2020)</ref>, two models that are about 10 and 25 times larger than our other baseline models, respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Downstream Tasks</head><p>We evaluate all models on a variety of downstream tasks. In all cases, we consider a prompted zeroshot setup -i.e., models are instructed to solve each task in natural language, but we do not provide any in-context examples. This is in contrast to prior work on tool use (e.g., <ref type="bibr">Gao et al., 2022;</ref><ref type="bibr" target="#b36">Parisi et al., 2022)</ref>, where models are provided with dataset-specific examples of how a tool can be used to solve a concrete task. We choose the more challenging zero-shot setup as we are interested in seeing whether Toolformer works in precisely those cases where a user does not specify in advance which tools should be used in which way for solving a specific problem. We use standard greedy decoding, but with one modification for Toolformer: We let the model start an API call not just when &lt;API&gt; is the most likely token, but whenever it is one of the k most likely tokens. For k = 1, this corresponds to regular greedy decoding; we instead use k = 10 to increase the disposition of our model to make use of the APIs that it has access to. At the same time, we only at most one API call per input to make sure the model does not get stuck in a loop where it constantly calls APIs without producing any actual output. The effect of these modifications is explored in Section 5.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.1">LAMA</head><p>We evaluate our models on the SQuAD, Google-RE and T-REx subsets of the LAMA benchmark <ref type="bibr" target="#b39">(Petroni et al., 2019)</ref>. For each of these subsets, the task is to complete a short statement with a missing fact (e.g., a date or a place). As LAMA was originally designed to evaluate masked language models (e.g., <ref type="bibr" target="#b10">Devlin et al., 2019)</ref>, we filter out examples where the mask token is not the final token, so that the remaining examples can be processed in a left-to-right fashion. To account for different tokenizations and added complexity from not informing the model that a single word is required, we use a slightly more lenient evaluation criterion than exact match and simply check whether the correct word is within the first five words predicted by the model. As LAMA is based on statements obtained directly from Wikipedia, we prevent Toolformer from using the Wikipedia Search API to avoid giving it an unfair advantage.</p><p>Results for all models can be seen in Table <ref type="table">3</ref>. All GPT-J models without tool use achieve similar performance. Crucially, Toolformer clearly outperforms these baseline models, improving upon the best baseline by 11.7, 5.2 and 18.6 points, respectively. It also clearly outperforms OPT (66B) and GPT-3 (175B), despite both models being much larger. This is achieved because the model independently decides to ask the question answering tool for the required information in almost all cases (98.1%); for only very few examples, it uses a different tool (0.7%) or no tool at all (1.2%).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.2">Math Datasets</head><p>We test mathematical reasoning abilities on ASDiv <ref type="bibr" target="#b34">(Miao et al., 2020)</ref>, SVAMP <ref type="bibr" target="#b37">(Patel et al., 2021)</ref> and the MAWPS benchmark <ref type="bibr" target="#b26">(Koncel-Kedziorski et al., 2016)</ref>. We again account for the fact that we test all models in a zero-shot setup by using a more lenient evaluation criterion: As the required output is always a number, we simply check for the first number predicted by the model. 7  Table <ref type="table" target="#tab_4">4</ref> shows results for all benchmarks. While GPT-J and GPT-J + CC perform about the same, Toolformer achieves stronger results even when API calls are disabled. We surmise that this is because the model is finetuned on many examples of API calls and their results, improving its own mathematical capabilities. Nonetheless, allowing the model to make API calls more than doubles performance for all tasks, and also clearly outperforms the much larger OPT and GPT-3 models. This is because across all benchmarks, for 97.9% of all examples the model decides to ask the calculator tool for help.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.3">Question Answering</head><p>We look at Web Questions <ref type="bibr" target="#b3">(Berant et al., 2013)</ref>, Natural Questions <ref type="bibr" target="#b27">(Kwiatkowski et al., 2019)</ref> and TriviaQA <ref type="bibr" target="#b21">(Joshi et al., 2017)</ref>, the three question answering datasets considered by <ref type="bibr" target="#b6">Brown et al. (2020)</ref>. For evaluation, we check whether the first 20 words predicted by a model contain the correct answer instead of requiring an exact match. For Toolformer, we disable the question answering tool as 7 An exception to this is if the model's prediction contains an equation (e.g., "The correct answer is 5+3=8"), in which case we consider the first number after the "=" sign to be its prediction. this would make solving the tasks trivial, especially given that the underlying QA system was finetuned on Natural Questions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Model</head><p>Results are shown in Table <ref type="table" target="#tab_5">5</ref>. Once again, Toolformer clearly outperforms all other models based on GPT-J, this time mostly relying on the Wikipedia search API (99.3%) to find relevant information. However, Toolformer still lags behind the much larger GPT-3 (175B) model. This is likely due to both the simplicity of our search engine (in many cases, it returns results that are clearly not a good match for a given query) and the inability of Toolformer to interact with it, e.g., by reformulating its query if results are not helpful or by browsing through multiple of the top results. We believe that adding this functionality is an exciting direction for future work.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.4">Multilingual Question Answering</head><p>We evaluate Toolformer and all baseline models on MLQA <ref type="bibr" target="#b29">(Lewis et al., 2019)</ref>, a multilingual question-answering benchmark. A context paragraph for each question is provided in English, while the question can be in Arabic, German, Spanish, Hindi, Vietnamese, or Simplified Chinese. In order to solve the task, the model needs to be able to understand both the paragraph and the question, so it may benefit from translating the question into English. Our evaluation metric is the percentage of times the model's generation, capped at 10 words, contains the correct answer.</p><p>Results are shown in Table <ref type="table">6</ref>. Using API calls consistently improves Toolformer's performance for all languages, suggesting that it has learned to make use of the machine translation tool. Depending on the language, this tool is used for 63.8% to 94.9% of all examples; the only exception to this is Hindi, for which the machine translation tool is used in only 7.3% of cases. However, Tool- former does not consistently outperform vanilla GPT-J. This is mainly because for some languages, finetuning on CCNet deteriorates performance; this might be due to a distribution shift compared to GPT-J's original pretraining data.</p><p>OPT and GPT-3 perform surprisingly weak across all languages, mostly because they fail to provide an answer in English despite being instructed to do so. A potential reason for GPT-J not suffering from this problem is that it was trained on more multilingual data than both OPT and GPT-3, including the EuroParl corpus <ref type="bibr" target="#b24">(Koehn, 2005;</ref><ref type="bibr" target="#b12">Gao et al., 2020)</ref>. As an upper bound, we also evaluate GPT-J and GPT-3 on a variant of MLQA where both the context and the question are provided in English. In this setup, GPT-3 performs better than all other models, supporting our hypothesis that its subpar performance on MLQA is due to the multilingual aspect of the task.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.5">Temporal Datasets</head><p>To investigate the calendar API's utility, we evaluate all models on TEMPLAMA <ref type="bibr" target="#b11">(Dhingra et al., 2022)</ref> and a new dataset that we call DATESET. TEMPLAMA is a dataset built from Wikidata that contains cloze queries about facts that change with time (e.g., "Cristiano Ronaldo plays for ___") as well as the correct answer for the years between 2010 and 2020. DATESET, described in Appendix D, is also generated through a series of templates, but populated using a combination of random dates/durations (e.g., "What day of the week was it 30 days ago?"). Critically, knowing the current date is required to answer these questions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Model TEMPLAMA DATESET</head><p>GPT-J 13.7 3.9 GPT-J + CC 12.9 2.9 Toolformer (disabled) 12.7 5.9 Toolformer 16.3 27.3</p><p>OPT (66B) 14.5 1.3 GPT-3 (175B) 15.5 0.8</p><p>Table <ref type="table">7</ref>: Results for the temporal datasets. Toolformer outperforms all baselines, but does not make use of the calendar tool for TEMPLAMA.</p><p>For both tasks, we use the same evaluation as for the original LAMA dataset.</p><p>Results shown in Table <ref type="table">7</ref> illustrate that Toolformer outperforms all baselines for both TEM-PLAMA and DATESET. However, closer inspection shows that improvements on TEMPLAMA can not be attributed to the calendar tool, which is only used for 0.2% of all examples, but mostly to the Wikipedia search and question answering tools, which Toolformer calls the most. This makes sense given that named entities in TEMPLAMA are often so specific and rare that even knowing the exact date alone would be of little help. The best course of action for this dataset -first querying the calendar API to get the current date, and then querying the question answering system with this date -is not only prohibited by our restriction of using at most one API call per example, but also hard to learn for Toolformer given that all API calls in its training data are sampled independently.</p><p>For DATESET, on the other hand, the considerable improvement of Toolformer compared to other models can be fully accredited to the calendar tool, which it makes use of for 54.8% of all examples.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Language Modeling</head><p>In addition to verifying improved performance on various downstream tasks, we also want to ensure that language modeling performance of Toolformer does not degrade through our finetuning with API calls. To this end, we evaluate our models on two language modeling datasets: WikiText (Merity et al., 2017) and a subset of 10,000 randomly selected documents from CCNet <ref type="bibr" target="#b51">(Wenzek et al., 2020)</ref>  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Scaling Laws</head><p>We investigate how the ability to ask external tools for help affects performance as we vary the size of our LM. To this end, we apply our approach not just to GPT-J, but also to four smaller models from the GPT-2 family <ref type="bibr" target="#b40">(Radford et al., 2019)</ref>, with 124M, 355M, 775M and 1.6B parameters, respectively. We do so using only a subset of three tools: the question answering system, the calculator, and the Wikipedia search engine. Apart from this, we follow the experimental setup described in Section 4.1.</p><p>Figure <ref type="figure">4</ref> shows that the ability to leverage the provided tools only emerges at around 775M parameters: smaller models achieve similar performance both with and without tools. An exception to this is the Wikipedia search engine used mostly for QA benchmarks; we hypothesize that this is because the API is comparably easy to use. While models become better at solving tasks without API calls as they grow in size, their ability to make good use of the provided API improves at the same time. As a consequence, there remains a large gap between predictions with and without API calls even for our biggest model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Analysis</head><p>Decoding Strategy We investigate the effect of our modified decoding strategy introduced in Section 4.2, where instead of always generating the Figure <ref type="figure">4</ref>: Average performance on LAMA, our math benchmarks and our QA benchmarks for GPT-2 models of different sizes and GPT-J finetuned with our approach, both with and without API calls. While API calls are not helpful to the smallest models, larger models learn how to make good use of them. Even for bigger models, the gap between model predictions with and without API calls remains high.</p><p>most likely token, we generate the &lt;API&gt; token if it is one of the k most likely tokens. Table <ref type="table">9</ref> shows performance on the T-REx subset of LAMA and on WebQS for different values of k. As expected, increasing k leads to the model doing API calls for more examples -from 40.3% and 8.5% with k = 1 (i.e., regular greedy decoding) to 98.1% and 100% for k = 10. While for T-REx, there is already a clear improvement in performance with greedy decoding, on WebQS our model only starts to make a substantial number of API calls as we slightly increase k. Interestingly, for k = 1 the model is calibrated to some extent: It decides to call APIs for examples that it would perform particularly badly on without making API calls. This can be seen from the fact that performance on examples where it decides not to make an API call (44.3 and 19.9) is higher than average performance if no API calls are made at all (34.9 and 18.9). However, this calibration is lost for higher values of k.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Data Quality</head><p>We qualitatively analyze some API calls generated with our approach for different APIs. Table <ref type="table" target="#tab_10">10</ref> shows some examples of texts from CCNet augmented with API calls, as well as the corresponding score L - i -L + i that is used as a filtering criterion, and whether the API calls made by the model are intuitively useful in the given context. As can be seen, high values of L - i -L + i typically correspond to useful API calls, whereas low values correspond to API calls that do not provide any information that is useful for predicting future tokens. There are some exceptions, e.g., an API call for "Fast train success" in the fourth example that does not give any relevant information but still reduces perplexity. However, some amount of noise in the API calls that are not filtered can actually be useful as it forces the model finetuned on C * to not always blindly follow the results of each call it makes.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Related Work</head><p>Language Model Pretraining There are various approaches that augment language models with some form of additional textual information during pretraining, including various forms of metadata <ref type="bibr" target="#b23">(Keskar et al., 2019)</ref>, HTML tags <ref type="bibr" target="#b0">(Aghajanyan et al., 2021)</ref>, Wikipedia markup <ref type="bibr" target="#b16">(Schick et al., 2022)</ref>, or related texts obtained from an information retrieval system <ref type="bibr" target="#b14">(Guu et al., 2020;</ref><ref type="bibr" target="#b4">Borgeaud et al., 2021;</ref><ref type="bibr">Izacard et al., 2022)</ref>. For all of these  approaches, additional information is always provided, regardless of whether it is helpful or not. In contrast, Toolformer learns for itself to explicitly asks for the right information.</p><p>Tool Use Several approaches aim to equip LMs with the ability to use external tools such as search engines <ref type="bibr" target="#b25">(Komeili et al., 2022;</ref><ref type="bibr">Thoppilan et al., 2022;</ref><ref type="bibr" target="#b28">Lazaridou et al., 2022;</ref><ref type="bibr" target="#b46">Shuster et al., 2022;</ref><ref type="bibr" target="#b52">Yao et al., 2022)</ref>, web browsers <ref type="bibr" target="#b35">(Nakano et al., 2021)</ref>, calculators <ref type="bibr" target="#b8">(Cobbe et al., 2021;</ref><ref type="bibr">Thoppilan et al., 2022)</ref>, translation systems <ref type="bibr">(Thoppilan et al., 2022)</ref> and Python interpreters <ref type="bibr">(Gao et al., 2022)</ref>. The way these models learn to use tools can roughly be divided into two approaches: Either they rely on large amounts of human supervision <ref type="bibr" target="#b25">(Komeili et al., 2022;</ref><ref type="bibr" target="#b35">Nakano et al., 2021;</ref><ref type="bibr">Thoppilan et al., 2022)</ref> or they work by prompting the language model in a few-shot setup tailored towards a specific task where it is known a priori which tools needs to be used <ref type="bibr">(Gao et al., 2022;</ref><ref type="bibr" target="#b28">Lazaridou et al., 2022;</ref><ref type="bibr" target="#b52">Yao et al., 2022)</ref>. In contrast, the self-supervised nature of Toolformer enables it to learn how and when to use tools without requiring a specific prompt that shows task-specific examples of how a tool could be used. Perhaps most closely related to our work is TALM <ref type="bibr" target="#b36">(Parisi et al., 2022)</ref>, an approach that uses a similar self-supervised objective for teaching a model to use a calculator and a search engine, but explores this only in settings where a model is finetuned for downstream tasks.</p><p>Bootstrapping The idea of using self-training and bootstrapping techniques to improve models has been investigated in various contexts, ranging from word sense disambiguation <ref type="bibr" target="#b53">(Yarowsky, 1995)</ref>, relation extraction <ref type="bibr" target="#b5">(Brin, 1999;</ref><ref type="bibr" target="#b1">Agichtein and Gravano, 2000)</ref>, parsing <ref type="bibr" target="#b32">(McClosky et al., 2006;</ref><ref type="bibr" target="#b41">Reichart and Rappoport, 2007)</ref>, sequence generation <ref type="bibr" target="#b15">(He et al., 2020)</ref>, few-shot text classi-fication <ref type="bibr">(Schick and Sch?tze, 2021a)</ref> and retrieval <ref type="bibr" target="#b17">(Izacard and Grave, 2021)</ref> to reasoning <ref type="bibr" target="#b54">(Zelikman et al., 2022)</ref>. In a similar spirit to these approaches, Toolformer is trained on its own predictions after applying a perplexity-based filtering step.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">Limitations</head><p>While our approach enables LMs to learn how to use a variety of tools in a self-supervised way, there are some clear limitations to what can be achieved with our method in its current form. One such limitation is the inability of Toolformer to use tools in a chain (i.e., using the output of one tool as an input for another tool). This is due to the fact that API calls for each tool are generated independently; as a consequence, there are no examples of chained tool use in the finetuning dataset. Our current approach also does not allow the LM to use a tool in an interactive way -especially for tools such as search engines, that could potentially return hundreds of different results, enabling a LM to browse through these results or to refine its search query in a similar spirit to <ref type="bibr" target="#b35">Nakano et al. (2021)</ref> can be crucial for certain applications. Beyond this, we found models trained with Toolformer to often be sensitive to the exact wording of their input when deciding whether or not to call an API; this is perhaps unsurprising given that LMs are known to be very sensitive to the prompt they are provided with in both zeroand few-shot settings <ref type="bibr" target="#b20">(Jiang et al., 2020;</ref><ref type="bibr">Schick and Sch?tze, 2021a)</ref>. Depending on the tool, our method is also very sample-inefficient; for example, processing more than a million documents results in only a few thousand examples of useful calls to the calculator API. A potential solution to this problem might be to iteratively apply our approach, similar to how this is done in related bootstrapping approaches <ref type="bibr">(Schick and Sch?tze, 2021a;</ref><ref type="bibr" target="#b17">Izacard and Grave, 2021;</ref><ref type="bibr" target="#b36">Parisi et al., 2022)</ref>. Finally, when deciding whether or not to make an API call, Toolformer currently does not take into account the tool-dependent, computational cost incurred from making an API call.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8">Conclusion</head><p>We have introduced Toolformer, a language model that learns in a self-supervised way how to use different tools such as search engines, calculators, and translation systems via simple API calls. This is done by finetuning on a large number of sampled API calls that are filtered based on whether they reduce perplexity on future tokens. Toolformer considerably improves zero-shot performance of a 6.7B parameter GPT-J model, enabling it to even outperform a much larger GPT-3 model on a range of different downstream tasks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A API Details</head><p>When sampling and filtering API calls, by default we use values of ? s = 0.05 and ? f = 1.0 -i.e., we only make API calls at positions where the probability of the &lt;API&gt; token is at least 5%, and we keep API calls if they reduce the loss by at least 1.0. We only keep the top k = 5 such positions and sample up to m = 5 API calls for each position identified in a piece of text. Due to the heuristic filtering described below, we generate API calls for the calculator and machine translation system on only a small subset of C; to compensate for this, we set ? s = 0.0, k = 20 and m = 10 for these tools. As the resulting sets of API calls are still comparably small, we additionally set ? f = 0.5.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.1 Implementation</head><p>Question Answering We use the Atlas model of <ref type="bibr">Izacard et al. (2022)</ref> finetuned on Natural Questions <ref type="bibr" target="#b27">(Kwiatkowski et al., 2019)</ref> as our question answering system. For creating C * we use Atlaslarge, enabling us to efficiently process millions of API calls; during inference, we use the larger Atlas-xxl model.</p><p>Calculator Our calculator is based on a simple Python script and only supports the operators "+", "-", " * ", and "/". It does not return any result for syntactically invalid equations. For sampling API calls, we apply heuristic filters to our subset of CCNet and only process documents that either (i) contain at least three numbers within a window of 100 tokens, where one of these numbers is the result of applying a mathematical operation to the other two, (ii) contain one of the sequences "=", "equals", "equal to", "total of", "average of" followed by a number, or (iii) contain at least three numbers; for texts that only match the last criterion, we only keep a random subset of 1%.</p><p>Calendar For creating our dataset C * , we operate under the assumption that the calendar date in such cases should be the date that the document was created. We approximate this by extracting the date from the URL, if it is present. We filter out texts for which a date cannot be extracted, leaving around 18% of the documents.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Machine Translation</head><p>For both training and inference, we use the 600M parameter NLLB <ref type="bibr">(Costajuss? et al., 2022)</ref> as our machine translation (MT) model. The source language is automatically detected using the fastText classifier <ref type="bibr" target="#b22">(Joulin et al., 2016)</ref>, while the target language is always set to English. Since most of the CCNet dataset is in English, we filter out the parts that contain only English text before generating API calls. More specifically, we only keep those paragraphs which contain text chunks in a language other than English preceded and followed by English text. We use text chunks of size 10 tokens. To determine whether the middle text chunk is in a language different than English we again use the fastText classifier with a confidence greater than 0.8. We also filter out any text chunks that contain only numbers or special symbols. This filtering mechanism allows us to generate data more efficiently by focusing our API call generations in places where the MT tool is likely to be helpful. After generating the MT API calls, we additionally remove from our training set those where the input to the MT tool appears after the API call but not before it. While during data generation the model can look ahead to generate API calls, this is not possible at inference time, so we want to dissuade the model from calling the API in such cases.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.2 Prompts</head><p>Below, we list the prompts used to sample API calls for each tool considered. Question Answering We use the following prompt for the question answering tool: Calculator We use the following prompt for the calculator:</p><p>Your task is to add calls to a Calculator API to a piece of text.</p><p>The calls should help you get information required to complete the text. You can call the API by writing "[Calculator(expression)]" where "expression" is the expression to be computed. Here are some examples of API calls: Input: The number in the next term is 18 + 12 x 3 = 54. Output: The number in the next term is 18 + 12 x 3 = [Calculator(18 + 12 * 3)] 54.</p><p>Input: The population is 658,893 people. This is 11.4% of the national average of 5,763,868 people. Output: The population is 658,893 people. This is 11.4% of the national average of [Calculator(658,893 / 11.4%)] 5,763,868 people.</p><p>Input: A total of 252 qualifying matches were played, and 723 goals were scored (an average of 2.87 per match). This is three times less than the 2169 goals last year. Output: A total of 252 qualifying matches were played, and 723 goals were scored (an average of [Calculator(723 / 252)] 2.87 per match). This is twenty goals more than the ] 703 goals last year.</p><p>Input: I went to Paris in 1994 and stayed there until 2011, so in total, it was 17 years. Output: I went to Paris in 1994 and stayed there until 2011, so in total, it was <ref type="bibr">[Calculator(2011</ref><ref type="bibr">[Calculator( -1994))</ref>] 17 years.</p><p>Input: From this, we have 4 * 30 minutes = 120 minutes. Output: From this, we have 4 * 30 minutes = [Calculator(4 * 30)] 120 minutes.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Input: x Output:</head><p>Wikipedia Search We use the following prompt for the Wikipedia search tool:</p><p>Your task is to complete a given piece of text. You can use a Wikipedia Search API to look up information. You can do so by writing "[WikiSearch(term)]" where "term" is the search term you want to look up. Here are some examples of API calls: Input: The colors on the flag of Ghana have the following meanings: red is for the blood of martyrs, green for forests, and gold for mineral wealth. Output: The colors on the flag of Ghana have the following meanings: red is for [WikiSearch("Ghana flag red meaning")] the blood of martyrs, green for forests, and gold for mineral wealth. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B Toolformer Training</head><p>We use up to 25k examples per API. Max sequence length 1,024. Effective batch size of 128. All models are trained using DeepSpeed's <ref type="bibr">ZeRO-3 (Rasley et al., 2020)</ref>. We used 8 NVIDIA A100 40GB GPUs with BF16. Training up to 2k steps, where we evaluate PPL on a small development set from CCNet containing 1,000 examples every 500 steps. We pick the checkpoint that performs best.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C Zero-Shot Prompts</head><p>C.1 LAMA and TEMPLAMA For both LAMA and TEMPLAMA, given an input text x, we use the following prompt: Please complete the following text so that it is factually correct: x.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C.2 Math Benchmarks</head><p>For all math benchmarks, given a context x and a question q, our prompt is: x q The answer is.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C.3 Question Answering</head><p>For all question answering datasets, including DATESET, we simply prefix the question with Answer the following question:. We append a question mark if the question does not already end with one.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C.4 Multilingual Question Answering</head><p>For MLQA, given a context x and a question q, our prompt is:</p><p>Your task is  to answer a question based on the following paragraph: x Now answer the following question in English: q.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D DATESET</head><p>DATESET is created by first randomly selecting 500 "current dates". For each current date, another relatively past/future date is randomly selected within a four-year range, and the two dates are used to fill the query templates in Table <ref type="table" target="#tab_13">11</ref>. An example of one such query using the first template would be, "How many days ago was August 14, 2020?" If called, the Calendar tool would return the presumed current date (e.g., "Today is Sunday, November 20, 2020").</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>The</head><label></label><figDesc>New England Journal of Medicine is a registered trademark of [QA("Who is the publisher of The New England Journal of Medicine?") ? Massachusetts Medical Society] the MMS. Out of 1400 participants, 400 (or [Calculator(400 / 1400) ? 0.29] 29%) passed the test. The name derives from "la tortuga", the Spanish word for [MT("tortuga") ? turtle] turtle. The Brown Act is California's law [WikiSearch("Brown Act") ? The Ralph M. Brown Act is an act of the California State Legislature that guarantees the public's right to attend and participate in meetings of local legislative bodies.] that requires legislative bodies, like city councils, to hold their meetings open to the public.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 1 :</head><label>1</label><figDesc>Figure1: Exemplary predictions of Toolformer. The model autonomously decides to call different APIs (from top to bottom: a question answering system, a calculator, a machine translation system, and a Wikipedia search engine) to obtain information that is useful for completing a piece of text.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: An exemplary prompt P (x) used to generate API calls for the question answering tool.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head></head><label></label><figDesc>Input: But what are the risks during production of nanomaterials? Some nanomaterials may give rise to various kinds of lung damage. Output: But what are the risks during production of nanomaterials? [WikiSearch("nanomaterial production risks")] Some nanomaterials may give rise to various kinds of lung damage. Input: Metformin is the first-line drug for patients with type 2 diabetes and obesity. Output: Metformin is the first-line drug for [WikiSearch("Metformin first-line drug")] patients with type 2 diabetes and obesity. Input: x Output: Machine Translation We use the following prompt for the machine translation tool: Your task is to complete a given piece of text by using a Machine Translation API. You can do so by writing "[MT(text)]" where text is the text to be translated into English. Here are some examples: Input: He has published one book: O homem suprimido ("The Supressed Man") Output: He has published one book: O homem suprimido [MT(O homem suprimido)] ("The Supressed Man")Input: In Morris de Jonge's Jeschuah, der klassische j?dische Mann, there is a description of a Jewish writer Output: In Morris de Jonge's Jeschuah, der klassische j?dische Mann [MT(der klassische j?dische Mann)], there is a description of a Jewish writerInput: ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? a plane ofreference Gaochun is one of seven districts of the provincial capital Nanjing Output: [MT(????????????? ??? ? ? ?)] a plane of reference Gaochun is one of seven districts of the provincial capital Nanjing Input: x Output: Calendar We use the following prompt for the calendar tool: Your task is to add calls to a Calendar API to a piece of text. The API calls should help you get information required to complete the text. You can call the API by writing "[Calendar()]" Here are some examples of API calls: Input: Today is the first Friday of the year. Output: Today is the first [Calendar()] Friday of the year. Input: The president of the United States is Joe Biden. Output: The president of the United States is [Calendar()] Joe Biden. Input: The current day of the week is Wednesday. Output: The current day of the week is [Calendar()] Wednesday. Input: The number of days from now until Christmas is 30. Output: The number of days from now until Christmas is [Calendar()] 30. Input: The store is never open on the weekend, so today it is closed. Output: The store is never open on the weekend, so today [Calendar()] it is closed. Input: x Output:</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 1 :</head><label>1</label><figDesc>Spin fishing is distinguished between fly fishing and bait cast fishing by the type of rod and reel used. There are two types of reels used when spin fishing, the open faced reel and the closed faced reel. Examples of inputs and outputs for all APIs used.</figDesc><table><row><cell>API Name</cell><cell cols="2">Example Input</cell><cell>Example Output</cell></row><row><cell cols="3">Question Answering Where was the Knights</cell><cell>New Haven, Connecticut</cell></row><row><cell></cell><cell cols="2">of Columbus founded?</cell><cell></cell></row><row><cell cols="4">Wikipedia Search Spin fishing &gt; Calculator Fishing Reel Types 27 + 4 * 2 35</cell></row><row><cell>Calendar</cell><cell>?</cell><cell></cell><cell>Today is Monday, January 30, 2023.</cell></row><row><cell cols="3">Machine Translation s?ret? nucl?aire</cell><cell>nuclear safety</cell></row><row><cell></cell><cell cols="3">Number of Examples</cell></row><row><cell>API</cell><cell cols="3">? f = 0.5 ? f = 1.0 ? f = 2.0</cell></row><row><cell>Question Answering</cell><cell>51,987</cell><cell>18,526</cell><cell>5,135</cell></row><row><cell>Wikipedia Search</cell><cell>207,241</cell><cell>60,974</cell><cell>13,944</cell></row><row><cell>Calculator</cell><cell>3,680</cell><cell>994</cell><cell>138</cell></row><row><cell>Calendar</cell><cell>61,811</cell><cell>20,587</cell><cell>3,007</cell></row><row><cell>Machine Translation</cell><cell>3,156</cell><cell>1,034</cell><cell>229</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 2 :</head><label>2</label><figDesc>Number of examples with API calls in C</figDesc><table /><note><p>* for different values of our filtering threshold ? f .</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 4 :</head><label>4</label><figDesc></figDesc><table><row><cell>Model</cell><cell cols="3">SQuAD Google-RE T-REx</cell></row><row><cell>GPT-J</cell><cell>17.8</cell><cell>4.9</cell><cell>31.9</cell></row><row><cell>GPT-J + CC</cell><cell>19.2</cell><cell>5.6</cell><cell>33.2</cell></row><row><cell>Toolformer (disabled)</cell><cell>22.1</cell><cell>6.3</cell><cell>34.9</cell></row><row><cell>Toolformer</cell><cell>33.8</cell><cell>11.5</cell><cell>53.5</cell></row><row><cell>OPT (66B)</cell><cell>21.6</cell><cell>2.9</cell><cell>30.1</cell></row><row><cell>GPT-3 (175B)</cell><cell>26.8</cell><cell>7.0</cell><cell>39.8</cell></row><row><cell cols="4">Table 3: Results on subsets of LAMA. Toolformer uses</cell></row><row><cell cols="4">the question answering tool for most examples, clearly</cell></row><row><cell cols="4">outperforming all baselines of the same size and achiev-</cell></row><row><cell cols="3">ing results competitive with GPT-3 (175B).</cell><cell></cell></row><row><cell>Model</cell><cell cols="3">ASDiv SVAMP MAWPS</cell></row><row><cell>GPT-J</cell><cell>7.5</cell><cell>5.2</cell><cell>9.9</cell></row><row><cell>GPT-J + CC</cell><cell>9.6</cell><cell>5.0</cell><cell>9.3</cell></row><row><cell>Toolformer (disabled)</cell><cell>14.8</cell><cell>6.3</cell><cell>15.0</cell></row><row><cell>Toolformer</cell><cell>40.4</cell><cell>29.4</cell><cell>44.0</cell></row><row><cell>OPT (66B)</cell><cell>6.0</cell><cell>4.9</cell><cell>7.9</cell></row><row><cell>GPT-3 (175B)</cell><cell>14.0</cell><cell>10.0</cell><cell>19.8</cell></row></table><note><p>Results for various benchmarks requiring mathematical reasoning. Toolformer makes use of the calculator tool for most examples, clearly outperforming even OPT (66B) and GPT-3 (175B).</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 5 :</head><label>5</label><figDesc>Results for various question answering dataset. Using the Wikipedia search tool for most examples, Toolformer clearly outperforms baselines of the same size, but falls short of GPT-3 (175B).</figDesc><table><row><cell></cell><cell cols="3">WebQS NQ TriviaQA</cell></row><row><cell>GPT-J</cell><cell>18.5</cell><cell>12.8</cell><cell>43.9</cell></row><row><cell>GPT-J + CC</cell><cell>18.4</cell><cell>12.2</cell><cell>45.6</cell></row><row><cell>Toolformer (disabled)</cell><cell>18.9</cell><cell>12.6</cell><cell>46.7</cell></row><row><cell>Toolformer</cell><cell>26.3</cell><cell>17.7</cell><cell>48.8</cell></row><row><cell>OPT (66B)</cell><cell>18.6</cell><cell>11.4</cell><cell>45.7</cell></row><row><cell>GPT-3 (175B)</cell><cell>29.0</cell><cell>22.6</cell><cell>65.9</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head></head><label></label><figDesc>that were not used during training. Perplexities of various models are shown in Table8. As one would expect, finetuning on CCNet leads to slightly improved performance on a different CC-Net subset, but it slightly deteriorates performance on WikiText, presumably because the original pre-</figDesc><table><row><cell>Model</cell><cell cols="2">WikiText CCNet</cell></row><row><cell>GPT-J</cell><cell>9.9</cell><cell>10.6</cell></row><row><cell>GPT-J + CC</cell><cell>10.3</cell><cell>10.5</cell></row><row><cell>Toolformer (disabled)</cell><cell>10.3</cell><cell>10.5</cell></row><row><cell cols="3">Table 8: Perplexities of different models on WikiText</cell></row><row><cell cols="3">and our validation subset of CCNet. Adding API calls</cell></row><row><cell cols="3">comes without a cost in terms of perplexity for lan-</cell></row><row><cell cols="2">guage modeling without any API calls.</cell><cell></cell></row><row><cell cols="3">training data for GPT-J is more similar to Wiki-</cell></row><row><cell cols="3">Text than our randomly selected subset of CCNet.</cell></row><row><cell cols="3">Most importantly, however, training on C  *  (our</cell></row><row><cell cols="3">dataset annotated with API calls) does not lead to</cell></row><row><cell cols="3">an increase in perplexity compared to training on</cell></row><row><cell cols="3">C when API calls are disabled at inference time. 8</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head></head><label></label><figDesc>UsefulThe Flodden Window (a war memorial dedicated to The Middleton Archers), in the Grade I-listed Church of St Leonard in Middleton is said to be the oldest war memorial in the United Kingdom. &lt;API&gt; WikiSearch(War memorial Flodden) ? Battle of Flodden &gt; Commemoration &gt; The stained-glass Flodden Window in Middleton Parish Church[. . . ]  was constructed by Sir Richard Assheton in memory of the Battle of Flodden and the archers from Middleton who fought in it. &lt;/API&gt; Sir Richard Assheton of Middleton (who built St Leonard) was granted knighthood [. . . ] Home / Featured / Catch this fast train to success! &lt;API&gt; WikiSearch(Fast train success) ? Fast Train &gt; It also peaked at #23 on the Canadian CHUM singles chart, on June 26, 1971. The success of this single established Myles Goodwyn as band's main songwriter, and made it possible for April Wine to record a second album. &lt;/API&gt; Don't wait weeks and [. . .] 0.92 Os Melhores Escolas em Jersey 2020 &lt;API&gt; MT(Os Melhores Escolas em Jersey) ? The Best Schools in Jersey &lt;/API&gt; On this page you can search for Universities, Colleges and Business schools in Jersey 0.70 Enjoy these pictures from the &lt;API&gt; Calendar() ? Today is Friday, April 19, 2013. &lt;/API&gt; Easter Egg Hunt.</figDesc><table><row><cell>Example</cell><cell>L -i -L + i</cell></row><row><cell></cell><cell>5.49</cell></row><row><cell>Note: The WL will be open on Friday, &lt;API&gt; Calendar() ? Today is Thursday, March 9, 2017. &lt;/API&gt; March 10, and Sunday, March 19 for regular hours.</cell><cell>2.11</cell></row><row><cell>The Nile has an approximate length of &lt;API&gt; QA(What is the approximate length of the Nile?) ? 6,853 km &lt;/API&gt; 6,853 kilometers, the White Nile being its main source.</cell><cell>2.08</cell></row><row><cell>If Venus had an atmosphere similar to Earth's then you would expect Venus' mean temperature to be</cell><cell></cell></row><row><cell>499 K (1.74 x 287) rather than 735 K which is &lt;API&gt; Calculator(735 / 499) ? 1.47 &lt;/API&gt; 1.47</cell><cell>1.59</cell></row><row><cell>(735 / 499) times hotter than it should be.</cell><cell></cell></row><row><cell>You are here:</cell><cell></cell></row></table><note><p>0.33 85 patients (23%) were hospitalised alive and admitted to a hospital ward. Of them, &lt;API&gt; Calculator(85 / 23) ? 3.70 &lt;/API&gt; 65% had a cardiac aetiology [. . .] -0.02 But hey, after the &lt;API&gt; Calendar() ? Today is Saturday, June 25, 2011. &lt;/API&gt; Disneyland fiasco with the fire drill, I think it's safe to say Chewey won't let anyone die in a fire. -0.41 The last time I was with &lt;API&gt; QA(Who was last time I was with?) ? The Last Time &lt;/API&gt; him I asked what he likes about me and he said he would tell me one day. -1.23</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_10"><head>Table 10 :</head><label>10</label><figDesc>Examples of API calls for different tools, sorted by the value of L - i -L + i that is used as a filtering criterion. High values typically correspond to API calls that are intuitively useful for predicting future tokens.</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_13"><head>Table 11 :</head><label>11</label><figDesc>Templates used to create DATESET where a current_date is randomly selected. For each cur-rent_date, a random past_date and future_date is generated and used to fill each template, if relevant. The federal holidays in the United States (e.g., Thanksgiving) were used in the templates involving holidays.</figDesc><table /></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0"><p>In practice, we use the token sequences " [", "]" and "-&gt;" to represent "&lt;API&gt;", "&lt;/API&gt;" and "?", respectively. This enables our approach to work without modifying the existing LM's vocabulary. For reasons of readability, we still refer to them as "&lt;API&gt;", "&lt;/API&gt;" and "?" throughout this section.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_1"><p>We discard all examples where M does not generate the &lt;/API&gt; token.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3" xml:id="foot_2"><p>We provide e(ci, ri) as a prefix instead of inserting it at position i because M is not yet finetuned on any examples containing API calls, so inserting it in the middle of x would interrupt the flow and not align with patterns in the pretraining corpus, thus hurting perplexity.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4" xml:id="foot_3"><p>While this filtering alters the distribution of training examples, we assume that the remaining examples are close enough to the original distribution so that M 's language modeling abilities remain unaffected. This assumption is empirically validated in Section 4.3.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="5" xml:id="foot_4"><p>This is achieved by manually setting the probability of the &lt;API&gt; token to 0.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="6" xml:id="foot_5"><p>We use the original davinci variant that is not finetuned on any instructions.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="8" xml:id="foot_6"><p>We do not evaluate the perplexity of Toolformer with API calls enabled as computing the probability pM (xt | x1, . . . , xt-1) of token xt given x1, . . . , xt-1 would require marginalizing over all potential API calls that the model could make at position t, which is intractable.</p></note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">Htlm: Hyper-text pre-training and prompting of language models</title>
		<author>
			<persName><forename type="first">Armen</forename><surname>Aghajanyan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dmytro</forename><surname>Okhonko</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mike</forename><surname>Lewis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mandar</forename><surname>Joshi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hu</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gargi</forename><surname>Ghosh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Luke</forename><surname>Zettlemoyer</surname></persName>
		</author>
		<idno type="DOI">10.48550/ARXIV.2107.06955</idno>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Snowball: Extracting relations from large plain-text collections</title>
		<author>
			<persName><forename type="first">Eugene</forename><surname>Agichtein</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Luis</forename><surname>Gravano</surname></persName>
		</author>
		<idno type="DOI">10.1145/336597.336644</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Fifth ACM Conference on Digital Libraries, DL &apos;00</title>
		<meeting>the Fifth ACM Conference on Digital Libraries, DL &apos;00<address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computing Machinery</publisher>
			<date type="published" when="2000">2000</date>
			<biblScope unit="page" from="85" to="94" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">Modern information retrieval</title>
		<author>
			<persName><forename type="first">Ricardo</forename><surname>Baeza-Yates</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Berthier</forename><surname>Ribeiro-Neto</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1999">1999</date>
			<publisher>ACM press</publisher>
			<biblScope unit="volume">463</biblScope>
			<pubPlace>New York</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Semantic parsing on Freebase from question-answer pairs</title>
		<author>
			<persName><forename type="first">Jonathan</forename><surname>Berant</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrew</forename><surname>Chou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Roy</forename><surname>Frostig</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Percy</forename><surname>Liang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2013 Conference on Empirical Methods in Natural Language Processing<address><addrLine>Seattle, Washington, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2013">2013</date>
			<biblScope unit="page" from="1533" to="1544" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title/>
		<author>
			<persName><forename type="first">Sebastian</forename><surname>Borgeaud</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Arthur</forename><surname>Mensch</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jordan</forename><surname>Hoffmann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Trevor</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Eliza</forename><surname>Rutherford</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Katie</forename><surname>Millican</surname></persName>
		</author>
		<author>
			<persName><forename type="first">George</forename><surname>Van Den Driessche</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jean-Baptiste</forename><surname>Lespiau</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bogdan</forename><surname>Damoc</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aidan</forename><surname>Clark</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Diego</forename><surname>De Las</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aurelia</forename><surname>Casas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jacob</forename><surname>Guy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Roman</forename><surname>Menick</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tom</forename><surname>Ring</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Saffron</forename><surname>Hennigan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Loren</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chris</forename><surname>Maggiore</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Albin</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andy</forename><surname>Cassirer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michela</forename><surname>Brock</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Geoffrey</forename><surname>Paganini</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Oriol</forename><surname>Irving</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Simon</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Karen</forename><surname>Osindero</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jack</forename><forename type="middle">W</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Erich</forename><surname>Rae</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Laurent</forename><surname>Elsen</surname></persName>
		</author>
		<author>
			<persName><surname>Sifre</surname></persName>
		</author>
		<idno type="DOI">10.48550/ARXIV.2112.04426</idno>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
	<note>Improving language models by retrieving from trillions of tokens</note>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Extracting patterns and relations from the world wide web</title>
		<author>
			<persName><forename type="first">Sergey</forename><surname>Brin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The World Wide Web and Databases</title>
		<meeting><address><addrLine>Berlin, Heidelberg; Berlin Heidelberg</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="1999">1999</date>
			<biblScope unit="page" from="172" to="183" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Alec Radford, Ilya Sutskever, and Dario Amodei</title>
		<author>
			<persName><forename type="first">Tom</forename><surname>Brown</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Benjamin</forename><surname>Mann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nick</forename><surname>Ryder</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Melanie</forename><surname>Subbiah</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jared</forename><forename type="middle">D</forename><surname>Kaplan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Prafulla</forename><surname>Dhariwal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Arvind</forename><surname>Neelakantan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pranav</forename><surname>Shyam</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Girish</forename><surname>Sastry</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Amanda</forename><surname>Askell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sandhini</forename><surname>Agarwal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ariel</forename><surname>Herbert-Voss</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gretchen</forename><surname>Krueger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tom</forename><surname>Henighan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rewon</forename><surname>Child</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aditya</forename><surname>Ramesh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Daniel</forename><surname>Ziegler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jeffrey</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Clemens</forename><surname>Winter</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chris</forename><surname>Hesse</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mark</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Eric</forename><surname>Sigler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mateusz</forename><surname>Litwin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Scott</forename><surname>Gray</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Benjamin</forename><surname>Chess</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jack</forename><surname>Clark</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christopher</forename><surname>Berner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sam</forename><surname>Mccandlish</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<publisher>Curran Associates, Inc</publisher>
			<date type="published" when="2020">2020</date>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page" from="1877" to="1901" />
		</imprint>
	</monogr>
	<note>Language models are few-shot learners</note>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title/>
		<author>
			<persName><forename type="first">Aakanksha</forename><surname>Chowdhery</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sharan</forename><surname>Narang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jacob</forename><surname>Devlin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Maarten</forename><surname>Bosma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gaurav</forename><surname>Mishra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Adam</forename><surname>Roberts</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Paul</forename><surname>Barham</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hyung</forename><forename type="middle">Won</forename><surname>Chung</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Charles</forename><surname>Sutton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sebastian</forename><surname>Gehrmann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Parker</forename><surname>Schuh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kensen</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sasha</forename><surname>Tsvyashchenko</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Joshua</forename><surname>Maynez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Abhishek</forename><surname>Rao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Parker</forename><surname>Barnes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yi</forename><surname>Tay</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Noam</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Emily</forename><surname>Vinodkumar Prabhakaran</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nan</forename><surname>Reif</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ben</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Reiner</forename><surname>Hutchinson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">James</forename><surname>Pope</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jacob</forename><surname>Bradbury</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><surname>Austin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Guy</forename><surname>Isard</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pengcheng</forename><surname>Gur-Ari</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Toju</forename><surname>Yin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Anselm</forename><surname>Duke</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sanjay</forename><surname>Levskaya</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sunipa</forename><surname>Ghemawat</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Henryk</forename><surname>Dev</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xavier</forename><surname>Michalewski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vedant</forename><surname>Garcia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kevin</forename><surname>Misra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Liam</forename><surname>Robinson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Denny</forename><surname>Fedus</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Daphne</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><surname>Ippolito</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hyeontaek</forename><surname>Luan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Barret</forename><surname>Lim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alexander</forename><surname>Zoph</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ryan</forename><surname>Spiridonov</surname></persName>
		</author>
		<author>
			<persName><surname>Sepassi ; Oleksandr</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Katherine</forename><surname>Polozov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zongwei</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xuezhi</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Brennan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mark</forename><surname>Saeta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Orhan</forename><surname>Diaz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michele</forename><surname>Firat</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jason</forename><surname>Catasta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kathy</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Douglas</forename><surname>Meier-Hellstern</surname></persName>
		</author>
		<author>
			<persName><surname>Eck</surname></persName>
		</author>
		<idno type="DOI">10.48550/ARXIV.2204.02311</idno>
		<editor>David Dohan, Shivani Agrawal, Mark Omernick, Andrew M. Dai, Thanumalayan Sankaranarayana Pillai, Marie Pellat, Aitor Lewkowycz, Erica Moreira, Rewon Child,</editor>
		<imprint>
			<pubPlace>Jeff Dean, Slav Petrov</pubPlace>
		</imprint>
	</monogr>
	<note>and Noah Fiedel. 2022. Palm: Scaling language modeling with pathways</note>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<author>
			<persName><forename type="first">Karl</forename><surname>Cobbe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vineet</forename><surname>Kosaraju</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mohammad</forename><surname>Bavarian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mark</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Heewoo</forename><surname>Jun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lukasz</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Matthias</forename><surname>Plappert</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jerry</forename><surname>Tworek</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jacob</forename><surname>Hilton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Reiichiro</forename><surname>Nakano</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2110.14168</idno>
		<title level="m">Training verifiers to solve math word problems</title>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<author>
			<persName><forename type="first">James</forename><surname>Marta R Costa-Juss?</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Onur</forename><surname>Cross</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Maha</forename><surname>?elebi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kenneth</forename><surname>Elbayad</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kevin</forename><surname>Heafield</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Elahe</forename><surname>Heffernan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Janice</forename><surname>Kalbassi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Daniel</forename><surname>Lam</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jean</forename><surname>Licht</surname></persName>
		</author>
		<author>
			<persName><surname>Maillard</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2207.04672</idno>
		<title level="m">No language left behind: Scaling human-centered machine translation</title>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">BERT: Pre-training of deep bidirectional transformers for language understanding</title>
		<author>
			<persName><forename type="first">Jacob</forename><surname>Devlin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ming-Wei</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kenton</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kristina</forename><surname>Toutanova</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/N19-1423</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</title>
		<title level="s">Long and Short Papers</title>
		<meeting>the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies<address><addrLine>Minneapolis, Minnesota</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="4171" to="4186" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Time-aware language models as temporal knowledge bases</title>
		<author>
			<persName><forename type="first">Bhuwan</forename><surname>Dhingra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jeremy</forename><forename type="middle">R</forename><surname>Cole</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Julian</forename><surname>Martin Eisenschlos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Daniel</forename><surname>Gillick</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jacob</forename><surname>Eisenstein</surname></persName>
		</author>
		<author>
			<persName><forename type="first">William</forename><forename type="middle">W</forename><surname>Cohen</surname></persName>
		</author>
		<idno type="DOI">10.1162/tacl_a_00459</idno>
	</analytic>
	<monogr>
		<title level="j">Transactions of the Association for Computational Linguistics</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="page" from="257" to="273" />
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<author>
			<persName><forename type="first">Leo</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stella</forename><surname>Biderman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sid</forename><surname>Black</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Laurence</forename><surname>Golding</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Travis</forename><surname>Hoppe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Charles</forename><surname>Foster</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jason</forename><surname>Phang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Horace</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Anish</forename><surname>Thite</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Noa</forename><surname>Nabeshima</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2101.00027</idno>
		<title level="m">The pile: An 800gb dataset of diverse text for language modeling</title>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">Yiming Yang, Jamie Callan, and Graham Neubig. 2022. Pal: Program-aided language models</title>
		<author>
			<persName><forename type="first">Luyu</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aman</forename><surname>Madaan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shuyan</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Uri</forename><surname>Alon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pengfei</forename><surname>Liu</surname></persName>
		</author>
		<idno type="DOI">10.48550/ARXIV.2211.10435</idno>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title/>
		<author>
			<persName><forename type="first">Kelvin</forename><surname>Guu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kenton</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zora</forename><surname>Tung</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Panupong</forename><surname>Pasupat</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ming-Wei</forename><surname>Chang</surname></persName>
		</author>
		<idno type="DOI">10.48550/ARXIV.2002.08909</idno>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note>Realm: Retrievalaugmented language model pre-training</note>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Revisiting self-training for neural sequence generation</title>
		<author>
			<persName><forename type="first">Junxian</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiatao</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiajun</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Marc'aurelio</forename><surname>Ranzato</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">Unnatural instructions: Tuning language models with (almost) no human labor</title>
		<author>
			<persName><forename type="first">Or</forename><surname>Honovich</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Thomas</forename><surname>Scialom</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Omer</forename><surname>Levy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Timo</forename><surname>Schick</surname></persName>
		</author>
		<idno type="DOI">10.48550/ARXIV.2212.09689</idno>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Distilling knowledge from reader to retriever for question answering</title>
		<author>
			<persName><forename type="first">Gautier</forename><surname>Izacard</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Edouard</forename><surname>Grave</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<author>
			<persName><forename type="first">Gautier</forename><surname>Izacard</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Patrick</forename><surname>Lewis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Maria</forename><surname>Lomeli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lucas</forename><surname>Hosseini</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fabio</forename><surname>Petroni</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Timo</forename><surname>Schick</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jane</forename><surname>Dwivedi-Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Armand</forename><surname>Joulin</surname></persName>
		</author>
		<idno type="DOI">10.48550/ARXIV.2208.03299</idno>
		<title level="m">Sebastian Riedel, and Edouard Grave. 2022. Atlas: Few-shot learning with retrieval augmented language models</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">Survey of hallucination in natural language generation</title>
		<author>
			<persName><forename type="first">Ziwei</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nayeon</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rita</forename><surname>Frieske</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tiezheng</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dan</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yan</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Etsuko</forename><surname>Ishii</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yejin</forename><surname>Bang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrea</forename><surname>Madotto</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pascale</forename><surname>Fung</surname></persName>
		</author>
		<idno type="DOI">10.1145/3571730</idno>
		<imprint>
			<date type="published" when="2022">2022</date>
			<publisher>ACM Computing Surveys</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">How can we know what language models know?</title>
		<author>
			<persName><forename type="first">Zhengbao</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Frank</forename><forename type="middle">F</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jun</forename><surname>Araki</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Graham</forename><surname>Neubig</surname></persName>
		</author>
		<idno type="DOI">10.1162/tacl_a_00324</idno>
	</analytic>
	<monogr>
		<title level="j">Transactions of the Association for Computational Linguistics</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page" from="423" to="438" />
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">TriviaQA: A large scale distantly supervised challenge dataset for reading comprehension</title>
		<author>
			<persName><forename type="first">Mandar</forename><surname>Joshi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Eunsol</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Daniel</forename><surname>Weld</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Luke</forename><surname>Zettlemoyer</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/P17-1147</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 55th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<publisher>Vancouver, Canada. Association for Computational Linguistics</publisher>
			<date type="published" when="2017">2017</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="1601" to="1611" />
		</imprint>
	</monogr>
	<note>Long Papers)</note>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<author>
			<persName><forename type="first">Armand</forename><surname>Joulin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Edouard</forename><surname>Grave</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Piotr</forename><surname>Bojanowski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Matthijs</forename><surname>Douze</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H?rve</forename><surname>J?gou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tomas</forename><surname>Mikolov</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1612.03651</idno>
		<title level="m">Fasttext. zip: Compressing text classification models</title>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">Ctrl: A conditional transformer language model for controllable generation</title>
		<author>
			<persName><forename type="first">Nitish</forename><surname>Shirish Keskar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bryan</forename><surname>Mccann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lav</forename><forename type="middle">R</forename><surname>Varshney</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Caiming</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<idno type="DOI">10.48550/ARXIV.1909.05858</idno>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Europarl: A parallel corpus for statistical machine translation</title>
		<author>
			<persName><forename type="first">Philipp</forename><surname>Koehn</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of machine translation summit x: papers</title>
		<meeting>machine translation summit x: papers</meeting>
		<imprint>
			<date type="published" when="2005">2005</date>
			<biblScope unit="page" from="79" to="86" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Internet-augmented dialogue generation</title>
		<author>
			<persName><forename type="first">Mojtaba</forename><surname>Komeili</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kurt</forename><surname>Shuster</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jason</forename><surname>Weston</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2022.acl-long.579</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 60th Annual Meeting of the Association for Computational Linguistics<address><addrLine>Dublin, Ireland</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2022">2022</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="8460" to="8478" />
		</imprint>
	</monogr>
	<note>Long Papers)</note>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">MAWPS: A math word problem repository</title>
		<author>
			<persName><forename type="first">Rik</forename><surname>Koncel-Kedziorski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Subhro</forename><surname>Roy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aida</forename><surname>Amini</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nate</forename><surname>Kushman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hannaneh</forename><surname>Hajishirzi</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/N16-1136</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2016 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</title>
		<meeting>the 2016 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies<address><addrLine>San Diego, California</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="1152" to="1157" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Natural questions: A benchmark for question answering research</title>
		<author>
			<persName><forename type="first">Tom</forename><surname>Kwiatkowski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jennimaria</forename><surname>Palomaki</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Olivia</forename><surname>Redfield</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><surname>Collins</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ankur</forename><surname>Parikh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chris</forename><surname>Alberti</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Danielle</forename><surname>Epstein</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Illia</forename><surname>Polosukhin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jacob</forename><surname>Devlin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kenton</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kristina</forename><surname>Toutanova</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Llion</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Matthew</forename><surname>Kelcey</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ming-Wei</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrew</forename><forename type="middle">M</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jakob</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Quoc</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Slav</forename><surname>Petrov</surname></persName>
		</author>
		<idno type="DOI">10.1162/tacl_a_00276</idno>
	</analytic>
	<monogr>
		<title level="j">Transactions of the Association for Computational Linguistics</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page" from="452" to="466" />
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<title level="m" type="main">Internetaugmented language models through few-shot prompting for open-domain question answering</title>
		<author>
			<persName><forename type="first">Angeliki</forename><surname>Lazaridou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Elena</forename><surname>Gribovskaya</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wojciech</forename><surname>Stokowiec</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nikolai</forename><surname>Grigorev</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2203.05115</idno>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<author>
			<persName><forename type="first">Patrick</forename><surname>Lewis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Barlas</forename><surname>Oguz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ruty</forename><surname>Rinott</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sebastian</forename><surname>Riedel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Holger</forename><surname>Schwenk</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1910.07475</idno>
		<title level="m">Mlqa: Evaluating cross-lingual extractive question answering</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
		<title level="m" type="main">Few-shot learning with multilingual language models</title>
		<author>
			<persName><forename type="first">Victoria</forename><surname>Xi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Todor</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mikel</forename><surname>Mihaylov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tianlu</forename><surname>Artetxe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shuohui</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Daniel</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Myle</forename><surname>Simig</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Naman</forename><surname>Ott</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shruti</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jingfei</forename><surname>Bhosale</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ramakanth</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sam</forename><surname>Pasunuru</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Punit</forename><surname>Shleifer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vishrav</forename><surname>Singh Koura</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Brian O'</forename><surname>Chaudhary</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jeff</forename><surname>Horo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Luke</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zornitsa</forename><surname>Zettlemoyer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mona</forename><surname>Kozareva</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Veselin</forename><surname>Diab</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xian</forename><surname>Stoyanov</surname></persName>
		</author>
		<author>
			<persName><surname>Li</surname></persName>
		</author>
		<idno type="DOI">10.48550/ARXIV.2112.10668</idno>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
		<title level="m" type="main">On faithfulness and factuality in abstractive summarization</title>
		<author>
			<persName><forename type="first">Joshua</forename><surname>Maynez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shashi</forename><surname>Narayan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bernd</forename><surname>Bohnet</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ryan</forename><surname>Mcdonald</surname></persName>
		</author>
		<idno type="DOI">10.48550/ARXIV.2005.00661</idno>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Effective self-training for parsing</title>
		<author>
			<persName><forename type="first">David</forename><surname>Mcclosky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Eugene</forename><surname>Charniak</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mark</forename><surname>Johnson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Human Language Technology Conference of the NAACL, Main Conference</title>
		<meeting>the Human Language Technology Conference of the NAACL, Main Conference<address><addrLine>New York City, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2006">2006</date>
			<biblScope unit="page" from="152" to="159" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Pointer sentinel mixture models</title>
		<author>
			<persName><forename type="first">Stephen</forename><surname>Merity</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Caiming</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">James</forename><surname>Bradbury</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">A diverse corpus for evaluating and developing English math word problem solvers</title>
		<author>
			<persName><forename type="first">Chao-Chun</forename><surname>Shen-Yun Miao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Keh-Yih</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName><surname>Su</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2020.acl-main.92</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 58th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="975" to="984" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<monogr>
		<author>
			<persName><forename type="first">Reiichiro</forename><surname>Nakano</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jacob</forename><surname>Hilton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Suchir</forename><surname>Balaji</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jeff</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Long</forename><surname>Ouyang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christina</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christopher</forename><surname>Hesse</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shantanu</forename><surname>Jain</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vineet</forename><surname>Kosaraju</surname></persName>
		</author>
		<author>
			<persName><forename type="first">William</forename><surname>Saunders</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xu</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Karl</forename><surname>Cobbe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tyna</forename><surname>Eloundou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gretchen</forename><surname>Krueger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kevin</forename><surname>Button</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Matthew</forename><surname>Knight</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Benjamin</forename><surname>Chess</surname></persName>
		</author>
		<author>
			<persName><forename type="first">John</forename><surname>Schulman</surname></persName>
		</author>
		<idno type="DOI">10.48550/ARXIV.2112.09332</idno>
		<title level="m">Webgpt: Browserassisted question-answering with human feedback</title>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<monogr>
		<author>
			<persName><forename type="first">Aaron</forename><surname>Parisi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yao</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Noah</forename><surname>Fiedel</surname></persName>
		</author>
		<idno type="DOI">10.48550/ARXIV.2205.12255</idno>
		<title level="m">Talm: Tool augmented language models</title>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Are NLP models really able to solve simple math word problems?</title>
		<author>
			<persName><forename type="first">Arkil</forename><surname>Patel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Satwik</forename><surname>Bhattamishra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Navin</forename><surname>Goyal</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2021.naacl-main.168</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</title>
		<meeting>the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</meeting>
		<imprint>
			<publisher>Online. Association for Computational Linguistics</publisher>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="2080" to="2094" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">KILT: a benchmark for knowledge intensive language tasks</title>
		<author>
			<persName><forename type="first">Fabio</forename><surname>Petroni</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aleksandra</forename><surname>Piktus</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Angela</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Patrick</forename><surname>Lewis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Majid</forename><surname>Yazdani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nicola</forename><forename type="middle">De</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">James</forename><surname>Thorne</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yacine</forename><surname>Jernite</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vladimir</forename><surname>Karpukhin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jean</forename><surname>Maillard</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vassilis</forename><surname>Plachouras</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tim</forename><surname>Rockt?schel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sebastian</forename><surname>Riedel</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2021.naacl-main.200</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</title>
		<meeting>the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</meeting>
		<imprint>
			<publisher>Online. Association for Computational Linguistics</publisher>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="2523" to="2544" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Language models as knowledge bases?</title>
		<author>
			<persName><forename type="first">Fabio</forename><surname>Petroni</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tim</forename><surname>Rockt?schel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sebastian</forename><surname>Riedel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Patrick</forename><surname>Lewis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Anton</forename><surname>Bakhtin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuxiang</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alexander</forename><surname>Miller</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/D19-1250</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)</title>
		<meeting>the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)<address><addrLine>Hong Kong, China</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="2463" to="2473" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Language models are unsupervised multitask learners</title>
		<author>
			<persName><forename type="first">Alec</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jeffrey</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rewon</forename><surname>Child</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><surname>Luan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dario</forename><surname>Amodei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">OpenAI blog</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page">9</biblScope>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Self-training for enhancement and domain adaptation of statistical parsers trained on small datasets</title>
		<author>
			<persName><forename type="first">Roi</forename><surname>Reichart</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ari</forename><surname>Rappoport</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 45th Annual Meeting of the Association of Computational Linguistics</title>
		<meeting>the 45th Annual Meeting of the Association of Computational Linguistics<address><addrLine>Prague, Czech Republic</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2007">2007</date>
			<biblScope unit="page" from="616" to="623" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Okapi at trec-3</title>
		<author>
			<persName><forename type="first">Steve</forename><surname>Stephen E Robertson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Susan</forename><surname>Walker</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Micheline</forename><forename type="middle">M</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mike</forename><surname>Hancock-Beaulieu</surname></persName>
		</author>
		<author>
			<persName><surname>Gatford</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nist Special Publication Sp</title>
		<imprint>
			<biblScope unit="volume">109</biblScope>
			<biblScope unit="page">109</biblScope>
			<date type="published" when="1995">1995</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<monogr>
		<author>
			<persName><forename type="first">Timo</forename><surname>Schick</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jane</forename><surname>Dwivedi-Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhengbao</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fabio</forename><surname>Petroni</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Patrick</forename><surname>Lewis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gautier</forename><surname>Izacard</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qingfei</forename><surname>You</surname></persName>
		</author>
		<idno type="DOI">10.48550/ARXIV.2208.11663</idno>
		<title level="m">Christoforos Nalmpantis, Edouard Grave, and Sebastian Riedel. 2022. Peer: A collaborative language model</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Exploiting cloze-questions for few-shot text classification and natural language inference</title>
		<author>
			<persName><forename type="first">Timo</forename><surname>Schick</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hinrich</forename><surname>Sch?tze</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2021.eacl-main.20</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 16th Conference of the European Chapter of the Association for Computational Linguistics: Main Volume</title>
		<meeting>the 16th Conference of the European Chapter of the Association for Computational Linguistics: Main Volume</meeting>
		<imprint>
			<publisher>Online. Association for Computational Linguistics</publisher>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="255" to="269" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Generating datasets with pretrained language models</title>
		<author>
			<persName><forename type="first">Timo</forename><surname>Schick</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hinrich</forename><surname>Sch?tze</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2021.emnlp-main.555</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2021 Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<publisher>Dominican Republic. Association for Computational Linguistics</publisher>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="6943" to="6951" />
		</imprint>
		<respStmt>
			<orgName>Online and Punta Cana</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title/>
		<author>
			<persName><forename type="first">Kurt</forename><surname>Shuster</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jing</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mojtaba</forename><surname>Komeili</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Da</forename><surname>Ju</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Eric</forename><forename type="middle">Michael</forename><surname>Smith</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stephen</forename><surname>Roller</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Megan</forename><surname>Ung</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Moya</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kushal</forename><surname>Arora</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Joshua</forename><surname>Lane</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Morteza</forename><surname>Behrooz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">William</forename><surname>Ngan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Spencer</forename><surname>Poff</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Naman</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Arthur</forename><surname>Szlam</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y-Lan</forename><surname>Boureau</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Melanie</forename><surname>Kambadur</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jason</forename><surname>Weston</surname></persName>
		</author>
		<idno type="DOI">10.48550/ARXIV.2208.03188</idno>
	</analytic>
	<monogr>
		<title level="j">Blenderbot</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
	<note>ployed conversational agent that continually learns to responsibly engage</note>
</biblStruct>

<biblStruct xml:id="b47">
	<monogr>
		<title/>
		<author>
			<persName><forename type="first">Romal</forename><surname>Thoppilan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Daniel</forename><forename type="middle">De</forename><surname>Freitas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jamie</forename><surname>Hall</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Noam</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Apoorv</forename><surname>Kulshreshtha</surname></persName>
		</author>
		<author>
			<persName><surname>Heng-Tze</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alicia</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Taylor</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Leslie</forename><surname>Bos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yu</forename><surname>Baker</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yaguang</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hongrae</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Huaixiu</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Amin</forename><surname>Steven Zheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Marcelo</forename><surname>Ghafouri</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yanping</forename><surname>Menegali</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Maxim</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dmitry</forename><surname>Krikun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">James</forename><surname>Lepikhin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dehao</forename><surname>Qin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuanzhong</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhifeng</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Adam</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Maarten</forename><surname>Roberts</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vincent</forename><surname>Bosma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yanqi</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chung-Ching</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Igor</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Will</forename><surname>Krivokon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Marc</forename><surname>Rusch</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pranesh</forename><surname>Pickett</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Laichee</forename><surname>Srinivasan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kathleen</forename><surname>Man</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Meredith</forename><forename type="middle">Ringel</forename><surname>Meier-Hellstern</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tulsee</forename><surname>Morris</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Renelito Delos</forename><surname>Doshi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Toju</forename><surname>Santos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Johnny</forename><surname>Duke</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ben</forename><surname>Soraker</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vinodkumar</forename><surname>Zevenbergen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mark</forename><surname>Prabhakaran</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ben</forename><surname>Diaz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kristen</forename><surname>Hutchinson</surname></persName>
		</author>
		<author>
			<persName><surname>Olson</surname></persName>
		</author>
		<idno type="DOI">10.48550/ARXIV.2201.08239</idno>
		<editor>Alejandra Molina, Erin Hoffman-John, Josh Lee, Lora Aroyo, Ravi Rajakumar, Alena Butryna, Matthew Lamm, Viktoriya Kuzmina, Joe Fenton, Aaron Cohen, Rachel Bernstein, Ray Kurzweil, Blaise Aguera-Arcas, Claire Cui, Marian Croak</editor>
		<imprint/>
	</monogr>
	<note>and Quoc Le. 2022. Lamda: Language models for dialog applications</note>
</biblStruct>

<biblStruct xml:id="b48">
	<monogr>
		<title level="m" type="main">GPT-J-6B: A 6 Billion Parameter Autoregressive Language Model</title>
		<author>
			<persName><forename type="first">Ben</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aran</forename><surname>Komatsuzaki</surname></persName>
		</author>
		<ptr target="https://github.com/kingoflolz/mesh-transformer-jax" />
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<monogr>
		<title level="m" type="main">Self-instruct: Aligning language model with self generated instructions</title>
		<author>
			<persName><forename type="first">Yizhong</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yeganeh</forename><surname>Kordi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Swaroop</forename><surname>Mishra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alisa</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Noah</forename><forename type="middle">A</forename><surname>Smith</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Daniel</forename><surname>Khashabi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hannaneh</forename><surname>Hajishirzi</surname></persName>
		</author>
		<idno type="DOI">10.48550/ARXIV.2212.10560</idno>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<monogr>
		<title/>
		<author>
			<persName><forename type="first">Jason</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yi</forename><surname>Tay</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rishi</forename><surname>Bommasani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Colin</forename><surname>Raffel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Barret</forename><surname>Zoph</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sebastian</forename><surname>Borgeaud</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dani</forename><surname>Yogatama</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Maarten</forename><surname>Bosma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Denny</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Donald</forename><surname>Metzler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ed</forename><forename type="middle">H</forename><surname>Chi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tatsunori</forename><surname>Hashimoto</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Oriol</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Percy</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jeff</forename><surname>Dean</surname></persName>
		</author>
		<author>
			<persName><forename type="first">William</forename><surname>Fedus</surname></persName>
		</author>
		<idno type="DOI">10.48550/ARXIV.2206.07682</idno>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
	<note>Emergent abilities of large language models</note>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">CCNet: Extracting high quality monolingual datasets from web crawl data</title>
		<author>
			<persName><forename type="first">Guillaume</forename><surname>Wenzek</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Marie-Anne</forename><surname>Lachaux</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alexis</forename><surname>Conneau</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vishrav</forename><surname>Chaudhary</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Francisco</forename><surname>Guzm?n</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Armand</forename><surname>Joulin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Edouard</forename><surname>Grave</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Twelfth Language Resources and Evaluation Conference</title>
		<meeting>the Twelfth Language Resources and Evaluation Conference<address><addrLine>Marseille, France</addrLine></address></meeting>
		<imprint>
			<publisher>European Language Resources Association</publisher>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="4003" to="4012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<monogr>
		<title level="m" type="main">React: Synergizing reasoning and acting in language models</title>
		<author>
			<persName><forename type="first">Shunyu</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jeffrey</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dian</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nan</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Izhak</forename><surname>Shafran</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Karthik</forename><surname>Narasimhan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuan</forename><surname>Cao</surname></persName>
		</author>
		<idno type="DOI">10.48550/ARXIV.2210.03629</idno>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">Unsupervised word sense disambiguation rivaling supervised methods</title>
		<author>
			<persName><forename type="first">David</forename><surname>Yarowsky</surname></persName>
		</author>
		<idno type="DOI">10.3115/981658.981684</idno>
	</analytic>
	<monogr>
		<title level="m">33rd Annual Meeting of the Association for Computational Linguistics</title>
		<meeting><address><addrLine>Cambridge, Massachusetts, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="1995">1995</date>
			<biblScope unit="page" from="189" to="196" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<monogr>
		<author>
			<persName><forename type="first">Eric</forename><surname>Zelikman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuhuai</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jesse</forename><surname>Mu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Noah</forename><forename type="middle">D</forename><surname>Goodman</surname></persName>
		</author>
		<idno type="DOI">10.48550/ARXIV.2203.14465</idno>
		<title level="m">Star: Bootstrapping reasoning with reasoning</title>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<monogr>
		<title/>
		<author>
			<persName><forename type="first">Susan</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stephen</forename><surname>Roller</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Naman</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mikel</forename><surname>Artetxe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Moya</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shuohui</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christopher</forename><surname>Dewan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mona</forename><surname>Diab</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xian</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xi</forename><surname>Victoria Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Todor</forename><surname>Mihaylov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Myle</forename><surname>Ott</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sam</forename><surname>Shleifer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kurt</forename><surname>Shuster</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Daniel</forename><surname>Simig</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Punit</forename><surname>Singh Koura</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Anjali</forename><surname>Sridhar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tianlu</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Luke</forename><surname>Zettlemoyer</surname></persName>
		</author>
		<idno type="DOI">10.48550/ARXIV.2205.01068</idno>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
	<note>Opt: Open pretrained transformer language models</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
