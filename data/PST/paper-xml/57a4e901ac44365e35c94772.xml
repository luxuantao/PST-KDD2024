<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">A deep feature based framework for breast masses classification</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Zhicheng</forename><surname>Jiao</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">School of Electronic Engineering</orgName>
								<orgName type="laboratory">Lab of Video and Image Processing Systems</orgName>
								<orgName type="institution">Xidian University</orgName>
								<address>
									<settlement>Xi&apos;an</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author role="corresp">
							<persName><forename type="first">Xinbo</forename><surname>Gao</surname></persName>
							<email>xbgao@mail.xidian.edu.cn</email>
							<affiliation key="aff0">
								<orgName type="department">School of Electronic Engineering</orgName>
								<orgName type="laboratory">Lab of Video and Image Processing Systems</orgName>
								<orgName type="institution">Xidian University</orgName>
								<address>
									<settlement>Xi&apos;an</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Ying</forename><surname>Wang</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">School of Electronic Engineering</orgName>
								<orgName type="laboratory">Lab of Video and Image Processing Systems</orgName>
								<orgName type="institution">Xidian University</orgName>
								<address>
									<settlement>Xi&apos;an</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Jie</forename><surname>Li</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">School of Electronic Engineering</orgName>
								<orgName type="laboratory">Lab of Video and Image Processing Systems</orgName>
								<orgName type="institution">Xidian University</orgName>
								<address>
									<settlement>Xi&apos;an</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">A deep feature based framework for breast masses classification</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">3BA7AADEF22D20E587CD05963A75B681</idno>
					<idno type="DOI">10.1016/j.neucom.2016.02.060</idno>
					<note type="submission">Received 26 November 2015 Received in revised form 29 January 2016 Accepted 9 February 2016</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.3" ident="GROBID" when="2023-07-28T03:03+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>Deep learning Convolutional neural network Breast mass classification Computer-aided diagnosis Feature visualization</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Characteristic classification of mass plays a role of vital importance in diagnosis of breast cancer. The existing computer aided diagnosis (CAD) methods used to benefit a lot from low-level or middle-level features which are not that good at the simulation of real diagnostic processes, adding difficulties in improving the classification performance. In this paper, we design a deep feature based framework for breast mass classification task. It mainly contains a convolutional neural network (CNN) and a decision mechanism. Combining intensity information and deep features automatically extracted by the trained CNN from the original image, our proposed method could better simulate the diagnostic procedure operated by doctors and achieved state-of-art performance. In this framework, doctors' global and local impressions left by mass images were represented by deep features extracted from two different layers called high-level and middle-level features. Meanwhile, the original images were regarded as detailed descriptions of the breast mass. Then, classifiers based on features above were used in combination to predict classes of test images. And outcomes of classifiers based on different features were analyzed jointly to determine the types of test images. With the help of two kinds of feature visualization methods, deep features extracted from different layers illustrate effective in classification performance and diagnosis simulation. In addition, our method was applied to DDSM dataset and achieved high accuracy under two objective evaluation measures.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.1.">Breast mass classification</head><p>Being the second cause of death, breast cancer is one of the most common cancers in women. According to a world health organization (WHO) report, breast cancer accounts for 22.9% of diagnosed cancers and 13.7% of cancer related death worldwide <ref type="bibr" target="#b0">[1]</ref>. To improve the five-year and ten-year survival rate and to relieve great suffering of patients, the early diagnosis is of crucial importance. Being a process of utilizing low-energy X-rays to examine the human breast, mammography is the most widely used screening and diagnostic tool in both clinical and scientific fields. In order to analyze such an amount of mammograms generated daily in medical centers and hospitals, traditional solution for this challenge is that radiologists have to browse all these images day and night. The next several diagnosis processes also exhaust the physicians, causing the diagnosis to be highly susceptible to errors. This situation also troubles physicians in other fields, and computer-aided diagnosis (CAD) systems have been playing more and more important parts in assisting and improving physicians' work. In previous works, Doi <ref type="bibr" target="#b1">[2]</ref> considered that CAD had become one of the major research subjects in medical imaging and diagnostic radiology. Ginneken et al. <ref type="bibr" target="#b2">[3]</ref> pointed out that CAD systems were of great help in diagnosis of chest radiography. Jiang et al. <ref type="bibr" target="#b3">[4]</ref> and Chan et al. <ref type="bibr" target="#b4">[5]</ref> obtained the conclusion that CAD could be used to improve radiologists' performance in breast cancer diagnosis. Identifying benign and malignant masses is among the core contents in diagnosis using mammography. Meanwhile, the building of systems which can effectively assist to do mass classification is one of the hotspots in the mammography related CAD field. Therefore, designing better classification algorithms and frameworks has been attracting more and more attention.</p><p>However, as shown in Fig. <ref type="figure" target="#fig_0">1</ref>, owing to the diversity in appearance, it is difficult to distinguish the malignant masses from benign ones. A number of researchers and their research teams have been devoted to designing learning and classifying framework to overcome this difficulty. Rangayyan et al. <ref type="bibr" target="#b5">[6]</ref> proposed using morphological features to characterize the roughness of tumor boundaries and applied them in classification tasks; Mavroforakis et al. <ref type="bibr" target="#b6">[7]</ref> used linear, neural and SVM classifiers to classify masses with the help of textural features and conducted fractal dimension analysis; Timp et al. <ref type="bibr" target="#b7">[8]</ref> came up with a novel method exploring the temporal change features among mammography series by regional registration; Rojas-Dom√≠nguez et al. <ref type="bibr" target="#b8">[9]</ref> performed the analysis of the gradient orientation, fuzziness, Contents lists available at ScienceDirect journal homepage: www.elsevier.com/locate/neucom speculation, and mutual information of mass margins; Employing BI-RADS mammographic features with SVM-REF classifier, Yoon et al. <ref type="bibr" target="#b9">[10]</ref> achieved a good performance in DDSM; Ramirez-Villegas et al. <ref type="bibr" target="#b10">[11]</ref> chose SVM and neural-based classification methods combining Wavelet packet energy, Tsallis entropy and statistical parameterization feature analysis; benefitting more from data structure high accuracy was also reported by Wang et al. <ref type="bibr" target="#b11">[12]</ref> in the way of formulating this task into one second order cone programming problem. Verma et al. <ref type="bibr" target="#b12">[13,</ref><ref type="bibr" target="#b13">14]</ref> extracted various kinds of features such as density, morphology, abnormity assessment rank, and so on, for description of the masses. Then the soft neural network and soft clustered based direct learning method were employed to do the classification. Wang et al. <ref type="bibr" target="#b14">[15]</ref> proposed a latent feature mining based method which characterized spatial and marginal information effectively and achieved good results. More recently, Beura et al. <ref type="bibr" target="#b15">[16]</ref> proposed a scheme utilizing 2D-DWT and GLCM in succession to derive feature matrix form mammograms for further classification. Besides, Xie et al. <ref type="bibr" target="#b16">[17]</ref> applied extreme learning machine method to improve the performance of mass classification tasks.</p><p>Methods which were mentioned above used to extract and utilize low-level features such as margin, texture and so on, or middle-level features such as shape and some variants of bag of words (BOW) <ref type="bibr" target="#b17">[18]</ref><ref type="bibr" target="#b18">[19]</ref><ref type="bibr" target="#b19">[20]</ref>, which has been proven to be effective by Avni et al. <ref type="bibr" target="#b20">[21]</ref>. Then the features are introduced into different kinds of classifiers to categorize masses. Although the wide range of traditional handcraft features seem like building a good description of an image, there has been a significant gap existing between these features and cognitive behaviors of physicians. And they do not seem to cover the basic strategies <ref type="bibr" target="#b1">[2]</ref> for development of CAD methods and techniques. Strategies aiming at achieving detection and quantitation of lesions in medical images should be based on the understanding of image readings by radiologists. In the real diagnosis process, doctors usually glance at the X-ray first to get preliminary understanding of it. Then several regions that might contain lesions would attract more attention, and the overall look of these regions and details of the entire image would leave impression on and result in different levels of knowledge in the physician's brain. To make the judgement of whether a mass is benign or malignant, doctors used to combine the varying levels of knowledge and awareness with previous experience in similar tasks. The procedure above is similar to that described by the attending doctor we consult from, and it agrees with two representational diagnosis methods: symptom comparisons and antidiastole. Among these processes, the hierarchical impression of mass images and information processing of human brain are difficult to be specified with traditional features and related methods. Nevertheless, all these things which could only be unspeakable account for a lot in the real diagnosis. So, methods utilizing hierarchical representations and a similar decision mechanism to the real diagnosis may be better choices.</p><p>In addition, various types of traditional features can improve classification performance in most situations, but they may have some negative impacts owing to the incompatibility. For example, incompatible extraction methods and corresponding features are less explicable when they are combined directly in a unified framework. However, designing an effective feature fusion strategy is also exhausting in previous papers. On the contrary, hierarchical frameworks could put features extracted from different levels together to form a more explainable and unified structure, avoiding fusing features directly with different models.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.2.">Deep learning and deep learning on biomedical image</head><p>From the year of 2006 on, a new machine learning paradigm, named deep learning <ref type="bibr" target="#b21">[22]</ref><ref type="bibr" target="#b22">[23]</ref><ref type="bibr" target="#b23">[24]</ref>, has been playing a much more important role in the academic community. And it has become a huge tide of technology trend in the field of big data and artificial intelligence. Simulating the hierarchical structure of human brain and its data processing mechanism which transfers information from lower level to higher level, deep learning introduces more semantic information to the final representations. Thus deep structure makes significant breakthroughs on image understanding, speech recognition, natural language processing and many other areas <ref type="bibr" target="#b21">[22]</ref>. The fever of deep learning has been sweeping the world and has been attracting more attention of top researchers. As results of all these efforts, a few outstanding deep structures are proposed and prove to be successful, such as convolutional neural network (CNN) <ref type="bibr" target="#b24">[25]</ref>, sparse autoencoder (SAE) <ref type="bibr" target="#b25">[26]</ref>, restricted boltzmann machine (RBM) <ref type="bibr" target="#b26">[27,</ref><ref type="bibr" target="#b27">28]</ref>, and so on.</p><p>As aforementioned, a convolutional neural network (CNN) is a representative structure of deep models. It is a type of feedforward artificial neural network where individual neurons are tiled in a way that they respond to overlapping regions called receptive fields <ref type="bibr" target="#b46">[47]</ref> in the visual field. And it is inspired by biological processes and is a variation of multilayer perceptrons which are designed to reduce preprocessing.</p><p>Having been developed for more than three decades, CNN has become an outstanding method. The powerful structures were introduced by Fukushima <ref type="bibr" target="#b28">[29]</ref>. CNN was later improved by LeCun et al. <ref type="bibr" target="#b29">[30]</ref>. The famous leNet-5 <ref type="bibr" target="#b30">[31]</ref> being in form of CNN obtained huge success in recognizing checking numbers. However, given more complex tasks, the computational complexity of network would continue to increase as network become deeper, causing the main limitation of deep structure at that time. In the subsequent years, development of computing power and optimization methods makes it possible to train a deeper CNN which is powerful enough <ref type="bibr" target="#b31">[32]</ref> to fulfill other more complicated tasks. Specially, CNN has been applied in aspects of visual object recognition and image classification tasks and has achieved superior performance <ref type="bibr" target="#b32">[33]</ref><ref type="bibr" target="#b33">[34]</ref><ref type="bibr" target="#b34">[35]</ref>. When it comes to the fields of biomedical image processing, many breakthroughs are also made by the powerful structure. Jain et al. <ref type="bibr" target="#b35">[36]</ref>, Jain and Seung <ref type="bibr" target="#b36">[37]</ref>, and Helmstaedter et al. <ref type="bibr" target="#b37">[38]</ref> applied CNNs to restore and segment the volumetric electron microscopy images. In the next few years, CNN based on patches was also applied by Ciresan et al. <ref type="bibr" target="#b38">[39,</ref><ref type="bibr" target="#b39">40]</ref> to detect mitosis in breast histology images. More recently, Zhang et al. <ref type="bibr" target="#b40">[41]</ref> proposed a CNN based method aiming at segmenting infant brain tissue images in the isointense stage. A patch based CNN was trained and applied to classify each pixel in the image to finish segmentation. And results show that their proposed model significantly outperformed previous methods on infant brain tissue segmentation.</p><p>Being in much the same way of brain's information processing and cognitive mechanism, deep learning could provide more effective features for computer vision tasks, such as detection and classification. And deep neural networks have been proved to be more similar to the primate visual system and hierarchical sensory processing systems in brain <ref type="bibr" target="#b41">[42,</ref><ref type="bibr" target="#b42">43]</ref>. Inspired by the achievements of CNN in other fields and its similarity to brain, we proposed a deep feature based framework for breast masses classification. In the proposed scheme, a convolutional neural network (CNN) was trained on a large number of natural images and was fine-tuned on a subset of breast mass images. The training strategy was chosen to overcome shortage of breast images. In this framework the data augmentation <ref type="bibr" target="#b58">[60]</ref> operation was also introduced and played a role. Then features of masses were extracted from different hierarchical levels of this model, with the help of which two classifiers were trained for the decision procedure. And we applied a strategy in the decision mechanism, which fused the outcomes from different classifiers to finish the classification.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Network training and decision mechanism</head><p>Our proposed breast mass classification method consists of a hierarchical representation network and a series of decision mechanism for these features. The fundamental blocks in this network are introduced respectively. Then, training for our CNN and decision mechanism is detailed as follows.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.">Network training.</head><p>As shown in Fig. <ref type="figure" target="#fig_1">2</ref>, the CNN architecture we used in this paper mainly includes 3 kinds of operational units: convolution, pooling and Rectified Linear Unit (ReLU) <ref type="bibr" target="#b43">[44]</ref> activation. Each convolution layer in a CNN contains some of these units. Being the indispensable component in CNN frameworks, convolution blocks simulate orientation-selective simple cells in primary visual cortex <ref type="bibr" target="#b24">[25]</ref>. It computes the convolution of the input map x with a bank of K multi-dimensional filters f and biases b. Here, H; W; D respectively stand for the height, width and depth of input map x, while H 0 ; W 0 ; D stand for the height, width and depth of convolution filters, d; d 0 ; d 00 stand for the channel index of filters, input map and output map. Besides, H 00 ; W 00 ; K stand for the scale of output map y of this layer.</p><formula xml:id="formula_0">x A R H√ÇW√ÇD , f A R H 0 √ÇW 0 √ÇD√ÇK , y A R H 00 √ÇW 00 √ÇD√ÇK , W 00 ¬º W √Ä W 0 √æ 1, H 00 ¬º H √Ä H 0 √æ1: y i 00 j 00 d 00 ¬º b d 00 √æ X H 0 i 0 ¬º 1 X W 0 j 0 ¬º 1 X D d 0 ¬º 1 f i 0 j 0 d √Ç x i 00 √æ i 0 √Ä 1;j 00 √æ j 0 √Ä 1;d 0 ;d 00<label>√∞1√û</label></formula><p>Pooling is also an important kind of operation in CNN structure. The pooling unit whose role is similar to complex cells in brain visual cortex has a number of variations <ref type="bibr" target="#b24">[25]</ref>. And all these versions have been discussed and compared in previous papers. From all these variations, we chose the one named max pooling which has been widely applied in many successful models. It computes the maximum response of each feature channel in x in a H 0 √Ç W 0 patch. Here, H; W; D and H 00 ; W 00 ; K represent the scale of input and out map of this kind of operation. x A R H√ÇW√ÇD , y A R H 00 √ÇW 00 √ÇK :</p><formula xml:id="formula_1">y i 00 j 00 d ¬º max H 0 √ÇW 0 x i 00 √æ i 0 √Ä 1;j 00 √æ j 0 √Ä 1;d<label>√∞2√û</label></formula><p>Besides, the pointwise activation function is another fundamental component in the model. It simulates excitability of neurons in brain when excited by stimuli. There are also a number kinds of activation functions in deep models. We chose the one named Rectified Linear Unit (ReLU) <ref type="bibr" target="#b43">[44]</ref> which has been proved more efficient in computation in the papers of <ref type="bibr" target="#b44">[45]</ref> and <ref type="bibr" target="#b45">[46]</ref>. Here, y ijd stands for the response in output map of this layer with input x ijd in the corresponding location.</p><formula xml:id="formula_2">y ijd ¬º max f0; x ijd g √∞<label>3√û</label></formula><p>The receptive fields <ref type="bibr" target="#b46">[47]</ref> of different neurons in the network appear in the forms of convolutional and pooling kernels which differ in both size and weight in different layers.</p><p>Beyond the aforementioned components, there are several other kinds of layers in this model. With the help of dropout operation <ref type="bibr" target="#b32">[33,</ref><ref type="bibr" target="#b45">46,</ref><ref type="bibr" target="#b48">49]</ref>, we could reduce overfitting of the network and learn more robust features. Besides, a cross-channel normalization operator is also implemented at each spatial location across all feature maps of the same layer to gain a better description of input. The last layers are fully-connected ones which were followed by logarithmic loss to be minimized.</p><p>Our feature representation net is a CNN inspired by <ref type="bibr" target="#b32">[33,</ref><ref type="bibr" target="#b45">46]</ref>. In order to overcome the challenge that a large training set is not available in the field of mammograms, we trained our CNN on LSVRC <ref type="bibr" target="#b48">[49]</ref> which is a dataset containing more than 1 million labeled natural images first. As the natural images in the database are all with 3 channels, which are not in accordance with gray level medical images and increased the computing consuming in both training and testing stages. So they were transformed into gray scale ones with a simple projection method. And the training strategy mainly followed the strategy proposed by <ref type="bibr" target="#b32">[33]</ref>. It is a supervised learning process forming hierarchical feature detectors, the learning rate of the training stage with LSVRC data was initialized at 0.01 and was divided by 10 when the validation error rate stopped improving with the current learning rate. The whole training process continued for 100 cycles through the natural image dataset. Then learning rate of training stage on breast mass images was initialized at 0.00001, and it changed as the strategy as that of the first training stage. And the second stage was executed 100 cycles. Meanwhile, main and details of the network structure are shown in Tables <ref type="table" target="#tab_0">1</ref> and<ref type="table" target="#tab_1">2</ref>. Then we applied fine-tuning operation which takes the already learned model trained on LSVRC, adapted the architecture, and resumed training from the already learned model weights on our dataset of mass images. Specifically, breast dataset in this paper is made of 600 mass images of 227 √Ç 227 which are extracted from DDSM <ref type="bibr">[54]</ref>. According to the successful application of <ref type="bibr" target="#b58">[60]</ref> on medical image and followed the diagnosis habits of the radiologist who we consulted from, instances we applied had been rotated angles of 90¬∞, 180¬∞, 270¬∞to enlarge the dataset 3 times. Network parameters obtained from training on LSVRC were set as initial values in the specific CNN for mass images and optimized the network according to mass dataset. The main parameters of each convolution architecture in this network are given in Table <ref type="table" target="#tab_1">2</ref>. Size of feature maps in each layer are determined by both sliding window size and striding of sliding window in previous layers. And the number of feature maps or the length of features in each layer is resulted from number of different types of convolution and pooling kernels.</p><p>Stochastic gradient descent (SGD) <ref type="bibr" target="#b49">[50]</ref> was employed to optimize our network which is very simple and efficient in the training process. The training point at each iteration was selected at random. Then the derivative of the loss term for that training sample was computed resulting in a gradient vector. And parameters were incrementally updated by moving toward the local minima in the direction of the gradient. The most important operation is computing derivative of the objective function, which is obtained by an application of the chain rule known as back-propagation. Generally speaking, a CNN model contains several of all these blocks above, forming directed acyclic graph (DAG). The DAG could be simplified as Fig. <ref type="figure" target="#fig_2">3</ref>, where each output of corresponding block (f 1 ; f 2 ; f 3 ; ‚ãØ‚ãØf l ) is described as x 1 ; x 2 ; x 3 ; ‚ãØ‚ãØx l , and the parameters of each layer was w 1 ; w 2 ; w 3 ; ‚ãØ‚ãØw l . So the derivative of w l in the loss layer is expressed in the form of back-propagation chain rule as follow.</p><formula xml:id="formula_3">dz d vec w l √∞ √û T ¬º dz d vec x L √∞ √û T d vec x L d vec x L √Ä 1 √∞ √û T ‚ãØ d vec x l √æ 1 d√∞vec x l √û T d vec x l d√∞vec w l √û T<label>√∞4√û</label></formula><p>To evaluate this CNN model, the loss of network was calculated by the standard cross entropy between the predicted probability distribution over two types of labels for each image and the ground truth distribution which equaled to the logarithmic loss. We used the backpropagation algorithm to calculate the gradient with respect to the parameters of the model and trained the network with stochastic gradient descent (SGD) by minimizing the loss as described above. To obtain a better result of this fine-tuning operation and to avoid the oscillatory of loss function, we applied a strategy that the learning rate was reduced gradually during the whole process. Specifically, the learning rate is set to a smaller value if reduction of loss function is less than the threshold after several times of iterations. In addition, the initial value of learning rate is set up as a small one.</p><p>After obtaining the fine-tuned CNN, the hierarchical features could be extracted through the feed-forward model. For example, an instance is input into this structure to be handled with various methods of different layers. As a result of each operation, output of each layer is presented in a modality of feature maps which contained a lot of channels, and these are actually the hierarchical features we applied in the paper. Meanwhile, in both training and testing processes we used some basic functions of the toolbox named matconvnet <ref type="bibr" target="#b59">[61]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.">Decision mechanism</head><p>The features we chose for training the classifiers which divide the input images into two types were extracted from two different layers of this network. Concretely speaking, they were from the layers of Conv5 and Fc7, both of which were in the form of column vector.</p><p>Then two linear SVM classifiers based on the hierarchical features were trained on training dataset and tested on test dataset to predict a test image was whether benign or malignant. The SVM we preferred was the most basic one in LIBSVM <ref type="bibr" target="#b50">[51]</ref> toolbox, resulting in fast calculations in both training and testing procedures. And all its parameters were chosen as the defaults without the time consuming procedure of parameter selection. Comparing with the results of kernel SVM experiments and k-fold cross validation operation, there was a trade-off between efficiency and effectiveness of the performance when we chose the basic linear SVM classifier. As were shown in Fig. <ref type="figure" target="#fig_4">5</ref> and Table <ref type="table" target="#tab_2">3</ref>, we applied the SVM with RBF kernel and spent plenty of time doing experiments on selecting the optimal value of C and Gamma which are widely believed as the most important parameters of RBF kernel SVM in LIBSVM. Concretely speaking, we set both C and Gamma to a substantial range of 2 √Ä 15 ; 2 √Ä 14 ; ‚ãØ; 2 0 ; ‚ãØ; 2 14 ; 2 15 and summarized the results. As it was shown in Fig. <ref type="figure" target="#fig_4">5</ref>, two coordinate axes represented the values of C and Gamma which were in the form of power-of-2 while the legend value stood for classification accuracy. The best performance was 97.0% (C ¬º 2 5 , Gamma ¬º 2 √Ä 12 ) which was a little bit better than that (96.7%) of the linear SVM with default parameters. However, it took us 77,180 s to do parameters selection experiments and it was more than ten thousand times of the experiment with default values. In addition, we applied k-fold cross validation in our linear SVM experiments. From the results in Table <ref type="table" target="#tab_2">3</ref>, we could clearly see that the time consuming k-fold cross validation did not improve the performance obviously either. So we still selected the linear SVM setting all parameters as default values for its far much higher efficiency and a good enough performance.</p><p>We propose a strategy to combine outcomes of these two classifiers. As shown in Fig. <ref type="figure" target="#fig_3">4</ref>, in the testing process, if the outcomes of two classifiers consist with each other, we take the outcomes as correct judgments and add them to a subset of final result which is named as result1. Otherwise, the test images causing inconsistent outcomes are joined to a dataset called uncertain set. To decide which type each instance in the testing set  belongs to, we use the original gray information to calculate closeness of these instances to benign and malignant ones in the training set. In the process of closeness calculation, benign and malignant images in the training dataset were clustered into several subclasses respectively, with the help of which we could obtain the cluster centers and number of each subclass in both two types for the next similarity measure.</p><p>In procedure of clustering, instances in training set are clustered by hierarchical K-means method <ref type="bibr" target="#b51">[52]</ref>. In this method, images could be separated into several subclasses with unbalanced scales. Owing to the hierarchical method, the native data structure of dataset is kept by these clustering centers and unbalanced scales, both of which play important roles in the whole mechanism. According to the result of closeness measure, the uncertain set is divided into two types, forming result2, the other subset of final result. Consequently, the final result contains result1 and result2 is obtained.</p><p>The similarity to each kind of mass images is defined as follows. Here, instance in the uncertain set is x U i ; i ¬º 1; 2; ‚Ä¶m; clusters of benign and malignant ones in training set is c B j ; j ¬º 1; 2; ‚Ä¶n and c M j ; j ¬º 1; 2; ‚Ä¶n respectively; numbers of instances in subclass are N B j ; j ¬º 1; 2; ‚Ä¶n in benign data and N M j ; j ¬º 1; 2; ‚Ä¶n in malignant data; jx U i √Ä c B j j and jx U i √Ä c M j j were both euclidean distance of uncertain instance x U i to center of clustering. The similarity of one instance in the uncertain set to benign ones is:</p><formula xml:id="formula_4">S U iB ¬º 1 P n j ¬º 1 N B j NB jx U i √Ä c B j j ; i ¬º 1; 2; ‚Ä¶m<label>√∞5√û</label></formula><p>And similarity of one instance in the uncertain set to malignant ones is:</p><formula xml:id="formula_5">S U iM ¬º 1 P n j ¬º 1 NM j NM jx U i √Ä c M j j ; i ¬º 1; 2; ‚Ä¶m<label>√∞6√û</label></formula><p>After having obtained the similarity value of each instance, we make the final decision for this subset obey the rules given below:</p><p>If S U iB 4 S U iM , the instance is considered as benign; If S U iB o S U iM , the instance is considered as malignant; If S U iB ¬º S U iM which is rare in our experiments. We consider the instance is benign in accordance with the statistics of American government that the risk of benign breast masses was almost 3 times bigger than that of malignant ones <ref type="bibr" target="#b52">[53]</ref>.</p><p>The deep structure framework is proposed for extracting global, local and detail symptoms of mass images, which helped to form a unified description of mass images. The decision mechanism imitated physicians' diagnosis process which mainly contained symptom comparisons and antidiastole. Deep features extracted by our CNN played important roles in the scheme as they were thought to be in accordance with cognitive principle of human brain, as they were better simulations of the impressions left by masses on doctors, respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Datasets and metrics</head><p>In this section, datasets and evaluation methods for both proposed method and reference methods were introduced.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Datasets</head><p>The dataset we chose to train our CNN model is a subset of Imagenet which is named LSVRC, and it contains more than 1 million natural images. Being the most popular natural image databases in the field of deep learning, LSVRC has become the choice of many researchers. It provides enough instances with determined labels, and this is quite important in training a supervised deep model.</p><p>Besides, the dataset we chose to do fine-tuning on for our CNN and conduct experiments on was a subset from the Digital Database for Screening Mammography (DDSM) <ref type="bibr">[54]</ref>. The database is provided by the University of South Florida. DDSM contains more than 2600 cases, and each case includes four images above breast, along with associated patient information (age at time of study, ACR breast density rating, subtlety rating for abnormalities, ACR keyword description of abnormalities) and image information (scanner, spatial resolution, etc.). In this database, images containing suspicious areas have associated pixel-level "ground truth" information about the locations and types of suspicious regions.</p><p>The mammograms in DDSM have been detected and labeled to generate a dataset of 600 images before the classification, of which 50% are benign and 50% are malignant. Meanwhile, all the mass images we chose were representative and challenging ones. The mass images were divided into both training and testing sets of same size, of each of these sets were 150 benign and 150 malignant ones. And each instance in the dataset is a gray scale image with the size of 227 √Ç 227. The method with which we detected the mass region was proposed in our previous work <ref type="bibr" target="#b53">[55]</ref>. In order to improve the performance of CNN, both the training and testing images had been normalized and whitened before they were input to the network. And these operations were widely applied in a few deep learning models. Instances in the dataset were subtracted by their mean and they were normalized to the range of [0, 1]  according to <ref type="bibr" target="#b54">[56]</ref>. Then we whitened normalized data with the method named PCA whitening by dividing the standard deviation of its elements.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Metrics</head><p>To evaluate the performance of our proposed framework, we used both objective and subjective evaluation. With the help of consideration from these two aspects, we could appraise our method more effectively.</p><p>The objective measures we chose were receiver operating characteristic (ROC) curves and classification accuracy with deviations, both of which were preferred in most papers for evaluation of classification methods.</p><p>The definition of ROC curve is:</p><formula xml:id="formula_6">sensitivity ¬º TP=√∞TP √æ FN√û specificity ¬º TN=√∞FP √æ TN√û</formula><p>Here TP stands for the true positive cases in detection results, and TN denotes the true negative cases. In addition, FP contains the false positive cases, and FN equals the false negative cases. In the figure of ROC curve, the ordinate and abscissa were sensitivity and specificity respectively. A larger area under this curve stands for a better classification performance. Another objective evaluation was the accuracy with deviations. In the task of cataloging mass images, classification accuracy is of vital importance. Performance with high accuracy could provide the doctor with a lot of help in the diagnostic process, which was a matter of survival and cure rates for patients. Given all these above, the accuracy with deviations was applied in this paper both for evaluation of our framework and for comparison with traditional methods. The calculation of these evaluation metrics were given as follows:</p><formula xml:id="formula_7">accuracy ¬º 1 N X N i ¬º 1 TP i √æTN i TP i √æFN i √æ FP i √æ TN i std ¬º accuracy √Ä mean√∞accuracy√û N</formula><p>Here, N is the number of testing times. TP, TN, FP and FN are in the same senses as they are mentioned above. Two subjective evaluations we chose were described next.</p><p>In order to show the performance of feature extraction more intuitively, we chose t-distributed stochastic neighbor embedding (t-SNE) <ref type="bibr" target="#b47">[48]</ref> as one of the subjective evaluation methods. The t-SNE method visualizes high-dimensional deep features and original images by giving each data point or instance a location in a two-dimensional map. This method was proposed by Hinton in 2006, and it has been proved to be quite effective in evaluation of various kinds of features. The t-SNE map can be explained as that the more linearly separable points in the two-dimensional map, the better this feature will perform.</p><p>The last subjective metric was inspired by deconvolution network. To show what features of different levels are like, we applied deconvolution, activation and other operations on features in middle and high levels <ref type="bibr" target="#b55">[57,</ref><ref type="bibr" target="#b56">58]</ref>. With the help of all these means, the rebuilding maps which were in the same size of input images assisted to show the emphasis of features on different levels. And we benefit from it to know whether these features could represent mass images in different scales.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Results and analysis</head><p>We compared the performance of the proposed framework with that of representative methods in the literature. Even the scales of datasets differed in these methods, they were in the same format. So the corresponding previously reported classification performance was still an effective standard to evaluate the improvement and setback of various methods. As shown in Table <ref type="table" target="#tab_3">4</ref>, a number of features were employed in these methods. It was obvious that frameworks with relatively good results used to apply 3 or more different kinds of features in the classification  task. As a result, the feature extraction process was complex in structure. Meanwhile, the challenge of making effective use of various types of features was inevitable in these frameworks, causing most of these researches made many efforts to find a fusion strategy for these features. However, methods with less complexity in feature extraction step did not seem to gain satisfactory results on a larger dataset which contained more than 200 instances. And results show clearly that a single kind of traditional feature was not sufficient for this task. The proposed framework took full advantage of deep features from a single CNN model, forming a unified feature extraction structure. From the comparison in the table, the proposed decision mechanism was not simple enough but effective to obtain good performance on a testing dataset of 300 instances. Both the scale of dataset applied and classification accuracy of our method were competitive among all these frameworks. Because it was a better simulation of physician's diagnosis procedure, our proposed method made more sense in terms of CAD. As shown in Table <ref type="table" target="#tab_3">4</ref> and Fig. <ref type="figure" target="#fig_5">6</ref>, if the middle-level and highlevel features were used individually for the classification task, performances were not that good. Once our decision mechanism joined, the whole framework could outperform a lot at both ROC curve and classification accuracy. Fig. <ref type="figure" target="#fig_6">7</ref> also demonstrates the ability of CNN model to extract discriminative features in a intuitive way. The method of t-SNE was used to form maps visualizing instances of the dataset in different levels. The evaluation criterion was that the more instances in the map were separable the better this feature performed. Here, the maps shown from left to right are the t-SNE maps of original images, middlelevel and high-level features extracted from our CNN respectively. Different colors represented instances of different types. From all these maps, we could obviously notice that deep features improve the differentiation of two types of instances, which equaled that instances were much more separable after they were represented by deep features. It was of great help for classifiers to distinguish different kinds of instances.</p><p>To show what the features from different layers focused on, we adopted a deconvolution based method to visualize images hierarchically. As shown in Figs. <ref type="figure" target="#fig_7">8</ref> and<ref type="figure" target="#fig_8">9</ref>, the first rows in both figures stood for original images of selected instances while the middle and bottom ones were middle-level and high-level features. As all the images came from the associated layers, they represented what levels emphasized on. And it was in evidence that maps from high-level feature paid more attention to the overall feel of mass images while that from middle-level feature and original image captured more local and details of the images. Meanwhile, the hierarchical expression was a progressive one during its  evaluation, which makes the feature representation more consistent with thinking activities in doctors' brain during the diagnosis.</p><p>In order to show the stability of our proposed algorithm when it comes to a situation that only a tiny database of breast mass images could be obtained, we execute our scheme on different scales of datasets and compare its outcomes with the state of art algorithm (BoW). In Fig. <ref type="figure" target="#fig_9">10</ref>, points in the red curve stand for the classification accuracy of our method when it comes to datasets of various scales (100, 200, ‚Ä¶600). And the blue curve is the performance of BoW-based scheme which is the state of art one in mass classification. The other two curves are also typical schemes which are histogram of oriented Gradient (HOG) and scale invariant feature transform (SIFT) based methods. From the two curves of different colors, we could significantly draw a conclusion that our proposed method outperforms the state of art one on both stability and the accuracy of classification. Meanwhile it also demonstrates our method to be effective when large datasets are not available in the medical field which is one of the main challenges presented by this special issue.</p><p>In addition, in order to achieve the state of art results that were described in our paper, we performed various experiments to obtain the most suitable network structure for this task. We compared our network structure with the most widely used variations of Alex net at aspects of model size, number of parameters, time consuming and classification accuracy. These two models have been proved to be effective in a variety of tasks. From the contents of Table <ref type="table" target="#tab_4">5</ref>, we could see that our network achieved quite a competitive result with less storage space than Caffe-ref net. Though the classification accuracy of our net is a little bit worse than that of the VGG net, the time consuming, storage space and number of parameters are far much better.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Conclusion</head><p>In this paper, we proposed a novel mammographic masses classification framework. With the help of our previous work, the mass images were obtained from DDSM to form a dataset first. Instances in the dataset were whitened for preprocessing. Based on the dataset, we applied fine-tuning operation on the trained deep CNN model to acquire the feature extraction network for the   next procedures. All blocks in the deep structure were the most effective ones in the state-of-the-art papers. Then middle-level and high-level features were extracted from different layers of this network for training two linear SVM classifiers. Owing to the validity of deep features, the simplest linear classifiers are powerful enough to distinguish instances. Combining outcomes of these two classifiers, we got result1 and the uncertain set. Meanwhile, the weighted similarity was calculated to decide labels of the uncertain set. Simulating the hierarchical structure of human brain and its data processing mechanism, the deep CNN model and features extracted from it was of great assistance in simulating multiscale impressions left by mass images on physician's brain.</p><p>And the decision mechanism we proposed was a better imitation of doctors' symptom comparisons and antidiastole procedures in the diagnosis. Evaluated by two objective measures and compared with traditional effective methods, the proposed framework achieved better performance in classification accuracy. According to the results of two visualization strategies, the effectiveness of deep features in classification task for mass images has been verified intuitively. In addition, the experimental results demonstrated that the CNN model with adjustment for certain data was effective even the scale of specific database was not that large. In the future, we will make effort to find a better variation of CNN to help obtain more describable features and design a decision mechanism which is more like the real diagnosis procedure.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 1 .</head><label>1</label><figDesc>Fig. 1. Examples of benign and malignant breast mass images. Instances in the first row are benign masses while ones in the second row are malignant.</figDesc><graphic coords="2,112.71,58.62,360.00,179.55" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 2 .</head><label>2</label><figDesc>Fig. 2. The main components and connections between them in a deep CNN model. (Convolution) stands for the convolution operation in each layer. (ReLU) is the activation function we chose. (Pooling) is the max pooling operation. These operation units connected successively in each layer. In addition, (Fully-connected layers) which are similar to the traditional neural networks appeared in the last few layers.</figDesc><graphic coords="3,86.51,449.12,432.00,266.23" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 3 .</head><label>3</label><figDesc>Fig. 3. The directed acyclic graph (DAG) representation of a CNN. Different layers in the structure were described by operation of weights and input successively.</figDesc><graphic coords="5,86.51,58.62,432.00,79.37" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Fig. 4 .</head><label>4</label><figDesc>Fig. 4. The flow chart of the test process. As illustrated from the top down in this figure, high-level and middle-level features of a test image were extracted from the fine-tuned network before. Then these features were classified by two classifiers in a two-step decision mechanism.</figDesc><graphic coords="5,317.03,165.26,240.23,475.77" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Fig. 5 .</head><label>5</label><figDesc>Fig. 5. Performance of kernel SVM with different C and Gamma.</figDesc><graphic coords="6,35.26,58.62,245.98,222.92" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Fig. 6 .</head><label>6</label><figDesc>Fig. 6. ROC curves of middle-level feature, high-level feature based classification and our proposed method.</figDesc><graphic coords="7,315.21,246.96,243.86,201.77" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Fig. 7 .</head><label>7</label><figDesc>Fig. 7. The t-SNE maps of hierarchical features. From left to right are respectively original images, middle-level feature and high-level feature.</figDesc><graphic coords="8,112.71,58.62,360.00,89.12" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Fig. 8 .</head><label>8</label><figDesc>Fig. 8. Hierarchical features visualization of some benign instances. Each column represents an instance and from top to bottom are original images, middle-level feature and high-level feature.</figDesc><graphic coords="8,112.71,184.14,360.00,268.78" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Fig. 9 .</head><label>9</label><figDesc>Fig. 9. Hierarchical features visualization of some malignant instances. Each column represents an instance and from top to bottom are original images, middle-level feature and high-level feature.</figDesc><graphic coords="9,122.51,58.62,360.00,269.07" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>Fig. 10 .</head><label>10</label><figDesc>Fig. 10. Comparison of our scheme with other feature based methods on datasets of different sizes.</figDesc><graphic coords="9,45.07,372.47,245.98,164.18" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1</head><label>1</label><figDesc>Detailed parameters of each layer.</figDesc><table><row><cell>Name</cell><cell>Filter size</cell><cell>Filter dimension</cell><cell>Stride</cell><cell>Padding</cell></row><row><cell>Conv1</cell><cell>7</cell><cell>1</cell><cell>2</cell><cell>0</cell></row><row><cell>ReLU1</cell><cell>1</cell><cell></cell><cell>1</cell><cell>0</cell></row><row><cell>Pooling1</cell><cell>3</cell><cell></cell><cell>3</cell><cell>2</cell></row><row><cell>Conv2</cell><cell>5</cell><cell>96</cell><cell>1</cell><cell>2</cell></row><row><cell>ReLU2</cell><cell>1</cell><cell></cell><cell>1</cell><cell>0</cell></row><row><cell>Pooling2</cell><cell>2</cell><cell></cell><cell>2</cell><cell>1</cell></row><row><cell>Conv3</cell><cell>3</cell><cell>256</cell><cell>1</cell><cell>1</cell></row><row><cell>ReLU3</cell><cell>1</cell><cell></cell><cell>1</cell><cell>0</cell></row><row><cell>Conv4</cell><cell>3</cell><cell>384</cell><cell>1</cell><cell>1</cell></row><row><cell>ReLU4</cell><cell>1</cell><cell></cell><cell>1</cell><cell>0</cell></row><row><cell>Conv5</cell><cell>3</cell><cell>384</cell><cell>1</cell><cell>1</cell></row><row><cell>ReLU5</cell><cell>1</cell><cell></cell><cell>1</cell><cell>0</cell></row><row><cell>Pooling5</cell><cell>3</cell><cell></cell><cell>3</cell><cell>1</cell></row><row><cell>Fc6</cell><cell>1</cell><cell>256</cell><cell>1</cell><cell>0</cell></row><row><cell>ReLU6</cell><cell>1</cell><cell></cell><cell>1</cell><cell>0</cell></row><row><cell>Fc7</cell><cell>1</cell><cell>2048</cell><cell>1</cell><cell>0</cell></row><row><cell>ReLU7</cell><cell>1</cell><cell></cell><cell>1</cell><cell>0</cell></row><row><cell>Fc8</cell><cell>1</cell><cell>2</cell><cell>1</cell><cell>0</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2</head><label>2</label><figDesc>Main parameters of each layer.The name, size, number of maps and whether there were ReLU and Pooling units in each layer were illustrated in the table. In the name column, (Input) stands for the mass images while (Conv) and (Fc) represent convolution and fully-connected layer respectively. Values in the size and number of maps column are the size and number of output map in each layer. (‚àö) and ( √Ç ) in the last two columns illustrate whether there were ReLU or Pooling units in each layer.</figDesc><table><row><cell>Name</cell><cell>Number of maps</cell><cell>ReLU</cell><cell>Pooling</cell></row><row><cell>Input</cell><cell>1</cell><cell>√Ç</cell><cell>√Ç</cell></row><row><cell>Conv1</cell><cell>96</cell><cell>‚àö</cell><cell>‚àö</cell></row><row><cell>Conv2</cell><cell>256</cell><cell>‚àö</cell><cell>‚àö</cell></row><row><cell>Conv3</cell><cell>384</cell><cell>‚àö</cell><cell>√Ç</cell></row><row><cell>Conv4</cell><cell>384</cell><cell>‚àö</cell><cell>√Ç</cell></row><row><cell>Conv5</cell><cell>256</cell><cell>‚àö</cell><cell>‚àö</cell></row><row><cell>Fc6</cell><cell>2048</cell><cell>‚àö</cell><cell>√Ç</cell></row><row><cell>Fc7</cell><cell>2048</cell><cell>‚àö</cell><cell>√Ç</cell></row><row><cell>Fc8</cell><cell>2</cell><cell>‚àö</cell><cell>√Ç</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 3</head><label>3</label><figDesc>Classification accuracy of k-fold cross validation.</figDesc><table><row><cell cols="2">k (k-fold) Classification accuracy (%)</cell></row><row><cell>2</cell><cell>96.7</cell></row><row><cell>3</cell><cell>97.1</cell></row><row><cell>4</cell><cell>97.3</cell></row><row><cell>5</cell><cell>97.6</cell></row><row><cell>6</cell><cell>97.5</cell></row><row><cell>7</cell><cell>97.6</cell></row><row><cell>8</cell><cell>97.5</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 4</head><label>4</label><figDesc>Classification performance of different methods in literature.</figDesc><table><row><cell>Reference</cell><cell>Features</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 5</head><label>5</label><figDesc>Comparison of our network with traditional ones.</figDesc><table><row><cell>Name</cell><cell>Size</cell><cell cols="2">Parameters Time per image</cell><cell>Classification accuracy</cell></row><row><cell></cell><cell>(MB)</cell><cell></cell><cell>(ms)</cell><cell>(%)</cell></row><row><cell cols="2">Caffe-ref 233</cell><cell>6.1e √æ 07</cell><cell>2.97</cell><cell>92.0</cell></row><row><cell>VGG</cell><cell>528</cell><cell>1.4e √æ 08</cell><cell>13.53</cell><cell>97.0</cell></row><row><cell>Ours</cell><cell>204</cell><cell>5.8e √æ07</cell><cell>1.10</cell><cell>96.7</cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" xml:id="foot_0"><p>Please cite this article as: Z. Jiao, et al., A deep feature based framework for breast masses classification, Neurocomputing (2016), http: //dx.doi.org/10.1016/j.neucom.2016.02.060i</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" xml:id="foot_1"><p>Z. Jiao et al. / Neurocomputing ‚àé (‚àé‚àé‚àé‚àé) ‚àé‚àé‚àé-‚àé‚àé‚àé</p></note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title/>
		<author>
			<persName><forename type="first">A</forename><surname>Jemal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Siegel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Ward</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">CA Cancer J. Clin</title>
		<imprint>
			<biblScope unit="volume">58</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="71" to="96" />
			<date type="published" when="2008">2008. 2008</date>
		</imprint>
	</monogr>
	<note>Cancer statistics</note>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Computer-aided diagnosis in medical imaging: historical review, current status and future potential</title>
		<author>
			<persName><forename type="first">K</forename><surname>Doi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Comput. Med. Imaging Graph</title>
		<imprint>
			<biblScope unit="volume">31</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="198" to="211" />
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Computer-aided diagnosis in chest radiography: a survey</title>
		<author>
			<persName><forename type="first">B</forename><surname>Van Ginneken</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">M</forename><surname>Ter Haar Romeny</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Viergever</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Med. Imaging</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page" from="1228" to="1241" />
			<date type="published" when="2001">2001</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Improving breast cancer diagnosis with computer-aided diagnosis</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">M</forename><surname>Nishikawa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">A</forename><surname>Schmidt</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Acad. Radiol</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="22" to="33" />
			<date type="published" when="1999">1999</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Improvement in radiologists&apos; detection of clustered microcalcifications on mammograms: the potential of computeraided diagnosis</title>
		<author>
			<persName><forename type="first">H.-P</forename><surname>Chan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Doi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">J</forename><surname>Vybrony</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Invest. Radiol</title>
		<imprint>
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page" from="1102" to="1110" />
			<date type="published" when="1990">1990</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Measures of acutance and shape for classification of breast tumors</title>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">M</forename><surname>Rangayyan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><forename type="middle">M</forename><surname>El-Faramawy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">L</forename><surname>Desautels</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Med. Imaging</title>
		<imprint>
			<biblScope unit="volume">16</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="799" to="810" />
			<date type="published" when="1997">1997</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Mammographic masses characterization based on localized texture and dataset fractal analysis using linear, neural and support vector machine classifiers</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">E</forename><surname>Mavroforakis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">V</forename><surname>Georgiou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Dimitropoulos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Artif. Intell. Med</title>
		<imprint>
			<biblScope unit="volume">37</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="145" to="162" />
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Temporal change analysis for characterization of mass lesions in mammography</title>
		<author>
			<persName><forename type="first">S</forename><surname>Timp</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Varela</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Karssemeijer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Med. Imaging</title>
		<imprint>
			<biblScope unit="volume">26</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="945" to="953" />
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Development of tolerant features for characterization of masses in mammograms</title>
		<author>
			<persName><forename type="first">A</forename><surname>Rojas-Dom√≠nguez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">K</forename><surname>Nandi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Comput. Biol. Med</title>
		<imprint>
			<biblScope unit="volume">39</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="678" to="688" />
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Mutual information-based SVM-RFE for diagnostic classification of digitized mammograms</title>
		<author>
			<persName><forename type="first">S</forename><surname>Yoon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Kim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pattern. Recognit. Lett</title>
		<imprint>
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="issue">16</biblScope>
			<biblScope unit="page" from="1489" to="1495" />
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Wavelet packet energy, Tsallis entropy and statistical parameterization for support vector-based and neuralbased classification of mammographic regions</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">F</forename><surname>Ramirez-Villegas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">F</forename><surname>Ramirez-Moreno</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neurocomputing</title>
		<imprint>
			<biblScope unit="volume">77</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="82" to="100" />
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Automatic detection of breast cancers in mammograms using structured support vector machines</title>
		<author>
			<persName><forename type="first">D</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">A</forename><surname>Heng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neurocomputing</title>
		<imprint>
			<biblScope unit="volume">72</biblScope>
			<biblScope unit="page" from="3296" to="3302" />
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">A novel soft cluster neural network for the classification of suspicious areas in digital mammograms</title>
		<author>
			<persName><forename type="first">B</forename><surname>Verma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Mcleod</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Klevansky</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pattern. Recognit</title>
		<imprint>
			<biblScope unit="volume">42</biblScope>
			<biblScope unit="issue">9</biblScope>
			<biblScope unit="page" from="1845" to="1852" />
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Classification of benign and malignant patterns in digital mammograms for the diagnosis of breast cancer</title>
		<author>
			<persName><forename type="first">B</forename><surname>Verma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Mcleod</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Klevansky</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Expert Syst. Appl</title>
		<imprint>
			<biblScope unit="volume">37</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="3344" to="3351" />
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Latent feature mining of spatial and marginal characteristics for mammographic mass classification</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Gao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neurocomputing</title>
		<imprint>
			<biblScope unit="volume">144</biblScope>
			<biblScope unit="page" from="107" to="118" />
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Mammogram classification using two dimensional discrete wavelet transform and gray-level co-occurrence matrix for detection of breast cancer</title>
		<author>
			<persName><forename type="first">S</forename><surname>Beura</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Majhi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Dash</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neurocomputing</title>
		<imprint>
			<biblScope unit="volume">154</biblScope>
			<biblScope unit="page" from="1" to="14" />
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Breast mass classification in digital mammography based on extreme learning machine</title>
		<author>
			<persName><forename type="first">W</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Ma</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neurocomputing</title>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Latent semantic kernels</title>
		<author>
			<persName><forename type="first">N</forename><surname>Cristianini</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Shawe-Taylor</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Lodhi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Intell. Inf. Syst</title>
		<imprint>
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="issue">2-3</biblScope>
			<biblScope unit="page" from="127" to="152" />
			<date type="published" when="2002">2002</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Visual categorization with bags of keypoints</title>
		<author>
			<persName><forename type="first">Gabriella</forename><surname>Csurka</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Fan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Workshop on Statistical Learning in Computer Vision</title>
		<meeting>the Workshop on Statistical Learning in Computer Vision</meeting>
		<imprint>
			<date type="published" when="2004">2004</date>
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Learning object categories from Google&apos;s image search</title>
		<author>
			<persName><forename type="first">Robert</forename><surname>Fergus</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Tenth IEEE International Conference on Computer Vision, ICCV</title>
		<meeting>the Tenth IEEE International Conference on Computer Vision, ICCV</meeting>
		<imprint>
			<date type="published" when="2005">2005</date>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="1816" to="1823" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">X-ray categorization and retrieval on the organ and pathology level, using patch-based visual words</title>
		<author>
			<persName><forename type="first">U</forename><surname>Avni</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Greenspan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Konen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Med. Imaging</title>
		<imprint>
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="733" to="746" />
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Deep learning</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nature</title>
		<imprint>
			<biblScope unit="volume">521</biblScope>
			<biblScope unit="issue">7553</biblScope>
			<biblScope unit="page" from="436" to="444" />
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<author>
			<persName><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">C</forename><surname>Courville</surname></persName>
		</author>
		<title level="m">Deep learning of representations, Handbook on Neural Information Processing</title>
		<imprint>
			<date type="published" when="2013">2013</date>
			<biblScope unit="volume">49</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Reducing the dimensionality of data with neural networks</title>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">R</forename><surname>Salakhutdinov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Science</title>
		<imprint>
			<biblScope unit="volume">313</biblScope>
			<biblScope unit="issue">5786</biblScope>
			<biblScope unit="page" from="504" to="507" />
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Cl√©ment Farabet, Convolutional networks and applications in vision</title>
		<author>
			<persName><forename type="first">Yann</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Koray</forename><surname>Kavukcuoglu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of IEEE International Symposium on Circuits and Systems (ISCAS)</title>
		<meeting>IEEE International Symposium on Circuits and Systems (ISCAS)</meeting>
		<imprint>
			<date type="published" when="2010">2010</date>
			<biblScope unit="page" from="253" to="256" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title/>
		<author>
			<persName><forename type="first">A</forename><surname>Ng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sparse</forename><surname>Autoencoder</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="s">CS294A Lecture Notes</title>
		<imprint>
			<biblScope unit="volume">72</biblScope>
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<author>
			<persName><forename type="first">Ruslan</forename><surname>Salakhutdinov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Geoffrey</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
		<title level="m">Proceedings of the 12th International Conference on Artificial Intelligence and Statistics</title>
		<meeting>the 12th International Conference on Artificial Intelligence and Statistics</meeting>
		<imprint>
			<date type="published" when="2009">2009</date>
			<biblScope unit="page" from="448" to="455" />
		</imprint>
	</monogr>
	<note>Deep boltzmann machines</note>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">The recurrent temporal restricted boltzmann machine</title>
		<author>
			<persName><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Geoffrey</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Graham</forename><forename type="middle">W</forename><surname>Taylor</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Adv. Neural Inf. Process. Syst</title>
		<imprint>
			<biblScope unit="page" from="1601" to="1608" />
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Neocognitron: a self-organizing neural network model for a mechanism of pattern recognition unaffected by shift in position</title>
		<author>
			<persName><forename type="first">K</forename><surname>Fukushima</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Biol. Cybern</title>
		<imprint>
			<biblScope unit="volume">36</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="193" to="202" />
			<date type="published" when="1980">1980</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Gradient-based learning applied to document recognition</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Bottou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Proc. IEEE</title>
		<imprint>
			<biblScope unit="volume">86</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="2278" to="2324" />
			<date type="published" when="1998">1998</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Handwritten digit recognition with a back-propagation network</title>
		<author>
			<persName><forename type="first">B</forename><surname>Le Cun</surname></persName>
		</author>
		<author>
			<persName><surname>Boser</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Adv. Neural Inf. Process. Syst</title>
		<imprint>
			<date type="published" when="1990">1990</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Greedy layer-wise training of deep networks</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Lamblin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Popovici</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Adv. neural Inf. Process. Syst</title>
		<imprint>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="page">153</biblScope>
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Imagenet classification with deep convolutional neural networks</title>
		<author>
			<persName><forename type="first">Alex</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Geoffrey</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Adv. Neural Inf. Process. Syst</title>
		<imprint>
			<biblScope unit="page" from="1097" to="1105" />
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Learning hierarchical features for scene labeling</title>
		<author>
			<persName><forename type="first">C</forename><surname>Farabet</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Couprie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Najman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Pattern. Anal. Mach. Intell</title>
		<imprint>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="1915" to="1929" />
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Rich feature hierarchies for accurate object detection and semantic segmentation</title>
		<author>
			<persName><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="580" to="587" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Supervised learning of image restoration with convolutional networks</title>
		<author>
			<persName><forename type="first">Viren</forename><surname>Jain</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 11th IEEE International Conference on Computer Vision, ICCV</title>
		<meeting>the 11th IEEE International Conference on Computer Vision, ICCV</meeting>
		<imprint>
			<date type="published" when="2007">2007</date>
			<biblScope unit="page" from="1" to="8" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Natural image denoising with convolutional networks</title>
		<author>
			<persName><forename type="first">Viren</forename><surname>Jain</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sebastian</forename><surname>Seung</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Adv. Neural Inf. Process. Syst</title>
		<imprint>
			<biblScope unit="page" from="769" to="776" />
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Connectomic reconstruction of the inner plexiform layer in the mouse retina</title>
		<author>
			<persName><forename type="first">M</forename><surname>Helmstaedter</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">L</forename><surname>Briggman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">C</forename><surname>Turaga</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nature</title>
		<imprint>
			<biblScope unit="volume">500</biblScope>
			<biblScope unit="issue">7461</biblScope>
			<biblScope unit="page" from="168" to="174" />
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Deep neural networks segment neuronal membranes in electron microscopy images</title>
		<author>
			<persName><forename type="first">Dan</forename><surname>Ciresan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Adv. Neural Inf. Process. Syst</title>
		<imprint>
			<biblScope unit="page" from="2843" to="2851" />
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Mitosis detection in breast cancer histology images with deep neural networks</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">C</forename><surname>Cire≈üan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Giusti</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">M</forename><surname>Gambardella</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">MICCAI</title>
		<imprint>
			<biblScope unit="page" from="411" to="418" />
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Deep convolutional neural networks for multimodality isointense infant brain image segmentation</title>
		<author>
			<persName><forename type="first">W</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Deng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neuroimage</title>
		<imprint>
			<biblScope unit="volume">108</biblScope>
			<biblScope unit="page" from="214" to="224" />
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Deep neural networks rival the representation of primate IT cortex for core visual object recognition</title>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">F</forename><surname>Cadieu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Hong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">L</forename><surname>Yamins</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">PLoS Comput. Biol</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page">1003963</biblScope>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Human-level control through deep reinforcement learning</title>
		<author>
			<persName><forename type="first">V</forename><surname>Mnih</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Kavukcuoglu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Silver</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nature</title>
		<imprint>
			<biblScope unit="volume">518</biblScope>
			<biblScope unit="issue">7540</biblScope>
			<biblScope unit="page" from="529" to="533" />
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Deep sparse rectifier neural networks</title>
		<author>
			<persName><forename type="first">Xavier</forename><surname>Glorot</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Antoine</forename><surname>Bordes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Conference on Artificial Intelligence and Statistics</title>
		<meeting>the International Conference on Artificial Intelligence and Statistics</meeting>
		<imprint>
			<date type="published" when="2011">2011</date>
			<biblScope unit="page" from="315" to="323" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Improving deep neural networks for LVCSR using rectified linear units and dropout</title>
		<author>
			<persName><forename type="first">George</forename><forename type="middle">E</forename><surname>Dahl</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tara</forename><forename type="middle">N</forename><surname>Sainath</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Geoffrey</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of IEEE International Conference on Acoustics, Speech and Signal Processing</title>
		<meeting>IEEE International Conference on Acoustics, Speech and Signal Processing</meeting>
		<imprint>
			<date type="published" when="2013">2013</date>
			<biblScope unit="page" from="8609" to="8613" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<monogr>
		<title level="m" type="main">Decaf: a deep convolutional activation feature for generic visual recognition</title>
		<author>
			<persName><forename type="first">J</forename><surname>Donahue</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Vinyals</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1310.1531</idno>
		<imprint>
			<date type="published" when="2013-10">Oct. 2013</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Receptive fields, binocular interaction and functional architecture in the cat&apos;s visual cortex</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">H</forename><surname>Hubel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">N</forename><surname>Wiesel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Physiol</title>
		<imprint>
			<biblScope unit="volume">160</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page">106</biblScope>
			<date type="published" when="1962">1962</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Visualizing data using t-SNE</title>
		<author>
			<persName><forename type="first">L</forename><surname>Van Der Maaten</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Mach. Learn. Res</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="page">85</biblScope>
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Imagenet: a large-scale hierarchical image database</title>
		<author>
			<persName><forename type="first">Jia</forename><surname>Deng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2009">2009</date>
			<biblScope unit="page" from="248" to="255" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Large-scale machine learning with stochastic gradient descent</title>
		<author>
			<persName><forename type="first">L√©on</forename><surname>Bottou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of COMPSTAT&apos;2010</title>
		<meeting>COMPSTAT&apos;2010</meeting>
		<imprint>
			<publisher>Physica-Verlag HD</publisher>
			<date type="published" when="2010">2010</date>
			<biblScope unit="page" from="177" to="186" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">LIBSVM: a library for support vector machines</title>
		<author>
			<persName><forename type="first">C.-C</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C.-J</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Trans. Intell. Syst. Tech</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page">27</biblScope>
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">VLFeat: an open and portable library of computer vision algorithms</title>
		<author>
			<persName><forename type="first">Andrea</forename><surname>Vedaldi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Brian</forename><surname>Fulkerson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 18th ACM International Conference on Multimedia</title>
		<meeting>the 18th ACM International Conference on Multimedia</meeting>
		<imprint>
			<date type="published" when="2010">2010</date>
			<biblScope unit="page" from="1469" to="1472" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<monogr>
		<author>
			<persName><forename type="first">N</forename><surname>Howlader</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Noone</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Krapcho</surname></persName>
		</author>
		<title level="m">SEER cancer statistics review</title>
		<meeting><address><addrLine>Bethesda, MD</addrLine></address></meeting>
		<imprint>
			<publisher>National Cancer Institute</publisher>
			<date type="published" when="1975">1975-2008. 2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">Mammographic mass segmentation: embedding multiple features in vector-valued level set in ambiguous regions</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Tao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Gao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pattern Recognit</title>
		<imprint>
			<biblScope unit="volume">44</biblScope>
			<biblScope unit="issue">9</biblScope>
			<biblScope unit="page" from="1903" to="1915" />
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">An analysis of single-layer networks in unsupervised feature learning</title>
		<author>
			<persName><forename type="first">Adam</forename><surname>Coates</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrew</forename><forename type="middle">Y</forename><surname>Ng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Honglak</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of International Conference on Artificial Intelligence and Statistics</title>
		<meeting>International Conference on Artificial Intelligence and Statistics</meeting>
		<imprint>
			<date type="published" when="2011">2011</date>
			<biblScope unit="page" from="215" to="223" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<analytic>
		<title level="a" type="main">Visualizing and understanding convolutional networks</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">D</forename><surname>Zeiler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Fergus</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Comput. Vision-ECCV</title>
		<imprint>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="818" to="833" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<analytic>
		<title level="a" type="main">Deconvolutional networks</title>
		<author>
			<persName><forename type="first">Mathew</forename><forename type="middle">D</forename><surname>Zeiler</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2010">2010</date>
			<biblScope unit="page" from="2528" to="2535" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<analytic>
		<title level="a" type="main">Mammographic image classification using histogram intersection</title>
		<author>
			<persName><forename type="first">Erkang</forename><surname>Cheng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2010 IEEE International Symposium on Biomedical Imaging: From Nano to Macro</title>
		<meeting>the 2010 IEEE International Symposium on Biomedical Imaging: From Nano to Macro</meeting>
		<imprint>
			<date type="published" when="2010">2010</date>
			<biblScope unit="page" from="197" to="200" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b58">
	<analytic>
		<title level="a" type="main">Improving Computer-aided detection using convolutional neural networks and random view aggregation</title>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">R</forename><surname>Roth</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Liu</surname></persName>
		</author>
		<idno type="DOI">10.1109/TMI.2015.2482920</idno>
		<ptr target="http://dx.doi.org/10.1109/TMI.2015.2482920" />
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Med. Imaging</title>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b59">
	<analytic>
		<title level="a" type="main">From to 2001, he was a postdoctoral research fellow in the Department of Information Engineering at the Chinese University of Hong Kong. Since 2001, he joined the School of Electronic Engineering at Xidian University. Currently, he is a Professor of Pattern Recognition and Intelligent System, and Director of the VIPS Lab</title>
		<author>
			<persName><forename type="first">A</forename><surname>Vedaldi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Lenc</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1412.4564</idno>
	</analytic>
	<monogr>
		<title level="m">these areas, he has published 5 books and around 150 technical articles in refereed journals and proceedings including IEEE Transactions on Image Processing, the IEEE Transactions on Circuits and Systems for Video Technology, the IEEE Transactions on Neural Networks, and the IEEE Transactions on Systems, Man, and Cybernetics. He is on the editorial boards of several journals including Signal Processing (Elsevier), and Neurocomputing</title>
		<title level="s">His research interests include intelligent information processing. Xinbo Gao (M&apos;02-SM&apos;07) received the B.Eng., M.Sc. and Ph.D. degrees in signal and information processing from Xidian University</title>
		<meeting><address><addrLine>China; Xi&apos;an, China; China</addrLine></address></meeting>
		<imprint>
			<publisher>Elsevier</publisher>
			<date type="published" when="1994">2014. 1994. 1997. 1999. 2003. 2006. 2010. 1995. 1998. 2005</date>
		</imprint>
		<respStmt>
			<orgName>Xidian University ; Computer Science at Shizuoka University, Japan ; Xidian University ; Circuit and System from Xidian University</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">arXiv preprint</note>
	<note>Since 1998, she joined the School of Electronic Engineering at Xidian University. Currently, she is a Professor of Xidian University. Her research interests include computational intelligence, machine learning, and image processing. In these areas, she has published over 30 technical articles in refereed journals and proceedings including IEEE TCSVT, IJFS etc</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
