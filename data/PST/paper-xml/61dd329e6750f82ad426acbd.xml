<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Marvel: A Data-Centric Approach for Mapping Deep Learning Operators on Spatial Accelerators</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Prasanth</forename><surname>Chatarasi</surname></persName>
							<email>prasanth@ibm.com</email>
						</author>
						<author>
							<persName><forename type="first">Hyoukjun</forename><surname>Kwon</surname></persName>
							<email>hyoukjunkwon@fb.com</email>
						</author>
						<author>
							<persName><forename type="first">Angshuman</forename><surname>Parashar</surname></persName>
							<email>aparashar@nvidia.com</email>
						</author>
						<author>
							<persName><roleName>NVIDIA</roleName><forename type="first">Michael</forename><surname>Pellauer</surname></persName>
							<email>mpellauer@nvidia.com</email>
						</author>
						<author>
							<persName><forename type="first">Tushar</forename><surname>Krishna</surname></persName>
							<email>tushar@ece.gatech.edu</email>
						</author>
						<author>
							<persName><forename type="first">Vivek</forename><surname>Sarkar</surname></persName>
							<email>vsarkar@gatech.edu</email>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="institution">Georgia Institute of Technology</orgName>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff1">
								<orgName type="institution">Georgia Institute of Technology</orgName>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff2">
								<orgName type="institution">Georgia Institute of Technology</orgName>
								<address>
									<addrLine>North Ave NW</addrLine>
									<postCode>30332</postCode>
									<settlement>Atlanta</settlement>
									<region>GA</region>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff3">
								<orgName type="institution">NVIDIA Corporation</orgName>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff4">
								<orgName type="institution">Technology</orgName>
								<address>
									<addrLine>Park Drive Floor 3 Westford</addrLine>
									<postCode>01886</postCode>
									<region>MA</region>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Marvel: A Data-Centric Approach for Mapping Deep Learning Operators on Spatial Accelerators</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="DOI">10.1145/3485137</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.2" ident="GROBID" when="2023-01-01T13:37+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Deep learning (spatial) accelerators</term>
					<term>compilers</term>
					<term>mappers/optimizers</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>A spatial accelerator's efficiency depends heavily on both its mapper and cost models to generate optimized mappings for various operators of DNN models. However, existing cost models lack a formal boundary over their input programs (operators) for accurate and tractable cost analysis of the mappings, and this results in adaptability challenges to the cost models for new operators. We consider the recently introduced Maestro Data-Centric (MDC) notation and its analytical cost model to address this challenge because any mapping expressed in the notation is precisely analyzable using the MDC's cost model.</p><p>In this article, we characterize the set of input operators and their mappings expressed in the MDC notation by introducing a set of conformability rules. The outcome of these rules is that any loop nest that is perfectly nested with affine tensor subscripts and without conditionals is conformable to the MDC notation. A majority of the primitive operators in deep learning are such loop nests. In addition, our rules enable us to automatically translate a mapping expressed in the loop nest form to MDC notation and use the MDC's cost model to guide upstream mappers. Our conformability rules over the input operators result in a structured mapping space of the operators, which enables us to introduce a mapper based on our decoupled off-chip/on-chip approach to accelerate mapping space exploration. Our mapper decomposes the original higher-dimensional mapping space of operators into two lower-dimensional off-chip and on-chip subspaces and then optimizes the off-chip subspace followed by the on-chip subspace. We implemented our overall approach in a tool called Marvel, and a benefit of our approach is that it applies to any operator conformable with the MDC notation. We evaluated Marvel over major DNN operators and compared it with past optimizers. CCS Concepts: • Hardware → Hardware accelerators; • Software and its engineering → Compilers; • Computing methodologies → Neural networks;</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>None of the prior work attempts at defining a formal boundary, and having no such boundaries can bring adaptability challenges to these cost models in the kernel optimizers (challenge 1).</head><p>Besides the lack of the existing cost model's formal boundaries, searching for optimal mappings is challenging because of the massive space of the operator's legal mappings on the accelerator configurations. For example, there are more than 10 19 valid mappings for the convolution operator on average for mapping ResNet50 <ref type="bibr" target="#b16">[17]</ref> and MobileNetV2 <ref type="bibr" target="#b42">[43]</ref> on a representative DNN edge accelerator. On one side, much of the prior work targeted hardware with limited capabilities (e.g., mRNA <ref type="bibr" target="#b56">[57]</ref> for the MAERI accelerator, TVM extensions <ref type="bibr" target="#b29">[30]</ref> for the VTA GEMM accelerator, and DeepTools <ref type="bibr" target="#b50">[51]</ref> for the RAPID AI accelerator), which makes them not directly applicable to generic spatial accelerators. On another side, prior work on generic templated spatial accelerators fixed certain aspects of the mapping space such as choice of parallel loops and loop orders <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b28">29,</ref><ref type="bibr" target="#b31">32,</ref><ref type="bibr" target="#b53">54,</ref><ref type="bibr" target="#b55">56]</ref>, and such limited exploration can limit the possibilities in achieving the accelerator's peak performance for diverse operators. To the best of our knowledge, Timeloop <ref type="bibr" target="#b33">[34]</ref> is the only framework that considers all aspects of a mapping for generic templated spatial accelerators. However, it employs either an exhaustive linear search or a random sampling-based heuristic to explore the search space. Hence, approaches supporting generic templated spatial accelerators and exploring all possible mappings suffer from a combinatoric explosion in the size of mapping space (challenge 2).</p><p>The key contributions of our work addressing the preceding two challenges are described next.</p><p>Conformable DNN operators. To address the first challenge, we consider the recently introduced Maestro Data-Centric (MDC) notation <ref type="bibr" target="#b23">[24]</ref> for expressing mappings onto generic templated spatial architectures. MDC is promising because any mapping expressed in the notation is precisely analyzable using the MDC's cost model <ref type="bibr" target="#b24">[25]</ref>. Instead of proposing an approach for validating an input mapping in the MDC notation, we slightly take a different direction. We introduce a set of conformability formal rules (Section 3) where if any operator satisfies/conforms to those rules, all possible mappings of the operator are expressible in the MDC notation. As an example, Table <ref type="table" target="#tab_0">1</ref> lists the conformability of the popular DNN operators with the MDC notation. Furthermore, all the primitive operators identified from profiling MLPerf benchmark suite <ref type="bibr" target="#b37">[38]</ref>, VGG16 <ref type="bibr" target="#b47">[48]</ref>, and AlexNet <ref type="bibr" target="#b21">[22]</ref> models are MDC conformable, and these operators did not require any rewriting to make them conformable with the MDC notation.</p><p>The MDC notation explicitly requires defining data movement aspects of a mapping, instead of inferring in loop nest notation, which makes estimating execution time and energy efficiency relatively faster. However, since MDC notation is relatively new, it can be challenging for architect/compiler experts to specify mappings with explicit data movement. Hence, we introduce a transformation (Section 4) that translates a mapping specified in the loop nest form to the MDC notation, and this can also be used for mapping space exploration.</p><p>Mapping space exploration. Our conformability rules over the input operators result in a structured mapping space of the operators, and this enables us to introduce a mapper based on our decoupled off-chip/on-chip approach to accelerate mapping space exploration. Our mapper decomposes the original higher-dimensional mapping space of operators into two lower-dimensional off-chip and on-chip subspaces, and then optimizes the off-chip subspace followed by the on-chip subspace. This decomposition's motivation is to dramatically reduce the search space's size and prioritize the optimization of off-chip data movement, which requires significantly more energy and latency than the on-chip data movement <ref type="bibr" target="#b7">[8]</ref>. In contrast to prior works <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b33">34,</ref><ref type="bibr" target="#b53">54]</ref> that use a single cost model for mapping space exploration, we use different approaches and different cost models for these subspaces, such as a classical distinct-block (DB) locality cost model <ref type="bibr" target="#b15">[16,</ref><ref type="bibr" target="#b43">44]</ref> to explore the off-chip subspace, and the MDC's cost model <ref type="bibr" target="#b23">[24]</ref> for the on-chip subspace. We used a different cost model for the off-chip subspace exploration because the MDC's cost model requires the full specification of a mapping and does not work with a partial specification (e.g., off-chip mapping part of a full mapping). Even though we restrict our attention to mapping an operator onto a single accelerator, our decoupled approach can be extended to multiple accelerators within a chip and then to a distributed setup by prioritizing the optimization based on the data movement costs.</p><p>We implemented our overall approach in a tool called Marvel, and a benefit of our approach is that it applies to any operator conformable with the MDC notation. Marvel can be leveraged for both training and inference, as long as the required operators are conformable with MDC notation. Given a conformable DNN operator, workload sizes, and a target accelerator configuration, Marvel explores the mapping space of the operator using the decoupled approach and then outputs the mappings optimized for runtime and energy. Overall, our approach reduced the mapping space by an O (10 10 ) factor for the convolution operators in four major CNN models (AlexNet, VGG16, ResNet50, and MobileNetV2) while generating mappings that demonstrate a geometric mean performance improvement of 10.25× higher throughput and 2.01× lower energy consumption compared with three state-of-the-art mapping styles from past work. We also evaluated our approach over the other operators (GEMM, LSTM, and MLP) and compared them with past work optimizers.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">BACKGROUND</head><p>In this section, we provide a brief overview of spatial accelerators and also the MDC notation to describe computation and mappings onto the spatial accelerators.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Spatial Accelerators</head><p>Spatial DNN accelerators based on ASICs and FPGAs have emerged to address extreme demands on performance and energy efficiency of CNN layers <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b6">7,</ref><ref type="bibr" target="#b19">20,</ref><ref type="bibr" target="#b32">33,</ref><ref type="bibr" target="#b34">35,</ref><ref type="bibr" target="#b45">46]</ref>. Such accelerators are built using an array of PEs to provide high parallelism and use direct communication instead of via shared memory for energy efficiency. An abstract model of spatial accelerators is shown in Figure <ref type="figure" target="#fig_5">2</ref>, where each PE of an accelerator consists of a single/multiple ALU(s) dedicated for multiplyaccumulate operations (MACs) and a local scratchpad (L1 buffer). In addition, accelerators employ various NoCs for direct communication among PEs and between the PE array and the L2 scratchpad buffer. The interconnection network often supports multi-casting data to multiple PEs, which can reduce the total number of data reads from the L2 buffer to PEs. Unlike GPU cores, PEs can communicate with adjacent PEs (data forwarding) using a NoC, which can significantly reduce the number of L2 buffer accesses with high energy overhead. Accelerators also typically employ a Marvel 6:5 Fig. <ref type="figure" target="#fig_5">2</ref>. The abstract spatial accelerator model, which is pervasive in many state-of-the-art accelerators <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b19">20,</ref><ref type="bibr" target="#b32">33]</ref>.</p><p>large shared L2 scratchpad buffer to stage data from DRAM and also partial accumulations from PE arrays. Both L1 and L2 scratchpad buffers are software-controlled memories-for instance, the programmer/compiler directly controls contents of the buffer, unlike cache memories, which implicitly manage them, and this is because the memory traffic in accelerators is known in advance. Many spatial accelerators can be further interconnected together to create a scale-out system <ref type="bibr" target="#b10">[11]</ref>.</p><p>Systolic arrays <ref type="bibr" target="#b19">[20,</ref><ref type="bibr" target="#b52">53]</ref> belong to spatial accelerators and entirely rely on the point-to-point connection among adjacent PEs for input data distribution and partial sum accumulations. Although systolic arrays can provide high throughput and energy efficiency, they lack flexibility in its dataflow due to their rigid NoC architecture. Such inflexibility allows limited dataflow styles, which can lead to low compute unit utilization depending on the operator and its dimensions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">MDC Notation</head><p>The MDC notation, for expressing a DNN operator onto a spatial accelerator, consists of two aspects: (1) operator computation and tensor sizes, and (2) data mapping directives over tensor dimensions. A major novelty of the MDC notation is that the data mappings of tensors across space (PEs) and time are explicitly specified using a set of data mapping directives. This explicit specification enables the underlying cost model to estimate reuse behavior of a mapping precisely and also faster. We briefly describe the data mapping directives using a sample mapping (shown in in Figure <ref type="figure" target="#fig_2">3</ref>  Third, we have directive order, in which the sequence of spatial and temporal map directives in a mapping dictates the change of PE data mappings across time. Similar to a loop order, all the dimension index values corresponding to a mapping directive must be explored before its immediate outer mapping directive exploring its next set of indices. For instance, the sequence of directives in the running example (i.e., spatial map over d O followed by temporal map over d W ) dictates that all the dimension index values of the weight tensors must be explored before exploring the next set of d O indices. The order in this example results in accumulating partial results of the output before computing another output, popularly referred to as "output stationary" mapping <ref type="bibr" target="#b12">[13]</ref>. However, the sequence notation has a limitation that it cannot capture scenarios where more than one dimension index value simultaneously changes over time (except at the dimension boundaries).</p><p>Fourth, the clusters (size) directive logically groups multiple PEs or sub-clusters, and the size parameter denotes the group size. For example, the Cluster(2) directive on an accelerator with four PEs arranges the PEs into two clusters with the cluster size as two, as shown in Figure <ref type="figure">4(a)</ref>. All the mapping directives above a cluster directive operate over the introduced logical sub-clusters (viewing each sub-cluster as a PE), whereas those below the cluster directive operate within a logical sub-cluster. The cluster directive is extremely useful in exploiting spatial distribution of more than one tensor dimension index value (e.g., row-stationary mapping <ref type="bibr" target="#b6">[7]</ref>). In Figure <ref type="figure">4</ref>(a), the tensor dimension d w via SpatialMap(2,2) is spatially partitioned across the clusters where as the d o via SpatialMap(1,1) is partitioned across the sub-clusters within a cluster.</p><p>Data blocking/tiling. Blocking is an essential mechanism to fit the required tensor's data across multiple levels of memory hierarchy of an accelerator and also exploit locality for better performance and energy efficiency. The clusters directive of MDC representation helps in realizing the data movement behavior arising from the blocking mechanism. Consider the example in Figure <ref type="figure">4(b)</ref>, where the Cluster(2) directive groups both the two PEs into a single cluster, and the sequence and sizes of temporal directives above the cluster dictate the block sizes and also the inter-block order. As can be observed from the usage of the preceding four directives, the PE data mappings are explicit in the notation, which enables the underlying cost model to estimate reuse behavior of a mapping precisely and also faster <ref type="bibr" target="#b23">[24]</ref>. In addition, these directives capture a wide range of mappings/dataflow styles, including sophisticated mapping styles such as row-stationary in Eyeriss <ref type="bibr" target="#b6">[7]</ref>, weight-stationary in NVDLA <ref type="bibr" target="#b32">[33]</ref>, and output-stationary in ShiDianNao <ref type="bibr" target="#b12">[13]</ref> accelerators. The GitHub repository <ref type="bibr" target="#b22">[23]</ref> includes different mapping styles applied on several accelerators and also different DL models using the MDC notation. In addition, an interesting property of the MDC notation is that any mapping expressed in the notation is precisely analyzable using the MDC's cost model. However, finding whether a computation and any of its mappings are expressible in the MDC notation is still an open question, and we address this in the next section.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">CONFORMABILITY RULES</head><p>In this section, we introduce a set of conformability rules to characterize the set of input operators and their mappings that can be expressed using the MDC notation. We discuss the rules over abstract loop nest description of operators that only describe the computation without any transformations for reuse and parallelization (e.g., CONV1D in Figure <ref type="figure" target="#fig_4">5</ref>), and these rules are formed based on the computation and mapping directives of the MDC representation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Rule 1: A conformable operator in its abstract loop nest description should be perfectly nested loops without any conditional statements inside the loop body.</head><p>The MDC notation restricts its computation to be uniform across all PEs at all timesteps. This restriction is satisfied only if a perfect loop nest encloses such uniform computation without any conditional statements. Most of the primitive DNN operators such as CONV2D, GEMM, and MLP (more in Table <ref type="table" target="#tab_0">1</ref>) can be expressed in the form of perfectly nested loops without any conditionals. However, there can be the implementation of certain operators, such as the fusion of two convolutions, where each PE requires executing the non-uniform computation and describing such mappings are not possible with the existing MDC notation. In addition, existing popular cost models for templated spatial architecture such as Timeloop <ref type="bibr" target="#b33">[34]</ref> do not support such mappings.  The MDC notation restricts the input and output tensors of an operator/computation to be different, resulting in not having any flow-and anti-dependences between the tensors. However, the notation can support reductions (e.g., add, max, min) in the computation, leading to supporting reduction dependences, such as flow-, anti-, output-dependences only on the output tensor. Like rule 1, most primitive DNN operators have only reduction dependences, except few operators such as parametric multi-step LSTMs with flow dependences. These multi-step LSTMs are unrolled in practice, leading to a sequence of loop nests, where each loop nest satisfies all the conformability rules, thereby enabling the MDC notation to capture each nest as a separate operator.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Rule 3: The dimension dependence graph of the perfectly nested loop must have a topological ordering, and the subscripts of each dependent dimension index variable of the DDG should be expressible as affine functions of the loop iterators.</head><p>The directive order (i.e., the sequence of mapping directives) of the MDC notation dictates the data mapping changes to PEs across time. As described in Section 2.2, the directive order has limitations in capturing more than one tensor dimension index variable changing simultaneously over time (except at boundaries). We introduce a new directed graph called the dimension dependence graph (DDG) to find possibilities of such data movement behaviors in an input computation/operator.</p><p>Nodes of the DDG. Each node of a DDG denotes a tensor dimension index variable along with a subscript referenced in that dimension. For instance, the node (d I :i 0 +i 1 ) in Figure <ref type="figure" target="#fig_4">5</ref>(a) represents the subscript i 0 +i 1 used in the input tensor (I) with dimension name d I . All such unique tensor dimension index variables with their subscripts are part of the DDG as nodes.</p><p>Edges of the DDG. The edges of the DDG denote deduction relations (i.e., a destination node's subscript) can be completely constructed from the source node's subscripts of its incoming edges. These deduction relations are constructed as follows:</p><p>(1) An edge is added from a node having a single index variable (SIV)/multiple index variable (MIV) subscript 2 to a node having a MIV subscript if there is a common loop iterator Marvel 6:9</p><p>in their subscripts. For example, there is a directed edge from the node (d O :i 0 ) to (d I :i 0 +i 1 ) in Figure <ref type="figure" target="#fig_4">5</ref>(a) because they have a loop iterator i 0 in common. (2) All the SIV subscripts are grouped based on their loop iterators, and then edges are added from the SIV subscript of a group having the lowest constant value (randomly choose if there exists multiple) to other SIV subscripts in the same group. For example, there is a directed edge from the node (d I :i 0 ) to all the nodes (d I :i 0 + 1), (d I :i 0 + 2), and (d O :i 0 ) in Figure <ref type="figure" target="#fig_4">5</ref>(b). (3) If the loop bound of an iterator (say i) is dependent on other loop iterators (say j), then construct an edge from a node with subscript having the iterator i to nodes having the iterator j in their subscripts. This is not common in DNN primitive operators because the loop bounds in these operators are generally fixed and do not vary with outer loop iterators.</p><p>Now, finding multiple dimension index variables changing simultaneously is reduced to the problem of finding a topological ordering in the DDG. In essence, the absence of a topological ordering indicates the presence of mutually dependent dimension index variables (e.g., the example in Figure <ref type="figure" target="#fig_4">5(c)</ref>). In the presence of a topological ordering, the MDC notation requires only the data mappings of independent dimension index variables to be specified, and these variables are identified from the nodes of the DDG having zero in-degree. In the case of CONV1D in Figure <ref type="figure" target="#fig_4">5(a)</ref>, only the data mappings of dimension index variables related to output and weight tensors must be specified. The underlying MDC's cost model infers the dimension variable related to the input tensor, and such dimension variables must not be specified. Furthermore, the subscripts of dependent dimension index variables should be affine expressions of loop iterators to be analyzable by the MDC's cost model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Rule 4: After loop nest normalization, the subscripts associated with each independent dimension index variable of a DDG should be simply loop iterators with positive unit coefficients and no constants.</head><p>A mapping directive (either spatial or temporal) over a dimension index variable restricts the index values to start from zero and increases with unit stride. These restrictions do not allow the index variable to have strided increments or negative strides. To characterize the preceding restrictions implication, we assume the abstract loop nest form of an input computation to be normalized-its loop iterators start from zero and have unit strides (i.e., the loops themselves need to be of the form (for i = 0; i &lt; X; i++)).</p><p>As described earlier, the MDC notation requires specification of directives over only independent dimension index variables of the DDG. Hence, to support the restrictions, each subscript (in the normalized form) associated with an independent dimension index variable must be simply loop iterators with positive unit coefficients and no constants (e.g., i 0 for d O , i 1 for d W in Figure <ref type="figure" target="#fig_4">5(a)</ref>). Despite forcing only unit strides in the loop itself, the dilation and stride parameters of convolutions operators can be captured by this notation as these parameters are affine and part of the dependent dimension index variable subscripts (thanks to affine expressions from rule 3).</p><p>Conformability. Finally, an input computation/operator is said to MDC conformable if the computation in its abstract loop nest form satisfies all the preceding four rules. This also means that any legal mapping of the computation on a spatial accelerator can be expressed using the MDC notation. Table <ref type="table" target="#tab_0">1</ref> lists the set of popular DNN primitive operators and the conformability of these operators with the MDC notation. Both the training and inference variants of the operators are conformable. As can be seen, the MDC notation captures most of the DNN operators except parametric LSTMs, and the mappings of these operators can be analyzable by the MDC's cost model. </p><formula xml:id="formula_0">Y Y Y Y CONV2D Regular Y Y Y Y Y Point-wise, Depth-wise Y Y Y Y Y Strided, Dilated Y Y Y Y Y MLP Fully connected Y Y Y Y Y Pooling Max, Avg Y Y Y Y Y GEMM Regular Y Y Y Y Y Triangular Y Y Y Y Y LSTM Single cell Y Y Y Y Y Parametric multi-cell Y N Y Y N Element wise Residual Y Y Y Y Y ReLU Y Y Y Y Y Stencils Regular Y Y Y Y Y</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">TRANSFORMING A LOOP NEST MAPPING INTO MDC NOTATION</head><p>The MDC notation is powerful in expressing and reasoning complex mappings of DNN operators onto the diverse spatial accelerators, but explicitly writing and exploring such mappings can be error-prone and tedious. In addition, computer architects <ref type="bibr" target="#b33">[34]</ref> and DNN compiler frameworks <ref type="bibr" target="#b5">[6]</ref> view the computations and their mappings majorly in the loop nest form <ref type="bibr" target="#b28">[29,</ref><ref type="bibr" target="#b33">34,</ref><ref type="bibr" target="#b55">56]</ref>. This section introduces an automatic transformation to translate a loop nest mapping of a conformable operator into its equivalent MDC notation. In this work, we assume the target spatial accelerator having three levels of the memory hierarchy (private L1 buffer, shared L2 buffer, and DRAM). However, our transformation can be easily extendable to more levels of hierarchy. As described in Section 2.2, the MDC notation consists of two components: (1) computation and tensor sizes, and (2) data mapping directives over independent tensor dimensions. The statements enclosed in the perfectly nested loop form of an input conformable operator are used as the computation, and the tensor sizes are extracted from the workload configuration. The computation and tensor sizes of the MDC notation remain the same for all mappings of the operator and its workload configuration. Then, the DDG is constructed to identify the set of independent tensor dimension index variables. If there are no such independent dimension index variables, then the operator is discarded as non-conformable. Otherwise, we will translate the mapping in loop nest form into data directives of the MDC notation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Generation of Data Mapping Directives</head><p>From rules 1 and 2 in Section 3, the loops of a conformable operator can be freely reordered, and hence it is legal to apply multi-level tiling to exploit temporal reuse across each level of the memory hierarchy and also to exploit parallelism via PEs of the accelerator. Each behavior arising from tiling, reuse, and parallelizing an operator onto a spatial accelerator is referred to as a "mapping. " The different aspects of mapping are briefly described in the following with a running example of CONV1D's loop nest mapping (shown in Figure <ref type="figure">6(c</ref>)) over a 3-level accelerator.</p><p>Multi-level tiling tile sizes. A mapping includes tile sizes of all loop iterators for each level of tiling: (1) level-1 tiling for the private L1 scratchpad buffer inside a PE, (2) level-2 tiling for the parallelism across PE array, and (3) level-3 tiling for the global L2 scratchpad buffer shared by all PEs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Marvel 6:11</head><p>Inter-tile loop orders. A mapping also includes inter-tile loop orders <ref type="foot" target="#foot_4">3</ref> to describe the execution order of tiles exploiting various reuse opportunities. For instance, the level-2 inter-tile loop order (i.e., the execution order of level-2 tiles) exploits spatio-temporal reuse via the on-chip interconnect. In addition, level-3 inter-tile loop order exploits temporal reuse via the on-chip L2 buffer. However, the level-1 inter-tile loop order does not reflect any reuse because these loops essentially reflect parallelism. The point loop's loop order does not provide any reuse opportunities because there is no more intermediate staging between the PE and its private L1 buffer.</p><p>An n-level tiling over an input conformable operator will have n set of tile loops (including parallel loops) and a set of point loops. Each set of loops can have a different data movement (reuse) behavior based on its sizes and loop order. We introduce a term called region to denote a sequence of data mapping directives (e.g., region R1 in Figure <ref type="figure">6(d)</ref>) without any cluster directives. Each region captures the data movement behavior present in each set of tile loops. Rules 3 and 4 help in constructing a one-to-one mapping between the tile loop orders and directive order-for example, if rule 3 (topological) is violated, we cannot find MDC's directive sequence order to reflect the required data movement behavior. Given a mapping in multi-level tile sizes and inter-tile loop orders, our approach transforms the mapping into the MDC notation as described next.</p><p>Point loops. As described in rule 4, each subscript associated with an independent dimension variable is simply a unique loop iterator. Our approach translates each loop of point loops into a temporal map directive over the corresponding independent tensor dimension index variable with size and offset parameters of the directive the point loop size. For example, the point loop t 1i with tile size T 1i in Figure <ref type="figure">6</ref>(c) is directly translated into TemporalMap(T 1i ,T 1i ) d O in the region R1 shown in Figure <ref type="figure">6(d)</ref>. Since the loop order among the point loops does not provide any reuse benefits, the directive order in region R1 does not matter. Lines 2 and 3 in Algorithm 1 are related to building a sequence of temporal maps in region R1 corresponding to the point loops in the given mapping.</p><p>Parallel loops. Since each independent dimension index variable is uniquely associated with a loop iterator, parallel execution of each loop iterator introduces a different data movement behavior.</p><p>Hence, we introduce a region with a spatial map over the dimension index variable associated with the parallel loop and the temporal maps for the rest of the dimension variables in that region for each parallel loop. For example, there are two regions with the names R2 and R3 for the parallel loops corresponding to t 2j and t 2i , respectively. In addition, the dimension d W associated with the iterator t 2j and the dimension d O associated with the iterator t 2i are translated into spatial maps in R2 and R3 regions, respectively. The size and offsets of each spatial map over a dimension variable is derived from the strides of the parallel loop iterators corresponding to the dimension variable. The order of directives in each region corresponding to parallel loops does not matter because the number of iterations arising from the rest of the temporal maps is one. Each region corresponding to a parallel loop (except the innermost) is ended with a cluster directive with size as the number of iterations in the parallel loop (lines 5-13 in Algorithm 1). For example, the region R3 is ended with a cluster directive with size as the number of iterations of the loop t 2i .</p><p>Inter-tile loops. For each set of tile loops excluding parallel loops, our transformation generates a region by creating a temporal map directive for each loop of the set with the size and offset of the directive as the loop stride. For example, the inter-tile loop t 3j with stride as T 2j in Figure <ref type="figure">6(c</ref>) is directly translated into TemporalMap(T 2j ,T 2j )d I in the region R4 shown in Figure <ref type="figure">6(d)</ref>. The loop order governs the order of directives in a region among the corresponding tile loops. For example,  the level-3 inter-tile loop order (t 3j ,t 3i ) dictates the temporal map over d W outer compared to the temporal map over d O in region R5. Furthermore, each region is separated by a cluster directive to support different data movement behaviors across each set of tile loops (lines 14-18 in Algorithm 1).</p><p>Marvel 6:13</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">MAPPING SPACE EXPLORATION</head><p>In addition to the different aspects of mapping described in Section 4, we also consider a limited form of data layout choices (i.e., dimension reordering <ref type="bibr" target="#b27">[28]</ref>) for operator tensors on the DRAM because the data movement between DRAM and on-chip global scratchpad buffer happens at the granularity of DRAM block sizes. Overall, the mapping space of an operator is a Cartesian product of six dimensions that represent different aspects of a mapping-for instance, (1) level-1 tile sizes, (2) level-2 tile sizes (parallelism), (3) level-2 inter-tile loop orders, (4) level-3 tile sizes, (5) level-3 inter-tile loop orders, and (6) data layout of tensors. The conformability rules enable to have such a structured mapping space of the conformable operators. Searching for optimal mapping can be challenging because of the massive space of the operator's legal mappings on the accelerator configurations. For example, there are over 10 19 valid mappings for the convolution operator on average for mapping ResNet50 and MobileNetV2 models on a representative DNN edge accelerator. This search challenge can get exacerbated with the evolution of new computations (e.g., depth-wise) and diverse hardware accelerator configurations (e.g., tree-based interconnect <ref type="bibr" target="#b25">[26]</ref>).</p><p>Our approach toward mapping space exploration is motivated by the observation that the offchip data movement between DRAM and on-chip global scratchpad requires significantly more energy and latency than the on-chip data movement <ref type="bibr" target="#b7">[8]</ref>. Hence, we propose an approach referred to as "decoupled off-chip/on-chip" that decomposes the original higher-dimensional mapping space into two lower-dimensional off-chip and on-chip subspaces, and then optimizes the off-chip subspace followed by the on-chip subspace, which is constructed with the optimal mappings from the off-chip subspace. The off-chip subspace consists of three dimensions of the original mapping space-level-3 tile size, level-3 inter-tile loop order, and data layouts-because they influence the off-chip data movement. Similarly, the on-chip subspace consists of the remaining three dimensions of the original space-level-1 tile sizes, level-2 tile sizes, and level-2 inter-tile loop order-because they contribute to parallelization and on-chip data movement. In contrast to prior work <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b33">34,</ref><ref type="bibr" target="#b53">54]</ref>, we use different approaches and cost models for these subspaces-that is, a classical DB locality cost model <ref type="bibr" target="#b15">[16,</ref><ref type="bibr" target="#b43">44]</ref> to explore the off-chip subspace, and the MDC's cost model <ref type="bibr" target="#b23">[24]</ref> for the on-chip subspace. We used a different cost model for the off-chip subspace exploration because the MDC's cost model requires the full specification of a mapping and does not work with a partial specification (e.g., off-chip mapping part of a full mapping). The overall approach is implemented as a stand-alone tool (shown in Figure <ref type="figure">7</ref>) that takes a conformable operator, workload sizes, and a target accelerator configuration, then explores the mapping space of the operator using the decoupled approach, and finally outputs the mappings optimized for runtime and energy.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Solving Off-Chip Mapping Subspace</head><p>The goal of finding an optimal mapping in the off-chip mapping subspace is to minimize off-chip data movement between DRAM and the L2 buffer of an accelerator. In our work, we assume the L2 buffer to be a software-managed scratchpad buffer, and reducing the off-chip data movement <ref type="foot" target="#foot_5">4</ref> is equivalent to finding a level-3 tile that has highest arithmetic intensity because the highest arithmetic intensity results in higher reuse and less data transfer.</p><p>In our approach, we consider the classical DB locality cost model <ref type="bibr" target="#b15">[16]</ref> to measure the off-chip data movement cost, which was developed as part of the memory cost analysis to guide the automatic selection of loop transformations and also optimal tile size selections <ref type="bibr" target="#b43">[44,</ref><ref type="bibr" target="#b44">45,</ref><ref type="bibr" target="#b46">47]</ref> in IBM XL compilers. The DB model is a good choice for our approach since it only focuses on optimizing for Fig. <ref type="figure">7</ref>. An overview of our approach along with pruning strategies for searching mapping space of convolutions. The pruning strategies in green color preserve optimal mappings, whereas the strategies in red color may prune optimal.</p><p>off-chip data movement. Moreover, it is limited to only perfectly nested loops, and conformable operators are perfectly nested loops as per the rule R1 in Section 3.</p><p>The DB model starts with data layouts of multi-dimensional arrays and a parametric tiled version of a perfectly nested loop. The model symbolically estimates the off-chip data movement cost involved in a tile of computation by measuring the number of the distinct number of DRAM blocks required for all the references in the tile of computation. For instance, assume that an array I is laid out in the row-major order layout, then the distinct number of DRAM blocks (with DRAM block size as B and tile sizes T X , T Y ) required for a reference I[y][x+y] enclosed in a triply nested loop with iterators x, y, z is computed as follows:</p><formula xml:id="formula_1">DB I (T X ,T Y ) ≈ T X + T Y b × (T Y ).</formula><p>In the preceding formulation, the innermost access of the reference is divided by the block size,<ref type="foot" target="#foot_6">5</ref> because the data movement with DRAM happens in multiples of block sizes. The total data movement cost (DMC), a.k.a. memory cost per iteration, involved in a tile is computed as the number of distinct DRAM blocks required for all references in the tile divided by the total number of iterations in the tile. The optimal level-3 tile sizes and data layouts are computed by minimizing the data movement cost function for every layout and tile sizes in the off-chip mapping subspace with the two constraints: for instance, (1) the tile size of a loop should be greater than 0 and should not exceed its corresponding loop bound, and (2) the total data required (including double buffering) for a level-3 computation tile should fit into the on-chip L2 buffer. In the past, determining optimal tiles using the DB model was modeled as a geometric program, transformed then into a convex optimization problem <ref type="bibr" target="#b38">[39,</ref><ref type="bibr" target="#b39">40]</ref>, and further solved using integer geometric programming frameworks instead of enumeration. Marvel currently has support for doing an exhaustive search (feasible because of only one level of tiling for off-chip data movement) and also using the integer geometric programming formulation for the tile size calculations.</p><p>After computing the optimal level-3 tile sizes and data layouts of tensors, our approach computes the partial derivatives (slopes) of the data movement cost function, based on the optimal data layout choice, with respect to parametric level-3 tile sizes, and evaluate the partial derivatives by substituting optimal level-3 tile sizes. The key insight is that having a higher negative value of a partial derivative along a loop indicates the lesser distinct number of elements referenced along the loop (i.e., highest reuse along the loop). It is suggested to keep it in the innermost position to exploit maximum temporal reuse, and similarly, the rest of the loops are ordered based on their partial derivative values.</p><p>Rationale for using the DB model. The DB model is a good choice for our approach, since the model only focuses on optimizing for off-chip data movement, and in addition, it focuses only on a perfectly nested loop, and conformable DNN operators are perfectly nested loops.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Solving On-Chip Mapping Subspace</head><p>The on-chip mapping subspace is constructed based on the optimal values of level-3 tile sizes. Our approach explores the constructed subspace to find optimal mappings for each of the three optimal goals: lower runtime (higher throughput), lower energy consumption, and lower energy delay product. For each mapping of the constructed subspace, our approach transforms the mapping in its loop nest form into its equivalent MDC notation (described in Section 4). Then, our approach uses the MDC's cost model <ref type="bibr" target="#b23">[24]</ref> to estimate various metrics such as latency and energy of each mapping in the on-chip subspace. The MDC's cost model precisely computes performance and energy, accounting for under-utilization, edge conditions, and data reuse or movement across time (via L1/L2 buffers <ref type="bibr" target="#b6">[7]</ref>), space (via broadcast links <ref type="bibr" target="#b25">[26]</ref>), and space-time (via neighboring links <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b19">20]</ref>) without requiring explicit RTL/cycle-level simulations or access to real hardware. Algorithm 2 shows an overview of our approach in exploring the on-chip mapping subspace along with pruning strategies. We introduce a parameter referred to as "PE utilization bound (p)" to prune search space of level-2 tile sizes by bounding the overall PE array utilization to be at least the parameter p. The preceding technique is beneficial in finding optimal on-chip mappings with the optimization goal being throughput because the highest throughput is typically obtained at higher PE utilization rates <ref type="bibr" target="#b9">[10]</ref>. Our approach also includes a pruning strategy to choose level-1 and level-2 tile sizes such that they do not result in any prologues or epilogues (i.e., the tile sizes are factors of loop bounds). The preceding strategy is used in all the prior mapping space explorers <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b28">29,</ref><ref type="bibr" target="#b33">34,</ref><ref type="bibr" target="#b53">54,</ref><ref type="bibr" target="#b55">56]</ref>. This strategy helps simplify the design of a PE and control signal generation inside the accelerator. All of the pruning strategies mentioned previously can be enabled/disabled in Marvel by passing them as input parameters.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">EXPERIMENTAL EVALUATION</head><p>In this section, we begin with coverage of MDC conformable operators in the existing popular DNN models. Then, we preset an overview of the experimental setup used in the evaluation of our decoupled off-chip/on-chip approach. We also present the evaluation of mappings generated by Marvel for a wide variety of DNN operators (CONV2D, GEMM, MLP, and LSTM) and discuss insights from the mappings while comparing them with previous work.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.1">Coverage of MDC Conformable Operators</head><p>We have used the TensorFlow profiler to identify the operators in the DNN models of MLPerf suite (i.e., MobileNet V1, ResNet50, SSD-MobileNet, SSD-ResNet34, and GNMT). We also included VGG16 and AlexNet models in our evaluation. Table <ref type="table" target="#tab_1">2</ref> lists those primitive operators and their occurrences in each DNN model. All the identified primitive operators are conformable with the MDC notation, and also we did not have to rewrite any of those operators to make it MDC conformable. Both the training and inference variants of those primitive operators are conformable.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.2">Evaluation of Decoupled Off-Chip/On-Chip Approac</head><p>Target accelerators. Marvel is applicable to any spatial accelerator since it abstracts accelerator details as #PEs, L1/L2 buffer sizes, NoC bandwidth, reduction/multi-cast support, and so on, which can be used to model a wide variety of accelerators including Eyeriss <ref type="bibr" target="#b6">[7]</ref>, NVDLA <ref type="bibr" target="#b32">[33]</ref>, TPU, and xDNN.</p><p>Due to space limitations, we present our evaluation for only two accelerator platforms (shown in Figure <ref type="figure">8</ref>): an accelerator (Eyeriss-like <ref type="bibr" target="#b6">[7]</ref>) having 168 PEs and 2.4 GB/s NoC bandwidth, and another accelerator having 1,024 PEs and 25.6 GB/s. We inherit the L1 buffer, L2 buffer, and clock frequency for both platforms from Eyeriss <ref type="bibr" target="#b6">[7]</ref> (i.e., 512B L1 buffer, 108-KB L2 buffer, and 200-MHz Marvel 6:17 Fig. <ref type="figure">9</ref>. Performance comparison of Marvel generated mappings with the mappings of the dMazeRunner-like optimizer <ref type="bibr" target="#b11">[12]</ref> and Interstellar-like optimizer <ref type="bibr" target="#b53">[54]</ref> relative to the roof-line peaks of the AlexNet and VGG16 models on both platforms (P1 and P2).</p><p>clock frequency). The bidirectional NoC used in our evaluation is a two-level hierarchical bus, which has support for multi-casting similar to Eyeriss.</p><p>Experimental variants. We have implemented few of the exploration strategies of recent optimizers such as Interstellar <ref type="bibr" target="#b53">[54]</ref> and dMazeRunner <ref type="bibr" target="#b11">[12]</ref> in our framework. For instance, the Interstellar optimizer focuses on parallelizing input and output channels of CONV2D operators, whereas the dMazeRunner optimizer focuses on parallelizing only output channels and a limited set of loop orders. We compare Marvel generated mappings for each workload and accelerator platform with three variants: (1) Marvel implemented Interstellar-like <ref type="bibr" target="#b53">[54]</ref> optimizer generated mappings, (2) Marvel implemented dMazeRunner-like <ref type="bibr" target="#b11">[12]</ref> optimizer generated mappings, and (3) roof-line peak based on the workload arithmetic intensities and accelerator configurations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Methodology.</head><p>We have evaluated all the mappings generated by the experimental variants using the MAESTRO cost model <ref type="bibr" target="#b23">[24]</ref>. Moreover, the analytical cost model within the MAESTRO framework is validated against the RTL implementations of Eyeriss <ref type="bibr" target="#b6">[7]</ref> and MAERI <ref type="bibr" target="#b25">[26]</ref> on VGG16 and AlexNet models. We passed a pruning option to the Marvel to choose tile sizes that divide loop bounds evenly without any remainder, and this has been the consideration in the other approaches <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b28">29,</ref><ref type="bibr" target="#b33">34,</ref><ref type="bibr" target="#b53">54,</ref><ref type="bibr" target="#b55">56]</ref>. We also set the minimum PE array utilization bound as 0.1 (i.e., at-least 10% of the PE array should be mapped with computation). We apply 8-bit fixed point precision for all the tensors used in our evaluation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.2.1">Evaluation on CONV2D</head><p>Variants. In our evaluation, we considered CONV2D (including depth-wise) operators of popular DNN models, such as AlexNet <ref type="bibr" target="#b21">[22]</ref>, VGG16 <ref type="bibr" target="#b47">[48]</ref>, ResNet50 <ref type="bibr" target="#b16">[17]</ref>, and MobileNetV2 <ref type="bibr" target="#b42">[43]</ref>. We assumed a batch size of one because this assumption captures a low latency requirement use case and also represents a more challenging setup for energy efficiency and throughput <ref type="bibr" target="#b9">[10]</ref>, especially for edge-/mobile-class accelerators. In addition, these models encompass a broad spectrum of CONV2D operators such as regular, point-wise, depth-wise, strided variants with different filter shapes.</p><p>Comparison with the existing optimizers. Figure <ref type="figure">9</ref> presents the runtimes of mappings generated by Marvel, the dMazeRunner-like optimizer, and the Interstellar-like optimizer normalized to the roofline peaks of the AlexNet and VGG16 models on both accelerator platforms (P1 and P2). Since each model has multiple CONV2D operations, we have added the runtimes of all the CONV2D operators in a model to present our evaluation at the level of DNN models. The Interstellar-like optimizer is almost equivalent to the brute-force exploration except that it allows exploiting parallelism along only input and output channels. As a result, the evaluation using the Interstellar-like optimizer is very time consuming (multiple days for MobileNetV2 and ResNet50), and hence we restricted the  comparison to only AlexNet and VGG16 models. Since we are comparing with the roof-line peak, we ignored comparing with TimeLoop <ref type="bibr" target="#b33">[34]</ref>'s time-taking brute-force exploration to identify the best mappings.</p><p>As can be observed from Figure <ref type="figure">9</ref>, Marvel generated mappings are geometrically 2.35× and 1.15× faster compared to the mappings obtained by the dMazeRunner-like optimizer and the Interstellarlike optimizer, respectively. The dMazeRunner-like optimizer focuses on exploiting parallelism along only output channels (in presence of unit batch size) to avoid inter-PE communication, and this results in under-utilization of the PE array for both models.</p><p>The Interstellar-like optimizer is able to perform close to Marvel, because the number of input and output channels in these models are larger (except at the initial layers). But, it can underperform for DNN models such as UNet <ref type="bibr" target="#b40">[41]</ref>, where input and output channels are smaller and output width and height are larger. Furthermore, our approach is able to identify mappings in seconds to few minutes for each operator on a local machine, unlike the Interstellar-like, dMazeRunner-like optimizer, which takes almost 1 to 5 hours for each operator. Figure <ref type="figure" target="#fig_11">10</ref> shows the impact of our decoupling and pruning strategies on the original search space of mappings of the four DNN models with an average reduction of O (10 10 ) in the mapping space.</p><p>Comparison with the popular mapping styles. The state-of-the-art mapping styles encoded in the hardware as dataflows are row-stationary from Eyeriss <ref type="bibr" target="#b6">[7]</ref>, weight-stationary from DLA <ref type="bibr" target="#b32">[33]</ref>, and output-stationary from ShiDianNao <ref type="bibr" target="#b12">[13]</ref>. In our evaluation, we encoded the preceding mapping styles in the form of parallelization and loop order constraints on the on-chip mapping space in our decoupled approach. For instance, DLA-like mappings <ref type="bibr" target="#b32">[33]</ref> exploit parallelization across input and output channels, Eyeriss-like mappings <ref type="bibr" target="#b6">[7]</ref> exploit parallelism along output width and filter width, and ShiDianNao-like mappings <ref type="bibr" target="#b12">[13]</ref> exploit along output width and height. To briefly explain the mappings generated by our approach and its difference with respect to the state-ofthe-art mapping styles, we consider two convolutions-a regular CONV2D from VGG16 and a depth-wise CONV2D from MobileNetV2-whose details are shown in Figure <ref type="figure">11</ref>.</p><p>Impact of level-3 tile sizes. The CONV2D operator in VGG16 layer 1 has higher output width and height (P, Q) compared to the output and input channels (K, C). However, the level-3 tile size corresponding to output height is shrinked to fit into the on-chip buffer with maximum temporal reuse. As a result, our approach exploited parallelism along output width (P) and filters (K) to utilize the PE array maximum. However, none of the state-of-the-art mapping styles and also dMazeRunnerlike/Interstellar-like optimizers exploit parallelism along P and K dimensions in their mapping.</p><p>Impact of modern operators. The modern DNN models such as MobileNetV2 have introduced depthwise CONV2D operators, and these operators reduce the total number of MAC operations by not performing reduction across input channels, thereby sacrificing arithmetic intensity. As a result, Marvel 6:19 Fig. <ref type="figure">11</ref>. Two layers from VGG16 and MobileNetV2 for brief discussion on our approach generated mappings. Level-3 tile sizes and degree of parallelism part of the mappings identified by our approach on platform P2. Fig. <ref type="figure" target="#fig_5">12</ref>. Runtime and energy of Marvel generated mappings with popular mapping styles such as rowstationary from Eyeriss <ref type="bibr" target="#b6">[7]</ref>, weight-stationary from DLA <ref type="bibr" target="#b32">[33]</ref>, and output-stationary from ShiDianNao <ref type="bibr" target="#b12">[13]</ref> for the AlexNet <ref type="bibr" target="#b21">[22]</ref>, VGG16 <ref type="bibr" target="#b47">[48]</ref>, ResNet-50 <ref type="bibr" target="#b16">[17]</ref>, MobileNet-V2 <ref type="bibr" target="#b42">[43]</ref> models on platforms P1 and P2.</p><p>these operators have less parallelization opportunities and are often bounded by NoC bandwidth. For example, the depth-wise CONV2D operator in Figure <ref type="figure">11</ref> has the value of K set to one and also the level-3 tile size of C is shrinked to a smaller value to fit into the on-chip buffer with maximum temporal reuse. To fully leverage the PE array, our approach generated mapping exploited parallelism along three dimensions-input channels (C), output width (P), and output height (Q), where none of the prior state-of-the-art mapping styles and dMazeRunner-like/Interstellar-like optimizers exploited more than two levels of parallelism. Furthermore, the performance of the generated mapping was close to the roof-line peak, which is dominated by NoC bandwidth. The overall performance (runtime) and energy comparison of Marvel generated mappings with respect to the prior state-of-the-art mapping strategies is shown in Figure <ref type="figure" target="#fig_5">12</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.2.2">Evaluation on GEMM.</head><p>We considered GEMM workloads from recent work <ref type="bibr" target="#b36">[37]</ref>. An interesting aspect of these workloads is that they are irregular in their shapes, making the rigid accelerators (e.g., TPUs) hard to reach their peak utilization <ref type="bibr" target="#b36">[37]</ref>. A summary of these workloads are shown in Figure <ref type="figure" target="#fig_2">13</ref>, where M, N, and K refer to the number of rows, columns of first matrix followed by the columns of second matrix. As can be seen from Figure <ref type="figure">14</ref>, the runtimes of Marvel generated mappings are only 1.24× and 1.10× higher relative to the roof-line peaks of accelerator Fig. <ref type="figure" target="#fig_2">13</ref>. GEMM workloads taken from <ref type="bibr" target="#b36">[37]</ref>. Fig. <ref type="figure">14</ref>. Comparison of Marvel generated mappings with the mappings of dMazeRunner-like <ref type="bibr" target="#b11">[12]</ref> and Interstellar-like optimizers <ref type="bibr" target="#b53">[54]</ref> normalized to the roof-line peaks of the GEMM, LSTM, and MLP workloads. platforms P1 and P2, respectively, thereby demonstrating the closeness of mappings obtained using our approach to the peak. Furthermore, we observed that maximum reuse (spatial, temporal, spatio-temporal) is exploited only when all the dimensions of the GEMM operator are parallelized. Hence, Marvel generated mappings included parallelization of the three dimensions to make the PE array occupied along with exploiting maximum reuse. This is in contrast to other approachesthat is, the Interstellar-like optimizer focusing on the parallelizing only (N, K) dimensions and the dMazeRunner-like optimizer focusing on the parallelizing only (K) dimension.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.2.3">Evaluation on MLP and LSTM.</head><p>In this evaluation, we have considered the MLP and LSTM workloads from the work of Yang et al. <ref type="bibr" target="#b53">[54]</ref>, and a summary of these workloads is shown in Figure <ref type="figure" target="#fig_12">15</ref>. Figure <ref type="figure">14</ref> presents the runtime of optimized mappings generated by Marvel, the dMazeRunner-like optimizer, and the Interstellar-like optimizer normalized to the roof-line peak of each workload. Marvel generated mappings are 4.46× and 1.22× faster compared to the mappings obtained by the dMazeRunner-like optimizer and Interstellar-like optimizer for all the workloads on both accelerator platforms. The benefits of our approach compared to the dMazeRunner-like optimizer is higher because of its parallelization across only a single dimension (embedding size in case of LSTM and output channels in MLP) and also exploring only limited loop orders for reuse. Marvel is able to do better compared to the Interstellar-like optimizer by exploring more levels of parallelism to make the PE array occupied (e.g., only 1.04× higher relative to the roof-line peak on P2).</p><p>Summarizing, our decoupled off-chip/on-chip approach is applicable to any MDC conformable operator, and we have demonstrated our approach over multiple CONV2D variants, GEMM, MLP, and LSTM workloads that are popular in DNN models.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">RELATED WORK</head><p>We categorize our discussion on prior work along the two directions: (1) expressiveness, formal boundary, and cost models, and (2) mapping exploration strategies.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.1">Expressiveness, Formal Boundary, and Cost Models</head><p>A major difference between the mappers for spatial accelerators and CPUs/GPUs is the need for "accurate" cost models that can estimate close to real-world values for a given mapping of a kernel (operator) for finding efficient mappings <ref type="bibr" target="#b23">[24,</ref><ref type="bibr" target="#b33">34]</ref>. This is because the spatial accelerator's performance is sensitive to the mapping parameters-for example, a small change in the tile size or degree of parallelism would drastically change the latency or energy-efficiency numbers. Recently, multiple analytical cost models <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b23">24,</ref><ref type="bibr" target="#b33">34,</ref><ref type="bibr" target="#b53">54]</ref> have been developed to estimate execution time and energy efficiency of mappings over spatial accelerators close to the real-world simulation/execution. These mappings are often expressed in the form of loop nests, and many cost models such as Timeloop <ref type="bibr" target="#b33">[34]</ref>, DMazeRunner <ref type="bibr" target="#b11">[12]</ref>, and Interstellar <ref type="bibr" target="#b53">[54]</ref> are developed over the loop nest description of mappings. The loop nest syntax is very generic and can help accelerator architects or compilers express a wide range of mappings, but the underlying cost models may not be able to estimate costs for all possible mappings that can be expressible in loop nests. For example, a mapping for parametric multi-step LSTM operator may include loop skewing transformation to exploit pipelined (wave-front) parallelism <ref type="bibr" target="#b2">[3]</ref>, but it is not clear if the cost models mentioned previously can precisely estimate costs for such mappings.</p><p>Challenges with generic loop nest representation for cost models. The generic loop nest forms encompasses various control flow structures and data-access patterns that make it harder for the cost models to precisely reason. For instance, most of the cost models based on the loop nest form assumes the loops to have constant bounds, and this makes the analysis simpler by estimating various forms of reuse in a particular iteration and extrapolating with the total number of iterations in the loop. However, such analyses can break in presence of if conditionals in the loop body and also with variable loop bounds such as in non-rectilinear and sparse iteration spaces.</p><p>TVM compiler infrastructure <ref type="bibr" target="#b5">[6]</ref> offers an ML-based cost model to find optimal implementations of convolutions on a variety of platforms including accelerators. However, we believe that such ML-based cost models are challenging for spatial accelerators for two reasons: (1) the statistical ML-based cost models are generally not accurate to precisely estimate performance and energy, and not accounting PE under-utilization, edge conditions can lead to significant imprecision, and (2) it requires training the ML-based cost models even for a slight change in number of PEs in the accelerator configuration, which makes it challenging to use for the design space exploration.</p><p>In this work, we have considered the recently introduced MDC notation <ref type="bibr" target="#b23">[24]</ref> for expressing mappings onto generic templated spatial architectures because any mapping expressed in the notation  <ref type="bibr" target="#b33">[34]</ref>) is precisely analyzable using the MDC's cost model <ref type="bibr" target="#b24">[25]</ref>. We introduced a set of conformability rules as a way to define a formal boundary over operators for precise and tractable cost analysis of the operator mappings using the MDC notation and its cost model.</p><p>Expressiveness. On the other side, even though high-level frameworks such as TVM <ref type="bibr" target="#b5">[6]</ref>, TC <ref type="bibr" target="#b49">[50]</ref>, PlaidML <ref type="bibr" target="#b35">[36]</ref>, Stripe <ref type="bibr" target="#b54">[55]</ref>, Polyhedral (Tiramisu <ref type="bibr" target="#b2">[3]</ref>), and MLIR <ref type="bibr" target="#b26">[27]</ref> have richer expressibility than the MDC notation, none of these frameworks yet have accurate cost models targeting flexible spatial accelerators. In addition, it is not clear if accurate cost models can exist for any operator expressed in their notations. An overview of the comparison of the MDC notation with prior notations in terms of expressiveness, mapping notation, and the presence of accurate cost models for spatial accelerators is shown in Table <ref type="table" target="#tab_2">3</ref>. Our approach, including mapping space exploration, can be plugged into popular frameworks such as TVM and MLIR for wide applicability.</p><p>In addition to Marvel supporting all the primitive operators supported by other frameworks (Figure <ref type="figure">16</ref>), Marvel can also support operators having non-rectilinear iteration spaces (e.g., Symmetric GEMM <ref type="bibr" target="#b17">[18]</ref>), which none of the other frameworks in Figure <ref type="figure">16</ref> support. Furthermore, a strong guarantee of our approach is that any operator conformable to the MDC notation can leverage our compiler along with the underlying accurate cost model. This is in contrast to other frameworks for spatial accelerators such as Timeloop and Interstellar, where there are no such guarantees. MDC limitations. Currently, the MDC representation restricts the input computation to be a perfectly affine loops nest without any conditionals statements. Hence, our representation cannot support computations such as parametric LSTM operators and fusion of multiple convolution layers. However, such computations can be transformed into a sequence of MDC conformable operations by performing the transformations such as unrolling of parametric LSTM operations and loop distribution of imperfectly nested loops into perfectly nested loops. The polyhedral model is capable of covering the MDC limitations, but there does not exist any cost model based on the polyhedral model to accurately estimate performance aspects of a mapping on spatial accelerators, and this is a good line of research work for the future.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.2">Mapping Exploration Strategies</head><p>Prior work <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b56">57]</ref> focused on developing mappers specific to their architectures-for example, mRNA mapper <ref type="bibr" target="#b56">[57]</ref> for the MAERI accelerator <ref type="bibr" target="#b25">[26]</ref> and Auto-TVM <ref type="bibr" target="#b5">[6]</ref> for the GEMM core of the VTA architecture <ref type="bibr" target="#b30">[31]</ref> limiting their applicability to generic spatial accelerators. Prior work such as Zhang et al. <ref type="bibr" target="#b55">[56]</ref> and Ma et al. <ref type="bibr" target="#b28">[29]</ref> focused on spatial accelerators without L1 buffers inside a PE, again limiting their mapping space formulation. Furthermore, they do not employ accurate cost models and focus only on optimizing for runtime.</p><p>In addition, other prior works such as Interstellar and dMazeRunner fixed certain aspects of mapping space such as choice of parallel loops and loop orders, and these choices may not reflect the efficient mappings for a wide variety of DNN operators. To the best of our knowledge, Timeloop Ma et al. <ref type="bibr" target="#b28">[29]</ref>, Auto-TVM <ref type="bibr" target="#b5">[6]</ref>, dMazeRunner <ref type="bibr" target="#b11">[12]</ref>, Interstellar <ref type="bibr" target="#b53">[54]</ref>, and Timeloop <ref type="bibr" target="#b33">[34]</ref>) for the mapping space exploration of operators. Our approach (Marvel) supports any operator conformable with the MDC notation.</p><p>is the only framework that considers all aspects of a mapping for a fully flexible spatial accelerator. However, it employs either an exhaustive linear search or a random sampling-based heuristic to explore the search space. In contrast to all preceding works, our approach considers all the aspects of mapping space and uses the decoupled strategy to navigate the mapping space efficiently.</p><p>Our work's key novelty is the formalization of MDC conformable operators using the four rules defined in Section 3, and with the conformability, our approach always generates a correct set of MDC directives corresponding to a loop nest mapping of the operator. The prior work introducing MDC directives <ref type="bibr" target="#b23">[24]</ref> does not have any formalization and also any correctness checker over the usage of MDC directives. Further, the prior work is limited to hardware DSE and does not have any mapping explorer, unlike our approach.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8">CONCLUSION AND FUTURE WORK</head><p>In this article, we characterize the set of input operators and their mappings expressed in the MDC notation by introducing a set of conformability rules. The outcome of these rules is that any loop nest which is perfectly nested with affine tensor subscripts and without conditionals is conformable to the MDC notation, and a majority of the primitive operators in DL are such loop nests. In addition, our rules enable us to automatically translate a mapping expressed in the loop nest form to MDC notation and use the MDC's cost model to guide upstream mappers. Our conformability rules over the input operators result in a structured mapping space of the operators, and this enables us to introduce a mapper based on our decoupled off-chip/on-chip approach to accelerate mapping space exploration. We implemented our decoupled approach in a tool called Marvel, <ref type="foot" target="#foot_8">6</ref> and a major benefit of our approach is that it applies to any DNN operator conformable with the MDC notation. Our approach reduced the search space of CONV2D operators from four major DNN models from 9.4 × 10 18 to 1.5 × 10 8 + 5.9 × 10 8 2.1 × 10 8 , which is a reduction factor of 10 billion (Figure <ref type="figure" target="#fig_11">10</ref>), while generating mappings that demonstrate a geometric mean performance improvement of 10.25× higher throughput and 2.01× lower energy consumption compared with three state-of-the-art mapping styles from past work. In the future, we envision (1) Marvel integration with the MLIR compiler infrastructure for wide usability, and (2) extending the MDC notation and its cost model to support non-conformable operators.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Marvel 6 : 3 Fig. 1 .</head><label>631</label><figDesc>Fig. 1. Overview of the design-time flow for computer architects developing new accelerators, and the compilation flow for ML programmers leveraging the accelerators. The scope of this work is the mapping explorer and the loop optimizer in the preceding diagram.</figDesc><graphic url="image-1.png" coords="3,107.46,74.18,268.80,131.76" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head></head><label></label><figDesc>(b)) for the CONV1D operator onto an accelerator having two PEs. First, TemporalMap (size, offset) d specifies a distribution of the tensor dimension d index values across timesteps in a PE, and the mapped set of dimension indices is same across PEs. The size parameter refers to the number of contiguous indices mapped, and the offset parameter describes the shift in the starting indices of d across consecutive timesteps. For instance, the directive TemporalMap(2,2) d w in the running example represents the distribution of first dimension (d w ) indices of the weight tensor, with two indices mapped in each timestep (e.g., d w ={0,1} in PE0 and PE1 at t = 0). In addition, the offset of two denotes the increment in the d w index after each timestep (e.g., d w ={0,1} at t = 0 to d w ={2,3} at t = 1) until the extent of the d w dimension is explored. Second, SpatialMap (size, offset) d specifies a distribution of the tensor dimension d index values across PEs. The size parameter refers to the number of contiguous indices mapped, and the offset describes the shift in the starting indices of d across adjacent PEs. For instance, the</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 3 .</head><label>3</label><figDesc>Fig. 3. A CONV1D's mapping in the MDC notation along with a visualization of its data mappings.</figDesc><graphic url="image-2.png" coords="6,125.78,74.19,231.96,182.76" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Marvel 6 : 7 Fig. 4 .</head><label>674</label><figDesc>Fig. 4. Additional mappings built on the example in Figure 3 for describing (a) parallelism across multiple dimensions and (b) tiling behavior.</figDesc><graphic url="image-3.png" coords="7,79.97,73.99,323.88,180.96" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Fig. 5 .</head><label>5</label><figDesc>Fig. 5. The DDG of simple operators such as CONV1D and stencil satisfying rule R3 and an example violating rule R3. d O /d I /d W : tensor dimension index variables corresponding to the output, input, and weight tensors.</figDesc><graphic url="image-4.png" coords="8,111.29,73.54,260.60,160.40" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Rule 2 :</head><label>2</label><figDesc>The loop nest must not have any dependences (flow-, anti-, output-) except reduction dependences, and thus the loops can be freely reordered.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>6 : 12 P</head><label>612</label><figDesc>. Chatarasi et al.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Fig. 6 .ALGORITHM 1 : 7 for 12 Level- 1 -</head><label>617121</label><figDesc>Fig.6. A brief overview of the CONV1D's mapping expressed in the loop nest form and its translation into the MDC notation with data mapping directives.</figDesc><graphic url="image-5.png" coords="12,127.78,74.63,227.64,229.32" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>ALGORITHM 2 : 8 // Translate mapping into MDC form 9</head><label>289</label><figDesc>Our approach to explore on-chip mapping subspace. 1 for every level-2 inter-tile loop order do 2 for every level-2 tile size do 3 Hardware pruning: PE utilization bound 4 Hardware pruning: No prologues/epilogues 5 for every level-1 tile size do 6 Hardware pruning: Finite L1 size buffer 7 Hardware pruning: No prologue/epilogue Invoke the MDC's cost model → (runtime, energy, and other metrics)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head>6 : 18 P</head><label>618</label><figDesc>. Chatarasi et al.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_11"><head>Fig. 10 .</head><label>10</label><figDesc>Fig.10. The statistics (min/avg/max) of the CONV2D mapping space in our evaluation and the resultant mapping subspaces after decoupling and pruning strategies.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_12"><head>Fig. 15 .</head><label>15</label><figDesc>Fig. 15. MLP and LSTM workloads taken from Interstellar [54].</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_13"><head>Marvel 6 : 23 Fig. 16 .</head><label>62316</label><figDesc>Fig.<ref type="bibr" target="#b15">16</ref>. Comparison of Marvel with prior mappers for spatial accelerators (mRNA<ref type="bibr" target="#b56">[57]</ref>, Zhang et al.<ref type="bibr" target="#b55">[56]</ref>, Ma et al.<ref type="bibr" target="#b28">[29]</ref>, Auto-TVM<ref type="bibr" target="#b5">[6]</ref>, dMazeRunner<ref type="bibr" target="#b11">[12]</ref>, Interstellar<ref type="bibr" target="#b53">[54]</ref>, and Timeloop<ref type="bibr" target="#b33">[34]</ref>) for the mapping space exploration of operators. Our approach (Marvel) supports any operator conformable with the MDC notation.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0"><head></head><label></label><figDesc></figDesc><graphic url="image-7.png" coords="19,47.47,242.23,388.80,147.24" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 .</head><label>1</label><figDesc>Conformability of the Popular DNN Operators onto the MDC Notation (Y/N Refers to YES/NO)</figDesc><table><row><cell>DNN Operator</cell><cell>Types</cell><cell>Rule 1 Rule 2 Rule 3 Rule 4 Conformable to MDC</cell></row><row><cell>CONV1D</cell><cell>Regular</cell><cell>Y</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 .</head><label>2</label><figDesc>Operators, Occurrences, and MDC Conformability in DNN Models of MLPerf<ref type="bibr" target="#b37">[38]</ref>, VGG16, and AlexNet</figDesc><table><row><cell>DNN Operator</cell><cell cols="5">MLPerf Suite MobileNetV1 ResNet50 SSD-MobileNet SSD-ResNet34 GNMT</cell><cell cols="2">VGG16 AlexNet</cell><cell>MDC Conformable?</cell></row><row><cell>Regular CONV2D</cell><cell>15</cell><cell>54</cell><cell>34</cell><cell>51</cell><cell>0</cell><cell>16</cell><cell>9</cell></row><row><cell>Depth-wise CONV2D</cell><cell>13</cell><cell>0</cell><cell>13</cell><cell>0</cell><cell>0</cell><cell>0</cell><cell>0</cell></row><row><cell>Bias Add</cell><cell>1</cell><cell>1</cell><cell>12</cell><cell>12</cell><cell>0</cell><cell>1</cell><cell>1</cell></row><row><cell>Batch Normalization</cell><cell>13</cell><cell>20</cell><cell>13</cell><cell>15</cell><cell>0</cell><cell>0</cell><cell>0</cell></row><row><cell>ReLU</cell><cell>27</cell><cell>49</cell><cell>35</cell><cell>37</cell><cell>0</cell><cell>15</cell><cell>8</cell></row><row><cell>Softmax</cell><cell>1</cell><cell>0</cell><cell>0</cell><cell>1</cell><cell>0</cell><cell>1</cell><cell>1</cell></row><row><cell>Avg pooling</cell><cell>1</cell><cell>0</cell><cell>0</cell><cell>0</cell><cell>0</cell><cell>0</cell><cell>0</cell></row><row><cell>Max Pooling</cell><cell>0</cell><cell>1</cell><cell>0</cell><cell>1</cell><cell>0</cell><cell>5</cell><cell>3</cell></row><row><cell>GEMM</cell><cell>0</cell><cell>0</cell><cell>0</cell><cell>0</cell><cell>9</cell><cell>0</cell><cell>0</cell></row><row><cell></cell><cell></cell><cell cols="4">Fig. 8. Accelerator setups in our evaluation.</cell><cell></cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 3 .</head><label>3</label><figDesc>Comparison of the MDC Notation with Prior Compilers in Terms of Expressiveness, Mapping Notation, and the Presence of Accurate Cost Models</figDesc><table><row><cell>Notation</cell><cell cols="3">Operator Expressiveness Loop Nest Array Iteration Structure Subscripts Domain</cell><cell>Mapping Representation</cell><cell>Accurate Cost Models for Spatial Accelerators</cell></row><row><cell>MDC</cell><cell>Perfect</cell><cell>Affine</cell><cell>Affine</cell><cell>Data-centric</cell><cell>YES</cell></row><row><cell cols="2">TVM, TC, PlaidML, Stripe, ISAMIR Perfect/Imperfect</cell><cell>Affine</cell><cell>Rectangular</cell><cell>Loop-centric</cell><cell>NO (YES for limited scenarios [54])</cell></row><row><cell>Polyhedral (e.g., Tiramisu)</cell><cell>Perfect/Imperfect</cell><cell>Affine</cell><cell>Affine</cell><cell>Loop-centric</cell><cell>NO</cell></row><row><cell>Generic loop nests (e.g., MLIR)</cell><cell>Perfect/Imperfect</cell><cell>Any</cell><cell>Any</cell><cell>Loop-centric</cell><cell>NO (YES only for limited scenarios</cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0">The generic loop nest forms encompass various control flow structures and data-access patterns that make it harder for the cost models to precisely reason (more details can be found inSection 7).ACM Transactions on Architecture and Code Optimization, Vol. 19, No. 1, Article 6. Publication date: December</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2021" xml:id="foot_1">.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" xml:id="foot_2">ACM Transactions on Architecture and Code Optimization, Vol. 19, No. 1, Article 6. Publication date: December 2021.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_3">The SIV subscript involves one loop iterator, whereas MIV subscripts involve more than one loop iterator<ref type="bibr" target="#b1">[2]</ref>.ACM Transactions on Architecture and Code Optimization, Vol. 19, No. 1, Article 6. Publication date: December 2021.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3" xml:id="foot_4">An n-dimensional loop nest after one level of tiling will have 2n loops. The outer n-loops are referred to as inter-tile loops and the later n-loops as intra-tile loops. The innermost n-loops after multi-level tiling are called point loops. ACM Transactions on Architecture and Code Optimization, Vol. 19, No. 1, Article 6. Publication date: December 2021.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4" xml:id="foot_5">In case of non-software-managed scratchpad buffers, reducing data movement between DRAM and the L2 buffer is equivalent to finding a level-3 tile whose memory footprint can fit into the L2 buffer and is maximum. ACM Transactions on Architecture and Code Optimization, Vol. 19, No. 1, Article 6. Publication date: December 2021.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="5" xml:id="foot_6">Setting block size to one ignores the impact of data layouts that we consider in our approach.ACM Transactions on Architecture and Code Optimization, Vol. 19, No. 1, Article</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="6" xml:id="foot_7">. Publication date: December 2021.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="6" xml:id="foot_8">Recently, we incorporated a variant of our decoupled off-chip/on-chip approach in the UNION framework<ref type="bibr" target="#b18">[19]</ref>.ACM Transactions on Architecture and Code Optimization, Vol. 19, No. 1, Article 6. Publication date: December 2021.</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>ACKNOWLEDGMENTS</head><p>We thank Saurabh Malik, Natesh Raina, and Vaisakh Haridas for their insightful comments and help in the initial discussions of this work. We also thank the anonymous reviewers for their valuable feedback.</p></div>
			</div>


			<div type="funding">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Prasanth Chatarasi is now at IBM Research. Hyoukjun Kwon is now at Facebook. This work was supported by NSF Awards 1755876, 1909900, and 1822939, and also the U.S. Department of Energy's National Nuclear Security Administration under contract DE-NA-0003525.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">Edge TPU: Google&apos;s Purpose-Built ASIC Designed to Run</title>
		<author>
			<persName><forename type="first">Google</forename><surname>Cloud</surname></persName>
		</author>
		<ptr target="https://cloud.google.com/edge-tpu/" />
		<imprint>
			<date type="published" when="2019-10-13">2019. October 13. 2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">Optimizing Compilers for Modern Architectures: A Dependence-Based Approach</title>
		<author>
			<persName><forename type="first">Randy</forename><surname>Allen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ken</forename><surname>Kennedy</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2001">2001</date>
			<publisher>Morgan Kaufmann</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Tiramisu: A polyhedral compiler for expressing fast and portable code</title>
		<author>
			<persName><forename type="first">Riyadh</forename><surname>Baghdadi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jessica</forename><surname>Ray</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Malek</forename><surname>Ben Romdhane</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Emanuele</forename><surname>Del Sozzo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Abdurrahman</forename><surname>Akkas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yunming</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Patricia</forename><surname>Suriana</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shoaib</forename><surname>Kamil</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Saman</forename><surname>Amarasinghe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2019 IEEE/ACM International Symposium on Code Generation and Optimization (CGO&apos;19)</title>
				<meeting>the 2019 IEEE/ACM International Symposium on Code Generation and Optimization (CGO&apos;19)<address><addrLine>Los Alamitos, CA</addrLine></address></meeting>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="193" to="205" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">End to end learning for self-driving cars</title>
		<author>
			<persName><forename type="first">Mariusz</forename><surname>Bojarski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Davide</forename><forename type="middle">Del</forename><surname>Testa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Daniel</forename><surname>Dworakowski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bernhard</forename><surname>Firner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Beat</forename><surname>Flepp</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Prasoon</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lawrence</forename><forename type="middle">D</forename><surname>Jackel</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1604.07316</idno>
		<imprint>
			<date type="published" when="2016">2016. 2016</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Diannao: A small-footprint high-throughput accelerator for ubiquitous machine-learning</title>
		<author>
			<persName><forename type="first">Tianshi</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zidong</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ninghui</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jia</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chengyong</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yunji</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Olivier</forename><surname>Temam</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Conference on Architectural Support for Programming Languages and Operating Systems (ASPLOS&apos;14)</title>
				<meeting>the International Conference on Architectural Support for Programming Languages and Operating Systems (ASPLOS&apos;14)</meeting>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">TVM: An automated end-to-end optimizing compiler for deep learning</title>
		<author>
			<persName><forename type="first">Tianqi</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Thierry</forename><surname>Moreau</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ziheng</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lianmin</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Eddie</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Meghan</forename><surname>Cowan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Haichen</forename><surname>Shen</surname></persName>
		</author>
		<ptr target="http://dl.acm.org/citation.cfm?id=3291168.3291211" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 12th USENIX Conference on Operating Systems Design and Implementation (OSDI&apos;18)</title>
				<meeting>the 12th USENIX Conference on Operating Systems Design and Implementation (OSDI&apos;18)</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="579" to="594" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Eyeriss: A spatial architecture for energy-efficient dataflow for convolutional neural networks</title>
		<author>
			<persName><forename type="first">Yu-Hsin</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Joel</forename><surname>Emer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vivienne</forename><surname>Sze</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Symposium on Computer Architecture (ISCA&apos;16)</title>
				<meeting>the International Symposium on Computer Architecture (ISCA&apos;16)</meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Eyeriss: An energy-efficient reconfigurable accelerator for deep convolutional neural networks</title>
		<author>
			<persName><forename type="first">Yu-Hsin</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tushar</forename><surname>Krishna</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Joel</forename><forename type="middle">S</forename><surname>Emer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vivienne</forename><surname>Sze</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Journal of Solid-State Circuits</title>
		<imprint>
			<biblScope unit="volume">52</biblScope>
			<biblScope unit="page" from="127" to="138" />
			<date type="published" when="2017">2017. 2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Eyeriss: An energy-efficient reconfigurable accelerator for deep convolutional neural networks</title>
		<author>
			<persName><forename type="first">Yu-Hsin</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tushar</forename><surname>Krishna</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Joel</forename><forename type="middle">S</forename><surname>Emer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vivienne</forename><surname>Sze</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Journal of Solid-State Circuits</title>
		<imprint>
			<biblScope unit="volume">52</biblScope>
			<biblScope unit="page" from="127" to="138" />
			<date type="published" when="2017">2017. 2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Eyeriss v2: A flexible accelerator for emerging deep neural networks on mobile devices</title>
		<author>
			<persName><forename type="first">Yu-Hsin</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tien-Ju</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Joel</forename><surname>Emer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vivienne</forename><surname>Sze</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Journal on Emerging and Selected Topics in Circuits and Systems</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="page" from="292" to="308" />
			<date type="published" when="2019">2019. 2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Serving DNNs in real time at datacenter scale with project brainwave</title>
		<author>
			<persName><forename type="first">Eric</forename><forename type="middle">S</forename><surname>Chung</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jeremy</forename><surname>Fowers</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kalin</forename><surname>Ovtcharov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><surname>Papamichael</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Adrian</forename><forename type="middle">M</forename><surname>Caulfield</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Todd</forename><surname>Massengill</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ming</forename><surname>Liu</surname></persName>
		</author>
		<idno type="DOI">10.1109/MM.2018.022071131</idno>
		<ptr target="https://doi.org/10.1109/MM.2018.022071131" />
	</analytic>
	<monogr>
		<title level="j">IEEE Micro</title>
		<imprint>
			<biblScope unit="volume">38</biblScope>
			<biblScope unit="page" from="8" to="20" />
			<date type="published" when="2018">2018. 2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">DMazeRunner: Executing perfectly nested loops on dataflow accelerators</title>
		<author>
			<persName><forename type="first">Shail</forename><surname>Dave</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Youngbin</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sasikanth</forename><surname>Avancha</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kyoungwoo</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aviral</forename><surname>Shrivastava</surname></persName>
		</author>
		<idno type="DOI">10.1145/3358198</idno>
		<ptr target="https://doi.org/10.1145/3358198" />
	</analytic>
	<monogr>
		<title level="j">ACM Transactions on Embedded Computing Systems</title>
		<imprint>
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="page">5</biblScope>
			<date type="published" when="2019-10">2019. Oct. 2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">ShiDianNao: Shifting vision processing closer to the sensor</title>
		<author>
			<persName><forename type="first">Zidong</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Robert</forename><surname>Fasthuber</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tianshi</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Paolo</forename><surname>Ienne</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ling</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tao</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaobing</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yunji</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Olivier</forename><surname>Temam</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Symposium on Computer Architecture (ISCA&apos;15)</title>
				<meeting>the International Symposium on Computer Architecture (ISCA&apos;15)</meeting>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">A scalable multi-TeraOPS deep learning processor core for AI trainina and inference</title>
		<author>
			<persName><forename type="first">M</forename><surname>Bruce</surname></persName>
		</author>
		<author>
			<persName><surname>Fleischer</surname></persName>
		</author>
		<idno type="DOI">10.1109/VLSIC.2018.8502276</idno>
		<ptr target="https://doi.org/10.1109/VLSIC.2018.8502276" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2018 IEEE Symposium on VLSI Circuits. IEEE</title>
				<meeting>the 2018 IEEE Symposium on VLSI Circuits. IEEE<address><addrLine>Los Alamitos, CA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2018">bibinfoyear2018</date>
			<biblScope unit="page" from="35" to="36" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Learning hierarchical features for scene labeling</title>
		<author>
			<persName><forename type="first">Clement</forename><surname>Farabet</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Camille</forename><surname>Couprie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Laurent</forename><surname>Najman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yann</forename><surname>Lecun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="page" from="1915" to="1929" />
			<date type="published" when="2013">2013. 2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">On estimating and enhancing cache effectiveness</title>
		<author>
			<persName><forename type="first">Jeanne</forename><surname>Ferrante</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vivek</forename><surname>Sarkar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wendy</forename><surname>Thrash</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Workshop on Languages and Compilers for Parallel Computing</title>
				<meeting>the International Workshop on Languages and Compilers for Parallel Computing</meeting>
		<imprint>
			<date type="published" when="1991">1991</date>
			<biblScope unit="page" from="328" to="343" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shaoqing</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
				<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="770" to="778" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<author>
			<persName><forename type="first">Shell</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sergey</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nikos</forename><surname>Zagoruyko</surname></persName>
		</author>
		<author>
			<persName><surname>Komodakis</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1812.11027[cs.LG]</idno>
		<title level="m">Exploring weight symmetry in deep neural networks</title>
				<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Union: A unified HW-SW Co-Design ecosystem in MLIR for evaluating tensor operations on spatial accelerators</title>
		<author>
			<persName><forename type="first">Geonhwa</forename><surname>Jeong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gokcen</forename><surname>Kestor</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chatarasi</forename><surname>Prasanth</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Angshuman</forename><surname>Parashar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Po-An</forename><surname>Tsai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sivasankaran</forename><surname>Rajamanickam</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Roberto</forename><surname>Gioiosa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tushar</forename><surname>Krishna</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 30th International Conference on Parallel Architectures and Compilation Techniques (PACT&apos;21)</title>
				<meeting>the 30th International Conference on Parallel Architectures and Compilation Techniques (PACT&apos;21)</meeting>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">In-datacenter performance analysis of a tensor processing unit</title>
		<author>
			<persName><forename type="first">Norman</forename><forename type="middle">P</forename><surname>Jouppi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Cliff</forename><surname>Young</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nishant</forename><surname>Patil</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><surname>Patterson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gaurav</forename><surname>Agrawal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Raminder</forename><surname>Bajwa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sarah</forename><surname>Bates</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Symposium on Computer Architecture (ISCA&apos;17)</title>
				<meeting>the International Symposium on Computer Architecture (ISCA&apos;17)<address><addrLine>Los Alamitos, CA</addrLine></address></meeting>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="1" to="12" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Deep visual-semantic alignments for generating image descriptions</title>
		<author>
			<persName><forename type="first">Andrej</forename><surname>Karpathy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Li</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Conference on Computer Vision and Pattern Recognition (CVPR&apos;15)</title>
				<meeting>the Conference on Computer Vision and Pattern Recognition (CVPR&apos;15)</meeting>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">ImageNet classification with deep convolutional neural networks</title>
		<author>
			<persName><forename type="first">A</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Geoffrey</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the NIPS Conference</title>
				<meeting>the NIPS Conference</meeting>
		<imprint>
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">MAESTRO: An Open-Source Infrastructure for Modeling Dataflows Within Deep Learning Accelerators</title>
		<author>
			<persName><forename type="first">Hyoukjun</forename><surname>Kwon</surname></persName>
		</author>
		<ptr target="https://github.com/maestro-project/maestro/tree/master/data/mapping" />
		<imprint>
			<date type="published" when="2020-10-13">2020. October 13</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Understanding reuse, performance, and hardware cost of DNN dataflow: A data-centric approach</title>
		<author>
			<persName><forename type="first">Hyoukjun</forename><surname>Kwon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Prasanth</forename><surname>Chatarasi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><surname>Pellauer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Angshuman</forename><surname>Parashar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vivek</forename><surname>Sarkar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tushar</forename><surname>Krishna</surname></persName>
		</author>
		<idno type="DOI">10.1145/3352460.3358252</idno>
		<ptr target="https://doi.org/10.1145/3352460.3358252" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 52nd Annual IEEE/ACM International Symposium on Microarchitecture (MICRO&apos;52)</title>
				<meeting>the 52nd Annual IEEE/ACM International Symposium on Microarchitecture (MICRO&apos;52)<address><addrLine>New York, NY</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="754" to="768" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">MAESTRO: A data-centric approach to understand reuse, performance, and hardware cost of DNN mappings</title>
		<author>
			<persName><forename type="first">Hyoukjun</forename><surname>Kwon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Prasanth</forename><surname>Chatarasi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vivek</forename><surname>Sarkar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tushar</forename><surname>Krishna</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><surname>Pellauer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Angshuman</forename><surname>Parashar</surname></persName>
		</author>
		<idno type="DOI">10.1109/MM.2020.2985963</idno>
		<ptr target="https://doi.org/10.1109/MM.2020.2985963" />
	</analytic>
	<monogr>
		<title level="j">IEEE Micro</title>
		<imprint>
			<biblScope unit="volume">40</biblScope>
			<biblScope unit="page" from="20" to="29" />
			<date type="published" when="2020">2020. 2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">MAERI: Enabling flexible dataflow mapping over DNN accelerators via reconfigurable interconnects</title>
		<author>
			<persName><forename type="first">Hyoukjun</forename><surname>Kwon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ananda</forename><surname>Samajdar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tushar</forename><surname>Krishna</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 23rd International Conference on Architectural Support for Programming Languages and Operating Systems (ASPLOS&apos;18)</title>
				<meeting>the 23rd International Conference on Architectural Support for Programming Languages and Operating Systems (ASPLOS&apos;18)<address><addrLine>New York, NY</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="461" to="475" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<author>
			<persName><forename type="first">Chris</forename><surname>Lattner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mehdi</forename><surname>Amini</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Uday</forename><surname>Bondhugula</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Albert</forename><surname>Cohen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andy</forename><surname>Davis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jacques</forename><surname>Pienaar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">River</forename><surname>Riddle</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tatiana</forename><surname>Shpeisman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nicolas</forename><surname>Vasilache</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Oleksandr</forename><surname>Zinenko</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2002.11054[cs.PL]</idno>
		<title level="m">MLIR: A compiler infrastructure for the end of Moore&apos;s law</title>
				<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Optimizing memory efficiency for deep convolutional neural networks on GPUs</title>
		<author>
			<persName><forename type="first">Chao</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yi</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Min</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Srimat</forename><surname>Chakradhar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Huiyang</forename><surname>Zhou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Conference for High Performance Computing, Networking, Storage, and Analysis (SC&apos;16)</title>
				<meeting>the International Conference for High Performance Computing, Networking, Storage, and Analysis (SC&apos;16)<address><addrLine>Los Alamitos, CA</addrLine></address></meeting>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="633" to="644" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Optimizing loop operation and dataflow in FPGA acceleration of deep convolutional neural networks</title>
		<author>
			<persName><forename type="first">Yufei</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yu</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sarma</forename><surname>Vrudhula</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jae-Sun</forename><surname>Seo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Symposium on Field-Programmable Gate Arrays (FPGA&apos;17)</title>
				<meeting>the International Symposium on Field-Programmable Gate Arrays (FPGA&apos;17)<address><addrLine>Los Alamitos, CA</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="45" to="54" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Leveraging the VTA-TVM hardware-software stack for FPGA acceleration of 8-Bit ResNet-18 inference</title>
		<author>
			<persName><forename type="first">Thierry</forename><surname>Moreau</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tianqi</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Luis</forename><surname>Ceze</surname></persName>
		</author>
		<idno type="DOI">10.1145/3229762.3229766</idno>
		<ptr target="https://doi.org/10.1145/3229762.3229766" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 1st Conference on Reproducible Quality-Efficient Systems Tournament on Co-Designing Pareto-Efficient Deep Learning (ReQuEST&apos;18)</title>
				<meeting>the 1st Conference on Reproducible Quality-Efficient Systems Tournament on Co-Designing Pareto-Efficient Deep Learning (ReQuEST&apos;18)<address><addrLine>New York, NY, Article</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">A hardware-software blueprint for flexible deep learning specialization</title>
		<author>
			<persName><forename type="first">Thierry</forename><surname>Moreau</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tianqi</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Luis</forename><surname>Vega</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jared</forename><surname>Roesch</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Eddie</forename><forename type="middle">Q</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lianmin</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Josh</forename><surname>Fromm</surname></persName>
		</author>
		<idno type="DOI">10.1109/MM.2019.2928962</idno>
		<ptr target="https://doi.org/10.1109/MM.2019.2928962" />
	</analytic>
	<monogr>
		<title level="j">IEEE Micro</title>
		<imprint>
			<biblScope unit="volume">39</biblScope>
			<biblScope unit="page" from="8" to="16" />
			<date type="published" when="2019">2019. 2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Design space exploration of FPGAbased deep convolutional neural networks</title>
		<author>
			<persName><forename type="first">Mohammad</forename><surname>Motamedi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Philipp</forename><surname>Gysel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Venkatesh</forename><surname>Akella</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Soheil</forename><surname>Ghiasi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2016 21st Asia and South Pacific Design Automation Conference (ASP-DAC&apos;16)</title>
				<meeting>the 2016 21st Asia and South Pacific Design Automation Conference (ASP-DAC&apos;16)<address><addrLine>Los Alamitos, CA</addrLine></address></meeting>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="575" to="580" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
		<title level="m" type="main">NVIDIA Deep Learning Accelerator (NVDLA)</title>
		<ptr target="https://nvidia.org" />
		<imprint>
			<date type="published" when="2018-10-13">2018. October 13</date>
		</imprint>
		<respStmt>
			<orgName>NVIDIA</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Timeloop: A systematic approach to DNN accelerator evaluation</title>
		<author>
			<persName><forename type="first">Angshuman</forename><surname>Parashar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Priyanka</forename><surname>Raina</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sophia</forename><surname>Yakun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yu-Hsin</forename><surname>Shao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Victor</forename><forename type="middle">A</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Anurag</forename><surname>Ying</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rangarajan</forename><surname>Mukkara</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Brucek</forename><surname>Venkatesan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stephen</forename><forename type="middle">W</forename><surname>Khailany</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Joel</forename><surname>Keckler</surname></persName>
		</author>
		<author>
			<persName><surname>Emer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2019 IEEE International Symposium on Performance Analysis of Systems and Software (ISPASS&apos;19)</title>
				<meeting>the 2019 IEEE International Symposium on Performance Analysis of Systems and Software (ISPASS&apos;19)</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">SCNN: An accelerator for compressed-sparse convolutional neural networks</title>
		<author>
			<persName><forename type="first">A</forename><surname>Parashar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Rhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Mukkara</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Puglielli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Venkatesan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Khailany</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Emer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">W</forename><surname>Keckler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">J</forename><surname>Dally</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Symposium on Computer Architecture (ISCA&apos;17)</title>
				<meeting>the International Symposium on Computer Architecture (ISCA&apos;17)</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="27" to="40" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<monogr>
		<title/>
		<author>
			<persName><surname>Github</surname></persName>
		</author>
		<ptr target="https://github.com/plaidml/plaidml" />
		<imprint>
			<date type="published" when="2020-10-13">2020. October 13</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">SIGMA: A sparse and irregular GEMM accelerator with flexible interconnects for DNN training</title>
		<author>
			<persName><forename type="first">Eric</forename><surname>Qin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ananda</forename><surname>Samajdar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hyoukjun</forename><surname>Kwon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vineet</forename><surname>Nadella</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sudarshan</forename><surname>Srinivasan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dipankar</forename><surname>Das</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bharat</forename><surname>Kaul</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tushar</forename><surname>Krishna</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2020 IEEE International Symposium on High Performance Computer Architecture (HPCA&apos;20)</title>
				<meeting>the 2020 IEEE International Symposium on High Performance Computer Architecture (HPCA&apos;20)</meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<monogr>
		<author>
			<persName><forename type="first">Vijay</forename><surname>Janapa Reddi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christine</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><surname>Kanter</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Peter</forename><surname>Mattson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Guenther</forename><surname>Schmuelling</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Carole-Jean</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Brian</forename><surname>Anderson</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1911.02549[cs.LG]</idno>
		<title level="m">MLPerf inference benchmark</title>
				<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">A geometric programming framework for optimal multi-level tiling</title>
		<author>
			<persName><forename type="first">Lakshminarayanan</forename><surname>Renganarayana</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sanjay</forename><surname>Rajopadhye</surname></persName>
		</author>
		<idno type="DOI">10.1109/SC.2004.3</idno>
		<ptr target="https://doi.org/10.1109/SC.2004.3" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2004 ACM/IEEE Conference on Supercomputing (SC&apos;04)</title>
				<meeting>the 2004 ACM/IEEE Conference on Supercomputing (SC&apos;04)<address><addrLine>Los Alamitos, CA</addrLine></address></meeting>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2004">2004</date>
			<biblScope unit="page">18</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Positivity, posynomials and tile size selection</title>
		<author>
			<persName><forename type="first">Lakshminarayanan</forename><surname>Renganarayana</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sanjay</forename><surname>Rajopadhye</surname></persName>
		</author>
		<ptr target="http://dl.acm.org/citation.cfm?id=1413370.1413426.6:26P.Chatarasietal." />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2008 ACM/IEEE Conference on Supercomputing (SC&apos;08)</title>
				<meeting>the 2008 ACM/IEEE Conference on Supercomputing (SC&apos;08)<address><addrLine>Los Alamitos, CA, Article</addrLine></address></meeting>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2008">2008</date>
			<biblScope unit="volume">55</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">U-Net: Convolutional networks for biomedical image segmentation</title>
		<author>
			<persName><forename type="first">O</forename><surname>Ronneberger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Fischer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Brox</surname></persName>
		</author>
		<ptr target="http://lmb.informatik.uni-freiburg.de/Publications/2015/RFB15a" />
	</analytic>
	<monogr>
		<title level="m">Medical Image Computing and Computer-Assisted Intervention</title>
		<title level="s">Lecture Notes in Computer Science</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2015">2015</date>
			<biblScope unit="volume">9351</biblScope>
			<biblScope unit="page" from="234" to="241" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Ima-geNet large scale visual recognition challenge</title>
		<author>
			<persName><forename type="first">Olga</forename><surname>Russakovsky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jia</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hao</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jonathan</forename><surname>Krause</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sanjeev</forename><surname>Satheesh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sean</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhiheng</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Computer Vision</title>
		<imprint>
			<biblScope unit="volume">115</biblScope>
			<biblScope unit="page" from="211" to="252" />
			<date type="published" when="2015">2015. 2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">MobileNetV2: Inverted residuals and linear bottlenecks</title>
		<author>
			<persName><forename type="first">Mark</forename><surname>Sandler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrew</forename><surname>Howard</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Menglong</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrey</forename><surname>Zhmoginov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Liang-Chieh</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
				<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="4510" to="4520" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Automatic selection of high-order transformations in the IBM XL FORTRAN compilers</title>
		<author>
			<persName><forename type="first">V</forename><surname>Sarkar</surname></persName>
		</author>
		<idno type="DOI">10.1147/rd.413.0233</idno>
		<ptr target="https://doi.org/10.1147/rd.413.0233" />
	</analytic>
	<monogr>
		<title level="j">IBM Journal of Research and Development</title>
		<imprint>
			<biblScope unit="volume">41</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="233" to="264" />
			<date type="published" when="1997-05">1997. May 1997</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">An analytical model for loop tiling and its solution</title>
		<author>
			<persName><forename type="first">V</forename><surname>Sarkar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Megiddo</surname></persName>
		</author>
		<ptr target="http://dl.acm.org/citation.cfm?id=1153923.1154542" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2000 IEEE International Symposium on Performance Analysis of Systems and Software (ISPASS&apos;00)</title>
				<meeting>the 2000 IEEE International Symposium on Performance Analysis of Systems and Software (ISPASS&apos;00)<address><addrLine>Los Alamitos, CA</addrLine></address></meeting>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2000">2000</date>
			<biblScope unit="page" from="146" to="153" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">From high-level deep neural models to FPGAs</title>
		<author>
			<persName><forename type="first">Hardik</forename><surname>Sharma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jongse</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Divya</forename><surname>Mahajan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Emmanuel</forename><surname>Amaro</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kyung</forename><surname>Joon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chenkai</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Asit</forename><surname>Shao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hadi</forename><surname>Mishra</surname></persName>
		</author>
		<author>
			<persName><surname>Esmaeilzadeh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/ACM International Symposium on Microarchitecture (MICRO&apos;16)</title>
				<meeting>the IEEE/ACM International Symposium on Microarchitecture (MICRO&apos;16)</meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Analytical bounds for optimal tile size selection</title>
		<author>
			<persName><forename type="first">Jun</forename><surname>Shirako</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kamal</forename><surname>Sharma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Naznin</forename><surname>Fauzia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Louis-Noël</forename><surname>Pouchet</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Ramanujam</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Sadayappan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vivek</forename><surname>Sarkar</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-642-28652-0_6</idno>
		<ptr target="https://doi.org/10.1007/978-3-642-28652-0_6" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 21st International Conference on Compiler Construction (CC&apos;12)</title>
				<meeting>the 21st International Conference on Compiler Construction (CC&apos;12)</meeting>
		<imprint>
			<date type="published" when="2012">2012</date>
			<biblScope unit="page" from="101" to="121" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Very deep convolutional networks for large-scale image recognition</title>
		<author>
			<persName><forename type="first">Karen</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrew</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Conference on Learning Representations (ICLR&apos;15)</title>
				<meeting>the International Conference on Learning Representations (ICLR&apos;15)</meeting>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">DeepPose: Human pose estimation via deep neural networks</title>
		<author>
			<persName><forename type="first">Alexander</forename><surname>Toshev</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christian</forename><surname>Szegedy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Conference on Computer Vision and Pattern Recognition (CVPR&apos;14)</title>
				<meeting>the Conference on Computer Vision and Pattern Recognition (CVPR&apos;14)</meeting>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<monogr>
		<author>
			<persName><forename type="first">Nicolas</forename><surname>Vasilache</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Oleksandr</forename><surname>Zinenko</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Theodoros</forename><surname>Theodoridis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Priya</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zachary</forename><surname>Devito</surname></persName>
		</author>
		<author>
			<persName><forename type="first">William</forename><forename type="middle">S</forename><surname>Moses</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sven</forename><surname>Verdoolaege</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrew</forename><surname>Adams</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Albert</forename><surname>Cohen</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1802.04730</idno>
		<title level="m">Tensor comprehensions: Framework-agnostic highperformance machine learning abstractions</title>
				<imprint>
			<date type="published" when="2018">2018. 2018</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">DeepTools: Compiler and execution runtime extensions for RaPiD AI accelerator</title>
		<author>
			<persName><forename type="first">S</forename><surname>Venkataramani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Srinivasan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Schaal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">J</forename><surname>Serrano</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Micro</title>
		<imprint>
			<biblScope unit="volume">39</biblScope>
			<biblScope unit="page" from="102" to="111" />
			<date type="published" when="2019">2019. 2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<monogr>
		<author>
			<persName><forename type="first">Yonghui</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mike</forename><surname>Schuster</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhifeng</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Quoc</forename><forename type="middle">V</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mohammad</forename><surname>Norouzi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wolfgang</forename><surname>Macherey</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Maxim</forename><surname>Krikun</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1609.08144</idno>
		<title level="m">Google&apos;s neural machine translation system: Bridging the gap between human and machine translation</title>
				<imprint>
			<date type="published" when="2016">2016. 2016</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b52">
	<monogr>
		<title level="m" type="main">Accelerating DNNs with Xilinx Alveo Accelerator Cards</title>
		<ptr target="https://www.xilinx.com/support/documentation/white_papers/wp504-accel-dnns.pdf" />
		<imprint>
			<date>October 13</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">Interstellar: Using Halide&apos;s scheduling language to analyze DNN accelerators</title>
		<author>
			<persName><forename type="first">Xuan</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mingyu</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qiaoyi</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jeff</forename><surname>Setter</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jing</forename><surname>Pu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ankita</forename><surname>Nayak</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Steven</forename><surname>Bell</surname></persName>
		</author>
		<idno type="DOI">10.1145/3373376.3378514</idno>
		<ptr target="https://doi.org/10.1145/3373376.3378514" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 25th International Conference on Architectural Support for Programming Languages and Operating Systems (ASPLOS&apos;20)</title>
				<meeting>the 25th International Conference on Architectural Support for Programming Languages and Operating Systems (ASPLOS&apos;20)<address><addrLine>New York, NY</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="369" to="383" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<monogr>
		<author>
			<persName><forename type="first">Tim</forename><surname>Zerrell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jeremy</forename><surname>Bruestle</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1903.06498[cs.DC]</idno>
		<title level="m">Stripe: Tensor compilation via the nested polyhedral model</title>
				<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<analytic>
		<title level="a" type="main">Optimizing FPGA-based accelerator design for deep convolutional neural networks</title>
		<author>
			<persName><forename type="first">Chen</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Peng</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Guangyu</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yijin</forename><surname>Guan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bingjun</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jason</forename><surname>Cong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2015 ACM/SIGDA International Symposium on Field-Programmable Gate Arrays</title>
				<meeting>the 2015 ACM/SIGDA International Symposium on Field-Programmable Gate Arrays<address><addrLine>New York, NY</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="161" to="170" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<analytic>
		<title level="a" type="main">mRNA: Enabling efficient mapping space exploration on a reconfigurable neural accelerator</title>
		<author>
			<persName><forename type="first">Zhongyuan</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hyoukjun</forename><surname>Kwon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sachit</forename><surname>Kuhar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Weiguang</forename><surname>Sheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhigang</forename><surname>Mao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tushar</forename><surname>Krishna</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of 2019 IEEE International Symposium on Performance Analysis of Systems and Software (ISPASS&apos;19)</title>
				<meeting>2019 IEEE International Symposium on Performance Analysis of Systems and Software (ISPASS&apos;19)<address><addrLine>Los Alamitos, CA</addrLine></address></meeting>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
