<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">MP-Rec: Hardware-Software Co-Design to Enable Multi-Path Recommendation</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date type="published" when="2023-02-21">21 Feb 2023</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Samuel</forename><surname>Hsia</surname></persName>
							<email>shsia@g.harvard.edu</email>
							<affiliation key="aff0">
								<orgName type="institution">Meta AI</orgName>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="institution">Harvard University</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Udit</forename><surname>Gupta</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Meta AI</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Bilge</forename><surname>Acun</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Meta AI</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Newsha</forename><surname>Ardalani</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Meta AI</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Pan</forename><surname>Zhong</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Meta AI</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Gu-Yeon</forename><surname>Wei</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution">Harvard University</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">David</forename><surname>Brooks</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Meta AI</orgName>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="institution">Harvard University</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Carole-Jean</forename><surname>Wu</surname></persName>
							<email>carolejeanwu@meta.com</email>
							<affiliation key="aff0">
								<orgName type="institution">Meta AI</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">MP-Rec: Hardware-Software Co-Design to Enable Multi-Path Recommendation</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2023-02-21">21 Feb 2023</date>
						</imprint>
					</monogr>
					<idno type="arXiv">arXiv:2302.10872v1[cs.AR]</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-01-03T08:32+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Deep learning recommendation systems serve personalized content under diverse tail-latency targets and inputquery loads. In order to do so, state-of-the-art recommendation models rely on terabyte-scale embedding tables to learn user preferences over large bodies of contents. The reliance on a fixed embedding representation of embedding tables not only imposes significant memory capacity and bandwidth requirements but also limits the scope of compatible system solutions. This paper challenges the assumption of fixed embedding representations by showing how synergies between embedding representations and hardware platforms can lead to improvements in both algorithmic-and system performance. Based on our characterization of various embedding representations, we propose a hybrid embedding representation that achieves higher quality embeddings at the cost of increased memory and compute requirements. To address the system performance challenges of the hybrid representation, we propose MP-Rec -a co-design technique that exploits heterogeneity and dynamic selection of embedding representations and underlying hardware platforms.</p><p>On real system hardware, we demonstrate how matching custom accelerators, i.e., GPUs, TPUs, and IPUs, with compatible embedding representations can lead to 16.65? performance speedup. Additionally, in queryserving scenarios, MP-Rec achieves 2.49? and 3.76? higher correct prediction throughput and 0.19% and 0.22% better model quality on a CPU-GPU system for the Kaggle and Terabyte datasets, respectively.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Deep learning (DL) recommendation models support a wide variety of applications, such as search <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b5">6,</ref><ref type="bibr" target="#b26">27,</ref><ref type="bibr" target="#b59">60]</ref>, social media <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b14">15,</ref><ref type="bibr" target="#b16">17,</ref><ref type="bibr" target="#b52">53]</ref>, e-commerce <ref type="bibr" target="#b60">[61,</ref><ref type="bibr" target="#b61">62]</ref>, and entertainment <ref type="bibr" target="#b17">[18]</ref>. Due to its overarching impact, neural recommendation has become a dominant source of compute cycles in large-scale AI infrastructures. In 2019, recommendation use cases contributed to 79% of the overall AI inference cycles at Meta, making it one of the most resource-demanding DL use cases at the data center scale <ref type="bibr" target="#b14">[15,</ref><ref type="bibr" target="#b16">17]</ref>. A critical component of state-of-the-art recommendation models is the embedding table <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b17">18,</ref><ref type="bibr" target="#b35">36,</ref><ref type="bibr" target="#b37">38,</ref><ref type="bibr" target="#b53">54,</ref><ref type="bibr" target="#b59">60,</ref><ref type="bibr" target="#b60">61,</ref><ref type="bibr" target="#b61">62]</ref>. Information, such as user preferences and content understanding, is represented as individual vectors within these embedding tables. To support increasingly complex applications and user preference models, embedding table sizes have grown super-linearly into the terabyte-scale <ref type="bibr" target="#b50">[51]</ref>. As a result, a plethora of systemand hardware-level solutions for neural recommendation have focused on addressing the memory capacity and bandwidth challenges of large-scale embedding tables <ref type="bibr" target="#b29">[30,</ref><ref type="bibr" target="#b33">34,</ref><ref type="bibr" target="#b48">49,</ref><ref type="bibr" target="#b23">24,</ref><ref type="bibr" target="#b35">36,</ref><ref type="bibr" target="#b2">3,</ref><ref type="bibr" target="#b6">7,</ref><ref type="bibr" target="#b32">33]</ref>.</p><p>However, there is additional room for algorithmic and system performance improvements if we go beyond exclusively using tables as embedding representation. Recent proposals examine alternative embedding representations use GEMM-heavy compute stacks to dynamically generate embedding vectors <ref type="bibr" target="#b53">[54,</ref><ref type="bibr" target="#b28">29]</ref>. While these representations significantly reduce memory capacity requirements, the techniques introduce orders of magnitude higher FLOPs.</p><p>Based on the detailed design space characterization for embedding representations (Figure <ref type="figure" target="#fig_0">1</ref>; Section 3), we identify significant performance improvement potential when utilizing custom accelerators for compatible representations. To demonstrate the impact of representationhardware compatibility, we perform real-system evaluations on a wide range of hardware, including CPUs and GPUs, as well as custom AI accelerators such as Google Tensor Processing Units (TPUs) <ref type="bibr" target="#b24">[25,</ref><ref type="bibr" target="#b25">26,</ref><ref type="bibr" target="#b26">27]</ref> and Graph-core Intelligence Processing Units (IPUs) <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b21">22]</ref> at core-, chip-, board, and pod-level configurations, demonstrating up to 16.65? performance speedup. Ultimately, there is no one-size-fits-all static solution as representation requirements and hardware capabilities vary.</p><p>In this work, we propose a new hybrid embedding representation for neural recommendation tasks. While prior representations focus exclusively on either memory-or compute-based execution paths, the hybrid representation leverages these contrasting execution patterns to increase learning capacity and produce higher quality embeddings. Our evaluation results show that the hybrid representation increases model quality by 0.19%, 0.22%, and 0.014% on the open-source Kaggle <ref type="bibr" target="#b27">[28]</ref>, Terabyte <ref type="bibr" target="#b45">[46]</ref>, and internal use-cases, respectively. Due to its increased complexity, the hybrid representation requires even higher capacity and memory requirements.</p><p>Taking a step further, to support the resource-intensive hybrid representation and dynamic mapping of heterogeneous accelerators and representations, we propose Multi-Path Recommendation (MP-Rec) -a dynamic representation-hardware co-design technique to maximize throughput of correct recommendations while meeting tail latency requirements. Depending on memory capacities of AI inference systems, MP-Rec first generates accuracyoptimal representation-hardware mappings based on the unique properties of different embedding representations (Figure <ref type="figure" target="#fig_0">1</ref> (right)), forming multiple potential embedding execution paths. At runtime, depending on input query sizes and application-specific performance targets, MP-Rec activates embedding path(s) by scheduling queries onto available representation-hardware configurations to jointly maximize prediction quality and throughput. To further speed up MP-Rec, we introduce MP-Cache to exploit novel caching opportunities introduced by the intermediate results of the new embedding representations. MP-Cache targets both data locality and value similarity of embedding accesses to make computationallyexpensive representations viable.</p><p>We evaluate the performance of MP-Rec on a real CPU-GPU platform, demonstrating 2.49? and 3.76? higher throughput of correct predictions on Kaggle <ref type="bibr" target="#b27">[28]</ref> and Terabyte <ref type="bibr" target="#b45">[46]</ref>, respectively, over the CPU baseline. In addition, when we integrate IPUs into MP-Rec for query serving, we observe a significant throughput improvement potential of 34.24? that can be unlocked with future software support. For the constant throughput scenarios at strict SLA latency targets, MP-Rec reduces SLA latency target violations by 27.59% compared to exclusively using the baseline embedding table-based recommendation models on CPUs. Overall, MP-Rec showcases the massive potential of algorithmic-and system-performance improvements when we integrate embedding representation design into the system design space for neural recommen-dation. Finally, these performance improvements can be further enhanced by custom AI accelerators that benefit from representation-level synergies.</p><p>The main contributions of this work are as follows:</p><p>? We propose a new hybrid embedding representation that increases learning capacity and produces higher quality embeddings at the cost of increased compute and memory requirements. The hybrid embedding representation demonstrates measurable improvements in model quality. ? We implement and characterize different embedding representations using state-of-the-art custom AI accelerators (i.e., TPUs and IPUs) at core-, chip-, board-, and pod-level configurations. We identify key system challenges of adapting specific representations to accelerators, highlighting distinct accelerator-specific advantages: TPUs for embedding tables, IPUs for compact compute-stacks, and GPUs for energy-efficient execution of large-capacity models (Section 3.4). ? We propose MP-Rec -a dynamic representationhardware co-design technique for deep learning recommendation inference. MP-Rec mitigates the performance and accuracy degradations from static representation-hardware mappings. We augment MP-Rec with a two-tier cache design (MP-Cache) to exploit unique caching opportunities found in compute-based representations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Background: Embedding Representations</head><p>Embeddings are a performance-critical component of neural recommendation models. In order to be processed by recommendation models, sparse feature IDs must first be transformed into dense embedding vectors. This transformation process -embedding access -can be realized through different embedding representations.</p><p>We start with two distinct classes of embedding representations: storage and generation. While storing learned embedding vectors as tables is a widely adopted approach, it introduces significant memory capacity and bandwidth requirements <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b12">13,</ref><ref type="bibr" target="#b18">19,</ref><ref type="bibr" target="#b29">30,</ref><ref type="bibr" target="#b33">34,</ref><ref type="bibr" target="#b35">36,</ref><ref type="bibr" target="#b48">49]</ref> (Section 2.1). On the other hand, generating embeddings with computeheavy encoder-decoder stacks trades off these memory system requirements with compute demand (Section 2.2). To leverage these contrasting qualities, we introduce new embedding representations -select and hybrid -that leverage complementary system resources to generate embeddings from table and DHE representations (Section 2.3). While the table representation is used by many state-ofthe-art neural recommender systems for its simplicity and relatively high accuracy, it has significant memory bandwidth and capacity requirements.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.">Generating Embeddings: DHE</head><p>Alternatively, embedding vectors can be dynamically generated from compute stacks. Examples include Tensor Train Compression (TT-Rec) <ref type="bibr" target="#b53">[54]</ref> and Deep Hash Embedding (DHE) <ref type="bibr" target="#b28">[29]</ref>. In this work, we focus on DHE (Figure <ref type="figure" target="#fig_1">2</ref> To learn valuable correlations from the ever-increasing data volume, the number of entries per embedding table have bloated to millions, leading to aggregate memory capacity requirements in the terabytes <ref type="bibr" target="#b35">[36,</ref><ref type="bibr" target="#b36">37]</ref>. In DHE, encoder-decoder stacks are first trained offline. During inference, embedding vectors are dynamically generated running sparse IDs through trained DHE stacks. Encoder hash functions and decoder MLPs contribute higher FLOPs, shifting the system bottleneck from memory capacity to computation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3.">Novel Representations: Select &amp; Hybrid</head><p>In Figure <ref type="figure" target="#fig_1">2</ref>(c), we present the proposed select embedding representation, where we select either embedding table or DHE representation at the feature-level (i.e., table-level) granularity. With the select embedding representation, recommendation model designers can make the aforementioned memory-compute tradeoffs for each sparse feature. In Figure <ref type="figure" target="#fig_1">2</ref>(d), we capitalize upon the dichotomy of embedding tables and DHE by proposing a table-compute hybrid representation. In this proposed hybrid representation, sparse IDs are used to both access embedding tables and dynamically generate embedding vectors. The resulting embeddings from both mechanisms are then concatenated. The embedding tables and decoder MLP stacks are trained together.</p><p>Our newly proposed hybrid representation is based on two key observations. First, embeddings learned from tables and DHE have different semantics. Generated embeddings from DHE can achieve higher model quality for some CTR predictions tasks <ref type="bibr" target="#b27">[28,</ref><ref type="bibr" target="#b41">42,</ref><ref type="bibr" target="#b45">46]</ref>, as demonstrated in Section 3.1. Second, embedding tables and DHE compute stacks stress independent system resources. The hybrid representation is unique in its ability to fully utilize both memory and compute resources of an underlying system for higher recommendation quality.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Design Space Exploration for Sparse Feature Representation</head><p>In this section, we characterize embedding table, DHE, select, and hybrid representations across the important dimensions of model accuracy (Section 3.1), model capacity (Section 3.2), execution latency (Section 3.3), and accelerator compatibility (Section 3.4). We show that this previously unexplored design space offers not only noticeable accuracy improvements but also hardware-specific optimization opportunities. Figure <ref type="figure" target="#fig_2">3</ref> provides an overview for the design space trade-offs of the four embedding representations along model accuracy (y-axis), capacity, and FLOPs (x-axis for (a) and (b), respectively).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Accuracy: Tuning DHE Parameters</head><p>Hybrid representation (violet points) configurations, which use both table and compute stacks, achieve the highest accuracies. Figure <ref type="figure" target="#fig_2">3</ref> illustrates that the most accurate hybrid representation configurations achieve 0.19% and 0.22% accuracy improvements over the embedding table baselines on Kaggle and Terabyte, respectively. Note that for many recommendation use cases, accuracy improvements &gt; 0.1% are considered significant. <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b47">48]</ref>.</p><p>We tune the hyperparameters of a DHE stack to realize these improved accuracies. Each point in Figure <ref type="figure" target="#fig_2">3</ref> corresponds to a model with unique hyperparameters. For embedding tables, we vary embedding dimension. For DHE stacks, we vary the number of parallel encoder hash functions k, the decoder MLP width d NN , and decoder MLP height h. We also vary the shape of FC layers for each decoder MLP stack.</p><p>Figure <ref type="figure" target="#fig_3">4</ref> depicts the compression ratio -relative to a 12.59 GB embedding table baseline model -(x-axis) and model accuracy (y-axis) for different DHE configurations. The color of each point denotes the number of hash functions used (k). We see that, as k increases from 2 to 2048 (i.e., color progression from red to black), model accuracy increases. Thus, k is an important factor in determining achievable model accuracy. In contrast, for a given k, varying the decoder MLP size and shape had a relatively insignifcant effect on model accuracy. This can be observed from how points with the same color (i.e., same k different (d NN , h)) have relatively similar model accuracies. A similar trend is observed for select and hybrid representations as well.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Capacity: Enabling Memory-Constrained Neural Recommendation</head><p>In Figure <ref type="figure" target="#fig_2">3</ref> (a), we observe that DHE configurations (red points) have model capacities 10 ? 1000? smaller than gigabyte-scale baseline embedding table configurations (blue points). Through DHE, we are able to construct a recommendation model that is 334? smaller in model capacity than the MLPerf baseline which uses embedding tables -without any accuracy degradation (i.e., horizontal dotted MLPerf accuracy baseline) <ref type="bibr" target="#b41">[42,</ref><ref type="bibr" target="#b45">46]</ref>. DHE accomplishes this by having a shared set of encoder-decoder parameters for generating embeddings as opposed to storing user-and item-specific embeddings. With these compression ratios, recommendation models can be compressed by orders of magnitude, from GBs to MBs, and deployed on a wider range of hardware platforms and use-cases.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Latency: Operator Breakdowns</head><p>While compute-based representations, such as DHE and hybrid, can improve model accuracies, the encoderdecoder stacks introduce FLOPs that lead to execution slowdowns. Figure <ref type="figure" target="#fig_13">3 (b)</ref> shows how models that use DHE and hybrid have 10 ? 100? more FLOPs than those relying on embedding tables.</p><p>Figure <ref type="figure" target="#fig_4">5</ref> shows operator breakdown of different representations for CPUs and GPUs. For DHE, we see 10.5? and 4.7? slowdown on CPUs and GPUs, respectively. DHE suffers less slowdown on GPUs because its encoder stack is composed of parallel hashing of k encoder hash functions. When k ? O(1000), GPU outshines CPU for such massively parallel operations. For select, we see only 2.1? and 1.5? slowdown on CPUs and GPUs, respectively. For this select representation, only the 3 largest embedding tables are replaced with DHE stacks. The rest of the sparse features use table representation, leading to faster execution. For hybrid, we observe 11.2? and 5.4? slowdown on CPUs and GPUs, respectively. Hybrid results in longest latencies because both embedding tables and DHE stacks are executed to generate highly accurate embedding vectors.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4.">Accelerator Compatibility: IPU, TPU Evaluation</head><p>Next, we explore accelerator-level synergies using real modern AI accelerators: Graphcore IPUs and Google TPUs <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b21">22,</ref><ref type="bibr" target="#b24">25,</ref><ref type="bibr" target="#b25">26,</ref><ref type="bibr" target="#b26">27]</ref>. CPU and GPU system specifications are detailed in Section 5.1. We evaluate TPUv3 at core-, chip-, and board-level configurations. For chip-and board-level configurations, we employ data-parallelism for increased throughput. We evaluate Graphcore GC200 IPU at chip-, board-, and pod-level configurations (Figure <ref type="figure" target="#fig_5">6</ref>). For a single IPU chip, table and hybrid configurations require backup Streaming Memory (i.e., DRAM) since the model does not fit within the 900 MB on-chip SRAM. For an IPU-M2000 board, we pipeline the model across the SRAM of the four chips. For IPU-POD16, we employ data-parallelism.</p><p>Figure <ref type="figure" target="#fig_6">7</ref> quantifies the design space trade-offs with four key observations:</p><p>O1: For embedding tables, TPUs achieve highest speedup due to their custom TPUEmbedding layers (Figure <ref type="figure" target="#fig_6">7 (a)</ref>). Each TPU core has access to 16 GB of HBM. TPUs utilize this HBM efficiently by: 1) sharding larger tables and replicating smaller tables across Tensor-Core HBMs and 2) pipelining embedding lookups with TensorCore computations. These optimizations form the basis of TPU custom TPUEmbedding layers.   While this SRAM has no default caching abilities, when all model parameters and activations fit within this SRAM budget, IPUs rely on virtually only compute and on-chip memory accesses, leading to significant speedups. Furthermore, the lack of off-chip DRAM access also contributes to high energy efficiency for DHE use cases. IPU-16 achieves 16.65? performance speedup over embedding table execution on CPUs. O3: GPUs offer higher energy efficiency for large embedding table-based models (Figure <ref type="figure" target="#fig_6">7</ref> (bottom). For large embedding table execution, GPU is most energyefficient. This is because while TPU shows 3.12?, 11.13? performance speedup for its chip-and board-level configurations, its single chip TDP is 1.8? higher than that of V100's. Additionally, the V100 is also more energy efficient than IPU for this specific use-case. This is because a single IPU chip's SRAM scratchpad cannot hold the entire model, leading to frequent off-chip DRAM access.</p><p>O4: No single hardware platform is optimal for all representations and optimization objectives, motivating the need for a dynamic representation-hardware co-design solution. Figure <ref type="figure" target="#fig_6">7</ref> shows that there is no one size fits all hardware solution across all possible embedding representations. While TPUs accelerate embedding lookups well, when the models are small enough to fit on-chip, IPUs perform better because of more efficient on-chip memory accesses. On the other hand, GPUs offer a competitive option from per-chip and ease-of-use standpoints (both TPUs and IPUs require lengthy one-time compilations).</p><p>The in-depth characterization based on real AI systems demonstrates the potential for performance improvement by considering heterogeneous representations and accelerators. Thus, to best exploit these algorithmic and system    by dynamically activating either Table, DHE, or hybrid execution paths on available hardware platforms.</p><p>To accelerate the encoder-decoder stack in the DHE and hybrid paths, we introduce MP-Cache encoder for the encoder stack and MP-Cache decoder for the decoder stack. MP-Cache encoder exploits the power law distribution of embedding access frequencies <ref type="bibr" target="#b12">[13]</ref>, whereas MP-Cache decoder considers value similarity of intermediate encoder stack results. Figures <ref type="figure" target="#fig_7">8</ref> and<ref type="figure" target="#fig_9">9</ref> provide the design overviews for MP-Rec and MP-Cache, respectively. Next, we present the major components of MP-Rec in detail.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Offline HW-Specific Representation Generation</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>MP-Rec factors in representation-level insights and exploration constraints to generate representation-hardware configurations (Algorithm 1).</head><p>Heterogeneous Hardware Platforms. MP-Rec considers available hardware platforms and their memory capacities for embedding access. Traditionally, neural recommender systems are deployed exclusively on serverclass CPUs and GPUs due to their large memory requirements <ref type="bibr" target="#b14">[15,</ref><ref type="bibr" target="#b16">17,</ref><ref type="bibr" target="#b36">37]</ref>. However, with DHE's potential for compression, MP-Rec is able to target a wider range of hardware platforms. For MP-Rec, we consider each individual hardware component by their memory capacity budget and map representation(s) accordingly to maximize memory capacity utilization.</p><p>Representation Exploration. Algorithm 1 details the if ?r j,DHE that still fits on then if h i has ? one r j mapping in S then S ? (r j,DHE(compact) , h i )</p><formula xml:id="formula_0">14:</formula><p>end if 15: end for 16: train all representations r i found within S steps for finding optimal representation-hardware mappings S . For each hardware component h i , we first see if there exists a hybrid embedding representation r j,hybrid that is 1) under capacity budget, 2) has large # of encoder hash functions k and 3) has a decoder MLP (i.e., d NN , h) as small as reasonably possible. As discussed in Section 3, we want high k for better model accuracy and small decoder MLP to minimize memory footprint and FLOPs.</p><p>With a hybrid representation that provides high accuracy, MP-Rec then searches for an embedding table representation r j,table that can be later activated to handle latency-critical situations (i.e., tight SLA latency targets). If there is still capacity available on h i , we search for a DHE representation r j,DHE that has accuracy-latency trade-offs in-between the hybrid and table configurations. We then repeat this process for all available hardware platforms. On memory-constrained devices, we search for compact representation r j,DHE(compact) . Selected representations are then profiled against the expected workload at different query sizes.</p><p>With a set of representations on each hardware platform, we can selectively activate mappings during the online stage to maximize recommendation quality while hitting SLA targets and maintaining high throughput to the best of our abilities.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Online Dynamic Multi-Path Activation</head><p>During online phase, MP-Rec dynamically activates representation-hardware execution paths to handle incoming queries based on query-level information (Algorithm 2). Currently, scheduling is dependent on query sizes and SLA latency targets. In real-world production environments, incoming queries arrive at different sizes and have to be served within application-specific SLA latency targets. Based on prior works, recommendation workloads can have query sizes between 1 -4K and SLA latency targets from 1 -100s ms <ref type="bibr" target="#b12">[13]</ref>.</p><p>Maximizing Throughput of Correct Predictions. We activate representation execution paths based on incoming query size n and SLA latency target t SLA (Algorithm 2). If there exists a hybrid configuration that can finish a query of size n within t SLA without throughput degradation, that representation execution path is activated to achieve highest possible accuracy. If no hybrid execution path exists, we see if there is a DHE representation path for moderately improved accuracy. If n and t SLA are too strict, MP-Rec then defaults to activating the Table representation path to satisfy SLA conditions. Ultimately, MP-Rec dynamically activates the path of highest recommendation quality while ensuring table-level system throughput for different SLA conditions and varying query sizes, thus maximizing the throughput of correct predictions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">MP-Cache: Mitigating Latency of the Compute-Stack Path</head><p>While DHE and hybrid paths offer accuracy and capacity improvements, activating either path comes with significant latency overheads. In large-scale inference serving experiments (Section 6), we see that these latency degradations lead to higher tail latencies. In order to close this performance gap, we devise MP-Cache, a two-part caching optimization that exploits both access frequency and value similarity of embedding accesses (Figure <ref type="figure" target="#fig_9">9</ref>).</p><p>MP-Cache encoder : Exploiting Access Frequency. In recommendation workloads, the access counts of powerpower users and items make up a sizable portion of total accesses <ref type="bibr" target="#b49">[50]</ref>. We exploit this observation by caching pre-computed embeddings of such hot, frequently accessed IDs in a cache within the encoder stage. If we encounter a hot ID, we can directly look up the ID's precalculated embedding vector and skip the entire encoderdecoder stack.</p><p>MP-Cache decoder : Exploiting Value Similarity. If a sparse feature ID does not hit in MP-Cache encoder , it goes through the encoder stack to generate an intermediate dense vector. This dense vector then becomes an input to the decoder MLP stack. As mentioned in Section 3, this decoder MLP stack can be costly in terms of latency. To mitigate this latency overhead, we propose MP-Cache decoder to exploit value similarity between intermediate dense vectors. We profile the intermediate dense vectors generated from a recommendation workload's sparse IDs and construct N centroids that best represent the overall distribution of possible intermediate vectors. With these centroids, our compute becomes knearest neighbors (kNN) search between the target intermediate dense vector and N centroids. In implementation, if the vectors are normalized, finding the nearest centroid can be simplified to parallelizable dot product followed by an argmax function, thus providing speedup over computation-heavy MLP stacks. The number of centroids N is an adjustable parameter: larger N gives better approximations at the cost of more compute.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Methodology</head><p>We present the experimental methodology used for evaluating MP-Rec on a design space spanning hardware platforms (Section 5.1), recommendation use cases (Section 5.2), inference runtime characteristics (Section 5.3), and evaluation metrics (Section 5.4).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.">Hardware Systems</head><p>One of MP-Rec's core features is its ability to generate optimal representation-hardware mappings for heterogeneous systems with different memory capacities. To demonstrate this flexibility, we evaluate MP-Rec at three different configurations: 1. HW-1: Single CPU-GPU node with 32 GB CPU DRAM and 32 GB GPU HBM2. Unless otherwise specified, we evaluate this configuration in Section 6. 2. HW-2: Resource-constrained case-study with 1 GB CPU DRAM and 200 MB GPU HBM2. 3. HW-3: Custom-accelerator case-study with 32 GB CPU DRAM and board-, pod-level IPU platforms. CPU, GPU, and IPU performance data is collected on real commodity hardware platforms (Table <ref type="table" target="#tab_2">1</ref>). For query serving experiments involving IPU platforms (Section 6.3), we exclude model compilation overheads. We profile IPU platforms across all possible query configurations and use this profiled information to get estimated performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.">Datasets and Models</head><p>We evaluate MP-Rec with open source recommendation datasets Criteo Kaggle <ref type="bibr" target="#b27">[28]</ref> and Terabyte <ref type="bibr" target="#b45">[46]</ref> trained on Meta's Deep Learning Recommendation Model (DLRM) <ref type="bibr" target="#b37">[38]</ref>. The MLPerf baseline model for Terabyte is 5.8? larger than the baseline model for Kaggle (12.59 GB and 2.16 GB, respectively). For each of the embedding representations covered in Section 2, we replace the embedding tables of DLRM with our implementation of the target representation. The encoder-decoder stack implementation is based on <ref type="bibr" target="#b28">[29]</ref> and in PyTorch <ref type="bibr" target="#b40">[41]</ref>. Respective characterization baselines (i.e., accuracy, capacity, FLOPs) in Section 3 are from the default MLPerf DLRM-Kaggle and Terabyte configurations <ref type="bibr" target="#b41">[42]</ref>. For IPU query serving use-cases (Section 6.3), we reduce the embedding dimension of the Terabyte model's tables to fit the model onto IPU-POD16 (Table <ref type="table" target="#tab_2">1</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3.">Inference Runtime Characteristics</head><p>As shown in Figure <ref type="figure" target="#fig_7">8</ref>, MP-Rec dynamically serves inference queries across different runtime conditions:</p><p>Query Sizes and Distribution. Representation-HW mappings perform differently based on query sizes. Unless otherwise specified, we evaluate a generated query set of size 10K, following a lognormal distribution with an average query size of 128 <ref type="bibr" target="#b12">[13,</ref><ref type="bibr" target="#b13">14]</ref>.</p><p>SLA Latency Targets. Inference query requests have to be finished under application-specific SLA latency targets. For recommendation workloads, latency targets can range from 1 -100s milliseconds <ref type="bibr" target="#b12">[13]</ref>. We overview results for a strict SLA scenario of 10ms (found in ecommerce platforms such as <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b26">27,</ref><ref type="bibr" target="#b39">40,</ref><ref type="bibr" target="#b60">61,</ref><ref type="bibr" target="#b61">62]</ref>) then demonstrate MP-Rec benefits at targets up to 200ms.</p><p>High Throughput Inference. Inference engines ideally maintain high throughput. However, large query execution may lead to QPS degradations. Unless otherwise specified, we target 1000 QPS.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.4.">Evaluation Metrics</head><p>In addition to model quality in click-through rate prediction accuracy, and model capacity in bytes, we evaluate: ? throughput correct_predictions : Throughput of Correct Predictions. For production use-cases, we care about not only the quality of individual recommendations but also how efficiently the models can be served at-scale. We evaluate </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Evaluation Results and Analysis</head><p>In this section, we show how MP-Rec improves upon various static representation-hardware deployment choices -in both throughput of correct predictions and accuracy -by dynamically switching between representations on heterogeneous hardware. We first evaluate MP-Rec -with MP-Cache enabled -on the HW-1 design point. Then, we cover production systems evaluation and consider resource-constrained (HW-2) and customaccelerator (HW-3) case studies. Next, we perform sensitivity studies on both query size distributions and SLA latency targets and explore query-splitting across heterogeneous hardware as an additional optimization. After that, we explore MP-Rec's dynamic switching mechanism and MP-Cache's two-stage structure. Finally, we quantify how MP-Rec reduces SLA violations for constant throughput use-cases and present analytical scaling implications on large-scale training systems.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.1.">MP-Rec Performance Overview</head><p>MP-Rec achieves the highest model accuracy among all the embedding representations on both Kaggle and Terabyte datasets by using more accurate representations like DHE and hybrid (Section 6.2 -Insight 1). For Kaggle and Terabyte use-cases, MP-Rec conditionally improves achievable model accuracy by 0.19% and 0.22%, respectively (Table <ref type="table" target="#tab_4">2</ref>). Despite their accuracy benefits, DHE and hybrid execution paths exhibit long latencies from their orders of magnitude higher FLOPs. Thus, statically deploying these compute-based representations on fixed-hardware platforms leads to throughput degradations. MP-Rec avoids  these performance degradations by dynamically switching execution paths at the representation-and HW-level granularities (Section 6.2 -Insights 2, 3). Furthermore, MP-Cache reduces the latency of DHE and hybrid encoder-decoder stacks, making these representations more viable for activation (Section 6.2 -Insight 4). MP-Rec optimizes throughput correct_predictions by using these factors to improve accuracy while maintaining performance. MP-Rec improves throughput correct_predictions by 2.49? and 3.76? on Kaggle and Terabyte, respectively (Figure <ref type="figure" target="#fig_10">10</ref>). We further break down improvements in throughput correct_predictions in Figure <ref type="figure" target="#fig_14">11</ref>.</p><p>To achieve these benefits, MP-Rec stores multiple representation execution paths on each hardware platform. This leads to increased memory footprint compared to statically using a single representation (Table <ref type="table">3</ref>). We demonstrate MP-Rec's ability to target memory-constrained and accelerator-based design points in Table <ref type="table" target="#tab_6">4</ref> (Insight 5) and Figure <ref type="figure" target="#fig_15">12</ref> (Insight 6), respectively.</p><p>Production Use-Case Evaluation. We implement and evaluate MP-Rec using recommendation tasks in a production setting where our baseline is an internal table-based recommendation model. We either replace the embedding tables of this baseline model with DHE stacks or augment the tables for hybrid representations. First, we observe a noticeable model compression ratio when replacing embedding tables with DHE stacks. Next, hybrid configurations achieve 0.014% improvement in model  accuracy. Finally, utilizing DHE stacks introduces flops that incur a throughput degradation of 23.59%.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.2.">Evaluation Result Insights for MP-Rec</head><p>We begin by evaluating MP-Rec at HW-1 (as specified in Section 5.1) on both the Kaggle and Terabyte use-cases.</p><p>Evaluation is done on 10K queries with targets of 1000 QPS and 10ms SLA latency target. We highlight the key MP-Rec features from evaluation results: Insight 1: MP-Rec improves achievable recommendation quality by including carefully-tuned DHE and hybrid execution paths. During its online phase, MP-Rec dynamically activates DHE and hybrid paths when there is no expected latency-bounded throughput degradation. Thus, during the execution of an entire query set, MP-Rec conditionally matches higher model accuracies of DHE, hybrid representations (Table <ref type="table" target="#tab_4">2</ref>).</p><p>Insight 2: MP-Rec mitigates the throughput degradation of DHE and hybrid representations by conditionally activating more accurate representation(s). Throughput of table-only configurations on either CPUs or GPUs (blue) is higher than that of DHE-, hybrid-only configurations (crimson, violet) (Figure <ref type="figure" target="#fig_10">10</ref>). On Kaggle, using exclusively DHE or hybrid on GPUs for their accuracy benefits degrades throughput correct_predictions by 62.8% and 63.3%, respectively -compared to exclusively using embedding tables on CPUs. This throughput degradation comes from the increased FLOPs of DHE encoderdecoder stacks. So, even though DHE and hybrid execu-  tion paths offer higher prediction accuracies for each individual query, throughput correct_predictions still drops significantly from worse system performance. MP-Rec is able to recover these throughput correct_predictions degradations by selectively activating DHE, hybrid paths based on incoming query characteristics. Namely, MP-Rec schedules queries onto DHE-, hybrid-paths when there are no expected latency-bounded throughput degradations. Insight 3: MP-Rec improves the throughput performance of embedding table baselines by dynamically switching at the hardware platform granularity. Depending on use-case (i.e., base model, query statistics, and latency constraints), optimal execution paths vary by hardware platform for a particular representation. We demonstrate this with an additional baseline where CPU-GPU switching is enabled for table-only representation (gray bars in Figure <ref type="figure" target="#fig_10">10</ref>). For example, for Kaggle, purely switching at the CPU-GPU granularity achieves 18% performance improvement over CPU-only execution (Figure <ref type="figure" target="#fig_10">10</ref> (left)). However, for Terabyte, CPU-GPU switching does not enable further speedups since throughput of CPU execution, at best, matches that of GPU execution (Figure <ref type="figure" target="#fig_10">10</ref> (right)). In either case, enabling CPU-GPU switching has a lower-bound performance of optimal static deployment configuration. The reason behind this is that, for throughput, CPU execution is favored when queries are small and model complexity is relatively low (i.e., Kaggle base model). In these scenarios, overheads for GPU-based model execution (e.g., data loading) are less amortized.</p><p>Insight 4: MP-Cache increases throughput of correct predictions by decreasing the latency of highly ac-  curate representations, namely, DHE and hybrid. MP-Cache reduces the long latency of executing DHE, hybrid encoder-decoder stacks. This allows these compute-based representations to be viable for more query serving opportunities. Without MP-Cache, MP-Rec will only switch onto long-latency execution paths when there are no expected throughput degradations. Thus, for scenarios like large queries -where table-based execution would have completed under strict latency targets -MP-Cache enables switching onto DHE and/or hybrid execution paths. MP-Cache enables MP-Rec to improve system throughput and quality of recommendations served hand in hand. We provide further breakdown in Figure <ref type="figure" target="#fig_14">11</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.3.">Additional Heterogeneous Hardware Case Studies</head><p>In addition to evaluating MP-Rec on a large-capacity CPU-GPU system (i.e., HW-1), we expand our analysis to memory-constrained and accelerator-enabled casestudies.</p><p>Insight 5: MP-Rec finds optimal representation-HW mappings on constrained HW design points. We introduce design point HW-2 with constrained memory capacities (Section 5.1). As seen in Table <ref type="table" target="#tab_6">4</ref>, MP-Rec utilizes HW-2's memory capacity budgets with both DHE and TBL execution paths. By doing so, MP-Rec matches optimal accuracy given by DHE (Table <ref type="table" target="#tab_4">2</ref>)) and, at the  same time, achieves higher throughput of CPU embedding table execution (Table <ref type="table" target="#tab_6">4</ref> (left)).</p><p>Insight 6: IPUs can offer potential speedups in heterogeneous hardware platforms -given sufficiently large multi-node configurations and further software support. We evaluate an IPU-POD16 in our query serving experiment for both Kaggle and Terabyte (Figure <ref type="figure" target="#fig_15">12</ref>). We choose a pod-level configuration -as opposed to chip-and board-level configurations -since the Terabyte model is on the order of 10 GBs and accessing backup DRAM significantly hampers performance (see Section 3.4). IPU's ability to first fit entire DHE stacks on-chip then leverage data parallelism across multiple nodes enables large potential speedups on DHE and MP-Rec configurations. Table and hybrid configurations for Terabyte offer less speedup from the lack of data parallelism -each of the 16 IPU nodes dedicate its on-chip SRAM to storing unique shards of the model's parameters. We assume that the IPU is able to handle incoming queries of different sizes. In practice, changes in input shapes require lengthy (i.e., ? 30 minutes) re-compilations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.4.">Sensitivity Studies</head><p>Figure <ref type="figure" target="#fig_16">13</ref> showcases sensitivity studies over the dimensions of query size and SLA latency target. When varying average query size, we maintain a log-normal distribution for the query set (Section 5.3). Figure <ref type="figure" target="#fig_16">13</ref> (left) shows that both table CPU-GPU switching and MP-Rec show more improvement as the average query size increases. This is because the larger the query sizes, the more GPU/accelerator-offloading opportunities. Figure <ref type="figure" target="#fig_16">13</ref> (right) shows that, as SLA latency target increases, speedup reduces. This is because when latency target budget for query execution is so high (e.g., 200ms), even CPU-embedding table baseline can achieve high throughput execution.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.5.">Additional Optimization: Query Splitting</head><p>In order to better exploit heterogeneous hardware platforms, one potential optimization that can be applied on top of this study is query splitting. In theory, splitting a query for a given representation across both CPU and GPU can better utilize available resources and result in lower query load per hardware platform. In Figure <ref type="figure" target="#fig_17">14</ref>, we explore this by evenly splitting each query for a representation across both CPU and GPU. We observe that for embedding table configurations, query splitting is better than the CPU-GPU switching baseline. However, for MP-Rec, where compute-intensive representations like DHE and hybrid are available, even query splitting is detrimental to performance. This is because for embedding table execution, query splitting results in smaller queries that CPUs are effective for. However, query splitting for compute-intensive representations forces CPU execution, which is extremely ineffective (see Figure <ref type="figure" target="#fig_4">5</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.6.">Dynamic Multi-Path Activation</head><p>During the offline phase, each representation -Table, DHE, and hybrid -are mapped onto both CPU and/or GPU platforms. During the online phase, MP-Rec switches between representation execution paths given different input query sizes and application SLA target. For example, when input query is small (i.e., O(10)), we activate table-CPU execution for tight SLA targets and hybrid/DHE-GPU for medium SLA targets. For large query sizes (i.e., O(100)), table execution swaps onto GPU platform and the hybrid/DHE-GPU path is only activated if there are no expected throughput degradations. For each query, we activate the path of highest recommendation accuracy if it won't lead to performance degradation. Otherwise, we activate embedding table paths to ensure throughput and latency targets are met.</p><p>Figure <ref type="figure" target="#fig_18">15</ref> presents representation switching breakdown of table (CPU-GPU switching) and MP-Rec for both Kaggle and Terabyte. For Kaggle, we see that TBL (CPU) is always present since the execution time of small queries on Kaggle is too fast for the GPU offloading overhead be effectively amortized. For Terabyte, we see that TBL (GPU) is always preferable compared to TBL (CPU). This contributes to the equal performance of the TBL(GPU) and TBL(CPU-GPU) configurations, as what Figure <ref type="figure" target="#fig_10">10</ref> shows.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.7.">MP-Cache Result Analysis</head><p>Figure <ref type="figure" target="#fig_19">16</ref> shows how MP-Cache exploits (a) access frequency and (b) value similarity. The access frequency opportunities come from power law distribution of recommendation workloads. Figure <ref type="figure" target="#fig_19">16 (</ref>  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.8.">Reducing SLA Violations</head><p>MP-Rec reduces SLA target latency violations at constant throughput scenario (Figure <ref type="figure" target="#fig_20">17</ref>). When we statically deploy a representation, SLA latency violations occur at constant throughput when the input query is too large to finish under latency target. At an SLA latency target of 10 ms, statically deploying embedding tables on CPUs will lead to 30.73% of queries violating SLA latency target. Without MP-Rec or MP-Cache, statically deploying DHE or hybrid at 400 QPS will lead to 100% SLA violation.</p><p>With MP-Rec, we dynamically switch to representations that help us meet target SLA latency target. Figure <ref type="figure" target="#fig_20">17</ref> shows that across a whole range of SLA latency targets, MP-Rec reduces percentage of queries violating SLA latency targets. Compared to embedding table-CPU execution, MP-Rec observes 3.14% SLA latency violations (27.59% improvement) at 10ms latency target.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.9.">Scaling Analysis for Multi-Node Systems</head><p>Production recommendation models have terabyte-scale embedding tables, leading to the requirement of multinode hardware systems for both inference and training tasks <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b15">16,</ref><ref type="bibr" target="#b19">20,</ref><ref type="bibr" target="#b35">36,</ref><ref type="bibr" target="#b56">57,</ref><ref type="bibr" target="#b58">59]</ref>. To distribute such model across multiple nodes, embedding tables have to be sharded. During execution, communication collectives, such as All-to-All and AllReduce, are used to gather embedding-table lookup and data-parallel MLP results from different compute nodes. These collectives contribute to inter-node communication time <ref type="bibr" target="#b42">[43]</ref>, which can be costly from a system performance perspective. For large-scale recommendation training systems, such as ZionEX <ref type="bibr" target="#b36">[37]</ref>, exposed inter-node communication contributes to nearly 40% of the total model training time.</p><p>DHE can reduce the memory capacity requirement of the Terabyte benchmark by 334? (Figure <ref type="figure" target="#fig_3">4</ref>). With this compression, MP-Rec can potentially enable larger-size, industry-scale recommendation models for single-node systems, mitigating the exposed multi-node communication time at the cost of additional DHE-specific computation (Figure <ref type="figure" target="#fig_21">18</ref>). Based on our analytical model, for a 128-GPU ZionEX system, the total execution time can be reduced by 36% by replacing embedding tables with DHE, thus eliminating inter-node communication <ref type="bibr" target="#b36">[37]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.">Related Work</head><p>State-of-the-art neural recommender systems use embedding tables for their embedding representation, resulting in substantial memory capacity requirements <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b37">38,</ref><ref type="bibr" target="#b59">60,</ref><ref type="bibr" target="#b60">61,</ref><ref type="bibr" target="#b61">62]</ref>. Recently proposed compute-based representations reduce these memory capacity constraints at the cost of increasing FLOPs <ref type="bibr" target="#b28">[29,</ref><ref type="bibr" target="#b53">54]</ref>. These works have explored point solutions for different embedding representations. In this paper, we explore the interaction between embedding representations and other algorithmic and systems metrics of interest.</p><p>Earlier works overview breakdown of recommendation workloads across server-class CPU and GPU systems <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b12">13,</ref><ref type="bibr" target="#b14">15,</ref><ref type="bibr" target="#b18">19,</ref><ref type="bibr" target="#b57">58]</ref>. Additionally, given the importance of neural recommendation, there are many proposals for custom hardware accelerators <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b13">14,</ref><ref type="bibr" target="#b20">21,</ref><ref type="bibr" target="#b22">23,</ref><ref type="bibr" target="#b31">32,</ref><ref type="bibr" target="#b34">35]</ref>. In particular, many of these accelerators are DRAM- <ref type="bibr" target="#b29">[30,</ref><ref type="bibr" target="#b33">34,</ref><ref type="bibr" target="#b38">39]</ref> and SSD-level <ref type="bibr" target="#b46">[47,</ref><ref type="bibr" target="#b48">49]</ref> modifications aimed at improving embedding access time.</p><p>Other works that take advantage of heterogeneous hardware platforms include FleetRec <ref type="bibr" target="#b23">[24]</ref> and Hercules <ref type="bibr" target="#b30">[31]</ref>. FleetRec demonstrates that, for a fixed recommendation model, operator-level splitting is faster than CPUonly execution. In contrast, we switch between heterogeneous embedding representations, resulting in accuracy improvements that do not exist in fixed-model approaches. If we apply both operator-level (i.e., FleetRec) and representation-level (i.e., MP-Rec) parallelisms for a recommendation workload, we could see improvements stemming from improved embedding table execution efficiency. For appropriate query sizes, hybrid execution will be activated more often due to speedups in its table stack. However, speedup would ultimately be limited as hybrid execution bottlenecks lie in its DHE stacks (Figure <ref type="figure" target="#fig_4">5</ref>). On the other hand, Hercules identifies optimal heterogeneous server architectures and system resource mappings -especially in the context of diurnal cycles of recommendation workloads.</p><p>Other embedding table optimizations usually fall under one of two objectives: compression or faster access. TT-Rec <ref type="bibr" target="#b53">[54]</ref>, ROBE <ref type="bibr" target="#b7">[8]</ref>, and <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b44">45]</ref> leverage matrix factorization and other related weight sharing/reduction techniques to reduce overall memory footprint of embedding tables. Other works such as cDLRM <ref type="bibr" target="#b4">[5]</ref>, Bandana <ref type="bibr" target="#b8">[9]</ref>, RecShard <ref type="bibr" target="#b42">[43]</ref>, DreamShard <ref type="bibr" target="#b55">[56]</ref>, Au-toShard <ref type="bibr" target="#b54">[55]</ref>, FlexShard <ref type="bibr" target="#b43">[44]</ref>, and Kraken <ref type="bibr" target="#b51">[52]</ref> leverage access frequency information to make embedding access more hardware-efficient.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8.">Conclusion</head><p>We explore alternative embedding representations for deep learning recommendation, where embedding vectors can be generated from embedding table lookups and/or encoder-decoder stacks. This is fundamentally different from prior work, where user-item relationships are learned using a single feature representation. To maximize throughput of correct predictions while meeting tail latency requirements, we propose a new representationsystem co-design approach for real-time inference, MP-Rec. Depending on memory capacities of AI inference systems, MP-Rec selects unique embedding representations, forming multiple embedding execution paths for recommendation inference. At runtime, depending on input query sizes and application-dependent performance targets, MP-Rec activates embedding path(s) to jointly maximize model quality and throughput. Using the opensource MLPerf-DLRM with Kaggle and Terabyte datasets, MP-Rec achieves higher throughput of correct predictions and model quality at the same time while meeting application-specific tail latency requirements.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: Compared to prior work, MP-Rec explores the embedding access design space through different embedding representations.</figDesc><graphic url="image-1.png" coords="1,315.00,215.98,225.00,94.37" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: (a) Table representation stores learned embeddings while (b) DHE dynamically generates embeddings via encoder-decoder stacks. We introduce (c) select representation that selects either (a) or (b) at table-level granularity and (d) hybrid representation that leverages both (a) and (b) for highly accurate embeddings.2.1. Storing Embeddings: Embedding Tables Figure 2 (a) depicts the embedding access mechanism of a typical neural recommendation model. Sparse feature lookup IDs are converted into multi-hot encoded vectors, which are then used as indices into embedding tables.While the table representation is used by many state-ofthe-art neural recommender systems for its simplicity and relatively high accuracy, it has significant memory bandwidth and capacity requirements.</figDesc><graphic url="image-2.png" coords="3,72.00,72.00,224.99,230.22" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: DHE representation saves memory capacity, hybrid representation enables optimal accuracies, and table representation has faster latency from less FLOPs. Evaluation is on the Criteo Kaggle data set.</figDesc><graphic url="image-4.png" coords="5,72.00,239.31,225.00,122.87" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 4 :</head><label>4</label><figDesc>Figure 4: DHE compute stacks can be tuned to improve either model accuracy or compression ratio.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 5 :</head><label>5</label><figDesc>Figure 5: Operator breakdown of Table, DHE, select, and hybrid execution on CPUs and GPUs. Hybrid shows worst performance in terms of latency while select offers a compromise between Table and DHE. O2: For DHE stacks, IPUs perform well primarily when all model parameters and inputs fit into the IPU scratchpad memory (Figure 7 (b)). Each Graphcore IPU has access to 900 MB of scratchpad SRAM.While this SRAM has no default caching abilities, when all model parameters and activations fit within this SRAM budget, IPUs rely on virtually only compute and on-chip memory accesses, leading to significant speedups. Furthermore, the lack of off-chip DRAM access also contributes to high energy efficiency for DHE use cases. IPU-16 achieves 16.65? performance speedup over embedding table execution on CPUs.</figDesc><graphic url="image-5.png" coords="5,72.00,397.67,224.99,121.46" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 6 :</head><label>6</label><figDesc>Figure 6: Hybrid representation deployment strategies for pod-scale IPUs. Single chip execution requires offloading three largest embedding tables (green, blue, and gray shade) to DRAM while pod-level execution duplicates board-level parallelism strategy four times for data parallelism.</figDesc><graphic url="image-6.png" coords="5,315.00,239.31,225.00,77.54" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 7 :</head><label>7</label><figDesc>Figure 7: Table, DHE, and hybrid embedding representations evaluated across different custom accelerators. TPUv3s see speedups from TPU-optimized TPUEmbeddings while Graphcore IPUs offer optimal performance when the model and activations fit within its 900 MB SRAM per-chip scratchpad. CPU: Broadwell Xeon; GPU: V100; IPU-1: 1-chip GC200; TPU-1: 1-core TPUv3 (TPU has 2 cores/chip).</figDesc><graphic url="image-8.png" coords="6,72.00,303.36,467.99,230.12" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure 8 :</head><label>8</label><figDesc>Figure 8: MP-Rec has two stages: offline mapping exploration and online query scheduler. In the offline phase, MP-Rec considers algorithmic and systems-level exploration constraints to generate optimal representation-hardware configurations. In the online phase, MP-Rec dynamically schedules queries onto available representation-HW execution paths based on query-level information to maximize for amount of high quality recommendation.level trade-offs, representation and hardware pairing must be a dynamic rather than static decision.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head></head><label></label><figDesc>4. MP-Rec: Representation-Hardware Co-Design Built upon the real system characterization results, we propose a Multi-Path embedding representation codesign technique for Recommendation inference, MP-Rec. MP-Rec maximizes throughput of correct predictions in two stages: Offline Stage [Section 4.1]. MP-Rec determines which embedding representation(s) will be used and their corresponding hardware mapping strategies (Algorithm 1). Embedding representation and mapping decisions are based on system memory capacities. Online Stage [Section 4.2]. MP-Rec considers service-level agreements (SLA), such as model accuracy and tail latency targets, of the application and runtime factors, such as input query sizes (Algorithm 2). MP-Rec produces embedding vectors from sparse features</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>Figure 9 :</head><label>9</label><figDesc>Figure 9: MP-Cache is comprised of two cascading stages. MP-Cache encoder exploits access frequency of sparse IDs while MP-Cache decoder exploits value similarity between intermediate results.</figDesc><graphic url="image-9.png" coords="7,72.00,72.00,225.00,246.14" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head>10 :S</head><label>10</label><figDesc>? (r j,DHE , h i )</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_12"><head>Figure 10 :</head><label>10</label><figDesc>Figure 10: Throughput of Correct Predictions for serving 10K queries in Kaggle and Terabyte use-cases, respectively. With MP-Cache, MP-Rec improves upon throughput of correct predictions by activating high accuracy execution paths. Statically deploying DHE, hybrid representations leads to throughput degradations compared to executing embedding tables on CPUs and/or GPUs.</figDesc><graphic url="image-10.png" coords="10,72.00,72.00,467.99,117.60" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_13"><head>Table 3 :</head><label>3</label><figDesc>Memory footprints for HW-1 on Kaggle and Terabyte, respectively. MP-Rec incurs greater memory footprint than static deployment choices since it stores multiple representations on each HW platform.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_14"><head>Figure 11 :</head><label>11</label><figDesc>Figure 11: Changes in raw throughput (hatched white bars) and throughput of correct predictions (colored bars) for Kaggle and Terabyte use-cases, respectively.</figDesc><graphic url="image-11.png" coords="11,72.00,72.00,225.00,97.72" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_15"><head>Figure 12 :</head><label>12</label><figDesc>Figure 12: IPU Query Serving: If model fits on IPUs and IPUs are able to handle dynamic query sizes, there are potential speedups across different representations.</figDesc><graphic url="image-12.png" coords="11,315.00,72.00,225.00,93.83" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_16"><head>Figure 13 :</head><label>13</label><figDesc>Figure 13: Sensitivity studies for query size and SLA latency target. Default settings assume average query size 128 and SLA target 10 ms. Results shown for Terabyte use-case.</figDesc><graphic url="image-13.png" coords="11,315.00,213.44,224.99,120.69" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_17"><head>Figure 14 :</head><label>14</label><figDesc>Figure 14: When incorporating DHE and hybrid, query splitting is sub-optimal without careful tuning of split ratios. Baseline is embedding table-CPU execution.</figDesc><graphic url="image-14.png" coords="12,72.00,72.00,224.99,112.65" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_18"><head>Figure 15 :</head><label>15</label><figDesc>Figure 15: Switching Breakdown. MP-Rec enables execution of compute-based representations.</figDesc><graphic url="image-15.png" coords="12,72.00,231.95,225.00,103.33" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_19"><head>Figure 16 :</head><label>16</label><figDesc>Figure 16: In recommendation workloads, ID access frequencies follow power law distributions. With both MP-Cache encoder and MP-Cache decoder , MP-Cache closes the performance gap between encoder-decoder stacks and embedding tables.</figDesc><graphic url="image-16.png" coords="12,315.00,72.00,225.00,89.96" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_20"><head>Figure 17 :</head><label>17</label><figDesc>Figure 17: At constant throughput, MP-Rec reduces SLA latency target violations by dynamically switching to suitable representation-hardware execution paths.</figDesc><graphic url="image-17.png" coords="13,72.00,72.00,225.00,113.93" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_21"><head>Figure 18 :</head><label>18</label><figDesc>Figure 18: Through model compression, DHE allows multi-node recommendation models to be run on a single node, reducing communication overheads.</figDesc><graphic url="image-18.png" coords="13,320.63,72.00,213.75,145.45" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head></head><label></label><figDesc>if ?r j,table that still fits on h i then 7:S ? (r j,table , h i )</figDesc><table><row><cell>3:</cell><cell>if ?r j,hybrid that fits on h i then</cell></row><row><cell>4:</cell><cell>S ? (r j,hybrid , h i )</cell></row><row><cell>5:</cell><cell>end if</cell></row><row><cell>6:</cell><cell></cell></row><row><cell>8:</cell><cell>end if</cell></row><row><cell>9:</cell><cell></cell></row></table><note><p><p>Algorithm 1 MP-Rec Offline Stage</p>Input: Embedding Representation Space R = {r i }, Hardware Platforms H = {h i } Output: Optimal representation-hardware mapping strategies S ? {(r i , h i )} | r i is accuracy optimal. 1: S ? {} 2: for all hardware h i ? H do</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 1 : Systems Configurations.</head><label>1</label><figDesc>Representation-hardware mappings S ? {(r i , h i )}, Input Query q Output: Selected query execution path (r i , h i ) 1: n,t SLA ? query size, SLA latency target 2: if (r j,hybrid , h i ) can process query size n under t SLA then 3: return (r j,hybrid , h i ) 4: else if (r j,DHE , h i ) can process query size n under t SLA then</figDesc><table><row><cell>Machines</cell><cell cols="4">Intel Broadwell CPU NVIDIA V100 GPU Graphcore IPU-M2000 (4 IPUs) Graphcore IPU-POD16 (16 IPUs)</cell></row><row><cell>Frequency</cell><cell>2.2 GHz</cell><cell>1.2 GHz</cell><cell>1.35 GHz</cell><cell>1.35 GHz</cell></row><row><cell>Cores</cell><cell>12</cell><cell>5120</cell><cell>5888</cell><cell>23552</cell></row><row><cell>Cache Sizes</cell><cell>0.3-3-30 MB</cell><cell>3 MB</cell><cell>3.6 GB</cell><cell>14.4 GB</cell></row><row><cell>DRAM Capacity</cell><cell>264 GB</cell><cell>32 GB HBM2</cell><cell>256 GB</cell><cell>1024 GB</cell></row><row><cell>DRAM Bandwidth</cell><cell>76.8 GB/s</cell><cell>900 GB/s</cell><cell>20 GB/s</cell><cell>80 GB/s</cell></row><row><cell>TDP</cell><cell>105 W</cell><cell>250 W</cell><cell>600 W</cell><cell>2400 W</cell></row><row><cell cols="2">Algorithm 2 MP-Rec Online Stage</cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="2">Input: 5: return (r j,DHE , h i )</cell><cell></cell><cell></cell><cell></cell></row><row><cell>6: else</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>7:</cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note><p><p>return (r j,table , h i ) 8: end if</p>Varying Query Sizes and SLA Tail Latency Targets.</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head></head><label></label><figDesc>Query Size ? Model Accuracy? SLA Latency Violations. Meeting SLA is crucial for recommendation use cases. Thus, we also evaluate the effectiveness of MP-Rec in reducing SLA violations.</figDesc><table><row><cell cols="3">Correct Samples Second</cell><cell cols="3">with:</cell></row><row><cell>Queries Second</cell><cell>?</cell><cell cols="2">Samples Query</cell><cell>?</cell><cell>Correct Samples Samples</cell></row><row><cell cols="2">= QPS ?</cell><cell></cell><cell></cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 2 : Achievable model accuracies of optimal representation-hardware mappings for Kaggle and Ter- abyte, respectively. By using DHE or Hybrid, MP-Rec in- creases achievable model accuracies over table base- lines.</head><label>2</label><figDesc></figDesc><table><row><cell></cell><cell>Table (Baseline)</cell><cell>DHE</cell><cell>Hybrid MP-Rec</cell></row><row><cell>Kaggle</cell><cell>78.79%</cell><cell cols="2">78.94% 78.98% 78.98%</cell></row><row><cell>Terabyte</cell><cell>80.81%</cell><cell cols="2">80.99% 81.03% 81.03%</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 4 : Achievable model accuracy, normalized through- put, and memory footprint for design point HW-2.</head><label>4</label><figDesc></figDesc><table><row><cell></cell><cell>Achievable</cell><cell>Normalized Throughput</cell><cell>Memory</cell></row><row><cell></cell><cell>Accuracy</cell><cell>of Correct Predictions</cell><cell>Capacity</cell></row><row><cell>TBL (CPU)</cell><cell>78.721%</cell><cell>1.00?</cell><cell>542 MB</cell></row><row><cell>DHE (GPU)</cell><cell>78.936%</cell><cell>0.43?</cell><cell>123 MB</cell></row><row><cell>MP-Rec</cell><cell>78.936%</cell><cell>2.26?</cell><cell>CPU: 665 MB GPU: 123 MB</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head></head><label></label><figDesc>a) depicts the access distribution for CriteoKaggle.  When we analyze the access counts of the largest sparse feature (Embedding table 2 comes with 10M entries and 3M total accesses), we confirm that hot row IDs have 10K+ access counts while others are barely accessed more than once, if at all. MP-Cache encoder statically exploits the access frequency locality pattern to speed up the encoder-decoder stack. With only 2KB dedicated to MP-Cache encoder , we see 1.57? performance improvement over using the entire encoder-decoder stack. With a 2MB cache, the performance improvement becomes 1.92?. To further close the latency gap between encoder-decoder stacks and embedding tables (? 5? difference), MP-Cache decoder converts the compute-heavy MLP in decoder stacks to kNN search. When all vectors are normalized, kNN search becomes parallelizable dot product, leading to further speedup. Figure16 (right)  shows that MP-Cache achieves comparable performance level for DHE as embedding table access.</figDesc><table /></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div><head n="9.">Acknowledgements</head><p>We would like to thank <rs type="person">Jay Li</rs> and <rs type="person">Sherman Wong</rs> for the countless discussions and invaluable feedback on recommendation model architecture design and experimentation across Meta's workloads. This collaboration not only helped refine representation designs but also enabled initial production evaluation.</p></div>
			</div>			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Artifact Appendix</head><p>A. <ref type="bibr">1. Abstract</ref> This artifact package includes CPU-and GPU-compatible PyTorch-based implementations of proposed deep learning recommendation architectures (i.e., embedding representations), range of relevant hyperparameters, and custom TPU-, IPU-compatible implementations. The base implementation is compatible with PyTorch-compliant CPUs and GPUs while TPU-, IPU-implementations require access to cloud-hosted TPUs/IPUs and their associated PyTorch branches (i.e., PyTorch/XLA and poptorch). Inference experiments and characterization require at least a single CPU/GPU node (TPU-IPU benchmarking scripts support single-and multi-node execution) while design space exploration involving accuracy evaluation is best executed with large-scale GPU clusters due to the large number of GPU training jobs. Open-sourcing these implementations of proposed embedding representations will allow other researchers to not only characterize and deploy these algorithmic innovations on their own choice of systems but also develop novel embedding processing techniques of their own. A.2. Artifact check-list (meta-information) We also provide a script to download Criteo Kaggle in dlrm_mprec/download_kaggle.sh.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.3.5.</head><p>Models Base DLRM and variations: DHE,hybrid, and select. See paper for more description and chracterization on these variations and dlrm_mprec/configurations.txt for sample commands for running each of these variations on both CPUs and GPUs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.4. Installation</head><p>Use pip, conda, or your choice of Python package manager to install requirements listed above in Software dependencies subsection (also in dlrm_mprec/requirements.txt).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.5. Evaluation and expected results</head><p>python dlrm_s_pytorch.py -mini-batch-size=2 -data-size=6 should be your first command to ensure within installation works. If the aforementioned command works, you should try running the other embedding representations with commands listed in: dlrm_mprec/configurations.txt. You should see profiled timing information.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.6. Experiment customization</head><p>Listed in dlrm_mprec/configurations.txt are DHE stack hyperparameters relevant to the characterization and discussion in the paper. We list ranges for number of encoder hash functions and decoder MLP shapes.</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Understanding training efficiency of deep learning recommendation models at scale</title>
		<author>
			<persName><forename type="first">B</forename><surname>Acun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Murphy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Nie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Hazelwood</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2021 IEEE International Symposium on High-Performance Computer Architecture (HPCA)</title>
		<meeting><address><addrLine>Los Alamitos, CA, USA</addrLine></address></meeting>
		<imprint>
			<publisher>IEEE Computer Society</publisher>
			<date type="published" when="2021-03">mar 2021</date>
			<biblScope unit="page" from="802" to="814" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">High-performance training by exploiting hotembeddings in recommendation systems</title>
		<author>
			<persName><forename type="first">Muhammad</forename><surname>Adnan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yassaman</forename><surname>Maboud</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Divya</forename><surname>Mahajan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Prashant</forename><forename type="middle">J</forename><surname>Nair</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 48th International Conference on Very Large Data Bases (VLDB)</title>
		<meeting>the 48th International Conference on Very Large Data Bases (VLDB)</meeting>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="volume">15</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title/>
		<author>
			<persName><forename type="first">Michael</forename><surname>Anderson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Benny</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stephen</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Summer</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jordan</forename><surname>Fix</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><surname>Gschwind</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aravind</forename><surname>Kalaiah</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Changkyu</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jaewon</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jason</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Haixin</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yinghai</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jack</forename><surname>Montgomery</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Arun</forename><surname>Moorthy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Satish</forename><surname>Nadathur</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sam</forename><surname>Naghshineh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Avinash</forename><surname>Nayak</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jongsoo</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chris</forename><surname>Petersen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Martin</forename><surname>Schatz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Narayanan</forename><surname>Sundaram</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bangsheng</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Peter</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Amy</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiecao</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hector</forename><surname>Yuen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ying</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aravind</forename><surname>Anbudurai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vandana</forename><surname>Balan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Harsha</forename><surname>Bojja</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Joe</forename><surname>Boyd</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Matthew</forename><surname>Breitbach</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Claudio</forename><surname>Caldato</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Anna</forename><surname>Calvo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Garret</forename><surname>Catron</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sneh</forename><surname>Chandwani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Panos</forename><surname>Christeas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Brad</forename><surname>Cottel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Brian</forename><surname>Coutinho</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Arun</forename><surname>Dalli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Abhishek</forename><surname>Dhanotia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Oniel</forename><surname>Duncan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Roman</forename><surname>Dzhabarov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Simon</forename><surname>Elmir</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chunli</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wenyin</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><surname>Fulthorp</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Adi</forename><surname>Gangidi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nick</forename><surname>Gibson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sean</forename><surname>Gordon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Beatriz</forename><forename type="middle">Padilla</forename><surname>Hernandez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Daniel</forename><surname>Ho</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yu-Cheng</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Olof</forename><surname>Johansson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shishir</forename><surname>Juluri</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shobhit</forename><surname>Kanaujia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Manali</forename><surname>Kesarkar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jonathan</forename><surname>Killinger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ben</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rohan</forename><surname>Kulkarni</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Meghan</forename><surname>Lele</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Huayu</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Huamin</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yueming</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Cynthia</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jerry</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bert</forename><surname>Maher</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chandra</forename><surname>Mallipedi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Seema</forename><surname>Mangla</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kiran</forename><surname>Kumar Matam</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jubin</forename><surname>Mehta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shobhit</forename><surname>Mehta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christopher</forename><surname>Mitchell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bharath</forename><surname>Muthiah ; Ruoxi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ming</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Whitney</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rui</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><surname>Zhu</surname></persName>
		</author>
		<editor>Nitin Nagarkatte, Ashwin Narasimha, Bernard Nguyen, Thiara Ortiz, Soumya Padmanabha, Deng Pan, Ashwin Poojary, Ye (Charlotte) Qi, Olivier Raginel, Dwarak Rajagopal, Tristan Rice, Craig Ross, Nadav Rotem, Scott Russ, Kushal Shah, Baohua Shan, Hao Shen, Pavan Shetty, Krish Skandakumaran, Kutta Srinivasan, Roshan Sumbaly, Michael Tauberg, Mor Tzur, Sidharth Verma, Hao Wang, Man Wang, Ben Wei, Alex Xia, Chenyu Xu, Martin Yang, Kai Zhang,</editor>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
	<note>Ajit Mathews, Lin Qiao, Misha Smelyanskiy, Bill Jia, and Vijay Rao. First-generation inference accelerator deployment at facebook</note>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">On the factory floor: Ml engineering for industrial-scale ads recommendation models</title>
		<author>
			<persName><forename type="first">Rohan</forename><surname>Anil</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sandra</forename><surname>Gadanho</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Da</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nijith</forename><surname>Jacob</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhuoshu</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dong</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Todd</forename><surname>Phillips</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Cristina</forename><surname>Pop</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kevin</forename><surname>Regan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gil</forename><forename type="middle">I</forename><surname>Shamir</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rakesh</forename><surname>Shivanna</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qiqi</forename><surname>Yan</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">CDLRM: Look Ahead Caching for Scalable Training of Recommendation Models</title>
		<author>
			<persName><forename type="first">Keshav</forename><surname>Balasubramanian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Abdulla</forename><surname>Alshabanah</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Joshua</forename><forename type="middle">D</forename><surname>Choe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Murali</forename><surname>Annavaram</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2021">2021</date>
			<publisher>Association for Computing Machinery</publisher>
			<biblScope unit="page" from="263" to="272" />
			<pubPlace>New York, NY, USA</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Wide &amp; deep learning for recommender systems</title>
		<author>
			<persName><forename type="first">Heng-Tze</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Levent</forename><surname>Koc</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jeremiah</forename><surname>Harmsen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tal</forename><surname>Shaked</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tushar</forename><surname>Chandra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hrishi</forename><surname>Aradhye</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Glen</forename><surname>Anderson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Greg</forename><surname>Corrado</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wei</forename><surname>Chai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mustafa</forename><surname>Ispir</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rohan</forename><surname>Anil</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zakaria</forename><surname>Haque</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lichan</forename><surname>Hong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vihan</forename><surname>Jain</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaobing</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hemal</forename><surname>Shah</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 1st Workshop on Deep Learning for Recommender Systems</title>
		<meeting>the 1st Workshop on Deep Learning for Recommender Systems<address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computing Machinery</publisher>
			<date type="published" when="2016">2016. 2016</date>
			<biblScope unit="page" from="7" to="10" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Maxim Naumov, Sam Naghshineh, and Mikhail Smelyanskiy. Low-precision hardware architectures meet recommendation model inference at scale</title>
		<author>
			<persName><forename type="first">Zhaoxia</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jongsoo</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ping</forename><surname>Tak</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Peter</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Haixin</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jie</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hector</forename><surname>Yuen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jianyu</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Daya</forename><surname>Khudia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaohan</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ellie</forename><surname>Wen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dhruv</forename><surname>Choudhary</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Raghuraman</forename><surname>Krishnamoorthi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Carole-Jean</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Satish</forename><surname>Nadathur</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Changkyu</forename><surname>Kim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Micro</title>
		<imprint>
			<biblScope unit="volume">41</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="93" to="100" />
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Random offset block embedding (robe) for compressed embedding tables in deep learning recommendation systems</title>
		<author>
			<persName><forename type="first">Aditya</forename><surname>Desai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Li</forename><surname>Chou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Anshumali</forename><surname>Shrivastava</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of Machine Learning and Systems</title>
		<editor>
			<persName><forename type="first">D</forename><surname>Marculescu</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Y</forename><surname>Chi</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">C</forename><surname>Wu</surname></persName>
		</editor>
		<meeting>Machine Learning and Systems</meeting>
		<imprint>
			<date type="published" when="2022">2022</date>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page" from="762" to="778" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">Using non-volatile memory for storing deep learning models</title>
		<author>
			<persName><forename type="first">Assaf</forename><surname>Eisenman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Maxim</forename><surname>Naumov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Darryl</forename><surname>Gardner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Misha</forename><surname>Smelyanskiy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sergey</forename><surname>Pupyrev</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kim</forename><surname>Hazelwood</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Asaf</forename><surname>Cidon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sachin</forename><surname>Katti</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018">2018</date>
			<pubPlace>Bandana</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">The graphcore second generation ipu</title>
		<author>
			<persName><forename type="first">Karl</forename><surname>Freund</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Patrick</forename><surname>Moorhead</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title/>
		<author>
			<persName><forename type="first">Amir</forename><surname>Gholami</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhewi</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sehoon</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><forename type="middle">W</forename><surname>Mahoney</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kurt</forename><surname>Keutzer</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2021">2021</date>
			<publisher>Ai and memory wall</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Mixed dimension embeddings with application to memory-efficient recommendation systems</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">A</forename><surname>Ginart</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Maxim</forename><surname>Naumov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dheevatsa</forename><surname>Mudigere</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiyan</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">James</forename><surname>Zou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2021 IEEE International Symposium on Information Theory (ISIT)</title>
		<imprint>
			<publisher>IEEE Press</publisher>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="2786" to="2791" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Deeprecsys: A system for optimizing endto-end at-scale neural recommendation inference</title>
		<author>
			<persName><forename type="first">Udit</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Samuel</forename><surname>Hsia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vikram</forename><surname>Saraph</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaodong</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Brandon</forename><surname>Reagen</surname></persName>
		</author>
		<author>
			<persName><surname>Gu-Yeon</surname></persName>
		</author>
		<author>
			<persName><surname>Wei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Hsien-Hsin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Carole-Jean</forename><surname>Brooks</surname></persName>
		</author>
		<author>
			<persName><surname>Wu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the ACM/IEEE 47th Annual International Symposium on Computer Architecture, ISCA &apos;20</title>
		<meeting>the ACM/IEEE 47th Annual International Symposium on Computer Architecture, ISCA &apos;20</meeting>
		<imprint>
			<publisher>IEEE Press</publisher>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="982" to="995" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Recpipe: Co-designing models and hardware to jointly optimize recommendation quality and performance</title>
		<author>
			<persName><forename type="first">Udit</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Samuel</forename><surname>Hsia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jeff</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mark</forename><surname>Wilkening</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Javin</forename><surname>Pombra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sean</forename><surname>Hsien-Hsin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gu-Yeon</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Carole-Jean</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><surname>Brooks</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">MICRO-54: 54th Annual IEEE/ACM International Symposium on Microarchitecture, MICRO &apos;21</title>
		<meeting><address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computing Machinery</publisher>
			<date type="published" when="2021">2021</date>
			<biblScope unit="volume">870</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">The architectural implications of facebook&apos;s dnn-based personalized recommendation</title>
		<author>
			<persName><forename type="first">Udit</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Carole-Jean</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaodong</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Maxim</forename><surname>Naumov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Brandon</forename><surname>Reagen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><surname>Brooks</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bradford</forename><surname>Cottel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kim</forename><surname>Hazelwood</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mark</forename><surname>Hempstead</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bill</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Hsien-Hsin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrey</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dheevatsa</forename><surname>Malevich</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mikhail</forename><surname>Mudigere</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Liang</forename><surname>Smelyanskiy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xuan</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2020 IEEE International Symposium on High Performance Computer Architecture (HPCA)</title>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="488" to="501" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Training recommender systems at scale: Communication-efficient model and data parallelism</title>
		<author>
			<persName><forename type="first">Vipul</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dhruv</forename><surname>Choudhary</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Peter</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaohan</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xing</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuzhen</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Arun</forename><surname>Kejariwal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kannan</forename><surname>Ramchandran</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><forename type="middle">W</forename><surname>Mahoney</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 27th ACM SIGKDD Conference on Knowledge Discovery &amp; Data Mining, KDD &apos;21</title>
		<meeting>the 27th ACM SIGKDD Conference on Knowledge Discovery &amp; Data Mining, KDD &apos;21<address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computing Machinery</publisher>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="2928" to="2936" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Applied machine learning at facebook: A datacenter infrastructure perspective</title>
		<author>
			<persName><forename type="first">Kim</forename><surname>Hazelwood</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sarah</forename><surname>Bird</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><surname>Brooks</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Soumith</forename><surname>Chintala</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Utku</forename><surname>Diril</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dmytro</forename><surname>Dzhulgakov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mohamed</forename><surname>Fawzy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bill</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yangqing</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aditya</forename><surname>Kalro</surname></persName>
		</author>
		<author>
			<persName><forename type="first">James</forename><surname>Law</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kevin</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jason</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pieter</forename><surname>Noordhuis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Misha</forename><surname>Smelyanskiy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Liang</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaodong</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2018 IEEE International Symposium on High Performance Computer Architecture (HPCA)</title>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="620" to="629" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Neural collaborative filtering</title>
		<author>
			<persName><forename type="first">Xiangnan</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lizi</forename><surname>Liao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hanwang</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Liqiang</forename><surname>Nie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xia</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tat-Seng</forename><surname>Chua</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 26th International Conference on World Wide Web, WWW &apos;17</title>
		<meeting>the 26th International Conference on World Wide Web, WWW &apos;17<address><addrLine>Geneva, Switzerland</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="173" to="182" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Cross-stack workload characterization of deep recommendation systems</title>
		<author>
			<persName><forename type="first">S</forename><surname>Hsia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">U</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Wilkening</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Brooks</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2020 IEEE International Symposium on Workload Characterization (IISWC)</title>
		<meeting><address><addrLine>Los Alamitos, CA, USA</addrLine></address></meeting>
		<imprint>
			<publisher>IEEE Computer Society</publisher>
			<date type="published" when="2020-10">oct 2020</date>
			<biblScope unit="page" from="157" to="168" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Hierarchical training: Scaling deep recommendation models on large cpu clusters</title>
		<author>
			<persName><forename type="first">Yuzhen</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaohan</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xing</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiyan</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bor-Yiing</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shivam</forename><surname>Bharuka</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dhruv</forename><surname>Choudhary</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zewei</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hai</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jack</forename><surname>Langman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 27th ACM SIGKDD Conference on Knowledge Discovery &amp; Data Mining, KDD &apos;21</title>
		<meeting>the 27th ACM SIGKDD Conference on Knowledge Discovery &amp; Data Mining, KDD &apos;21<address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computing Machinery</publisher>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="3050" to="3058" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Centaur: A chiplet-based, hybrid sparse-dense accelerator for personalized recommendations</title>
		<author>
			<persName><forename type="first">Ranggi</forename><surname>Hwang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Taehun</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Youngeun</forename><surname>Kwon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Minsoo</forename><surname>Rhu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the ACM/IEEE 47th Annual International Symposium on Computer Architecture, ISCA &apos;20</title>
		<meeting>the ACM/IEEE 47th Annual International Symposium on Computer Architecture, ISCA &apos;20</meeting>
		<imprint>
			<publisher>IEEE Press</publisher>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="968" to="981" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">Dissecting the graphcore ipu architecture via microbenchmarking</title>
		<author>
			<persName><forename type="first">Zhe</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Blake</forename><surname>Tillman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Marco</forename><surname>Maggioni</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Daniele</forename><forename type="middle">Paolo</forename><surname>Scarpazza</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Microrec: Efficient recommendation inference by hardware and data structure solutions</title>
		<author>
			<persName><forename type="first">Wenqi</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhenhao</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shuai</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Thomas</forename><forename type="middle">B</forename><surname>Preu? Er</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kai</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Liang</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiansong</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tongxuan</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yong</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jingren</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ce</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gustavo</forename><surname>Alonso</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of Machine Learning and Systems</title>
		<editor>
			<persName><forename type="first">A</forename><surname>Smola</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">A</forename><surname>Dimakis</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">I</forename><surname>Stoica</surname></persName>
		</editor>
		<meeting>Machine Learning and Systems</meeting>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page" from="845" to="859" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Fleetrec: Large-scale recommendation inference on hybrid gpu-fpga clusters</title>
		<author>
			<persName><forename type="first">Wenqi</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhenhao</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shuai</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kai</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Liang</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiansong</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tongxuan</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yong</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jingren</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ce</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gustavo</forename><surname>Alonso</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 27th ACM SIGKDD Conference on Knowledge Discovery &amp; Data Mining, KDD &apos;21</title>
		<meeting>the 27th ACM SIGKDD Conference on Knowledge Discovery &amp; Data Mining, KDD &apos;21<address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computing Machinery</publisher>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="3097" to="3105" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Ten lessons from three generations shaped google&apos;s tpuv4i : Industrial product</title>
		<author>
			<persName><forename type="first">Norman</forename><forename type="middle">P</forename><surname>Jouppi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Doe</forename><surname>Hyun Yoon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Matthew</forename><surname>Ashcraft</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mark</forename><surname>Gottscho</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Thomas</forename><forename type="middle">B</forename><surname>Jablin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">George</forename><surname>Kurian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">James</forename><surname>Laudon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sheng</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Peter</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaoyu</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Thomas</forename><surname>Norrie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nishant</forename><surname>Patil</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sushma</forename><surname>Prasad</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Cliff</forename><surname>Young</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zongwei</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><surname>Patterson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2021 ACM/IEEE 48th Annual International Symposium on Computer Architecture (ISCA)</title>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="1" to="14" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">A domain-specific supercomputer for training deep neural networks</title>
		<author>
			<persName><forename type="first">Norman</forename><forename type="middle">P</forename><surname>Jouppi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Doe</forename><surname>Hyun Yoon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">George</forename><surname>Kurian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sheng</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nishant</forename><surname>Patil</surname></persName>
		</author>
		<author>
			<persName><forename type="first">James</forename><surname>Laudon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Cliff</forename><surname>Young</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><surname>Patterson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Commun. ACM</title>
		<imprint>
			<biblScope unit="volume">63</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="67" to="78" />
			<date type="published" when="2020-06">jun 2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">In-datacenter performance analysis of a tensor processing unit</title>
		<author>
			<persName><forename type="first">Cliff</forename><surname>Norman P Jouppi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nishant</forename><surname>Young</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><surname>Patil</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gaurav</forename><surname>Patterson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Raminder</forename><surname>Agrawal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sarah</forename><surname>Bajwa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Suresh</forename><surname>Bates</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nan</forename><surname>Bhatia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Al</forename><surname>Boden</surname></persName>
		</author>
		<author>
			<persName><surname>Borchers</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the ACM/IEEE 44th Annual International Symposium on Computer Architecture (ISCA)</title>
		<meeting>the ACM/IEEE 44th Annual International Symposium on Computer Architecture (ISCA)</meeting>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="1" to="12" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<title level="m" type="main">Display advertising challenge: Predict clickthrough rates on display ads</title>
		<author>
			<persName><forename type="first">Criteo</forename><surname>Kaggle</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Learning to embed categorical features without embedding tables for recommendation</title>
		<author>
			<persName><forename type="first">Wang-Cheng</forename><surname>Kang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Derek</forename><forename type="middle">Zhiyuan</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tiansheng</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xinyang</forename><surname>Yi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ting</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lichan</forename><surname>Hong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ed</forename><forename type="middle">H</forename><surname>Chi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 27th ACM SIGKDD Conference on Knowledge Discovery &amp;and Data Mining, KDD &apos;21</title>
		<meeting>the 27th ACM SIGKDD Conference on Knowledge Discovery &amp;and Data Mining, KDD &apos;21<address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computing Machinery</publisher>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="840" to="850" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Recnmp: Accelerating personalized recommendation with near-memory processing</title>
		<author>
			<persName><forename type="first">Udit</forename><surname>Liu Ke</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Benjamin</forename><forename type="middle">Youngjae</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vikas</forename><surname>Brooks</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Utku</forename><surname>Chandra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Amin</forename><surname>Diril</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kim</forename><surname>Firoozshahian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bill</forename><surname>Hazelwood</surname></persName>
		</author>
		<author>
			<persName><surname>Jia</surname></persName>
		</author>
		<author>
			<persName><surname>Hsien-Hsin S Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2020 ACM/IEEE 47th Annual International Symposium on Computer Architecture (ISCA)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="790" to="803" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Hercules: Heterogeneity-aware inference serving for at-scale personalized recommendation</title>
		<author>
			<persName><forename type="first">Udit</forename><surname>Liu Ke</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mark</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Carole-Jean</forename><surname>Hempstead</surname></persName>
		</author>
		<author>
			<persName><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Hsien-Hsin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xuan</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2022 IEEE International Symposium on High Performance Computer Architecture (HPCA)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Near-memory processing in action: Accelerating personalized recommendation with axdimm</title>
		<author>
			<persName><forename type="first">Xuan</forename><surname>Liu Ke</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jinin</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jong-Geon</forename><surname>So</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shin-Haeng</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sukhan</forename><surname>Kang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Songyi</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yeongon</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jin</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hyun</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yongsuk</forename><surname>Kwon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kyungsoo</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jin</forename><surname>Jung</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ilkwon</forename><surname>Yun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sung</forename><forename type="middle">Joo</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hyunsun</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Joonho</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jeonghyeon</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kyomin</forename><surname>Sohn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nam</forename><forename type="middle">Sung</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Hsien-Hsin</surname></persName>
		</author>
		<author>
			<persName><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Micro</title>
		<imprint>
			<biblScope unit="volume">42</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="116" to="127" />
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
		<title level="m" type="main">Fbgemm: Enabling high-performance low-precision deep learning inference</title>
		<author>
			<persName><forename type="first">Daya</forename><surname>Khudia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jianyu</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Protonu</forename><surname>Basu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Summer</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Haixin</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jongsoo</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mikhail</forename><surname>Smelyanskiy</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2101.05615</idno>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Tensordimm: A practical near-memory processing architecture for embeddings and tensor operations in deep learning</title>
		<author>
			<persName><forename type="first">Youngeun</forename><surname>Kwon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yunjae</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Minsoo</forename><surname>Rhu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 52nd Annual IEEE/ACM International Symposium on Microarchitecture</title>
		<meeting>the 52nd Annual IEEE/ACM International Symposium on Microarchitecture</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="740" to="753" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Tensor casting: Co-designing algorithm-architecture for personalized recommendation training</title>
		<author>
			<persName><forename type="first">Youngeun</forename><surname>Kwon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yunjae</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Minsoo</forename><surname>Rhu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2021 IEEE International Symposium on High-Performance Computer Architecture (HPCA)</title>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="235" to="248" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Understanding capacity-driven scale-out neural recommendation inference</title>
		<author>
			<persName><forename type="first">M</forename><surname>Lui</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Yetim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Ozkan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Tsai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Hempstead</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2021 IEEE International Symposium on Performance Analysis of Systems and Software (ISPASS)</title>
		<meeting><address><addrLine>Los Alamitos, CA, USA</addrLine></address></meeting>
		<imprint>
			<publisher>IEEE Computer Society</publisher>
			<date type="published" when="2021-03">mar 2021</date>
			<biblScope unit="page" from="162" to="171" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Software-hardware co-design for fast and scalable training of deep learning recommendation models</title>
		<author>
			<persName><forename type="first">Dheevatsa</forename><surname>Mudigere</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuchen</forename><surname>Hao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jianyu</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhihao</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrew</forename><surname>Tulloch</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Srinivas</forename><surname>Sridharan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xing</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mustafa</forename><surname>Ozdal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jade</forename><surname>Nie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jongsoo</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Liang</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">(</forename><surname>Jie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">)</forename><surname>Amy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Leon</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dmytro</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aarti</forename><surname>Ivchenko</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuxi</forename><surname>Basant</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiyan</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ehsan</forename><forename type="middle">K</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaodong</forename><surname>Ardestani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rakesh</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ching-Hsiang</forename><surname>Komuravelli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Serhat</forename><surname>Chu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Huayu</forename><surname>Yilmaz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiyuan</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhuobo</forename><surname>Qian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yinbin</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Junjie</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ellie</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hong</forename><surname>Wen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lin</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chonglin</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Whitney</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dimitry</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Krishna</forename><surname>Melts</surname></persName>
		</author>
		<author>
			<persName><surname>Dhulipala</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tyler</forename><surname>Kr Kishore</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Assaf</forename><surname>Graf</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kiran</forename><surname>Eisenman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Adi</forename><surname>Kumar Matam</surname></persName>
		</author>
		<author>
			<persName><surname>Gangidi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jerry</forename><surname>Guoqiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Manoj</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Avinash</forename><surname>Krishnan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Krishnakumar</forename><surname>Nayak</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bharath</forename><surname>Nair</surname></persName>
		</author>
		<author>
			<persName><surname>Muthiah</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 49th Annual International Symposium on Computer Architecture, ISCA &apos;22</title>
		<editor>
			<persName><forename type="first">Mahmoud</forename><surname>Khorashadi</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Pallab</forename><surname>Bhattacharya</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Petr</forename><surname>Lapukhov</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Maxim</forename><surname>Naumov</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Ajit</forename><surname>Mathews</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Lin</forename><surname>Qiao</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Mikhail</forename><surname>Smelyanskiy</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Bill</forename><surname>Jia</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Vijay</forename><surname>Rao</surname></persName>
		</editor>
		<meeting>the 49th Annual International Symposium on Computer Architecture, ISCA &apos;22<address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computing Machinery</publisher>
			<date type="published" when="2022">2022</date>
			<biblScope unit="page" from="993" to="1011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<monogr>
		<title level="m" type="main">Deep learning recommendation model for personalization and recommendation systems</title>
		<author>
			<persName><forename type="first">Maxim</forename><surname>Naumov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dheevatsa</forename><surname>Mudigere</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><surname>Hao-Jun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jianyu</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Narayanan</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jongsoo</forename><surname>Sundaraman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaodong</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Udit</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Carole-Jean</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alisson</forename><forename type="middle">G</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><surname>Azzolini</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1906.00091</idno>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Trim: Enhancing processor-memory interfaces with scalable tensor reduction in memory</title>
		<author>
			<persName><forename type="first">Jaehyun</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Byeongho</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sungmin</forename><surname>Yun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Eojin</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Minsoo</forename><surname>Rhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jung</forename><surname>Ho Ahn</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">MICRO-54: 54th Annual IEEE/ACM International Symposium on Microarchitecture, MICRO &apos;21</title>
		<meeting><address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computing Machinery</publisher>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="268" to="281" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<monogr>
		<title level="m" type="main">Deep learning inference in facebook data centers: Characterization, performance optimizations and hardware implications</title>
		<author>
			<persName><forename type="first">Jongsoo</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Maxim</forename><surname>Naumov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Protonu</forename><surname>Basu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Summer</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aravind</forename><surname>Kalaiah</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Daya</forename><surname>Khudia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">James</forename><surname>Law</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Parth</forename><surname>Malani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrey</forename><surname>Malevich</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Satish</forename><surname>Nadathur</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Juan</forename><surname>Pino</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Martin</forename><surname>Schatz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alexander</forename><surname>Sidorov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Viswanath</forename><surname>Sivakumar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrew</forename><surname>Tulloch</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaodong</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yiming</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hector</forename><surname>Yuen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Utku</forename><surname>Diril</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dmytro</forename><surname>Dzhulgakov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kim</forename><surname>Hazelwood</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bill</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yangqing</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lin</forename><surname>Qiao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vijay</forename><surname>Rao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nadav</forename><surname>Rotem</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sungjoo</forename><surname>Yoo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mikhail</forename><surname>Smelyanskiy</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<monogr>
		<title level="m" type="main">PyTorch: An Imperative Style, High-Performance Deep Learning Library</title>
		<author>
			<persName><forename type="first">Adam</forename><surname>Paszke</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sam</forename><surname>Gross</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Francisco</forename><surname>Massa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Adam</forename><surname>Lerer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">James</forename><surname>Bradbury</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gregory</forename><surname>Chanan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Trevor</forename><surname>Killeen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zeming</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Natalia</forename><surname>Gimelshein</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Luca</forename><surname>Antiga</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alban</forename><surname>Desmaison</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andreas</forename><surname>K?pf</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Edward</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zach</forename><surname>Devito</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Martin</forename><surname>Raison</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alykhan</forename><surname>Tejani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sasank</forename><surname>Chilamkurthy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Benoit</forename><surname>Steiner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lu</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Junjie</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Soumith</forename><surname>Chintala</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019">2019</date>
			<publisher>Curran Associates Inc</publisher>
			<pubPlace>Red Hook, NY, USA</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Mlperf inference benchmark</title>
		<author>
			<persName><forename type="first">Vijay</forename><surname>Janapa Reddi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christine</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><surname>Kanter</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Peter</forename><surname>Mattson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Guenther</forename><surname>Schmuelling</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Carole-Jean</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Brian</forename><surname>Anderson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Maximilien</forename><surname>Breughe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mark</forename><surname>Charlebois</surname></persName>
		</author>
		<author>
			<persName><forename type="first">William</forename><surname>Chou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ramesh</forename><surname>Chukka</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Cody</forename><surname>Coleman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sam</forename><surname>Davis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pan</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Greg</forename><surname>Diamos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jared</forename><surname>Duke</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dave</forename><surname>Fick</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">Scott</forename><surname>Gardner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Itay</forename><surname>Hubara</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sachin</forename><surname>Idgunji</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Thomas</forename><forename type="middle">B</forename><surname>Jablin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jeff</forename><surname>Jiao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tom</forename><surname>St</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pankaj</forename><surname>John</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><surname>Kanwar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jeffery</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Anton</forename><surname>Liao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Francisco</forename><surname>Lokhmotov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Peng</forename><surname>Massa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Paulius</forename><surname>Meng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Colin</forename><surname>Micikevicius</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gennady</forename><surname>Osborne</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Arun</forename><surname>Pekhimenko</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dilip</forename><surname>Tejusve Raghunath Rajan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ashish</forename><surname>Sequeira</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fei</forename><surname>Sirasao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hanlin</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Frank</forename><surname>Thomson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ephrem</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lingjie</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Koichi</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bing</forename><surname>Yamada</surname></persName>
		</author>
		<author>
			<persName><forename type="first">George</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aaron</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Peizhao</forename><surname>Zhong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuchen</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><surname>Zhou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the ACM/IEEE 47th Annual International Symposium on Computer Architecture, ISCA &apos;20</title>
		<meeting>the ACM/IEEE 47th Annual International Symposium on Computer Architecture, ISCA &apos;20</meeting>
		<imprint>
			<publisher>IEEE Press</publisher>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="446" to="459" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Recshard: Statistical feature-based memory optimization for industry-scale neural recommendation</title>
		<author>
			<persName><forename type="first">Geet</forename><surname>Sethi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bilge</forename><surname>Acun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Niket</forename><surname>Agarwal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christos</forename><surname>Kozyrakis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Caroline</forename><surname>Trippel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Carole-Jean</forename><surname>Wu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">27th ACM International Conference on Architectural Support for Programming Languages and Operating Systems (ASPLOS)</title>
		<imprint>
			<biblScope unit="page">2022</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<monogr>
		<title level="m" type="main">Flexshard: Flexible sharding for industry-scale sequence recommendation models</title>
		<author>
			<persName><forename type="first">Geet</forename><surname>Sethi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pallab</forename><surname>Bhattacharya</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dhruv</forename><surname>Choudhary</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Carole-Jean</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christos</forename><surname>Kozyrakis</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<monogr>
		<title level="m" type="main">Compositional embeddings using complementary partitions for memory-efficient recommendation systems</title>
		<author>
			<persName><forename type="first">Michael</forename><surname>Hao-Jun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dheevatsa</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Maxim</forename><surname>Mudigere</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiyan</forename><surname>Naumov</surname></persName>
		</author>
		<author>
			<persName><surname>Yang</surname></persName>
		</author>
		<idno>CoRR, abs/1909.02107</idno>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<monogr>
		<title level="m" type="main">Criteo terabyte click logs</title>
		<author>
			<persName><forename type="first">Criteo</forename><surname>Terabyte</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">FlashEmbedding: Storing Embedding Tables</title>
		<author>
			<persName><forename type="first">Xuan</forename><surname>Hu Wan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yufei</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chia-Lin</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tei-Wei</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chun</forename><surname>Kuo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xue</forename><surname>Jason</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SSD for Large-Scale Recommender Systems</title>
		<meeting><address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computing Machinery</publisher>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="9" to="16" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Billion-scale commodity embedding for e-commerce recommendation in alibaba</title>
		<author>
			<persName><forename type="first">Jizhe</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pipei</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Huan</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhibo</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Binqiang</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dik</forename><surname>Lun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lee</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 24th ACM SIGKDD International Conference on Knowledge Discovery &amp; Data Mining, KDD &apos;18</title>
		<meeting>the 24th ACM SIGKDD International Conference on Knowledge Discovery &amp; Data Mining, KDD &apos;18<address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computing Machinery</publisher>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="839" to="848" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Recssd: Near data processing for solid state drive based recommendation inference</title>
		<author>
			<persName><forename type="first">Mark</forename><surname>Wilkening</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Udit</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Samuel</forename><surname>Hsia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Caroline</forename><surname>Trippel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Carole-Jean</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><surname>Brooks</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gu-Yeon</forename><surname>Wei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">26th ACM International Conference on Architectural Support for Programming Languages and Operating Systems (ASPLOS)</title>
		<imprint>
			<biblScope unit="page">2021</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<monogr>
		<title level="m" type="main">Developing a recommendation benchmark for mlperf training and inference</title>
		<author>
			<persName><forename type="first">Carole-Jean</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Robin</forename><surname>Burke</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ed</forename><surname>Chi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Joseph</forename><forename type="middle">A</forename><surname>Konstan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Julian</forename><forename type="middle">J</forename><surname>Mcauley</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yves</forename><surname>Raimond</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hao</forename><surname>Zhang</surname></persName>
		</author>
		<idno>CoRR, abs/2003.07336</idno>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Sustainable ai: Environmental implications, challenges and opportunities</title>
		<author>
			<persName><forename type="first">Carole-Jean</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ramya</forename><surname>Raghavendra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Udit</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bilge</forename><surname>Acun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Newsha</forename><surname>Ardalani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kiwan</forename><surname>Maeng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gloria</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fiona</forename><surname>Aga</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jinshi</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Charles</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><surname>Gschwind</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Anurag</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Myle</forename><surname>Ott</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Anastasia</forename><surname>Melnikov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Salvatore</forename><surname>Candido</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><surname>Brooks</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Geeta</forename><surname>Chauhan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Benjamin</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hsien-Hsin</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bugra</forename><surname>Akyildiz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Maximilian</forename><surname>Balandat</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Joe</forename><surname>Spisak</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ravi</forename><surname>Jain</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mike</forename><surname>Rabbat</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kim</forename><surname>Hazelwood</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of Machine Learning and Systems</title>
		<meeting>Machine Learning and Systems</meeting>
		<imprint>
			<date type="published" when="2022">2022</date>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page" from="795" to="813" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Kraken: Memory-efficient continual learning for large-scale realtime recommendations</title>
		<author>
			<persName><forename type="first">Minhui</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kai</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Youyou</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Guangxu</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qingxing</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bihai</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiazhen</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hongbo</forename><surname>Ao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wanhong</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiwu</forename><surname>Shu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SC20: International Conference for High Performance Computing, Networking, Storage and Analysis</title>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="1" to="17" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">Factorized deep retrieval and distributed tensorflow serving</title>
		<author>
			<persName><forename type="first">Xinyang</forename><surname>Yi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yi-Fan</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sukriti</forename><surname>Ramesh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vinu</forename><surname>Rajashekhar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lichan</forename><surname>Hong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Noah</forename><surname>Fiedel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nandini</forename><surname>Seshadri</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lukasz</forename><surname>Heldt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiang</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ed</forename><forename type="middle">H</forename><surname>Chi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of Machine Learning and Systems, SysML&apos;18</title>
		<meeting>Machine Learning and Systems, SysML&apos;18</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">Ttrec: Tensor train compression for deep learning recommendation models</title>
		<author>
			<persName><forename type="first">Chunxing</forename><surname>Yin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bilge</forename><surname>Acun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xing</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Carole-Jean</forename><surname>Wu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of Machine Learning and Systems</title>
		<meeting>Machine Learning and Systems</meeting>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page" from="448" to="462" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">Autoshard: Automated embedding table sharding for recommender systems</title>
		<author>
			<persName><forename type="first">Daochen</forename><surname>Zha</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Louis</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bhargav</forename><surname>Bhushanam</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dhruv</forename><surname>Choudhary</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jade</forename><surname>Nie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuandong</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jay</forename><surname>Chae</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yinbin</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Arun</forename><surname>Kejariwal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xia</forename><surname>Hu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 28th ACM SIGKDD Conference on Knowledge Discovery and Data Mining, KDD &apos;22</title>
		<meeting>the 28th ACM SIGKDD Conference on Knowledge Discovery and Data Mining, KDD &apos;22<address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computing Machinery</publisher>
			<date type="published" when="2022">2022</date>
			<biblScope unit="page" from="4461" to="4471" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<monogr>
		<title level="m" type="main">Dreamshard: Generalizable embedding table placement for recommender systems</title>
		<author>
			<persName><forename type="first">Daochen</forename><surname>Zha</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Louis</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qiaoyu</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zirui</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kwei-Herng</forename><surname>Lai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bhargav</forename><surname>Bhushanam</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuandong</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Arun</forename><surname>Kejariwal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xia</forename><surname>Hu</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<monogr>
		<title level="m" type="main">Distributed hierarchical gpu parameter server for massive scale deep learning ads systems</title>
		<author>
			<persName><forename type="first">Weijie</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Deping</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ronglai</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yulei</forename><surname>Qian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ruiquan</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mingming</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ping</forename><surname>Li</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<monogr>
		<title level="m" type="main">Distributed hierarchical gpu parameter server for massive scale deep learning ads systems</title>
		<author>
			<persName><forename type="first">Weijie</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Deping</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ronglai</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yulei</forename><surname>Qian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ruiquan</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mingming</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ping</forename><surname>Li</surname></persName>
		</author>
		<editor>MLSys</editor>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b58">
	<analytic>
		<title level="a" type="main">Aibox: Ctr prediction model training on a single node</title>
		<author>
			<persName><forename type="first">Weijie</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jingyuan</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Deping</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yulei</forename><surname>Qian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ronglai</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ping</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 28th ACM International Conference on Information and Knowledge Management, CIKM &apos;19</title>
		<meeting>the 28th ACM International Conference on Information and Knowledge Management, CIKM &apos;19<address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computing Machinery</publisher>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="319" to="328" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b59">
	<analytic>
		<title level="a" type="main">Recommending what video to watch next: A multitask ranking system</title>
		<author>
			<persName><forename type="first">Zhe</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lichan</forename><surname>Hong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Li</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jilin</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aniruddh</forename><surname>Nath</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shawn</forename><surname>Andrews</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aditee</forename><surname>Kumthekar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Maheswaran</forename><surname>Sathiamoorthy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xinyang</forename><surname>Yi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ed</forename><surname>Chi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 13th ACM Conference on Recommender Systems, RecSys &apos;19</title>
		<meeting>the 13th ACM Conference on Recommender Systems, RecSys &apos;19<address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="43" to="51" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b60">
	<analytic>
		<title level="a" type="main">Deep interest evolution network for click-through rate prediction</title>
		<author>
			<persName><forename type="first">Guorui</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Na</forename><surname>Mou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ying</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qi</forename><surname>Pi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Weijie</forename><surname>Bian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chang</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaoqiang</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kun</forename><surname>Gai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the AAAI Conference on Artificial Intelligence</title>
		<meeting>the AAAI Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page" from="5941" to="5948" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b61">
	<analytic>
		<title level="a" type="main">Deep interest network for click-through rate prediction</title>
		<author>
			<persName><forename type="first">Guorui</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaoqiang</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chenru</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ying</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Han</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiao</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yanghui</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Junqi</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Han</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kun</forename><surname>Gai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 24th ACM SIGKDD International Conference on Knowledge Discovery &amp; Data Mining</title>
		<meeting>the 24th ACM SIGKDD International Conference on Knowledge Discovery &amp; Data Mining</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="1059" to="1068" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
