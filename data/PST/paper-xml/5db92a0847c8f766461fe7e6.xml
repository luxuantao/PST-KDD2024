<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Beyond temperature scaling: Obtaining well-calibrated multiclass probabilities with Dirichlet calibration</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Meelis</forename><surname>Kull</surname></persName>
							<email>meelis.kull@ut.ee</email>
						</author>
						<author>
							<persName><forename type="first">Miquel</forename><surname>Perello-Nieto</surname></persName>
							<email>miquel.perellonieto@bris.ac.uk</email>
						</author>
						<author>
							<persName><forename type="first">Markus</forename><surname>Kängsepp</surname></persName>
							<email>markus.kangsepp@ut.ee</email>
						</author>
						<author>
							<persName><forename type="first">Silva</forename><surname>Telmo</surname></persName>
							<email>telmo@de.ufpb.br</email>
						</author>
						<author>
							<persName><surname>Filho</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Hao</forename><surname>Song</surname></persName>
							<email>hao.song@bristol.ac.uk</email>
						</author>
						<author>
							<persName><forename type="first">Peter</forename><surname>Flach</surname></persName>
							<email>peter.flach@bristol.ac.uk</email>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science</orgName>
								<orgName type="institution">University of Tartu</orgName>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff1">
								<orgName type="department">Department of Computer Science</orgName>
								<orgName type="institution">University of Bristol</orgName>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff2">
								<orgName type="department">Department of Computer Science</orgName>
								<orgName type="institution">University of Tartu</orgName>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff3">
								<orgName type="department">Department of Statistics</orgName>
								<orgName type="institution">Universidade Federal da Paraíba</orgName>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff4">
								<orgName type="department">Department of Computer Science</orgName>
								<orgName type="institution">University of Bristol</orgName>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff5">
								<orgName type="department">Department of Computer Science</orgName>
								<orgName type="institution">University of Bristol</orgName>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff6">
								<orgName type="institution">The Alan Turing Institute</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Beyond temperature scaling: Obtaining well-calibrated multiclass probabilities with Dirichlet calibration</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">A901DBE8866E2043C3C945CFB25E73C6</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.3" ident="GROBID" when="2023-07-28T12:32+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Class probabilities predicted by most multiclass classifiers are uncalibrated, often tending towards over-confidence. With neural networks, calibration can be improved by temperature scaling, a method to learn a single corrective multiplicative factor for inputs to the last softmax layer. On non-neural models the existing methods apply binary calibration in a pairwise or one-vs-rest fashion. We propose a natively multiclass calibration method applicable to classifiers from any model class, derived from Dirichlet distributions and generalising the beta calibration method from binary classification. It is easily implemented with neural nets since it is equivalent to log-transforming the uncalibrated probabilities, followed by one linear layer and softmax. Experiments demonstrate improved probabilistic predictions according to multiple measures (confidence-ECE, classwise-ECE, log-loss, Brier score) across a wide range of datasets and classifiers. Parameters of the learned Dirichlet calibration map provide insights to the biases in the uncalibrated model.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>A probabilistic classifier is well-calibrated if among test instances receiving a predicted probability vector p, the class distribution is (approximately) distributed as p. This property is of fundamental importance when using a classifier for cost-sensitive classification, for human decision making, or within an autonomous system. Due to overfitting, most machine learning algorithms produce overconfident models, unless dedicated procedures are applied, such as Laplace smoothing in decision trees <ref type="bibr" target="#b7">[8]</ref>. The goal of (post-hoc) calibration methods is to use hold-out validation data to learn a calibration map that transforms the model's predictions to be better calibrated. Meteorologists were among the first to think about calibration, with <ref type="bibr" target="#b2">[3]</ref> introducing an evaluation measure for probabilistic forecasts, which we now call Brier score; <ref type="bibr" target="#b20">[21]</ref> proposing reliability diagrams, which allow us to visualise calibration (reliability) errors; and <ref type="bibr" target="#b5">[6]</ref> discussing proper scoring rules for forecaster evaluation and the decomposition of these loss measures into calibration and refinement losses. Calibration methods for binary classifiers have been well studied and include: logistic calibration, also known as 'Platt scaling' <ref type="bibr" target="#b23">[24]</ref>; binning calibration <ref type="bibr" target="#b25">[26]</ref> with either equal-width or equal-frequency bins; isotonic calibration <ref type="bibr" target="#b26">[27]</ref>; and beta calibration <ref type="bibr" target="#b14">[15]</ref>. Extensions of the above approaches include: <ref type="bibr" target="#b21">[22]</ref> which performs Bayesian averaging of multiple calibration maps obtained with equal-frequency binning; <ref type="bibr" target="#b22">[23]</ref> which uses near-isotonic regression to allow for some non-monotonic segments in the calibration maps; and <ref type="bibr" target="#b0">[1]</ref> which introduces a non-parametric Bayesian isotonic calibration method.</p><p>Calibration in multiclass scenarios has been approached by decomposing the problem into k onevs-rest binary calibration tasks <ref type="bibr" target="#b26">[27]</ref>, one for each class. The predictions of these k calibration models form unnormalised probability vectors, which, after normalisation, are not guaranteed to be calibrated. Native multiclass calibration methods were introduced recently with a focus on neural networks, including: matrix scaling, vector scaling and temperature scaling <ref type="bibr" target="#b8">[9]</ref>, which can all be seen as multiclass extensions of Platt scaling and have been proposed as calibration layers which should be applied to the logits of a neural network, replacing the softmax layer. An alternative to post-hoc calibration is to modify the classifier learning algorithm itself: MMCE <ref type="bibr" target="#b16">[17]</ref> trains neural networks by optimising the combination of log-loss with a kernel-based measure of calibration loss; SWAG <ref type="bibr" target="#b18">[19]</ref> models the posterior distribution over the weights of the neural network and then samples from this distribution to perform Bayesian model averaging; <ref type="bibr" target="#b19">[20]</ref> proposed a method to transform the classification task into regression and to learn a Gaussian Process model. Calibration methods have been proposed for the regression task as well, including a method by <ref type="bibr" target="#b12">[13]</ref> which adopts isotonic regression to calibrate the predicted quantiles. The theory of calibration functions and empirical calibration evaluation in classification was studied by <ref type="bibr" target="#b24">[25]</ref>, also proposing a statistical test of calibration.</p><p>While there are several calibration methods tailored for deep neural networks, we propose a generalpurpose, natively multiclass calibration method called Dirichlet calibration, applicable for calibrating any probabilistic classifier. We also demonstrate that the multiclass setting introduces numerous subtleties that have not always been recognised or correctly dealt with by other authors. For example, some authors use the weaker notion of confidence calibration (our term), which requires only that the classifier's predicted probability for what it considers the most likely class is calibrated. There are also variations in the evaluation metric used and in the way calibrated probabilities are visualised. Consequently, Section 2 is concerned with clarifying such fundamental issues. We then propose the approach of Dirichlet calibration in Section 3, present and discuss experimental results in Section 4, and conclude in Section 5.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Evaluation of calibration and temperature scaling</head><p>Consider a probabilistic classifier p : X → ∆ k that outputs class probabilities for k classes 1, . . . , k. For any given instance x in the feature space X it would output some probability vector p(x) = ( p1 (x), . . . , pk (x)) belonging to</p><formula xml:id="formula_0">∆ k = {(q 1 , . . . , q k ) ∈ [0, 1] k | ∑ k i=1 q i = 1} which is the (k -1)-dimensional probability simplex over k classes.</formula><p>Definition 1. A probabilistic classifier p : X → ∆ k is multiclass-calibrated, or simply calibrated, if for any prediction vector q = (q 1 , . . . , q k ) ∈ ∆ k , the proportions of classes among all possible instances x getting the same prediction p(x) = q are equal to the prediction vector q:</p><formula xml:id="formula_1">P(Y = i | p(X) = q) = q i for i = 1, . . . , k.<label>(1)</label></formula><p>One can define several weaker notions of calibration <ref type="bibr" target="#b24">[25]</ref> which provide necessary conditions for the model to be fully calibrated. One of these weaker notions was originally proposed by <ref type="bibr" target="#b26">[27]</ref>, requiring that all one-vs-rest probability estimators obtained from the original multiclass model are calibrated. Definition 2. A probabilistic classifier p : X → ∆ k is classwise-calibrated, if for any class i and any predicted probability q i for this class:</p><formula xml:id="formula_2">P(Y = i | pi (X) = q i ) = q i .<label>(2)</label></formula><p>Another weaker notion of calibration was used by <ref type="bibr" target="#b8">[9]</ref>, requiring that among all instances where the probability of the most likely class is predicted to be c (the confidence), the expected accuracy is c. </p><formula xml:id="formula_3">P Y = argmax p(X) max p(X) = c = c.<label>(3)</label></formula><p>For practical evaluation purposes these idealistic definitions need to be relaxed. A common approach for checking confidence-calibration is to do equal-width binning of predictions according to confidence level and check if Eq.( <ref type="formula" target="#formula_3">3</ref>) is approximately satisfied within each bin. This can be visualised using the reliability diagram (which we will call the confidence-reliability diagram), see Fig. <ref type="figure" target="#fig_0">1a</ref>, where the wide blue bars show observed accuracy within each bin (empirical version of the conditional probability in Eq.( <ref type="formula" target="#formula_3">3</ref>)), and narrow red bars show the gap between the two sides of Eq.( <ref type="formula" target="#formula_3">3</ref>). With accuracy below the average confidence in most bins, this figure about a wide ResNet trained on CIFAR-10 shows over-confidence, typical for neural networks which predict probabilities through the last softmax layer and are trained by minimising cross-entropy.</p><p>The calibration method called temperature scaling was proposed by <ref type="bibr" target="#b8">[9]</ref> and it uses a hold-out validation set to learn a single temperature-parameter t &gt; 0 which decreases confidence (if t &gt; 1) or increases confidence (if t &lt; 1). This is achieved by rescaling the logit vector z (input to softmax σ ), so that instead of σ (z) the predicted class probabilities will be obtained by σ (z/t). The confidencereliability diagram in Fig. <ref type="figure" target="#fig_0">1b</ref> shows that the same c10_resnet_wide32 model has come closer to being confidence-calibrated after temperature scaling, having smaller gaps to the accuracy-equalsconfidence diagonal. This is reflected in a lower Expected Calibration Error (confidence-ECE), defined as the average gap across bins, weighted by the number of instances in the bin. In fact, confidence-ECE is low enough that the statistical test proposed by <ref type="bibr" target="#b24">[25]</ref> with significance level α = 0.01 does not reject the hypothesis that the model is confidence-calibrated (p-value 0.017). The main idea behind this test is that for a perfectly calibrated model, ECE against actual labels is in expectation equal to the ECE against pseudo-labels which have been drawn from the categorical distributions corresponding to the predicted class probability vectors. The above p-value was obtained by randomly drawing 10,000 sets of pseudo-labels and finding 170 of these to have higher ECE than the actual one.</p><p>While the above temperature-scaled model is (nearly) confidence-calibrated, it is far from being classwise-calibrated. This becomes evident in Fig <ref type="figure" target="#fig_0">1c</ref>, demonstrating that it systematically overestimates the probability of instances to belong to class 2, with predicted probability (x-axis) smaller than the observed frequency of class 2 (y-axis) in all the equal-width bins. In contrast, the model systematically under-estimates class 4 probability (Supplementary Fig. <ref type="figure" target="#fig_4">12a</ref>). Having only a single tuneable parameter, temperature scaling cannot learn to act differently on different classes. We propose plots such as Fig. <ref type="figure" target="#fig_0">1c</ref>,d across all classes to be used for evaluating classwise-calibration, and we will call these the classwise-reliability diagrams. We propose classwise-ECE as a measure of classwise-calibration, defined as the average gap across all classwise-reliability diagrams, weighted by the number of instances in each bin:</p><formula xml:id="formula_4">classwise-ECE = 1 k k ∑ j=1 m ∑ i=1 |B i, j | n |y j (B i, j ) -p j (B i, j )|<label>(4)</label></formula><p>where k, m, n are the numbers of classes, bins and instances, respectively, |B i, j | denotes the size of the bin, and p j (B i, j ) and y j (B i, j ) denote the average prediction of class j probability and the actual proportion of class j in the bin B i, j . The contribution of a single class j to the classwise-ECE will be called classj-ECE. As seen in Fig. <ref type="figure" target="#fig_0">1(d</ref>), the same model gets closer to being class-2-calibrated after applying our proposed Dirichlet calibration. By averaging class-j-ECE across all classes we get the overall classwise-ECE which for temperature scaling is cwECE = 0.1857 and for Dirichlet calibration cwECE = 0.1795. This small difference in classwise-ECE appears more substantial when running the statistical test of <ref type="bibr" target="#b24">[25]</ref>, rejecting the null hypothesis that temperature scaling is classwisecalibrated (p &lt; 0.0001), while for Dirichlet calibration the decision depends on the significance level (p = 0.016). A similar measure of classwise-calibration called L 2 marginal calibration error was proposed in a concurrent work by <ref type="bibr" target="#b15">[16]</ref>.</p><p>Before explaining the Dirichlet calibration method, let us highlight the fundamental limitation of evaluation using any of the above reliability diagrams and ECE measures. Namely, it is easy to obtain almost perfectly calibrated probabilities by predicting the overall class distribution, regardless of the given instance. Therefore, it is always important to consider other evaluation measures as well. In addition to the error rate, the obvious candidates are proper losses (such as Brier score or log-loss), as they evaluate probabilistic predictions and decompose into calibration loss and refinement loss <ref type="bibr" target="#b13">[14]</ref>. Proper losses are often used as objective functions in post-hoc calibration methods, which take an uncalibrated probabilistic classifier p and use a hold-out validation dataset to learn a calibration map μ : ∆ k → ∆ k that can be applied as μ( p(x)) on top of the uncalibrated outputs of the classifier to make them better calibrated. Every proper loss is minimised by the same calibration map, known as the canonical calibration function <ref type="bibr" target="#b24">[25]</ref> of p, defined as</p><formula xml:id="formula_5">µ(q) = (P(Y = 1 | p(X) = q), . . . , P(Y = k | p(X) = q</formula><p>)) The goal of Dirichlet calibration, as of any other post-hoc calibration method, is to estimate this canonical calibration map µ for a given probabilistic classifier p.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Dirichlet calibration</head><p>A key decision in designing a calibration method is the choice of parametric family. Our choice was based on the following desiderata: (1) the family needs enough capacity to express biases of particular classes or pairs of classes; (2) the family must contain the identity map for the case where the model is already calibrated; (3) for every map in the family we must be able to provide a semi-reasonable synthetic example where it is the canonical calibration function; (4) the parameters should be interpretable to some extent at least. Dirichlet calibration map family. Inspired by beta calibration for binary classifiers <ref type="bibr" target="#b14">[15]</ref>, we consider the distribution of prediction vectors p(x) separately on instances of each class, and assume these k distributions are Dirichlet distributions with different parameters:</p><formula xml:id="formula_6">p(X) | Y = j ∼ Dir(α ( j) )<label>(5)</label></formula><p>where μDirGen</p><formula xml:id="formula_7">α ( j) = (α ( j) 1 , . . . , α<label>( j)</label></formula><formula xml:id="formula_8">(q; α, π) = (π 1 f 1 (q), . . . , π k f k (q)) /z<label>(6)</label></formula><p>where z = ∑ k j=1 π j f j (q) is the normaliser, and f j is the probability density function of the Dirichlet distribution with parameters α ( j) , gathered into a matrix α. It will also be convenient to have two alternative parametrisations of the same family: a linear parametrisation for fitting purposes and a canonical parametrisation for interpretation purposes. These parametrisations are defined as follows:</p><formula xml:id="formula_9">linear parametrisation: μDirLin (q; W, b) = σ (W ln q + b)<label>(7)</label></formula><p>where W ∈ R k×k is a k × k parameter matrix, ln is a vector function that calculates the natural logarithm component-wise and b ∈ R k is a parameter vector of length k;</p><formula xml:id="formula_10">canonical parametrisation: μDir (q; A, c) = σ (A ln q 1/k + ln c)<label>(8)</label></formula><p>where each column in the k-by-k matrix A ∈ [0, ∞) k×k with non-negative entries contains at least one value 0, division of q by 1/k is component-wise, and c ∈ ∆ k is a probability vector of length k. 2.16 0.00 0.21 0.25 0.13 0.37 0.00 0.23 0.00 0.16 0.05 0.15 2.13 0.39 0.01 0.12 0.18 0.17 0.00 0.04 0.21 0.02 0.12 0.09 1.82 0.07 0.00 0.00 0.09 0.06 0.63 0.09 0.10 0.17 0.14 0.24 2.12 0.08 0.13 0.22 0.01 0.21 0.08 0.02 0.00 0.09 0.10 0.00 1.35 0.22 0.12 0.15 0.23 0.27 0.23 0.10 0.10 0.12 0.22 0.05 1.94 0.14 0.17 Class -18.00 -1.00 2.00 -8.00 4.00 0.00 18.00 1.00 0.00 2.00 -6.00 -16.00 11.00 1.00 1.00 0.00 3.00 3.00 1.00 2.00 -12.00-12.00 50.00 -3.00 0.00 1.00 0.00 1.00 -26.00 1.00 9.00 -4.00 3.00 -14.00 3.00 0.00 0.00 0.00 2.00 1.00 -1.00 -11.00 -6.00 -1.00 16.00 7.00 0.00 -4.00 -4.00 4.00 -3.00 -5.00 -3.00 -7.00 1.00 19.00 0.00 -7.00 1.00 4.00 -24.00 -9.00 2.00 -1.00 1.00 0.00 28.00 1.00 2.00 0.00 4.00 -10.00 0.00 -6.00 3.00 4.00 0.00 -3.00 6.00 2.00</p><p>1.00 -34.00 17.00 -5.00 3.00 1.00 2.00 -2.00 19.00 -2.00 -12.00-10.00 2.00 -2.00 1.00 1.00 1.00 -7.00 6.00 20.00 30 Theorem 1 (Equivalence of generative, linear and canonical parametrisations). The parametric families μDirGen (q; α, π), μDirLin (q; W, b) and μDir (q; A, c) are equal, i.e. they contain exactly the same calibration maps.</p><p>Proof. All proofs are given in the Supplemental Material.</p><p>The benefit of the linear parametrisation is that it can be easily implemented as (additional) layers in a neural network: a logarithmic transformation followed by a fully connected layer with softmax activation. Out of the three parametrisations only the canonical parametrisation is unique, in the sense that any function in the Dirichlet calibration map family can be represented by a single pair of matrix A and vector c satisfying the requirements set by the canonical parametrisation μDir (q; A, c).</p><p>Interpretability. In addition to providing uniqueness, the canonical parametrisation is to some extent interpretable. As demonstrated in the proof of Thm. 1 provided in the Supplemental Material, the linear parametrisation W, b obtained after fitting can be easily transformed into the canonical parametrisation by a i j = w i jmin i w i j and c = σ (W ln u + b), where u = (1/k, . . . , 1/k). In the canonical parametrisation, increasing the value of element a i j in matrix A increases the calibrated probability of class i (and decreases the probabilities of all other classes), with effect size depending on the uncalibrated probability of class j. E.g., element a 3,9 = 0.63 of Fig. <ref type="figure" target="#fig_4">2b</ref> increases class 2 probability whenever class 8 has high predicted probability, modifying decision boundaries and resulting in 26 less confusions of class 2 for 8 as seen in Fig. <ref type="figure" target="#fig_4">2c</ref>. Looking at the matrix A and vector c, it is hard to know the effect of the calibration map without performing the computations. However, at k + 1 'interpretation points' this is (approximately) possible. One of these is the centre of the probability simplex, which maps to c. The other k points are vectors where one value is (almost) zero and the other values are equal, summing up to 1. Figure <ref type="figure" target="#fig_4">2a</ref> shows the 3+1 interpretation points in an example for k = 3, where each arrow visualises the result of calibration (end of arrow) at a particular point (beginning of arrow). The result of calibration map at the interpretation points in the centres of sides (facets) is each determined by a single column of A only. The k columns of matrix A and the vector c determine, respectively, the behaviour of the calibration map near the k + 1 points</p><formula xml:id="formula_11">ε, 1 -ε k -1 , . . . , 1 -ε k -1 , . . . , 1 -ε k -1 , . . . , 1 -ε k -1 , ε , and 1 k , . . . , 1 k</formula><p>The first k points are infinitesimally close to the centres of facets of the probability simplex, and the last point is the centre of the whole simplex. For 3 classes these 4 points have been visualised on the simplex in Fig. <ref type="figure" target="#fig_4">2a</ref>. The Dirichlet calibration map μDir (q; A, c) transforms these k + 1 points into:</p><p>(ε a 11 , . . . , ε a k1 ) /z 1 , . . . , (ε a 1k , . . . , ε a kk ) /z k , and (c 1 , . . . , c k )</p><p>where z i are normalising constants, and a i j , c j are elements of the matrix A and vector c, respectively. However, the effect of each parameter goes beyond the interpretation points and also changes classification decision boundaries. This can be seen for the calibration map for a model SVHN_convnet in Fig. <ref type="figure" target="#fig_4">2b</ref> where larger off-diagonal coefficients a i j often result in a bigger change in the confusion matrix as seen in Fig. <ref type="figure" target="#fig_4">2c</ref> (particularly in the 3rd row and 9th column).</p><p>Relationship to other families. For 2 classes, the Dirichlet calibration map family coincides with the beta calibration map family <ref type="bibr" target="#b14">[15]</ref>. Although temperature scaling has been defined on logits z, it can be expressed in terms of the model outputs p = σ (z) as well. It turns out that temperature scaling maps all belong to the Dirichlet family, with μTempS (q;t) = μDirLin (q; 1 t I, 0), where I is the identity matrix and 0 is the zero vector (see Prop.1 in the Supplemental Material). The Dirichlet calibration family is also related to the matrix scaling family μMatS (z; W, b) = σ (Wz + b) proposed by <ref type="bibr" target="#b8">[9]</ref> alongside with temperature scaling. Both families use a fully connected layer with softmax activation, but the crucial difference is in the inputs to this layer. Matrix scaling uses logits z, while the linear parametrisation of Dirichlet calibration uses log-transformed probabilities ln( p) = ln(σ (z)). As softmax followed by log-transform is losing information, matrix scaling has an informational advantage over Dirichlet calibration on deep neural networks, which we will turn back to in the experiments section.</p><p>Fitting and ODIR regularisation. The results of <ref type="bibr" target="#b8">[9]</ref> showed poor performance for matrix scaling (with ECE, log-loss, error rate), leading the authors to the conclusion that "[a]ny calibration model with tens of thousands (or more) parameters will overfit to a small validation set, even when applying regularization". We agree that some overfitting happens, but in our experiments a simple L2 regularisation suffices on non-neural models, whereas for deep neural nets we propose a novel ODIR (Off-Diagonal and Intercept Regularisation) scheme, which is efficient enough in fighting overfitting to make both Dirichlet calibration and matrix scaling outperform temperature scaling on many occasions, including cases with 100 classes and hence 10100 parameters. Fitting of Dirichlet calibration maps is performed by minimising log-loss, and by adding ODIR regularisation terms to the loss function as follows:</p><formula xml:id="formula_12">L = 1 n n ∑ i=1 logloss μDirLin ( p(x i ); W, b), y i + λ • 1 k(k -1) ∑ i = j w 2 i j + µ • 1 k ∑ j b 2 j</formula><p>where (x i , y i ) are validation instances and w i j , b j are elements of W and b, respectively, and λ , µ are hyper-parameters tunable with internal cross-validation on the validation data. The intuition is that the diagonal is allowed to freely follow the biases of classes, whereas the intercept is regularised separately from the off-diagonal elements due to having different scales (additive vs. multiplicative).</p><p>Implementation details. Implementation of Dirichlet calibration is straightforward in standard deep neural network frameworks (we used Keras <ref type="bibr" target="#b4">[5]</ref> in the neural experiments). Alternatively, it is also possible to use the Newton-Raphson method on the L2 regularised objective function, which is constructed by applying multinomial logistic regression with k features (log-transformed predicted class probabilities). Both the gradient and Hessian matrix can be calculated either analytically or using automatic differentiation libraries (e.g. JAX <ref type="bibr" target="#b1">[2]</ref>). Such implementations normally yield faster convergence given the convexity of the multinomial logistic loss, which is a better choice with a small number of target classes (tractable Hessian). One can also simply adopt existing implementations of logistic regression (e.g. scikit-learn) with the log transformed predicted probabilities. If the uncalibrated model outputs zero probability for some class, then this needs to be clipped to a small positive number (we used 2.2e -308 , the smallest positive usable number for the type float64 in Python).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experiments</head><p>The main goals of our experiments are to: (1) compare performance of Dirichlet calibration with other general-purpose calibration methods on a wide range of datasets and classifiers; (2) compare Dirichlet calibration with temperature scaling on several deep neural networks and study the effectiveness of ODIR regularisation; and (3) study whether the neural-specific calibration methods outperform general-purpose calibration methods due to the information loss going from logits to softmax outputs. , adaboost on trees (adas), linear discriminant analysis (lda), quadratic discriminant analysis (qda), decision tree (tree), K-nearest neighbours (knn), multilayer perceptron (mlp), support vector machine with linear (svc-linear) and RBF kernel (svc-rbf ).</p><p>In each of the 21 × 11 = 231 settings we performed nested cross-validation to evaluate 6 calibration methods: one-vs-rest isotonic calibration (OvR_Isotonic) which learns an isotonic calibration map on each class vs rest separately and renormalises the individual calibration map outputs to add up to one at test time; one-vs-rest equal-width binning (OvR_Width_Bin) where one-vs-rest calibration maps predict the empirical proportion of labels in each of the equal-width bins of the range [0, 1]; one-vs-rest equal-frequency binning (OvR_Freq_Bin) constructing bins with equal numbers of instances; onevs-rest beta calibration (OvR_Beta); temperature scaling (Temp_Scaling); and Dirichlet Calibration with L2 regularisation (Dirichlet_L2). We used 3-fold internal cross-validation to train the calibration maps within the 5 times 5-fold external cross-validation. Following <ref type="bibr" target="#b23">[24]</ref>, the 3 calibration maps learned in the internal cross-validation were all used as an ensemble by averaging their predictions.</p><p>For calibration methods with hyperparameters we used the training fold of the classifier to choose the hyperparameter values with the lowest log-loss.</p><p>We used 8 evaluation measures: accuracy, log-loss, Brier score, maximum calibration error (MCE), confidence-ECE (conf-ECE), classwise-ECE (cw-ECE), as well as significance measures p-conf-ECE and p-cw-ECE evaluating how often the respective ECE measures are not significantly higher than when assuming calibration. For p-conf-ECE and p-cw-ECE we used significance level α = 0.05 in the test of <ref type="bibr" target="#b24">[25]</ref> as explained in Section 2, and counted the proportion of significance tests accepting the model being calibrated out of 5 × 5 cases of external cross-validation. With each of the 8 evaluation measures we ranked the methods on each of the 21 × 11 tasks and performed Friedman tests to find statistical differences <ref type="bibr" target="#b6">[7]</ref>. When the p-value of the Friedman test was under 0.005 we performed a post-hoc one-tailed Bonferroni-Dunn test to obtain Critical Differences (CDs) which indicated the minimum ranking difference to consider the methods significantly different. Further details of the experimental setup are provided in the Supplemental Material.</p><p>Results. The results showed that Dirichlet_L2 was among the best calibrators for every measure.</p><p>In particular, it was the best calibration method based on log-loss, p-cw-ECE and accuracy, and in the group of best calibrators for the other measures. The rankings have been averaged into grouping by classifier learning algorithm and shown for log-loss in Table <ref type="table" target="#tab_1">2</ref>, and for p-cw-ECE in Table <ref type="table" target="#tab_0">1</ref>. The critical difference diagram for p-cw-ECE is presented in Fig. <ref type="figure" target="#fig_5">3a</ref>. Fig. <ref type="figure" target="#fig_5">3b</ref> shows the average p-cw-ECE for each calibration method across all datasets and shows how frequently the statistical test accepted the null hypothesis of classifier being calibrated (higher p-cw-ECE is better). The results show that Dirichlet_L2 was considered calibrated on more than 60% of the p-cw-ECE tests. An evaluation of classwise-calibration without post-hoc calibration is given in Fig. <ref type="figure" target="#fig_5">3c</ref>. Note that svc-linear and svc-rbf have an unfair advantage because their sklearn implementation uses Platt scaling with 3-fold internal cross-validation to provide probabilities. Supplemental material contains the final ranking tables and CD diagrams for every metric, an analysis of the best calibrator hyperparameters, and a more detailed comparison of the classwise calibration for the 11 classifiers.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Calibration of deep neural networks</head><p>Experimental setup. We used 3 datasets (CIFAR-10, CIFAR-100 and SVHN), training 11 deep convolutional neural nets with various architectures: ResNet 110 <ref type="bibr" target="#b9">[10]</ref>, ResNet 110 SD <ref type="bibr" target="#b11">[12]</ref>, ResNet 152 SD <ref type="bibr" target="#b11">[12]</ref>, DenseNet 40 <ref type="bibr" target="#b10">[11]</ref>, WideNet 32 <ref type="bibr" target="#b27">[28]</ref>, LeNet 5 <ref type="bibr" target="#b17">[18]</ref>, and acquiring 3 pretrained models from <ref type="bibr" target="#b3">[4]</ref>. For the latter we set aside 5,000 test instances for fitting the calibration map. On other models we followed <ref type="bibr" target="#b8">[9]</ref>, setting aside 5,000 training instances (6,000 in SVHN) for calibration purposes and training the models as in the original papers. For calibration methods with hyperparameters we used 5-fold cross-validation on the validation set to find optimal regularisation parameters. We used all 5 calibration models with the optimal hyperparameter values by averaging their predictions as in <ref type="bibr" target="#b23">[24]</ref>.</p><p>Among general-purpose calibration methods we compared 2 variants of Dirichlet calibration (with L2 regularisation and with ODIR) against temperature scaling (as discussed in Section 3, it can equivalently act on probabilities instead of logits and is therefore general-purpose). Other methods from our non-neural experiment were not included, as these were outperformed by temperature scaling in the experiments of <ref type="bibr" target="#b8">[9]</ref>. Among methods that use logits (neural-specific calibration methods) we included matrix scaling with ODIR regularisation, and vector scaling, which restricts the matrix scaling family, fixing off-diagonal elements to 0. As reported by <ref type="bibr" target="#b8">[9]</ref>, the non-regularised matrix scaling performed very poorly and was not included in our comparisons. Full details and source code for training the models are in the Supplemental Material.</p><p>Results. Tables <ref type="table" target="#tab_2">3</ref> and<ref type="table" target="#tab_3">4</ref> show that the best among three general-purpose calibration methods depends heavily on the model and dataset. Both variants of Dirichlet calibration (with L2 and with ODIR) outperformed temperature scaling in most cases on CIFAR-10. On CIFAR-100, Dir-L2 is poor, but Dir-ODIR outperforms TempS in cw-ECE, showing the effectiveness of ODIR regularisation. However, this comes at the expense of minor increase in log-loss. According to the average rank across all deep net experiments, Dir-ODIR is best, but without statistical significance.</p><p>The full comparison including calibration methods that use logits confirms that information loss going from logits to softmax outputs has an effect and MS-ODIR (matrix scaling with ODIR) outperforms Dir-ODIR in 8 out of 14 cases on cw-ECE and 11 out of 14 on log-loss. However, the effect is numerically usually very small, as average relative reduction of cw-ECE and log-loss is less than 1% (compared to the average relative reduction of over 30% from the uncalibrated model). According to the average rank on cw-ECE the best method is vector scaling, but this comes at the expense of increased log-loss. According to the average rank on log-loss the best method is MS-ODIR, while its cw-ECE is on average bigger than for vector scaling by 2%.</p><p>As the difference between MS-ODIR and vector scaling was on some models quite small, we further investigated the importance of off-diagonal coefficients in MS-ODIR. For this we introduced a new model MS-ODIR-zero which was obtained from the respective MS-ODIR model by replacing the offdiagonal entries with zeroes. In 6 out of 14 cases (c10_convnet, c10_densenet40, c10_resnet110_SD, c100_convnet, c100_resnet110_SD, SVHN_resnet152_SD) MS-ODIR-zero and MS-ODIR had almost identical performance (difference in log-loss of less than 0.0001), indicating that ODIR regularisation had forced the off-diagonal entries to practically zero. However, MS-ODIR-zero was significantly worse in the remaining 8 out of 14 cases, indicating that the learned off-diagonal coefficients in MS-ODIR were meaningful. In all of those cases MS-ODIR outperformed VecS in log-loss. To eliminate the potential explanation that this could be due to random chance, we retrained each of these networks on 2 more train-test splits (except for SVHN_convnet which we had used as pretrained). In all the reruns MS-ODIR remained better than VecS, confirming that it is important to model the pairwise effects between classes in these cases. Detailed results have been presented in the Supplemental Material.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusion</head><p>In this paper we proposed a new parametric general-purpose multiclass calibration method called Dirichlet calibration, which is a natural extension of the two-class beta calibration method. Dirichlet calibration is easy to implement as a layer in a neural net, or as multinomial logistic regression on log-transformed class probabilities, and its parameters provide insights into the biases of the model. While derived from Dirichlet-distributed likelihoods, it does not assume that the probability vectors are actually Dirichlet-distributed within each class, similarly as logistic calibration (Platt scaling) does not assume that the scores are Gaussian-distributed, while it can be derived from Gaussian likelihoods.</p><p>Comparisons with other general-purpose calibration methods across 21 datasets × 11 models showed best or tied best performance for Dirichlet calibration on all 8 evaluation measures. Evaluation with our proposed classwise-ECE measures how calibrated are the predicted probabilities on all classes, not only on the most likely predicted class as with the commonly used (confidence-)ECE. On neural networks we advance the state-of-the-art by introducing the ODIR regularisation scheme for matrix scaling and Dirichlet calibration, leading these to outperform temperature scaling on many deep neural networks.</p><p>Interestingly, on many deep nets Dirichlet calibration learns a map which is very close to being in a temperature scaling family. This raises a fundamental theoretical question of which neural architectures and training methods result in a classifier with its canonical calibration function contained in the temperature scaling family. But even in those cases Dirichlet calibration can become useful after any kind of dataset shift, learning an interpretable calibration map to reveal the shift and recalibrate the predictions for the new context.</p><p>Deriving calibration maps from Dirichlet distributions opens up the possibility of using other distributions of the exponential family to obtain new calibration maps designed for various score types, as well as investigating scores coming from mixtures of distributions inside each class.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: Reliability diagrams of c10_resnet_wide32 on CIFAR-10: (a) confidence-reliability before calibration; (b) confidence-reliability after temperature scaling; (c) classwise-reliability for class 2 after temperature scaling; (d) classwise-reliability for class 2 after Dirichlet calibration.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head></head><label></label><figDesc>k ) ∈ (0, ∞) k are the Dirichlet parameters for class j. Combining likelihoods P( p(X) | Y ) with priors P(Y ) expressing the overall class distribution π ∈ ∆ k , we can use Bayes' rule to express the canonical calibration function P(Y | p(X)) as follows: generative parametrisation:</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head></head><label></label><figDesc>0.03 0.15 0.14 0.27 0.08 0.03 0.23 0.09 0.26 2.02 0.05 0.05 0.13 0.05 0.12 0.11 0.08 0.14 0.17 0.27 0.15 1.89 0.23 0.00 0.08 0.07 0.19 0.00 0.10 0.09 0.26 0.05 0.10 1.88 0.33 0.13 0.16 0.08 0.06 0.08 0.05 0.29 0.24 0.09 0.05 1.64 0</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: Interpretation of Dirichlet calibration maps: (a) calibration map for MLP on the abalone dataset, 4 interpretation points shown by black dots, and canonical parametrisation as a matrix with A, c; (b) canonical parametrisation of a map on SVHN_convnet; (c) changes to the confusion matrix after applying this calibration map.</figDesc><graphic coords="5,378.45,77.62,105.84,123.30" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: Summarised results for p-cw-ECE: (a) CD diagram; (b) proportion of times each calibrator was calibrated (α = 0.05); (c) proportion of times each classifier was already calibrated (α = 0.05).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 :</head><label>1</label><figDesc>Ranking of calibration methods for p-cw-ECE (Friedman's test significant with p-value 7.54e -85 ).</figDesc><table><row><cell></cell><cell cols="7">DirL2 Beta FreqB Isot WidB TempS Uncal</cell></row><row><cell>adas</cell><cell>2.4</cell><cell>3.2</cell><cell>4.1</cell><cell>4.2</cell><cell>3.9</cell><cell>5.0</cell><cell>5.2</cell></row><row><cell>forest</cell><cell>3.5</cell><cell>2.3</cell><cell>5.7</cell><cell>3.0</cell><cell>3.6</cell><cell>5.0</cell><cell>5.0</cell></row><row><cell>knn</cell><cell>2.5</cell><cell>4.0</cell><cell>4.5</cell><cell>2.1</cell><cell>3.2</cell><cell>5.8</cell><cell>6.0</cell></row><row><cell>lda</cell><cell>1.9</cell><cell>3.1</cell><cell>5.8</cell><cell>3.0</cell><cell>3.5</cell><cell>5.0</cell><cell>5.8</cell></row><row><cell>logistic</cell><cell>2.2</cell><cell>2.8</cell><cell>6.4</cell><cell>3.0</cell><cell>4.2</cell><cell>3.9</cell><cell>5.5</cell></row><row><cell>mlp</cell><cell>2.2</cell><cell>2.9</cell><cell>6.7</cell><cell>4.0</cell><cell>5.2</cell><cell>3.0</cell><cell>4.1</cell></row><row><cell>nbayes</cell><cell>1.4</cell><cell>3.6</cell><cell>4.8</cell><cell>2.6</cell><cell>4.2</cell><cell>5.3</cell><cell>6.1</cell></row><row><cell>qda</cell><cell>2.2</cell><cell>2.8</cell><cell>6.3</cell><cell>2.5</cell><cell>3.8</cell><cell>4.8</cell><cell>5.6</cell></row><row><cell>svc-linear</cell><cell>2.3</cell><cell>2.7</cell><cell>6.7</cell><cell>3.8</cell><cell>4.0</cell><cell>3.7</cell><cell>4.8</cell></row><row><cell>svc-rbf</cell><cell>2.9</cell><cell>3.0</cell><cell>6.3</cell><cell>3.5</cell><cell>4.1</cell><cell>3.9</cell><cell>4.3</cell></row><row><cell>tree</cell><cell>2.4</cell><cell>4.3</cell><cell>5.9</cell><cell>4.2</cell><cell>5.2</cell><cell>3.0</cell><cell>3.0</cell></row><row><cell>avg rank</cell><cell>2.34</cell><cell>3.15</cell><cell>5.73</cell><cell cols="2">3.27 4.11</cell><cell>4.37</cell><cell>5.02</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 :</head><label>2</label><figDesc>Ranking of calibration methods for log-loss (p-value 4.39e -77 ). Calibration methods were compared on 21 UCI datasets (abalone, balancescale, car, cleveland, dermatology, glass, iris, landsat-satellite, libras-movement, mfeat-karhunen, mfeat-morphological, mfeat-zernike, optdigits, page-blocks, pendigits, segment, shuttle, vehicle, vowel, waveform-5000, yeast) with 11 classifiers: multiclass logistic regression (logistic), naive Bayes (nbayes), random forest (forest)</figDesc><table><row><cell cols="7">DirL2 Beta FreqB Isot WidB TempS Uncal</cell></row><row><cell>1.4</cell><cell>3.1</cell><cell>3.2</cell><cell>4.3</cell><cell>3.5</cell><cell>5.9</cell><cell>6.6</cell></row><row><cell>4.2</cell><cell>1.9</cell><cell>4.7</cell><cell>4.1</cell><cell>2.9</cell><cell>5.2</cell><cell>5.2</cell></row><row><cell>3.8</cell><cell>4.8</cell><cell>3.0</cell><cell>1.6</cell><cell>2.0</cell><cell>6.5</cell><cell>6.5</cell></row><row><cell>1.6</cell><cell>2.2</cell><cell>5.2</cell><cell>5.2</cell><cell>3.5</cell><cell>4.6</cell><cell>5.7</cell></row><row><cell>1.3</cell><cell>2.1</cell><cell>5.8</cell><cell>6.1</cell><cell>3.5</cell><cell>3.6</cell><cell>5.6</cell></row><row><cell>2.2</cell><cell>2.3</cell><cell>6.5</cell><cell>6.2</cell><cell>4.7</cell><cell>2.9</cell><cell>3.4</cell></row><row><cell>1.1</cell><cell>3.4</cell><cell>3.4</cell><cell>4.0</cell><cell>4.4</cell><cell>5.5</cell><cell>6.3</cell></row><row><cell>1.7</cell><cell>2.7</cell><cell>5.6</cell><cell>4.6</cell><cell>3.4</cell><cell>4.2</cell><cell>5.8</cell></row><row><cell>1.3</cell><cell>2.3</cell><cell>6.1</cell><cell>6.1</cell><cell>4.3</cell><cell>3.0</cell><cell>4.8</cell></row><row><cell>2.6</cell><cell>2.2</cell><cell>4.3</cell><cell>4.8</cell><cell>4.5</cell><cell>4.0</cell><cell>5.6</cell></row><row><cell>3.9</cell><cell>5.1</cell><cell>3.4</cell><cell>2.1</cell><cell>2.4</cell><cell>5.6</cell><cell>5.6</cell></row><row><cell>2.25</cell><cell>2.92</cell><cell>4.66</cell><cell cols="2">4.48 3.54</cell><cell>4.61</cell><cell>5.54</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 3 :</head><label>3</label><figDesc>Scores and ranking of calibration methods for cw-ECE.</figDesc><table><row><cell></cell><cell></cell><cell cols="5">general-purpose calibrators calibrators using logits</cell></row><row><cell></cell><cell cols="5">Uncal TempS Dir-L2 Dir-ODIR VecS</cell><cell>MS-ODIR</cell></row><row><cell>c10_convnet</cell><cell cols="3">0.104 6 0.044 4 0.043 2</cell><cell>0.045 5</cell><cell>0.043 1</cell><cell>0.044 3</cell></row><row><cell>c10_densenet40</cell><cell cols="3">0.114 6 0.040 5 0.034 1</cell><cell>0.037 4</cell><cell>0.036 2</cell><cell>0.037 3</cell></row><row><cell>c10_lenet5</cell><cell cols="3">0.198 6 0.171 5 0.052 1</cell><cell>0.059 4</cell><cell>0.057 2</cell><cell>0.059 3</cell></row><row><cell>c10_resnet110</cell><cell cols="3">0.098 6 0.043 5 0.032 1</cell><cell>0.039 4</cell><cell>0.037 3</cell><cell>0.036 2</cell></row><row><cell>c10_resnet110_SD</cell><cell cols="3">0.086 6 0.031 4 0.031 5</cell><cell>0.029 3</cell><cell>0.027 2</cell><cell>0.027 1</cell></row><row><cell>c10_resnet_wide32</cell><cell cols="3">0.095 6 0.048 5 0.032 3</cell><cell>0.029 2</cell><cell>0.032 4</cell><cell>0.029 1</cell></row><row><cell>c100_convnet</cell><cell cols="3">0.424 6 0.227 1 0.402 5</cell><cell>0.240 3</cell><cell>0.241 4</cell><cell>0.240 2</cell></row><row><cell>c100_densenet40</cell><cell cols="3">0.470 6 0.187 2 0.330 5</cell><cell>0.186 1</cell><cell>0.189 3</cell><cell>0.191 4</cell></row><row><cell>c100_lenet5</cell><cell cols="3">0.473 6 0.385 5 0.219 4</cell><cell>0.213 2</cell><cell>0.203 1</cell><cell>0.214 3</cell></row><row><cell>c100_resnet110</cell><cell cols="3">0.416 6 0.201 3 0.359 5</cell><cell>0.186 1</cell><cell>0.194 2</cell><cell>0.203 4</cell></row><row><cell>c100_resnet110_SD</cell><cell cols="3">0.375 6 0.203 4 0.373 5</cell><cell>0.189 3</cell><cell>0.170 1</cell><cell>0.186 2</cell></row><row><cell cols="4">c100_resnet_wide32 0.420 6 0.186 4 0.333 5</cell><cell>0.180 2</cell><cell>0.171 1</cell><cell>0.180 3</cell></row><row><cell>SVHN_convnet</cell><cell cols="3">0.159 6 0.038 4 0.043 5</cell><cell>0.026 2</cell><cell>0.025 1</cell><cell>0.027 3</cell></row><row><cell cols="4">SVHN_resnet152_SD 0.019 2 0.018 1 0.022 6</cell><cell>0.020 3</cell><cell>0.021 5</cell><cell>0.021 4</cell></row><row><cell>Average rank</cell><cell>5.71</cell><cell>3.71</cell><cell>3.79</cell><cell>2.79</cell><cell>2.29</cell><cell>2.71</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 4 :</head><label>4</label><figDesc>Scores and ranking of calibration methods for log-loss.</figDesc><table><row><cell></cell><cell cols="5">general-purpose calibrators calibrators using logits</cell></row><row><cell cols="5">Uncal TempS Dir-L2 Dir-ODIR VecS</cell><cell>MS-ODIR</cell></row><row><cell cols="3">0.391 6 0.195 1 0.197 4</cell><cell>0.195 2</cell><cell>0.197 5</cell><cell>0.196 3</cell></row><row><cell cols="3">0.428 6 0.225 5 0.220 1</cell><cell>0.224 4</cell><cell>0.223 3</cell><cell>0.222 2</cell></row><row><cell cols="3">0.823 6 0.800 5 0.744 2</cell><cell>0.744 3</cell><cell>0.747 4</cell><cell>0.743 1</cell></row><row><cell cols="3">0.358 6 0.209 5 0.203 1</cell><cell>0.205 3</cell><cell>0.206 4</cell><cell>0.204 2</cell></row><row><cell cols="3">0.303 6 0.178 5 0.177 4</cell><cell>0.176 3</cell><cell>0.175 2</cell><cell>0.175 1</cell></row><row><cell cols="3">0.382 6 0.191 5 0.185 4</cell><cell>0.182 2</cell><cell>0.183 3</cell><cell>0.182 1</cell></row><row><cell cols="3">1.641 6 0.942 1 1.189 5</cell><cell>0.961 2</cell><cell>0.964 4</cell><cell>0.961 3</cell></row><row><cell cols="3">2.017 6 1.057 2 1.253 5</cell><cell>1.059 4</cell><cell>1.058 3</cell><cell>1.051 1</cell></row><row><cell cols="3">2.784 6 2.650 5 2.595 4</cell><cell>2.490 2</cell><cell>2.516 3</cell><cell>2.487 1</cell></row><row><cell cols="3">1.694 6 1.092 3 1.212 5</cell><cell>1.096 4</cell><cell>1.089 2</cell><cell>1.074 1</cell></row><row><cell cols="3">1.353 6 0.942 3 1.198 5</cell><cell>0.945 4</cell><cell>0.923 1</cell><cell>0.927 2</cell></row><row><cell cols="3">1.802 6 0.945 3 1.087 5</cell><cell>0.953 4</cell><cell>0.937 2</cell><cell>0.933 1</cell></row><row><cell cols="3">0.205 6 0.151 5 0.142 3</cell><cell>0.138 2</cell><cell>0.144 4</cell><cell>0.138 1</cell></row><row><cell cols="3">0.085 6 0.079 1 0.085 5</cell><cell>0.080 2</cell><cell>0.081 4</cell><cell>0.081 3</cell></row><row><cell>6.0</cell><cell>3.5</cell><cell>3.79</cell><cell>2.93</cell><cell>3.14</cell><cell>1.64</cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" xml:id="foot_0"><p>33rd Conference on Neural Information Processing Systems (NeurIPS 2019), Vancouver, Canada.</p></note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgements</head><p>The work of MKu and MKä was supported by the Estonian Research Council under grant PUT1458. The work of MPN and HS was supported by the SPHERE Next Steps Project funded by the UK Engineering and Physical Sciences Research Council (EPSRC), Grant EP/R005273/1. The work of PF and HS was supported by The Alan Turing Institute under EPSRC, Grant EP/N510129/1.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Non-parametric Bayesian isotonic calibration: Fighting overconfidence in binary classification</title>
		<author>
			<persName><forename type="first">M.-L</forename><surname>Allikivi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Kull</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Machine Learning and Knowledge Discovery in Databases (ECML-PKDD&apos;19)</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="68" to="85" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title/>
		<author>
			<persName><forename type="first">J</forename><surname>Bradbury</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Frostig</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Hawkins</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">J</forename><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Leary</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Maclaurin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Wanderman-Milne</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note>JAX: composable transformations of Python+NumPy programs</note>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Verification of forecasts expressed in terms of probability</title>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">W</forename><surname>Brier</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Monthly Weather Review</title>
		<imprint>
			<biblScope unit="volume">78</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1" to="3" />
			<date type="published" when="1950">1950</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">Base pretrained models and datasets in pytorch</title>
		<author>
			<persName><forename type="first">A</forename><surname>Cheni</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title/>
		<author>
			<persName><forename type="first">F</forename><surname>Chollet</surname></persName>
		</author>
		<ptr target="https://keras.io" />
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">The comparison and evaluation of forecasters</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">H</forename><surname>Degroot</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">E</forename><surname>Fienberg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of the Royal Statistical Society. Series D (The Statistician)</title>
		<imprint>
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="issue">1/2</biblScope>
			<biblScope unit="page" from="12" to="22" />
			<date type="published" when="1983">1983</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Statistical comparisons of classifiers over multiple data sets</title>
		<author>
			<persName><forename type="first">J</forename><surname>Demšar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page" from="1" to="30" />
			<date type="published" when="2006-01">Jan. 2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Improving the AUC of probabilistic estimation trees</title>
		<author>
			<persName><forename type="first">C</forename><surname>Ferri</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">A</forename><surname>Flach</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Hernández-Orallo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Machine Learning</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2003">2003</date>
			<biblScope unit="page" from="121" to="132" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">On Calibration of Modern Neural Networks</title>
		<author>
			<persName><forename type="first">C</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Pleiss</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">Q</forename><surname>Weinberger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Thirty-fourth International Conference on Machine Learning</title>
		<meeting><address><addrLine>Sydney, Australia</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2017-06">jun 2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
		<idno>CoRR, abs/1512.03385</idno>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">Densely connected convolutional networks</title>
		<author>
			<persName><forename type="first">G</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">Q</forename><surname>Weinberger</surname></persName>
		</author>
		<idno>CoRR, abs/1608.06993</idno>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">Deep networks with stochastic depth</title>
		<author>
			<persName><forename type="first">G</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Sedra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">Q</forename><surname>Weinberger</surname></persName>
		</author>
		<idno>CoRR, abs/1603.09382</idno>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">Accurate uncertainties for deep learning using calibrated regression</title>
		<author>
			<persName><forename type="first">V</forename><surname>Kuleshov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Fenner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Ermon</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1807.00263</idno>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Novel decompositions of proper scoring rules for classification: Score adjustment as precursor to calibration</title>
		<author>
			<persName><forename type="first">M</forename><surname>Kull</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Flach</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Machine Learning and Knowledge Discovery in Databases (ECML-PKDD&apos;15)</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="68" to="85" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Beyond sigmoids: How to obtain well-calibrated probabilities from binary classifiers with beta calibration</title>
		<author>
			<persName><forename type="first">M</forename><surname>Kull</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">M</forename><surname>Silva Filho</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Flach</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Electron. J. Statist</title>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="5052" to="5080" />
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Verified uncertainty calibration</title>
		<author>
			<persName><forename type="first">A</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Ma</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems (NeurIPS&apos;19)</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Trainable calibration measures for neural networks from kernel mean embeddings</title>
		<author>
			<persName><forename type="first">A</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Sarawagi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">U</forename><surname>Jain</surname></persName>
		</author>
		<idno>PMLR</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 35th International Conference on Machine Learning</title>
		<editor>
			<persName><forename type="first">J</forename><surname>Dy</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">A</forename><surname>Krause</surname></persName>
		</editor>
		<meeting>the 35th International Conference on Machine Learning<address><addrLine>Stockholmsmässan, Stockholm Sweden</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2018-07">Jul 2018</date>
			<biblScope unit="volume">80</biblScope>
			<biblScope unit="page" from="10" to="15" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Gradient-based learning applied to document recognition</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Bottou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Haffner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Proceedings of the IEEE</title>
		<imprint>
			<biblScope unit="volume">86</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="2278" to="2324" />
			<date type="published" when="1998">1998</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">A simple baseline for bayesian uncertainty in deep learning</title>
		<author>
			<persName><forename type="first">W</forename><surname>Maddox</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Garipov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Izmailov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">P</forename><surname>Vetrov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">G</forename><surname>Wilson</surname></persName>
		</author>
		<idno>CoRR, abs/1902.02476</idno>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Dirichlet-based gaussian processes for large-scale calibrated classification</title>
		<author>
			<persName><forename type="first">D</forename><surname>Milios</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Camoriano</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Michiardi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Rosasco</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Filippone</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="6005" to="6015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Reliability of subjective probability forecasts of precipitation and temperature</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">H</forename><surname>Murphy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">L</forename><surname>Winkler</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of the Royal Statistical Society. Series C (Applied Statistics)</title>
		<imprint>
			<biblScope unit="volume">26</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="41" to="47" />
			<date type="published" when="1977">1977</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Obtaining well calibrated probabilities using bayesian binning</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">P</forename><surname>Naeini</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Cooper</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Hauskrecht</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI Conference on Artificial Intelligence</title>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Binary classifier calibration using an ensemble of near isotonic regression models</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">P</forename><surname>Naeini</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">F</forename><surname>Cooper</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2016 IEEE 16th International Conference on Data Mining (ICDM)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="360" to="369" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Probabilities for SV machines</title>
		<author>
			<persName><forename type="first">J</forename><surname>Platt</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Large Margin Classifiers</title>
		<editor>
			<persName><forename type="first">A</forename><surname>Smola</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">P</forename><surname>Bartlett</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">B</forename><surname>Schölkopf</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">D</forename><surname>Schuurmans</surname></persName>
		</editor>
		<imprint>
			<publisher>MIT Press</publisher>
			<date type="published" when="2000">2000</date>
			<biblScope unit="page" from="61" to="74" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Evaluating model calibration in classification</title>
		<author>
			<persName><forename type="first">J</forename><surname>Vaicenavicius</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Widmann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Andersson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Lindsten</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Roll</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Schön</surname></persName>
		</author>
		<idno>PMLR</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of Machine Learning Research</title>
		<editor>
			<persName><forename type="first">K</forename><surname>Chaudhuri</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">M</forename><surname>Sugiyama</surname></persName>
		</editor>
		<meeting>Machine Learning Research</meeting>
		<imprint>
			<date type="published" when="2019-04-18">18 Apr 2019</date>
			<biblScope unit="volume">89</biblScope>
			<biblScope unit="page" from="3459" to="3467" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Obtaining calibrated probability estimates from decision trees and naive bayesian classifiers</title>
		<author>
			<persName><forename type="first">B</forename><surname>Zadrozny</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Elkan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. 18th Int. Conf. on Machine Learning (ICML&apos;01)</title>
		<meeting>18th Int. Conf. on Machine Learning (ICML&apos;01)</meeting>
		<imprint>
			<date type="published" when="2001">2001</date>
			<biblScope unit="page" from="609" to="616" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Transforming classifier scores into accurate multiclass probability estimates</title>
		<author>
			<persName><forename type="first">B</forename><surname>Zadrozny</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Elkan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. 8th Int. Conf. on Knowledge Discovery and Data Mining (KDD&apos;02)</title>
		<meeting>8th Int. Conf. on Knowledge Discovery and Data Mining (KDD&apos;02)</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2002">2002</date>
			<biblScope unit="page" from="694" to="699" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<title level="m" type="main">Wide residual networks</title>
		<author>
			<persName><forename type="first">S</forename><surname>Zagoruyko</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Komodakis</surname></persName>
		</author>
		<idno>CoRR, abs/1605.07146</idno>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
