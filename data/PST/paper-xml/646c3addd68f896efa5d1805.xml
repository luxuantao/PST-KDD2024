<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Is Synthetic Data From Diffusion Models Ready for Knowledge Distillation?</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date type="published" when="2023-05-22">22 May 2023</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Zheng</forename><surname>Li</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Nankai University</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Yuxuan</forename><surname>Li</surname></persName>
							<email>yuxuan.li.17@ucl.ac.uk</email>
							<affiliation key="aff0">
								<orgName type="institution">Nankai University</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Penghai</forename><surname>Zhao</surname></persName>
							<email>zhaopenghai@mail.nankai.edu.cn</email>
							<affiliation key="aff0">
								<orgName type="institution">Nankai University</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Renjie</forename><surname>Song</surname></persName>
							<email>songrenjie@megvii.com</email>
							<affiliation key="aff1">
								<orgName type="department">Megvii Technology</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Xiang</forename><surname>Li</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Nankai University</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Jian</forename><surname>Yang</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Nankai University</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Is Synthetic Data From Diffusion Models Ready for Knowledge Distillation?</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2023-05-22">22 May 2023</date>
						</imprint>
					</monogr>
					<idno type="arXiv">arXiv:2305.12954v1[cs.CV]</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-01-03T09:04+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Diffusion models have recently achieved astonishing performance in generating high-fidelity photo-realistic images. Given their huge success, it is still unclear whether synthetic images are applicable for knowledge distillation when real images are unavailable. In this paper, we extensively study whether and how synthetic images produced from state-of-the-art diffusion models can be used for knowledge distillation without access to real images, and obtain three key conclusions: (1) synthetic data from diffusion models can easily lead to state-of-the-art performance among existing synthesis-based distillation methods, (2) low-fidelity synthetic images are better teaching materials, and (3) relatively weak classifiers are better teachers. Code is available at https://github.com/zhengli97/DM-KD.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Knowledge distillation <ref type="bibr" target="#b21">[22]</ref> aims to train a lightweight student model on a target dataset under the supervision of a pre-trained teacher model. Various forms <ref type="bibr" target="#b50">[51,</ref><ref type="bibr" target="#b64">65,</ref><ref type="bibr" target="#b24">25,</ref><ref type="bibr" target="#b8">9,</ref><ref type="bibr" target="#b9">10,</ref><ref type="bibr" target="#b29">30]</ref> and paradigms <ref type="bibr" target="#b67">[68,</ref><ref type="bibr" target="#b69">70,</ref><ref type="bibr" target="#b66">67,</ref><ref type="bibr" target="#b6">7,</ref><ref type="bibr" target="#b25">26,</ref><ref type="bibr" target="#b46">47]</ref> have been proposed to improve the efficiency of distillation. However, training datasets might not always be available due to security and privacy concerns, which makes existing data-dependent distillation methods no longer applicable. To address this issue, several synthesisbased distillation methods are proposed. These methods utilize either white-box teacher statistics <ref type="bibr" target="#b33">[34,</ref><ref type="bibr" target="#b7">8,</ref><ref type="bibr" target="#b62">63,</ref><ref type="bibr" target="#b14">15]</ref> or data augmentation techniques <ref type="bibr" target="#b1">[2]</ref> to generate synthetic samples that serve as proxy training datasets for distillation. By training on such synthetic data, the student model can successfully learn from the teacher model without access to real training data.</p><p>Recently, diffusion models <ref type="bibr" target="#b22">[23,</ref><ref type="bibr" target="#b51">52,</ref><ref type="bibr" target="#b48">49,</ref><ref type="bibr" target="#b47">48]</ref> are attracting increasing attention in image generation tasks. Several high-performance diffusion models, including GLIDE <ref type="bibr" target="#b39">[40]</ref>, Stable Diffusion <ref type="bibr" target="#b49">[50]</ref>, and DiT <ref type="bibr" target="#b45">[46]</ref>, have demonstrated impressive abilities to generate high-fidelity photo-realistic images at high resolutions. This leads to a natural question: Can these synthetic images be used for downstream tasks? He et al. <ref type="bibr" target="#b20">[21]</ref> made the first attempt using synthetic data to improve the zero-shot and few-shot recognition performance of a classifier. Azizi et al. <ref type="bibr" target="#b2">[3]</ref> utilized the diffusion model as a generative data augmentation method to increase the scale of existing datasets. However, to our best knowledge, few works have explored the impact of diffusion generative models on knowledge distillation.</p><p>In this paper, we aim to investigate whether and how synthetic images generated from state-of-the-art diffusion models can be utilized for knowledge distillation without access to real images. Through our research, we have identified three key findings, which are outlined below:</p><p>(3) Relatively weak classifiers are better teachers. On real datasets, knowledge distillation is typically achieved by distilling a larger teacher model (e.g., ResNet34) to a smaller student model (e.g., ResNet18). However, on synthetic datasets, we observe the opposite phenomenon, where relatively weak classifiers tend to perform better than strong classifiers. Specifically, when training ResNet34 on the synthetic dataset, using ResNet18 as the teacher model leads to a 3% improvement in performance compared to using ResNet50 as the teacher model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Work</head><p>Conditional Diffusion Models. Diffusion model <ref type="bibr" target="#b54">[55,</ref><ref type="bibr" target="#b22">23,</ref><ref type="bibr" target="#b40">41]</ref> is a type of generative model that learns to model the probability distribution of a dataset by gradually adding sampled Gaussian noise to the data to "diffuse" the data and then trying to recover the original data by denoising the noise step by step until high-quality images are obtained. Conditional diffusion models is a type of diffusion model that is conditioned on some additional information, such as a text prompt <ref type="bibr" target="#b51">[52,</ref><ref type="bibr" target="#b39">40,</ref><ref type="bibr" target="#b49">50,</ref><ref type="bibr" target="#b47">48]</ref> or a class label <ref type="bibr" target="#b23">[24,</ref><ref type="bibr" target="#b45">46]</ref>. The conditioning signal is used to guide the denoising process so that the model can generate samples conditioned on a specific input. Recent text-to-image generation models based on diffusion, such as Imagen <ref type="bibr" target="#b51">[52]</ref>, GLIDE <ref type="bibr" target="#b39">[40]</ref>, SD <ref type="bibr" target="#b49">[50]</ref>, and class label conditional model DiT <ref type="bibr" target="#b45">[46]</ref>, have demonstrated the ability to generate highly realistic and visually stunning images, highlighting the potential power of diffusion models in generative modeling. Its outstanding performance makes it a popular choice for a variety of low-level image processing tasks, such as super-resolution <ref type="bibr" target="#b27">[28,</ref><ref type="bibr" target="#b52">53]</ref>, image inpainting <ref type="bibr" target="#b55">[56,</ref><ref type="bibr" target="#b36">37,</ref><ref type="bibr" target="#b0">1]</ref>, and dehazing <ref type="bibr" target="#b42">[43]</ref>. However, the extent to which generative diffusion models can contribute to high-level tasks has yet to be well explored. Several recent studies utilize well-trained open vocabulary text-to-image diffusion models as synthetic data generators for classification tasks. For example, He et al. <ref type="bibr" target="#b19">[20]</ref> show that synthetic data generated by diffusion models can improve model pre-training, as well as zero-shot and few-shot image classification performance. Trabucco et al. <ref type="bibr" target="#b57">[58]</ref> augment images by editing them using a pre-trained text-to-image diffusion model to bring more semantic attribute diversity, leading to improvements in few-shot settings. Additionally, Azizi et al. <ref type="bibr" target="#b2">[3]</ref> demonstrate that finetuned class-label conditional diffusion models can produce high-quality, in-distribution synthetic data that, when trained alongside real data, can achieve state-of-the-art classification performance. However, to our best knowledge, few works have explored the impact of diffusion generation models on knowledge distillation.</p><p>Synthesis-based Knowledge Distillation. In recent years, knowledge distillation <ref type="bibr" target="#b21">[22]</ref> has received increasing attention from the research community, and it has been widely utilized in various vision tasks <ref type="bibr" target="#b31">[32,</ref><ref type="bibr" target="#b58">59,</ref><ref type="bibr" target="#b65">66,</ref><ref type="bibr" target="#b30">31,</ref><ref type="bibr" target="#b9">10,</ref><ref type="bibr" target="#b61">62]</ref>. Conventional distillation methods <ref type="bibr" target="#b43">[44,</ref><ref type="bibr" target="#b6">7,</ref><ref type="bibr" target="#b28">29,</ref><ref type="bibr" target="#b60">61,</ref><ref type="bibr" target="#b68">69,</ref><ref type="bibr" target="#b29">30]</ref> require labeled training sets to train the student model by transferring knowledge from a large pre-trained teacher model. However, the original training dataset might not always be available due to privacy and safety concerns, making existing data-dependent methods hard to apply to data-scarce scenarios. Syntheticbased distillation methods <ref type="bibr" target="#b33">[34,</ref><ref type="bibr" target="#b7">8,</ref><ref type="bibr" target="#b62">63,</ref><ref type="bibr" target="#b37">38,</ref><ref type="bibr" target="#b17">18,</ref><ref type="bibr" target="#b3">4,</ref><ref type="bibr" target="#b14">15,</ref><ref type="bibr" target="#b1">2]</ref> are proposed to solve the data-dependent  problem. It typically follows a distilling-by-generating paradigm <ref type="bibr" target="#b14">[15]</ref> wherein a proxy dataset will be synthesized by utilizing different generative methods. Lopes et al. <ref type="bibr" target="#b33">[34]</ref> first proposes to reconstruct the dataset from the metadata (e.g., activation statistics). DeepInversion <ref type="bibr" target="#b62">[63]</ref> further optimizes the metadata-based synthesis method by introducing a feature regularization term. FastDFKD <ref type="bibr" target="#b14">[15]</ref> proposes to speed up the generation process by using common features. In contrast to previous GANand inversion-based distillation methods, One-Image-Distill (OID) <ref type="bibr" target="#b1">[2]</ref> utilizes data augmentation to construct a proxy dataset based on a single real image. Our method stands out from previous approaches that rely on complex generative processes that are based on white-box teachers or require careful selection of the individual real image. These methods are often time-consuming and require a significant amount of effort. In contrast, our approach is simpler, more efficient, and more effective. It only requires the use of publicly available diffusion models for data synthesis.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Method</head><p>Data Generation from Diffusion Model. In recent years, diffusion models have been demonstrated to produce images of higher quality, more realistic textures, and lower FID <ref type="bibr" target="#b12">[13,</ref><ref type="bibr" target="#b49">50,</ref><ref type="bibr" target="#b39">40]</ref> than traditional GAN <ref type="bibr" target="#b16">[17]</ref> models. It gradually adds sampled Gaussian noise to an image by: q(x t |x t-1 ) = N (x t ; ? ?t x t-1 , (1 -?t )I) (1) where ?t is a hyperparameter and x t is the noised image at timestep t. When generating an image, the model ? carries out the denoising process over a given number of T timesteps. At each timestep, the model attempts to predict the sampled Gaussian noise with the following equation:</p><formula xml:id="formula_0">p ? (x t-1 |x t ) = N (? ? (x t ), ? ? (x t ))<label>(2)</label></formula><p>In this paper, our study is mainly based on three popular conditional diffusion models, i.e. DiT <ref type="bibr" target="#b45">[46]</ref>, GLIDE <ref type="bibr" target="#b39">[40]</ref> and Stable Diffusion <ref type="bibr" target="#b49">[50]</ref>, for synthetic data generation. These models incorporate a conditioning input c, which allows for the noise prediction to be conditioned on c such as:</p><formula xml:id="formula_1">p ? (x t-1 |x t , c) = N (? ? (x t |c), ? ? (x t |c))<label>(3)</label></formula><p>By introducing a noise prediction network ? to model ? ? , a simple mean squared error loss function is used for training the model:</p><formula xml:id="formula_2">L simple (?) = M SE( ? (x t ), t )<label>(4)</label></formula><p>The objective is to minimize the distance between the predicted noise ? (x t ) and the GT noise t .</p><p>More specifically, to generate images with more distinctive features related to the given conditions, a classifier-free sampling strategy is employed, which encourages a high value of p(c|x t ). p(c|x t ) can be represented in terms of p(x t |c) and p(x t ) by using Bayes' Rule as:</p><formula xml:id="formula_3">p(c|x t ) = p(x t |c) ? p(c) p(x t )<label>(5)</label></formula><p>By taking the logarithm and derivative on x t , the following equation is obtained:</p><formula xml:id="formula_4">? xt log p(c|x t ) ? ? xt log p(x t |c) -? xt log p(x t )<label>(6)</label></formula><p>Therefore, the denoising process can be directed towards removing the conditional noise by interpreting the predicted noise from the models ? as the score function:</p><formula xml:id="formula_5">? ? (x t |c) = ? (x t |?) -s ? (? t ? xt log p(c|x t )) ? ? (x t |?) + s ? ( ? (x t |c) -? (x t |?)) (7)</formula><p>where ? (x t |c) is the sampled noise predicted at timestep t with condition c, and ? (x t |?) is the unconditional predicted noise. Here, the hyperparameter s ? 1 is used to adjust the scale of the guidance, with s=1 indicating that no classifier-free guidance is employed. Additionally, ? represents a trainable "null" condition. Based on the performance comparison between DiT, Glide, and SD in Table <ref type="table">1</ref>, we default to use DiT as our synthetic data generator for knowledge distillation in this study. Data generation and student training details are presented in the supplementary material.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Method</head><p>#Syn Images Epoch Acc (%) GLIDE <ref type="bibr" target="#b39">[40]</ref> 200K 100 44.58 SD <ref type="bibr" target="#b49">[50]</ref> 200K 100 39.95 DiT <ref type="bibr" target="#b45">[46]</ref> 200K 100 54.64</p><p>Table <ref type="table">1</ref>: Comparison of three state-of-the-art diffusion models on ImageNet-1K using their default hyperparameters to generate synthetic images. "#Syn Images" represents the total number of synthetic images. We use the pre-trained ResNet18 as the teacher to train the vanilla ResNet18 student model.</p><p>Knowledge Distillation. Originally proposed by Hinton et al. <ref type="bibr" target="#b21">[22]</ref>, knowledge distillation aims to transfer the knowledge of a pretrained heavy teacher model to a lightweight student model. After the distillation, the student can master the expertise of the teacher and be used for final deployment. Specifically, the Kullback-Leibler (KL) divergence loss is utilized to match the output distribution of two models, which can be simply formulated as follows:</p><p>L kd (q t , q s ) = ? 2 KL(?(q t /? ), ?(q s /? )), (8) where q t and q s denote the logits predicted by the teacher and student. ?(?) is the softmax function and ? is the temperature hyperparameter which controls the softness of probability distribution.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Synthetic Data Knowledge Distillation Based on Diffusion Models (DM-KD</head><p>). An overview of our proposed method is illustrated in Fig. <ref type="figure" target="#fig_1">1</ref>. The framework consists of three models: the publicly available diffusion model, the pre-trained teacher model, and the untrained target student model. The diffusion model is responsible for generating specified synthetic images, denoted as x n , based on given conditions, such as class labels or text. After synthesis, we aim to distill the teacher's knowledge to the student on the synthetic dataset. Given the unlabeled synthetic training dataset D = {x n } N n=1 generated by the diffusion model, we minimize the distillation loss between teacher and student models:</p><formula xml:id="formula_6">L stu = N n L kd (f t (x n ), f s (x n )). (<label>9</label></formula><formula xml:id="formula_7">)</formula><p>where f t and f s represent the function of teacher and student, respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experiment</head><p>4.1 Settings.</p><p>Dataset. The CIFAR-100 <ref type="bibr" target="#b26">[27]</ref> dataset consists of colored natural images with 32 ? 32 pixels. The train and test sets have 50K images and 10K images respectively. ImageNet-1K <ref type="bibr" target="#b11">[12]</ref> contains 1.28M images for training, and 50K for validation, from 1K classes. We also extend our method to other datasets, including ImageNet-100 (100 category), and Flowers-102 (102 category) <ref type="bibr" target="#b41">[42]</ref>. We use the synthetic dataset to train our student model and the real validation set to evaluate the performance.</p><p>Data generation. We select the 1K classes from ImageNet-1K as the default conditions for our data synthesis. Each category generates an equal number of images, all of which have a size of 256 ? 256.</p><p>For instance, in a synthesized dataset of 200K images, every category contains 200 images. To conduct our knowledge distillation experiments on ImageNet-1K, ImageNet-100, and Flowers-102 datasets, we resize the synthetic dataset to 224 ? 224. For CIFAR-100, the synthetic training dataset contains 50K images that are resized to 32 ? 32.</p><p>Implementation details. All of the experiments are implemented by Pytorch <ref type="bibr" target="#b44">[45]</ref> and conducted on a server containing eight NVIDIA RTX 2080Ti GPUs. We follow the same training schedule as previous knowledge distillation methods <ref type="bibr" target="#b56">[57,</ref><ref type="bibr" target="#b8">9]</ref>. We use the stochastic gradient descents (SGD) as the optimizer with momentum 0.9 and weight decay 5e-4. For CIFAR-100, the initial learning rate is 0.05 and divided by 10 at 150, 180, and 210 epochs, for a total of 240 epochs. The mini-batch size is 64. For ImageNet-1K, the weight decay is 1e-4 and the batch size is 256. The initial learning rate is set to 0.1 and divided by 10 at 30, 60, and 90 epochs, for a total of 100 epochs. We set the temperature ? to 10 by default. For each dataset, we report the Top-  Data-free Distillation. Existing data-free distillation methods are all based on the white-box teacher model for distillation. These methods primarily utilize information within the white-box teacher model, such as layer statistics, to generate samples and construct training sets that approximate the original data for distillation. Previous methods have three limitations. Firstly, it requires careful of design of the generative method, which is complex and time-consuming. Secondly, it becomes ineffective when the white-box teacher model is not available and only predictions through APIs are provided. Thirdly, current methods face difficulties in scaling with larger data volumes due to the limited diversity of synthetic data <ref type="bibr" target="#b37">[38,</ref><ref type="bibr" target="#b10">11,</ref><ref type="bibr" target="#b15">16]</ref>. Our method effectively solves the above problems. By adopting the publicly available advanced diffusion model, samples can also be generated when the teacher is a black-box model. At the same time, the large-scale diffusion model can easily generate a large number of diverse high-resolution samples for distillation. In Tables <ref type="table" target="#tab_0">2</ref> and<ref type="table" target="#tab_1">3</ref>, we compare our method with mainstream data-free methods, and our method shows very competitive performance when trained on the same amount of synthetic data. By simply introducing more synthetic training samples, our method significantly improves the performance of data-free distillation by a large margin, as shown in Table <ref type="table" target="#tab_1">3</ref>.</p><p>One Image Distillation. One-Image-Distill <ref type="bibr" target="#b1">[2]</ref> first performs multiple random crops on a large image (i.e., 2560 ? 1920), and then applies data augmentation techniques to the cropped images to create a synthetic image set. The synthetic dataset contains approximately 50K images for CIFAR-100 and 1.28M images for ImageNet-1K. It's critical to carefully select the source image for One-Image-Distill since sparse images will perform much worse than dense images as mentioned in the original paper. However, our method doesn't require such detailed selection operations. We can directly generate images through the diffusion model given the target label space. As shown in Table <ref type="table" target="#tab_0">2</ref> and Table <ref type="table" target="#tab_1">3</ref>, our method shows better performance for the same or larger data volume. Note that the results in Table <ref type="table" target="#tab_0">2</ref> were not reported in the original paper, so we adopt the "Animals" image and reimplement the method based on the official code <ref type="foot" target="#foot_0">3</ref> .</p><p>Extension to other datasets. Our synthetic dataset is generated based on the 1K classes of ImageNet-1K. To verify the generalizability of our method, we extended it to other datasets, including CIFAR-100 <ref type="bibr" target="#b26">[27]</ref>, ImageNet-100 <ref type="bibr" target="#b11">[12]</ref>, and Flowers-102 <ref type="bibr" target="#b41">[42]</ref>. Specifically, the teacher pre-trains on the specified real dataset and then performs knowledge distillation based on our synthetic dataset. The  results are reported in Tables <ref type="table" target="#tab_0">2</ref> and<ref type="table">4</ref>, and the excellent performance on these three datasets indicates our method demonstrates good generalization to other datasets. Notably, it is surprising to find that our method achieved a great distillation performance on the fine-grained Flowers-102 dataset, even though the synthetic dataset categories do not intersect with the Flowers-102 dataset categories. There are two possible reasons for such a good generalization. First, the 1K classes of the synthetic data contain most of the common classes, enabling students to learn robust general features during training. Second, in knowledge distillation, the influence of data domain shift on students can be effectively weakened by the supervision of the pretrained teacher model. <ref type="bibr" target="#b11">[12]</ref> Objects (100) 89.6 84.4 85.9 Flowers-102 <ref type="bibr" target="#b41">[42]</ref> Flower types (102) 87.9 81.5 85.4</p><formula xml:id="formula_8">Datasets Categories Teacher One-Image [2] Ours (#Classes) ResNet18 ResNet50 ResNet50 ImageNet-100</formula><p>Table <ref type="table">4</ref>: Student accuracy on ImageNet-100, and Flowers-102 datasets. Our DM-KD demonstrates good generalization to other datasets. Notably, even when there is no intersection of categories between the synthetic dataset and the Flowers-102 dataset, our method still achieves high performance.</p><p>4.3 Low-fidelity synthetic images are better teaching materials.</p><p>When synthesizing images, two hyperparameters determine the overall fidelity of the synthetic images: the value of the classifier-free guidance scaling factor s, and the total number of sampling steps T . In general, the parameter s controls the strength of the injection of conditional distinction features.n A higher value of s produces an image with richer conditional features and higher image fidelity. Conversely, a lower value of s (where s=1 means no guidance) produces an image with lower fidelity, sometimes, even with image distortion and deformation. Another parameter, T , determines the number of denoising timesteps carried out during image generation. Typically, a larger value of T results in a more detailed image with higher fidelity, but it also increases the generation time. DiT uses s=4 and T =250 as its default parameters for high-fidelity image synthesis.</p><p>To investigate the relationship between synthetic fidelity and distillation performance, datasets are synthesized using the DiT model with varying classifier-free guidance scaling factors (s) and sampling steps (T ). Examples of different categories from the synthesized datasets with different scaling factors s and sampling steps T are visualized in Fig. <ref type="figure" target="#fig_2">2</ref>. In this paper, the fidelity of the synthesized datasets is evaluated using the ImageReward <ref type="bibr" target="#b59">[60]</ref> metric, which is the state-of-the-art in synthesis dataset evaluation. The ImageReward metric scores are presented in  Results. For image classification tasks, it is commonly believed that classification models will demonstrate better generalization ability on real data when high-fidelity, photo-realistic synthesized images are utilized as the training dataset. Existing data-free distillation methods <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b62">63]</ref> follow a similar idea by synthesizing realistic datasets for distillation. However, we find that the distillation performance of images synthesized with default parameters is suboptimal. To assess the distillation performance achieved with different synthetic datasets, we report the student accuracy in Table <ref type="table" target="#tab_4">6</ref> and Table <ref type="table" target="#tab_5">7</ref>, which correspond to the datasets in Table <ref type="table" target="#tab_2">5</ref>. Our study shows that high-fidelity images, as indicated by a high ImageReward <ref type="bibr" target="#b59">[60]</ref> score, generated with default parameters, exhibit weaker distillation performance than low-fidelity ones. By progressively decreasing the values of both the scaling factor s and sampling step T , a better distillation performance can be achieved. These findings suggest that low-fidelity images are more effective as learning materials for students during the distillation process. In addition, when setting s=1, which means that there is no classifier-free guidance in the image generation process, a significant drop in student performance is observed. This suggests that poor fidelity generated images may hinder the student model's ability to learn effectively in logit representation. Our experiments show that setting s=2 and using a sampling step of 100 can generate images with relatively low fidelity, which results in the best performance for ImageNet-1K knowledge distillation (see in Table <ref type="table" target="#tab_4">6</ref> and Table <ref type="table" target="#tab_5">7</ref>).</p><p>Scaling with more synthetic data. Based on the above findings, we proceed to evaluate the distillation performance on large-scale data by generating synthetic images of varying data amounts, ranging from 100K to 2.0M. In the following experiments, we use a scaling factor of s=2 and sampling step T =100 to construct the synthetic dataset, unless otherwise specified. Fig. <ref type="figure" target="#fig_7">5</ref> shows that the performance continues to improve as the number of synthetic images increases up to 2.0M. As the data amount increases, we observe that the performance improvement begins to plateau.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Res18 Res34 Res50</head><p>Architecture of the Teacher (a) Res18 as the student.  Data diversity. In this experiment, we aim to validate whether generating more samples brings greater diversity to the dataset and thus leads to better distillation performance. To achieve this, we fix the number of total training iterations (i.e., Data Amount?Train Epochs) and scale the training schedule based on the data volume. Our results, presented in Table <ref type="table" target="#tab_7">9</ref>, demonstrate that generating more data increases the diversity in the synthetic dataset, resulting in improved performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Relatively weak classifiers are better teachers.</head><p>Experiment setup. To validate if relatively weak classifiers are better teachers, we carefully select multiple teacher-student model pairs with different architectures, including ResNet <ref type="bibr" target="#b18">[19]</ref>, VGG <ref type="bibr" target="#b53">[54]</ref>, and ShuffleNetV2 (SNV2) <ref type="bibr" target="#b38">[39]</ref>. The experiments are conducted on 200K synthetic datasets and the students are trained by 100 epochs.</p><p>Results. When training on real datasets, it is common to use a relatively large teacher model to train the student model, such as distilling ResNet34 to ResNet18. In general, smaller teacher models often fail to achieve satisfactory distillation performance compared to larger teacher models. However, when working with synthetic datasets, we observe the opposite phenomenon: relatively weak teacher models can actually achieve better distillation performance than strong ones, as shown in Fig. <ref type="figure">3</ref> and Fig. <ref type="figure">4</ref>. Interestingly, we found that as the capacity of the teacher model increases, a significant drop in performance is observed. Specifically, when training ResNet34 on the synthetic dataset, using ResNet18 as the teacher model leads to a 3% improvement in performance compared to using ResNet50 as the teacher model. These results highlight the importance of carefully selecting a teacher model when performing knowledge distillation and suggest that using a smaller, weaker teacher model may be preferable when working with synthetic datasets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Should teachers be as weak as possible?</head><p>To answer this question, we conduct a series of experiments using three groups of teacher-student pairs with large capacity gaps, as shown in Table . 8. We choose SNV2-0.5 as the teacher to test the effect that the teacher is obviously weaker than the student. Our results show that when the teacher-student gap is large, it does not necessarily lead to better performance. In fact, our experiments suggest that using a relatively weak teacher model may be a better choice for optimizing knowledge transfer.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5">Ablation Study</head><p>Temperature hyperparameter. As discussed in previous works <ref type="bibr" target="#b21">[22,</ref><ref type="bibr" target="#b5">6,</ref><ref type="bibr" target="#b29">30]</ref>, the temperature parameter is a crucial factor in controlling the smoothness of probability distributions. It determines the level   of difficulty involved in the process and is an essential component in achieving optimal results. The temperature parameter ? has been set to different values in previous works (e.g., 3 in DeepInv <ref type="bibr" target="#b62">[63]</ref>, 8 in One-Image <ref type="bibr" target="#b1">[2]</ref>, 20 in FastDFKD <ref type="bibr" target="#b14">[15]</ref>). In Fig. <ref type="figure" target="#fig_9">6</ref>, we present the detailed distillation results obtained under different temperature values. The best result is obtained when we set ? =10. ResNet18-&gt;ResNet18 ResNet34-&gt;ResNet18 Training with hard labels. The DiT model uses class labels as input to generate images, making it possible to use these labels as hard labels to supervise the model's training. In order to investigate the potential benefits of hard label supervision for synthetic datasets, we conduct experiments as presented in Table <ref type="table" target="#tab_9">10</ref>. The experiments involved training the student model with soft labels only, hard labels only, and a combination of hard and soft labels.</p><p>For the joint hard label and soft label training, we follow the traditional distillation methods <ref type="bibr" target="#b21">[22,</ref><ref type="bibr" target="#b67">68,</ref><ref type="bibr" target="#b68">69]</ref> to weights the two losses at a 1:1 ratio. The results indicate that utilizing hard labels during training actually leads to worse performance compared to using only soft labels. This finding confirms the existence of a domain shift between the synthetic and real datasets, as mentioned in <ref type="bibr" target="#b19">[20]</ref>. However, by using the distillation method, the impact of the domain shift is largely reduced. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusion</head><p>In this paper, we systematically study whether and how synthetic images generated from state-of-theart diffusion models can be used for knowledge distillation without access to real data. Through our research, we have three key findings: <ref type="bibr" target="#b0">(1)</ref> extensive experiments demonstrate that synthetic data from diffusion models can easily achieve state-of-the-art performance among existing synthesis-based distillation methods, (2) low-fidelity synthetic images are better teaching materials for knowledge distillation, and (3) relatively weak classifiers are better teachers.</p><p>Limitations and future work. Due to limited computing resources, we were not able to conduct experiments on large models (e.g., ViT <ref type="bibr" target="#b13">[14]</ref>, Swin Transformer <ref type="bibr" target="#b32">[33]</ref>) with larger data volumes.</p><p>Besides, this paper focuses on the original sampling manner of the considered diffusion models and does not discuss the influence of the advanced sampling manners, e.g., DPM-Solver <ref type="bibr" target="#b34">[35]</ref>, DPM++ <ref type="bibr" target="#b35">[36]</ref>. We plan to discuss them in our future work.  </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: An overview of our proposed synthetic data Knowledge Distillation approach based on Diffusion Models (DM-KD). We propose to use the diffusion model to synthesize images given the target label space when the real dataset is not available. The student model is optimized to minimize the prediction discrepancy between itself and the teacher model on the synthetic dataset.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>4. 2</head><label>2</label><figDesc>Comparison to existing synthesis-based distillation methods.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: Visualized fidelity of synthetic images with different scaling factors s and sampling steps T .Increasing the value of s or T results in higher fidelity images. The example images generated with s=2 contain unrealistic elements, such as a dog with two heads or a church building that is unnaturally distorted, and their fidelity and coherence are much lower than those generated with s=4.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure 5 :</head><label>5</label><figDesc>Figure 5: Improved classification accuracy of the student model with increasing numbers of synthetic images used for distillation. Student models are trained for 100 epochs.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>Figure 6 :</head><label>6</label><figDesc>Figure 6: Grid search for values of the temperature hyperparameter. The best performance is achieved when ? =10.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 2 :</head><label>2</label><figDesc>Student accuracy on CIFAR-100 validation set.</figDesc><table><row><cell>Method</cell><cell cols="2">Syn Method</cell><cell cols="5">T:ResNet34 S:ResNet18 S:ResNet18 S:WRN16-1 S:WRN40-1 S:WRN16-2 T:VGG11 T:WRN40-2 T:WRN40-2 T:WRN40-2</cell></row><row><cell>Teacher</cell><cell>-</cell><cell></cell><cell cols="2">78.05</cell><cell>71.32</cell><cell>75.83</cell><cell>75.83</cell><cell>75.83</cell></row><row><cell>Student</cell><cell>-</cell><cell></cell><cell cols="2">77.10</cell><cell>77.10</cell><cell>65.31</cell><cell>72.19</cell><cell>73.56</cell></row><row><cell>KD</cell><cell>-</cell><cell></cell><cell cols="2">77.87</cell><cell>75.07</cell><cell>64.06</cell><cell>68.58</cell><cell>70.79</cell></row><row><cell>DeepInv [63]</cell><cell cols="2">Inversion</cell><cell cols="2">61.32</cell><cell>54.13</cell><cell>53.77</cell><cell>61.33</cell><cell>61.34</cell></row><row><cell>DAFL [8]</cell><cell cols="2">Inversion</cell><cell cols="2">74.47</cell><cell>54.16</cell><cell>20.88</cell><cell>42.83</cell><cell>43.70</cell></row><row><cell>DFQ [11]</cell><cell cols="2">Inversion</cell><cell cols="2">77.01</cell><cell>66.21</cell><cell>51.27</cell><cell>54.43</cell><cell>64.79</cell></row><row><cell>FastDFKD [15]</cell><cell cols="2">Inversion</cell><cell cols="2">74.34</cell><cell>67.44</cell><cell>54.02</cell><cell>63.91</cell><cell>65.12</cell></row><row><cell cols="3">One-Image [2] Augmentation</cell><cell cols="2">74.56</cell><cell>68.51</cell><cell>34.62</cell><cell>52.39</cell><cell>54.71</cell></row><row><cell>DM-KD (Ours)</cell><cell cols="2">Diffusion</cell><cell cols="2">76.58</cell><cell>70.83</cell><cell>56.29</cell><cell>65.01</cell><cell>66.89</cell></row><row><cell>Method</cell><cell cols="3">Syn Method</cell><cell cols="2">Data Amount Epoch</cell><cell cols="2">T: ResNet50/18 T: ResNet50/18 S: ResNet50 S: ResNet18</cell></row><row><cell cols="2">Places365+KD</cell><cell>-</cell><cell></cell><cell>1.8M</cell><cell>200</cell><cell>55.74</cell><cell>45.53</cell></row><row><cell cols="2">BigGAN [5]</cell><cell>GAN</cell><cell></cell><cell>215K</cell><cell>90</cell><cell>64.00</cell><cell>-</cell></row><row><cell cols="2">DeepInv [63]</cell><cell cols="2">Inversion</cell><cell>140K</cell><cell>90</cell><cell>68.00</cell><cell>-</cell></row><row><cell cols="2">FastDFKD [15]</cell><cell cols="2">Inversion</cell><cell>140K</cell><cell>200</cell><cell>68.61</cell><cell>53.45</cell></row><row><cell cols="4">One-Image [2] Augmentation</cell><cell>1.28M</cell><cell>200</cell><cell>66.20</cell><cell>-</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>140K</cell><cell>200</cell><cell>66.74</cell><cell>60.10</cell></row><row><cell cols="2">DM-KD (Ours)</cell><cell cols="2">Diffusion</cell><cell>200K</cell><cell>200</cell><cell>68.63</cell><cell>61.61</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>1.28M</cell><cell>200</cell><cell>72.43</cell><cell>68.25</cell></row></table><note><p>1 classification accuracy. The results are averaged over 3 runs. More training details are presented in the supplementary material.</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 3 :</head><label>3</label><figDesc>Student accuracy on ImageNet-1K validation set.</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 5 ,</head><label>5</label><figDesc>where higher scores indicate higher fidelity and human preference. In this section, both teacher and student models adopt the ResNet18 model. For the teacher model, we use the pre-trained weights on torchvision4 . The experiments are conducted on 200K synthetic datasets and the students are trained by 100 epochs for experimental efficiency.</figDesc><table><row><cell>Scaling</cell><cell></cell><cell cols="3">Sampling Step T</cell><cell></cell></row><row><cell>Factor s</cell><cell>50</cell><cell>100</cell><cell>150</cell><cell>200</cell><cell>250</cell></row><row><cell>1</cell><cell cols="5">-1.087 -1.010 -0.984 -0.966 -0.948</cell></row><row><cell>2</cell><cell cols="5">-0.332 -0.292 -0.279 -0.270 -0.264</cell></row><row><cell>3</cell><cell cols="5">-0.112 -0.093 -0.086 -0.086 -0.076</cell></row><row><cell>4</cell><cell cols="5">-0.026 -0.016 -0.011 -0.007 -0.003</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 5 :</head><label>5</label><figDesc>ImageReward<ref type="bibr" target="#b59">[60]</ref> with different classifierfree guidance scaling factors s and sampling steps T . A larger value denotes higher image fidelity.</figDesc><table><row><cell>Scaling</cell><cell></cell><cell cols="3">Sampling Step T</cell><cell></cell></row><row><cell>Factor s</cell><cell>50</cell><cell>100</cell><cell>150</cell><cell>200</cell><cell>250</cell></row><row><cell>1</cell><cell cols="5">52.46 54.44 54.50 55.03 54.90</cell></row><row><cell>2</cell><cell cols="5">56.88 57.22 57.12 57.14 57.04</cell></row><row><cell>3</cell><cell cols="5">56.02 56.12 56.28 56.25 56.09</cell></row><row><cell>4</cell><cell cols="5">54.92 54.78 54.95 54.81 54.64</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 6 :</head><label>6</label><figDesc>Student accuracy with different scaling factors s and sampling steps T . Student models are trained for 100 epochs.</figDesc><table><row><cell>Teacher</cell><cell cols="5">ResNet18 ResNet34 VGG16 ResNet50 ShuffleV2</cell></row><row><cell>Acc. (%)</cell><cell>69.75</cell><cell>73.31</cell><cell>73.36</cell><cell>76.13</cell><cell>69.36</cell></row><row><cell>Student</cell><cell cols="5">ResNet18 ResNet18 VGG11 ResNet34 ResNet50</cell></row><row><cell>Low Fidelity (s=2, T =100, reward=-0.292)</cell><cell>57.22</cell><cell>54.17</cell><cell>51.81</cell><cell>57.78</cell><cell>63.66</cell></row><row><cell>High Fidelity (s=4, T =250, reward=-0.003)</cell><cell>56.19</cell><cell>49.73</cell><cell>47.51</cell><cell>52.24</cell><cell>56.20</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 7 :</head><label>7</label><figDesc>Distillation performance for different teacher-student pairs under low-and high-fidelity synthetic images. The low-fidelity images are more effective for various teacher-student pairs.</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 8 :</head><label>8</label><figDesc>Knowledge distillation with large teacher-student capacity gaps. A relatively weak teacher with a small teacher-student gap generally leads to better performance.</figDesc><table><row><cell>69.75 57.22</cell><cell>73.31 54.17</cell><cell>76.13 51.88 Teacher Student (Res18)</cell><cell>Top-1 Accuracy (%)</cell><cell>55 60 65 70 75 80</cell><cell>69.75 61.11</cell><cell>73.31 60.19</cell><cell>76.13 57.78 Teacher Student (Res34)</cell><cell>Top-1 Accuracy (%)</cell><cell>55 60 65 70 75</cell><cell>70.37 56.61</cell><cell cols="2">73.36 56.07 Teacher Student (VGG11)</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="3">Res18 Architecture of the Teacher Res34 Res50 (b) Res34 as the student.</cell><cell></cell><cell></cell><cell cols="3">VGG11 Architecture of the Teacher VGG16 (c) VGG11 as the student.</cell></row><row><cell cols="13">Figure 3: Top-1 classification accuracy (%) of different teacher-student pairs on ImageNet dataset. Relatively</cell></row><row><cell cols="12">weak classifiers bring better distillation performance. Student models are trained for 100 epochs.</cell></row><row><cell cols="12">Teacher SNV2-0.5 Res18 Res34 SNV2-0.5 Res18 Res34 SNV2-0.5 Res18</cell></row><row><cell cols="2">Acc. (%)</cell><cell>60.55</cell><cell cols="3">69.75 73.31</cell><cell>60.55</cell><cell cols="3">69.75 73.31</cell><cell>60.55</cell><cell>69.75</cell></row><row><cell cols="2">Student</cell><cell>Res18</cell><cell cols="3">Res18 Res18</cell><cell>Res34</cell><cell cols="3">Res34 Res34</cell><cell>Res50</cell><cell>Res50</cell></row><row><cell cols="2">SynKD</cell><cell>54.03</cell><cell cols="3">57.22 54.17</cell><cell>56.83</cell><cell cols="3">61.11 60.19</cell><cell>58.11</cell><cell>62.74</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">#Syn Images</cell><cell cols="5">50K 100K 200K 400K</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">Train Epoch</cell><cell></cell><cell>400</cell><cell>200</cell><cell>100</cell><cell>50</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="7">ResNet18?ResNet18 54.84 55.00 57.22 57.69</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 9 :</head><label>9</label><figDesc>Data diversity. Generating more data samples increases the diversity of the synthetic dataset, leading to better distillation performance.</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head>Table 10 :</head><label>10</label><figDesc>Accuracy comparison of hard and soft labels. Soft labels work better than hard labels and both.</figDesc><table><row><cell>Hard Label Soft Label</cell><cell cols="3">T: ResNet18 T: ResNet34 T: VGG16 S: ResNet18 S: ResNet18 S: VGG11</cell></row><row><cell></cell><cell>57.22</cell><cell>54.17</cell><cell>51.81</cell></row><row><cell></cell><cell>42.40</cell><cell>42.58</cell><cell>41.10</cell></row><row><cell></cell><cell>56.08</cell><cell>53.61</cell><cell>50.32</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_10"><head>6 Acknowledgement</head><label>6</label><figDesc>This work was supported by the Young Scientists Fund of the National Natural Science Foundation of China (Grant No.62206134) and the Tianjin Key Laboratory of Visual Computing and Intelligent Perception (VCIP). Computation is supported by the Supercomputing Center of Nankai University (NKSC). scaling factor in the table. (2) Weak classifiers are less discriminative compared to strong classifiers and tend to produce smoother outputs. In Table 12, we compare the output variance of pretrained ResNet18 and ResNet34 teachers on the synthetic dataset. Our results indicate that for different scaling factors s, ResNet18 consistently achieves a lower variance than ResNet34. This shows that ResNet18 can produce smoother output for distillation.</figDesc><table><row><cell cols="2">Variance (10 -4 ) s=2 s=3 s=4</cell></row><row><cell>ResNet18</cell><cell>6.80 7.57 7.72</cell></row><row><cell>ResNet34</cell><cell>7.25 7.95 8.10</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_11"><head>Table 12 :</head><label>12</label><figDesc>The variance(10 -4 ) of the probability distribution output by the pretrained teacher model on the synthetic dataset. The sampling step is fixed to 100. Smaller variances represent smoother probability distributions.</figDesc><table /></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3" xml:id="foot_0"><p>https://github.com/yukimasano/single-img-extrapolating</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4" xml:id="foot_1"><p>https://pytorch.org/vision/main/models.html</p></note>
		</body>
		<back>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A Appendix</head><p>A.1 Data generation.</p><p>To generate the desired images, we utilize off-the-shelf text-to-image diffusion models such as GLIDE <ref type="bibr" target="#b39">[40]</ref> and Stable Diffusion <ref type="bibr" target="#b49">[50]</ref>. The text prompt used in these models has a fixed format of "A realistic photo of class," where "class" specifies the category of the target image. When using GLIDE, we employ the official default settings for inference, which consist of a sampling step of 100 and a classifier-free guidance scale of 3.0. Similarly, for Stable Diffusion, we utilize the official settings of a sampling step of 50 and a classifier-free guidance scale of 7.5.</p><p>DiT <ref type="bibr" target="#b45">[46]</ref> differs from the text-to-image method in that it uses class labels as input to generate synthetic images. To generate an image corresponding to a specific class in the ImageNet-1K dataset, we input the index of that class. For example, to generate an image of a Pembroke Welsh Corgi, we would input index 263.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.2 Training details.</head><p>CIFAR-100. We first resize the synthesized image of size 256 ? 256 to 32 ? 32. In addition to applying the classic distillation data augmentation scheme, i.e., random cropping and horizontal flipping, we use the CutMix <ref type="bibr" target="#b63">[64]</ref> augmentation method during training, in order to align with the One-Image <ref type="bibr" target="#b1">[2]</ref> augmentation scheme. The temperature ? is set to 10.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>ImageNet-1K.</head><p>We have two training schedules in this paper. The first is the classic distillation training strategy, which is to train 100 epochs and divide the learning rate by 10 in the 30th, 60th, and 90th epochs. We use this as the default training strategy unless otherwise stated. In Table <ref type="table">3</ref>, we adopted the second training strategy, which is the same as that of FastDFKD <ref type="bibr" target="#b14">[15]</ref>. This involves training for 200 epochs, with the learning rate divided by 10 at the 120th, 150th, and 180th epochs. Specifically, we choose the teacher model with the same structure as the student for distillation in Table <ref type="table">3</ref>. In the 5th column, we use ResNet50 as the teacher to train the student ResNet50, while in the 6th column, we use ResNet18 as the teacher to train the student ResNet18. For data augmentation, following the setting of One-Image <ref type="bibr" target="#b1">[2]</ref>, we adopt the CutMix method during training.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.3 Analysis.</head><p>When using DiT to generate images, due to the classifier-free guidance mechanism, the generated images will have a distinct appearance that aligns with the specific class. This allows the classifier to classify these generated images more easily than real images, resulting in higher class confidence, as shown in Table <ref type="table">11</ref>. However, this high confidence in the synthesized images can also result in a sharp output distribution, making it challenging for knowledge distillation to effectively transfer knowledge of class similarity. A smooth target distribution would be more effective for knowledge transfer. Both our second and third findings in Section 1 are attempts to reduce the sharpness of the distribution and create a smooth learning target for distillation. <ref type="bibr" target="#b0">(1)</ref> The goal of creating low-fidelity samples is to increase the classification difficulty of the classifier and decrease its tendency to become overconfident in predicting a certain class. As shown in Table <ref type="table">12</ref>, by gradually reducing the scaling factor s, the variance is gradually reduced, which means that the distribution generated by the teacher becomes smoother. The best performance is achieved when we set s=2, which corresponds to the lowest</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Learning sparse masks for diffusion-based image inpainting</title>
		<author>
			<persName><forename type="first">Tobias</forename><surname>Alt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pascal</forename><surname>Peter</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Joachim</forename><surname>Weickert</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Pattern Recognition and Image Analysis</title>
		<imprint>
			<date type="published" when="2022">2022</date>
			<biblScope unit="page" from="528" to="539" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">The augmented image prior: Distilling 1000 classes by extrapolating from a single image</title>
		<author>
			<persName><forename type="first">M</forename><surname>Yuki</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aaqib</forename><surname>Asano</surname></persName>
		</author>
		<author>
			<persName><surname>Saeed</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In ICLR</title>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">Synthetic data from diffusion models improves imagenet classification</title>
		<author>
			<persName><forename type="first">Shekoofeh</forename><surname>Azizi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Simon</forename><surname>Kornblith</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chitwan</forename><surname>Saharia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mohammad</forename><surname>Norouzi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><forename type="middle">J</forename><surname>Fleet</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2304.08466</idno>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Robust and resource-efficient data-free knowledge distillation by generative pseudo replay</title>
		<author>
			<persName><forename type="first">Kuluhan</forename><surname>Binici</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shivam</forename><surname>Aggarwal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nam</forename><forename type="middle">Trung</forename><surname>Pham</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Karianto</forename><surname>Leman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tulika</forename><surname>Mitra</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">Large scale gan training for high fidelity natural image synthesis</title>
		<author>
			<persName><forename type="first">Andrew</forename><surname>Brock</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jeff</forename><surname>Donahue</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Karen</forename><surname>Simonyan</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1809.11096</idno>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Revisiting label smoothing and knowledge distillation compatibility: What was missing?</title>
		<author>
			<persName><forename type="first">Keshigeyan</forename><surname>Chandrasegaran</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ngoc-Trung</forename><surname>Tran</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yunqing</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ngai-Man</forename><surname>Cheung</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2022">2022</date>
			<biblScope unit="page" from="2890" to="2916" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Online knowledge distillation with diverse peers</title>
		<author>
			<persName><forename type="first">Defang</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jian-Ping</forename><surname>Mei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Can</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yan</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chun</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="3430" to="3437" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Data-free learning of student networks</title>
		<author>
			<persName><forename type="first">Hanting</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yunhe</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chang</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhaohui</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chuanjian</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Boxin</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chunjing</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chao</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qi</forename><surname>Tian</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="3514" to="3522" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Distilling knowledge via knowledge review</title>
		<author>
			<persName><forename type="first">Pengguang</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shu</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hengshuang</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiaya</forename><surname>Jia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="5008" to="5017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<author>
			<persName><forename type="first">Yudong</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sen</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiajun</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xuwei</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Frank</forename><surname>De Hoog</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zi</forename><surname>Huang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2210.15274</idno>
		<title level="m">Improved feature distillation via projector ensemble</title>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Data-free network quantization with adversarial knowledge distillation</title>
		<author>
			<persName><forename type="first">Yoojin</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jihwan</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mostafa</forename><surname>El-Khamy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jungwon</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR Workshops</title>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="710" to="711" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Imagenet: A large-scale hierarchical image database</title>
		<author>
			<persName><forename type="first">Jia</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wei</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Li-Jia</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kai</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Li</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2009">2009</date>
			<biblScope unit="page" from="248" to="255" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Diffusion models beat gans on image synthesis</title>
		<author>
			<persName><forename type="first">Prafulla</forename><surname>Dhariwal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alexander</forename><surname>Nichol</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">NeurIPS</title>
		<imprint>
			<biblScope unit="page" from="8780" to="8794" />
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">An image is worth 16x16 words: Transformers for image recognition at scale</title>
		<author>
			<persName><forename type="first">Alexey</forename><surname>Dosovitskiy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lucas</forename><surname>Beyer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alexander</forename><surname>Kolesnikov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dirk</forename><surname>Weissenborn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaohua</forename><surname>Zhai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Thomas</forename><surname>Unterthiner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mostafa</forename><surname>Dehghani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Matthias</forename><surname>Minderer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Georg</forename><surname>Heigold</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sylvain</forename><surname>Gelly</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2010.11929</idno>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Up to 100x faster data-free knowledge distillation</title>
		<author>
			<persName><forename type="first">Gongfan</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kanya</forename><surname>Mo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xinchao</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jie</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shitao</forename><surname>Bei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Haofei</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mingli</forename><surname>Song</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<author>
			<persName><forename type="first">Gongfan</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jie</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xinchao</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chengchao</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xingen</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mingli</forename><surname>Song</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2105.08584</idno>
		<title level="m">Contrastive model inversion for data-free knowledge distillation</title>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Generative adversarial networks</title>
		<author>
			<persName><forename type="first">Ian</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jean</forename><surname>Pouget-Abadie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mehdi</forename><surname>Mirza</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bing</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><surname>Warde-Farley</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sherjil</forename><surname>Ozair</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aaron</forename><surname>Courville</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Communications of the ACM</title>
		<imprint>
			<biblScope unit="page" from="139" to="144" />
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">The knowledge within: Methods for data-free model compression</title>
		<author>
			<persName><forename type="first">Matan</forename><surname>Haroush</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Itay</forename><surname>Hubara</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Elad</forename><surname>Hoffer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Daniel</forename><surname>Soudry</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shaoqing</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="770" to="778" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">Is synthetic data from generative models ready for image recognition</title>
		<author>
			<persName><forename type="first">Ruifei</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shuyang</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xin</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chuhui</forename><surname>Xue</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wenqing</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Philip</forename><surname>Torr</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Song</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaojuan</forename><surname>Qi</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2210.07574</idno>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Is synthetic data from generative models ready for image recognition</title>
		<author>
			<persName><forename type="first">Ruifei</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shuyang</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xin</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chuhui</forename><surname>Xue</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wenqing</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Philip</forename><surname>Torr</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Song</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaojuan</forename><surname>Qi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">Distilling the knowledge in a neural network</title>
		<author>
			<persName><forename type="first">Geoffrey</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Oriol</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jeff</forename><surname>Dean</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1503.02531</idno>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Denoising diffusion probabilistic models</title>
		<author>
			<persName><forename type="first">Jonathan</forename><surname>Ho</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ajay</forename><surname>Jain</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pieter</forename><surname>Abbeel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">NeurIPS</title>
		<imprint>
			<biblScope unit="page" from="6840" to="6851" />
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Cascaded diffusion models for high fidelity image generation</title>
		<author>
			<persName><forename type="first">Jonathan</forename><surname>Ho</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chitwan</forename><surname>Saharia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">William</forename><surname>Chan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><forename type="middle">J</forename><surname>Fleet</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mohammad</forename><surname>Norouzi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tim</forename><surname>Salimans</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title level="m" type="main">Paraphrasing complex network: Network compression via factor transfer</title>
		<author>
			<persName><forename type="first">Jangho</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Seounguk</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nojun</forename><surname>Kwak</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1802.04977</idno>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Self-knowledge distillation with progressive refinement of targets</title>
		<author>
			<persName><forename type="first">Kyungyul</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Byeongmoon</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Doyoung</forename><surname>Yoon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sangheum</forename><surname>Hwang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="6567" to="6576" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<title level="m" type="main">Learning multiple layers of features from tiny images</title>
		<author>
			<persName><forename type="first">Alex</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Geoffrey</forename><surname>Hinton</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
	<note type="report_type">Technical Report</note>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Srdiff: Single image super-resolution with diffusion probabilistic models</title>
		<author>
			<persName><forename type="first">Haoying</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yifan</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Meng</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shiqi</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Huajun</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhihai</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qi</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yueting</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neurocomputing</title>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Online knowledge distillation via multi-branch diversity enhancement</title>
		<author>
			<persName><forename type="first">Zheng</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ying</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Defang</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tianren</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ning</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhigeng</forename><surname>Pan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACCV</title>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<author>
			<persName><forename type="first">Zheng</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiang</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lingfeng</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Borui</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Renjie</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lei</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jun</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jian</forename><surname>Yang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2211.16231</idno>
		<title level="m">Curriculum temperature for knowledge distillation</title>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Online knowledge distillation for efficient pose estimation</title>
		<author>
			<persName><forename type="first">Zheng</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jingwen</forename><surname>Ye</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mingli</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ying</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhigeng</forename><surname>Pan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="11740" to="11750" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Structured knowledge distillation for semantic segmentation</title>
		<author>
			<persName><forename type="first">Yifan</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ke</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chris</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zengchang</forename><surname>Qin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhenbo</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jingdong</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="2604" to="2613" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Swin transformer: Hierarchical vision transformer using shifted windows</title>
		<author>
			<persName><forename type="first">Ze</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yutong</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yue</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Han</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yixuan</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zheng</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stephen</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Baining</forename><surname>Guo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="10012" to="10022" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
		<title level="m" type="main">Data-free knowledge distillation for deep neural networks</title>
		<author>
			<persName><forename type="first">Gontijo</forename><surname>Raphael</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stefano</forename><surname>Lopes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Thad</forename><surname>Fenu</surname></persName>
		</author>
		<author>
			<persName><surname>Starner</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1710.07535</idno>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b34">
	<monogr>
		<author>
			<persName><forename type="first">Cheng</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuhao</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fan</forename><surname>Bao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jianfei</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chongxuan</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jun</forename><surname>Zhu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2206.00927</idno>
		<title level="m">Dpm-solver: A fast ode solver for diffusion probabilistic model sampling in around 10 steps</title>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b35">
	<monogr>
		<title level="m" type="main">Dpm-solver++: Fast solver for guided sampling of diffusion probabilistic models</title>
		<author>
			<persName><forename type="first">Cheng</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuhao</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fan</forename><surname>Bao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jianfei</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chongxuan</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jun</forename><surname>Zhu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2211.01095</idno>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Repaint: Inpainting using denoising diffusion probabilistic models</title>
		<author>
			<persName><forename type="first">Andreas</forename><surname>Lugmayr</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Martin</forename><surname>Danelljan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andres</forename><surname>Romero</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fisher</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Radu</forename><surname>Timofte</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Luc</forename><surname>Van Gool</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2022">2022</date>
			<biblScope unit="page" from="11461" to="11471" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<monogr>
		<title level="m" type="main">Large-scale generative data-free distillation</title>
		<author>
			<persName><forename type="first">Liangchen</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mark</forename><surname>Sandler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zi</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrey</forename><surname>Zhmoginov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrew</forename><surname>Howard</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2012.05578</idno>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Shufflenet v2: Practical guidelines for efficient cnn architecture design</title>
		<author>
			<persName><forename type="first">Ningning</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hai-Tao</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="116" to="131" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<monogr>
		<title level="m" type="main">Glide: Towards photorealistic image generation and editing with text-guided diffusion models</title>
		<author>
			<persName><forename type="first">Alex</forename><surname>Nichol</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Prafulla</forename><surname>Dhariwal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aditya</forename><surname>Ramesh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pranav</forename><surname>Shyam</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pamela</forename><surname>Mishkin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bob</forename><surname>Mcgrew</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mark</forename><surname>Chen</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2112.10741</idno>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Improved denoising diffusion probabilistic models</title>
		<author>
			<persName><forename type="first">Alexander</forename><surname>Quinn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nichol</forename></persName>
		</author>
		<author>
			<persName><forename type="first">Prafulla</forename><surname>Dhariwal</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">A visual vocabulary for flower classification</title>
		<author>
			<persName><forename type="first">M-E</forename><surname>Nilsback</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrew</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2006">2006</date>
			<biblScope unit="page" from="1447" to="1454" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Restoring vision in adverse weather conditions with patch-based denoising diffusion models</title>
		<author>
			<persName><forename type="first">Ozan</forename><surname>?zdenizci</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Robert</forename><surname>Legenstein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">PAMI</title>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Relational knowledge distillation</title>
		<author>
			<persName><forename type="first">Wonpyo</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dongju</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yan</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Minsu</forename><surname>Cho</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="3967" to="3976" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<monogr>
		<title level="m" type="main">Pytorch: An imperative style, high-performance deep learning library</title>
		<author>
			<persName><forename type="first">Adam</forename><surname>Paszke</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sam</forename><surname>Gross</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Francisco</forename><surname>Massa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Adam</forename><surname>Lerer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">James</forename><surname>Bradbury</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gregory</forename><surname>Chanan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Trevor</forename><surname>Killeen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zeming</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Natalia</forename><surname>Gimelshein</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Luca</forename><surname>Antiga</surname></persName>
		</author>
		<editor>NeurIPS</editor>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="8026" to="8037" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<monogr>
		<author>
			<persName><forename type="first">William</forename><surname>Peebles</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Saining</forename><surname>Xie</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2212.09748</idno>
		<title level="m">Scalable diffusion models with transformers</title>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Switchable online knowledge distillation</title>
		<author>
			<persName><forename type="first">Yang</forename><surname>Biao Qian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hongzhi</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Richang</forename><surname>Yin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Meng</forename><surname>Hong</surname></persName>
		</author>
		<author>
			<persName><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2022">2022</date>
			<biblScope unit="page" from="449" to="466" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<monogr>
		<title level="m" type="main">Hierarchical text-conditional image generation with clip latents</title>
		<author>
			<persName><forename type="first">Aditya</forename><surname>Ramesh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Prafulla</forename><surname>Dhariwal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alex</forename><surname>Nichol</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Casey</forename><surname>Chu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mark</forename><surname>Chen</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2204.06125</idno>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Zero-shot text-to-image generation</title>
		<author>
			<persName><forename type="first">Aditya</forename><surname>Ramesh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mikhail</forename><surname>Pavlov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gabriel</forename><surname>Goh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Scott</forename><surname>Gray</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chelsea</forename><surname>Voss</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alec</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mark</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="8821" to="8831" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">High-resolution image synthesis with latent diffusion models</title>
		<author>
			<persName><forename type="first">Robin</forename><surname>Rombach</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andreas</forename><surname>Blattmann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dominik</forename><surname>Lorenz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Patrick</forename><surname>Esser</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bj?rn</forename><surname>Ommer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<monogr>
		<author>
			<persName><forename type="first">Adriana</forename><surname>Romero</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nicolas</forename><surname>Ballas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Samira</forename><forename type="middle">Ebrahimi</forename><surname>Kahou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Antoine</forename><surname>Chassang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Carlo</forename><surname>Gatta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName><surname>Fitnets</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1412.6550</idno>
		<title level="m">Hints for thin deep nets</title>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b51">
	<monogr>
		<title level="m" type="main">Photorealistic text-toimage diffusion models with deep language understanding</title>
		<author>
			<persName><forename type="first">Chitwan</forename><surname>Saharia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">William</forename><surname>Chan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Saurabh</forename><surname>Saxena</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lala</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jay</forename><surname>Whang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Emily</forename><forename type="middle">L</forename><surname>Denton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kamyar</forename><surname>Ghasemipour</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Raphael</forename><forename type="middle">Gontijo</forename><surname>Lopes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Burcu</forename><surname>Karagol Ayan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tim</forename><surname>Salimans</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2022">2022</date>
			<publisher>NeurIPS</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<monogr>
		<title level="m" type="main">Image super-resolution via iterative refinement</title>
		<author>
			<persName><forename type="first">Chitwan</forename><surname>Saharia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jonathan</forename><surname>Ho</surname></persName>
		</author>
		<author>
			<persName><forename type="first">William</forename><surname>Chan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tim</forename><surname>Salimans</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><forename type="middle">J</forename><surname>Fleet</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mohammad</forename><surname>Norouzi</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<monogr>
		<title level="m" type="main">Very deep convolutional networks for large-scale image recognition</title>
		<author>
			<persName><forename type="first">Karen</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrew</forename><surname>Zisserman</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1409.1556</idno>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">Deep unsupervised learning using nonequilibrium thermodynamics</title>
		<author>
			<persName><forename type="first">Jascha</forename><surname>Sohl-Dickstein</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Eric</forename><surname>Weiss</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Niru</forename><surname>Maheswaranathan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Surya</forename><surname>Ganguli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<monogr>
		<author>
			<persName><forename type="first">Yang</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jascha</forename><surname>Sohl-Dickstein</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Diederik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Abhishek</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stefano</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ben</forename><surname>Ermon</surname></persName>
		</author>
		<author>
			<persName><surname>Poole</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2011.13456</idno>
		<title level="m">Score-based generative modeling through stochastic differential equations</title>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b56">
	<monogr>
		<title/>
		<author>
			<persName><forename type="first">Yonglong</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dilip</forename><surname>Krishnan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Phillip</forename><surname>Isola</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1910.10699</idno>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">Contrastive representation distillation. arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b57">
	<monogr>
		<title level="m" type="main">Effective data augmentation with diffusion models</title>
		<author>
			<persName><forename type="first">Brandon</forename><surname>Trabucco</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kyle</forename><surname>Doherty</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Max</forename><surname>Gurinas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ruslan</forename><surname>Salakhutdinov</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2302.07944</idno>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b58">
	<analytic>
		<title level="a" type="main">Intra-class feature variation distillation for semantic segmentation</title>
		<author>
			<persName><forename type="first">Yukang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wei</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tao</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiang</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yongchao</forename><surname>Xu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="346" to="362" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b59">
	<monogr>
		<title level="m" type="main">Imagereward: Learning and evaluating human preferences for text-to-image generation</title>
		<author>
			<persName><forename type="first">Jiazheng</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiao</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuchen</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuxuan</forename><surname>Tong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qinkai</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ming</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jie</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuxiao</forename><surname>Dong</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2304.05977</idno>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b60">
	<analytic>
		<title level="a" type="main">Knowledge distillation via softmax regression representation learning</title>
		<author>
			<persName><forename type="first">Jing</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Brais</forename><surname>Martinez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Adrian</forename><surname>Bulat</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Georgios</forename><surname>Tzimiropoulos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b61">
	<monogr>
		<author>
			<persName><forename type="first">Zhendong</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhe</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ailing</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zexian</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chun</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yu</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><surname>Vitkd</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2209.02432</idno>
		<title level="m">Practical guidelines for vit feature knowledge distillation</title>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b62">
	<analytic>
		<title level="a" type="main">Dreaming to distill: Data-free knowledge transfer via deepinversion</title>
		<author>
			<persName><forename type="first">Pavlo</forename><surname>Hongxu Yin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jose</forename><forename type="middle">M</forename><surname>Molchanov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhizhong</forename><surname>Alvarez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Arun</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Derek</forename><surname>Mallya</surname></persName>
		</author>
		<author>
			<persName><surname>Hoiem</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Niraj</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jan</forename><surname>Jha</surname></persName>
		</author>
		<author>
			<persName><surname>Kautz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="8715" to="8724" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b63">
	<analytic>
		<title level="a" type="main">Cutmix: Regularization strategy to train strong classifiers with localizable features</title>
		<author>
			<persName><forename type="first">Sangdoo</forename><surname>Yun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dongyoon</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Seong</forename><surname>Joon Oh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sanghyuk</forename><surname>Chun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Junsuk</forename><surname>Choe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Youngjoon</forename><surname>Yoo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="6023" to="6032" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b64">
	<monogr>
		<title level="m" type="main">Paying more attention to attention: Improving the performance of convolutional neural networks via attention transfer</title>
		<author>
			<persName><forename type="first">Sergey</forename><surname>Zagoruyko</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nikos</forename><surname>Komodakis</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1612.03928</idno>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b65">
	<analytic>
		<title level="a" type="main">Fast human pose estimation</title>
		<author>
			<persName><forename type="first">Feng</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiatian</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mao</forename><surname>Ye</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="3517" to="3526" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b66">
	<analytic>
		<title level="a" type="main">Be your own teacher: Improve the performance of convolutional neural networks via self distillation</title>
		<author>
			<persName><forename type="first">Linfeng</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiebo</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Anni</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jingwei</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chenglong</forename><surname>Bao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kaisheng</forename><surname>Ma</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="3713" to="3722" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b67">
	<analytic>
		<title level="a" type="main">Deep mutual learning</title>
		<author>
			<persName><forename type="first">Ying</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tao</forename><surname>Xiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Timothy</forename><forename type="middle">M</forename><surname>Hospedales</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Huchuan</forename><surname>Lu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="4320" to="4328" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b68">
	<analytic>
		<title level="a" type="main">Decoupled knowledge distillation</title>
		<author>
			<persName><forename type="first">Borui</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Quan</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Renjie</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yiyu</forename><surname>Qiu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiajun</forename><surname>Liang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2022">2022</date>
			<biblScope unit="page" from="11953" to="11962" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b69">
	<analytic>
		<title level="a" type="main">Knowledge distillation by on-the-fly native ensemble</title>
		<author>
			<persName><forename type="first">Xiatian</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shaogang</forename><surname>Gong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NeurIPS</title>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="7517" to="7527" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
