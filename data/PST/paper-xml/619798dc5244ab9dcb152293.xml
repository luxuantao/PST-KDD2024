<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Graph neural networks in node classification: survey and evaluation</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date type="published" when="2021-11-02">2 November 2021</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Shunxin</forename><surname>Xiao</surname></persName>
							<idno type="ORCID">0000-0003-1187-8719</idno>
						</author>
						<author>
							<persName><forename type="first">Shiping</forename><surname>Wang</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Yuanfei</forename><surname>Dai</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Wenzhong</forename><surname>Guo</surname></persName>
						</author>
						<title level="a" type="main">Graph neural networks in node classification: survey and evaluation</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2021-11-02">2 November 2021</date>
						</imprint>
					</monogr>
					<idno type="DOI">10.1007/s00138-021-01251-0</idno>
					<note type="submission">Received: 27 October 2019 / Revised: 8 September 2021 / Accepted: 23 September 2021 /</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.2" ident="GROBID" when="2023-01-01T13:37+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Deep learning</term>
					<term>Graph neural networks</term>
					<term>Node classification</term>
					<term>Convolutional mechanism</term>
					<term>Attention mechanism</term>
					<term>Autoencoder mechanism</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Neural networks have been proved efficient in improving many machine learning tasks such as convolutional neural networks and recurrent neural networks for computer vision and natural language processing, respectively. However, the inputs of these deep learning paradigms all belong to the type of Euclidean structure, e.g., images or texts. It is difficult to directly apply these neural networks to graph-based applications such as node classification since graph is a typical non-Euclidean structure in machine learning domain. Graph neural networks are designed to deal with the particular graph-based input and have received great developments because of more and more research attention. In this paper, we provide a comprehensive review about applying graph neural networks to the node classification task. First, the state-of-the-art methods are discussed and divided into three main categories: convolutional mechanism, attention mechanism and autoencoder mechanism. Afterward, extensive comparative experiments are conducted on several benchmark datasets, including citation networks and co-author networks, to compare the performance of different methods with diverse evaluation metrics. Finally, several suggestions are provided for future research based on the experimental results.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>With the rapid increase in computing resources and trainable data, the performance of neural networks has been excellent improved in various fields. In computer vision area, convolutional neural networks (CNNs) <ref type="bibr" target="#b0">[1]</ref> are widely exploited for different research directions such as image classification <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b2">3]</ref>, semantic segmentation <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b4">5]</ref> and image captioning <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b6">7]</ref>. In natural language processing area, recurrent neural networks (RNNs) <ref type="bibr" target="#b7">[8]</ref> or long short-term network (LSTM) <ref type="bibr" target="#b8">[9]</ref> are used for several important tasks including sentiment analysis <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b10">11]</ref>, machine translation <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b12">13]</ref>, question-answering system <ref type="bibr" target="#b13">[14,</ref><ref type="bibr" target="#b14">15]</ref>. In addition, deep neural networks can also be used for cancer detection <ref type="bibr" target="#b15">[16,</ref><ref type="bibr" target="#b16">17]</ref>, earthquake magnitude prediction <ref type="bibr" target="#b17">[18,</ref><ref type="bibr" target="#b18">19]</ref>, or intelligent game <ref type="bibr" target="#b19">[20]</ref>.</p><p>Deep learning architectures have achieved great success in the Euclidean domain, including image, text and audio <ref type="bibr" target="#b20">[21,</ref><ref type="bibr" target="#b21">22]</ref>. As one of the typical non-Euclidean structures in B Wenzhong Guo guowenzhong@fzu.edu.cn 1 College of Mathematics and Computer Sciences, Fuzhou University, Fuzhou, China the machine learning area, graph data have the characteristics of arbitrary size, complex topological structure and always have an unfixed node ordering <ref type="bibr" target="#b22">[23]</ref>. Therefore, it is difficult to directly exploit existing learning paradigms (e.g., convolutional or pooling operation) to graph structure data. However, graph data are a ubiquitous and essential structure in machine learning domain due to the powerful ability to represent objects and their relationships in various scenarios such as community detection <ref type="bibr" target="#b23">[24,</ref><ref type="bibr" target="#b24">25]</ref>, traffic flow prediction <ref type="bibr" target="#b25">[26,</ref><ref type="bibr" target="#b26">27]</ref>, knowledge graphs <ref type="bibr" target="#b27">[28,</ref><ref type="bibr" target="#b28">29]</ref>. It is imperative to generalize these successful neural networks to graph analysis, and more and more researchers are devoting themselves to do this task <ref type="bibr" target="#b29">[30]</ref>. As a result, graph neural networks (GNNs) have been developed rapidly and achieved a series of breakthroughs <ref type="bibr" target="#b30">[31]</ref><ref type="bibr" target="#b31">[32]</ref><ref type="bibr" target="#b32">[33]</ref>.</p><p>In the past few years, GNNs have been widely used in various graph analysis tasks, including node-focused tasks (e.g., node classification, link prediction) <ref type="bibr" target="#b33">[34]</ref><ref type="bibr" target="#b34">[35]</ref><ref type="bibr" target="#b35">[36]</ref> and graph-focused tasks (e.g., graph similarity detection, graph classification) <ref type="bibr" target="#b36">[37]</ref><ref type="bibr" target="#b37">[38]</ref><ref type="bibr" target="#b38">[39]</ref>. Among them, node classification is one of the most typical research directions of graph analysis due to the widespread application scenarios. In particular, the objective of the node classification task is to predict a particular class for each unlabeled node in the graph based on the Fig. <ref type="figure">1</ref> Illustration of semi-supervised node classification. Blue and green denote those nodes that the label is known beforehand, and gray corresponds to the unlabeled nodes. The objective is to assign each gray node a label according to the information of those colorful nodes graph information <ref type="bibr" target="#b39">[40]</ref>. For example, node classification can predict the research topic to which each article belongs in the citation networks <ref type="bibr" target="#b40">[41,</ref><ref type="bibr" target="#b41">42]</ref>. In the protein-protein interaction network, each node can be assigned several gene ontology types <ref type="bibr" target="#b42">[43,</ref><ref type="bibr" target="#b43">44]</ref>. Figure <ref type="figure">1</ref> shows the objective of semisupervised node classification in which only a few nodes have labels in the training dataset.</p><p>To provide a comparison of different algorithms in node classification, we present a review of graph neural networks based on comprehensive experiments. The contributions of this paper are shown as follows:</p><p>-This survey provides a comprehensive review of existing graph neural network models in the scene of node classification. It introduces a new taxonomy of these models and presents several popular algorithms for each category. -Several popular algorithms derived from each category are compared based on a comprehensive experiment. In detail, these algorithms are rerunning on several popular benchmark datasets with four evaluation metrics.</p><p>Besides, an objective analysis is conducted based on the experiment results. -According to the experimental analysis, several challenges of existing GNNs models are discussed and several future research directions are provided for interested researchers.</p><p>The rest of this paper is organized as follows. In Sect. 2, we first define several commonly used notations and then introduce some definitions related to node classification. Then, various graph neural network models of different categories are introduced in Sect. 3. In Sect. 4, we conduct experiments about node classification by using graph neural networks on several datasets and discuss the experimental results. Based on the experimental analysis, several open problems and future directions are provided in Sect. 5. In Sect. 6, a conclusion of this survey is conducted.  </p><formula xml:id="formula_0">I n</formula><p>The identity matrix with dimension n</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>X</head><p>The input node feature matrix</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>X i</head><p>The input feature of node v i</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A</head><p>The adjacency matrix</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D</head><p>The diagonal degree matrix</p><formula xml:id="formula_1">X T The transpose matrix of X X i, j</formula><p>The i-th row, j-th column element</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>L</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A loss function σ (•)</head><p>A</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>activation function N (•)</head><p>A Gaussian distribution function</p><formula xml:id="formula_2">N (i) The neighbors of node v i [•||•]</formula><p>Concatenation of matrices or vectors Element-wise multiplication operation</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Notations and definitions</head><p>In this section, we provide a set of commonly used notations and present some critical definitions that will be used in this article. Specifically, we provide a number of notations summarized in Table <ref type="table" target="#tab_0">1</ref> before going further into the following contents.</p><p>The degree matrix D is derived from the corresponding adjacency matrix A and can be defined as</p><formula xml:id="formula_3">D i,i = i = j A i, j .</formula><p>(</p><p>For other variables, we use lowercase italic characters to represent scalers, bold lowercase characters to denote vectors and bold uppercase characters to represent matrices. Then, several definitions are introduced in the following.</p><formula xml:id="formula_5">Definition 1 (graph) Let G = (V, E) be a graph where V = {v 1 , v 2 , . . . , v n } is a set of nodes, E ⊆ V × V is a set of edges between nodes in V.</formula><p>Note that G can be weighted or unweighted (the element's value of A is either 0 or 1), directed or undirected. In this paper, we focus on dealing with undirected graphs, which are augmented with the node attribute feature X = {X 1 , X 2 , . . . , X n }, where X i is the corresponding feature vector of node v i . In addition, for unattributed graphs, one-hot encoding is used as the features for all nodes, i.e., X = I. Definition 2 (K-hop neighbors) Let G = (V, E) be a graph, the k-hop neighbors of v i denote the set of nodes which are exactly k hops away from v i and is formulated as</p><formula xml:id="formula_6">N k (i) = {v j |i = j, min(sp(i, j), K ) = k, ∀v j ∈ V}.</formula><p>(</p><p>Here, sp(i, j) denotes the shortest path between v i and v j , and it will be infinite when there is no edge between them. K represents the maximum number of hops.</p><p>Definition 3 (node classification) Let G = (V, E) be a graph, the objective of node classification is to produce labels for each node which can be formed as y = {y 1 , y 2 , . . . , y n } or Y = y 1 , y 2 , . . . , y n for multi-labels classification.</p><p>Note that the information of graph structure, node features, edge connections can be helpful to complete the node classification task.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Graph neural networks</head><p>In this section, we provide a comprehensive overview of various graph neural networks, which can be used for node classification, through three categories: convolutional mechanism, attention mechanism, and autoencoder mechanism. Several popular algorithms of each category are introduced.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Convolutional mechanism</head><p>Graph convolutional mechanism is one of the most commonly used information aggregation paradigms in graph analysis. The basic idea of this mechanism is that it utilizes convolutional or pooling operations on graph structure to extract higher representation for each node and then used in node classifier. GNNs models based on this mechanism are denoted as graph convolutional networks (GCNs), which are unrelated to node order and different from CNNs on images.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.1">ChebNet</head><p>In order to generalize operators of CNNs to graph domain, Defferrard et al. <ref type="bibr" target="#b44">[45]</ref> proposed a spectral-based graph convolutional network called ChebNet, which contains a fast localized spectral graph filter derived from Chebyshev polynomial. Specifically, ChebNet includes three main steps: the design of localized convolutional filters, a graph coarsening procedure, and a graph pooling operation.</p><p>Graph Laplacian is an important operator of spectral graph analysis <ref type="bibr" target="#b45">[46]</ref> and can be defined as L = D − A ∈ R n×n . In addition, L has a normalized form which is formed as</p><formula xml:id="formula_8">L = I n − D − 1 2 AD − 1 2 . (<label>3</label></formula><formula xml:id="formula_9">)</formula><p>The diagonalized form of graph Laplacian is defined as L = UΛU T , where Λ is the diagonal matrix of eigenvalues derived from L and U denotes the Fourier basis. diag(•) denotes a diagonal matrix derived from the input matrix or vector.</p><p>Then, a spectral filter g θ is designed to filter the input signal x ∈ R n (each element associate with a node), which is shown as follows:</p><formula xml:id="formula_10">g θ (L)x = g θ (UΛU T )x = Ug θ (Λ)U T x.<label>(4)</label></formula><p>Here, g θ (Λ) = diag(θ ) is a nonparametric filter, θ ∈ R n denotes the Fourier coefficients, and U T x ∈ R n represents the graph Fourier transform of x. However, the nonparametric filter g θ cannot localize in space and is complex to learn. To alleviate these problems, Defferrard et al. <ref type="bibr" target="#b44">[45]</ref> introduce a polynomial filter and compute g θ (L) in a recursive formulation based on the Chebyshev polynomial function. As a result, the filter can be parametrized as</p><formula xml:id="formula_11">g θ (Λ) = K −1 k=0 θ k T k ( Λ),<label>(5)</label></formula><p>where Λ = 2Λ/λ max −I n denotes a diagonal matrix which all elements in the range [−1, 1], λ max represents the maximum eigenvalue of L and the parameter θ ∈ R K denotes all coefficients of the Chebyshev polynomial T k (x). In detail, T k (x) can be computed in a recursive way</p><formula xml:id="formula_12">T k (x) = 2xT k−1 (x) − T k−2 (x)</formula><p>, where T 0 (x) = 1 and T 1 (x) = x. Thus, Eq. ( <ref type="formula" target="#formula_10">4</ref>) can be reformed as follows:</p><formula xml:id="formula_13">g θ (L)x = K −1 k=0 θ k T k ( L)x,<label>(6)</label></formula><p>where L = 2L/λ max − I n is a scaled graph Laplacian and can then be used to evaluate the k-th order Chebyshev polynomial T k ( L) ∈ R n×n . Thus, the central vertex depends on its K -hop neighbors since Eq. ( <ref type="formula" target="#formula_13">6</ref>) is a K -order polynomial in the Laplacian. Benefit from Eq. ( <ref type="formula" target="#formula_13">6</ref>), ChebNet bypasses the computation of the graph Fourier basis. Furthermore, Defferrard et al. <ref type="bibr" target="#b44">[45]</ref> denoted x k = T k ( L)x ∈ R n and the hidden state is recursively updated as x k = Fig. <ref type="figure" target="#fig_6">2</ref> Illustration of the iterative process used in ChebNet. The initial states x 0 and x 1 is set by x 0 = x and x 1 = Lx, respectively. The final representation x K is obtained by several iterative computations 2 Lx k−1 − x k−2 , where x 0 = x and x 1 = Lx. This iterative process is shown in Fig. <ref type="figure" target="#fig_6">2</ref>. Finally, the complete filter can be defined as z = g θ (L)x = x 0 , x 1 , . . . , x K −1 θ which contains O(K m) operations.</p><p>To provide a meaningful graph for pooling operation, similar nodes should be clustered together where this process can be regarded as graph clustering, which is NP-hard. Thus, Defferrard et al. <ref type="bibr" target="#b44">[45]</ref> utilized the Graclus multi-level clustering algorithm <ref type="bibr" target="#b46">[47]</ref> to aggregate similar nodes and generate a coarsening graph. However, the order of vertices is not arranged in a meaningful way after coarsening. Thus, it needs to store all matched vertices when applying pooling operation, which would cause inefficient memory and computation. In order to accelerate the pooling operations, Defferrard et al. <ref type="bibr" target="#b44">[45]</ref> proposed a pooling strategy that has the same efficiency as a 1D pooling. In detail, the procedure contains two steps: (i) construct a balanced binary tree and (ii) rearrange the nodes. As a result, it makes the pooling fast and suitable for parallel architectures.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.2">GCN</head><p>Instead of using the information derived from K -hop neighbors to represent each node introduced in ChebNet [45], Kipf and Welling <ref type="bibr" target="#b47">[48]</ref> also propose a spectral-based graph convolutional network (GCN) that encapsulates the hidden representation of each target node by aggregating the feature information of its first-order approximate neighbors <ref type="bibr" target="#b48">[49]</ref>. Then, a deep neural network model is built by stacking such graph convolutional layers multi-times and is then used to obtain the final hidden representation of each node. As a result, the learned representation also contains information of its multi-hop neighbors similar to ChebNet <ref type="bibr" target="#b44">[45]</ref>.</p><p>Specifically, Kipf and Welling <ref type="bibr" target="#b47">[48]</ref> set the number of hops as K = 1. Therefore, Eq. ( <ref type="formula" target="#formula_13">6</ref>) becomes a linear function and is reformed as</p><formula xml:id="formula_14">z = θ 0 T 0 ( L)x + θ 1 T 1 ( L)x = θ 0 x + θ 1 2 λ max L − I n x,<label>(7)</label></formula><p>with only two parameters θ 0 and θ 1 . Furthermore, Kipf and Welling <ref type="bibr" target="#b47">[48]</ref> set the maximum eigenvalue as a constant λ max = 2 and used a unique parameter θ = θ 0 = −θ 1 to address the problem of overfitting on the local structure of a graph and minimize the number of operations. Under this setting and Eqs. (3), ( <ref type="formula" target="#formula_14">7</ref>) is reformed as</p><formula xml:id="formula_15">z = θ I n + D − 1 2 AD − 1 2 x.<label>(8)</label></formula><p>Here, all eigenvalues of expression I n +D − 1 2 AD − 1 2 are lying in [0, 2] and the parameters of the filter are shared by all layers.</p><p>Note that stacking such convolutional operator to build a deep neural network model might produce some problems such as numerical instabilities and exploding/vanishing gradients. To solve these problems, Kipf and Welling <ref type="bibr" target="#b47">[48]</ref> used a renormali zation trick strategy for the above expression as</p><formula xml:id="formula_16">I n + D − 1 2 AD − 1 2 → D− 1 2 Ã D− 1 2 , (<label>9</label></formula><formula xml:id="formula_17">)</formula><p>where Ã = A + I n is the adjacency matrix of the graph with a self-connection. Similar to Eq. ( <ref type="formula" target="#formula_4">1</ref>), Dii = j Ãi j is the degree matrix with respect to Ã.</p><p>Then, Kipf and Welling <ref type="bibr" target="#b47">[48]</ref> generalized the definition of Eq. ( <ref type="formula" target="#formula_16">9</ref>), which is used for input signal x ∈ R n with only one channel, to the situation where each signal has multiple channels X ∈ R n×d . Here, d denotes the number of input channels (i.e., the dimension of node feature vector). To this end, the convolutional filter for signal X is defined as follows:</p><formula xml:id="formula_18">Z = D− 1 2 Ã D− 1 2 XW.<label>(10)</label></formula><p>Here, W ∈ R d× f is the matrix of filter parameters, Z ∈ R n× f is the convolved feature matrix for all nodes and f denotes the dimensional of the embedding feature.</p><p>After defining the convolutional filter of each layer, Kipf and Welling <ref type="bibr" target="#b47">[48]</ref> intent to build a multi-layer GCN with a layer-wise propagation rule:</p><formula xml:id="formula_19">H (l+1) = σ D− 1 2 Ã D− 1 2 H (l) W (l) , (<label>11</label></formula><formula xml:id="formula_20">)</formula><p>where W (l) is a trainable weighted matrix, H (l) ∈ R n×h is the matrix of hidden states for the l-th layer, and h denotes the dimension of higher representation. H (0) is initialized by using the input signal X. The orange and purple lines denote the information stream for first-hop and second-hop neighbors, respectively. As a result, the output contains both first-hop and second-hop information For semi-supervised node classification, Kipf and Welling <ref type="bibr" target="#b47">[48]</ref> proposed a two-layer GCN model, which is schematically depicted in Fig. <ref type="figure" target="#fig_1">3</ref>. In practice, Kipf and Welling <ref type="bibr" target="#b47">[48]</ref> computed Â = D−1/2 Ã D−1/2 in pre-processing phase and introduced a forward propagation model as follows: (1) , <ref type="bibr" target="#b11">(12)</ref> where W (0) ∈ R d×h is a matrix that maps the input feature to the hidden representation, W (1) ∈ R h× f is a hiddento-output matrix, and σ (•) is implemented by a softmax function.</p><formula xml:id="formula_21">Z = f (X, A) = σ Â ReLU ÂXW (0) W</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.3">GraphSAGE</head><p>However, ChebNet and GCN are inherently transductive that need all nodes are existing during the training process and cannot generalize to previously unseen nodes. In order to enable a model to become inductive that has the ability to deal with those unseen nodes, Hamilton et al. <ref type="bibr" target="#b49">[50]</ref> proposed a spatial-based graph convolutional network called Graph-SAGE (SAmple and aggreGatE), which utilizes both the feature information of nodes (e.g., the TF-IDF feature when one node represents for one document) and the structural attributes of a node's local neighborhood to learn a higher representation for each node. Instead of training different hidden representations for each node separately, Hamilton et al. <ref type="bibr" target="#b49">[50]</ref> designed a group of aggregators to learn embeddings by combining the information of nearby nodes of the current central node. Then, these Here, the maximum number of neighbors for each hop is set as one and the maximum hop is set as two. The orange and purple edges denote different aggregators. After sampling, the information of white nodes are not used in the aggregation stage aggregators are combined to form the forward propagation algorithm of GraphSAGE. There are existing K aggregators, which can be denoted as AGGREGATE k , ∀k ∈ {1, . . . , K }, and K parameter matrices W k , ∀k ∈ {1, . . . , K }, which are used as the transformer between different hops.</p><p>In order to obtain the hidden state h k i of each target node v i in each step k ∈ {1, . . . , K } , Hamilton et al. <ref type="bibr" target="#b49">[50]</ref> firstly used AGGREGATE k to produce the aggregated neighborhood vector h k N (i) by using the information of all immediate neighbors of v i which was produced in the previous time step. Then, h k N (i) is concatenated with the previous hidden state h k−1 i of node v i , and this concatenated vector is then transformed by W k with an activation function to form the current state of the target node v i . Repeats the above process, the final feature representation of each node Z i is produced in the K -th step.</p><p>In order to reduce the complexity of GraphSAGE and scale to a large graph, Hamilton et al. <ref type="bibr" target="#b49">[50]</ref> utilized a sample strategy to choose a fixed-size set of neighbors for each target node. The illustration of the GraphSAGE model with a sampling strategy is shown in Fig. <ref type="figure" target="#fig_2">4</ref>.</p><p>Therefore, the complexity of GraphSAGE becomes O K k=1 S k , where S k denotes the size of the neighbor's set in the k-th step.</p><p>Additionally, Hamilton et al. <ref type="bibr" target="#b49">[50]</ref> introduced three different aggregators, including mean aggregator, LSTM aggregator and pooling aggregator. In the mean aggregator, the aggregated neighbor vector h k N (i) is derived by simply utilizing the element-wise mean operation on the hidden state of neighbor h k−1 j , ∀v j ∈ N (i) , which can be formed as</p><formula xml:id="formula_22">h k N (i) ← σ W • MEAN h k−1 j , ∀v j ∈ N (i) (13)</formula><p>Equation ( <ref type="formula">13</ref>) is similar to the convolutional operator in the GCN <ref type="bibr" target="#b47">[48]</ref> and can be modified to an inductive variant from the original transductive framework. In the LSTM aggregator, Hamilton et al. <ref type="bibr" target="#b49">[50]</ref> simply used an unordered sequence of node's neighbors as the input of an LSTM model. In the pooling method, an element-wise max-pooling operator is used to gather information for each node, which is shown as follows:</p><formula xml:id="formula_23">h k N (i) ← max σ W pool h k j + b , ∀v j ∈ N (i) , (<label>14</label></formula><formula xml:id="formula_24">)</formula><p>where max represents the pooling operator. Note that the mean and pooling aggregator are symmetric means that they are permutation invariant.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.4">APPNP</head><p>Recently, Xu et al. <ref type="bibr" target="#b50">[51]</ref> proposed an algorithm to reduce the complexity of GNNs by using the relationship between a random walk algorithm and a common message passing algorithm. However, this model considers a graph as a whole and loses the ability to capture the different information of local neighbors for each starting root node. Instead of using the random walk, Klicpera et al. <ref type="bibr" target="#b51">[52]</ref> utilized the relationship between GCN and PageRank <ref type="bibr" target="#b52">[53]</ref> to formulate a personalized PageRank-based propagation architecture. This propagation scheme is then used to design a graph convolution-based network called personalized propagation of neural prediction (PPNP).</p><p>In detail, Klicpera et al. <ref type="bibr" target="#b51">[52]</ref> defined the personalized PageRank for each starting node v i by using a recursive formulation, which is shown as follows:</p><formula xml:id="formula_25">π ppr (R i ) = (1 − α) Âπ ppr (R i ) + αR i , (<label>15</label></formula><formula xml:id="formula_26">)</formula><p>where R i is a one-hot indicator vector that denotes the teleport vector of node v i , and α ∈ (0, 1] is the teleport (or restart) probability that can be used to control the range of neighbor for each target node. Thus, Eq. ( <ref type="formula" target="#formula_25">15</ref>) can be solved as the following form</p><formula xml:id="formula_27">π ppr (R i ) = α I n − (1 − α) Â −1 R i . (<label>16</label></formula><formula xml:id="formula_28">)</formula><p>Note that R i is helpful to preserve the information of a node's local neighbors. Further, Klicpera et al. <ref type="bibr" target="#b51">[52]</ref> defined the fully personalized PageRank matrix by stacking all π ppr (R i ) and replacing it with an identity matrix. The final representation of this PageRank matrix is shown as</p><formula xml:id="formula_29">Π ppr = α I n − (1 − α) Â −1 . (<label>17</label></formula><formula xml:id="formula_30">)</formula><p>Here, each element of Π ppr represents the influence score between two nodes. For example, the influence score of node v i on node v j is proportional to the value of Π ( ji) ppr and can be defined as</p><formula xml:id="formula_31">I (i, j) ∝ Π ( ji) ppr .</formula><p>Fig. <ref type="figure">5</ref> Illustration of the PPNP model. First, a neural network is used to generate predictions. Then, an adaptation of personalized PageRank is utilized to propagate information between samples. α represents the teleport (or restart) probability. The self-connection is not considered</p><p>The process of the PPNP model can be divided into two consecutive parts, prediction and propagation, which is illustrated by Fig. <ref type="figure">5</ref>. In the prediction stage, PPNP uses a neural network f θ with parameter set θ to generate predictions for each node based on the input node features X. In the propagation step, the predictions matrix H ∈ R n× f is used to produce the final result via the fully personalized PageRank scheme. So, the equation of PPNP can be defined as</p><formula xml:id="formula_32">Z = σ α I n − (1 − α) Â −1 H , (<label>18</label></formula><formula xml:id="formula_33">)</formula><p>where H i = f θ (x i ) and σ (•) is implemented by a softmax function. Note that the propagation matrix Â can be replaced with other forms such as A rw = AD −1 . Furthermore, the number of propagation layers is arbitrary (in fact, infinitely many) and can avoid the oversmoothing problem. The computational complexity and memory requirement of the PPNP model is O n 2 . It is inefficient and may cause the outing of memory more easily for a large graph. In order to solve this problem, Klicpera et al. <ref type="bibr" target="#b51">[52]</ref> proposed an approximate personalized propagation of neural predictions (APPNP) by replacing the full personalized PageRank with a variant of topic-sensitive PageRank <ref type="bibr" target="#b53">[54]</ref>. In APPNP, the final predictions of all nodes are calculated via some propagation layer by a recursive formulation, which is shown as</p><formula xml:id="formula_34">Z k+1 = (1 − α) ÂZ k + αH, (<label>19</label></formula><formula xml:id="formula_35">)</formula><p>where Z 0 = H = f θ (X) and the output of the last layer is followed by a softmax function. Here, the SGC model reduces the entire procedure of a multi-layer model to a single feature propagation step. However, the final output also contains both first-hop and second-hop information in this example</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.5">SGC</head><p>The development of traditional neural networks is from simplicity to complexity, e.g., from the logistic regression to CNNs for grid-like data or RNNs used in the sequence-like data. However, the computational complexity and memory requirement of most existing GNNs are always higher even in the early algorithms <ref type="bibr" target="#b47">[48]</ref> and do not follow the trend that appears in the development of neural networks used for Euclidean data. In order to reduce the complexity of existing models, Wu et al. <ref type="bibr" target="#b54">[55]</ref> proposed a linear graph neural network called simple graph convolutional (SGC) which is illustrated in Fig. <ref type="figure" target="#fig_3">6</ref>.</p><p>The SGC model holds a hypothesis that the improved performance of GCNs is derived from the usage of information aggregated from neighborhoods.</p><p>Under this assumption, Wu et al. <ref type="bibr" target="#b54">[55]</ref> removed all nonlinear transition functions except the softmax function in the original GCNs to reduce the cost of computing. Therefore, the definition of this linear network can be obtained by modifying Eq. ( <ref type="formula">12</ref>) and is shown as follows:</p><formula xml:id="formula_36">Z = σ ( Â . . . Â ÂXW 0 W 1 . . . W K ), (<label>20</label></formula><formula xml:id="formula_37">)</formula><p>where σ (•) is implemented by a softmax function. Note that Eq. ( <ref type="formula" target="#formula_36">20</ref>) can aggregate information from K -hop neighbors similar to the K -layer GCN model. Further, Wu et al. <ref type="bibr" target="#b54">[55]</ref> collapse all the weight matrices between different GCN layers into a single matrix W = W 0 W 1 . . . W K . Similarly, the repeated multiplication of Â is collapsed into K power ÂK .</p><p>After this simplify, the definition of SGC can be reformed as</p><formula xml:id="formula_38">Z = σ ( ÂK XW),<label>(21)</label></formula><p>where ÂK is a low-pass-type filter and calculated in the preprocessing stage. SGC is naturally to interpret because of the dividing of feature extraction H = ÂK X and classification Z = σ (HW). The latter is implemented by logistic regression and trained with any second-order method or stochastic gradient descent <ref type="bibr" target="#b55">[56]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Attention mechanism</head><p>Attention mechanism is one of the most useful architecture in artificial intelligence, including computer vision, graph analysis. Instead of using a constant weight, different neighbors should have variant contributions for the target node. Additionally, the number of neighbors is variant to each node. The benefits of attention mechanism are dealing with variablesized inputs and focusing on the most relevant part. Thus, it is natural to apply attention mechanism in node classification.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.1">GAT</head><p>In order to capture different importance of neighborhood for a target node when producing the higher hidden representation, Veličković et al. <ref type="bibr" target="#b56">[57]</ref> introduced an attention-based graph convolutional layer and is then used to construct arbitrary deep graph attention networks (GAT) by stacking such attention layers. On the whole, the objective of the GAT model is to produce a new set of features Z = {Z 1 , Z 2 , . . . , Z n } ∈ R n× f for all nodes by using X = {X 1 , X 2 , . . . , X n } as the model inputs. In detail, GAT firstly utilizes a learnable linear transformer to obtain the hidden representations H i = WX i ∈ R f for each node v i . Here, W ∈ R f ×d denotes the parametrized weight matrix of this transformer. Then, Veličković et al. <ref type="bibr" target="#b56">[57]</ref> designed an attentional function ATTENTION : R f ×R f → R, to compute attention weights between each pair of nodes by using the above hidden representation</p><formula xml:id="formula_39">e i j = ATTENTION H i , H j . (<label>22</label></formula><formula xml:id="formula_40">)</formula><p>Specifical, e i j denotes the importance of node v j when computing the representation of node v i . Note that, Veličković et al. <ref type="bibr" target="#b56">[57]</ref> only use the first-order appropriate neighbors in Eq. <ref type="bibr" target="#b21">(22)</ref>. In order to make the weight coefficients more comparable across different nodes, a softmax function is used to normalize e i j and the improved form is defined as</p><formula xml:id="formula_41">α i j = softmax j e i j = exp e i j v k ∈N (i) exp (e ik ) . (<label>23</label></formula><formula xml:id="formula_42">)</formula><p>In fact, Veličković et al. <ref type="bibr" target="#b56">[57]</ref> only used a single forward neural network with an activation function to compute the normalized coefficients. Thus, Eq. ( <ref type="formula" target="#formula_41">23</ref>) is reformed as Fig. <ref type="figure">7</ref> Illustration of multi-head attention mechanism used in the GAT model. Here, the number of attention is set as K = 3. The aggregated features from each head are concatenated or averaged to obtain a higher representation for each node. The average operation is only used in the output layer. The self-connection is not considered</p><formula xml:id="formula_43">α i j = exp σ a T H i H j v k ∈N (i) exp σ a T [H i H k ] , (<label>24</label></formula><formula xml:id="formula_44">)</formula><p>where a ∈ R 2 f is the weight vector of the proposed attention mechanism implemented by a single-layer feedforward network and σ (•) is implemented by function LeakyReLU.</p><p>Once the coefficients of a node's neighbors are all computed, a linear combination for all neighbors is used to obtain the final representation for each target node which is shown as follows:</p><formula xml:id="formula_45">Z i = σ ⎛ ⎝ v j ∈N (i) α i j H j ⎞ ⎠ . (<label>25</label></formula><formula xml:id="formula_46">)</formula><p>Furthermore, Veličković et al. <ref type="bibr" target="#b56">[57]</ref> applied a multi-head attention mechanism to improve the stability of the learning process, which is inspired by Vaswani et al. <ref type="bibr" target="#b57">[58]</ref>. This improved mechanism is illustrated in Fig. <ref type="figure">7</ref> and can be defined as</p><formula xml:id="formula_47">Z i = K k=1 σ ⎛ ⎝ v j ∈N (i) α k i j W k X j ⎞ ⎠ , (<label>26</label></formula><formula xml:id="formula_48">)</formula><p>where α k i j denotes the normalized coefficients computed by the k-th attention function ATTENTION k and represents the operator which concatenating the output feature of all attention functions. Note that the concatenate operator of Eq. ( <ref type="formula" target="#formula_47">26</ref>) will be replaced with an averaging operator if the multi-head attention is used in the last layer, and then, Eq. ( <ref type="formula" target="#formula_47">26</ref>) can be redefined as</p><formula xml:id="formula_49">Z i = σ ⎛ ⎝ 1 K K k=1 j∈N (i) α k i j H k j ⎞ ⎠ . (<label>27</label></formula><formula xml:id="formula_50">)</formula><p>Here, H k j = W k X j is computed by the k-th linear transformer. Additional, GAT is efficient due to the parallelized compute of the attention layer and the complexity of one layer is appropriate to O (nd f + m f ) .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.2">AGNN</head><p>Similarly, Kiran et al. <ref type="bibr" target="#b58">[59]</ref> proposed a new attention-based graph neural network (AGNN) with a dynamic and adaptive propagation layer and then used it to deal with the situation where the supervised information in the original data is scarce.</p><p>The AGNN model consists of one word-embedding layer and several attention-guided propagation layers. The front is used to map a bag-of-words representation of a document into an averaged word embedding. In the word-embedding layer, a linear transformer followed by an activation function is used to derive the initially hidden representation for each node by using the input features. Thus, the first layer of the AGNN model can be defined as H 0 = σ XW 0 , where W 0 ∈ R d×h denotes the transform matrix, and σ (•) is implemented by the rectified linear unit (ReLU) that can be calculated as ReLU(x) = max(0, x). In fact, H 0 ∈ R n×h is a hidden matrix by stacking the hidden state of all nodes.</p><p>Then, the normalized attention coefficient from node v j to node v i in the k-th layer is computed as follow</p><formula xml:id="formula_51">α k i j = exp β k cos H k i , H k j v r ∈N (i)∪{v i } exp β k cos H k i , H k r . (<label>28</label></formula><formula xml:id="formula_52">)</formula><p>Here, cos(x, y) = x T y/ x y with an L 2 norm, e.g., x and β k ∈ R is the only scalar parameter of k-th attention-guided propagation layer. Note that in Eq. ( <ref type="formula" target="#formula_51">28</ref>), the self-connected attention coefficient is also considered. Similar to the work produced by Veličković et al. <ref type="bibr" target="#b56">[57]</ref>, the coefficient of a node's neighbors is used to compute the next hidden representation of target node v i and is defined as</p><formula xml:id="formula_53">H k+1 i = v j ∈N (i)∪{v i } α k i j H k j . (<label>29</label></formula><formula xml:id="formula_54">)</formula><p>Since the whole AGNN model consists of several same propagation layer, Kiran et al. <ref type="bibr" target="#b58">[59]</ref> propose the following layer-wise propagation rule based on a recursive formulation H k+1 = P k H k , where P k ∈ R n×n is the coefficient matrix derived from the k-th propagation layer. Finally, the output layer consists of a linear transformer and an activation function to produce the final representation for each node</p><formula xml:id="formula_55">Z = f (X, A) = σ H K W 1 , (<label>30</label></formula><formula xml:id="formula_56">)</formula><p>where W 1 ∈ R h× f is the transformation weight matrix, K denotes the number of attentional propagation layers, and σ (•) is implemented by softmax function. Note that W 0 , W 1 , and β (k) are the parameters that need to learn in the training process.</p><p>Instead of using all nodes in a graph to compute attention coefficients <ref type="bibr" target="#b59">[60,</ref><ref type="bibr" target="#b60">61]</ref>, Kiran et al. <ref type="bibr" target="#b58">[59]</ref> only used the first-order neighbors similar to the GAT model <ref type="bibr" target="#b56">[57]</ref>. The illustration of AGNN is depicted in Fig. <ref type="figure" target="#fig_4">8</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Autoencoder mechanism</head><p>Autoencoder mechanism is one of the most commonly used unsupervised technology to learn a low-dimensional embedding from large unlabeled training data. Moreover, there is existing a large number of node classification data with scarce labeled nodes. Thus, it is essential to utilize this mechanism to learn a higher representation for all nodes. <ref type="bibr" target="#b61">[62]</ref> firstly employed the variational autoencoder (VAE) <ref type="bibr" target="#b62">[63]</ref> to deal with the graph-structured data and proposed an unsupervised learning algorithm variational graph autoencoder (VGAE). The framework of VGAE can be separated into two parts: the inference model (encoder) and the generative model (decoder). denotes the importance from node v j to v i in the k-th layer. The representation of target node is obtained by combining its neighborhoods with various weights in different layers. The self-connection is not considered</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.1">VGAE</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Kipf and Welling</head><p>In detail, the inference model is implemented by only using a two-layer GCN model <ref type="bibr" target="#b47">[48]</ref> and can be defined as</p><formula xml:id="formula_57">q(Z|X, A) = n i=1 q (Z i |X, A) ,<label>(31)</label></formula><p>where q</p><formula xml:id="formula_58">(Z i |X, A) = N Z i |μ i , Σ i , μ i ∈ R f is the mean vector, Σ i ∈ R f × f</formula><p>denotes the covariance matrix of above Gaussian distribution. Z i denotes the stochastic latent variable for the i-th node and is used to construct the latent matrix Z ∈ R n× f . Specifically, μ i and Σ i are obtained by using the GCN layer which is defined as μ i = GCN μ (X i , A) and log Σ i = GCN Σ (X i , A), respectively. Then, the definition of this two-layer GCN model is shown as follows:</p><formula xml:id="formula_59">GCN(X, A) = Åσ ÅXW 0 W 1 ,<label>(32)</label></formula><p>where Å = D − 1 2 AD − 1 2 denotes the symmetrically normalized adjacency matrix and σ (•) is implemented by a ReLU function; W 0 and W 1 are the learnable weight matrices and W 0 is shared between GCN μ (X i , A) and GCN Σ (X i , A).</p><p>In the generative process, Kipf and Welling <ref type="bibr" target="#b61">[62]</ref> simply employed an inner product operation between different learned latent variables to implement the decoder and can be formed as:</p><formula xml:id="formula_60">p A i j = 1|Z i , Z j = σ Z T i Z j . (<label>33</label></formula><formula xml:id="formula_61">)</formula><p>Here, σ (•) is implemented by a sigmoid activation function sigmoid(x) = 1/ 1 + e −x . Finally, the goal of VGAE is to minimize the variational lower bound:</p><formula xml:id="formula_62">L = E q(Z|X,A) [log p(A|Z)] − KL[q(Z|X, A) p(Z)], (<label>34</label></formula><formula xml:id="formula_63">)</formula><p>with a Gaussian prior distribution</p><formula xml:id="formula_64">p(Z) = n i=1 p (Z i ) = n i=1 N (Z i |0, I n ) . (<label>35</label></formula><formula xml:id="formula_65">)</formula><p>Here, 0 is a zero vector and KL[• •] operation denotes the Kullback-Leibler (KL) divergence between two distributions. Note that, Kipf and Welling <ref type="bibr" target="#b61">[62]</ref> set A i j = 1 in Eq. <ref type="bibr" target="#b33">(34)</ref> when A is very sparse and used the renormali zation trick strategy during the training process.</p><p>Additionally, Kipf and Welling <ref type="bibr" target="#b61">[62]</ref> proposed a nonprobabilistic variant of VGAE called graph autoencoder (GAE). In the GAE model, the embeddings of all nodes are computed as same as Eq. ( <ref type="formula" target="#formula_59">32</ref>) and are then used to reconstruct A = σ ZZ T .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.2">G2G</head><p>In fact, the previous approaches represent each node by a single point vector in a low-dimensional continuous space, and it is difficult to obtain information about the uncertainty of representation. However, diverse nodes may contain different uncertainties. On the other hand, identical uncertainty is insufficient for modeling <ref type="bibr" target="#b63">[64]</ref>. As a solution to this problem, Bojchevski et al. <ref type="bibr" target="#b64">[65]</ref> proposed a new graph autoencoder model called Graph2Gauss (G2G) to represent each node as a low-dimensional Gaussian distribution which allows capturing the uncertainty of representations.</p><p>In detail, G2G uses a deep encoder f θ (X i ) to map nodes to a middle representation M i which is then used as an input to obtain the mean vector μ i = σ (μ θ (M i )) and covariance matrix Σ i = σ (Σ θ (M i )) of the Gaussian distribution, respectively. Here,</p><formula xml:id="formula_66">μ i ∈ R f , Σ i ∈ R f × f with f</formula><p>n, d, and σ (•) is implemented by ReLU or other nonlinear functions. θ are parameters that need to be trained and shared by all instances. Although μ θ (•) and Σ θ (•) are usually feed-forward neural networks, it can be CNNs or RNNs model where each node denotes an image or a sequence, respectively. Then, μ i and Σ i are pass through a Gaussian distribution to obtain the node embedding H i , which is shown as follows:</p><formula xml:id="formula_67">H i = N (μ i , Σ i ) (<label>36</label></formula><formula xml:id="formula_68">)</formula><p>In order to utilize the information of graph structure during learning node representation, Bojchevski et al. <ref type="bibr" target="#b64">[65]</ref> proposed an unsupervised personalized ranking algorithm. The algorithm is constrained to satisfy the following pairwise constraints</p><formula xml:id="formula_69">Δ(H i , H j ) &lt; Δ(H i , H j ) (<label>37</label></formula><formula xml:id="formula_70">)</formula><p>where ∀v i ∈ V, ∀v j ∈ N k (i), ∀v j ∈ N k (i), ∀k &lt; k and Δ(H i , H j ) denotes the dissimilarity measure between the Gaussian distribution embedding of v i and v j . Equation <ref type="bibr" target="#b36">(37)</ref> expresses the intuition which the 1-hop neighbors of v i should be closer to node v i than the 2-hop neighbors in the embedding space and so on. Since the dissimilarity measure Δ(H i , H j ) is defined between the Gaussian distribution of two nodes, it is natural to use the (KL) divergence similar to He et al. <ref type="bibr" target="#b63">[64]</ref> and Santos et al. <ref type="bibr" target="#b65">[66]</ref>. Specifically, Δ(H i , H j ) can be defined by an asymmetric KL divergence which is shown as follows:</p><formula xml:id="formula_71">Δ(H i , H j ) = KL(N (i) N ( j)) = 1 2 [tr(Σ −1 i Σ j ) + P − L − log det(Σ j ) det(Σ i ) ] (<label>38</label></formula><formula xml:id="formula_72">)</formula><p>where P = (μ i − μ j ) T Σ −1 i (μ i −μ j ), tr(•) denotes the trace of a matrix, and det(•) denotes the determinant of a matrix.</p><p>Since it is difficult to solve Eq. ( <ref type="formula" target="#formula_69">37</ref>), Bojchevski et al. <ref type="bibr" target="#b64">[65]</ref> introduced an energy-based method to define the energy between two nodes by using the KL divergence. Then, the objective of G2G is to optimize the following loss function</p><formula xml:id="formula_73">L = i k&lt;l v j k ∈N k (i) v j l ∈N l (i) (E i j k 2 + exp −E i j l ) = (i, j k , j l )∈D t (E i j k 2 + exp −E i j l ) (<label>39</label></formula><formula xml:id="formula_74">)</formula><p>where E i j = KL(N (i)||N ( j)) is the energy between two nodes, and D t = {(i, j k , j l )| sp(i, j k ) &lt; sp(i, j l )} is the set of all valid instances. Additionally, Bojchevski et al. <ref type="bibr" target="#b64">[65]</ref> proposed an unbiased node-anchored sampling (NaS) strategy to solve the unbalanced problem, which may occur in the original sampling strategy.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.3">DGI</head><p>The previous unsupervised learning methods used for generating node embedding are mostly based on the random walk objective. However, these approaches pay too much attention to the localized structure information <ref type="bibr" target="#b66">[67]</ref>, and the performance relies heavily on the setting of hyperparameter <ref type="bibr" target="#b67">[68]</ref>. To solve this problem, Veličković et al. <ref type="bibr" target="#b68">[69]</ref> proposed an unsupervised learning method call deep graph infomax (DGI), which is based on the mutual information between the global representation of the entire graph and the patch representation of special input. Specifically, mutual information maximization is introduced into the graph data by DGI.</p><p>In detail, Veličković et al. <ref type="bibr" target="#b68">[69]</ref> produced a high-level embedding H i for each node v i by using a graph convolutional encoder f : R n×d × R n×n → R n× f , such that H = f (X, A). Note that Veličković et al. <ref type="bibr" target="#b68">[69]</ref> referred H i as the patch representation of v i since this embedding is formed by summarizing a patch of graph information. Veličković et al. <ref type="bibr" target="#b68">[69]</ref> introduced a summary vector s to denote the global information of the entire graph. The summary vector s is computed by using a readout function R : R n×d → R d , which uses the derived patch representations as input and can be formulated as</p><formula xml:id="formula_75">s = R( f (X, A)) = R(H). (<label>40</label></formula><formula xml:id="formula_76">)</formula><p>Then, Veličković et al. <ref type="bibr" target="#b68">[69]</ref> assigned a probability score for each patch-summary pair by using a discriminator, D :</p><formula xml:id="formula_77">R d × R d → R.</formula><p>Note that this score will then be used in the objective function. Furthermore, a negative sample strategy is employed to produce the product of marginals (negative samples). Specifically, an explicit (stochastic) corruption function, C : R n×d × R n×n → R h×d ×R h×h , is used to obtain the alternative graph derived from the original graph which is shown as follows:</p><formula xml:id="formula_78">G = ( X, Ȃ) = C(X, A), (<label>41</label></formula><formula xml:id="formula_79">)</formula><p>where h denotes the number of nodes in the alternative graph G. Note that Eq. ( <ref type="formula" target="#formula_78">41</ref>) is only used for the situation where the dataset only contains one single graph. For the multigraphs situation, some other graphs of the dataset will be used as G. Thus, the negative samples can be derived from the alternative graph. Finally, in order to maximize the mutual information between H i and s, Veličković et al. <ref type="bibr" target="#b68">[69]</ref> used a noisecontrastive type objective with a standard binary crossentropy loss between the samples from the joint and the product of marginals and can be formed as</p><formula xml:id="formula_80">L = 1 n + h L pos + L neg , (<label>42</label></formula><formula xml:id="formula_81">)</formula><p>with</p><formula xml:id="formula_82">L pos = n i=1 E (X,A) log D(H i , s) ,</formula><p>and</p><formula xml:id="formula_83">L neg = h j=1 E ( X, Ȃ) log 1 − D H j , s .</formula><p>Here, the calculation of Eq. ( <ref type="formula" target="#formula_80">42</ref>) is based on the Jensen-Shannon divergence between the positive examples and the negative examples. The DGI model is illustrated in Fig. <ref type="figure">9</ref>. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experimental analysis</head><p>In this section, we compare the aforementioned graph neural networks on several popular node classification datasets. Firstly, we describe the statistic information of datasets and parameter settings in detail. Then, four commonly used evaluation metrics are introduced to evaluate the performance of various algorithms. The classification results of different methods are provided at the end.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Datasets</head><p>We conduct our experiments on several applications, including citation networks, co-author networks, Amazon networks, protein-protein interaction (PPI) networks. The detailed descriptions of each dataset are introduced in the following, and the statistic information of all datasets is summarized in Table <ref type="table">2</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.1">Transductive learning</head><p>Transductive learning means that the trained models miss the ability to generalize to unseen nodes. In this category, several standard benchmark datasets, including five citation networks, one co-author network and two Amazon networks, are used to compare. For all of these datasets, the transductive experimental setup is similar to Yang et al. <ref type="bibr" target="#b69">[70]</ref>.</p><p>In detail, Cora, Citeseer and Pubmed are firstly introduced by Sen et al. <ref type="bibr" target="#b40">[41]</ref> and DBLP is proposed by Pan et al. <ref type="bibr" target="#b70">[71]</ref>. The Cora-ML dataset is similar to Cora and was produced by Bojchevski et al. <ref type="bibr" target="#b64">[65]</ref>. In all of these citation datasets, each node represents a scientific paper and edges correspond to citations. Note that the citation networks are represented as undirected graphs according to <ref type="bibr" target="#b69">[70]</ref>. Node features are bagof-words encoded documents. The class labels represent the research domains to which each document belongs, and each node has a unique tag. For co-author graph CS, each node represents a researcher connected by an edge if they co-author a paper. Node features correspond to paper keywords for each author's papers. The labels of nodes denote the most active fields of study for each author. In Amazon networks, Computers and Photo are segments of the Amazon co-purchase graph <ref type="bibr" target="#b71">[72]</ref>, where vertices denote goods and node features correspond to elements of a bag-of-words representation of a review. If two products are frequently bought together, there is an edge between them. The class labels represent the category of each product.</p><p>In order to obtain the performance of each model in all transductive datasets, only 20 nodes for each class are used to train, and the other 1000 nodes are used in the test phase. Note that all of the node features are utilized in the unsupervised learning methods and all graphs are undirected.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.2">Inductive learning</head><p>Contrary to transductive learning, an algorithm is inductive means that it can be generalized to unseen graphs or nodes. In our experiments, a popular PPI network benchmark dataset is used in this setting.</p><p>In detail, the complete PPI dataset is provided by Zitnik and Leskovec <ref type="bibr" target="#b72">[73]</ref> and then was preprocessed again by Hamilton et al. <ref type="bibr" target="#b49">[50]</ref>. The version of PPI used in our experiments is the latter one. In this dataset, each graph corresponds to different human tissues, and nodes represent various protein functions. The features of each node are derived from positional gene sets, motif gene sets and immunological signatures. Each label represents a gene ontology set collected from the Molecular Signatures Database <ref type="bibr" target="#b42">[43]</ref>, and each node can hold several labels simultaneously. The dataset contains 20 graphs used in the training stage and two completely unobserved graphs for testing.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Experimental setup</head><p>Since most parameters of all algorithms are set according to the original papers, the validation procedure is stripped in our experiments. For all models, they have some identical settings introduced in the following. The maximum number of training epochs is 2000, and the training stage would be early stopped when the loss is not reducing in one consecutive hundred epochs. We repeatedly conduct each experiment five times and provide the values of mean and variance across all repeats. The parameters of all methods are optimized by using Adam <ref type="bibr" target="#b73">[74]</ref>. For supervised or semi-supervised learning models, the objective of model training is to minimize the cross-entropy loss. On the other hand, G2G and DGI both have a customize loss function. For instance, crossentropy and KL divergence are both used in VGAE. Similar to transductive datasets, the multi-label classification of the PPI dataset is regarded as a multi-class classification problem.</p><p>For those models in which node classification is not considered in the original paper, the initial configurations are referring to algorithms similar to their architecture. For instance, the parameter initialization of ChebNet is derived from GCN's settings, and VGAE is similar to G2G. Note that the sampling technology of GraphSAGE introduced in Sect. 3 is not used in our experiments, and the mean aggregator is used to propagate information. After the embedding matrix is obtained, a linear transform function is trained to gain the probability distribution of classes for unsupervised learning methods, including VGAE, G2G and DGI.</p><p>The major software and hardware devices used in our experiments are listed in the following. Based on the deep learning framework PyTorch 1 , an open source graph representation learning tool called PyTorch Geometric 2 (PyG) was used in our experiments. Specifically, the latest released version of PyG, 1.3.0, is used to implement all baseline methods except the G2G model. G2G is implemented by using the code provided by Bojchevski et al. <ref type="bibr" target="#b64">[65]</ref> based on the Tensor-Flow framework 3 . Note that the train data and test data used in G2G are proportional to the full dataset according to a rule in which the final size of training data is approximate to other baseline methods. In order to accelerate the training process, one graphics processing unit (GPU) card with 16 GB memory is used in all experiments. The implementations of all algorithms are available at https://github.com/Xiaoshunxin/ GNN-Survey.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Evaluation metrics</head><p>In order to compare the performance of diverse algorithms, several commonly used evaluation metrics of the classification task, including macro-precision (macro-P), macro-recall (macro-R), macro-F1 and micro-F1, are used in our experiments.</p><p>Firstly, the true positive, false positive, true negative and false negative of i-th class are denoted as T P i , F P i , T N i and F N i , respectively. Then, the definitions of these metrics are introduced in the following.</p><p>-In order to obtain the macro-precision of multi-class classification, the precision of each class should be calculated at first and can be formed as P i = T P i /(T P i + F P i ).</p><p>Then, macro-P is obtained by averaging all the precisions, 1 https://github.com/pytorch/pytorch. </p><p>where n denotes the number of classes. -Similar, the value of macro-R can be obtained by averaging all the recalls</p><formula xml:id="formula_85">macro-R = 1 n n i=1 R i , (<label>44</label></formula><formula xml:id="formula_86">)</formula><p>where R i = T P i /(T P i + F N i ) denotes the recall of the i-th class. -In order to balance macro-P and macro-R, Macro-F1 is introduced and can be formed as</p><formula xml:id="formula_87">macro-F1 = 2 × macro-P × macro-R macro-P + macro-R . (<label>45</label></formula><formula xml:id="formula_88">)</formula><p>-Since macro-F1 does not consider the sample size of each class, it is suitable to use micro-F1 when the distribution of classes is unbalanced. The definition of micro-F1 is formed as </p><formula xml:id="formula_89">micro-F1 = 2 × micro-P × micro-R micro-P + micro-R ,<label>(46)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Experimental results</head><p>In this section, the compared algorithms are tested on nine publicly available datasets with several evaluation metrics mentioned above. The macro-P, macro-R, macro-F1 and micro-F1 of all algorithms on all datasets are listed in Table <ref type="table">3</ref>.</p><p>The blank cell described by one horizontal line represents that this algorithm is essential unsuitable to current dataset except for G2G and DGI. For instance, transductive learning algorithms including GCN, VGAE, ChebNet, APPNP and SGC cannot be used on the PPI dataset because this dataset is inductive. However, the performance of G2G and DGI on the PPI dataset is not obtained because the setting of inductive learning is not provided in the source code. In addition, the SGC model and GAT model have both obtained the best</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Table 3</head><p>The performance (mean% and standard deviation%) of diverse GNNs algorithms on different datasets. The best results are marked in bold (The higher the better) Furthermore, several observations are derived from the results shown in Table <ref type="table">3</ref>. From the perspective of the algorithm, some algorithms are suitable for a special graph. For instance, GAT achieves the best performance on two transductive datasets (Cora and Photo) and one inductive benchmark (PPI) with all evaluation metrics. This observation can reflect the importance of attention mechanism in both transductive and inductive learning. SGC model obtains the minimum variances on all transductive learning datasets with all evaluation metrics. This phenomenon may reflect that the simpler a model is, the more stable it is. In addition, SGC achieves comparable performance on several datasets and obtains the best result on the CiteSeer benchmark with all evaluation metrics. It demonstrates that a simple method, which only has a few layers, may have comparable performance to complex models. G2G algorithm also achieves the best performance on three datasets (PubMed, DBLP and Cora-ML) in four criterions, except for the Macro-recall result on the DBLP dataset. This phenomenon demonstrates that not only the supervised information (e.g., node labels) is critical, but also the extensive unsupervised information (e.g., the information of graph structure) is all important for the node classification task. On the other hand, observations would be derived from the perspective of datasets. AGNN usually has a big variance on large datasets such as DBLP, Photo and Computers. In addition, the performance of different algorithms is greatly different on the same dataset. For instance, the gap between the best method and the worst network on the CiteSeer dataset is close to forty percent. It demonstrates that different algorithms may be suitable for different tasks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Datasets</head><p>The runtimes of each training epoch of different GNNs algorithms on several datasets are illustrated in Fig. <ref type="figure" target="#fig_9">10</ref>. For those models which training embeddings in an unsupervised way, only the time cost of training hidden representation for each node is considered in our experiments. We also have some observations from this figure. The training time of unsupervised learning algorithms including VGAE, G2G and DGI is longer than those semi-supervised or supervised methods on most datasets. ChebNet has a larger time cost on CS because this model is sensitive to the size of nodes set and edge set, as well as the dimension of node features. The SGC model spends very little time on all datasets because there is only one linear transformation in the training stage.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Challenges and future directions</head><p>In this section, several challenges of the development of GNNs would be introduced based on the experimental results mentioned above. Then, some potential future directions are provided for interested researchers.</p><p>Based on the observations derived from the experimental results, we can find many existing problems in previous research works.</p><p>-In transductive learning, several aforementioned graph analysis algorithms are difficultly scaled to large graphs in real-world applications such as community detection, which usually contains millions of nodes and edges. -Most of the existing graph neural networks are essential transductive learning algorithms. This means that they cannot handle nodes or graphs that never appear and are unsuitable for many applications. -Several deep learning paradigms used for graph analysis are supervised or semi-supervised learning. These methods often require lots of supervised information in the training stage to obtain a good performance. However, most of the existing graph data are unsupervised and it is difficult to apply the supervised algorithms to these unlabeled data. -The applications of node classification are mainly focused on those data in which there are existing real-world relationships between samples. However, it is a possible way to apply GNNs in different kinds of data via node classification. -Stacking several layers of graph convolution may cause over-smoothing that the learned representations of all nodes tend to be the same. As a result, the performance of graph convolution models would decrease dramatically.</p><p>Then, several potential future directions are introduced based on the challenges mentioned above.</p><p>-The ability to deal with a large graph is important in many scenes. From the perspective of computational resources, the possible solution is to design a parallelized algorithm that can run with multiple GPU cards. On the other hand, many effective sampling techniques would be introduced to reduce the scale of large graphs. -Generalizing to unseen nodes or graphs has many benefits. For instance, the model can be trained only in some parts of the whole graph and then used to predict other parts. One of the essential reasons for losing the inductive ability is that they usually apply the information aggregation on the propagation matrix. As a result, we suggest that the information aggregation should be partial instead of global. -There is a lot of unsupervised information in graph data.</p><p>Using this information can produce a meaningful representation for nodes or graphs. We can use unsupervised deep learning technologies such as autoencoder mechanism to achieve this objective. Finally, we believe that the designed GNNs model would be more accurate and more quickly by further studying these directions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Conclusion</head><p>In this paper, we introduced an overview of graph neural networks and provided a comparison among them in node classification tasks. According to the major learning paradigms, these graph analysis algorithms were divided into three categories: convolutional mechanism, attention mechanism and autoencoder mechanism. A comprehensive introduction of several popular methods for each category was provided. Furthermore, extensive comparative experiments were conducted on nine benchmark datasets to compare the performance of node classification. Finally, a number of challenges are introduced based on the experimen-tal results and several potential future directions are provided to interested researchers. In the future work, we will explore more efficient graph neural networks for node classification based on these future directions.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head></head><label></label><figDesc>An edge and e i j = (v i , v j ) ∈ E | • | The length of a list n The number of nodes and n = |V| m The number of edges and m = |E| d The dimension of input node feature f The dimension of output node vector c The number of node classes R m m-dimensional Euclidean space</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 3</head><label>3</label><figDesc>Fig.<ref type="bibr" target="#b2">3</ref> Schematic diagram of a two-layer GCN model. The dark green denotes target nodes that need to aggregate information from neighbors. The orange and purple lines denote the information stream for first-hop and second-hop neighbors, respectively. As a result, the output contains both first-hop and second-hop information</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 4</head><label>4</label><figDesc>Fig.<ref type="bibr" target="#b3">4</ref> Illustration of GraphSAGE model with a sampling strategy. Here, the maximum number of neighbors for each hop is set as one and the maximum hop is set as two. The orange and purple edges denote different aggregators. After sampling, the information of white nodes are not used in the aggregation stage</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Fig. 6</head><label>6</label><figDesc>Fig.<ref type="bibr" target="#b5">6</ref> Illustration of SGC. Here, the SGC model reduces the entire procedure of a multi-layer model to a single feature propagation step. However, the final output also contains both first-hop and second-hop information in this example</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Fig. 8</head><label>8</label><figDesc>Fig. 8 Schematic depiction of the multi-layer AGNN model. α k i j</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Fig. A</head><label></label><figDesc>Fig. A high-level overview of DGI. f represents a graph convolutional encoder. R, D, C denote the readout function, discriminator and corruption function, respectively. The hidden embedding H is used as the input of R to obtain the summary vector s</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Table 2</head><label>2</label><figDesc>Photo</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head></head><label></label><figDesc>P i + F N i ).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head></head><label></label><figDesc>Photo</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>Fig. 10</head><label>10</label><figDesc>Fig. 10 Runtime of training stage. The time cost of each training epoch of diverse GNNs algorithms on several transductive learning datasets</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1</head><label>1</label><figDesc>Summary of commonly used notations in this article. A list of notations including different variables and operations are provided in this table</figDesc><table><row><cell>Notations</cell><cell>Descriptions</cell></row></table><note>GA graphVThe set of nodes E The set of edges v i A node and v i ∈ V</note></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgements</head><p>This work is partly supported by the National Natural Science Foundation of China under Grant Nos. 61502104 and 61672159.</p></div>
			</div>

			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Declarations</head><p>Conflict of interest The authors declare that they have no conflict of interest. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Yuanfei</head></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Backpropagation applied to handwritten zip code recognition</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Boser</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">S</forename><surname>Denker</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Henderson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">E</forename><surname>Howard</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Hubbard</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">D</forename><surname>Jackel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural Comput</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="541" to="551" />
			<date type="published" when="1989">1989</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Imagenet classification with deep convolutional neural networks</title>
		<author>
			<persName><forename type="first">A</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 26th Conference on Neural Information Processing Systems</title>
				<meeting>the 26th Conference on Neural Information Processing Systems</meeting>
		<imprint>
			<date type="published" when="2012">2012</date>
			<biblScope unit="page" from="1097" to="1105" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Rethinking feature distribution for loss functions in image classification</title>
		<author>
			<persName><forename type="first">W</forename><surname>Wan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2018 IEEE Conference on Computer Vision and Pattern Recognition</title>
				<meeting>the 2018 IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="9117" to="9126" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Fully convolutional networks for semantic segmentation</title>
		<author>
			<persName><forename type="first">J</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Shelhamer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2015 IEEE Conference on Computer Vision and Pattern Recognition</title>
				<meeting>the 2015 IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="3431" to="3440" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">A novel object-based deep learning framework for semantic segmentation of very high-resolution remote sensing data: Comparison with convolutional and fully convolutional networks</title>
		<author>
			<persName><forename type="first">M</forename><surname>Papadomanolaki</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Vakalopoulou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Karantzalos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Remote Sens</title>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page">684</biblScope>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Sca-cnn: Spatial and channel-wise attention in convolutional networks for image captioning</title>
		<author>
			<persName><forename type="first">L</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Nie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Shao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">S</forename><surname>Chua</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2017 IEEE Conference on Computer Vision and Pattern Recognition</title>
				<meeting>the 2017 IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="5659" to="5667" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Convolutional image captioning</title>
		<author>
			<persName><forename type="first">J</forename><surname>Aneja</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Deshpande</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">G</forename><surname>Schwing</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2018 IEEE Conference on Computer Vision and Pattern Recognition</title>
				<meeting>the 2018 IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="5561" to="5570" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Finding structure in time</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">L</forename><surname>Elman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Cogn. Sci</title>
		<imprint>
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="179" to="211" />
			<date type="published" when="1990">1990</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Long short-term memory</title>
		<author>
			<persName><forename type="first">S</forename><surname>Hochreiter</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Schmidhuber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural Comput</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="1735" to="1780" />
			<date type="published" when="1997">1997</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Document modeling with gated recurrent neural network for sentiment classification</title>
		<author>
			<persName><forename type="first">D</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Qin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing</title>
				<meeting>the 2015 Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="1422" to="1432" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Targeted aspect-based sentiment analysis via embedding commonsense knowledge into an attentive lstm</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Cambria</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 32nd AAAI Conference on Artificial Intelligence</title>
				<meeting>the 32nd AAAI Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="5876" to="5883" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">A recursive recurrent neural network for statistical machine translation</title>
		<author>
			<persName><forename type="first">S</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Zhou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics</title>
				<meeting>the 52nd Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="1491" to="1500" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Variational recurrent neural machine translation</title>
		<author>
			<persName><forename type="first">J</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 32nd AAAI Conference on Artificial Intelligence</title>
				<meeting>the 32nd AAAI Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="5488" to="5495" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Dynamic memory networks for visual and textual question answering</title>
		<author>
			<persName><forename type="first">C</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Merity</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Socher</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 33nd International Conference on Machine Learning</title>
				<meeting>the 33nd International Conference on Machine Learning</meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="2397" to="2406" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Denoising distantly supervised open-domain question answering</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics</title>
				<meeting>the 56th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="1736" to="1745" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">An expert system for detection of breast cancer based on association rules and neural network</title>
		<author>
			<persName><forename type="first">M</forename><surname>Karabatak</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">C</forename><surname>Ince</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Expert Syst. Appl</title>
		<imprint>
			<biblScope unit="volume">36</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="3465" to="3469" />
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Mitosis detection in breast cancer histology images with deep neural networks</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">C</forename><surname>Cireşan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Giusti</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">M</forename><surname>Gambardella</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Schmidhuber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 16th International Conference on Medical Image Computing and Computer-assisted Intervention</title>
				<meeting>the 16th International Conference on Medical Image Computing and Computer-assisted Intervention</meeting>
		<imprint>
			<date type="published" when="2013">2013</date>
			<biblScope unit="page" from="11" to="418" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Neural network models for earthquake magnitude prediction using multiple seismicity indicators</title>
		<author>
			<persName><forename type="first">A</forename><surname>Panakkat</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Adeli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Int. J. Neural Syst</title>
		<imprint>
			<biblScope unit="volume">17</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="13" to="33" />
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">A probabilistic neural network for earthquake magnitude prediction</title>
		<author>
			<persName><forename type="first">H</forename><surname>Adeli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Panakkat</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural Netw</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="1018" to="1024" />
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Mastering the game of go with deep neural networks and tree search</title>
		<author>
			<persName><forename type="first">D</forename><surname>Silver</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">J</forename><surname>Maddison</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Guez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Sifre</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Van Den Driessche</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Schrittwieser</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Antonoglou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Panneershelvam</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Lanctot</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nature</title>
		<imprint>
			<biblScope unit="volume">529</biblScope>
			<biblScope unit="issue">7587</biblScope>
			<biblScope unit="page">484</biblScope>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">A survey of deep neural network architectures and their applications</title>
		<author>
			<persName><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><forename type="middle">E</forename><surname>Alsaadi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neurocomputing</title>
		<imprint>
			<biblScope unit="volume">234</biblScope>
			<biblScope unit="page" from="11" to="26" />
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">A survey on deep learning for big data</title>
		<author>
			<persName><forename type="first">Q</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">T</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Inf. Fusion</title>
		<imprint>
			<biblScope unit="volume">42</biblScope>
			<biblScope unit="page" from="146" to="157" />
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Complex brain networks: graph theoretical analysis of structural and functional systems</title>
		<author>
			<persName><forename type="first">E</forename><surname>Bullmore</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Sporns</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nat. Rev. Neurosci</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page">186</biblScope>
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Community-affiliation graph model for overlapping network community detection</title>
		<author>
			<persName><forename type="first">J</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Leskovec</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of 12th IEEE International Conference on Data Mining</title>
				<meeting>12th IEEE International Conference on Data Mining</meeting>
		<imprint>
			<date type="published" when="2012">2012</date>
			<biblScope unit="page" from="1170" to="1175" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">A distributed overlapping community detection model for large graphs using autoencoder</title>
		<author>
			<persName><forename type="first">V</forename><surname>Bhatia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Rani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Futur. Gener. Comp. Syst</title>
		<imprint>
			<biblScope unit="volume">94</biblScope>
			<biblScope unit="page" from="16" to="26" />
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Deep architecture for traffic flow prediction: Deep belief networks with multitask learning</title>
		<author>
			<persName><forename type="first">W</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Hong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Xie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Intell. Transp. Syst</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="2191" to="2201" />
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Graphical solution for arterial road traffic flow model considering spillover</title>
		<author>
			<persName><forename type="first">H</forename><surname>Qi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Access</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page" from="6755" to="6764" />
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Knowledge graph completion with adaptive sparse transfer matrix</title>
		<author>
			<persName><forename type="first">G</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Zhao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 30th AAAI Conference on Artificial Intelligence</title>
				<meeting>the 30th AAAI Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="985" to="991" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Knowledge transfer for out-of-knowledge-base entities: A graph neural network approach</title>
		<author>
			<persName><forename type="first">T</forename><surname>Hamaguchi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Oiwa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Shimbo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Matsumoto</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 26th International Joint Conference on Artificial Intelligence</title>
				<meeting>the 26th International Joint Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="1802" to="1808" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">A new model for learning in graph domains</title>
		<author>
			<persName><forename type="first">M</forename><surname>Gori</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Monfardini</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Scarselli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2005 IEEE International Joint Conference on Neural Networks</title>
				<meeting>the 2005 IEEE International Joint Conference on Neural Networks</meeting>
		<imprint>
			<date type="published" when="2005">2005</date>
			<biblScope unit="page" from="729" to="734" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">The graph neural network model</title>
		<author>
			<persName><forename type="first">F</forename><surname>Scarselli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Gori</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">C</forename><surname>Tsoi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Hagenbuchner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Monfardini</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Neural Netw</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="61" to="80" />
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Convolutional networks on graphs for learning molecular fingerprints</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">K</forename><surname>Duvenaud</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Maclaurin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Iparraguirre</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Bombarell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Hirzel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Aspuru-Guzik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">P</forename><surname>Adams</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 29th Conference on Neural Information Processing Systems</title>
				<meeting>the 29th Conference on Neural Information Processing Systems</meeting>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="2224" to="2232" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Gated graph sequence neural networks</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Tarlow</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Brockschmidt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">S</forename><surname>Zemel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 4th International Conference on Learning Representations</title>
				<meeting>the 4th International Conference on Learning Representations</meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Diffusion-convolutional neural networks</title>
		<author>
			<persName><forename type="first">J</forename><surname>Atwood</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Towsley</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 30th Conference on Neural Information Processing Systems</title>
				<meeting>the 30th Conference on Neural Information Processing Systems</meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="1993" to="2001" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Deeper insights into graph convolutional networks for semi-supervised learning</title>
		<author>
			<persName><forename type="first">Q</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><forename type="middle">M</forename><surname>Wu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 32nd AAAI Conference on Artificial Intelligence</title>
				<meeting>the 32nd AAAI Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="3538" to="3545" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Link prediction based on graph neural networks</title>
		<author>
			<persName><forename type="first">M</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 32nd Conference on Neural Information Processing Systems</title>
				<meeting>the 32nd Conference on Neural Information Processing Systems</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="5165" to="5175" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Neural network-based graph embedding for cross-platform binary code similarity detection</title>
		<author>
			<persName><forename type="first">X</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Yin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Song</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2017 ACM SIGSAC Conference on Computer and Communications Security</title>
				<meeting>the 2017 ACM SIGSAC Conference on Computer and Communications Security</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="363" to="376" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Learning convolutional neural networks for graphs</title>
		<author>
			<persName><forename type="first">M</forename><surname>Niepert</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Ahmed</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Kutzkov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 33rd International Conference on Machine Learning</title>
				<meeting>the 33rd International Conference on Machine Learning</meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="2014" to="2023" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">An end-to-end deep learning architecture for graph classification</title>
		<author>
			<persName><forename type="first">M</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Neumann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 32nd AAAI Conference on Artificial Intelligence</title>
				<meeting>the 32nd AAAI Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="4438" to="4445" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Label-dependent node classification in the network</title>
		<author>
			<persName><forename type="first">P</forename><surname>Kazienko</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Kajdanowicz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neurocomputing</title>
		<imprint>
			<biblScope unit="volume">75</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="199" to="209" />
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Collective classification in network data</title>
		<author>
			<persName><forename type="first">P</forename><surname>Sen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Namata</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Bilgic</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Getoor</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Galligher</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Eliassi-Rad</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">AI Mag</title>
		<imprint>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="93" to="93" />
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Line: Large-scale information network embedding</title>
		<author>
			<persName><forename type="first">J</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Qu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Mei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 24th International Conference on World Wide Web</title>
				<meeting>the 24th International Conference on World Wide Web</meeting>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="1067" to="1077" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Gene set enrichment analysis: a knowledgebased approach for interpreting genome-wide expression profiles</title>
		<author>
			<persName><forename type="first">A</forename><surname>Subramanian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Tamayo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><forename type="middle">K</forename><surname>Mootha</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Mukherjee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">L</forename><surname>Ebert</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">A</forename><surname>Gillette</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Paulovich</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">L</forename><surname>Pomeroy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">R</forename><surname>Golub</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">S</forename><surname>Lander</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Proc. Natl. Acad. Sci USA</title>
		<imprint>
			<biblScope unit="volume">102</biblScope>
			<biblScope unit="issue">43</biblScope>
			<biblScope unit="page" from="15545" to="15550" />
			<date type="published" when="2005">2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Discovering disease-genes by topological features in human protein-protein interaction network</title>
		<author>
			<persName><forename type="first">J</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Bioinformatics</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="issue">22</biblScope>
			<biblScope unit="page" from="2800" to="2805" />
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Convolutional neural networks on graphs with fast localized spectral filtering</title>
		<author>
			<persName><forename type="first">M</forename><surname>Defferrard</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Bresson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Vandergheynst</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 30th Conference on Neural Information Processing Systems</title>
				<meeting>the 30th Conference on Neural Information Processing Systems</meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="3844" to="3852" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Spectral graph theory and its applications</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">A</forename><surname>Spielman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 48th Annual IEEE Symposium on Foundations of Computer Science</title>
				<meeting>the 48th Annual IEEE Symposium on Foundations of Computer Science</meeting>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2007">2007</date>
			<biblScope unit="page" from="29" to="38" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Weighted graph cuts without eigenvectors a multilevel approach</title>
		<author>
			<persName><forename type="first">I</forename><forename type="middle">S</forename><surname>Dhillon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Guan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Kulis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Pattern Anal. Mach. Intell</title>
		<imprint>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="1944" to="1957" />
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Semi-supervised classification with graph convolutional networks</title>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">N</forename><surname>Kipf</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Welling</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 4th International Conference on Learning Representations</title>
				<meeting>the 4th International Conference on Learning Representations</meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Wavelets on graphs via spectral graph theory</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">K</forename><surname>Hammond</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Vandergheynst</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Gribonval</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Appl. Comput. Harmon. Anal</title>
		<imprint>
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="129" to="150" />
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Inductive representation learning on large graphs</title>
		<author>
			<persName><forename type="first">W</forename><surname>Hamilton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Ying</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Leskovec</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 31st Conference on Neural Information Processing Systems</title>
				<meeting>the 31st Conference on Neural Information Processing Systems</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="1024" to="1034" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Representation learning on graphs with jumping knowledge networks</title>
		<author>
			<persName><forename type="first">K</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Sonobe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Kawarabayashi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Jegelka</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 35th International Conference on Machine Learning</title>
				<meeting>the 35th International Conference on Machine Learning</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="5449" to="5458" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Predict then propagate: Graph neural networks meet personalized pagerank</title>
		<author>
			<persName><forename type="first">J</forename><surname>Klicpera</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Bojchevski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Günnemann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 7th International Conference on Learning Representations</title>
				<meeting>the 7th International Conference on Learning Representations</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<monogr>
		<title level="m" type="main">The pagerank citation ranking: Bringing order to the web</title>
		<author>
			<persName><forename type="first">L</forename><surname>Page</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Brin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Motwani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Winograd</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1999">1999</date>
			<publisher>Stanford InfoLab</publisher>
		</imprint>
	</monogr>
	<note type="report_type">Tech. rep</note>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">Topic-sensitive pagerank</title>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">H</forename><surname>Haveliwala</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 11st International Conference on World Wide Web</title>
				<meeting>the 11st International Conference on World Wide Web</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2002">2002</date>
			<biblScope unit="page" from="517" to="526" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">Simplifying graph convolutional networks</title>
		<author>
			<persName><forename type="first">F</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">H J</forename><surname>Souza</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Fifty</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">Q</forename><surname>Weinberger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 36th International Conference on Machine Learning</title>
				<meeting>the 36th International Conference on Machine Learning</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="6861" to="6871" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<analytic>
		<title level="a" type="main">Large-scale machine learning with stochastic gradient descent</title>
		<author>
			<persName><forename type="first">L</forename><surname>Bottou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 19th International Conference on Computational Statistics</title>
				<meeting>the 19th International Conference on Computational Statistics</meeting>
		<imprint>
			<date type="published" when="2010">2010</date>
			<biblScope unit="page" from="177" to="186" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<analytic>
		<title level="a" type="main">Graph attention networks</title>
		<author>
			<persName><forename type="first">P</forename><surname>Veličković</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Cucurull</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Casanova</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Romero</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Lio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 5th International Conference on Learning Representations</title>
				<meeting>the 5th International Conference on Learning Representations</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<analytic>
		<title level="a" type="main">Attention is all you need</title>
		<author>
			<persName><forename type="first">A</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">N</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ł</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Polosukhin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 31st Conference on Neural Information Processing Systems</title>
				<meeting>the 31st Conference on Neural Information Processing Systems</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="5998" to="6008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b58">
	<monogr>
		<title level="m" type="main">Attentionbased graph neural network for semi-supervised learning</title>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">K</forename><surname>Thekumparampil</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Oh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">J</forename><surname>Li</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1803.03735</idno>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b59">
	<analytic>
		<title level="a" type="main">One-shot imitation learning</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Duan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Andrychowicz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Stadie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><forename type="middle">J</forename><surname>Ho</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Schneider</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Abbeel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Zaremba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 31st Conference on Neural Information Processing Systems</title>
				<meeting>the 31st Conference on Neural Information Processing Systems</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="1087" to="1098" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b60">
	<analytic>
		<title level="a" type="main">Vain: Attentional multi-agent predictive modeling</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Hoshen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 31st Conference on Neural Information Processing Systems</title>
				<meeting>the 31st Conference on Neural Information Processing Systems</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="2701" to="2711" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b61">
	<monogr>
		<title level="m" type="main">Variational graph auto-encoders</title>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">N</forename><surname>Kipf</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Welling</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1611.07308</idno>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b62">
	<analytic>
		<title level="a" type="main">Auto-encoding variational bayes</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">P</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Welling</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2nd International Conference on Learning Representations</title>
				<meeting>the 2nd International Conference on Learning Representations</meeting>
		<imprint>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b63">
	<analytic>
		<title level="a" type="main">Learning to represent knowledge graphs with gaussian embedding</title>
		<author>
			<persName><forename type="first">S</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Zhao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 24th ACM International on Conference on Information and Knowledge Management</title>
				<meeting>the 24th ACM International on Conference on Information and Knowledge Management</meeting>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="623" to="632" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b64">
	<analytic>
		<title level="a" type="main">Deep gaussian embedding of graphs: Unsupervised inductive learning via ranking</title>
		<author>
			<persName><forename type="first">A</forename><surname>Bojchevski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Günnemann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 5th International Conference on Learning Representations</title>
				<meeting>the 5th International Conference on Learning Representations</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b65">
	<analytic>
		<title level="a" type="main">Multilabel classification on heterogeneous graphs with gaussian embeddings</title>
		<author>
			<persName><forename type="first">Dos</forename><surname>Santos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Piwowarski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Gallinari</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2016 Joint European Conference on Machine Learning and knowledge Discovery in Databases</title>
				<meeting>the 2016 Joint European Conference on Machine Learning and knowledge Discovery in Databases</meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="606" to="622" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b66">
	<monogr>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">F</forename><surname>Ribeiro</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">H</forename><surname>Saverese</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">R</forename><surname>Figueiredo</surname></persName>
		</author>
		<title level="m">Proceedings of the 23rd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining</title>
				<meeting>the 23rd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="385" to="394" />
		</imprint>
	</monogr>
	<note>Struc2vec: Learning node representations from structural identity</note>
</biblStruct>

<biblStruct xml:id="b67">
	<analytic>
		<title level="a" type="main">Node2vec: Scalable feature learning for networks</title>
		<author>
			<persName><forename type="first">A</forename><surname>Grover</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Leskovec</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 22nd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining</title>
				<meeting>the 22nd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining</meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="855" to="864" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b68">
	<analytic>
		<title level="a" type="main">Deep graph infomax</title>
		<author>
			<persName><forename type="first">P</forename><surname>Veličković</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Fedus</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">L</forename><surname>Hamilton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Liò</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">D</forename><surname>Hjelm</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 6th International Conference on Learning Representations</title>
				<meeting>the 6th International Conference on Learning Representations</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b69">
	<analytic>
		<title level="a" type="main">Revisiting semisupervised learning with graph embeddings</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">W</forename><surname>Cohen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Salakhutdinov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 33rd International Conference on Machine Learning</title>
				<meeting>the 33rd International Conference on Machine Learning</meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="40" to="48" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b70">
	<analytic>
		<title level="a" type="main">Tri-party deep network representation</title>
		<author>
			<persName><forename type="first">S</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 25th International Joint Conference on Artificial Intelligence</title>
				<meeting>the 25th International Joint Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="1895" to="1901" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b71">
	<analytic>
		<title level="a" type="main">Imagebased recommendations on styles and substitutes</title>
		<author>
			<persName><forename type="first">J</forename><surname>Mcauley</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Targett</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName><surname>Van Den</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Hengel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 38th International ACM SIGIR Conference on Research and Development in Information Retrieval</title>
				<meeting>the 38th International ACM SIGIR Conference on Research and Development in Information Retrieval</meeting>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="43" to="52" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b72">
	<analytic>
		<title level="a" type="main">Predicting multicellular function through multi-layer tissue networks</title>
		<author>
			<persName><forename type="first">M</forename><surname>Zitnik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Leskovec</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Bioinformatics</title>
		<imprint>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="issue">14</biblScope>
			<biblScope unit="page" from="190" to="198" />
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b73">
	<analytic>
		<title level="a" type="main">Adam: A method for stochastic optimization</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">P</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Ba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 4th International Conference on Learning Representations</title>
				<meeting>the 4th International Conference on Learning Representations</meeting>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b74">
	<analytic>
		<title level="a" type="main">Publisher&apos;s Note Springer Nature remains neutral with regard to jurisdictional claims in published maps and institutional affiliations</title>
	</analytic>
	<monogr>
		<title level="m">2016, and the M.S. degree in computer software and theory from Fuzhou University</title>
		<title level="s">College of Mathematics and Computer Science. His research interests include machine learning</title>
		<meeting><address><addrLine>Guilin, China; Fuzhou, China</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note>Shunxin Xiao received the B.S. degree in software engineering from Guilin University of Electronic Technology. where he is currently pursuing the Ph.D. degree with the. graph neural networks, and computer vision</note>
</biblStruct>

<biblStruct xml:id="b75">
	<analytic>
		<title level="a" type="main">He is currently a Full Professor and Qishan Scholar with the College of Mathematics and Computer Science, Fuzhou University</title>
	</analytic>
	<monogr>
		<title level="m">His research interests include machine learning</title>
				<meeting><address><addrLine>Chengdu, China; Fuzhou, China</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2014-08">2014. August 2015 to August 2016</date>
		</imprint>
		<respStmt>
			<orgName>Shiping Wang received the Ph.D. degree from the School of Computer Science and Engineering, University of Electronic Science and Technology of China</orgName>
		</respStmt>
	</monogr>
	<note>He was a Research Fellow with Nanyang Technological University from. data mining, computer vision, optimization theory, and granular computing</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
