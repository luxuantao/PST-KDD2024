<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Deformations with Radia</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Jun-Yong</forename><surname>Noh</surname></persName>
							<email>noh@graphics.usc.edu</email>
						</author>
						<author>
							<persName><forename type="first">Douglas</forename><surname>Fidaleo</surname></persName>
							<email>dfidaleo@graphics.usc.edu</email>
						</author>
						<author>
							<persName><forename type="first">Ulrich</forename><surname>Neumann</surname></persName>
							<email>uneumann@graphics.usc.edu</email>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="department">Computer Science Department</orgName>
								<orgName type="institution">IMSC University of Southern California</orgName>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff1">
								<orgName type="department">Computer Science Department</orgName>
								<orgName type="institution">IMSC University of Southern California</orgName>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff2">
								<orgName type="department">Computer Science Department</orgName>
								<orgName type="institution">IMSC University of Southern California</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Deformations with Radia</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">4DC91FACD671D4AFE30355207D69CFE7</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.3" ident="GROBID" when="2023-07-28T16:08+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Geometric algorithms, languages, and systems</term>
					<term>1.3.7 [Computer Graphics]: Animation</term>
					<term>Algorithms, Performance, Design Geometry Deformation, Animation, MPEG-4</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>We present a novel approach to creating deformations of polygonal models using Radial Basis Functions (RBFs) to produce localized real-time deformations.</p><p>Radial Basis Functions assume surface smoothness as a minimal constraint and animations produce smooth displacements of affected vertices in a model. Animations are produced by controlling an arbitrary sparse set of control points defined on or near the surface of the model. The ability to directly manipulate a facial surface with a small number of point motions facilitates an intuitive method for creating facial expressions for virtual environment applications such as an immersive teleconferencing system or entertainment. Smooth deformations of the human face or other models are possible and illustrated with examples of a variety of expressions and mouth shapes.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Model-based representations of real and virtual scenes often contain deformable surfaces such as faces, bodies, cloth, paper, etc. The transmission, storage, and dynamic deformation of these surfaces are efficiently performed by sensing (encoding) and synthesizing (decoding) the deformed surfaces.</p><p>For example, a 3D model of a person can be transformed and deformed to match the behavior of the conference participant. In addition, the viewpoint at the decoding side can be changed to an arbitrary position allowing the inspection of the person from different angles.</p><p>High-level behavior encoding of the scene also leads to high compression ratios. For human faces, typically, the 3D model is transmitted once initially and animation parameters are sent at subsequent time frames. In an MPEG-4 implementation, when a model is not provided by the encoder, the decoder uses the model that already resides in the decoder side. Preparing person specific models and animating them require two sets of parameters. The first is the definition parameter set, primarily responsible for creating a person specific model, which includes the specification of shape, size and texture information. The second set contains the animation parameters that allow a variety of facial expressions and body postures.</p><p>Due to the complexity of the structure and behavior of human faces, modeling and animation of the Pace is often considered separately from whole body modeling and animation. Indeed, deformation mechanisms for facial animation are different from body animation controllers that manipulate joint angles of a human figure. In this paper, we will concentrate on deformations required for fi~cial animation.</p><p>We develop methods for deforming a given 3D model to create facial expressions and mouth shapes. The creation of person specific models is a separate sub-problem and example approaches can be found in <ref type="bibr" target="#b5">[5,</ref><ref type="bibr" target="#b6">6,</ref><ref type="bibr" target="#b13">15,</ref><ref type="bibr">20,</ref><ref type="bibr" target="#b23">26]</ref>. We briefly survey common facial animation techniques in the following subsection.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>1,1 Background</head><p>Pighin et al. <ref type="bibr">[20]</ref> combine 2D morphing with 3D transformations of a geometric model to produce facial animation. The success of their animation depends on how realistically a collection of facial models can be created with various expressions. It requires the selection of a number of feature points and careful preparation of texture maps. This approaci~ is a good modeling technique but animations are limited to interpolations between pro-made models. Facial animation tlsing }:mite Element Methods (FEM) [2, 7, 8, t l, 19] faithfully reconstructs ~TaciaI geometry-. FEM implicitly defines i~terpoiation t:imc~ions between nodes based on a description of the physical properties of the material, typically a stress-strain relationship. When external forces are applied, the displacements of the nodes are computed to minimize local stresses and suains imposed onto the nodes.</p><p>Layered skin models based on mass-spring systems mimic the anatomical structure and dynamics of the imman face <ref type="bibr" target="#b13">[15]</ref>. The mesh consists of three-layers corresponding to skin, fi~tty tissue, and muscles tied to bones. Elastic spring elements connect each mesh node and each layer. Muscle forces propagate through the mesh to create deformations. The computational cost tbr such spring systems can be very high. Kalra et al <ref type="bibr" target="#b11">[13]</ref> per%tin £i~cial animations using l?ee-%ma deformation (FFD). EFD defom~s volumetric objects by manipulating control points arranged in three-dimensional cubic lattices. Conceptually, the .fiace mesh is embedded in an iraaginary, clear, and flexible control box containing a 3D grid of control points. As the control box is squashed, bent, or twisted into arbitrary shapes, the embedded mesh deforms accordingly. The basis for the cont~cot points is a tri-variate tensor prod~.tct Bemstein polynomial.</p><p>Vector-based muscle models <ref type="bibr" target="#b24">[27,</ref><ref type="bibr" target="#b25">28]</ref> are adapted widely for their compact representation. A delineated defbrmation field models the action of muscles upon skin, The muscle definition includes the vector field direction, an origin, and an insertion point. The cone shaped field extent is defined by cosine Nnctions and 't~dl off factorx Facial animation is achieved by changing the contraction parameters of the embedded muscles under Ne Nce mesh. This approach assumes l(hat muscles aa'e placed under the face mesh in correct locations. Placing muscles in 3D space, however, is not intuitive or consistent from model to model Deformation methods also include spring muscles <ref type="bibr" target="#b18">[21]</ref>, spline models [411, and parametric models <ref type="bibr" target="#b16">[18]</ref>. Perfbrmance driven facial animation (PDFA) <ref type="bibr">[9, 10, I4]</ref> has used video streams as an input, generating animations that mimic the tracked subject using the methods described above.</p><p>We use Radial Basis Functions (RBF) to create various expressions and their animations. In our previous work <ref type="bibr" target="#b9">[9]</ref>, we showed defbrmations using RBF in perfbrmemce driven fhcial animation (PDFA). Briefly, a small number of tracked £?atare points determine new positions of vertices on the mesh fbr every' frame.</p><p>The use of RBF~s guarantees smooth geometric defoNnation. Our method is similar to Guenter et al.~ [10] system in that 3D f~ature points are used as a driving :forCe, however RBFg allow a more direct and compact representation of animation parameters than an ad hoc smoothing method. Our tbrmer work shares similar limitation with others, however, in that the mesh needs to be equipped with predefined feature points to ensure correct defonnations.</p><p>We eliminate the necessity fbr pro-defined feature points in our current approach. In addition, instead of having a single global RBF defbrmation engine, we exploit a number of small RBF deformation elements to localize the deformations.</p><p>Our approach provides unique advantages over existing methods {br creating facial animation. First. most existing approaches require animation mechanisms (e.g.. muscles, or FFD) explicitly embedded in the facial mesh. For example, muscles must be placed under Ne mesh <ref type="bibr" target="#b24">[27,</ref><ref type="bibr" target="#b25">28]</ref>. or spring and stiflhess constamts must be determined m advance [2] and tuned each time a new" face mesh is created. In contrast~ our RBF method works on m~y mesh witi~out modification. Second, the creation of Ncial animations can be easily automated with our method. When video data of an actor are available, feature points tracked on the subject are directly applied to deform the face model to mimic his/her ~:xpresshms. ['he nmnber and locations of tracked feature points does nn~ need ~o be fixed. As :note potnls are tracked. defbrmation control and fidelity arc increased. This flexibility is due to the d~rect deformation capability of the RBF system. Third the behavior of our detbrmation method ~s very predictable. External fbrces are applied directly to the Ncial surt~lce [~y each tracked poink ancl nearby nodes are displaced smoothly. Since the surihce is directly manipulated, the process ~s more intuitive than methods that indirectly af'f;~ct the thcial surface | 131.</p><p>Yhe geometry defbrmation mechanism using RBF is discussed m ~ccmm 2. Automated ammanon generation is described in sectmn 3.</p><p>Results are shown m section ~1~ tbllowed by discussions in section 5 and conclusions in section 6.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>! '</head><p>A red point represents initial control point position while a bh.m point represents new position Green points arc anchor points and the area inside the green points is the regmn of influence. The shape of the region of influence is affected by the mesh regularity</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Figure i Geometry Deformation Element defined on the facial surface</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">G E O M E T R Y D E F O R M A T I O N</head><p>Face mesh geometry is locally deformed by a geometry deformation element (GDE I, A GDE is the smallest detrbrmation unit defined on the surface of" the face, A GDE consists of a control point, the region of influence around the control point. anchor points that lie mtiae boundary of the influence region, and an underlying RBF system (figure I. 41. The movable control point and the stationary anchor points determine the displacement of the vertices in the influence region, Specifying any point on the face creates a GDE. A control point may be derived from a 2D image by projecting it to the 3D mesh surface. The region of influence is bound by a distance metric that deteNnines the stationary anchor points. The number of mesh vertices in the influence region can be large or small. An influence region of one vertex reduces deformation to vertex manipulation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Algorithm Summary</head><p>Creating a deformation element:</p><p>1. Specify a control point on the mesh and an influence extent to control the deformation around the point. The selected point does not have to coincide with any of the vertices in the mesh.</p><p>2. For points selected in 2D images, convert them to 3D points on the model surface by ray casting.</p><p>3. Find the nearest vertex on the mesh to the selected 3D point. This vertex becomes the root for the search tree of mesh edges.</p><p>4. Search down the tree of mesh edges with a Breadth First Search, determining all vertices within a specified distance metric.</p><p>5. Leaf nodes of the search tree become the anchor points and, together with the specified control point, initialize the RBF system associated with the GDE (figure <ref type="figure">4</ref>).</p><p>Actuating a deformation element: </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Ray Casting</head><p>Note that the first step in creating and actuating deformation elements can be accomplished by manual input or automation. In either case a 2D point is identified (by mouse click or feature detection and tracking). Unless the selected point coincides exactly with a mesh vertex, its 3D location is unknown. However, we have a 3D face model so we can approximate the 3D point position by ray casting. The intersection point of the model with the ray emitted from the camera center through the specified 2D image point gives the 3D location on the face mesh (figure <ref type="figure">2</ref>). Direct specification of 3D control point positions are also possible to handle cases where the control points move out of the mesh or along silhouettes.</p><p>Edge based method selects lower mouth region while distance based method selects whole mouth region starting from the same location. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Search Methods and Distance Metrics</head><p>Once a 3D control point is specified, the region of influence and anchor points can be determined. We consider the edges of the face mesh to be an arbitrary tree with a root located at the nearest vertex to the specified control point. The GDE influence region and anchor points are determined by searching down the tree using a Breadth First Search <ref type="bibr" target="#b14">[ 16]</ref>. During traversal, vertices are tested against a distance metric to see if they fall in or out of the influence region. We experimented with two distance metrics.</p><p>One is based on edge depths and the other is based on Euclidean distance. The edge depth metric marks all vertices within some integer number N of mesh edges as in the influence region. The distance metric computes the Euclidian distance between traversed vertices and the control point, and when that distance is below threshold, the vertex is marked as within the influence region. The threshold is a real number scaled to the mesh coordinate units.</p><p>The two metrics serve different purposes. For example, when opening the mouth, the influence regions should be separate in the upper lip area and the lower lip area. In this case, an edge based metric finds the lower part of the mouth mesh for any control point on the lower lip without affecting the upper mouth region (Figure <ref type="figure" target="#fig_0">3</ref>). In cases where mesh density is very irregular, for example in the eye regions, an edge metric produces very irregular shaped influence regions. The distance metric produces regular shaped influence regions regardless of mesh density variations. In many cases, we find that both metrics produce similar deformations. For large influence regions or veu dense meshes, the number of boundary points can become large, In these cases we limit the number of perimeter points to 20 sampled evenly along the boundary vertex set, </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.4">Radial Basis Functions</head><p>Inspired by the recent success of RBF approaches in 3D model generation <ref type="bibr" target="#b5">[5,</ref><ref type="bibr" target="#b17">19,</ref><ref type="bibr" target="#b23">26]</ref> and its demonstrated capability in image warping <ref type="bibr" target="#b21">[24]</ref> we exploit RBF volume morphing to directly drive 3D geometry deformation of face models. Radial Basis functions are often chosen as an approximation function F for its power to deal with irregular sets of data in multi-dimensional space in approximating high dimensional smooth surfaces <ref type="bibr" target="#b19">[22]</ref>. Examples of the RBFs are Gaussian</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.4.1">Interpolation~Approximation with RBF</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>_(L)2 ~r 2 functions h(r) = e c , multi-quadrics h(r) = + c 2 ,</head><p>and thin plate splines h(r 2) = r 2 logr with a linear term added. Radial basis functions are named because of their radially symmetric distances parameters. They can be implemented using simple neural network with one hidden layer.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.4.2">Face Deformations with RBF</head><p>In this section, we refer to a specified GDE control point and its anchor points as "feature" points since their distinctions are meaningless to the RBF system. As depicted in figure <ref type="figure">4</ref>, each GDE has one RBF system for displacements computations. A RBF system deforms the tacial mesh based upon the motions of all feature points. The mappings between initial positions and new positions of the feature points are described in terms of the vector coefficients. We compute this mapping at each frame. (This is known as training in tile neural network terminologybut this is implicit and no explicit training process is required). The rest of the nodes in the influence region are transformed based upon the coefficients computed. The radial basis function approximation equation is The solution given by equation ( <ref type="formula">4</ref>) assumes that there is no spurious data. In general, however, data is not noise-free. In the presence of noise, using the matrix H constructed with the assumption of perfect data may not produce a useful result. In this case, Thikhonov and Arsenin <ref type="bibr" target="#b22">[25]</ref> provide a very simple solution. We simply replace the matrix H by (H + M) for equation <ref type="bibr" target="#b4">(4)</ref>. Then equation ( <ref type="formula">4</ref>) becomes c = (H+/q/)-ly <ref type="bibr" target="#b5">(5)</ref> where ~, is a "small" parameter. The magnitude of 9~ is proportional to the noise magnitude. Notice that equation ( <ref type="formula">5</ref>) becomes identical to (4) by setting ~, to 0 where noise free perfect data are assumed. We simply set ~, to be 0.01, determined experimentally.</p><formula xml:id="formula_0">,V f(2)= ~c,h(ll ~-2, 11)<label>(1)</label></formula><p>The linear system of equation ( <ref type="formula">5</ref>) is easily solved by LU decomposition <ref type="bibr" target="#b20">[23]</ref> to obtain the coefficient set ~. The LU decomposition of the matrix H happens only once at the initialization of the RBF system for each ODE. Only a backsubstitution is computed for deformation frames with new positions of the feature points f~target. Thus the deformation computation is fast. Once the system is solved, the deformed positions for vertices in the mesh influence region are obtained from the computed coefficients.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Smile consists of two ODEs</head><p>Eye up consists of two ODEs Sadness consists of four ODEs </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.5">Generating Expressions</head><p>A geometry deformation element is the smallest unit for surface deformation. One or more deformation elements constitute an expression. For example, two deformation elements make one smile expression (figure <ref type="figure" target="#fig_3">5</ref>). In this way, a variety of expressions are possible by using various combinations of deformation elements. We can control a set of deformation elements with a single parameter d where d=0 corresponds to a neutral expression, and d=l corresponds to the maximum displacements of all the control points of the member deformation elements.</p><p>The expression can then be animated simply by changing the control parameter over the range of [0, 1]. Mouth shapes used for speech synthesis are created and controlled similarly. Mixtures of multiple control parameters are also possible.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">AUTOMATION</head><p>In the previous section, we described how to define deformation elements on the surface of the face and manually create and control various expressions. Once a gallery of expressions is constructed, animation across existing expressions can be achieved by key frame interpolations of expression parameter values. It is desirable to automate or at least semi-automate the construction of the initial expression database (figure6).</p><p>We take the approach of perfomaance driven facial animation (PDFA) to construct expressions. In PDFA~ a human actor is tracked with a camera while generating facial expressions and mouth shapes. This recorded or on-line video stream is analyzed to extract the motion of salient facial features. These motions then drive the deformation of the face model to produce similar expression animations. PDFA was first introduced in t990 <ref type="bibr" target="#b26">[29]</ref> and used in various contexts. PDFA has been used to drive 2D animation <ref type="bibr">[t, 7, 14]</ref> and 3D animation <ref type="bibr">[9, t0]</ref>. A major difficulty of using PDFA to automatically generate 3D facial animations lies in the ambiguous relationship between the tracked features and the animation mechanism. Given a sparse set of tracked displaced points on the face, estimating the animation parameters that invoke the displacements is an inverse problem not easily solved with many existing approaches <ref type="bibr" target="#b11">[ 13,</ref><ref type="bibr" target="#b25">28]</ref>.  In contrast, our GDEs can be controlled directly by the feature motion vectors measured in the images. This direct relationship between the tracked feature points and the ODE control points is a major advantage in simplifying a PDFA system. The steps to automate the generation of facial expressions can be summarized as follows:</p><p>1. Video streams are captured containing the subject making various expressions and mouth shapes.</p><p>2. Salient control points are identified (manually or automatically) on the subject face(s) and tracked over the expressive sequences.</p><p>3. The 2D tracked points from the video streams are treated as ODE control points and converted into 3D points using ray casting.</p><p>4. The 3D control point motions are input to the ODEs where defbrmations are produced as depicted in section 2.1.</p><p>For the tracking of the feature points and pose estimation of the head in the video streams, we use existing work <ref type="bibr" target="#b27">[30]</ref> adapted to suit our purposes. Feature tracking and pose estimation methods are likely to produce erroneous res~alts due to analysis errors ~md non-ideal imaging conditions.</p><p>Our animation applicatior~ provides an interactive editing interface that allows the animator to manually correct or override the tracking a~d pose results to achieve the desired m~imations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">RESULTS</head><p>We create a variety of expressions and mouth shapes by choosing different ~racking/coatml points and different influence regions and directly manipulating these points on the face. Figure <ref type="figure">9</ref> shows sample expressioas~ Small modification of the lip region conveys a feeling of dissatisfi~ction (a) or decisiveness (b). Moving one eyebrow upward shows cleverness (c). Pulling lips and eyebrows corners down create sadness (d). For a sty look, only a small ~mmber of vertices are displaced around the lip corner and eyebrow using small influence zone (e, t). "{{he same impression may be created by totally different expressio~s~ For i~stance, facial expressions for anger may vary from person to person, or even within the same person (g, h) Sampie animation sequences from a neuronal state to {~ll expression (figure <ref type="figure">7,</ref><ref type="figure">8</ref>) at~d between two expressions are shown (figure <ref type="figure">I0</ref>). Wi~h the 3D model, inspectmn from ~m arbitrary viewpoint is possible (figure <ref type="figure">7,</ref><ref type="figure">8</ref>). Figure <ref type="figure" target="#fig_7">11</ref> shows models generated with the automated process. WN~out any prior preparation, the same GDE techniques are applied to "Skips" 3[) model. Three red poir~ts on the eye sockets and the tip of~he nose are used for pose estimation while the motions of yellow points are used for deformations of the fi~ce. Additional MPEG movies are also available onhttpi//scgusc.e&amp;,/~noh. We use a fi~cial mesh with 1954 polygons. Real time (30HZ) animations are achieved with a moderately confiN~red 500MHz PC:, or all of the Our def(&gt;m~atton mecha~ism can be eom~ie~e|y MPEG-4 ¢dmpiiant and extendible if ada{itional tracking data is available, ta the mouth mNons, %r example, additional control data may come from a speech stream analysis. Animations using tea:}miques s~mh as aritt~metic coding or DCTF coding as descrit~d in <ref type="bibr" target="#b15">[17]</ref> can achieve data rate as low as 300 ~-o 2000 bits/s.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">CONCLUSION</head><p>C~ating li~;c{ike fi~ciat animation is a crucial ~imtor in developing an irr~nersive virtual environment. We presented a method t~r animating deformations of 3D Nee models. In scenes containing faces, the mmlysis of t~cial images can e{:~icientty encode the data needed ~o control the deform:nations. Our GDE method is a novel approach to defbrming t2aee models by directly manipulating feature poims. The RBF based compumnon produces localized smooth deti&gt;rmat~ons on the face. °Fh~s approach is applicable to mos~ meshes w~thout special initializati~m. The process requires mi~ima/ and intumve human ~mervention and is easily auto{hated by the use of a feature tracking 5ys~em w~th video streams</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.">ACKNOWLEDGEMENT</head><p>This work was supported by DARPA and the Annenberg Center at USC~ Additional Suppor~ m~d research ~acitittes are provided by the NSF through its ERC fhading of the Integrated Media Systems (.?enter. We recognize the contributions to this work ii'om all our colleagues in the CGFr lakmratory. Special ttaanks go ~o Tae-Yong Kim, Bo/an Jmng, S~ya Yam and Reyes Enciso.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">DISCUSSION</head><p>Radial basis f~mctions guarantee a smooth surfi~ce defom~ation from a sparse set of control points. However, if a control point is moved too far flom its original position, say outside the iT~fiuence region, large discontinuities occur arom'~d the anchor points. Because anchors paints are statior~ary at the boundary of the influence region, no influence of the control point will propagate through the anchor points. Such large control point motions do not occur in practice and most de:{brmation enNnes would produce unnatural effects under simitar conditions. By limiting the deformation regions we eliminate the need to prepare or alter the mesh as required by other methods, including our previous approach <ref type="bibr" target="#b9">[9]</ref>. Currently, we assume that a specified control point can only be moved within the region of influence. As a possible adaptation to altow the control point to move beyond the influence region, one can consider a dynamic tree search to regenerate a iarger region of influence with new anchor points.</p><p>Our method estimates the required 3D control points using ray casting. However, eiToneous results can occur We allow manual editing of the 3D control points as needed to compensate fbr such errors. An ultimate solution fbr this problem may be to impose constraints on the facial surface such as bone structures. However, this introduces limitations to the use of simple meshes,  </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 3</head><label>3</label><figDesc>Figure 3 Comparison between edge based search method and distance based search method ' ~</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head></head><label></label><figDesc>Classical approximation theory solves the problem of approximating or interpolating a continuous multivariate fimction f(~') by an approximation function F(.~,c') with an appropriate choice of parameter set C where ~" and C are real vectors (.~ = Xl, x2 ....... s and c = c~, c2 ........ ~): Finding a parameter set C is often referred to as learning or training in the neural network sense. In the training stage, a goal is to figure out C given an approximation function F and a set of training examples, which will provide the best approximation of f.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>3 )</head><label>3</label><figDesc>r) = vr2/7~+ s 2 for Hmdy multi-quadrics, s is called a stiffness constant that regulates the local or global effects of tile feature points and f is the Euclidian distance between the feature point and the input point. When computing mapping coefficients, input points are the feature points themselves and when evaluating the new positions, input points are the points in the influence region. Plugging the Hardy basis function into equation I l) results in N ~t a,getj .... ~;o u,'ce ", ~ fl~--2,~ot~ ---2 2 =P(X j)=2.6}~/ X j--.~,. +Si (2) i=1 where 1 _&lt; j _&lt; Number of feature points, in our case. N. The dimension of 2 is three (i.e. x, y, z coordinates of each feature point}..~so,,,.(.~ denotes the initial positions of the feature points and ~target denotes new positions of the f~ature points. Stiff:hess coefficient S i is determined as suggested by Eck [3] for sorer deformation where feature points are widely scattered and stronger where closely located. ,, -minll ~'°% -&lt; II (Substitution of N feature points into the equation (2) results in a linear system of N equation whose solution is of the form = H -l ff t arg et (4)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 5</head><label>5</label><figDesc>Figure 5 Expressions as a collection of Geometry'Deformation Elements (ODEs)</figDesc><graphic coords="5,52.56,172.89,237.12,174.24" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head></head><label></label><figDesc>.................................................................................................................................. i t~ (b) a. Input video stream b. 3D model with eyebrows up c. The deformed 3D model overlaid on the video steam transparently to show correct aligmnent of eyebrows.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 6</head><label>6</label><figDesc>Figure 6 Deformations driven by feature points in the video stream</figDesc><graphic coords="5,316.67,213.75,243.36,175.68" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>~Figure 7 Figure 8 Figure 9 Figure</head><label>789</label><figDesc>Figure 7 Sequence of making "~'sound</figDesc><graphic coords="8,96.15,56.73,413.76,192.96" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure 11</head><label>11</label><figDesc>Figure 11 Automated animations with input video stream</figDesc><graphic coords="9,66.67,391.26,473.76,139.20" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head></head><label></label><figDesc>........................................................................... i Figure 4 Relationships between GDE and RBF</figDesc><table><row><cell></cell><cell>Control point</cell><cell></cell><cell>Feature</cell><cell></cell><cell></cell></row><row><cell>Input</cell><cell>Anchor Points</cell><cell>Z-2__~_.,)</cell><cell>Points c-q</cell><cell>i i</cell><cell>i</cell></row><row><cell></cell><cell></cell><cell></cell><cell>Mapping</cell><cell>i</cell><cell></cell></row><row><cell>output</cell><cell cols="2">Movable Points i ~-~</cell><cell cols="2">Coefficients I</cell><cell></cell></row><row><cell>User</cell><cell>GDE</cell><cell></cell><cell>RBF</cell><cell></cell><cell></cell></row></table><note><p>.</p></note></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title/>
		<author>
			<persName><surname>References</surname></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">A, Pen(land. Visually C(mtroiied Graphics</title>
		<author>
			<persName><forename type="first">T</forename><surname>•o Abayejani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Starnero</surname></persName>
		</author>
		<author>
			<persName><surname>Horowitz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE PAMI</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="issue">61</biblScope>
			<biblScope unit="page">993</biblScope>
			<date>June</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<author>
			<persName><forename type="first">S</forename><surname>Basu~</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename></persName>
		</author>
		<title level="m">liver, A~ Penttand, 3D Modeling and Tracking of Human Lip Mo~ions~ ICCV</title>
		<imprint>
			<date type="published" when="1998">1998</date>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page">343</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<author>
			<persName><forename type="first">M</forename><surname>Eck</surname></persName>
		</author>
		<title level="m">lr~terpotation Methods fbr Rectmsm~ction of 3D Surfaces from Sequences of Planar Slices, CAD und Computergraphik, Vat</title>
		<imprint>
			<date>Feb. 199t, 109</date>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="page">20</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Analyzing Facial Expressions for Virtual Conferencing</title>
		<author>
			<persName><forename type="first">P</forename><surname>Eisert</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Girod</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE, Computer Graphics and Applications</title>
		<imprint>
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="70" to="78" />
			<date type="published" when="1998">1998</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<author>
			<persName><forename type="first">R</forename><surname>Eneiso</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Ll D. Fida/Eo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T-Y.</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J-Y</forename><surname>Noh</surname></persName>
		</author>
		<author>
			<persName><surname>Neumann</surname></persName>
		</author>
		<title level="m">Synthesis of 3D Faces, lnternatio~val Workshop on Digital and Computational Video</title>
		<imprint>
			<date type="published" when="2000">2000</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Facial Defbrmations for MPEG-4</title>
		<author>
			<persName><surname>Escher</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Pandzic</surname></persName>
		</author>
		<author>
			<persName><surname>Thalmann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Computer Animation</title>
		<imprint>
			<biblScope unit="volume">998</biblScope>
			<biblScope unit="page" from="56" to="62" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Tracking and [meractive Animation of Faces and Heads using Input fi-om Video</title>
		<author>
			<persName><forename type="first">I</forename><forename type="middle">A</forename><surname>Essa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Basu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Penttand</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Modeling</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of Computer Animation June 1996 Conference</title>
		<meeting>Computer Animation June 1996 Conference<address><addrLine>Geneva, Switzerland</addrLine></address></meeting>
		<imprint>
			<publisher>EEE Computer Society Press</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Tracking Facial Motion</title>
		<author>
			<persName><forename type="first">I</forename><forename type="middle">A</forename><surname>Essa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Pentland</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Workshop on Non-rigid and Articulate Motion</title>
		<meeting>the IEEE Workshop on Non-rigid and Articulate Motion<address><addrLine>Austin, Texas</addrLine></address></meeting>
		<imprint>
			<date type="published" when="1994-11">November, 1994</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Classification and Volume Morphing for Performance-Driven Facial Animation</title>
		<author>
			<persName><forename type="first">D</forename><surname>Fidaleo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J-Y</forename><surname>Noh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Enciso</surname></persName>
		</author>
		<author>
			<persName><forename type="first">U</forename><surname>Neumann ; Guenter</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Grimm</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Wood</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Matvar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Pighin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Workshop on Digital and Computational Video</title>
		<imprint>
			<date type="published" when="1992">2000. 1998. 1992</date>
			<biblScope unit="page" from="191" to="202" />
		</imprint>
	</monogr>
	<note>State of the Art in Computer Animation</note>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">Convergence properties of radial basis fi.mctions. Constructive approximation</title>
		<author>
			<persName><forename type="first">I</forename><forename type="middle">R</forename><surname>Jackson</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1988">1988</date>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page" from="243" to="264" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<author>
			<persName><forename type="first">P</forename><surname>Kalra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Mangili</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><forename type="middle">M</forename><surname>Thalmann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Thalmann</surname></persName>
		</author>
		<title level="m">Simulation of Facial Muscle Actions Based on Rational Free From Deformations, Eurographics</title>
		<imprint>
			<date type="published" when="1992">1992</date>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="page" from="59" to="69" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">Very low bit rate face video compression using linear combination of 2D face views and principal components analysis, Image and Vision computing</title>
		<author>
			<persName><forename type="first">I</forename><surname>Koufakis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">F</forename><surname>Buxton</surname></persName>
		</author>
		<imprint>
			<biblScope unit="volume">999</biblScope>
			<biblScope unit="page" from="1031" to="1051" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Realistic face modeling for animation</title>
		<author>
			<persName><forename type="first">Y</forename><forename type="middle">C</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Terzopoulos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Waters</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Siggraph proceedings</title>
		<imprint>
			<date type="published" when="1995">1995</date>
			<biblScope unit="page" from="55" to="62" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">The shortest path through a maze</title>
		<author>
			<persName><forename type="first">Edward</forename><forename type="middle">F</forename><surname>Moore</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Symposium on the Theory of Switching</title>
		<meeting>the International Symposium on the Theory of Switching</meeting>
		<imprint>
			<publisher>Harvard University Press</publisher>
			<date type="published" when="1959">1959</date>
			<biblScope unit="page" from="285" to="292" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Animation of Synthetic Faces in MPEG-4</title>
		<author>
			<persName><forename type="first">J</forename><surname>Ostermann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Computer Animation</title>
		<imprint>
			<biblScope unit="page" from="49" to="55" />
			<date type="published" when="1998">1998</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Parameterized models for facial animation</title>
		<author>
			<persName><forename type="first">F</forename><forename type="middle">I</forename><surname>Parke</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Computer Graphics and Applications</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="issue">9</biblScope>
			<biblScope unit="page" from="61" to="68" />
			<date type="published" when="1982">1982</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Interactive Graphics for plastic surgery: A task level analysis and implementation</title>
		<author>
			<persName><forename type="first">S</forename><surname>Pieper</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Rosen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Zeltzer ; Pighin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Hecker</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Lischinski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Szeliski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">H</forename><surname>Salesin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Synthesizing Realistic Facial Expressions from Photographs, Siggraph proceedings</title>
		<imprint>
			<date type="published" when="1998">20. 1998</date>
			<biblScope unit="page" from="75" to="84" />
		</imprint>
	</monogr>
	<note>Special Issue: ACM Siggraph, 1992 Symposium on Interactive 3D Graphics</note>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Animating facial expression</title>
		<author>
			<persName><forename type="first">S</forename><surname>Ptatt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Badler</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computer Graphics</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="245" to="252" />
			<date type="published" when="1981">1981</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">A theory of networks for approximation and learning</title>
		<author>
			<persName><forename type="first">T</forename><surname>Poggio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename></persName>
		</author>
		<author>
			<persName><forename type="first">Giros</forename></persName>
		</author>
		<idno>A.I. Memo No. 1140</idno>
	</analytic>
	<monogr>
		<title level="j">Artificial Intelligence Lab</title>
		<imprint>
			<date type="published" when="1989-07">July 1989</date>
			<pubPlace>Cambridge, MA</pubPlace>
		</imprint>
	</monogr>
	<note type="report_type">Technical Report</note>
	<note>MIT</note>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">H</forename><surname>Press</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">A</forename><surname>Teukolsky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">T</forename><surname>Vetterling</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">P</forename><surname>Flannery</surname></persName>
		</author>
		<title level="m">Numerical Recipes in C</title>
		<imprint>
			<publisher>Cambridge University Press</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Image Warping with Scattered Data Interpolation</title>
		<author>
			<persName><forename type="first">D</forename><surname>Ruprecht</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Muller</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Computer Graphics and Applications</title>
		<imprint>
			<biblScope unit="page" from="37" to="43" />
			<date type="published" when="1995">1995</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Solution of Ill-Posed Problems and the regularization method</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">N</forename><surname>Tikhonov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><forename type="middle">Y</forename><surname>Arsenin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Soviet Math. Dokl</title>
		<imprint>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page" from="1035" to="1038" />
			<date type="published" when="1963">1963</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">A step Toward universal facial animation via volume morphing, 6 th IEEE International Workshop on Robot and Human communication</title>
		<author>
			<persName><forename type="first">F</forename><surname>Ulgen</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1997">1997</date>
			<biblScope unit="page" from="358" to="363" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<author>
			<persName><forename type="first">K</forename><surname>Waters</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Frisbie</surname></persName>
		</author>
		<title level="m">A Coordinated Muscle Model for Speech Animation, Graphics Interface</title>
		<imprint>
			<date type="published" when="1995">1995</date>
			<biblScope unit="page" from="163" to="170" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Waters. A muscle model for animating three-dimensional facial expression</title>
		<author>
			<persName><forename type="first">K</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Graphics (Siggraph proceedings, 1987)</title>
		<editor>
			<persName><forename type="first">Maureen</forename><forename type="middle">C</forename><surname>Stone</surname></persName>
		</editor>
		<imprint>
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="page" from="17" to="24" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Performance Driven Facial Animation</title>
		<author>
			<persName><forename type="first">L</forename><surname>Williams</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Siggraph proceedings</title>
		<imprint>
			<date type="published" when="1990">1990</date>
			<biblScope unit="page" from="235" to="242" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">A Fast Method for Detection Facial Features Under Varied Poses</title>
		<author>
			<persName><forename type="first">P</forename><surname>Zhenyun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Suya</forename><surname>You</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Guangyou</forename><surname>Xu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">China Journal of Image and Graphics</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="issue">4</biblScope>
			<date type="published" when="1997">1997</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
