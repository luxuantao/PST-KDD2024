<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Attention Mesh: High-fidelity Face Mesh Prediction in Real-time</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date type="published" when="2020-06-19">19 Jun 2020</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Ivan</forename><surname>Grishchenko</surname></persName>
							<email>igrishchenko@google.com</email>
							<affiliation key="aff0">
								<orgName type="institution">Google Research 1600 Amphitheatre Pkwy</orgName>
								<address>
									<postCode>94043</postCode>
									<settlement>Mountain View</settlement>
									<region>CA</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Artsiom</forename><surname>Ablavatski</surname></persName>
							<email>artsiom@google.com</email>
							<affiliation key="aff0">
								<orgName type="institution">Google Research 1600 Amphitheatre Pkwy</orgName>
								<address>
									<postCode>94043</postCode>
									<settlement>Mountain View</settlement>
									<region>CA</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Yury</forename><surname>Kartynnik</surname></persName>
							<email>kartynnik@google.com</email>
							<affiliation key="aff0">
								<orgName type="institution">Google Research 1600 Amphitheatre Pkwy</orgName>
								<address>
									<postCode>94043</postCode>
									<settlement>Mountain View</settlement>
									<region>CA</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Karthik</forename><surname>Raveendran</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Google Research 1600 Amphitheatre Pkwy</orgName>
								<address>
									<postCode>94043</postCode>
									<settlement>Mountain View</settlement>
									<region>CA</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Matthias</forename><surname>Grundmann</surname></persName>
							<email>grundman@google.com</email>
							<affiliation key="aff0">
								<orgName type="institution">Google Research 1600 Amphitheatre Pkwy</orgName>
								<address>
									<postCode>94043</postCode>
									<settlement>Mountain View</settlement>
									<region>CA</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Attention Mesh: High-fidelity Face Mesh Prediction in Real-time</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2020-06-19">19 Jun 2020</date>
						</imprint>
					</monogr>
					<idno type="arXiv">arXiv:2006.10962v1[cs.CV]</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.2" ident="GROBID" when="2023-01-01T13:45+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>We present Attention Mesh, a lightweight architecture for 3D face mesh prediction that uses attention to semantically meaningful regions. Our neural network is designed for real-time on-device inference and runs at over 50 FPS on a Pixel 2 phone. Our solution enables applications like AR makeup, eye tracking and AR puppeteering that rely on highly accurate landmarks for eye and lips regions. Our main contribution is a unified network architecture that achieves the same accuracy on facial landmarks as a multistage cascaded approach, while being 30 percent faster.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>In this work, we address the problem of registering a detailed 3D mesh template to a human face on an image. This registered mesh can be used for the virtual try-on of lipstick or puppeteering of virtual avatars where the accuracy of lip and eye contours is critical to realism.</p><p>In contrast to methods that use a parametric model of the human face <ref type="bibr" target="#b0">[1]</ref>, we directly predict the positions of face mesh vertices in 3D. We base our architecture on earlier efforts in this field <ref type="bibr" target="#b4">[5]</ref> that use a two stage architecture involving a face detector followed by a landmark regression network. However, using a single regression network for the entire face leads to degraded quality in regions that are perceptually more significant (e.g. lips, eyes).</p><p>One possible way to alleviate this issue is a cascaded approach: use the initial mesh prediction to produce tight crops around these regions and pass them to specialized networks to produce higher quality landmarks. While this directly addresses the problem of accuracy, it introduces performance issues, e.g. relatively large separate models that use the original image as input, and additional synchronization steps between the GPU and CPU that are very costly on mobile phones. In this paper, we show that it is possible for a single model to achieve the same quality as the cascaded approach by employing region-specific heads that transform the feature maps with spatial transformers <ref type="bibr" target="#b3">[4]</ref>, while being up to 30 percent faster during inference. We term this architecture as attention mesh. An added benefit is that it is easier to train and distribute since it is internally consistent compared to multiple disparate networks that are chained together.</p><p>We use an architecture similar to one described in <ref type="bibr" target="#b6">[7]</ref>, where the authors build a network that is robust to the initialization provided by different face detectors. Despite the differing goals of the two papers, it is interesting to note that both suggest that a combination of using spatial transformers with heads corresponding to salient face regions produces marked improvements over a single large network. We provide the details of our implementation for producing landmarks corresponding to eyes, irises, and lips, as well as quality and inference performance benchmarks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Attention mesh</head><p>Model architecture The model accepts a 256 × 256 image as input. This image is provided by either the face detector or via tracking from a previous frame. After extract- ing a 64 × 64 feature map, the model splits into several sub-models (Figure <ref type="figure" target="#fig_1">2</ref>). One submodel predicts all 478 face mesh landmarks in 3D and defines crop bounds for each region of interest. The remaining submodels predict region landmarks from the corresponding 24×24 feature maps that are obtained via the attention mechanism.</p><p>We concentrate on three facial regions with key contours: the lips and two eyes (Figure <ref type="figure" target="#fig_0">1</ref>). Each eye submodel predicts the iris as a separate output after reaching the spatial resolution of 6 × 6. This allows the reuse of eye features while keeping dynamic iris independent from the more static eye landmarks.</p><p>Individual submodels allow us to control the network capacity dedicated to each region and boost quality where necessary. To further improve the accuracy of the predictions, we apply a set of normalizations to ensure that the eyes and lips are aligned with the horizontal and are of uniform size.</p><p>We train the attention mesh network in two phases. First, we employ ideal crops from the ground truth with slight augmentations and train all submodels independently. Then, we obtain crop locations from the model itself and train again to adapt the region submodels to them.</p><p>Attention mechanism Several attention mechanisms (soft and hard) have been developed for visual feature extraction <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b3">4]</ref>. These attention mechanisms sample a grid of 2D points in feature space and extract the features under the sampled points in a differentiable manner (e.g. using 2D Gaussian kernels or affine transformations and differentiable interpolations). This allows to train architectures endto-end and enrich the features that are used by the attention mechanism. Specifically, we use a spatial transformer mod-ule <ref type="bibr" target="#b3">[4]</ref> to extract 24 × 24 region features from the 64 × 64 feature map. The spatial transformer is controlled by an affine transformation matrix θ (Equation <ref type="formula" target="#formula_0">1</ref>) and allows us to zoom, rotate, translate, and skew the sampled grid of points.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>θ =</head><p>x x sh x t x sh y s y t y</p><p>This affine transformation can be constructed either via supervised prediction of matrix parameters, or by computing them from the output of the face mesh submodel. Dataset Our dataset contains 30K in-the-wild mobile camera photos taken with numerous camera sensors and in varied conditions. We used manual annotation with special emphasis on consistency for salient contours to obtain the ground truth mesh vertex coordinates in 2D. The Z coordinate was approximated using a synthetic model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Results</head><p>To evaluate our unified approach, we compare it against the cascaded model which consists of independently trained region-specific models for the base mesh, eyes and lips that are run in succession.</p><p>Performance Table <ref type="table" target="#tab_0">1</ref> demonstrates that the attention mesh runs 25%+ faster than the cascade of separate face and region models on a typical modern mobile device. The performance has been measured using the TFLite GPU inference engine <ref type="bibr" target="#b5">[6]</ref>. An additional 5% speed-up is achieved due to the reduction of costly CPU-GPU synchronizations, since the whole attention mesh inference is performed in one pass on the GPU.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Model</head><p>Inference Mesh quality A quantitative comparison of both models is presented in Table <ref type="table" target="#tab_1">2</ref>. As the representative metric, we employ the mean distance between the predicted and ground truth locations of a specific subset of the points, normalized by 3D interocular distance (or the distance between the corners in the case of lips and eyes) for scale invariance. The attention mesh model outperforms the cascade of models on the eye regions and demonstrates comparable performance on the lips region. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Applications</head><p>The performance of our model enables several real-time AR applications like virtual try-on of makeup and puppeteering.</p><p>AR Makeup Accurate registration of the face mesh is critical to applications like AR makeup where even small errors in alignment can drive the rendered effect into the "uncanny valley" <ref type="bibr" target="#b7">[8]</ref>. We built a lipstick rendering solution (Figure <ref type="figure" target="#fig_3">4</ref>) on top of our attention mesh model by using the contours provided by the lip submodel. A/B testing on 10 images and 80 people showed that 46% of AR samples were actually classified as real and 38% of real samplesas AR. Puppeteering Our model can also be used for puppeteering and facial triggers. We built a small fully connected model that predicts 10 blend shape coefficients for the mouth and 8 blend shape coefficients for each eye. We feed the output of the attention mesh submodels to this blend shape network. In order to handle differences between various human faces, we apply Laplacian mesh editing to morph a canonical mesh into the predicted mesh <ref type="bibr" target="#b2">[3]</ref>. This lets us use the blend shape coefficients for different human faces without additional fine-tuning. We demonstrate some results in Figure <ref type="figure" target="#fig_4">5</ref>. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Conclusion</head><p>We present a unified model that enables accurate face mesh prediction in real-time. By using a differentiable attention mechanism, we are able to devote computational resources to salient face regions without incurring the performance penalty of running independent regionspecific models. Our model and demos will soon be avail-able in MediaPipe (https://github.com/google/ mediapipe).</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 .</head><label>1</label><figDesc>Figure 1. Salient contours predicted by Attention Mesh submodels</figDesc><graphic url="image-1.png" coords="1,332.49,227.72,188.99,188.99" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 .</head><label>2</label><figDesc>Figure 2. The inference pipeline and the model architecture overview</figDesc><graphic url="image-2.png" coords="2,50.11,82.96,495.00,210.06" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 .</head><label>3</label><figDesc>Figure 3. Spatial transformer as the attention mechanism</figDesc><graphic url="image-3.png" coords="2,320.68,479.18,212.61,202.40" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 4 .</head><label>4</label><figDesc>Figure 4. Virtual makeup base mesh without refinements (left) vs. attention mesh with submodels (right)</figDesc><graphic url="image-4.png" coords="3,309.98,191.08,115.77,79.29" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 5 .</head><label>5</label><figDesc>Figure 5. Puppeteering</figDesc><graphic url="image-6.png" coords="3,309.98,472.22,115.76,108.95" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 .</head><label>1</label><figDesc>Performance on Pixel 2XL (ms)</figDesc><table><row><cell></cell><cell>Time (ms)</cell></row><row><cell>Mesh</cell><cell>8.82</cell></row><row><cell>Lips</cell><cell>4.18</cell></row><row><cell>Eye &amp; iris</cell><cell>4.70</cell></row><row><cell>Cascade (sum of above)</cell><cell>22.4</cell></row><row><cell>Attention Mesh</cell><cell>16.6</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 .</head><label>2</label><figDesc>Mean normalized error in 2D.</figDesc><table><row><cell>Model</cell><cell cols="2">All Lips Eyes</cell></row><row><cell>Mesh</cell><cell>2.99 3.28</cell><cell>6.6</cell></row><row><cell>Cascade</cell><cell cols="2">2.99 2.70 6.28</cell></row><row><cell cols="3">Attention mesh 3.11 2.89 6.04</cell></row></table></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">A morphable model for the synthesis of 3D faces</title>
		<author>
			<persName><forename type="first">Volker</forename><surname>Blanz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Thomas</forename><surname>Vetter</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of 36th Internaional Conference and Exhibition on Computer Graphics and Interactive Techniques</title>
				<meeting>36th Internaional Conference and Exhibition on Computer Graphics and Interactive Techniques</meeting>
		<imprint>
			<date type="published" when="1999">1999</date>
			<biblScope unit="page" from="187" to="194" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">Draw: A recurrent neural network for image generation</title>
		<author>
			<persName><forename type="first">Karol</forename><surname>Gregor</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ivo</forename><surname>Danihelka</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alex</forename><surname>Graves</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Danilo</forename><surname>Jimenez Rezende</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Daan</forename><surname>Wierstra</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1502.04623</idno>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Dual laplacian morphing for triangular meshes</title>
		<author>
			<persName><forename type="first">Jianwei</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ligang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Guozhao</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computer Animation and Virtual Worlds</title>
		<imprint>
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="issue">45</biblScope>
			<biblScope unit="page" from="271" to="277" />
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Spatial transformer networks</title>
		<author>
			<persName><forename type="first">Max</forename><surname>Jaderberg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Karen</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrew</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
				<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="2017" to="2025" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<author>
			<persName><forename type="first">Yury</forename><surname>Kartynnik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Artsiom</forename><surname>Ablavatski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ivan</forename><surname>Grishchenko</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Matthias</forename><surname>Grundmann</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1502.04623</idno>
		<title level="m">Real-time Facial Surface Geometry from Monocular Video on Mobile GPUs</title>
				<imprint>
			<date type="published" when="2001">July 2019. 1</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<author>
			<persName><forename type="first">Juhyun</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nikolay</forename><surname>Chirkov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ekaterina</forename><surname>Ignasheva</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yury</forename><surname>Pisarchyk</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mogan</forename><surname>Shieh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fabio</forename><surname>Riccardi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Raman</forename><surname>Sarokin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrei</forename><surname>Kulik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Matthias</forename><surname>Grundmann</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1907.01989</idno>
		<title level="m">On-device neural net inference with mobile gpus</title>
				<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">A deep regression architecture with two-stage re-initialization for high performance facial landmark detection</title>
		<author>
			<persName><forename type="first">J</forename><surname>Lv</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Shao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Xing</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Zhou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2017 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
				<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="3691" to="3700" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">The uncanny valley: Effect of realism on the impression of artificial human faces</title>
		<author>
			<persName><forename type="first">Junichiro</forename><surname>Seyama</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ruth</forename><forename type="middle">S</forename><surname>Nagayama</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Presence: Teleoper. Virtual Environ</title>
		<imprint>
			<biblScope unit="volume">16</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page">337351</biblScope>
			<date type="published" when="2003">Aug. 2007. 3</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
