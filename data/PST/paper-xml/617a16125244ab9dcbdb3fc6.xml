<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Large Scale Learning on Non-Homophilous Graphs: New Benchmarks and Strong Simple Methods</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Derek</forename><surname>Lim</surname></persName>
							<affiliation key="aff6">
								<orgName type="laboratory">Equal contribution. 35th Conference on Neural Information Processing Systems</orgName>
								<address>
									<postCode>2021)</postCode>
									<settlement>Sydney</settlement>
									<region>NeurIPS</region>
									<country key="AU">Australia</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Felix</forename><surname>Hohne</surname></persName>
							<affiliation key="aff6">
								<orgName type="laboratory">Equal contribution. 35th Conference on Neural Information Processing Systems</orgName>
								<address>
									<postCode>2021)</postCode>
									<settlement>Sydney</settlement>
									<region>NeurIPS</region>
									<country key="AU">Australia</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Xiuyu</forename><surname>Li</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Sijia</forename><forename type="middle">Linda</forename><surname>Huang</surname></persName>
							<affiliation key="aff6">
								<orgName type="laboratory">Equal contribution. 35th Conference on Neural Information Processing Systems</orgName>
								<address>
									<postCode>2021)</postCode>
									<settlement>Sydney</settlement>
									<region>NeurIPS</region>
									<country key="AU">Australia</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Vaishnavi</forename><surname>Gupta</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Omkar</forename><surname>Bhalerao</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Ser-Nam</forename><surname>Lim</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Facebook</forename><surname>Ai</surname></persName>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="institution">Cornell University</orgName>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff1">
								<orgName type="institution">Cornell University</orgName>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff2">
								<orgName type="institution">Cornell University</orgName>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff3">
								<orgName type="institution">Cornell University</orgName>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff4">
								<orgName type="institution">Cornell University</orgName>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff5">
								<orgName type="institution">Cornell University</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Large Scale Learning on Non-Homophilous Graphs: New Benchmarks and Strong Simple Methods</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.2" ident="GROBID" when="2022-12-25T13:51+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Many widely used datasets for graph machine learning tasks have generally been homophilous, where nodes with similar labels connect to each other. Recently, new Graph Neural Networks (GNNs) have been developed that move beyond the homophily regime; however, their evaluation has often been conducted on small graphs with limited application domains. We collect and introduce diverse nonhomophilous datasets from a variety of application areas that have up to 384x more nodes and 1398x more edges than prior datasets. We further show that existing scalable graph learning and graph minibatching techniques lead to performance degradation on these non-homophilous datasets, thus highlighting the need for further work on scalable non-homophilous methods. To address these concerns, we introduce LINKX -a strong simple method that admits straightforward minibatch training and inference. Extensive experimental results with representative simple methods and GNNs across our proposed datasets show that LINKX achieves state-ofthe-art performance for learning on non-homophilous graphs. Our codes and data are available at https://github.com/CUAI/Non-Homophily-Large-Scale.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Graph learning methods generate predictions by leveraging complex inductive biases captured in the topology of the graph <ref type="bibr" target="#b6">[7]</ref>. A large volume of work in this area, including graph neural networks (GNNs), exploits homophily as a strong inductive bias, where connected nodes tend to be similar to each other in terms of labels <ref type="bibr" target="#b45">[46,</ref><ref type="bibr" target="#b2">3]</ref>. Such assumptions of homophily, however, do not always hold true. For example, malicious node detection, a key application of graph machine learning, is known to be non-homophilous in many settings <ref type="bibr" target="#b54">[55,</ref><ref type="bibr" target="#b12">13,</ref><ref type="bibr" target="#b24">25,</ref><ref type="bibr" target="#b10">11]</ref>. Further, while new GNNs that work better in these non-homophilous settings have been developed <ref type="bibr" target="#b81">[82,</ref><ref type="bibr" target="#b43">44,</ref><ref type="bibr" target="#b80">81,</ref><ref type="bibr" target="#b16">17,</ref><ref type="bibr" target="#b14">15,</ref><ref type="bibr" target="#b72">73,</ref><ref type="bibr" target="#b35">36,</ref><ref type="bibr" target="#b34">35,</ref><ref type="bibr" target="#b8">9,</ref><ref type="bibr" target="#b53">54]</ref>, their evaluation is limited to a few graph datasets used by Pei et al. <ref type="bibr" target="#b57">[58]</ref> (collected by <ref type="bibr" target="#b60">[61,</ref><ref type="bibr" target="#b65">66,</ref><ref type="bibr" target="#b47">48]</ref>) that have certain undesirable properties such as small size, narrow range of application areas, and high variance between different train/test splits <ref type="bibr" target="#b81">[82]</ref>. Consequently, method scalability has not been thoroughly studied in non-homophilous graph learning. In fact, many non-homophilous techniques frequently require more parameters and computational resources <ref type="bibr" target="#b81">[82,</ref><ref type="bibr" target="#b0">1,</ref><ref type="bibr" target="#b16">17]</ref>, which is neither evident nor detrimental when they are evaluated on very small datasets. Even though scalable graph learning techniques do exist, these methods generally cannot be directly applied to the non-homophilous setting, as they oftentimes assume homophily in their construction <ref type="bibr" target="#b70">[71,</ref><ref type="bibr" target="#b31">32,</ref><ref type="bibr" target="#b19">20,</ref><ref type="bibr" target="#b9">10]</ref>.</p><p>Non-homophily in graphs also degrades proven graph learning techniques that have been instrumental to strong performance in scalable graph learning. For instance, label propagation, personalized PageRank, and low-pass graph filtering have been used for scalable graph representation learning models, but these methods all assume homophily <ref type="bibr" target="#b70">[71,</ref><ref type="bibr" target="#b31">32,</ref><ref type="bibr" target="#b19">20,</ref><ref type="bibr" target="#b9">10]</ref>. Moreover, we give empirical evidence that existing minibatching techniques in graph learning <ref type="bibr" target="#b15">[16,</ref><ref type="bibr" target="#b76">77]</ref> significantly degrade performance in non-homophilous settings. In response, we develop a novel model, LINKX, that addresses these concerns; LINKX outperforms existing graph learning methods on large-scale nonhomophilous datasets and admits a simple minibatching procedure that maintains strong performance.</p><p>To summarize, we demonstrate three key areas of deficiency as mentioned above, namely: (1) that there is a lack of large, high-quality datasets covering different non-homophilous applications, (2) that current graph minibatching techniques and scalable methods do not work well in non-homophilous settings, and (3) that prior non-homophilous methods are not scalable. To these ends, this paper makes the following contributions:</p><p>Dataset Collection and Benchmarking. We collect a diverse series of large, non-homophilous graph datasets and define new node features and tasks for classification. These datasets are substantially larger than previous non-homophilous datasets, span wider application areas, and capture different types of complex label-topology relationships. With these proposed datasets, we conduct extensive experiments with 14 graph learning methods and 3 graph minibatching techniques that are broadly representative of the graph machine learning model space.</p><p>Analyzing Scalable Methods and Minibatching. We analyze current graph minibatching techniques like GraphSAINT <ref type="bibr" target="#b76">[77]</ref> in non-homophilous settings, showing that they substantially degrade performance in experiments. Also, we show empirically that scalable methods for graph learning like SGC and C&amp;S <ref type="bibr" target="#b70">[71,</ref><ref type="bibr" target="#b31">32]</ref> do not perform well in non-homophilous settings -even though they achieve state-of-the-art results on many homophilous graph benchmarks. Finally, we demonstrate that existing non-homophilous methods often suffer from issues with scalability and performance in large non-homophilous graphs, in large part due to a lack of study of large-scale non-homophilous graph learning.</p><p>LINKX: a strong, simple method. We propose a simple method LINKX that achieves excellent results for non-homophilous graphs while overcoming the above-mentioned minibatching issues. LINKX works by separately embedding the adjacency A and node features X, then combining them with multilayer perceptrons and simple transformations, as illustrated in Figure <ref type="figure" target="#fig_0">1</ref>. It generalizes node feature MLP and LINK regression <ref type="bibr" target="#b78">[79]</ref>, two baselines that often work well on non-homophilous graphs. This method is simple to train and evaluate in a minibatched fashion, and does not face the performance degradation that other methods do in the minibatch setting. We develop the model and give more details in Section 4. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Prior Work</head><p>Graph Representation Learning. Graph neural networks <ref type="bibr" target="#b27">[28,</ref><ref type="bibr" target="#b37">38,</ref><ref type="bibr" target="#b68">69]</ref> have demonstrated their utility on a variety of graph machine learning tasks. Most GNNs are constructed by stacking layers that propagate transformed node features, which are then aggregated via different mechanisms. The neighborhood aggregation used in many existing GNNs implicitly leverage homophily, so they often fail to generalize on non-homophilous graphs <ref type="bibr" target="#b81">[82,</ref><ref type="bibr" target="#b5">6]</ref>. Indeed, a wide range of GNNs operate as low-pass graph filters <ref type="bibr" target="#b52">[53,</ref><ref type="bibr" target="#b70">71,</ref><ref type="bibr" target="#b5">6]</ref> that smooth features over the graph topology, which produces similar representations and thus similar predictions for neighboring nodes.</p><p>Scalable methods. A variety of scalable graph learning methods have been developed for efficient computation in larger datasets <ref type="bibr" target="#b76">[77,</ref><ref type="bibr" target="#b15">16,</ref><ref type="bibr" target="#b74">75,</ref><ref type="bibr" target="#b27">28,</ref><ref type="bibr" target="#b70">71,</ref><ref type="bibr" target="#b31">32,</ref><ref type="bibr" target="#b19">20,</ref><ref type="bibr" target="#b9">10]</ref>. Many of these methods explicitly make use of an assumption of homophily in the data <ref type="bibr" target="#b70">[71,</ref><ref type="bibr" target="#b31">32,</ref><ref type="bibr" target="#b19">20,</ref><ref type="bibr" target="#b9">10]</ref>. By leveraging this assumption, several simple, inexpensive models are able to achieve state-of-the-art performance on homophilic datasets <ref type="bibr" target="#b70">[71,</ref><ref type="bibr" target="#b31">32]</ref>. However, these methods are unable to achieve comparable performance in non-homophilous settings, as we show empirically in Section 5.</p><p>Graph sampling. As node representations depend on other nodes in the graph, there are no simple minibatching techniques in graph learning as there are for i.i.d. data. To scale to large graphs, one line of work samples nodes that are used in each layer of a graph neural network <ref type="bibr" target="#b27">[28,</ref><ref type="bibr" target="#b74">75,</ref><ref type="bibr" target="#b13">14]</ref>. Another family of methods samples subgraphs of an input graph, then passes each subgraph through a GNN to make a prediction for each node of the subgraph <ref type="bibr" target="#b15">[16,</ref><ref type="bibr" target="#b75">76,</ref><ref type="bibr" target="#b76">77]</ref>. While these methods are useful for scalable graph learning, we show that they substantially degrade performance in our non-homphilous experiments (see Section 5).</p><p>Non-Homophilous methods. Various GNNs have been proposed to achieve higher performance in low-homophily settings <ref type="bibr" target="#b81">[82,</ref><ref type="bibr" target="#b43">44,</ref><ref type="bibr" target="#b80">81,</ref><ref type="bibr" target="#b16">17,</ref><ref type="bibr" target="#b14">15,</ref><ref type="bibr" target="#b72">73,</ref><ref type="bibr" target="#b35">36,</ref><ref type="bibr" target="#b34">35]</ref>. Geom-GCN <ref type="bibr" target="#b57">[58]</ref> introduces a geometric aggregation scheme, MixHop <ref type="bibr" target="#b0">[1]</ref> proposes a graph convolutional layer that mixes powers of the adjacency matrix, GPR-GNN <ref type="bibr" target="#b16">[17]</ref> features learnable weights that can be positive and negative in feature propagation, GCNII <ref type="bibr" target="#b14">[15]</ref> allows deep graph convolutional networks with relieved oversmoothing, which empirically performs better in non-homophilous settings, and H 2 GCN <ref type="bibr" target="#b81">[82]</ref> shows that separation of ego and neighbor embeddings, aggregation in higher-order neighborhoods, and the combination of intermediate representations improves GNN performance in low-homophily.</p><p>There are several recurring design decisions across these methods that appear to strengthen performance in non-homophilous settings: using higher-order neighborhoods, decoupling neighbor information from ego information, and combining graph information at different scales <ref type="bibr" target="#b81">[82]</ref>. Many of these design choices require additional overhead (see Section 4.3), thus reducing their scalability.</p><p>Datasets. The widely used citation networks Cora, Citeseer, and Pubmed <ref type="bibr" target="#b61">[62,</ref><ref type="bibr" target="#b73">74]</ref> are highly homophilous (see Appendix A) <ref type="bibr" target="#b81">[82]</ref>. Recently, the Open Graph Benchmark <ref type="bibr" target="#b30">[31]</ref> has provided a series of datasets and leaderboards that improve the quality of evaluation in graph representation learning; however, most of the node classification datasets tend to be homophilous, as noted in past work <ref type="bibr" target="#b81">[82]</ref> and expanded upon in Appendix A.2. A comparable set of high-quality benchmarks to evaluate non-homophilous methods does not currently exist.</p><p>3 Datasets for Non-Homophilous Graph Learning</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Currently Used Datasets</head><p>The most widely used datasets to evaluate non-homophilous graph representation learning methods were used by Pei et al. <ref type="bibr" target="#b57">[58]</ref> (and collected by <ref type="bibr" target="#b60">[61,</ref><ref type="bibr" target="#b65">66,</ref><ref type="bibr" target="#b47">48]</ref>); see our Table <ref type="table" target="#tab_1">1</ref> for statistics. However, these datasets have fundamental issues. First, they are very small -the Cornell, Texas, and Wisconsin datasets have between 180-250 nodes, and the largest dataset Actor has 7,600 nodes. In analogy to certain pitfalls of graph neural network evaluation on small (homophilic) datasets discussed in <ref type="bibr" target="#b62">[63]</ref>, evaluation on the datasets of Pei et al. <ref type="bibr" target="#b57">[58]</ref> is plagued by high variance across different train/test splits (see results in <ref type="bibr" target="#b81">[82]</ref>). The small size of these datasets may tend to create models that are more prone to overfitting <ref type="bibr" target="#b20">[21]</ref>, which prevents the scaling up of GNNs designed for non-homophilous settings.</p><p>Peel <ref type="bibr" target="#b56">[57]</ref> also studies node classification on network datasets with various types of relationships between edges and labels. However, they only study methods that act on graph topology, and thus their datasets do not necessarily have node features. We take inspiration from their work, by testing on Pokec and Facebook networks with node features that we define, and by introducing other year-prediction tasks on citation networks that have node features.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">An Improved Homophily Measure</head><p>Various metrics have been proposed to measure the homophily of a graph. However, these metrics are sensitive to the number of classes and the number of nodes in each class. Let G = (V, E) be a graph with n nodes, none of which are isolated. Further let each node u ∈ V have a class label k u ∈ {0, 1, . . . , C − 1} for some number of classes C, and denote by C k the set of nodes in class k. The edge homophily <ref type="bibr" target="#b81">[82]</ref> is the proportion of edges that connect two nodes of the same class:</p><formula xml:id="formula_0">h = |{(u, v) ∈ E : k u = k v }| |E| .<label>(1)</label></formula><p>Another related measure is what we call the node homophily <ref type="bibr" target="#b57">[58]</ref>, defined as is the number of neighbors of u that have the same class label. We focus on the edge homophily (1) in this work, but find that node homophily tends to have similar qualitative behavior in experiments.</p><p>The sensitivity of edge homophily to the number of classes and size of each class limits its utility. We consider a null model for graphs in which the graph topology is independent of the labels; suppose that nodes with corresponding labels are fixed, and include edges uniformly at random in the graph that are independent of node labels. Under this null model, a node u ∈ V would be expected to have d</p><formula xml:id="formula_1">(ku) u /d u ≈ |C ku |/n</formula><p>as the proportion of nodes of the same class that they connect to <ref type="bibr" target="#b2">[3]</ref>. For a dataset with C balanced classes, we would thus expect the edge homophily to be around 1  C , so the interpretation of the measure depends on the number of classes. Also, if classes are imbalanced, then the edge homophily may be misleadingly large. For instance, if 99% of nodes were of one class, then most edges would likely be within that same class, so the edge homophily would be high, even when the graph is generated from the null model where labels are independent of graph topology. Thus, the edge homophily does not capture deviation of the label distribution from the null model.</p><p>We introduce a metric that better captures the presence or absence of homophily. Unlike the edge homophily, our metric measures excess homophily that is not expected from the above null model where edges are randomly wired. Our metric does not distinguish between different non-homophilous settings (such as heterophily or independent edges); we believe that there are too many degrees of freedom in non-homophilous settings for a single scalar quantity to be able to distinguish them all. Our measure is given as:</p><formula xml:id="formula_2">ĥ = 1 C − 1 C−1 k=0 h k − |C k | n + ,<label>(2)</label></formula><p>where [a] + = max(a, 0), and h k is the class-wise homophily metric</p><formula xml:id="formula_3">h k = u∈C k d (ku) u u∈C k d u .<label>(3)</label></formula><p>Note that ĥ ∈ [0, 1], with a fully homophilous graph (in which every node is only connected to nodes of the same class) having ĥ = 1. Since each class-wise homophily metric h k only contributes positive deviations from the null expected proportion |C k |/n, the class-imbalance problem is substantially mitigated. Also, graphs in which edges are independent of node labels are expected to have ĥ ≈ 0, for any number of classes. Our measure ĥ measures presence of homophily, but does not distinguish between the many types of possibly non-homophilous relationships. This is reasonable given the diversity of non-homophilous relationships. For example, non-homophily can imply independence of edges and classes, extreme heterophily, connections only among subsets of classes, or certain chemically / biologically determined relationships. Indeed, these relationships are very different, and are better captured by more than one scalar quantity, such as the compatibility matrices presented in the appendix. Further discussion is given in Appendix A. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Proposed Datasets</head><p>Here, we detail the non-homophilous datasets that we propose for graph machine learning evaluation.</p><p>Our datasets and tasks span diverse application areas. Penn94 <ref type="bibr" target="#b66">[67]</ref>, Pokec <ref type="bibr" target="#b40">[41]</ref>, genius <ref type="bibr" target="#b42">[43]</ref>, and twitch-gamers <ref type="bibr" target="#b59">[60]</ref> are online social networks, where the task is to predict reported gender, certain account labels, or use of explicit content on user accounts. For the citation networks arXiv-year <ref type="bibr" target="#b30">[31]</ref> and snap-patents <ref type="bibr" target="#b41">[42,</ref><ref type="bibr" target="#b40">41]</ref> the goal is to predict year of paper publication or the year that a patent is granted. The dataset wiki consists of Wikipedia articles, where the goal is to predict total page views of each article. Detailed descriptions about the graph structure, node features, node labels, and licenses of each dataset are given in Appendix D.2.</p><p>Most of these datasets have been used for evaluation of graph machine learning models in past work; we make adjustments such as modifying node labels and adding node features that allow for evaluation of GNNs in non-homophilous settings. We define node features for Pokec, genius, and snap-patents, and we also define node labels for arXiv-year, snap-patents, and genius. Additionally, we crawl and clean the large-scale wiki dataset -a new Wikipedia dataset where the task is to predict page views, which is non-homophilous with respect to the graph of articles connected by links between articles (see Appendix D.3). This wiki dataset has 1,925,342 nodes and 303,434,860 edges, so training and inference require scalable algorithms.</p><p>Basic dataset statistics are given in Table <ref type="table" target="#tab_2">2</ref>. Note the substantial difference between the size of our datasets and those of Pei et al. <ref type="bibr" target="#b57">[58]</ref> in Table <ref type="table" target="#tab_1">1</ref>; our datasets have up to 384x more nodes and 1398x more edges. The homophily measures along with the lower empirical performance of homophilyassuming models (Section 5) and examination of compatibility matrices (Appendix A) show that our datasets are indeed non-homophilous. As there is little study in large-scale non-homophilous graph learning, our proposed large datasets strongly motivate the need for developing a new, scalable approach that can accurately learn on non-homophilous graphs. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Motivation from two simple baselines</head><p>Here, we detail two simple baselines for node classification that we build on to develop LINKX.</p><p>MLP on node features. A naïve method for node classification is to ignore the graph topology and simply train an MLP on node features. For the same reason that the graph topology has more complicated relationships with label distributions in non-homophilous graphs, many GNNs are not able to effectively leverage the graph topology in these settings. Thus, MLPs can actually perform comparatively well on non-homophilous graphs -achieving higher or approximately equal performance to various GNNs <ref type="bibr" target="#b81">[82]</ref>.</p><p>LINK regression on graph topology. On the other extreme, there is LINK <ref type="bibr" target="#b78">[79]</ref> -a simple baseline that only utilizes graph topology. In particular, we consider LINK regression, which trains a logistic regression model in which each node's features are taken from a column of the adjacency matrix.</p><p>Letting A ∈ {0, 1} n×n be the binary adjacency matrix of the graph, and W ∈ R c×n be a learned weight matrix, LINK computes class probabilities as</p><formula xml:id="formula_4">Y = softmax(WA).<label>(4)</label></formula><p>Let u ∈ {1, . . . , n} be a specific node, and let k ∈ {1, . . . , c} be a specific class. Then, expanding the matrix multiplication, the log-odds of node u belonging to class k is given by</p><formula xml:id="formula_5">(WA) ku = v∈N (u) W kv ,<label>(5)</label></formula><p>where N (u) contains the 1-hop neighbors of u. In other words, the logit is given by the sum of weights W kv across the 1-hop neighbors of u. If a specific node v has many neighbors of class k, then W kv is probably large, as we would expect with a high probability that any neighbor of v is of class k. In this sense, LINK is like a 2-hop method: for a given node u, the probability of being in a given class is related to the class memberships of u's 2-hop neighbors in N (v) for each neighbor v ∈ N (u). Related interpretations of LINK as a method acting on 2-hop paths between nodes are given by Altenburger and Ugander <ref type="bibr" target="#b2">[3]</ref>.</p><p>Though it is simple and has been overlooked in the recent non-homophilous GNN literature, LINK has been found to perform well in certain node classification tasks like gender prediction in social networks <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b3">4]</ref>. A major reason why LINK does well in many settings is exactly because it acts as a 2-hop method. For example, while 1-hop neighbors are often not so informative for gender prediction in social networks due to lack of homophily, 2-hop neighbors are very informative due to so-called "monophily," whereby many nodes have extreme preferences for connecting to a certain class <ref type="bibr" target="#b2">[3]</ref>. Beyond just gender prediction, we show in Section 5 that LINK empirically outperforms many models across the various application areas of the non-homophilous datasets we propose.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">LINKX</head><p>We combine these two simple baselines through simple linear transformations and component-wise nonlinearities. Let X ∈ R D×n denote the matrix of node features with input dimension D, and let [h 1 ; h 2 ] denote concatenation of vectors h 1 and h 2 . Then our model outputs predictions Y through the following mapping:</p><formula xml:id="formula_6">h A = MLP A (A) ∈ R d×n (6) h X = MLP X (X) ∈ R d×n<label>(7)</label></formula><formula xml:id="formula_7">Y = MLP f σ W[h A ; h X ] + h A + h X ,<label>(8)</label></formula><p>in which d is the hidden dimension, W ∈ R d×2d is a weight matrix, and σ is a component-wise nonlinearity (which we take to be ReLU). We call our model LINKX, as it extends LINK with node feature information from the matrix X. A diagram of LINKX is given in Figure <ref type="figure" target="#fig_0">1</ref>.</p><p>First, LINKX computes hidden representations h A of the adjacency (extending LINK) and h X of the feature matrix (as in node-feature MLPs). Then it combines these hidden representations through a linear transform W of their concatenation, with skip connections that add back in h A and h X to better preserve pure adjacency or node feature information. Finally, it puts this combined representation through a non-linearity and another MLP to make a prediction.</p><p>Separating then mixing adjacency and feature information. LINKX separately embeds the adjacency A to h A and the features X into h X before mixing them for a few reasons. First, we note that this design is reminiscent of fusion architectures in multimodal networks, where data from different modalities are processed and combined in a neural network <ref type="bibr" target="#b23">[24,</ref><ref type="bibr" target="#b77">78]</ref>. In our setting, we can view adjacency information and node feature information as separate modalities. Since node feature MLPs and LINK do well independently on different datasets, this allows us to preserve their individual performance if needed. Ignoring h X information is similar to just using LINK, and ignoring h A information is just using an node feature MLP. Still, to preserve the ability to just learn a similar mapping to LINK or to a node feature MLP, we find that having the additive skip connections helps to get performance at least as good as either baseline. Our initial empirical results showed that simply concatenating adjacency and node features as input to a network does worse overall empirically (see Appendix C.1).</p><p>There are also computational benefits to our design choices. Embedding A is beneficial for depth as adding more layers to the MLPs only gives an O(d 2 ) cost -depending only on the hidden dimension d -and thus does not scale in the number of edges |E| as when adding layers to message-passing GNNs. This is because the graph information in A is already compressed to hidden feature vectors after the first linear mapping of MLP A , and we do not need to propagate along the graph in later steps. Moreover, this enables a sparse-dense matrix product to compute the first linear mapping of MLP A on A, which greatly increases efficiency as A is typically very sparse for real-world graphs.</p><p>Separate embeddings are key here, as this would not be possible if we for instance concatenated A and X when X is large and dense.</p><p>Simple minibatching. Message-passing GNNs must take graph topology into account when minibatching with techniques such as neighbor sampling, subgraph sampling, or graph partitioning. However, LINKX does not require this, as it utilizes graph information solely through defining adjacency matrix columns as features. Thus, we can train LINKX with standard stochastic gradient descent variants by taking i.i.d. samples of nodes along with the corresponding columns of the adjacency and feature matrix as features. This is much simpler than the graph minibatching procedures for message-passing GNNs, which require specific hyperparameter choices, have to avoid exponential blowup of number of neighbors per layer, and are generally more complex to implement <ref type="bibr" target="#b76">[77]</ref>. In Section 5.3, we use the simple LINKX minibatching procedure for large-scale experiments that show that LINKX with this minibatching style outperforms GNNs with graph minibatching methods. This is especially important on the scale of the wiki dataset, where none of our tested methods -other than MLP -is capable of running on a Titan RTX GPU with 24 GB GPU RAM (see Section 5).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Complexity Analysis</head><p>Using the above notation, a forward pass of LINKX has a time complexity of O d|E| + nd 2 L , in which d is the hidden dimension (which we assume to be on the same order as the input feature dimension D), L is the number of layers, n is the number of nodes, and |E| is the number of edges. We require a O(d|E|) cost for the first linear mapping of A and a O(d 2 ) cost per layer for MLP operations on hidden features, for L total layers and each of n nodes.</p><p>As mentioned above, message passing GNNs have to propagate using the adjacency in each layer, so they have an L|E| term in the complexity. For instance, an L-layer GCN <ref type="bibr" target="#b37">[38]</ref> with d hidden dimensions has O(dL|E| + nd 2 L) complexity, as it costs O(d|E|) to propagate features in each layer, and O(nd 2 ) to multiply by the weight matrix in each layer.</p><p>Non-homophilous methods often make modifications to standard architectures that increase computational cost, such as using higher-order neighborhoods or using additional hidden embeddings <ref type="bibr" target="#b81">[82]</ref>.</p><p>For instance, the complexity of MixHop <ref type="bibr" target="#b0">[1]</ref> is O(K(dL|E| + nd 2 L)), which has an extra factor K that is the number of adjacency powers to propagate with. The complexity of GCNII <ref type="bibr" target="#b14">[15]</ref> is asymptotically the same as that of GCN, but in practice it requires more computations per layer due to residual connections and linear combinations, and it also often achieves best performance with a large number of layers L. H 2 GCN <ref type="bibr" target="#b81">[82]</ref> is significantly more expensive due to its usage of strict two-hop neighborhoods, which requires it to form the squared adjacency A 2 . This makes the memory requirements intractable even for medium sized graphs (see Section 5).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Experiments</head><p>We conduct two sets of experiments for node classification on our proposed non-homophilous datasets. One set of experiments does full batch gradient descent training for all applicable methods. This of course limits the size of each model, as the large datasets require substantial GPU memory to train on. Our other set of experiments uses minibatching methods. As all graph-based methods run out of memory on the wiki dataset, even on 24 GB GPUs, we only include wiki results in the minibatching section. In all settings, our LINKX model matches or outperforms other methods. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Experimental Setup</head><p>Methods. We include both methods that are graph-agnostic and node-feature-agnostic as simple baselines. The node-feature-agnostic models of two-hop label propagation <ref type="bibr" target="#b56">[57]</ref> and LINK (logistic regression on the adjacency matrix) <ref type="bibr" target="#b78">[79]</ref> have been found to perform well in various non-homophilous settings, but they have often been overlooked by recent graph representation learning work. Also, we include SGC <ref type="bibr" target="#b70">[71]</ref> and C&amp;S <ref type="bibr" target="#b31">[32]</ref> as simple, scalable methods that perform well on homophilic datasets. We include a two-hop propagation variant of C&amp;S in analogy with two-step label propagation. In addition to representative general GNNs, we also include GNNs recently proposed for non-homophilous settings. The full list of methods is: Only node features: MLP <ref type="bibr" target="#b25">[26]</ref>. Only graph topology: label propagation (standard and two-hop) <ref type="bibr" target="#b79">[80,</ref><ref type="bibr" target="#b56">57]</ref>, LINK <ref type="bibr" target="#b78">[79]</ref>. Simple methods: SGC <ref type="bibr" target="#b70">[71]</ref>, C&amp;S <ref type="bibr" target="#b31">[32]</ref> and their two-hop variants. General GNNs: GCN <ref type="bibr" target="#b37">[38]</ref>, GAT <ref type="bibr" target="#b68">[69]</ref>, jumping knowledge networks (GCNJK, GATJK) <ref type="bibr" target="#b71">[72]</ref>, and APPNP <ref type="bibr" target="#b38">[39]</ref>. Non-homophilous methods: H 2 GCN <ref type="bibr" target="#b81">[82]</ref>, MixHop <ref type="bibr" target="#b0">[1]</ref>, GPR-GNN <ref type="bibr" target="#b16">[17]</ref>, GCNII <ref type="bibr" target="#b14">[15]</ref>, and LINKX (ours).</p><p>Minibatching methods. We also evaluate GNNs with various minibatching methods. We take GC-NJK <ref type="bibr" target="#b71">[72]</ref> and MixHop <ref type="bibr" target="#b0">[1]</ref> as our base models for evaluation, as they are representative of many GNN design choices and MixHop performs very well in full batch training. As other minibatching methods are trickier to make work with these models, we use the Cluster-GCN <ref type="bibr" target="#b15">[16]</ref> and GraphSAINT <ref type="bibr" target="#b76">[77]</ref> minibatching methods, which sample subgraphs. We include both the node based sampling and random walk based sampling variants of GraphSAINT. We compare these GNNs with MLP, LINK, and our LINKX, which use simple i.i.d. node minibatching.</p><p>Training and evaluation. Following other works in non-homophilous graph learning evaluation, we take a high proportion of training nodes <ref type="bibr" target="#b81">[82,</ref><ref type="bibr" target="#b57">58,</ref><ref type="bibr" target="#b72">73]</ref>; we run each method on the same five random 50/25/25 train/val/test splits for each dataset. All methods requiring gradient-based optimization are run for 500 epochs, with test performance reported for the learned parameters of highest validation performance. We use ROC-AUC as the metric for the class-imbalanced genius dataset (about 80% of nodes are in the majority class), as it is less sensitive to class-imbalance than accuracy. For other datasets, we use classification accuracy as the metric. Further experimental details are in Appendix B.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Full-Batch Results</head><p>Table <ref type="table" target="#tab_3">3</ref> lists the results of each method across the datasets that we propose. Our datasets reveal several important properties of non-homophilous node classification. Firstly, the stability of performance across runs is better for our datasets than those of Pei et al. <ref type="bibr" target="#b57">[58]</ref> (see <ref type="bibr" target="#b81">[82]</ref> results). Secondly, as suggested by prior theory and experiments <ref type="bibr" target="#b81">[82,</ref><ref type="bibr" target="#b0">1,</ref><ref type="bibr" target="#b16">17]</ref>, the non-homophilous GNNs usually do wellthough not necessarily on every dataset.</p><p>The core assumption of homophily in SGC and C&amp;S that enables them to be simple and efficient does not hold on these non-homophilous datasets, and thus the performance of these methods is typically relatively low. Still, as expected, two-hop variants generally improve upon their one-hop counter-parts in these low-homophily settings.</p><p>One consequence of using larger datasets for benchmarks is that the tradeoff between scalability and learning performance of non-homophilous methods has become starker, with some methods facing memory issues. This tradeoff is especially important to consider in light of the fact that many scalable graph learning methods rely on implicit or explicit homophily assumptions <ref type="bibr" target="#b70">[71,</ref><ref type="bibr" target="#b31">32,</ref><ref type="bibr" target="#b19">20,</ref><ref type="bibr" target="#b9">10]</ref>, and thus face issues when used in non-homophilous settings.</p><p>Finally, LINKX achieves superior performance on all datasets, taking advantage of LINK's power, while also being able to utilize node features where they provide additional information. Our experimental results for minibatched methods on our proposed datasets are in Table <ref type="table" target="#tab_4">4</ref>. Since GraphSAINT does not partition the nodes of the graph into subgraphs that cover all nodes, we test on the full input graph for the smaller datasets and uniformly random partitions of the graph into 10 induced subgraphs for the larger datasets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3">Minibatching Results</head><p>First, we note that both Cluster-GCN and GraphSAINT sampling lead to performance degradation for these methods on our proposed non-homophilous datasets. When compared to the full-batch training results of the previous section, classification accuracy is typically substantially lower. Further experiments in Appendix C.2 give evidence that the performance degradation is often more substantial in non-homophilous settings, and provides possible explanations for why this may be the case.</p><p>On the other hand, LINKX does not suffer much performance degradation with the simple i.i.d. node minibatching technique. In fact, it matches or outperforms all methods in this setting, often by a wide margin. Though LINK performs on par with LINKX in arXiv-year and pokec, our LINKX model significantly outperforms it on other datasets, again due to LINKX's ability to integrate node feature information. We again stress that the LINKX minibatching is very simple to implement, yet it still substantially outperforms other methods. Consequently, LINKX is generally well-suited for scalable node classification across a broad range of non-homophilous settings, surpassing even specially designed non-homophilous GNNs with current graph minibatching techniques.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Discussion and Conclusion</head><p>In this paper, we propose new, high-quality non-homophilous graph learning datasets, and we benchmark simple baselines and representative graph representation learning methods across these datasets. Further, we develop LINKX: a strong, simple, and scalable method for non-homophilous classification. Our experiments show that LINKX significantly outperforms other methods on our proposed datasets, thus providing one powerful method in the underexplored area of scalable learning on non-homophilous graphs. We hope that our contributions will provide researchers with new avenues of research in learning on non-homophilous graphs, along with better tools to test models and evaluate utility of new techniques.</p><p>While we do find utility in our proposed datasets and LINKX model, this work is somewhat limited by only focusing on transductive node classification. This setting is the most natural for studying performance in the absence of homophily, since here we define homophily in terms of the node labels, and previous non-homophilous GNN work using the Pei et al. <ref type="bibr" target="#b57">[58]</ref> data also studies this setting exclusively <ref type="bibr" target="#b81">[82,</ref><ref type="bibr" target="#b16">17]</ref>. Using other Facebook 100 datasets besides Penn94 <ref type="bibr" target="#b66">[67]</ref> would allow for inductive node classification, but LINKX does not directly generalize to this setting. Our proposed datasets and model LINKX could be used for link prediction, but this is left for future work.</p><p>Broader Impact. Fundamental research in graph learning on non-homophilous graphs has the potential for positive societal benefit. As a major application, it enables malicious node detection techniques in social networks and transaction networks that are not fooled by fraudsters' connections to legitimate users and customers. This is a widely studied task, and past works have noted that non-homophilous structures are present in many such networks <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b24">25,</ref><ref type="bibr" target="#b54">55]</ref>. We hope that this paper provides insight on the homophily limitations of existing scalable graph learning models and help researchers design scalable models that continue to work well in the non-homophilous regime, thus improving the quality of node classification on graphs more broadly. As our proposed datasets have diverse structures and our model performs well across all of these datasets, the potential for future application of our work to important non-homophilous tasks is high.</p><p>Nevertheless, our work could also have potential for different types of negative social consequences. Nefarious behavior by key actors could be one source of such consequences. Nonetheless, we expect that the actors that can make use of large-scale social networks for gender prediction as studied in our work are limited in number. Actors with both the capability and incentive to perform such operations probably mostly consist of entities with access to large social network data such as social media companies or government actors with auxiliary networks <ref type="bibr" target="#b49">[50]</ref>. Smaller actors can perform certain attacks, but this may be made more difficult by resource requirements such as the need for certain external information <ref type="bibr" target="#b49">[50]</ref> or the ability to add nodes and edges before an anonymized version of a social network is released <ref type="bibr" target="#b4">[5]</ref>. Furthermore, additional actors could make use of deanonymization attacks <ref type="bibr" target="#b29">[30,</ref><ref type="bibr" target="#b48">49,</ref><ref type="bibr" target="#b49">50]</ref> to reveal user identities in supposedly anonymized datasets.</p><p>Also, accidental consequences and implicit biases are a potential issue, even if the applications of the learning algorithms are benign and intended to benefit society <ref type="bibr" target="#b46">[47]</ref>. Performance of algorithms may vary substantially between intersectional subgroups of subjects -as in the case of vision-based gender predictors <ref type="bibr" target="#b11">[12]</ref> (and some have questioned the propriety of vision-based gender classifiers altogether). Thus, there may be disparate effects on different populations, so care should be taken to understand the impact of those differences across subgroups. Moreover, large datasets require computing resources, so projects can only be pursued by large entities at the possible expense of the individual and smaller research groups <ref type="bibr" target="#b7">[8]</ref>. This is alleviated by the fact that our experiments are each run on one GPU, and hence have significantly less GPU computing requirements than much current deep learning research. Thus, smaller research groups and independent researchers should find our work beneficial, and should be able to build on it.</p><p>Finally, the nature of collection of online user information also comes with notable ethical concerns.</p><p>Common notice-and-consent policies are often ineffective in actually protecting user privacy <ref type="bibr" target="#b51">[52]</ref>. Indeed, users may not actually have much choice in using certain platforms or sharing data due to social or economic reasons. Also, users are generally unable to fully read and understand all of the different privacy policies that they come across, and may not understand the implications of having their data available for long periods of time to entities with powerful inference algorithms. Furthermore, people may rely on obscurity for privacy <ref type="bibr" target="#b28">[29]</ref>, but this assumption may be ignored in courts of law, and it may be directly broken when data leaks or is released in aggregated form without sufficient privacy protections. Overall, while we believe that our work will benefit machine learning research and enable positive applications, we must still be aware of possible negative consequences. We include our proposed datasets along with codes for reproducing our results in the supplemental material. (d) Did you discuss whether and how consent was obtained from people whose data you're using/curating? [Yes] We discuss this in Appendix D. Most of the datasets were constructed and presented in previously published academic work. This mostly does not apply to the wiki dataset that we collect, as it is encyclopedic in nature. (e) Did you discuss whether the data you are using/curating contains personally identifiable information or offensive content? [Yes] In Appendix D, we discuss this. Our datasets are primarily numerical, so they do not contain offensive content. To the best of our knowledge, our datasets do not contain personally identifiable content. (a) h = 1, ĥ = 1 (b) h = 0, ĥ = 0 (c) h = .5, ĥ = 0 (d) h = .33, ĥ = 0 (e) h = .53, ĥ = .07 (f) h = .66, ĥ = .04 node is connected to one member of each class. Edge homophily depends on the number of classes, while our measure ĥ does not. (e,f) Random Erdős-Rényi graphs in which edges are independent of labels. Edge homophily is sensitive to class imbalance, while our measure ĥ is not.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A Compatibility Matrices and Statistics</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.1 Measuring Homophily</head><p>In Section 3.2, we consider metrics that attempt to capture the level of class-label homophily in a graph in a single scalar quantity. These metrics could be useful for practitioners who need to choose appropriate graph learning algorithms for some graph data that they have -performance of algorithms heavily depends on the homophily of the graph. Moreover, they are useful for choosing datasets to benchmark on, as we do in this paper.</p><p>A better representation of homophily may be given by the compatibility matrix, which consists of C 2 values instead of a single scalar value. Following previous work <ref type="bibr" target="#b81">[82]</ref>, for a graph G with C node classes we define the C × C compatibility matrix H by</p><formula xml:id="formula_8">H kl = |(u, v) ∈ E : k u = k, k v = l| |(u, v) ∈ E : k u = k| .<label>(9)</label></formula><p>This captures finer details of label-topology relationships in graphs than single scalar metrics capture.</p><p>For classes k and l, the entry H kl measures the proportion of edges from nodes of class k that are connected to nodes of class l. A homophilous graph has high values of H kk for each class k.</p><p>Compatibility matrices for our proposed datasets are shown in Figure <ref type="figure" target="#fig_5">3</ref>. As evidenced by the different patterns, the proposed datasets show interesting types of label-topology relationships besides homophily. For instance, the citation datasets arXiv-year and snap-patents have primarily lowertriangular structure, since most citations reference past work. In wiki, low view articles tend to link to each other, while mid-rank articles frequently link to highly viewed articles and vice versa. In Pokec, there is some heterophily, in that one gender has some preference for friends of another gender.</p><p>However, the compatibility matrix can be unwieldy for datasets with many classes. For the cases where a single scalar representation of homophily is desired, our introduced measure better captures the presence or absence of homophily than existing metrics. Figure <ref type="figure" target="#fig_3">2</ref> compares our measure ĥ to edge homophily on example graphs.    In contrast to the different compatibility matrix structures of our proposed non-homophilous datasets, much other graph data have primarily homophilous relationships, as can be seen in Figure <ref type="figure">4</ref> and Table <ref type="table" target="#tab_6">5</ref>. The Cora, CiteSeer, PubMed, ogbn-arXiv, and ogbn-products datasets are widely used as benchmarks for node classification <ref type="bibr" target="#b73">[74,</ref><ref type="bibr" target="#b30">31]</ref>, and are highly homophilous, as can be seen by the diagonally dominant structure of the compatibility matrices and by the high edge homophily and ĥ.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.2 Homophilous Data Statistics</head><p>We collected the oeis dataset displayed in the bottom right of Figure <ref type="figure">4</ref>. The nodes are entries in the Online Encyclopedia of Integer Sequences <ref type="bibr" target="#b63">[64]</ref>, and directed edges link an entry to any other entry that it cites. In analogy to arXiv-year and snap-patents, the node labels are the time of posting of the sequence. However, in this case the graph relationships are homophilous, even as we vary the number of distinct classes (time periods). This is in part due to differences between posting in this online encyclopedia and publication of academic papers or patents. For instance, there is less overhead to posting an entry in the OEIS, so users often post separate related entries and variants of these entries in rapid succession. Also, an entry in the encyclopedia often inspires other people to work on similar entries, which can be created in much less time than an academic follow-up work to a given paper.    <ref type="bibr" target="#b60">[61,</ref><ref type="bibr" target="#b65">66,</ref><ref type="bibr" target="#b47">48]</ref>). The "film" dataset is also referred to as "Actor". Note that there are no edges leading out of the nodes of class 1 in the Cornell dataset, so there is an empty row in its matrix.</p><p>These related entries tend to cite each other, which contributes to homophilic relationships over time. Thus, the data here does not follow the special temporal citation structure of academic publications and patents.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.3 Previous Non-Homophilous Data</head><p>For the six datasets in Pei et al. <ref type="bibr" target="#b57">[58]</ref> often used in evaluation of graph representation learning methods in non-homophilous regimes <ref type="bibr" target="#b81">[82]</ref>, basic statistics are listed in Table <ref type="table" target="#tab_1">1</ref> and compatibility matrices are displayed in Figure <ref type="figure" target="#fig_2">5</ref>. We propose datasets that have up to orders of magnitude more nodes and edges and come from a wider range of contexts. There are several cases of class-imbalance in these previously used datasets, which may make the edge homophily misleading. As discussed in Appendix A.1, our measure may be able to alleviate issues with edge homophily in measuring homophily of these datasets, and offers a way to distinguish between the Chameleon, Actor, and Squirrel datasets that all have similar edge homophily.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.4 General Non-homophilous Settings</head><p>Different settings in which non-homophilous / disassortative relationships are prevalent have been identified in the literature, and many of these non-homophilous settings are represented by our proposed datasets. We list these general non-homophilous settings for reference, with our proposed datasets that belong to these settings in parentheses:</p><p>• Gender relations in social or interaction networks <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b17">18,</ref><ref type="bibr" target="#b33">34]</ref> (Penn94, Pokec).</p><p>• Technological and internet relationships, such as in web page connections <ref type="bibr" target="#b50">[51,</ref><ref type="bibr" target="#b57">58]</ref> (wiki).</p><p>• Malicious or fraudulent nodes, such as in auction networks <ref type="bibr" target="#b12">[13,</ref><ref type="bibr" target="#b54">55]</ref> (genius).</p><p>• Publication time in citation networks <ref type="bibr" target="#b56">[57]</ref> (arXiv-year, snap-patents).</p><p>• Biological structures such as in food webs <ref type="bibr" target="#b24">[25]</ref> and protein interactions <ref type="bibr" target="#b50">[51]</ref>.</p><p>• Specific online user attributes <ref type="bibr" target="#b59">[60]</ref> (twitch gamers)</p><p>While not all example graph data from these contexts are non-homophilous, a diverse range are. In order to succeed in future applications in these contexts, it is of importance to develop methods that are able to handle non-homophilous structures.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.5 Class-Imbalance and Metrics</head><p>In this section, we present experiments that demonstrate an instance in which our metric is not affected by imbalanced classes, while edge homophily is. We generate graphs in which node labels are independent of edges by randomly choosing node labels and generating graph edges by the Erdős-Rényi random graph model <ref type="bibr" target="#b21">[22]</ref>. In particular, we fix the number of classes to two, the number of nodes to 100, and the probability of edge formation as .25 between every pair of nodes. Then we generate 100 samples of these random graphs, and compute the mean and standard deviation of both edge homophily h and our measure ĥ. As seen in Figure <ref type="figure">6</ref>, our measure ĥ is constantly near zero as we increase the size of the majority class, while the edge homophily h increases as the size of majority class increases. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Metric value h h</head><p>Figure <ref type="figure">6</ref>: Comparison of edge homophily h and our measure ĥ on random class-imbalanced graph data with edges independent of node labels. Three standard deviations are shaded. Our measure is mostly constant as the classes become more imbalanced, while edge homophily increases.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.6 Degree Distributions of Proposed Non-homophilous datasets</head><p>Past work has found that the degree distribution can affect the performance of models on node classification <ref type="bibr" target="#b81">[82]</ref>. As a result, we provide degree distributions of all of our proposed non-homophilous datasets in Figure <ref type="figure">7</ref>. The degree distributions are all heavy-tailed, as is typically expected in realworld graph data. The wiki distribution also has an interesting additional property, where the mode appears to be at a degree between 10 and 100.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.7 Two-hop homophily levels</head><p>While the homophily measures are based on one-hop information, we may also consider properties of two-hop neighborhoods. Even though there are limitations to measuring higher-order homophily <ref type="bibr" target="#b67">[68]</ref>, here we estimate an interesting quantity in two-hop neighborhoods. We use a node homophily measure, where the neighborhood of each node is defined to be the nodes of exactly two hops away: </p><p>This is an expensive operation to compute, so we approximate the sum over a random sample of k nodes, and then replace the normalization factor of 1 |V | by 1 k . Note that for each node v in the sample, the inner sum is still using the two-hop neighborhoods of the original graph. Table <ref type="table" target="#tab_8">6</ref> compares the computed one-hop and two-hop measures. The two-hop measure is mostly similar to or somewhat higher than the standard one-hop node homophily. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B Experimental Details</head><p>For gradient-based optimization, we use the AdamW optimizer <ref type="bibr" target="#b36">[37,</ref><ref type="bibr" target="#b44">45]</ref> with weight decay .001 and learning rate .01 by default, unless we tune the optimizer for a particular method (as noted below in B.1). Hyperparameter tuning is conducted using grid search for most methods. Tuning for C&amp;S is done as in the original paper <ref type="bibr" target="#b31">[32]</ref>, which uses Optuna <ref type="bibr" target="#b1">[2]</ref> for Bayesian hyperparameter optimization. All graphs are treated as undirected besides arXiv-year and snap-patents, in which the directed nature of the edges capture useful temporal information; however, we find that label propagation and C&amp;S (which builds on label propagation) perform better with undirected graphs in these cases, so we keep the graphs as undirected for these methods.</p><p>We implement our methods and run experiments in PyTorch <ref type="bibr" target="#b55">[56]</ref> (3-clause BSD license), and make heavy use of the PyTorch-Geometric library <ref type="bibr" target="#b22">[23]</ref> (MIT license) for graph representation learning.</p><p>For full-batch training, simple methods are run on a NVIDIA 2080 Ti with 11 GB GPU memory.</p><p>In cases where the NVIDIA 2080Ti did not provide enough memory, we re-ran experiments on a NVIDIA Titan RTX with 24 GB GPU memory, reporting (M) in Table <ref type="table" target="#tab_3">3</ref> if the GPU memory was still insufficient. For minibatch training on wiki, we also make use of NVIDIA RTX 3090 GPUs with 24 GB GPU memory. Each experiment was run on one GPU at a time.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.1 Full-batch Hyperparameters</head><p>Experimental results for full-batch training are reported on the hyperparameter settings below, where we choose the settings that achieve the highest performance on the validation set. We choose hyperparameter grids that do not necessarily give optimal performance, but hopefully cover enough regimes so that each model is reasonably evaluated on each dataset. Unless otherwise stated, each GNN has dropout of .5 <ref type="bibr" target="#b64">[65]</ref> and BatchNorm <ref type="bibr" target="#b32">[33]</ref> in each layer. The hyperparameter grids for the different methods are:</p><p>• MLP: hidden dimension ∈ {16, 32, 64, 128, 256}, number of layers ∈ {2, 3}. We use ReLU activations.</p><p>• Label propagation: α ∈ {.01, .1, .25, .5, .75, .9, .99}. We use 50 propagation iterations.</p><p>• LINK: weight decay ∈ {.001, .01, .1}.</p><p>• SGC: weight decay ∈ {.001, .01, .1}.</p><p>• C&amp;S: Normalized adjacency matrix A 1 , A 2 ∈ {D − 1 2 AD − 1 2 , D −1 A, AD −1 } for the residual propagation and label propagation, where A is the adjacency matrix of the graph and D is the diagonal degree matrix; α 1 , α 2 ∈ (0.0, 1.0) for the two propagations. Both Autoscale and FDiff-scale were used for all experiments, and scale ∈ (0.1, 10.0) was searched in FDiff-scale settings. The base predictor is chosen as the best MLP model for each dataset.</p><p>• GCN: lr ∈ {.1, .01, .001}, hidden dimension ∈ {4, 8, 16, 32, 64}, except for snap-patents and pokec, where we omit hidden dimension = 64. Each activation is a ReLU. 2 layers were used for all experiments.</p><p>• GAT: lr ∈ {.1, .01, .001}. For snap-patents and pokec: hidden channels ∈ {4, 8, 12} and gat heads ∈ {2, 4}. For all other datasets: hidden channels ∈ {4, 8, 12, 32} and gat heads ∈ {2, 4, 8}. We use the ELU activation <ref type="bibr" target="#b18">[19]</ref>. 2 layers were used for all experiments.</p><p>• GCNJK: Identical for GCN, also including JK Type ∈ {cat, max}.</p><p>• GATJK: Identical for GAT, also including JK Type ∈ {cat, max}.</p><p>• APPNP: MLP hidden dimension ∈ {16, 32, 64, 128, 256}, learning rate ∈ {.01, .05, .002}, α ∈ {.1, .2, .5, .9}. We truncate the series at the K = 10th power of the adjacency.</p><p>• H 2 GCN: hidden dimension ∈ {8, 16, 32, 64}, number of layers ∈ {1, 2}, dropout ∈ {0, .5}. The architecture follows Section 3.2 of <ref type="bibr" target="#b81">[82]</ref>.</p><p>• MixHop: hidden dimension ∈ {8, 16, 32}, number of layers ∈ {2, 3}. Each layer has uses the 0th, 1st, and 2nd powers of the adjacency and has ReLU activations. The last layer is a linear projection layer, instead of the attention output mechanism in <ref type="bibr" target="#b0">[1]</ref>.</p><p>• GPR-GNN: The basic setup and grid is the same as that of APPNP. We use their Personalized PageRank weight initialization.</p><p>• GCNII: number of layers ∈ {2, 8, 16, 32, 64}, strength of the initial residual connection α ∈ {0.1, 0.2, 0.5}, hyperparameter used to compute the strength of the identity mapping λ ∈ {0.5, 1.0, 1.5}.</p><p>• LINKX: We use take MLP A and MLP X to be one layer networks, i.e. linear mappings of size d × n and d × D, respectively. The hidden dimension is taken to be d ∈ {16, 32, 128, 256}, and the number of layers of MLP f is in {1, 2, 3}.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.2 Minibatching hyperparameters</head><p>The setup for minibatching is similar to the setup of full-batch training as above, with some differences that we note here. For all GNNs, we fix the hidden dimension to 128, which is a common hidden dimension used in Cluster-GCN <ref type="bibr" target="#b15">[16]</ref> and GraphSAINT <ref type="bibr" target="#b76">[77]</ref>. We use concatenation jumping knowledge connections <ref type="bibr" target="#b71">[72]</ref> for GCNJK. For GCNJK and MixHop, our hyperparameter grid only chooses a number of layers L ∈ {2, 4}, along with the hyperparameters for the minibatching methods that we give below.</p><p>• MLP, LINK, and LINKX use the same hyperparameter grids as in the full-batch setting, and they are trained with standard minibatching with a batch size of n/10. We train with one such batch in each of 500 epochs.</p><p>• All Cluster-GCN based experiments partition the graph into 200 parts and process a number of parts in {1, 5} at once. We train over all partitions in each of 500 epochs.</p><p>• All GraphSAINT-Node based experiments have a budget of nodes in {5,000, 10,000}. We train over five subgraphs for each of 500 epochs.</p><p>• All GraphSAINT-RandWalk based experiments use a random walk length of the same size as the number of layers L of the GNN, and use a number of roots in {5,000, 10,000}. We train over five subgraphs for each of 500 epochs.</p><p>These hyperparameter settings are in the same range as those used in the original papers <ref type="bibr" target="#b15">[16,</ref><ref type="bibr" target="#b76">77]</ref> for datasets that have similar node counts to our proposed datasets. For a total number of nodes n in the full graph, note that the MLP, LINK, and LINKX minibatching method processes about 50n total nodes across all batches in training. The Cluster-GCN method processes 500n nodes. When the number of nodes is 10,000, the GraphSAINT-Node method approximately processes between 601n nodes (Penn94) and 8.5n nodes (snap-patents). The GraphSAINT-RandWalk method with 10,000 root nodes processes a few times more nodes (usually between two to five times more) than the GraphSAINT-Node method, as more nodes are sampled from the random walks (and we take the number of layers L to be 2 or 4).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C Further Experiments</head><p>C.1 Benefits of Separate Embeddings Here, we give more evidence to justify separately embedding A and X in LINKX. Recall from Section 4.2 that there are computational benefits to separately embedding them. Table <ref type="table" target="#tab_9">7</ref> give results for concatenating A and X so that they are jointly embedded as MLP([A; X]). For this concatenation model, we search over the same hyperparameter grid as that of MLP f in LINKX. We see that LINKX generally outperforms the concatenation model (besides on the genius dataset). Moreover, LINKX always outperforms both MLP and LINK, while the concatenation model does not do as well as LINK on snap-patents and is within a standard deviation on twitch-gamers.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C.2 Minibatching experiments</head><p>Table <ref type="table">8</ref>: Comparison of different methods on the arXiv graphs with (homophilous) ogbn-arXiv subject labels and (non-homophilous) arXiv-year publication year labels. Percent relative error of minibatch methods against corresponding full batch methods are in parentheses.</p><p>Homophilous vs Non-Homophilous. To see differences in graph minibatching between homophilous and non-homophilous settings, we setup experiments using the same graph with homophilous labels and then non-homophilous labels; we take the ogbn-arXiv graph and train GNNs with minibatching with on the original ogbn-arXiv subject area labels (that are homophilous, see Table <ref type="table" target="#tab_6">5</ref>), and also train GNNs with minibatching on the arXiv-year labels that we defined in this work (that are non-homophilous, see Table <ref type="table" target="#tab_2">2</ref>). For the minibatching methods, we use similar hyperparameters as in Appendix C.2 (Cluster-GCN uses 200 partitions and processes 5 parts at once, GraphSAINT-Node has a 10,000 node budget and GraphSAINT-RandWalk has 10,000 roots), which we call the "coarse" setting. In the so-called "fine" setting, we use smaller subgraph minibatches (Cluster-GCN uses 750 partitions and processes 5 parts at once, GraphSAINT-Node has a 2,500 node budget and GraphSAINT-RandWalk has 2,500 roots). Both the GCNJK and MixHop architectures are fixed to have two layers and 128 hidden dimensions.</p><p>Table <ref type="table">8</ref> shows the results of these arXiv experiments on two different label sets. We see that performance degradation as measured by percent relative error in test accuracy is more substantial in the non-homophilous arXiv-year experiments for the GraphSAINT methods, while it is mostly comparable for the Cluster-GCN methods.</p><p>Graph minibatching could perform poorly in non-homophilous settings for a variety of reasons. It has been shown both theoretically and empirically that higher-order information from more than one-hop neighbors are important for classification in certain non-homophilous settings <ref type="bibr" target="#b81">[82,</ref><ref type="bibr" target="#b2">3]</ref>. One reason why graph minibatching may have issues here is that it is difficult to preserve higher-order neighborhood structure when minibatching on graphs. For instance, suppose we minibatch a graph with n nodes by taking an induced subgraph on n/2 randomly selected nodes. Then for a node u in the subgraph, each one hop path u → v has probability 1/2 of being in the subgraph, whereas each two-hop path u → w → v has probability 1/4 of being in the subgraph, since both w and v must be sampled.</p><p>Finally, even if the percent relative error of minibatching methods is similar in homophilous vs. non-homophilous graphs, the performance degradation would generally be more detrimental in non-homophilous graphs. This is because the gap between GNNs and methods that do not use the graph topology like MLPs are lower in many non-homophilous settings. Since MLPs do not face much performance degradation when trained with simple i.i.d. node minibatching, the gap between GNNs and MLPs is even lower in minibatched settings. Indeed, we see that MLPs perform on par with or outperform many GNNs in the minibatched setting in Table <ref type="table" target="#tab_4">4</ref>.</p><p>Table <ref type="table">9</ref>: Minibatching results when using GCN as a base model. The setup is the same as in Section 5.3. LINKX results are provided as reference.</p><p>LINKX performs well on datasets with thousands of nodes, despite being primarily a scalable method, only falling short on the tiniest of datasets. Overall, evaluation on these datasets is very noisy, highly dependent on hyperparameter grid selection, and few conclusions can be drawn on the relative performance of different methods in non-homophily more broadly. In particular, note that methods that do well on these datasets do not necessarily do well on our large, non-homophilous datasets. For example, while MixHop is the best non-homophilous GNN on the datasets we present in this paper, it does poorly on the datasets of Pei et al. <ref type="bibr" target="#b57">[58]</ref>; in contrast, H 2 GCN achieves excellent performance here, but its design choices make it run out of memory on even medium-sized datasets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C.4 Experiments on Homophilous datasets</head><p>Though LINKX was not designed to do well on small homophilous datasets, we also report LINKX on the homophilous datasets of Cora, Citeseer and Pubmed <ref type="bibr" target="#b73">[74]</ref>, with the standard 48/32/20 training, validation, and test proportions. We use the same hyper-parameter grid for LINKX as in Section C.3. In this section, we note the licenses of the datasets we collect:</p><p>• wiki: Wikipedia is licensed under Creative Commons Attribution-ShareAlike 3.0 Unported License and the GNU Free Documentation License, unversioned, with no invariant sections, front-cover texts, or back-cover texts. • Penn94: The Facebook 100 datasets are available online (https://archive.org/details/oxford-2005-facebook-matrix), and to the best of our knowledge were not released with a license, though the corresponding paper has an arXiv non-exclusive license to distribute. Upon release, there were privacy concerns <ref type="bibr" target="#b82">[83]</ref>, as the data release may not have respected certain privacy settings, and the initial release of the data included unique identifiers. We use a version that does not include the unique identifiers, but of course may still be subject to deanonymization attacks. While we do not add any sensitive information to the dataset, we acknowledge that deanonymization is possible, though our work does not directly contribute to deanonymization risks. • Pokec: We retrieved the data from SNAP; the original source of the data is <ref type="bibr" target="#b39">[40]</ref>. To the best of our knowledge, the data was not released with a license. This is a social network, so there may be privacy concerns with the user data. Still, we include it as a suitable benchmark that has been previously used, as the dataset is large and has interesting feature information. We only provide numerical values and do not provide any of the raw text in the dataset. • arxiv-year: The dataset is licensed under ODC-BY. We originally downloaded the data from the Open Graph Benchmark <ref type="bibr" target="#b30">[31]</ref>, and the data is a subset of the Microsoft Academic Graph <ref type="bibr" target="#b69">[70]</ref> • snap-patents: The data was originally publically released by NBER in 2001 in a working paper by <ref type="bibr" target="#b26">[27]</ref>. To the best of our knowledge, the dataset was not released with a license. • genius: The dataset is open-sourced by the authors with no attached license. It was originally introduced in a conference paper <ref type="bibr" target="#b42">[43]</ref>. While this is a social network and thus may face privacy concerns, we believe that the task of predicting undesired nodes can be very beneficial to society, so we benchmark methods on it. We only provide numerical values, and omit raw text information that has been previously released in the dataset.</p><p>• twitch-gamer:</p><p>The dataset is open-sourced by the authors in a repository with the MIT license. It was originally introduced in an academic paper <ref type="bibr" target="#b45">[46]</ref>. This is also a social network that may face privacy concerns, but the prediction task of detecting situationally undesired nodes may be socially beneficial, so we benchmark methods on it. There is no raw text in the dataset.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D.2 Dataset Properties</head><p>Penn94 <ref type="bibr" target="#b66">[67]</ref> is a friendship network from the Facebook 100 networks of university students from 2005, where nodes represent students. Each node is labeled with the reported gender of the user. The node features are major, second major/minor, dorm/house, year, and high school.</p><p>Pokec <ref type="bibr" target="#b40">[41]</ref> is the friendship graph of a Slovak online social network, where nodes are users and edges are directed friendship relations. Nodes are labeled with reported gender. We derive node features from profile information, such as geographical region, registration time, and age.</p><p>arXiv-year <ref type="bibr" target="#b30">[31]</ref> is the ogbn-arXiv network with different labels. Our contribution is to set the class labels to be the year that the paper is posted, instead of paper subject area. The nodes are arXiv papers, and directed edges connect a paper to other papers that it cites. The node features are averaged word2vec token features of both the title and abstract of the paper. The five classes are chosen by partitioning the posting dates so that class ratios are approximately balanced.</p><p>snap-patents <ref type="bibr" target="#b41">[42,</ref><ref type="bibr" target="#b40">41]</ref> is a dataset of utility patents in the US. Each node is a patent, and edges connect patents that cite each other. Node features are derived from patent metadata. Our contribution is to set the task to predict the time at which a patent was granted, resulting in five classes.</p><p>genius <ref type="bibr" target="#b42">[43]</ref> is a subset of the social network on genius.com -a site for crowdsourced annotations of song lyrics. Nodes are users, and edges connect users that follow each other on the site. This social network has not been used for node classification in the literature, so we define the task of predicting certain marks on the accounts. About 20% of users in the dataset are marked "gone" on the site, which appears to often include spam users. Thus, we predict whether nodes are marked. The node features are user usage attributes like the Genius assigned expertise score, counts of contributions, and roles held by the user.</p><p>twitch-gamers <ref type="bibr" target="#b59">[60]</ref> is a connected undirected graph of relationships between accounts on the streaming platform Twitch. Each node is a Twitch account, and edges exist between accounts that are mutual followers. The node features include number of views, creation and update dates, language, life time, and whether the account is dead. The binary classification task is to predict whether the channel has explicit content.</p><p>wiki is a dataset of Wikipedia articles, where nodes represent pages and edges represent links between them. We collect this new dataset, with a process that we describe further in Appendix D.3. Node features are constructed using averaged title and abstract GloVe embeddings <ref type="bibr" target="#b58">[59]</ref>. Labels represent total page views over 60 days, which are partitioned into quintiles to make five classes.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D.3 Wiki collection details</head><p>Here, we detail the process of crawling and cleaning the wiki dataset. We generated the graph using a breadth-first search, where we started from the Wikipedia page on Hilbert Spaces, then proceeded to visit all its neighbors, and so forth. For each Wikipedia page visited, we used the MediaWiki web service API to get a list of pages linked to by the given page -this forms the directed edges of the graph. In each API query, we also received the number of page views per day in the past 60 days. We crawled these articles throughout April and May of 2021. To convert this into discrete labels, we used an even quantile function, which set view boundaries to make the number of nodes in each class as even as possible. The output of this function formed the labels of the graph.</p><p>For the node features, we formed 300 dimensional Wikipedia Glove vectors <ref type="bibr" target="#b58">[59]</ref> for each word in the title and abstract, then averaged the word vectors in the title and abstract, thus resulting in 600 dimensional feature vectors for each node. For words not found in the Glove dictionary, we used the zero vector. This procedure was modeled on the construction of the ogbn-arxiv dataset by <ref type="bibr" target="#b30">[31]</ref>. In particular, we avoided a one-hot vector because of the vast dimensionality that would be required. Finally, to clean the dataset, we pruned edges if either of the nodes that it spanned where not in our subset of wikipedia articles. Ultimately, we decided to stop collection at approximately one third of English wikipedia due to limitations of computational resources and time. Already, it is not possible to run full batch experiments on the wiki dataset, while requiring over 80GB of CPU RAM for some minibatching techniques. As such, we believe that the full wikipedia dataset would have been too large and unwieldy to use as an evaluation dataset for many research labs, and our dataset is at a good size that may hopefully provide utility to many researchers.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: Our model LINKX separately embeds node features and adjacency information with MLPs, combines the embeddings together by concatenation, then uses a final MLP to generate predictions.</figDesc><graphic url="image-1.png" coords="2,118.28,541.66,149.64,121.75" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>( c )</head><label>c</label><figDesc>Did you discuss any potential negative societal impacts of your work? [Yes] See discussion / conclusion Section 6. (d) Have you read the ethics review guidelines and ensured that your paper conforms to them? [Yes] 2. If you are including theoretical results... (a) Did you state the full set of assumptions of all theoretical results? [N/A] (b) Did you include complete proofs of all theoretical results? [N/A] 3. If you ran experiments... (a) Did you include the code, data, and instructions needed to reproduce the main experimental results (either in the supplemental material or as a URL)? [Yes] We include our proposed datasets and our codes for reproducing the experimental results in the supplemental material. (b) Did you specify all the training details (e.g., data splits, hyperparameters, how they were chosen)? [Yes] See Section 5.1 and Appendix Section B. (c) Did you report error bars (e.g., with respect to the random seed after running experiments multiple times)? [Yes] We include standard deviation of performance metrics across multiple runs in Section 5. (d) Did you include the total amount of compute and the type of resources used (e.g., type of GPUs, internal cluster, or cloud provider)? [Yes] We provide GPU information in Appendix Section B. 4. If you are using existing assets (e.g., code, data, models) or curating/releasing new assets... (a) If your work uses existing assets, did you cite the creators? [Yes] We cite the creators of existing datasets and methods in the main paper. Also, in Appendix B, we cite codes that we used. (b) Did you mention the license of the assets? [Yes] In Appendix D, we note the licenses of datasets, if provided in published work. Also, we include the license of open-source codes we build off of in Appendix B. (c) Did you include any new assets either in the supplemental material or as a URL? [Yes]</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>5 .</head><label>5</label><figDesc>If you used crowdsourcing or conducted research with human subjects... (a) Did you include the full text of instructions given to participants and screenshots, if applicable? [N/A] (b) Did you describe any potential participant risks, with links to Institutional Review Board (IRB) approvals, if applicable? [N/A] (c) Did you include the estimated hourly wage paid to participants and the total amount spent on participant compensation? [N/A]</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: Examples of graphs with different label-topology relationships and comparison of our measure ĥ with the edge homophily ratio h. The node classes are labeled by color. Pink edges link nodes of the same class, while purple edges link nodes of different classes. (a,b) Pure homophily and pure heterophily. Both measures equal 1 in homophily and 0 in heterophily. (c,d) Graphs where each node is connected to one member of each class. Edge homophily depends on the number of classes, while our measure ĥ does not. (e,f) Random Erdős-Rényi graphs in which edges are independent of labels. Edge homophily is sensitive to class imbalance, while our measure ĥ is not.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: Compatibility matrices of our proposed datasets. These datasets from a variety of different contexts exhibit a wide range of non-homophilous structures.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure 5 :</head><label>5</label><figDesc>Figure 5: Compatibility matrices of datasets in Pei et al.<ref type="bibr" target="#b57">[58]</ref> (collected by<ref type="bibr" target="#b60">[61,</ref><ref type="bibr" target="#b65">66,</ref><ref type="bibr" target="#b47">48]</ref>). The "film" dataset is also referred to as "Actor". Note that there are no edges leading out of the nodes of class 1 in the Cornell dataset, so there is an empty row in its matrix.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>v∈V</head><label></label><figDesc>Number of exact two-hop neighbors of v with same class as vNumber of v's exact two-hop neighbors .</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 1 :</head><label>1</label><figDesc>Statistics for previously used datasets from Pei et al.<ref type="bibr" target="#b57">[58]</ref> (collected by<ref type="bibr" target="#b60">[61,</ref><ref type="bibr" target="#b65">66,</ref><ref type="bibr" target="#b47">48]</ref>). #C is the number of node classes. The highest number of nodes or edges overall are bolded.</figDesc><table><row><cell>Dataset</cell><cell cols="4"># Nodes # Edges # Feat. # C</cell><cell cols="3">Context Edge hom. ĥ (ours)</cell></row><row><cell>Chameleon</cell><cell>2,277</cell><cell>36,101</cell><cell>2,325</cell><cell>5</cell><cell>Wiki pages</cell><cell>.23</cell><cell>.062</cell></row><row><cell>Cornell</cell><cell>183</cell><cell>295</cell><cell>1,703</cell><cell>5</cell><cell>Web pages</cell><cell>.30</cell><cell>.047</cell></row><row><cell>Actor</cell><cell>7,600</cell><cell>29,926</cell><cell>931</cell><cell cols="2">5 Actors in movies</cell><cell>.22</cell><cell>.011</cell></row><row><cell>Squirrel</cell><cell cols="2">5,201 216,933</cell><cell>2,089</cell><cell>5</cell><cell>Wiki pages</cell><cell>.22</cell><cell>.025</cell></row><row><cell>Texas</cell><cell>183</cell><cell>309</cell><cell>1,703</cell><cell>5</cell><cell>Web pages</cell><cell>.11</cell><cell>.001</cell></row><row><cell>Wisconsin</cell><cell>251</cell><cell>499</cell><cell>1,703</cell><cell>5</cell><cell>Web pages</cell><cell>.21</cell><cell>.094</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2 :</head><label>2</label><figDesc>Statistics of our proposed non-homophilous graph datasets. # C is the number of distinct node classes. Note that our datasets come from more diverse applications areas and are much larger than those shown in Table1, with up to 384x more nodes and 1398x more edges.</figDesc><table><row><cell>Dataset</cell><cell># Nodes</cell><cell cols="3"># Edges # Feat. # C</cell><cell cols="3">Class types Edge hom. ĥ (ours)</cell></row><row><cell>Penn94</cell><cell>41,554</cell><cell>1,362,229</cell><cell>5</cell><cell>2</cell><cell>gender</cell><cell>.470</cell><cell>.046</cell></row><row><cell>pokec</cell><cell>1,632,803</cell><cell>30,622,564</cell><cell>65</cell><cell>2</cell><cell>gender</cell><cell>.445</cell><cell>.000</cell></row><row><cell>arXiv-year</cell><cell>169,343</cell><cell>1,166,243</cell><cell>128</cell><cell>5</cell><cell>pub year</cell><cell>.222</cell><cell>.272</cell></row><row><cell>snap-patents</cell><cell>2,923,922</cell><cell>13,975,788</cell><cell>269</cell><cell>5</cell><cell>time granted</cell><cell>.073</cell><cell>.100</cell></row><row><cell>genius</cell><cell>421,961</cell><cell>984,979</cell><cell>12</cell><cell>2</cell><cell>marked act.</cell><cell>.618</cell><cell>.080</cell></row><row><cell>twitch-gamers</cell><cell>168,114</cell><cell>6,797,557</cell><cell>7</cell><cell cols="2">2 mature content</cell><cell>.545</cell><cell>.090</cell></row><row><cell>wiki</cell><cell cols="2">1,925,342 303,434,860</cell><cell>600</cell><cell>5</cell><cell>views</cell><cell>.389</cell><cell>.107</cell></row><row><cell cols="3">4 LINKX: A New Scalable Model</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="8">In this section, we introduce our novel model, LINKX, for scalable node classification in non-</cell></row><row><cell cols="8">homophilous settings. LINKX is built out of multilayer perceptrons (MLPs) and linear transforma-</cell></row><row><cell cols="8">tions, thus making it simple and scalable. It also admits simple row-wise minibatching procedures</cell></row><row><cell cols="8">that allow it to perform well on large non-homophilous graphs. As a result, LINKX is able to</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 3 :</head><label>3</label><figDesc>Experimental results. Test accuracy is displayed for most datasets, while genius displays test ROC AUC. Standard deviations are over 5 train/val/test splits. The three best results per dataset are highlighted. (M) denotes some (or all) hyperparameter settings run out of memory.</figDesc><table><row><cell></cell><cell>Penn94</cell><cell>pokec</cell><cell>arXiv-year</cell><cell>snap-patents</cell><cell>genius</cell><cell>twitch-gamers</cell></row><row><cell>MLP</cell><cell>73.61 ± 0.40</cell><cell>62.37 ± 0.02</cell><cell>36.70 ± 0.21</cell><cell>31.34 ± 0.05</cell><cell>86.68 ± 0.09</cell><cell>60.92 ± 0.07</cell></row><row><cell>L Prop 1-hop</cell><cell>63.21 ± 0.39</cell><cell>53.09 ± 0.05</cell><cell>43.42 ± 0.17</cell><cell>30.28 ± 0.09</cell><cell>66.02 ± 0.16</cell><cell>62.77 ± 0.24</cell></row><row><cell>L Prop 2-hop</cell><cell>74.13 ± 0.46</cell><cell>76.76 ± 0.03</cell><cell>46.07 ± 0.15</cell><cell>38.61 ± 0.07</cell><cell>67.04 ± 0.20</cell><cell>63.88 ± 0.24</cell></row><row><cell>LINK</cell><cell>80.79 ± 0.49</cell><cell>80.54 ± 0.03</cell><cell>53.97 ± 0.18</cell><cell>60.39 ± 0.07</cell><cell>73.56 ± 0.14</cell><cell>64.85 ± 0.21</cell></row><row><cell>SGC 1-hop</cell><cell>66.79 ± 0.27</cell><cell>53.61 ± 0.17</cell><cell>32.83 ± 0.13</cell><cell>30.31 ± 0.06</cell><cell>82.36 ± 0.37</cell><cell>58.97 ± 0.19</cell></row><row><cell>SGC 2-hop</cell><cell>76.09 ± 0.45</cell><cell>62.81 ± 1.42</cell><cell>32.27 ± 0.06</cell><cell>29.09 ± 0.09</cell><cell>82.10 ± 0.14</cell><cell>59.94 ± 0.21</cell></row><row><cell>C&amp;S 1-hop</cell><cell>74.28 ± 1.19</cell><cell>62.35 ± 0.06</cell><cell>44.51 ± 0.16</cell><cell>35.55 ± 0.05</cell><cell>82.93 ± 0.15</cell><cell>64.86 ± 0.27</cell></row><row><cell>C&amp;S 2-hop</cell><cell>78.40 ± 3.12</cell><cell>81.69 ± 0.09</cell><cell>49.78 ± 0.26</cell><cell>49.08 ± 0.04</cell><cell>84.94 ± 0.49</cell><cell>65.02 ± 0.16</cell></row><row><cell>GCN</cell><cell>82.47 ± 0.27</cell><cell>75.45 ± 0.17</cell><cell>46.02 ± 0.26</cell><cell>45.65 ± 0.04</cell><cell>87.42 ± 0.37</cell><cell>62.18 ± 0.26</cell></row><row><cell>GAT</cell><cell>81.53 ± 0.55</cell><cell>71.77 ± 6.18 (M)</cell><cell>46.05 ± 0.51</cell><cell>45.37 ± 0.44 (M)</cell><cell>55.80 ± 0.87</cell><cell>59.89 ± 4.12</cell></row><row><cell>GCNJK</cell><cell>81.63 ± 0.54</cell><cell>77.00 ± 0.14</cell><cell>46.28 ± 0.29</cell><cell>46.88 ± 0.13</cell><cell>89.30 ± 0.19</cell><cell>63.45 ± 0.22</cell></row><row><cell>GATJK</cell><cell>80.69 ± 0.36</cell><cell>71.19 ± 6.96 (M)</cell><cell>45.80 ± 0.72</cell><cell>44.78 ± 0.50</cell><cell>56.70 ± 2.07</cell><cell>59.98 ± 2.87</cell></row><row><cell>APPNP</cell><cell>74.33 ± 0.38</cell><cell>62.58 ± 0.08</cell><cell>38.15 ± 0.26</cell><cell>32.19 ± 0.07</cell><cell>85.36 ± 0.62</cell><cell>60.97 ± 0.10</cell></row><row><cell>H 2 GCN</cell><cell>(M)</cell><cell>(M)</cell><cell>49.09 ± 0.10</cell><cell>(M)</cell><cell>(M)</cell><cell>(M)</cell></row><row><cell>MixHop</cell><cell>83.47 ± 0.71</cell><cell>81.07 ± 0.16</cell><cell>51.81 ± 0.17</cell><cell>52.16 ± 0.09 (M)</cell><cell>90.58 ± 0.16</cell><cell>65.64 ± 0.27</cell></row><row><cell>GPR-GNN</cell><cell>81.38 ± 0.16</cell><cell>78.83 ± 0.05</cell><cell>45.07 ± 0.21</cell><cell>40.19 ± 0.03</cell><cell>90.05 ± 0.31</cell><cell>61.89 ± 0.29</cell></row><row><cell>GCNII</cell><cell>82.92 ± 0.59</cell><cell>78.94 ± 0.11 (M)</cell><cell>47.21 ± 0.28</cell><cell>37.88 ± 0.69 (M)</cell><cell>90.24 ± 0.09</cell><cell>63.39 ± 0.61</cell></row><row><cell>LINKX</cell><cell>84.71 ± 0.52</cell><cell>82.04 ± 0.07</cell><cell>56.00 ± 1.34</cell><cell>61.95 ± 0.12</cell><cell>90.77 ± 0.27</cell><cell>66.06 ± 0.19</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 4 :</head><label>4</label><figDesc>Minibatching results on our proposed datasets. † denotes that 10 random partitions of the graphs are used for testing GraphSAINT sampling. (T) denotes that five runs takes ≥ 48 hours for a single hyperparameter setting. Best results up to a standard deviation are highlighted.</figDesc><table><row><cell></cell><cell>Penn94</cell><cell>pokec  †</cell><cell>arXiv-year</cell><cell>snap-patents  †</cell><cell>genius</cell><cell>twitch-gamers  †</cell><cell>wiki  †</cell></row><row><cell>MLP Minibatch</cell><cell>74.24±0.55</cell><cell>62.14±0.05</cell><cell>36.89±0.11</cell><cell>22.96±0.81</cell><cell>82.35±0.38</cell><cell>61.01±0.06</cell><cell>37.38±0.21</cell></row><row><cell>LINK Minibatch</cell><cell>81.61±0.34</cell><cell>81.15±0.25</cell><cell>53.76±0.28</cell><cell>45.65±8.25</cell><cell>80.95±0.07</cell><cell>64.38±0.26</cell><cell>57.11±0.26</cell></row><row><cell>GCNJK-Cluster</cell><cell>69.99±0.85</cell><cell>72.67±0.05</cell><cell>44.05±0.11</cell><cell>37.62±0.31</cell><cell>83.04±0.56</cell><cell>61.15±0.16</cell><cell>(T)</cell></row><row><cell>GCNJK-SAINT-Node</cell><cell>72.80±0.43</cell><cell>63.68±0.06</cell><cell>44.30±0.22</cell><cell>26.97±0.10</cell><cell>80.96±0.09</cell><cell>59.50±0.35</cell><cell>44.86±0.19</cell></row><row><cell>GCNJK-SAINT-RW</cell><cell>72.29±0.49</cell><cell>65.00±0.11</cell><cell>47.40±0.17</cell><cell>33.05±0.06</cell><cell>81.04±0.14</cell><cell>59.82±0.27</cell><cell>47.39±0.19</cell></row><row><cell>MixHop-Cluster</cell><cell>75.79±0.44</cell><cell>76.67±0.07</cell><cell>48.41±0.31</cell><cell>46.82±0.11</cell><cell>81.12±0.10</cell><cell>62.95±0.08</cell><cell>(T)</cell></row><row><cell>MixHop-SAINT-Node</cell><cell>75.61±0.55</cell><cell>66.42±0.06</cell><cell>44.84±0.18</cell><cell>27.45±0.11</cell><cell>81.06±0.08</cell><cell>59.58±0.27</cell><cell>47.39±0.18</cell></row><row><cell>MixHop-SAINT-RW</cell><cell>76.38±0.50</cell><cell>67.92±0.06</cell><cell>50.55±0.20</cell><cell>34.21±0.07</cell><cell>82.25±0.78</cell><cell>60.39±0.16</cell><cell>49.15±0.26</cell></row><row><cell>LINKX Minibatch</cell><cell>84.50±0.65</cell><cell>81.27±0.38</cell><cell>53.74±0.27</cell><cell>60.27±0.29</cell><cell>85.81±0.10</cell><cell>65.84±0.19</cell><cell>59.80±0.41</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 5 :</head><label>5</label><figDesc>Statistics for homophilic graph datasets. # C is the number of node classes.</figDesc><table><row><cell>Dataset</cell><cell># Nodes</cell><cell cols="4"># Edges # C Edge hom. ĥ (ours)</cell></row><row><cell>Cora</cell><cell>2,708</cell><cell>5,278</cell><cell>7</cell><cell>.81</cell><cell>.766</cell></row><row><cell>Citeseer</cell><cell>3,327</cell><cell>4,552</cell><cell>6</cell><cell>.74</cell><cell>.627</cell></row><row><cell>Pubmed</cell><cell>19,717</cell><cell>44,324</cell><cell>3</cell><cell>.80</cell><cell>.664</cell></row><row><cell>ogbn-arXiv</cell><cell>169,343</cell><cell>1,166,243</cell><cell>40</cell><cell>.66</cell><cell>.416</cell></row><row><cell cols="3">ogbn-products 2,449,029 61,859,140</cell><cell>47</cell><cell>.81</cell><cell>.459</cell></row><row><cell>oeis</cell><cell>226,282</cell><cell>761,687</cell><cell>5</cell><cell>.50</cell><cell>.532</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head></head><label></label><figDesc>Figure 4: Compatibility matrices of homophilic datasets. The diagonal dominance indicates strong homophily.</figDesc><table><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>PubMed</cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>659</cell><cell></cell><cell>0.106</cell><cell></cell><cell>0.235</cell><cell>0.7 0.8</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>0.6</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>0.049</cell><cell></cell><cell>0.853</cell><cell></cell><cell>0.099</cell><cell>0.4 0.5</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>0.3</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>0.096</cell><cell></cell><cell>0.087</cell><cell></cell><cell>0.817</cell><cell>0.1 0.2</cell></row><row><cell>0 2 4 6 8 10 12 14 16 18 20 22 24 26 28 30 32 34 36 38</cell><cell></cell><cell></cell><cell cols="2">ogbn-arxiv</cell><cell></cell><cell>0.0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8</cell><cell>0 39 42 45 30 33 36 21 24 27 3 6 9 12 18 15</cell><cell></cell><cell cols="3">ogbn-products</cell><cell></cell><cell>0.0 0.2 0.4 0.6 0.8</cell><cell>4 3 2 0 1</cell><cell>0.018 0.018 0.018 0.209 0.031</cell><cell>0.02 0.025 0.032 0.251 0.539</cell><cell>oeis 0.012 0.023 0.704 0.153 0.134</cell><cell>0.016 0.742 0.136 0.176 0.138</cell><cell>0.934 0.192 0.11 0.211 0.158</cell><cell>0.2 0.4 0.6 0.8</cell></row><row><cell></cell><cell cols="5">0 2 4 6 8 10 12 14 16 18 20 22 24 26 28 30 32 34 36 38</cell><cell></cell><cell cols="6">0 2 4 6 8 10 12 14 16 18 20 22 24 26 28 30 32 34 36 38 40 42 44 46</cell><cell></cell><cell></cell><cell>0</cell><cell>1</cell><cell>2</cell><cell>3</cell><cell>4</cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell>cornell</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>chameleon</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>film</cell><cell></cell><cell></cell><cell></cell></row><row><cell>0 1 2</cell><cell>0.169 0.214</cell><cell>0.008 0</cell><cell>0.073 0.286</cell><cell>0.629 0.357</cell><cell>0.121 0.143</cell><cell>0.3 0.4 0.5 0.6</cell><cell>0 1 2</cell><cell>0.272 0.477 0.121</cell><cell>0.165 0.146 0.146</cell><cell>0.181 0.13 0.256</cell><cell>0.115 0.096 0.228</cell><cell>0.267 0.151 0.248</cell><cell>0.25 0.30 0.35 0.40 0.45</cell><cell>2 1 0</cell><cell>0.113 0.104 0.111</cell><cell>0.163 0.167 0.163</cell><cell>0.192 0.196 0.192</cell><cell>0.276 0.278 0.276</cell><cell>0.255 0.255 0.257</cell><cell>0.18 0.20 0.22 0.24 0.26 0.28</cell></row><row><cell>3 4</cell><cell>0.238 0.229</cell><cell>0.041 0.057</cell><cell>0.107 0.143</cell><cell>0.484 0.457</cell><cell>0.131 0.114</cell><cell>0.0 0.1 0.2</cell><cell>4 3</cell><cell>0.071 0.115</cell><cell>0.107 0.111</cell><cell>0.272 0.229</cell><cell>0.265 0.232</cell><cell>0.285 0.312</cell><cell>0.10 0.15 0.20</cell><cell>4 3</cell><cell>0.115 0.108</cell><cell>0.16 0.174</cell><cell>0.201 0.195</cell><cell>0.265 0.283</cell><cell>0.258 0.24</cell><cell>0.12 0.14 0.16</cell></row><row><cell></cell><cell>0</cell><cell>1</cell><cell>2</cell><cell>3</cell><cell>4</cell><cell></cell><cell></cell><cell>0</cell><cell>1</cell><cell>2</cell><cell>3</cell><cell>4</cell><cell></cell><cell></cell><cell>0</cell><cell>1</cell><cell>2</cell><cell>3</cell><cell>4</cell><cell></cell></row><row><cell>0</cell><cell>0.204</cell><cell>0.196</cell><cell>0.169</cell><cell>0.198</cell><cell>0.233</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>1</cell><cell>0.171</cell><cell>0.187</cell><cell>0.194</cell><cell>0.213</cell><cell>0.235</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>2</cell><cell>0.134</cell><cell>0.178</cell><cell>0.219</cell><cell>0.233</cell><cell>0.237</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>3</cell><cell>0.119</cell><cell>0.171</cell><cell>0.224</cell><cell>0.243</cell><cell>0.243</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>4</cell><cell>0.139</cell><cell>0.18</cell><cell>0.208</cell><cell>0.237</cell><cell>0.235</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>0</cell><cell>1</cell><cell>2</cell><cell>3</cell><cell>4</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>Table 6 :</head><label>6</label><figDesc>Comparison of node homophily over one-hop neighborhoods and strict two-hop neighborhoods. The measure over two-hop neighborhoods is estimated with k = 500 sampled nodes.</figDesc><table><row><cell>Dataset</cell><cell cols="2">Two-hop One-hop</cell></row><row><cell>Penn94</cell><cell>.474</cell><cell>.483</cell></row><row><cell>pokec</cell><cell>.611</cell><cell>.428</cell></row><row><cell>arXiv-year</cell><cell>.341</cell><cell>.289</cell></row><row><cell>snap-patents</cell><cell>.330</cell><cell>.221</cell></row><row><cell>genius</cell><cell>.749</cell><cell>.508</cell></row><row><cell cols="2">twitch-gamers .513</cell><cell>.556</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head>Table 7 :</head><label>7</label><figDesc>Comparing separately embedding A and X (as in LINKX) with direct concatenation. LINKX outperforms the concatenation based model.</figDesc><table><row><cell></cell><cell>Penn94</cell><cell>pokec</cell><cell>arXiv-year</cell><cell>snap-patents</cell><cell>genius</cell><cell>twitch-gamers</cell></row><row><cell>MLP</cell><cell>73.61 ± 0.40</cell><cell>62.37 ± 0.02</cell><cell>36.70 ± 0.21</cell><cell>31.34 ± 0.05</cell><cell>86.68 ± 0.09</cell><cell>60.92 ± 0.07</cell></row><row><cell>LINK</cell><cell>80.79 ± 0.49</cell><cell>80.54 ± 0.03</cell><cell>53.97 ± 0.18</cell><cell>60.39 ± 0.07</cell><cell>73.56 ± 0.14</cell><cell>64.85 ± 0.21</cell></row><row><cell>MLP([A; X])</cell><cell>84.64 ± 0.33</cell><cell>81.74 ± 0.15</cell><cell>54.15 ± 0.20</cell><cell>59.12 ± 0.29</cell><cell>91.61 ± 0.05</cell><cell>64.89 ± 0.18</cell></row><row><cell>LINKX</cell><cell>84.71 ± 0.52</cell><cell>82.04 ± 0.07</cell><cell>56.00 ± 1.34</cell><cell>61.95 ± 0.12</cell><cell>90.77 ± 0.27</cell><cell>66.06 ± 0.19</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_10"><head>Table 11 :</head><label>11</label><figDesc>Comparison of on homophilous datasets. Results other than LINKX reported from [82]. Best three results per dataset are highlighted. 77.07 ± 1.64 89.40 ± 0.34 86.92 ± 1.37 H2GCN-2 76.88 ± 1.77 89.59 ± 0.33 87.81 ± 1.35 MixHop 76.26 ± 1.33 85.31 ± 0.61 87.61 ± 0.85 GCN 76.68 ± 1.64 87.38 ± 0.66 87.28 ± 1.26 GAT 75.46 ± 1.72 84.68 ± 0.44 82.68 ± 1.80</figDesc><table><row><cell></cell><cell>CiteSeer</cell><cell>PubMed</cell><cell>Cora</cell></row><row><cell># Nodes</cell><cell>3327</cell><cell>19717</cell><cell>2708</cell></row><row><cell>H2GCN-1 LINKX</cell><cell cols="3">73.19 ± 0.99 87.86 ± 0.77 84.64 ± 1.13</cell></row><row><cell cols="2">D Further dataset details</cell><cell></cell><cell></cell></row><row><cell>D.1 Licenses</cell><cell></cell><cell></cell><cell></cell></row></table></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgements</head><p>We thank Abhay Singh, Austin Benson, and Horace He for insightful discussions. We also thank the rest of Cornell University Artificial Intelligence for their support and discussion. We thank Facebook AI for funding equipment that made this work possible.</p></div>
			</div>

			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0" />			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Mixhop: Higher-order graph convolutional architectures via sparsified neighborhood mixing</title>
		<author>
			<persName><forename type="first">Sami</forename><surname>Abu-El-Haija</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bryan</forename><surname>Perozzi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Amol</forename><surname>Kapoor</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nazanin</forename><surname>Alipourfard</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kristina</forename><surname>Lerman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hrayr</forename><surname>Harutyunyan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Greg</forename><surname>Ver Steeg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aram</forename><surname>Galstyan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
				<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="21" to="29" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">Optuna: A next-generation hyperparameter optimization framework</title>
		<author>
			<persName><forename type="first">Takuya</forename><surname>Akiba</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shotaro</forename><surname>Sano</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Toshihiko</forename><surname>Yanase</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Takeru</forename><surname>Ohta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Masanori</forename><surname>Koyama</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Monophily in social networks introduces similarity among friends-of-friends</title>
		<author>
			<persName><forename type="first">M</forename><surname>Kristen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Johan</forename><surname>Altenburger</surname></persName>
		</author>
		<author>
			<persName><surname>Ugander</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nature human behaviour</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="284" to="290" />
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">Node attribute prediction: An evaluation of withinversus across-network tasks. NeurIPS Workshop on Relational Representation Learning</title>
		<author>
			<persName><forename type="first">M</forename><surname>Kristen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Johan</forename><surname>Altenburger</surname></persName>
		</author>
		<author>
			<persName><surname>Ugander</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Wherefore art thou r3579x?: anonymized social networks, hidden patterns, and structural steganography</title>
		<author>
			<persName><forename type="first">Lars</forename><surname>Backstrom</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Cynthia</forename><surname>Dwork</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jon</forename><forename type="middle">M</forename><surname>Kleinberg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">WWW &apos;07</title>
				<imprint>
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Analyzing the expressive power of graph neural networks in a spectral perspective</title>
		<author>
			<persName><forename type="first">Muhammet</forename><surname>Balcilar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Guillaume</forename><surname>Renton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pierre</forename><surname>Héroux</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Benoit</forename><surname>Gaüzère</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sébastien</forename><surname>Adam</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Paul</forename><surname>Honeine</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
				<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">Relational inductive biases, deep learning, and graph networks</title>
		<author>
			<persName><forename type="first">Jessica</forename><forename type="middle">B</forename><surname>Peter W Battaglia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Victor</forename><surname>Hamrick</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alvaro</forename><surname>Bapst</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vinicius</forename><surname>Sanchez-Gonzalez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mateusz</forename><surname>Zambaldi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrea</forename><surname>Malinowski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><surname>Tacchetti</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Adam</forename><surname>Raposo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ryan</forename><surname>Santoro</surname></persName>
		</author>
		<author>
			<persName><surname>Faulkner</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1806.01261</idno>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">The values encoded in machine learning research</title>
		<author>
			<persName><forename type="first">Abeba</forename><surname>Birhane</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pratyusha</forename><surname>Kalluri</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dallas</forename><surname>Card</surname></persName>
		</author>
		<author>
			<persName><forename type="first">William</forename><surname>Agnew</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ravit</forename><surname>Dotan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michelle</forename><surname>Bao</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">Beyond low-frequency information in graph convolutional networks</title>
		<author>
			<persName><forename type="first">Deyu</forename><surname>Bo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiao</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chuan</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Huawei</forename><surname>Shen</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2101.00797</idno>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Scaling graph neural networks with approximate pagerank</title>
		<author>
			<persName><forename type="first">Aleksandar</forename><surname>Bojchevski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Johannes</forename><surname>Klicpera</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bryan</forename><surname>Perozzi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Amol</forename><surname>Kapoor</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Martin</forename><surname>Blais</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Benedek</forename><surname>Rózemberczki</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michal</forename><surname>Lukasik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stephan</forename><surname>Günnemann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 26th ACM SIGKDD International Conference on Knowledge Discovery &amp; Data Mining</title>
				<meeting>the 26th ACM SIGKDD International Conference on Knowledge Discovery &amp; Data Mining</meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="2464" to="2473" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">Friend or faux: Graph-based early detection of fake accounts on social networks</title>
		<author>
			<persName><forename type="first">Adam</forename><surname>Breuer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Roee</forename><surname>Eilat</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Udi</forename><surname>Weinsberg</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Gender shades: Intersectional accuracy disparities in commercial gender classification</title>
		<author>
			<persName><forename type="first">Joy</forename><surname>Buolamwini</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Timnit</forename><surname>Gebru</surname></persName>
		</author>
		<ptr target="https://proceedings.mlr.press/v81/buolamwini18a.html" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 1st Conference on Fairness, Accountability and Transparency</title>
				<editor>
			<persName><forename type="first">A</forename><surname>Sorelle</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Christo</forename><surname>Friedler</surname></persName>
		</editor>
		<editor>
			<persName><surname>Wilson</surname></persName>
		</editor>
		<meeting>the 1st Conference on Fairness, Accountability and Transparency</meeting>
		<imprint>
			<publisher>PMLR</publisher>
			<date type="published" when="2018-02">Feb 2018</date>
			<biblScope unit="volume">81</biblScope>
			<biblScope unit="page" from="23" to="24" />
		</imprint>
	</monogr>
	<note>of Proceedings of Machine Learning Research</note>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Detecting fraudulent personalities in networks of online auctioneers</title>
		<author>
			<persName><forename type="first">Duen</forename><surname>Horng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chau</forename></persName>
		</author>
		<author>
			<persName><forename type="first">Shashank</forename><surname>Pandit</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christos</forename><surname>Faloutsos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European conference on principles of data mining and knowledge discovery</title>
				<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2006">2006</date>
			<biblScope unit="page" from="103" to="114" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Fastgcn: Fast learning with graph convolutional networks via importance sampling</title>
		<author>
			<persName><forename type="first">Jie</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tengfei</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Cao</forename><surname>Xiao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
				<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Simple and deep graph convolutional networks</title>
		<author>
			<persName><forename type="first">Ming</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhewei</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zengfeng</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bolin</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yaliang</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
				<imprint>
			<publisher>PMLR</publisher>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="1725" to="1735" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Cluster-gcn: An efficient algorithm for training deep and large graph convolutional networks</title>
		<author>
			<persName><forename type="first">Wei-Lin</forename><surname>Chiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xuanqing</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Si</forename><surname>Si</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yang</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Samy</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Cho-Jui</forename><surname>Hsieh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 25th ACM SIGKDD International Conference on Knowledge Discovery &amp; Data Mining</title>
				<meeting>the 25th ACM SIGKDD International Conference on Knowledge Discovery &amp; Data Mining</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="257" to="266" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Adaptive universal generalized pagerank graph neural network</title>
		<author>
			<persName><forename type="first">Eli</forename><surname>Chien</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jianhao</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pan</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Olgica</forename><surname>Milenkovic</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
				<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Decoupled smoothing on graphs</title>
		<author>
			<persName><forename type="first">Alex</forename><surname>Chin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yatong</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kristen</forename><forename type="middle">M</forename><surname>Altenburger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Johan</forename><surname>Ugander</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The World Wide Web Conference</title>
				<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="263" to="272" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">Fast and accurate deep network learning by exponential linear units (elus)</title>
		<author>
			<persName><forename type="first">Djork-Arné</forename><surname>Clevert</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Thomas</forename><surname>Unterthiner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sepp</forename><surname>Hochreiter</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1511.07289</idno>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Graphzoom: A multi-level spectral approach for accurate and scalable graph embedding</title>
		<author>
			<persName><forename type="first">Chenhui</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhiqiang</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yongyu</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhiru</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhuo</forename><surname>Feng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
				<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<author>
			<persName><forename type="first">Vijay</forename><surname>Prakash Dwivedi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chaitanya</forename><forename type="middle">K</forename><surname>Joshi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Thomas</forename><surname>Laurent</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xavier</forename><surname>Bresson</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2003.00982</idno>
		<title level="m">Benchmarking graph neural networks</title>
				<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">On the evolution of random graphs</title>
		<author>
			<persName><forename type="first">Paul</forename><surname>Erdős</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alfréd</forename><surname>Rényi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Publ. Math. Inst. Hung. Acad. Sci</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="17" to="60" />
			<date type="published" when="1960">1960</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">Fast graph representation learning with pytorch geometric</title>
		<author>
			<persName><forename type="first">Matthias</forename><surname>Fey</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jan</forename><surname>Eric Lenssen</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1903.02428</idno>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Early vs late fusion in multimodal convolutional neural networks</title>
		<author>
			<persName><forename type="first">Konrad</forename><surname>Gadzicki</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Razieh</forename><surname>Khamsehashari</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christoph</forename><surname>Zetzsche</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2020 IEEE 23rd International Conference on Information Fusion (FUSION)</title>
				<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="1" to="6" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title level="m" type="main">Semi-supervised learning with heterophily</title>
		<author>
			<persName><forename type="first">Wolfgang</forename><surname>Gatterbauer</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1412.3100</idno>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title level="m" type="main">Deep learning</title>
		<author>
			<persName><forename type="first">Ian</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aaron</forename><surname>Courville</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016">2016</date>
			<publisher>MIT press Cambridge</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">The nber patent citation data file: Lessons, insights and methodological tools</title>
		<author>
			<persName><forename type="first">Bronwyn</forename><forename type="middle">H</forename><surname>Hall</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Adam</forename><forename type="middle">B</forename><surname>Jaffe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Manuel</forename><surname>Trajtenberg</surname></persName>
		</author>
		<ptr target="http://www.nber.org/papers/w8498" />
	</analytic>
	<monogr>
		<title level="j">Working Paper</title>
		<imprint>
			<biblScope unit="volume">8498</biblScope>
			<date type="published" when="2001-10">October 2001</date>
		</imprint>
		<respStmt>
			<orgName>National Bureau of Economic Research</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Inductive representation learning on large graphs</title>
		<author>
			<persName><forename type="first">Will</forename><surname>Hamilton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhitao</forename><surname>Ying</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jure</forename><surname>Leskovec</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
				<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="1024" to="1034" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">The case for online obscurity</title>
		<author>
			<persName><forename type="first">Woodrow</forename><surname>Hartzog</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Frederic</forename><surname>Stutzman</surname></persName>
		</author>
		<ptr target="http://www.jstor.org/stable/23409387" />
	</analytic>
	<monogr>
		<title level="j">California Law Review</title>
		<idno type="ISSN">00081221</idno>
		<imprint>
			<biblScope unit="volume">101</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1" to="49" />
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Resisting structural identification in anonymized social networks</title>
		<author>
			<persName><forename type="first">Michael</forename><surname>Hay</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gerome</forename><surname>Miklau</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><surname>Jensen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Donald</forename><surname>Towsley</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Philipp</forename><surname>Weis</surname></persName>
		</author>
		<idno type="DOI">10.14778/1453856.1453873</idno>
	</analytic>
	<monogr>
		<title level="j">PVLDB</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">2008</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
		<title level="m" type="main">Open graph benchmark: Datasets for machine learning on graphs</title>
		<author>
			<persName><forename type="first">Weihua</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Matthias</forename><surname>Fey</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Marinka</forename><surname>Zitnik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuxiao</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hongyu</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bowen</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michele</forename><surname>Catasta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jure</forename><surname>Leskovec</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2005.00687</idno>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Combining label propagation and simple models out-performs graph neural networks</title>
		<author>
			<persName><forename type="first">Qian</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Horace</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Abhay</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ser-Nam</forename><surname>Lim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Austin</forename><surname>Benson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
				<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Batch normalization: Accelerating deep network training by reducing internal covariate shift</title>
		<author>
			<persName><forename type="first">Sergey</forename><surname>Ioffe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christian</forename><surname>Szegedy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International conference on machine learning</title>
				<imprint>
			<publisher>PMLR</publisher>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="448" to="456" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Residual correlation in graph neural network regression</title>
		<author>
			<persName><forename type="first">Junteng</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName><surname>Austion R Benson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 26th ACM SIGKDD International Conference on Knowledge Discovery &amp; Data Mining</title>
				<meeting>the 26th ACM SIGKDD International Conference on Knowledge Discovery &amp; Data Mining</meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="588" to="598" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Node similarity preserving graph convolutional networks</title>
		<author>
			<persName><forename type="first">Wei</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tyler</forename><surname>Derr</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yiqi</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yao</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zitao</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiliang</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 14th ACM International Conference on Web Search and Data Mining</title>
				<meeting>the 14th ACM International Conference on Web Search and Data Mining</meeting>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="148" to="156" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">How to find your friendly neighborhood: Graph attention design with self-supervision</title>
		<author>
			<persName><forename type="first">Dongkwan</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alice</forename><surname>Oh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
				<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<monogr>
		<author>
			<persName><forename type="first">P</forename><surname>Diederik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jimmy</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName><surname>Ba</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1412.6980</idno>
		<title level="m">Adam: A method for stochastic optimization</title>
				<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b37">
	<monogr>
		<title level="m" type="main">Semi-supervised classification with graph convolutional networks</title>
		<author>
			<persName><forename type="first">Thomas</forename><forename type="middle">N</forename><surname>Kipf</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Max</forename><surname>Welling</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Predict then propagate: Graph neural networks meet personalized pagerank</title>
		<author>
			<persName><forename type="first">Johannes</forename><surname>Klicpera</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aleksandar</forename><surname>Bojchevski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stephan</forename><surname>Günnemann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
				<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<monogr>
		<author>
			<persName><forename type="first">M</forename><surname>Zabovsky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Takac</surname></persName>
		</author>
		<title level="m">Data analysis in public social networks. International Scientific Conference &amp; International Workshop Present Day Trends of Innovations</title>
				<imprint>
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<monogr>
		<author>
			<persName><forename type="first">Jure</forename><surname>Leskovec</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrej</forename><surname>Krevl</surname></persName>
		</author>
		<ptr target="http://snap.stanford.edu/data" />
		<title level="m">SNAP Datasets: Stanford large network dataset collection</title>
				<imprint>
			<date type="published" when="2014-06">June 2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Graphs over time: densification laws, shrinking diameters and possible explanations</title>
		<author>
			<persName><forename type="first">Jure</forename><surname>Leskovec</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jon</forename><surname>Kleinberg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christos</forename><surname>Faloutsos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the eleventh ACM SIGKDD international conference on Knowledge discovery in data mining</title>
				<meeting>the eleventh ACM SIGKDD international conference on Knowledge discovery in data mining</meeting>
		<imprint>
			<date type="published" when="2005">2005</date>
			<biblScope unit="page" from="177" to="187" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Expertise and dynamics within crowdsourced musical knowledge curation: A case study of the genius platform</title>
		<author>
			<persName><forename type="first">Derek</forename><surname>Lim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Austin</forename><forename type="middle">R</forename><surname>Benson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International AAAI Conference on Web and Social Media</title>
				<meeting>the International AAAI Conference on Web and Social Media</meeting>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<monogr>
		<title level="m" type="main">Non-local graph neural networks</title>
		<author>
			<persName><forename type="first">Meng</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhengyang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shuiwang</forename><surname>Ji</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Decoupled weight decay regularization</title>
		<author>
			<persName><forename type="first">Ilya</forename><surname>Loshchilov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Frank</forename><surname>Hutter</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
				<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Birds of a feather: Homophily in social networks</title>
		<author>
			<persName><forename type="first">Lynn</forename><surname>Miller Mcpherson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">James</forename><forename type="middle">M</forename><surname>Smith-Lovin</surname></persName>
		</author>
		<author>
			<persName><surname>Cook</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Annual review of sociology</title>
		<imprint>
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="415" to="444" />
			<date type="published" when="2001">2001</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<monogr>
		<title level="m" type="main">A survey on bias and fairness in machine learning</title>
		<author>
			<persName><forename type="first">Ninareh</forename><surname>Mehrabi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fred</forename><surname>Morstatter</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nripsuta</forename><surname>Saxena</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kristina</forename><surname>Lerman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aram</forename><surname>Galstyan</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<monogr>
		<title level="m" type="main">The 4 universities data set</title>
		<author>
			<persName><forename type="first">Tom</forename><surname>Mitchell</surname></persName>
		</author>
		<ptr target="http://www.cs.cmu.edu/afs/cs.cmu.edu/project/theo-20/www/data/" />
		<imprint>
			<date type="published" when="1997">1997</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Robust de-anonymization of large sparse datasets</title>
		<author>
			<persName><forename type="first">Arvind</forename><surname>Narayanan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vitaly</forename><surname>Shmatikov</surname></persName>
		</author>
		<idno type="DOI">10.1109/SP.2008.33</idno>
	</analytic>
	<monogr>
		<title level="m">2008 IEEE Symposium on Security and Privacy (sp 2008)</title>
				<imprint>
			<date type="published" when="2008">2008</date>
			<biblScope unit="page" from="111" to="125" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">De-anonymizing social networks</title>
		<author>
			<persName><forename type="first">Arvind</forename><surname>Narayanan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vitaly</forename><surname>Shmatikov</surname></persName>
		</author>
		<idno type="DOI">10.1109/SP.2009.22</idno>
	</analytic>
	<monogr>
		<title level="m">2009 30th IEEE Symposium on Security and Privacy</title>
				<imprint>
			<date type="published" when="2009">2009</date>
			<biblScope unit="page" from="173" to="187" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Mixing patterns in networks</title>
		<author>
			<persName><surname>Mark Ej Newman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Physical review E</title>
		<imprint>
			<biblScope unit="volume">67</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page">26126</biblScope>
			<date type="published" when="2003">2003</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">A contextual approach to privacy online</title>
		<author>
			<persName><forename type="first">Helen</forename><surname>Nissenbaum</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Daedalus</title>
		<imprint>
			<biblScope unit="volume">140</biblScope>
			<biblScope unit="page" from="32" to="48" />
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<monogr>
		<author>
			<persName><forename type="first">Hoang</forename><surname>Nt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Takanori</forename><surname>Maehara</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1905.09550</idno>
		<title level="m">Revisiting graph neural networks: All we have is low-pass filters</title>
				<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b53">
	<monogr>
		<author>
			<persName><forename type="first">N</forename><forename type="middle">T</forename><surname>Hoang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Takanori</forename><surname>Maehara</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tsuyoshi</forename><surname>Murata</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2011.10988</idno>
		<title level="m">Stacked graph filter</title>
				<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">Netprobe: a fast and scalable system for fraud detection in online auction networks</title>
		<author>
			<persName><forename type="first">Shashank</forename><surname>Pandit</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Horng</forename><surname>Duen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Samuel</forename><surname>Chau</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christos</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><surname>Faloutsos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 16th international conference on World Wide Web</title>
				<meeting>the 16th international conference on World Wide Web</meeting>
		<imprint>
			<date type="published" when="2007">2007</date>
			<biblScope unit="page" from="201" to="210" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<analytic>
		<title level="a" type="main">Pytorch: An imperative style, high-performance deep learning library</title>
		<author>
			<persName><forename type="first">Adam</forename><surname>Paszke</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sam</forename><surname>Gross</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Francisco</forename><surname>Massa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Adam</forename><surname>Lerer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">James</forename><surname>Bradbury</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gregory</forename><surname>Chanan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Trevor</forename><surname>Killeen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zeming</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Natalia</forename><surname>Gimelshein</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Luca</forename><surname>Antiga</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
				<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="page" from="8026" to="8037" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<analytic>
		<title level="a" type="main">Graph-based semi-supervised learning for relational networks</title>
		<author>
			<persName><forename type="first">Leto</forename><surname>Peel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2017 SIAM International Conference on Data Mining</title>
				<meeting>the 2017 SIAM International Conference on Data Mining</meeting>
		<imprint>
			<publisher>SIAM</publisher>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="435" to="443" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<analytic>
		<title level="a" type="main">Geom-gcn: Geometric graph convolutional networks</title>
		<author>
			<persName><forename type="first">Hongbin</forename><surname>Pei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bingzhe</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kevin</forename><surname>Chen-Chuan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yu</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bo</forename><surname>Lei</surname></persName>
		</author>
		<author>
			<persName><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
				<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b58">
	<analytic>
		<title level="a" type="main">GloVe: Global vectors for word representation</title>
		<author>
			<persName><forename type="first">Jeffrey</forename><surname>Pennington</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christopher</forename><surname>Manning</surname></persName>
		</author>
		<idno type="DOI">10.3115/v1/D14-1162</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP)</title>
				<meeting>the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP)<address><addrLine>Doha, Qatar</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2014-10">October 2014</date>
			<biblScope unit="page" from="1532" to="1543" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b59">
	<monogr>
		<title level="m" type="main">Twitch gamers: a dataset for evaluating proximity preserving and structural role-based node embeddings</title>
		<author>
			<persName><forename type="first">Benedek</forename><surname>Rozemberczki</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rik</forename><surname>Sarkar</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2101.03091</idno>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b60">
	<analytic>
		<title level="a" type="main">Multi-scale attributed node embedding</title>
		<author>
			<persName><forename type="first">Carl</forename><surname>Benedek Rozemberczki</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rik</forename><surname>Allen</surname></persName>
		</author>
		<author>
			<persName><surname>Sarkar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Complex Networks</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page">14</biblScope>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b61">
	<analytic>
		<title level="a" type="main">Collective classification in network data</title>
		<author>
			<persName><forename type="first">Prithviraj</forename><surname>Sen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Galileo</forename><surname>Namata</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mustafa</forename><surname>Bilgic</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lise</forename><surname>Getoor</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Brian</forename><surname>Galligher</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tina</forename><surname>Eliassi-Rad</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">AI magazine</title>
		<imprint>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="93" to="93" />
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b62">
	<monogr>
		<author>
			<persName><forename type="first">Oleksandr</forename><surname>Shchur</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Maximilian</forename><surname>Mumme</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aleksandar</forename><surname>Bojchevski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stephan</forename><surname>Günnemann</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1811.05868</idno>
		<title level="m">Pitfalls of graph neural network evaluation</title>
				<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b63">
	<analytic>
		<title level="a" type="main">The on-line encyclopedia of integer sequences</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">A</forename><surname>Neil</surname></persName>
		</author>
		<author>
			<persName><surname>Sloane</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Towards mechanized mathematical assistants</title>
				<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2007">2007</date>
			<biblScope unit="page" from="130" to="130" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b64">
	<analytic>
		<title level="a" type="main">Dropout: a simple way to prevent neural networks from overfitting</title>
		<author>
			<persName><forename type="first">Nitish</forename><surname>Srivastava</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Geoffrey</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alex</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ruslan</forename><surname>Salakhutdinov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The journal of machine learning research</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1929" to="1958" />
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b65">
	<analytic>
		<title level="a" type="main">Social influence analysis in large-scale networks</title>
		<author>
			<persName><forename type="first">Jie</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jimeng</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chi</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zi</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 15th ACM SIGKDD international conference on Knowledge discovery and data mining</title>
				<meeting>the 15th ACM SIGKDD international conference on Knowledge discovery and data mining</meeting>
		<imprint>
			<date type="published" when="2009">2009</date>
			<biblScope unit="page" from="807" to="816" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b66">
	<analytic>
		<title level="a" type="main">Social structure of facebook networks</title>
		<author>
			<persName><forename type="first">Peter</forename><forename type="middle">J</forename><surname>Amanda L Traud</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mason</forename><forename type="middle">A</forename><surname>Mucha</surname></persName>
		</author>
		<author>
			<persName><surname>Porter</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Physica A: Statistical Mechanics and its Applications</title>
		<imprint>
			<biblScope unit="volume">391</biblScope>
			<biblScope unit="issue">16</biblScope>
			<biblScope unit="page" from="4165" to="4180" />
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b67">
	<monogr>
		<author>
			<persName><forename type="first">Nate</forename><surname>Veldt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Austin</forename><forename type="middle">R</forename><surname>Benson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jon</forename><surname>Kleinberg</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2103.11818</idno>
		<title level="m">Higher-order homophily is combinatorially impossible</title>
				<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b68">
	<analytic>
		<title level="a" type="main">Graph attention networks</title>
		<author>
			<persName><forename type="first">Petar</forename><surname>Veličković</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Guillem</forename><surname>Cucurull</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Arantxa</forename><surname>Casanova</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Adriana</forename><surname>Romero</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pietro</forename><surname>Liò</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
				<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b69">
	<analytic>
		<title level="a" type="main">Microsoft Academic Graph: When experts are not enough</title>
		<author>
			<persName><forename type="first">Kuansan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhihong</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chiyuan</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chieh-Han</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuxiao</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Anshul</forename><surname>Kanakia</surname></persName>
		</author>
		<idno type="DOI">10.1162/qss_a_00021</idno>
		<ptr target="https://doi.org/10.1162/qss_a_00021" />
	</analytic>
	<monogr>
		<title level="j">Quantitative Science Studies</title>
		<idno type="ISSN">2641-3337</idno>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="396" to="413" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b70">
	<analytic>
		<title level="a" type="main">Simplifying graph convolutional networks</title>
		<author>
			<persName><forename type="first">Felix</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Amauri</forename><surname>Souza</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tianyi</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christopher</forename><surname>Fifty</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tao</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kilian</forename><surname>Weinberger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International conference on machine learning</title>
				<imprint>
			<publisher>PMLR</publisher>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="6861" to="6871" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b71">
	<analytic>
		<title level="a" type="main">Representation learning on graphs with jumping knowledge networks</title>
		<author>
			<persName><forename type="first">Keyulu</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chengtao</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yonglong</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tomohiro</forename><surname>Sonobe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ken-Ichi</forename><surname>Kawarabayashi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stefanie</forename><surname>Jegelka</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
				<imprint>
			<publisher>PMLR</publisher>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="5453" to="5462" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b72">
	<monogr>
		<title level="m" type="main">Two sides of the same coin: Heterophily and oversmoothing in graph convolutional neural networks</title>
		<author>
			<persName><forename type="first">Yujun</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Milad</forename><surname>Hashemi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kevin</forename><surname>Swersky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yaoqing</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Danai</forename><surname>Koutra</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2102.06462</idno>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b73">
	<analytic>
		<title level="a" type="main">Revisiting semi-supervised learning with graph embeddings</title>
		<author>
			<persName><forename type="first">Zhilin</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">William</forename><surname>Cohen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ruslan</forename><surname>Salakhudinov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International conference on machine learning</title>
				<imprint>
			<publisher>PMLR</publisher>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="40" to="48" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b74">
	<analytic>
		<title level="a" type="main">Graph convolutional neural networks for web-scale recommender systems</title>
		<author>
			<persName><forename type="first">Rex</forename><surname>Ying</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ruining</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kaifeng</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pong</forename><surname>Eksombatchai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">William</forename><forename type="middle">L</forename><surname>Hamilton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jure</forename><surname>Leskovec</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 24th ACM SIGKDD International Conference on Knowledge Discovery &amp; Data Mining</title>
				<meeting>the 24th ACM SIGKDD International Conference on Knowledge Discovery &amp; Data Mining</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="974" to="983" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b75">
	<analytic>
		<title level="a" type="main">Rajgopal Kannan, and Viktor Prasanna. Accurate, efficient and scalable graph embedding</title>
		<author>
			<persName><forename type="first">Hanqing</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hongkuan</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ajitesh</forename><surname>Srivastava</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2019 IEEE International Parallel and Distributed Processing Symposium (IPDPS)</title>
				<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="462" to="471" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b76">
	<analytic>
		<title level="a" type="main">Graphsaint: Graph sampling based inductive learning method</title>
		<author>
			<persName><forename type="first">Hanqing</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hongkuan</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ajitesh</forename><surname>Srivastava</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rajgopal</forename><surname>Kannan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Viktor</forename><surname>Prasanna</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
				<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b77">
	<analytic>
		<title level="a" type="main">Deep surface normal estimation with hierarchical rgb-d fusion</title>
		<author>
			<persName><forename type="first">Jin</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yanfeng</forename><surname>Tong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yunmu</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qiong</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wenxiu</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jing</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yongtian</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
				<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="6153" to="6162" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b78">
	<analytic>
		<title level="a" type="main">To join or not to join: the illusion of privacy in social networks with mixed public and private user profiles</title>
		<author>
			<persName><forename type="first">Elena</forename><surname>Zheleva</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lise</forename><surname>Getoor</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 18th international conference on World wide web</title>
				<meeting>the 18th international conference on World wide web</meeting>
		<imprint>
			<date type="published" when="2009">2009</date>
			<biblScope unit="page" from="531" to="540" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b79">
	<analytic>
		<title level="a" type="main">Learning with local and global consistency</title>
		<author>
			<persName><forename type="first">Dengyong</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Olivier</forename><surname>Bousquet</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jason</forename><surname>Thomas N Lal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bernhard</forename><surname>Weston</surname></persName>
		</author>
		<author>
			<persName><surname>Schölkopf</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
				<imprint>
			<date type="published" when="2004">2004</date>
			<biblScope unit="page" from="321" to="328" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b80">
	<monogr>
		<author>
			<persName><forename type="first">Jiong</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ryan</forename><forename type="middle">A</forename><surname>Rossi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Anup</forename><surname>Rao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tung</forename><surname>Mai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nedim</forename><surname>Lipka</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nesreen</forename><forename type="middle">K</forename><surname>Ahmed</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Danai</forename><surname>Koutra</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2009.13566</idno>
		<title level="m">Graph neural networks with heterophily</title>
				<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b81">
	<analytic>
		<title level="a" type="main">Beyond homophily in graph neural networks: Current limitations and effective designs</title>
		<author>
			<persName><forename type="first">Jiong</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yujun</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lingxiao</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mark</forename><surname>Heimann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Leman</forename><surname>Akoglu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Danai</forename><surname>Koutra</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
				<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="volume">33</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b82">
	<monogr>
		<title level="m" type="main">Facebook data of 1.2 million users from 2005 released: Limited exposure, but very problematic</title>
		<author>
			<persName><forename type="first">Michael</forename><surname>Zimmer</surname></persName>
		</author>
		<ptr target="https://michaelzimmer.org/2011/02/15/facebook-data-of-1-2-million-users-from-2005-released/" />
		<imprint>
			<date type="published" when="2011-02">Feb 2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b83">
	<analytic>
		<title level="a" type="main">The results are qualitatively the same, though GCN generally performs worse than the other two GNNs. C.3 Experiments on Prior Datasets Although the Pei et al. [58] datasets suffer issues with evaluation of graph learning methods as discussed in the main paper, we test LINKX on these datasets for comparison. We use the 10 fixed splits of [58], directly reporting results from papers where they also use the official splits, and re-running methods when this was not the case. In particular, GPR-GNN used larger 60-20-20 random splits, while GCNII did not evaluate on Actor and Squirrel. Thus, we re-run GPR-GNN and GCNII using the hyperparameters of Section B.1. Table 10: Comparison of non-homophilous methods on datasets of Pei et al. [58] (collected by [61, 66, 48]). † represents re-run result, if unofficial dataset splits were used in their method paper, or the paper did not evaluate their method on the specific dataset. Best results up to a standard deviation are highlighted. Geom-GCN and GCNII did not report standard deviation information</title>
	</analytic>
	<monogr>
		<title level="m">Section 5.3, we use GCNJK and MixHop as base models for minibatching evaluation. For completeness, we also include results for using GCN as a base model in Table 5.3</title>
				<imprint>
			<biblScope unit="volume">183</biblScope>
			<biblScope unit="page" from="277" to="183" />
		</imprint>
	</monogr>
	<note>GCN results. 600 5,201 2</note>
</biblStruct>

<biblStruct xml:id="b84">
	<monogr>
		<title level="m">For these experiments, we used the following hyper-parameter grid for LINKX: MLP A ∈ {1, 2}, MLP X ∈ {1, 2}, hidden channels ∈ {64</title>
				<imprint>
			<biblScope unit="volume">128</biblScope>
		</imprint>
	</monogr>
	<note>512}, number of MLP f layers ∈ {1, 2, 3, 4} learning rate in {0.05, 0.01, 0.002} and dropout ∈ {0.0, 0.5}.</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
