<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">HyperPrompt: Prompt-based Task-Conditioning of Transformers</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Yun</forename><surname>He</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Texas A&amp;M University</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Huaixiu</forename><forename type="middle">Steven</forename><surname>Zheng</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Yi</forename><surname>Tay</surname></persName>
							<email>&lt;yitay@google.com&gt;.</email>
						</author>
						<author>
							<persName><forename type="first">Jai</forename><surname>Gupta</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Yu</forename><surname>Du</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Vamsi</forename><surname>Aribandi</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Zhe</forename><surname>Zhao</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Yaguang</forename><surname>Li</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Zhao</forename><surname>Chen</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Donald</forename><surname>Metzler</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Heng-Tze</forename><surname>Cheng</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Ed</forename><forename type="middle">H</forename><surname>Chi</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Steven</forename><surname>Huaixiu</surname></persName>
							<email>&lt;stevenzheng@google.com&gt;</email>
						</author>
						<author>
							<persName><surname>Zheng</surname></persName>
						</author>
						<author>
							<affiliation key="aff1">
								<address>
									<settlement>Yi Tay</settlement>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">HyperPrompt: Prompt-based Task-Conditioning of Transformers</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.2" ident="GROBID" when="2022-12-25T13:38+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Prompt-Tuning is a new paradigm for finetuning pre-trained language models in a parameterefficient way. Here, we explore the use of Hyper-Networks to generate hyper-prompts: we propose HyperPrompt, a novel architecture for promptbased task-conditioning of self-attention in Transformers. The hyper-prompts are end-to-end learnable via generation by a HyperNetwork. Hyper-Prompt allows the network to learn task-specific feature maps where the hyper-prompts serve as task global memories for the queries to attend to, at the same time enabling flexible information sharing among tasks. We show that HyperPrompt is competitive against strong multi-task learning baselines with as few as 0.14% of additional taskconditioning parameters, achieving great parameter and computational efficiency. Through extensive empirical experiments, we demonstrate that HyperPrompt can achieve superior performances over strong T5 multi-task learning baselines and parameter-efficient adapter variants including Prompt-Tuning and HyperFormer++ on Natural Language Understanding benchmarks of GLUE and SuperGLUE across many model sizes.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Prompt-Tuning <ref type="bibr" target="#b12">(Lester et al., 2021)</ref>, learning to condition large language models with soft learnable memory tokens, have recently garnered attention owing to their ability for parameter-efficient finetuning. Prompts are lightly tuned, allowing the model to be trained quickly since the main body of the pretrained model is kept frozen. To this end, this paradigm is strongly reminiscent of adapter layers <ref type="bibr">(Houlsby</ref> Proceedings of the 39 th International Conference on Machine Learning, Baltimore, Maryland, USA, PMLR 162, 2022. Copyright 2022 by the author(s). HyperPrompt achieves state-of-the-art performance on SuperGLUE for T5 models up to XXL. Prompt-tuning <ref type="bibr" target="#b12">(Lester et al., 2021)</ref> with tuning prompt parameters only achieves competitive performance against multi-task learning (MTL) baseline for the 11B parameter model with a big performance gap for smaller models. HyperPrompt-Global outperforms the strong parameterefficient adapter variant HyperFormer++ <ref type="bibr" target="#b9">(Karimi Mahabadi et al., 2021)</ref>, the MTL baseline, and the full fine-tuning of Prompt-Tuning (our implementation) across model sizes with a large margin [e.g. 91.3 vs 90.2 (MTL) for T5 XXL]. <ref type="bibr">et al., 2019a;</ref><ref type="bibr" target="#b9">Karimi Mahabadi et al., 2021;</ref><ref type="bibr" target="#b33">Zaken et al., 2021;</ref><ref type="bibr" target="#b6">He et al., 2021)</ref> which are also efficiently finetuned.</p><p>We introduce HyperPrompt, a natural but novel extension of Prompt-Tuning to multi-task learning (MTL) for language. HyperPrompt introduces task-conditioned hyper-prompts that conditions the model on task-specific information for constructing these prompts. Hyper-prompts are injected to the keys and values in the self-attention module, reminiscent of memory augmented Transformers <ref type="bibr" target="#b23">(Sukhbaatar et al., 2019)</ref>. This mitigates the cost of having prompts pass through the standard FFN layers in Transformers and serves as additional task-specific memory tokens for queries to attend to.</p><p>We further improve upon this by introducing task-aware and layer-aware HyperNetworks <ref type="bibr" target="#b4">(Ha et al., 2017)</ref> that parameterize and generate weights for the prompt generation process. The usage of HyperNetwork imbues our model with the necessary flexibility and expressiveness, especially when it comes to incorporating task-specific and layer-specific information to the network. Meanwhile, HyperPrompt remains very parameter and computational efficient and friendly to multi-task scaling: the additional parameters scale sub-linearly with, and are independent of the number of tasks in practice. While Hypernetworks have enjoyed some success in learning adapters <ref type="bibr" target="#b9">(Karimi Mahabadi et al., 2021;</ref><ref type="bibr" target="#b24">Tay et al., 2020)</ref> and/or continual learning <ref type="bibr" target="#b26">(von Oswald et al., 2019)</ref>, we note that this is the first exploration of HyperNetworks as a prompt generator.</p><p>Contrary to prior work, we additionally propose to finetune the entire network instead of only the hyper-prompts. We make several compelling arguments for this. Firstly, <ref type="bibr" target="#b12">Lester et al. (2021)</ref> shows that parameter efficient Prompt-Tuning only shines for large (e.g., 11B) models and substantially pales in comparison to fine-tuning when the model is moderately parameterized (e.g., 220M). Secondly, finetuning only adaptive parameters (e.g., prompts/adapters) simply presents an illusion of efficiency <ref type="bibr" target="#b9">(Dehghani et al., 2021)</ref>. In reality, the FLOPs incurred by the model is still identical on the forward pass, which saves no compute during inference. Parameter counts, especially when including only prompts and adapters, are not the only measurement of computational efficiency. Instead, the FLOPs and training time should be considered together to provide a holistic view.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Our Contributions Our main contributions include:</head><p>• We propose a novel HyperPrompt Transformer architecture with learnable hyper-prompts for multi-task fine-tuning with great parameter and computational efficiency.</p><p>• We demonstrate that for difficult tasks, it is crucial to fine-tune the task-specific parameters together with the backbone model to achieve Pareto efficiency on all tasks.</p><p>• We explore HyperNetworks as a prompt generator, and inject hyper-prompts into the self-attention module as global task memory tokens.</p><p>• HyperPrompt outperforms state-of-the-art parameterefficient T5 models <ref type="bibr" target="#b18">(Raffel et al., 2019)</ref> using Prompt-Tuning or adapters on well-established benchmarks such as SuperGLUE and GLUE, across all explored model sizes (see Figure <ref type="figure">1</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Problem Statement</head><p>We consider the general setting of multi-task learning for a set of tasks {D τ } T τ =1 , where T is the total number of tasks and</p><formula xml:id="formula_0">{D τ } = {x (n) τ , y (n) τ } Nτ</formula><p>n=1 indicates the corresponding training set of the τ -th task with N τ samples. We assume that a pre-trained Transformer model f θ (•) (e.g., T5) is given, where the model is parameterized by θ. To tackle such multi-task learning problem with f θ (•), we minimize the following objective function</p><formula xml:id="formula_1">L(θ) = T τ =1 Nτ n=1 C(f θ (x (n) τ ), y (n) τ ), where C(•, •) is typically the cross-entropy loss and f θ (x (n) τ ) is the output for training sample x (n) τ .</formula><p>Transformer-based pre-trained language models such as T5 <ref type="bibr" target="#b18">(Raffel et al., 2019)</ref> and BART <ref type="bibr" target="#b13">(Lewis et al., 2020)</ref> are unified text-to-text frameworks where all tasks share the same encoder-decoder architecture -{{x</p><formula xml:id="formula_2">(n) τ } Nτ n=1 } T τ =1</formula><p>are fed into the same encoder and {{ŷ</p><formula xml:id="formula_3">(n) τ } Nτ n=1 } T τ =1</formula><p>are generated by the same decoder. For such universal modules, multi-task learning simply corresponds to mixing task data sets together and there is no task-specific classification or regression networks for each task as in encoder-only modules <ref type="bibr" target="#b3">Devlin et al. (2019)</ref>; <ref type="bibr" target="#b16">Liu et al. (2019b)</ref>.</p><p>Previous work <ref type="bibr" target="#b18">Raffel et al. (2019)</ref> shows that co-learning all tasks together on a pre-trained Transformer model is inferior to fine-tuning on each task separately. A possible reason is that θ is task-agnostic (i.e., all parameters are shared) and hence task-specific information is not well captured which can be especially true for low-resource tasks. Therefore, a natural way to improve the performance of Transformers on multi-task learning is to introduce a set of task-conditioned parameters {δ τ } T τ =1 into f θ (.). The objective function can be updated as</p><formula xml:id="formula_4">L(θ, {δ τ } T τ =1 ) = T τ =1 Nτ n=1 C(f θ,δτ (x (n) τ ), y<label>(n)</label></formula><p>τ ), where δ τ is the taskspecific parameterization for the τ -th task. During training, both θ and {δ τ } T τ =1 are updated via back-propagation because we observe a large performance drop in SuperGLUE when backbone model θ is frozen and only task-conditioned parameters are tuned, as done in Karimi Mahabadi et al. ( <ref type="formula">2021</ref>), which will be detailed in Section 4.3.</p><p>To this end, our goal is to design task-conditioned parameterization of Transformer models to achieve greater parameter and computational efficiency as well as Pareto efficiency for multi-task learning. More explicitly, we have two goals: (1) improving the finetuning performance of most tasks in {D τ } T τ =1 by introducing task-conditioned parameters {δ τ } T τ =1 into f θ (.) and (2) under the constraint that τ {δ τ } T τ =1 0 θ 0 , which means that the model capacity will not be significantly increased. And the computational cost would not increase substantially either.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Methods</head><p>In this section, we introduce HyperPrompt which has three variants: HyperPrompt-Share, HyperPrompt-Sep and HyperPrompt-Global (Figure <ref type="figure">2</ref>). We follow two key design principles to formulate HyperPrompt: (1) injecting taskconditioning into self-attention module for better computational efficiency and more expressive power via token-level interactions, and (2) using HyperNetworks to simultaneously improve the parameter efficiency and allow a flexible degree of task sharing for better generalization. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Prompt-Based Task-Conditioned Transformer</head><p>Previous adapter-based methods <ref type="bibr" target="#b9">(Karimi Mahabadi et al., 2021;</ref><ref type="bibr" target="#b24">Tay et al., 2020)</ref> for multi-task learning normally add an adapter (i.e., dense-relu-dense network) for each task after the feed-forward layers at every Transformer block. Instead, the key idea of our approach is to prepend l taskconditioned trainable vectors to the keys and values of the multihead self-attention layer at every Transformer block, where the task-specific attention feature maps are jointly learned with the task-agnostic representation.</p><p>The idea of prepending learnable prompts to the network is explored before by <ref type="bibr" target="#b14">Li &amp; Liang (2021)</ref>; <ref type="bibr" target="#b12">Lester et al. (2021)</ref>; <ref type="bibr">Liu et al. (2021)</ref> for single-task fine-tuning. We first introduce and expand this idea for multi-task learning in this subsection. Specifically, we design a novel method called HyperPrompt following the design principle #1 of injecting hyper-prompts into self-attention and #2 using HyperNetworks as generators for hyper-prompts.</p><p>At a multihead self-attention layer, the original key, value and query are calculated as</p><formula xml:id="formula_5">K τ = X τ W k , V τ = X τ W v , Q τ = X τ W q , where X τ ∈ R L×d is the input sequence of a training sample from the τ -th task, L is the sequence length, d is the model dimension. W k ∈ R d×h×d h , W v ∈ R d×h×d h and W q ∈ R d×h×d h project the input into original key K τ ∈ R L×h×d h , value V τ ∈ R L×h×d h and query Q τ ∈ R L×h×d h , h</formula><p>is the number of heads, d h is the dimension of each head and typically set to d/h to save parameters.</p><p>To learn the task-specific information for the τ -th task, we have l trainable d-dimensional vectors as the hyper-prompts for the key and the value respectively, denoted as P τ,k ∈ R l×h×d h and P τ,v ∈ R l×h×d h , as shown in Figure <ref type="figure">2</ref>(a). Then, the hyper-prompts are concatenated with the original key and value:</p><formula xml:id="formula_6">K τ = concat(P τ,k , K τ )<label>(1)</label></formula><formula xml:id="formula_7">V τ = concat(P τ,v , V τ )<label>(2)</label></formula><p>where the new key (value) K τ (V τ ) ∈ R (l+L)×h×d h are used to compute the multihead self-attention.</p><p>After that, the multihead self-attention can be operated:</p><formula xml:id="formula_8">O τ = Attention(Q τ , K τ , V τ ) = softmax(Q τ K T τ )V τ where O τ ∈ R L×d is the output of multihead attention.</formula><p>The hyper-prompts benefit Transformers for multi-task learning in two ways: (1) Prompt for key P τ,k is prepended with the original key and will participate in the calculation of attention feature map: softmax(Q τ K T τ ). P τ,k directly interacts (matrix multiplication) with the original query Q τ , allowing tokens to acquire task-specific semantics. (2) Prompt for value P τ,v is prepended with the original value and will be absorbed into the self-attention output O τ , where each position in O τ is the weighted-sum of vectors in V τ with weights from the attention scores. This way, P τ,v can serve as task-specific memories for multihead attention to retrieve information from.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">HyperPrompt</head><p>How to obtain the prompts for the m-th Transformer block? A straightforward way is to directly initialize P m τ,k and P m τ,v . However, this way is parameter-inefficient, as it scales linearly with both the number of tasks T and the number layers M as O(T × M ).</p><p>Instead, we initialize a global 1 prompt P τ for each task and apply local HyperNetworks at every Transformer block to project this prompt into {P m τ,k } M m=1 and {P m τ,v } M m=1 . Global Prompts. Specifically, we initialize a set of global prompts {P τ } T τ =1 , where P τ ∈ R l×d is a trainable matrix to learn the task-specific information of the τ -th task, d is the model dimension and l is the length of the prompt.</p><p>Local HyperNetworks. At the m-th Transformer block, we apply two local HyperNetworks h m k and h m v to transform the global prompt P τ into layer-specific and task-specific prompts as shown in Figure <ref type="figure">2(b)</ref>:</p><formula xml:id="formula_9">P m τ,k = h m k (P τ ) = U m k (Relu(D m k (P τ ))),<label>(3)</label></formula><formula xml:id="formula_10">P m τ,v = h m v (P τ ) = U m v (Relu(D m v (P τ ))),<label>(4)</label></formula><p>where P m τ,k/v ∈ R l×h×d h . We call these generated prompts hyper-prompts to distinguish from global prompts.</p><p>In particular, to limit the number of parameters, the local HyperNetworks are designed using a bottleneck architecture: HyperPrompt-Share. We first have all tasks share the same two local HyperNetworks defined by the down-project matrices D m k and D m v , and the up-project matrices U m k and U m v . We refer to this design choice as HyperPrompt-Share. Despite the saving of parameters, one drawback of HyperPrompt-Share is that the task conflicts could arise given the limited model capacity <ref type="bibr" target="#b31">(Wu et al., 2020;</ref><ref type="bibr" target="#b29">Wang et al., 2020)</ref> of the shared local HyperNetworks.</p><formula xml:id="formula_11">D m k/v ∈ R d×b and U m k/v ∈ R b×h×d h are</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>HyperPrompt-Sep.</head><p>In the opposite extreme of HyperPrompt-Share, each task can have its own local Hy-perNetworks h m τ,k (P τ ) and h m τ,v (P τ ) as following:</p><formula xml:id="formula_12">P m τ,k = h m τ,k (P τ ) = U m τ,k (Relu(D m τ,k (P τ ))),<label>(5)</label></formula><formula xml:id="formula_13">P m τ,v = h m τ,v (P τ ) = U m τ,v (Relu(D m τ,v (P τ ))),<label>(6)</label></formula><p>where D m τ,k/v and U m τ,k/v are down-projection and upprojection matrices for the τ task, respectively. In this case, each task hyper-prompt is trained independently and hence there is no information sharing.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">HyperPrompt-Global</head><p>We further propose a novel design of HyperPrompt-Global to flexibly share information and knowledge among tasks and blocks while maintaining a low parameter cost. As shown in Figure <ref type="figure">2</ref>(c), the key idea of HyperPrompt-Global is to generate the local HyperNetworks using the same 1 we term it global because it is independent of the layer number as opposed to layer-dependent prompt P m τ .</p><p>global HyperNetwork shared by all tasks and all Transformer blocks.</p><p>Layer-Aware Task Embedding. Following the same recipe in Karimi Mahabadi et al. ( <ref type="formula">2021</ref>), we define a layer-aware task embedding for better generalization. Let k τ ∈ R t denote the task embedding for the τ task and t is the dimension. To capture the layer-specific information, layer embedding z m ∈ R t is introduced. After that, a task projection network h t (•, •) is applied to fuse the task embedding and the layer embedding into the final layer-awared task embedding I m τ = h t (k τ , z m ), where I m τ is the input to the shared global HyperNetworks as shown in Figure <ref type="figure">2(c</ref> </p><formula xml:id="formula_14">(U m τ,k , D m τ,k ) = H k (I m τ ) = (W U k , W D k )I m τ ,<label>(7)</label></formula><formula xml:id="formula_15">(U m τ,v , D m τ,v ) = H v (I m τ ) = (W Uv , W Dv )I m τ ,<label>(8)</label></formula><p>where I m τ ∈ R t is the layer-aware task embedding for the τ task at the m-th block.</p><formula xml:id="formula_16">W D k ∈ R (d×b)×t , W Dv ∈ R (d×b)×t , W U k ∈ R (b×h×d h )×t and W Uv ∈ R (b×h×d h )×t are the weight matrices of H k (•) and H v (•).</formula><p>Given that U m τ,k/v , and D m τ,k/v are generated by the global HyperNetworks, we project the global prompts P τ,k/v into hyper-promtps P m τ,k/v following Eqs. 5 and 6. Finally, the hyper-prompts P m τ,k/v are prepended with original key and value at every self-attention layer as shown in Figure <ref type="figure">2(a</ref>  <ref type="formula">10</ref>) is required to achieve the optimal performance, which will be detailed in Section 4.7. On the other hand, typical values for b ∼ 24 and t ≥ 32, and therefore 4bt lT is satisfied in most cases. Hence, the space complexity could be further simplified as O(bdt). In conclusion, the space complexity of HyperPrompt-Global mainly comes from the global HyperNetworks and is practically independent of the prompt length l, the number of Transformer layers M , and the number of tasks T .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Experiments</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Experimental Setup</head><p>Datasets. We evaluate the performance of the models on GLUE <ref type="bibr" target="#b27">(Wang et al., 2018)</ref> and SuperGLUE <ref type="bibr" target="#b28">(Wang et al., 2019)</ref> respectively. Each of them is a collection of text classification tasks to test the general language understanding ability. Specifically, the tasks include: sentence acceptability (CoLA), sentiment analysis (SST-2), paraphrasing/sentence similarity (MRPC, STS-B and QQP), natural language inference (MNLI, QNLI, RTE and CB), coreference resolution (WSC), sentence completion (COPA), word sense disambiguation (WIC) and question answering (Mul-tiRC and ReCoRD, BoolQ).</p><p>Transformers. Following previous work Karimi Mahabadi et al. ( <ref type="formula">2021</ref>) and <ref type="bibr" target="#b24">Tay et al. (2020)</ref>, our models are built on top of the state-of-the-art Transformer model T5 <ref type="bibr" target="#b18">(Raffel et al., 2019)</ref>, which uses encoder-decoder architecture from <ref type="bibr" target="#b25">Vaswani et al. (2017)</ref>. We use already pre-trained T5 with sizes from Base (220M parameters) to XXL (11B).</p><p>Evaluation. We save a checkpoint every 2000 steps for all models and follow the same convention as <ref type="bibr" target="#b18">Raffel et al. (2019)</ref> in selecting the best checkpoint for each task. The emphasis of our evaluation is not to find the best single checkpoint for all tasks but to test the model's ability of transfer learning among the co-trained tasks. We first calculate the average of all metrics for each task and then report the average of all tasks for GLUE and SuperGLUE.</p><p>Baselines. We compare our proposed HyperPrompt-Share/Sep/Global with vanilla T5 models <ref type="bibr" target="#b18">(Raffel et al., 2019)</ref> for multi-task learning, which is referred to MTL. Another baseline is Vanilla Adapter proposed in <ref type="bibr" target="#b8">Houlsby et al. (2019b)</ref> that add adapters modules for each task after each of the the two feed-forward modules in each Transformer block of the T5 model. The state-of-the-art adapter-based method for multi-task learning is HyperFormer++ proposed in Karimi Mahabadi et al. ( <ref type="formula">2021</ref>) that use HyperNetworks to generate adapters for each task and add them after the feed-forward modules following <ref type="bibr" target="#b8">Houlsby et al. (2019b)</ref>. In addition, Prompt-Tuning <ref type="bibr" target="#b12">(Lester et al., 2021)</ref> is originally for parameter-efficient single-task fine-tuning and only prepends prompts to the input word embeddings in the first layer. We slightly modify it by initializing and prepending prompts for each task respectively so that Prompt-Tuning can be applied to multi-task learning.</p><p>We defer additional details of the experiments to A.2</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Key Results</head><p>Figure <ref type="figure">1</ref> provides an overall summary of the results of Hy-perPrompt. Previous prompt-tuning <ref type="bibr" target="#b12">(Lester et al., 2021;</ref><ref type="bibr" target="#b14">Li &amp; Liang, 2021</ref>) methods focus on parameter-efficient single-task fine-tuning and hence freeze the backbone and only fine-tune the prompts. Their experiments show that the performance of only tuning the prompts can match the full model training with a very large 11B model (Figure <ref type="figure">1</ref>), but substantially pales for moderate model sizes.</p><p>Our HyperPrompt-Global architecture when fully fine-tuned achieves state-of-the-art performance on SuperGLUE across four different model sizes. Competitive adapter-tuning variants including Prompt-Tuning and HyperFormer++ can either match or slightly improve upon the multi-task learning (MTL) baseline on the SuperGLUE dataset. In contrast, HyperPrompt-Global outperforms the strong MTL baseline by a large margin on SuperGLUE score (78.9 vs 77.2 for T5 Base). Interestingly, such a performance gain continues all the way to model size as big as XXL (e.g. 91.3 vs 90.2) with only 0.14% additional parameters.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Tuning all vs Task-Conditioned Parameters</head><p>Recently, Karimi <ref type="bibr" target="#b9">Mahabadi et al. (2021)</ref> show that only tuning adapters can be competitive against the full finetuning. However, the evaluation is conducted only on the GLUE with smaller models including T5 Small and Base.</p><p>In the experiments, we first compare tuning the full model vs. only task-conditioned parameters. parison on the GLUE and SuperGLUE average scores using T5 large (for per-task performance, please refer to A.4). For GLUE, the observation is consistent with <ref type="bibr" target="#b9">(Karimi Mahabadi et al., 2021)</ref>, where task-specific only fine-tuning of Hy-perFormer++ and HyperPrompt-Global is comparable to the MTL baseline. However, on SuperGLUE, we observe a large gap: the average score drops by 5.5 and 5.9 for HyperPrompt-Global and HyperFormer++, respectively.</p><p>Therefore, these experiments show that only tuning the taskconditioned parameters is not enough to achieve competitive results as full model training for multi-task learning on highdifficulty tasks such as SuperGLUE. This is consistent with the results of Prompt-Tuning <ref type="bibr" target="#b12">(Lester et al., 2021)</ref>. Hence, the rest of the experiments are conducted with tuning all model parameters.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.">Computational Efficiency</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5.">Ablation Study</head><p>Table <ref type="table" target="#tab_6">3</ref> presents the results on T5 Base and Table <ref type="table" target="#tab_8">4</ref> presents the results on T5 Large (see more detailed results in A.4). HyperPrompt-Global outperforms all baselines in terms of the average score of GLUE and SuperGLUE.</p><p>HyperPrompt-Global vs. Prompt-Tuning. The original Prompt-Tuning <ref type="bibr" target="#b12">(Lester et al., 2021</ref>) is for single-task finetuning. To be parameter-efficient, it only trains the prompts with the backbone frozen. To make a fair comparison, we modify Prompt-Tuning by ( <ref type="formula" target="#formula_6">1</ref>) training both prompts and backbone, and (2) adding prompt to each task and co-train all tasks together. As shown in Table <ref type="table" target="#tab_8">3 and 4</ref>, HyperPrompt-Global outperforms Prompt-Tuning by 2.0 (0.6) and 1.6</p><p>(1.4) on GLUE and SuperGLUE using T5 Base (Large), respectively. HyperPrompt-Global improves upon Prompt-Tuning in two places: (1) Prompt-Tuning only adds prompts to the word embedding layer while HyperPrompt-Global adds hyper-prompts at every Transformer layer and hence is more expressive; and (2) Prompts of tasks are trained independently in Prompt-Tuning while HyperPrompt-Global enables a flexible information sharing via HyperNetworks.</p><p>HyperPrompt-Global vs. HyperFormer++. Our method is superior to the state-of-the-art baseline HyperFormer++ in the average score of GLUE and SuperGLUE for both Base and Large T5 model. For example, HyperPrompt-Global of T5 large achieves 87.0 on the SuperGLUE compared to 86.4 by HyperFormer++ (  ence between the two methods is that HyperPrompt-Global inserts the task-conditioned parameters as prompts into selfattention layers while HyperFormer++ insert adapters after each block. We believe task-conditioning in self-attention gives more expressive power than in the feed-forward network as done in adapters. Hyper-prompts that are prepended with the key and value participate in the attention interactions between different token positions, which helps the model to better capture the task-dependent semantics.</p><p>HyperPrompt-Global vs. MTL. Next, we observe that using HyperPrompt-Global can greatly improve the performance upon the vanilla Transformer model (referred to MTL): 1.7 (1.1) gain on SuperGLUE score for T5 Base (Large) with 4% (2%) additional paramters. In conclusion, the experiments show that HyperPrompt-Global is a parameter-efficient and effective task-conditioned parameterization of Transformers for multi-task learning.</p><p>HyperPrompt-Global HyperPrompt-Share/Sep. Interestingly, HyperPrompt-Share is better than HyperPrompt-Sep on the SuperGLUE on both Base and Large models while the opposite is true for GLUE. Notice that all tasks share the same two projection networks in HyperPrompt-Share while each task has its own projection networks in HyperPrompt-Sep. More importantly, we observe that HyperPrompt-Global, where the projection networks are generated by the global HyperNetworks, always achieves the best performance on both GLUE and SuperGLUE. Hence, the experiments show that HyperPrompt-Global can adjust the degree of information sharing for better multi-task generalization, compared to HyperPrompt-Share/Sep.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.6.">Peeking into Hyper-Prompts</head><p>To shed light on how hyper-prompts help improve the multi-task generalization via task-conditioning, we peek into HyperPrompt-Global models by looking at the distribution of attention scores. We choose the GLUE task MRPC as an example. To avoid biasing on individual examples, we aggregate over 100 validation examples to compute the quantity of interest (see A.3 for details). First, we compute the attention mass on hyper-prompts for each encoder layer. Figure <ref type="figure" target="#fig_2">3</ref> (top) shows that the network has lower attention mass on hyper-prompts in the lower layers and gradually increases attention mass for higher layers. This phenomenon indicates that higher-levels of Transformer becomes more task-specialized while it is beneficial for the lower-levels to learn task-agnostic representation <ref type="bibr" target="#b32">(Yosinski et al., 2014)</ref> by casting lower attention mass on hyper-prompts. Furthermore, we calculate the entropy of the attention scores on the tokens. For HyperPrompt-Global, we remove the hyperprompts from the calculation and re-normalize the attention scores on the tokens to make a fair comparison with the MTL baseline. distribution towards higher values for HyperPrompt-Global. This signifies that injecting hyper-prompts encourages a more diverse attention distribution, which seems to be beneficial to model generalization.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.7.">Impact of Hyper-Prompt Length</head><p>HyperPrompt prepends l trainable hyper-prompts to the keys and values of self-attention layer at every Transformer layer. In Figure <ref type="figure" target="#fig_3">4</ref>, we present the results of tuning the prompt length l on GLUE using T5 Base as the example for HyperPrompt-Global (similar patterns are observed on T5 Large and SuperGLUE). We first add hyper-prompts on the decoder and search the best l and then search the best l for the encoder with the fixed best decoder hyper-prompt length. As shown in Figure <ref type="figure" target="#fig_3">4</ref>(a), l = 6 is the best for the decoder. As shown in Figure <ref type="figure" target="#fig_3">4</ref>(b), HyperPrompt-Global  achieves the best result of 86.8 when l = 16 on the encoder with l = 6 fixed for the decoder. The experiments show that hyper-prompts with length l ∼ O(10) are good enough to achieve superior performance. Note that the original sequence length is 512 on the encoder and 32 on the decoder. Therefore, HyperPrompt does not substantially increase the time complexity of self-attention layers in practice.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.8.">Encoder vs Decoder</head><p>To understand the effect of adding task-conditioned parameters to different parts of the network, we present the results of HyperPrompt-Global and HyperFormer++ with adding hyper-prompts/adapters to: (1) encoder-only, (2) decoderonly, and (3) both encoder-decoder. As shown in Table <ref type="table" target="#tab_10">5</ref>, we observe adding task-conditioned parameters to encoder (encoder-only) performs better than decoder-only on GLUE. However, the opposite is true for SuperGLUE, where encoder-only is substantially worse than decoderonly. This potentially could be a trainability issue when prompts are inserted into encoders, i.e. a different learning rate might be required to learn the prompt parameters from scratch. We leave this investigation as a future work. Based on this experiment, we add task-conditioned parameters to the decoder for SuperGLUE in our experiments. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Model</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Related Work</head><p>Prompt-Tuning. Prompt tuning is becoming a new paradigm for adapting pre-trained general-purpose language models to downstream tasks, as a lightweight alternative to the popular fine-tuning approach. Here, we use the term Prompt-Tuning to cover a family of methods following the prompting idea in GPT-3 <ref type="bibr" target="#b2">(Brown et al., 2020)</ref>. To avoid manually design the prompts, recent efforts have focused on search for discrete prommpting words automatically <ref type="bibr" target="#b21">(Shin et al., 2020)</ref>. On the other hand, soft prompts <ref type="bibr" target="#b14">(Li &amp; Liang, 2021;</ref><ref type="bibr" target="#b5">Hambardzumyan et al., 2021;</ref><ref type="bibr" target="#b12">Lester et al., 2021;</ref><ref type="bibr">Liu et al., 2021)</ref> in the form of continuous vectors are introduced to simplify the process and have shown competitive results <ref type="bibr" target="#b12">(Lester et al., 2021;</ref><ref type="bibr">Liu et al., 2021;</ref><ref type="bibr" target="#b14">Li &amp; Liang, 2021)</ref>. In particular, <ref type="bibr" target="#b12">Lester et al. (2021)</ref> show that soft prompts can become competitive against full fine-tuning for a 11B parameters model, but with a big performance gap for moderate-size models. In our work, we close this gap in the full fine-tuning setting and demonstrate that Hyper-Prompt can outperform strong baselines across all model sizes studied.</p><p>Adapter-Tuning. Adapter tuning <ref type="bibr" target="#b7">(Houlsby et al., 2019a;</ref><ref type="bibr">b;</ref><ref type="bibr" target="#b9">Karimi Mahabadi et al., 2021)</ref> is an alternative approach for parameter-efficient lightweight tuning of pre-trained langauge models. Task-specific adapter layers <ref type="bibr" target="#b7">(Houlsby et al., 2019a)</ref> are inserted into the Transformer block for fine-tuning while the rest of the backbone model is frozen. By adding only a few percent of additional parameters, Karimi <ref type="bibr" target="#b9">Mahabadi et al. (2021)</ref> show that competitive performance can be obtained on NLU benchmarks such as GLUE <ref type="bibr" target="#b27">(Wang et al., 2018)</ref>. However, one limitation from the existing work is the evaluation of NLU on GLUE dataset, which is known to be no longer suitable for measuring the progress of language understanding <ref type="bibr" target="#b28">(Wang et al., 2019)</ref>. In our work, we evaluate HyperPrompt on SuperGLUE in addition to GLUE, and show that indeed higher-difficulty tasks such as SuperGLUE requires full-tuning of the model beyond adapter tuning, to be competitive against state-of-the-art baselines. We also demonstrate that it is advantageous to inject prompts into self-attention than adding adapters.</p><p>Multi-task Natural Language Understanding. Multitask learning is an important and challenge research direction in both full fine-tuning and prompt-tuning paradigms because of the competing needs of training and serving a single model while achieving Pareto efficiency in all tasks.</p><p>The T5 model <ref type="bibr" target="#b18">(Raffel et al., 2019</ref>) renders all NLP tasks as a Text-to-Text problem. However, the best results are obtained by task-specific fine-tuning. MTDNN (multi-task deep neural network) <ref type="bibr" target="#b15">(Liu et al., 2019a)</ref> shares parameters between several NLP tasks, and achieves strong performance on the GLUE benchmark. <ref type="bibr" target="#b0">Aghajanyan et al. (2021)</ref>   <ref type="formula">2021</ref>) also illustrated how a multi-task learning stage can greatly improve the zero-shot prompting performance of large language models.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Conclusion</head><p>We propose a novel architecture for prompt-based taskconditioning of self-attention in Transformers. The hyperprompts are generated by a HyperNetwork to enable flexible information sharing among tasks while remain efficient in parameters and computation. HyperPrompt allows the network to learn task-specific feature maps where the hyper-prompts serve as task global memories, encouraging a more diverse distribution of attention. Extensive experiments show that HyperPrompt can achieve superior performances over strong T5 multi-task learning baselines and parameter-efficient models including Prompt-Tuning and HyperFormer++ on GLUE and SuperGLUE benchmarks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Appendix</head><p>This section covers the parameter count of HyperPrompt, the experimental details, the calculation of attention mass and entropy, and per-task performance of GLUE and SuperGLUE.</p><p>A.1. Parameter Count of HyperPrompt-Global ( §3.4)</p><p>Since the encoder and the decoder of Transformers have approximately the same capacity, the calculation considers only the decoder-side for simplicity. First, we have global task prompts P τ ∈ R l×d for the τ -th task, which contains dlT parameters for T tasks. The global HyperNetworks contain four weight matrices</p><formula xml:id="formula_17">W D k ∈ R (d×b)×t , W Dv ∈ R (d×b)×t , W U k ∈ R (b×h×d h</formula><p>)×t and W Uv ∈ R (b×h×d h )×t , which result in 4(bdt) parameters (we let d = h × d h ). To obtain layer-aware task embedding, HyperPrompt-Global learns task embedding k τ ∈ R t for the τ task and layer embedding z m ∈ R t for the m-th Transformer block, which in total results in T t + M t parameters. Besides, a task projection network h t is applied to fuse the task embedding and the layer embedding into the final layer-aware task embedding I m τ ∈ R t . h t is a two-layer feed-forward networks and contains (2t + t)e parameters, where e is the hidden dimension for h t .</p><p>A.2. Experimental Details ( §4.1) Our models were implemented using Mesh Tensorflow<ref type="foot" target="#foot_0">2</ref>  <ref type="bibr" target="#b20">(Shazeer et al., 2018)</ref> with the T5 library<ref type="foot" target="#foot_1">3</ref>  <ref type="bibr" target="#b18">(Raffel et al., 2019)</ref>. Following <ref type="bibr" target="#b18">Raffel et al. (2019)</ref>, all data are preprocessed as into a "sequence-to-sequence" format. The length of the sequence is 512 at the encoder and 32 at the decoder. For all experiments, we train models 300K steps with a batch size of 128 and each batch is a mixture which samples each task proportionately to the number of examples in the dataset. Learning rate is a constant of 1e-3 with Adam optimizer <ref type="bibr" target="#b10">(Kingma &amp; Ba, 2014)</ref>.</p><p>For hyper-parameters tuning, the length of prompt l is selected from {12, 16, 20, 20, 24} at the encoder and {2, 4, 6, 8, 10, 12, 14, 16} at the decoder. The bottleneck dimension b in the transform matrices is set to d/r, where d is the model dimension of the T5 models and r is a reduction factor and selected from {16, 32, 64}. The dimension t of the layer-aware task embedding is selected from {32, 64, 128}. For a fair comparison, the hyper-parameters of baseline methods are set to have approximately the same numbers of parameters as HyperPrompt-Global with the exception that Prompt-Tuning and HyperPrompt-Share are extremely parameter-efficient with significantly fewer parameters.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.3. Attention Mass and Entropy calculation ( §4.6)</head><p>To calculate the attention mass over hyper-prompts per layer, we averaged the hyper-prompt attention softmax scores across 100 validation examples and each attention head in a layer, and summed across each query attending to the hyper-prompts. In other words, we aggregated the amount of attention given to hyper-prompts by queries. To calculate the attention entropy over tokens (other than hyper-prompts), we calculated the entropy of the attention distributions (averaged across attention heads) for 100 validation examples. This results in 100 n=1 12 L=1 |X n | entropies calculated and visualized in Figure <ref type="figure" target="#fig_2">3</ref> (bottom). For the HyperPrompt model, this involved re-normalizing the softmax distribution after removing hyper-prompts, as we wanted to understand how the original tokens are attended to.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.4. Per-Task Performance of GLUE and SuperGLUE</head><p>Table <ref type="table" target="#tab_12">6</ref> and 7 below show the comparison of fine-tuning the entire model against task-specific parameters only on GLUE and SuperGLUE datasets. Table <ref type="table" target="#tab_15">8 and 9</ref> show the detailed results of full-tuning of HyperPrompt against baselines on T5 Base. Table <ref type="table" target="#tab_16">10</ref> and 11 show the detailed results of full-tuning of HyperPrompt against baselines on T5 Large. </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head></head><label></label><figDesc>Figure1. HyperPrompt achieves state-of-the-art performance on SuperGLUE for T5 models up to XXL. Prompt-tuning<ref type="bibr" target="#b12">(Lester et al., 2021)</ref> with tuning prompt parameters only achieves competitive performance against multi-task learning (MTL) baseline for the 11B parameter model with a big performance gap for smaller models. HyperPrompt-Global outperforms the strong parameterefficient adapter variant HyperFormer++<ref type="bibr" target="#b9">(Karimi Mahabadi et al., 2021)</ref>, the MTL baseline, and the full fine-tuning of Prompt-Tuning (our implementation) across model sizes with a large margin [e.g. 91.3 vs 90.2 (MTL) for T5 XXL].</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head></head><label></label><figDesc>down-projection and up-projection matrices, respectively. b is the bottleneck dimension satisfying b d.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 .</head><label>3</label><figDesc>Figure 3. Visualization of attention mass and entropy distribution.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 4 .</head><label>4</label><figDesc>Figure 4. Impact of hyper-prompt length in HyperPrompt-Global (GLUE score on T5 Base).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head></head><label></label><figDesc>). h t is a MLP consisting of two feed-forward layers and a ReLU non-linearity, which takes the concatenation of k τ and z m as input.</figDesc><table><row><cell>Global HyperNetworks. H k (•) generates the weight matri-</cell></row><row><cell>ces (U m τ,k , D m τ,k ) in the local HyperNetworks of key hyper-</cell></row><row><cell>prompts and another global HyperNetwork H v (•) generates</cell></row><row><cell>the weight matrices (U m τ,v , D m τ,v ) in the local HyperNet-</cell></row><row><cell>works of value hyper-prompts:</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head></head><label></label><figDesc>) to calculate the task-conditioned attention scores. have far fewer parameters than the global HyperNetworks, the additional task-conditioned parameters is almost independent of T . where d is the model dimension, l is the length of the prompts, T is the total number of tasks, b is the bottleneck dimension of the weight matrices of the local HyperNetworks, d is the model dimension, t /t is the dimension of the raw/final layer-aware task embedding, and e is the hidden dimension of h k/v . Therefore, the space complexity is O(d(lT + 4bt)), given that in practice M ∼ T , t dl, and e bd. This leads to a sub-linear scaling with respect to T . Furthermore, T is typical ∼ O(10) for multi-task learning. A reasonable l ∼ O(</figDesc><table><row><cell>3.4. Parameter Efficiency of HyperPrompt</cell></row><row><cell>As shown in A.1, the total number of additional parame-</cell></row><row><cell>ters from HyperPrompt-Global is dlT + 4(bdt) + T t +</cell></row><row><cell>M t + (2t + t)e,</cell></row><row><cell>Using global HyperNetworks to generate the projection</cell></row><row><cell>networks has two benefits:</cell></row><row><cell>1. It enables a more flexible way to share information</cell></row><row><cell>across tasks and layers: the transformation matrices are</cell></row><row><cell>decomposed into H k/v (•) that are shared by all tasks</cell></row><row><cell>and all layers. Therefore, the model can adjust the</cell></row><row><cell>degree of information sharing across tasks and layers</cell></row><row><cell>through learning the appropriate parameter values in</cell></row><row><cell>H k/v (•) during the end-to-end training.</cell></row></table><note>2. A parameter-efficient task conditioned parameterization is enabled. The number of extra task-conditioned parameters doesn't depend on the number of layers M , and scales sub-linearly with respect to the total number of tasks T . In practice, since task embeddings and task prompts</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 1 .</head><label>1</label><figDesc>Table1shows the com-Comparison of fine-tuning all vs task-specific parameters using T5 Large. The average scores of GLUE and SuperGLUE are reported on T5 Large.</figDesc><table><row><cell>Tunable</cell><cell>Model</cell><cell cols="2">GLUE SuperGLUE</cell></row><row><cell>All</cell><cell>MTL</cell><cell>88.3</cell><cell>85.9</cell></row><row><cell>All</cell><cell>HyperFormer++</cell><cell>88.8</cell><cell>86.4</cell></row><row><cell>All</cell><cell>HyperPrompt-Global</cell><cell>89.4</cell><cell>87</cell></row><row><cell>Task</cell><cell>HyperFormer++</cell><cell>87.3</cell><cell>80.5</cell></row><row><cell>Task</cell><cell>HyperPrompt-Global</cell><cell>87.5</cell><cell>81.5</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 2</head><label>2</label><figDesc></figDesc><table><row><cell cols="3">presents the computational efficiency of the</cell></row><row><cell cols="3">Adapter/Prompt models. HyperPrompt-Global (together</cell></row><row><cell cols="3">with HyperPrompt-Share) has the lowest # Ops since hyper-</cell></row><row><cell cols="3">prompts are injected into self-attention and skip the stan-</cell></row><row><cell cols="3">dard FFN layers. In contrast, HyperFormer++ has ∼ 3x #</cell></row><row><cell cols="3">Ops compared to other variants. Regarding training time,</cell></row><row><cell cols="3">HyperPrompt-Share is fastest given that the local Hyper-</cell></row><row><cell cols="3">Networks are shared across tasks. Vanilla Adapter and</cell></row><row><cell cols="3">HyperPrompt-Global are comparable while HyperFormer++</cell></row><row><cell cols="3">and Prompt-Tuning take significant longer to do the full</cell></row><row><cell cols="3">fine-tuning. This shows the computational efficiency of</cell></row><row><cell cols="3">HyperPrompt for both training and inference.</cell></row><row><cell>Model</cell><cell># Ops</cell><cell>Training Time (hours)</cell></row><row><cell>Vanilla Adapter</cell><cell>1.01 ×10 13</cell><cell>8.4</cell></row><row><cell>HyperFormer++</cell><cell>3.14 ×10 13</cell><cell>10.3</cell></row><row><cell>Prompt-Tuning</cell><cell>1.16 ×10 13</cell><cell>11.1</cell></row><row><cell>HyperPrompt-Sep</cell><cell>1.01 ×10 13</cell><cell>8.9</cell></row><row><cell>HyperPrompt-Share</cell><cell>9.8×10 12</cell><cell>8.0</cell></row><row><cell cols="2">HyperPrompt-Global 9.8×10 12</cell><cell>8.7</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 2 .</head><label>2</label><figDesc>The number of operations for a single forward pass and training time on T5 Base.</figDesc><table><row><cell>Model</cell><cell>#Params</cell><cell>GLUE</cell><cell>SuperGLUE</cell></row><row><cell>MTL</cell><cell>1.0x</cell><cell>85.5 (0.9)</cell><cell>77.2 (0.2)</cell></row><row><cell>Vanilla Adapter</cell><cell>1.06x</cell><cell>86.7 (0.3)</cell><cell>77.5 (0.1)</cell></row><row><cell>HyperFormer++</cell><cell>1.04x</cell><cell>86.5 (0.0)</cell><cell>78.2 (0.7)</cell></row><row><cell>Prompt-Tuning</cell><cell cols="2">1.0003x 84.8 (0.6)</cell><cell>77.3 (0.2)</cell></row><row><cell>HyperPrompt-Share</cell><cell>1.008x</cell><cell>86.4 (0.6)</cell><cell>78.2 (0.7)</cell></row><row><cell>HyperPrompt-Sep</cell><cell>1.06x</cell><cell>86.8 (0.1)</cell><cell>77.5 (0.1)</cell></row><row><cell>HyperPrompt-Global</cell><cell>1.04x</cell><cell>86.8 (0.4)</cell><cell>78.9 (0.5)</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 3 .</head><label>3</label><figDesc>GLUE and SuperGLUE average scores (standard deviations) over 3 runs of HyperPrompt against baselines on T5 Base.</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head></head><label></label><figDesc>Table4). Note that the main differ-</figDesc><table><row><cell>Model</cell><cell>#Params</cell><cell>GLUE</cell><cell>SuperGLUE</cell></row><row><cell>MTL</cell><cell>1.0x</cell><cell>88.3 (0.6)</cell><cell>85.9 (0.3)</cell></row><row><cell>Vanilla Adapter</cell><cell>1.06x</cell><cell>88.8 (0.2)</cell><cell>86.1 (0.5)</cell></row><row><cell>HyperFormer++</cell><cell>1.02x</cell><cell>88.8 (0.0)</cell><cell>86.4 (0.5)</cell></row><row><cell>Prompt-Tuning</cell><cell cols="2">1.0001x 88.8 (0.3)</cell><cell>85.6 (0.1)</cell></row><row><cell>HyperPrompt-Share</cell><cell>1.008x</cell><cell>89.3 (0.1)</cell><cell>86.8 (0.2)</cell></row><row><cell>HyperPrompt-Sep</cell><cell>1.06x</cell><cell>89.4 (0.2)</cell><cell>86.1 (0.3)</cell></row><row><cell>HyperPrompt-Global</cell><cell>1.02x</cell><cell>89.4 (0.1)</cell><cell>87.0 (0.5)</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>Table 4 .</head><label>4</label><figDesc>GLUE</figDesc><table /><note>and SuperGLUE average scores (standard deviations) over 3 runs of HyperPrompt against baselines on T5 Large.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_10"><head>Table 5 .</head><label>5</label><figDesc>Ablation of inserting hyper-prompts or adapters into Encoder/Decoder/Enc-Dec (T5 Base).</figDesc><table><row><cell></cell><cell cols="3">#Params GLUE SuperGLUE</cell></row><row><cell>MTL</cell><cell>1.0x</cell><cell>85.5</cell><cell>77.2</cell></row><row><cell cols="3">HyperFormer++-Encoder 1.02x 85.9</cell><cell>74.4</cell></row><row><cell cols="3">HyperFormer++-Decoder 1.02x 85.7</cell><cell>78.2</cell></row><row><cell cols="3">HyperFormer++-Enc-Dec 1.04x 86.5</cell><cell>74.8</cell></row><row><cell>HyperPrompt-Encoder</cell><cell cols="2">1.02x 86.6</cell><cell>76.5</cell></row><row><cell>HyperPrompt-Decoder</cell><cell cols="2">1.02x 86.3</cell><cell>78.9</cell></row><row><cell>HyperPrompt-Enc-Dec</cell><cell cols="2">1.04x 86.8</cell><cell>78.7</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_12"><head>Table 6 .</head><label>6</label><figDesc>Comparison of fine-tuning all vs task-specific parameters on GLUE.</figDesc><table><row><cell>Tunable Parameters</cell><cell>Model</cell><cell cols="2">CoLA SST-2</cell><cell cols="2">MRPC</cell><cell>SST-B</cell><cell>QQP</cell><cell>MNLI</cell><cell>QNLI RTE AVG</cell></row><row><cell>All</cell><cell>MTL</cell><cell>59.4</cell><cell>96.6</cell><cell cols="5">93.3/90.7 90.6/90.4 89.8/92.3 90.8/90.8</cell><cell>95.2</cell><cell>90.8 88.3</cell></row><row><cell>All</cell><cell>HyperFormer++-T5.1.1 LARGE</cell><cell>63.3</cell><cell>96.6</cell><cell cols="5">93.2/90.7 92.1/91.9 89.7/92.3 90.5/90.7</cell><cell>95.1</cell><cell>89.9 88.8</cell></row><row><cell>All</cell><cell>HyperPrompt-T5.1.1 LARGE</cell><cell>64.6</cell><cell>96.7</cell><cell cols="5">94.0/91.8 91.3/91.4 90.0/92.4 90.8/91.0</cell><cell>95.4</cell><cell>91.9 89.4</cell></row><row><cell>Task-Specific</cell><cell>HyperFormer++-T5.1.1 LARGE</cell><cell>58.9</cell><cell>95.7</cell><cell cols="5">92.7/90.0 91.6/91.5 87.7/90.7 89.8/90.0</cell><cell>94.5</cell><cell>87.0 87.3</cell></row><row><cell>Task-Specific</cell><cell>HyperPrompt-T5.1.1 LARGE</cell><cell>57.5</cell><cell>96.7</cell><cell cols="5">93.6/91.2 91.9/92.0 87.0/90.1 90.3/90.6</cell><cell>95.0</cell><cell>87.7 87.5</cell></row><row><cell>Tunable Parameters</cell><cell>Model</cell><cell>BoolQ</cell><cell cols="2">CB</cell><cell cols="4">COPA MultiRC ReCoRD RTE WiC WSC AVG</cell></row><row><cell>All</cell><cell>MTL</cell><cell>88.5</cell><cell cols="2">95.8/98.2</cell><cell>87.0</cell><cell cols="3">85.5/56.3 89.2/88.6 91.7 74.0 89.4</cell><cell>85.9</cell></row><row><cell>All</cell><cell>HyperFormer++-T5.1.1 LARGE</cell><cell>88.9</cell><cell cols="2">98.7/98.2</cell><cell>86.7</cell><cell cols="3">85.4/56.7 89.4/88.8 92.1 74.5 90.7</cell><cell>86.4</cell></row><row><cell>All</cell><cell>HyperPrompt-T5.1.1 LARGE</cell><cell>88.7</cell><cell cols="2">99.1/98.8</cell><cell>91.0</cell><cell cols="3">85.0/55.6 89.8/89.1 91.3 74.2 92.0</cell><cell>87.0</cell></row><row><cell>Task-Specific</cell><cell>HyperFormer++-T5.1.1 LARGE</cell><cell>85.2</cell><cell cols="2">90.9/94.6</cell><cell>76.7</cell><cell cols="3">81.5/48.8 87.2/86.4 87.7 67.8 82.1</cell><cell>80.5</cell></row><row><cell>Task-Specific</cell><cell>HyperPrompt-T5.1.1 LARGE</cell><cell>85.2</cell><cell cols="2">95.2/95.5</cell><cell>75.5</cell><cell cols="3">82.9/52.9 89.1/88.3 85.7 71.1 82.2</cell><cell>81.5</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_13"><head>Table 7 .</head><label>7</label><figDesc>Comparison of fine-tuning all vs task-specific parameters on SuperGLUE.</figDesc><table><row><cell>Model</cell><cell cols="3">#Params CoLA SST-2</cell><cell>MRPC</cell><cell>SST-B</cell><cell>QQP</cell><cell>MNLI</cell><cell>QNLI RTE AVG</cell></row><row><cell>MTL</cell><cell>1.0x</cell><cell>49.8</cell><cell>94.6</cell><cell cols="4">92.5/89.8 90.7/90.5 89.2/91.9 88.8/88.5</cell><cell>93.3</cell><cell>85.0 85.5</cell></row><row><cell>Vanilla Adapter</cell><cell>1.06x</cell><cell>60.0</cell><cell>95.4</cell><cell cols="4">92.7/89.8 90.2/90.2 89.3/91.9 88.5/88.1</cell><cell>93.5</cell><cell>84.4 86.7</cell></row><row><cell>HyperFormer++</cell><cell>1.04x</cell><cell>56.9</cell><cell>94.8</cell><cell cols="4">92.9/90.1 91.1/90.9 88.9/91.7 88.7/88.3</cell><cell>93.4</cell><cell>85.6 86.5</cell></row><row><cell>Prompt-Tuning</cell><cell>1.0003x</cell><cell>48.0</cell><cell>95.0</cell><cell cols="4">92.2/89.0 90.3/90.2 89.0/91.7 88.8/88.5</cell><cell>93.2</cell><cell>82.9 84.8</cell></row><row><cell>HyperPrompt-Share (ours)</cell><cell>1.008x</cell><cell>56.2</cell><cell>94.7</cell><cell cols="4">93.0/90.4 90.6/90.4 89.2/91.9 88.7/88.4</cell><cell>93.4</cell><cell>85.2 86.4</cell></row><row><cell>HyperPrompt-Sep (ours)</cell><cell>1.06x</cell><cell>57.2</cell><cell>94.6</cell><cell cols="4">93.8/91.4 91.0/90.8 89.2/91.9 88.5/88.4</cell><cell>93.4</cell><cell>86.6 86.8</cell></row><row><cell>HyperPrompt-Global (ours)</cell><cell>1.04x</cell><cell>57.0</cell><cell>95.2</cell><cell cols="4">93.4/90.9 90.4/90.2 89.2/92.0 88.7/88.5</cell><cell>93.4</cell><cell>87.1 86.8</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_14"><head>Table 8 .</head><label>8</label><figDesc>Comparison of HyperPrompt with baselines on GLUE using T5 Base.</figDesc><table><row><cell>Model</cell><cell cols="2">#Params BoolQ</cell><cell>CB</cell><cell cols="4">COPA MultiRC ReCoRD RTE WIC WSC AVG</cell></row><row><cell>MTL</cell><cell>1.0x</cell><cell>82.6</cell><cell>93.4/93.5</cell><cell>65.7</cell><cell>76.7/39.7 80.9/80.2 85.6 70.5</cell><cell>81.4</cell><cell>77.2</cell></row><row><cell>Vanilla Adapter</cell><cell>1.03x</cell><cell>83.5</cell><cell>93.4/94.6</cell><cell>65.3</cell><cell>77.6/42.7 81.0/80.2 88.2 71.0</cell><cell>76.9</cell><cell>77.5</cell></row><row><cell>HyperFormer++</cell><cell>1.02x</cell><cell>83.5</cell><cell>96.2/97.0</cell><cell>66.3</cell><cell>77.8/41.9 81.2/80.4 87.4 71.0</cell><cell>80.1</cell><cell>78.2</cell></row><row><cell>Prompt-Tuning</cell><cell>1.0003x</cell><cell>82.5</cell><cell>94.0/95.8</cell><cell>68.0</cell><cell>76.9/40.2 80.9/80.2 84.1 69.3</cell><cell>80.8</cell><cell>77.3</cell></row><row><cell>HyperPrompt-Share (ours)</cell><cell>1.004x</cell><cell>83.1</cell><cell>95.7/95.2</cell><cell>67.7</cell><cell>77.3/41.3 81.9/81.0 87.4 70.4</cell><cell>80.8</cell><cell>78.2</cell></row><row><cell>HyperPrompt-Sep (ours)</cell><cell>1.03x</cell><cell>83.3</cell><cell>97.8/97.0</cell><cell>61.7</cell><cell>77.6/42.3 81.5/80.6 86.8 71.4</cell><cell>78.2</cell><cell>77.5</cell></row><row><cell>HyperPrompt-Global (ours)</cell><cell>1.02x</cell><cell>83.3</cell><cell>96.6/96.4</cell><cell>69.7</cell><cell>77.5/41.0 81.7/80.9 86.8 70.5</cell><cell>83.7</cell><cell>78.9</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_15"><head>Table 9 .</head><label>9</label><figDesc>Comparison of HyperPrompt with baselines on SuperGLUE using T5 Base.</figDesc><table><row><cell>Model</cell><cell cols="3">#Params CoLA SST-2</cell><cell>MRPC</cell><cell>SST-B</cell><cell>QQP</cell><cell>MNLI</cell><cell>QNLI RTE AVG</cell></row><row><cell>MTL</cell><cell>1.0x</cell><cell>59.4</cell><cell>96.6</cell><cell cols="4">93.3/90.7 90.6/90.4 89.8/92.3 90.8/90.8</cell><cell>95.2</cell><cell>90.8 88.3</cell></row><row><cell>Vanilla Adapter</cell><cell>1.06x</cell><cell>63.8</cell><cell>96.5</cell><cell cols="4">93.7/91.3 92.0/91.9 90.0/92.5 90.6/90.5</cell><cell>94.9</cell><cell>88.7 88.8</cell></row><row><cell>HyperFormer++</cell><cell>1.02x</cell><cell>63.3</cell><cell>96.6</cell><cell cols="4">93.2/90.7 92.1/91.9 89.7/92.3 90.5/90.7</cell><cell>95.1</cell><cell>89.9 88.8</cell></row><row><cell>Prompt-Tuning</cell><cell>1.0001x</cell><cell>62.5</cell><cell>96.7</cell><cell cols="4">93.4/91.0 91.3/91.0 90.0/92.4 90.9/91.0</cell><cell>95.4</cell><cell>89.9 88.8</cell></row><row><cell>HyperPrompt-Share (ours)</cell><cell>1.008x</cell><cell>65.0</cell><cell>96.7</cell><cell cols="4">93.8/91.6 91.1/90.8 90.0/92.4 90.8/91.1</cell><cell>95.3</cell><cell>91.3 89.3</cell></row><row><cell>HyperPrompt-Sep (ours)</cell><cell>1.06x</cell><cell>63.9</cell><cell>96.6</cell><cell cols="4">94.6/92.6 92.0/91.7 90.0/92.4 90.9/91.0</cell><cell>95.2</cell><cell>91.6 89.4</cell></row><row><cell>HyperPrompt-Global (ours)</cell><cell>1.02x</cell><cell>64.6</cell><cell>96.7</cell><cell cols="4">94.0/91.8 91.3/91.4 90.0/92.4 90.8/91.0</cell><cell>95.4</cell><cell>91.9 89.4</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_16"><head>Table 10 .</head><label>10</label><figDesc>Comparison of HyperPrompt with baselines on GLUE using T5 Large.</figDesc><table><row><cell>Model</cell><cell cols="2">#Params BoolQ</cell><cell>CB</cell><cell cols="4">COPA MultiRC ReCoRD RTE WIC WSC AVG</cell></row><row><cell>MTL</cell><cell>1.0x</cell><cell>88.5</cell><cell>95.8/98.2</cell><cell>87.0</cell><cell>85.5/56.3 89.2/88.6 91.7 74.0</cell><cell>89.4</cell><cell>85.9</cell></row><row><cell>Vanilla Adapter</cell><cell>1.03x</cell><cell>88.8</cell><cell>98.3/98.8</cell><cell>86.0</cell><cell>85.3/56.0 89.3/88.7 91.2 73.6</cell><cell>91.3</cell><cell>86.1</cell></row><row><cell>HyperFormer++</cell><cell>1.01x</cell><cell>88.9</cell><cell>98.7/98.2</cell><cell>86.7</cell><cell>85.4/56.7 89.4/88.8 92.1 74.5</cell><cell>90.7</cell><cell>86.4</cell></row><row><cell>Prompt-Tuning</cell><cell>1.0001x</cell><cell>88.5</cell><cell>97.6/98.8</cell><cell>85.0</cell><cell>84.9/55.2 89.0/88.4 91.5 72.8</cell><cell>90.1</cell><cell>85.6</cell></row><row><cell>HyperPrompt-Share (ours)</cell><cell>1.004x</cell><cell>88.5</cell><cell>98.7/98.2</cell><cell>88.0</cell><cell>85.2/55.8 89.7/89.1 91.8 74.1</cell><cell>93.9</cell><cell>86.8</cell></row><row><cell>HyperPrompt-Sep (ours)</cell><cell>1.03x</cell><cell>88.6</cell><cell>97.6/98.8</cell><cell>87.7</cell><cell>85.2/56.4 89.7/89.1 91.6 73.5</cell><cell>89.4</cell><cell>86.1</cell></row><row><cell>HyperPrompt-Global (ours)</cell><cell>1.01x</cell><cell>88.7</cell><cell>99.1/98.8</cell><cell>91.0</cell><cell>85.0/55.6 89.8/89.1 91.3 74.2</cell><cell>92.0</cell><cell>87.0</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_17"><head>Table 11 .</head><label>11</label><figDesc>Comparison of HyperPrompt with baselines on SuperGLUE using T5 Large.</figDesc><table /></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_0">https://github.com/tensorflow/mesh</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3" xml:id="foot_1">https://github.com/google-research/text-to-text-transfer-Transformer</note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Massive multitask representations with pre-finetuning</title>
		<author>
			<persName><forename type="first">A</forename><surname>Aghajanyan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Shrivastava</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Zettlemoyer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName><surname>Muppet</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2021.emnlp-main.468</idno>
		<ptr target="https://aclanthology.org/2021.emnlp-main.468" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing</title>
				<meeting>the 2021 Conference on Empirical Methods in Natural Language Processing<address><addrLine>Punta Cana, Dominican Republic</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2021-11">November 2021</date>
			<biblScope unit="page" from="5799" to="5811" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">Ext5: Towards extreme multi-task scaling for transfer learning</title>
		<author>
			<persName><forename type="first">V</forename><surname>Aribandi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Tay</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Schuster</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Rao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">S</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">V</forename><surname>Mehta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Zhuang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><forename type="middle">Q</forename><surname>Tran</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Bahri</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Ni</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Hui</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Ruder</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Metzler</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Language models are few-shot learners</title>
		<author>
			<persName><forename type="first">T</forename><surname>Brown</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Mann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Ryder</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Subbiah</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">D</forename><surname>Kaplan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Dhariwal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Neelakantan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Shyam</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Sastry</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Askell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Agarwal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Herbert-Voss</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Krueger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Henighan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Child</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Ramesh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Ziegler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Winter</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Hesse</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Sigler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Litwin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Gray</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Chess</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Clark</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Berner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Mccandlish</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Amodei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Arnab</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Beyer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Tay</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename></persName>
		</author>
		<idno type="arXiv">arXiv:2110.12894</idno>
		<ptr target="https://proceedings.neurips.cc/paper/2020/file/1457c0d6bfcb4967418bfb8ac142f64a-Paper.pdf.Dehghani" />
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
				<editor>
			<persName><forename type="first">H</forename><surname>Larochelle</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">M</forename><surname>Ranzato</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">R</forename><surname>Hadsell</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">M</forename><forename type="middle">F</forename><surname>Balcan</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">H</forename><surname>Lin</surname></persName>
		</editor>
		<imprint>
			<publisher>Curran Associates, Inc</publisher>
			<date type="published" when="2020">2020. 2021</date>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page" from="1877" to="1901" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
	<note>The efficiency misnomer</note>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Pre-training of deep bidirectional transformers for language understanding</title>
		<author>
			<persName><forename type="first">J</forename><surname>Devlin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M.-W</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Toutanova</surname></persName>
		</author>
		<author>
			<persName><surname>Bert</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/N19-1423</idno>
		<ptr target="https://aclanthology.org/N19-1423" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</title>
		<title level="s">Long and Short Papers</title>
		<meeting>the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies<address><addrLine>Minneapolis, Minnesota</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2019-06">June 2019</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="4171" to="4186" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<author>
			<persName><forename type="first">D</forename><surname>Ha</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">M</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><forename type="middle">V</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName><surname>Hypernetworks</surname></persName>
		</author>
		<ptr target="https://openreview.net/forum?id=rkpACe1lx" />
		<title level="m">5th International Conference on Learning Representations</title>
				<meeting><address><addrLine>Toulon, France</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2017-04-24">2017. April 24-26, 2017. 2017</date>
		</imprint>
	</monogr>
	<note>Conference Track Proceedings. OpenReview.net</note>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">WARP: Word-level Adversarial ReProgramming</title>
		<author>
			<persName><forename type="first">K</forename><surname>Hambardzumyan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Khachatrian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>May</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2021.acl-long.381</idno>
		<ptr target="https://aclanthology.org/2021.acl-long.381" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing</title>
		<title level="s">Long Papers</title>
		<meeting>the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing</meeting>
		<imprint>
			<publisher>Online</publisher>
			<date type="published" when="2021-08">August 2021</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="4921" to="4933" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">Towards a unified view of parameter-efficient transfer learning</title>
		<author>
			<persName><forename type="first">J</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Berg-Kirkpatrick</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Neubig</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Parameter-efficient transfer learning for NLP</title>
		<author>
			<persName><forename type="first">N</forename><surname>Houlsby</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Giurgiu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Jastrzebski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Morrone</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>De Laroussilhe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Gesmundo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Attariyan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Gelly</surname></persName>
		</author>
		<idno>PMLR, 09-15</idno>
		<ptr target="https://proceedings.mlr.press/v97/houlsby19a.html" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 36th International Conference on Machine Learning</title>
				<editor>
			<persName><forename type="first">K</forename><surname>Chaudhuri</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">R</forename><surname>Salakhutdinov</surname></persName>
		</editor>
		<meeting>the 36th International Conference on Machine Learning</meeting>
		<imprint>
			<date type="published" when="2019-06">Jun 2019a</date>
			<biblScope unit="volume">97</biblScope>
			<biblScope unit="page" from="2790" to="2799" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Parameter-efficient transfer learning for nlp</title>
		<author>
			<persName><forename type="first">N</forename><surname>Houlsby</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Giurgiu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Jastrzebski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Morrone</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>De Laroussilhe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Gesmundo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Attariyan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Gelly</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
				<imprint>
			<publisher>PMLR</publisher>
			<date type="published" when="2019">2019b</date>
			<biblScope unit="page" from="2790" to="2799" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Parameter-efficient multi-task fine-tuning for transformers via shared hypernetworks</title>
		<author>
			<persName><forename type="first">R</forename><surname>Karimi Mahabadi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Ruder</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Dehghani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Henderson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing</title>
		<title level="s">Long Papers</title>
		<meeting>the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2021-08">August 2021</date>
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">P</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Ba</surname></persName>
		</author>
		<author>
			<persName><surname>Adam</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1412.6980</idno>
		<title level="m">A method for stochastic optimization</title>
				<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Crafting papers on machine learning</title>
		<author>
			<persName><forename type="first">P</forename><surname>Langley</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 17th International Conference on Machine Learning (ICML 2000)</title>
				<editor>
			<persName><forename type="first">P</forename><surname>Langley</surname></persName>
		</editor>
		<meeting>the 17th International Conference on Machine Learning (ICML 2000)<address><addrLine>Stanford, CA</addrLine></address></meeting>
		<imprint>
			<publisher>Morgan Kaufmann</publisher>
			<date type="published" when="2000">2000</date>
			<biblScope unit="page" from="1207" to="1216" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">The power of scale for parameter-efficient prompt tuning</title>
		<author>
			<persName><forename type="first">B</forename><surname>Lester</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Al-Rfou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Constant</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2104.08691</idno>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Denoising sequence-to-sequence pre-training for natural language generation, translation, and comprehension</title>
		<author>
			<persName><forename type="first">M</forename><surname>Lewis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Ghazvininejad</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Mohamed</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Levy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Stoyanov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Zettlemoyer</surname></persName>
		</author>
		<author>
			<persName><surname>Bart</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics</title>
				<meeting>the 58th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="7871" to="7880" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">Prefix-tuning: Optimizing continuous prompts for generation</title>
		<author>
			<persName><forename type="first">X</forename><forename type="middle">L</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Liang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2101.00190</idno>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Multi-task deep neural networks for natural language understanding</title>
		<author>
			<persName><forename type="first">X</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Gao</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/P19-1441</idno>
		<ptr target="https://aclanthology.org/P19-1441" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics</title>
				<meeting>the 57th Annual Meeting of the Association for Computational Linguistics<address><addrLine>Florence, Italy</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2019-07">July 2019a</date>
			<biblScope unit="page" from="4487" to="4496" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Multi-task deep neural networks for natural language understanding</title>
		<author>
			<persName><forename type="first">X</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Gao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics</title>
				<meeting>the 57th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2019">2019b</date>
			<biblScope unit="page" from="4487" to="4496" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title/>
		<author>
			<persName><forename type="first">X</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Qian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Gpt understands</title>
		<imprint>
			<biblScope unit="page">2021</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">Exploring the limits of transfer learning with a unified text-to-text transformer</title>
		<author>
			<persName><forename type="first">C</forename><surname>Raffel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Roberts</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Narang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Matena</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">J</forename><surname>Liu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1910.10683</idno>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">Multitask prompted training enables zero-shot task generalization</title>
		<author>
			<persName><forename type="first">V</forename><surname>Sanh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Webson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Raffel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">H</forename><surname>Bach</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Sutawika</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Alyafeai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Chaffin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Stiegler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">L</forename><surname>Scao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Raja</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Dey</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">S</forename><surname>Bari</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">U</forename><surname>Thakker</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">S</forename><surname>Sharma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Szczechla</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Chhablani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Nayak</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Datta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">T</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">.-J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Manica</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Yong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><forename type="middle">X</forename><surname>Pandey</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Bawden</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Neeraj</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Rozen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Sharma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Santilli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Fevry</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Fries</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">A</forename><surname>Teehan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Biderman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Bers</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Wolf</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Rush</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">M</forename></persName>
		</author>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">Mesh-tensorflow: Deep learning for supercomputers</title>
		<author>
			<persName><forename type="first">N</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Tran</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Koanantakool</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Hawkins</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Hong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Young</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1811.02084</idno>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Eliciting Knowledge from Language Models with Automatically Generated Prompts</title>
		<author>
			<persName><forename type="first">T</forename><surname>Shin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Razeghi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><forename type="middle">V</forename><surname>Logan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">L</forename><surname>Wallace</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Autoprompt</surname></persName>
		</author>
		<idno>doi: 10.18653</idno>
		<ptr target="/v1/2020.emnlp-main" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing</title>
				<meeting>the 2020 Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2020-11">November 2020</date>
			<biblScope unit="page" from="4222" to="4235" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<ptr target="https://aclanthology.org/2020.emnlp-main.346" />
		<title level="m">URL</title>
				<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">Augmenting self-attention with persistent memory</title>
		<author>
			<persName><forename type="first">S</forename><surname>Sukhbaatar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Grave</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Lample</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Jegou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Joulin</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1907.01470</idno>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<author>
			<persName><forename type="first">Y</forename><surname>Tay</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Bahri</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Metzler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D.-C</forename><surname>Juan</surname></persName>
		</author>
		<author>
			<persName><surname>Hypergrid</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2007.05891</idno>
		<title level="m">Efficient multi-task transformers with gridwise decomposable hyper projections</title>
				<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Attention is all you need</title>
		<author>
			<persName><forename type="first">A</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">N</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ł</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Polosukhin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
				<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="5998" to="6008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<author>
			<persName><forename type="first">J</forename><surname>Von Oswald</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Henning</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Sacramento</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">F</forename><surname>Grewe</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1906.00695</idno>
		<title level="m">Continual learning with hypernetworks</title>
				<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<author>
			<persName><forename type="first">A</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Michael</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Hill</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Levy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">R</forename><surname>Bowman</surname></persName>
		</author>
		<author>
			<persName><surname>Glue</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1804.07461</idno>
		<title level="m">A multi-task benchmark and analysis platform for natural language understanding</title>
				<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<title level="m" type="main">Superglue: A stickier benchmark for general-purpose language understanding systems</title>
		<author>
			<persName><forename type="first">A</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Pruksachatkun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Nangia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Michael</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Hill</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Levy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">R</forename><surname>Bowman</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1905.00537</idno>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<author>
			<persName><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Fifty</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Hong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">H</forename><surname>Chi</surname></persName>
		</author>
		<title level="m">Small towers make big differences</title>
				<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
		<title level="m" type="main">Finetuned language models are zero-shot learners</title>
		<author>
			<persName><forename type="first">J</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Bosma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><forename type="middle">Y</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Guu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">W</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Lester</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">M</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><forename type="middle">V</forename><surname>Le</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Understanding and improving information transfer in multi-task learning</title>
		<author>
			<persName><forename type="first">S</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">R</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Ré</surname></persName>
		</author>
		<ptr target="https://openreview.net/forum?id=SylzhkBtDB" />
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
				<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">How transferable are features in deep neural networks?</title>
		<author>
			<persName><forename type="first">J</forename><surname>Yosinski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Clune</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Lipson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
				<imprint>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="3320" to="3328" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
		<title level="m" type="main">Simple parameter-efficient fine-tuning for transformerbased masked language-models</title>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">B</forename><surname>Zaken</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Ravfogel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Goldberg</surname></persName>
		</author>
		<author>
			<persName><surname>Bitfit</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2106.10199</idno>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
