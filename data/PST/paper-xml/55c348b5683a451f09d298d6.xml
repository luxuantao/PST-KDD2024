<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Incremental Dense Semantic Stereo Fusion for Large-Scale Semantic Scene Reconstruction</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Vibhav</forename><surname>Vineet</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Ondrej</forename><surname>Miksik</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Morten</forename><surname>Lidegaard</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Matthias</forename><surname>Nießner</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Stuart</forename><surname>Golodetz</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Victor</forename><forename type="middle">A</forename><surname>Prisacariu</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Olaf</forename><surname>Kähler</surname></persName>
						</author>
						<author>
							<persName><forename type="first">David</forename><forename type="middle">W</forename><surname>Murray</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Shahram</forename><surname>Izadi</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Patrick</forename><surname>Pérez</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Philip</forename><forename type="middle">H S</forename><surname>Torr</surname></persName>
						</author>
						<title level="a" type="main">Incremental Dense Semantic Stereo Fusion for Large-Scale Semantic Scene Reconstruction</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">C17A36B707A495C7E53B60E0FABB6082</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.3" ident="GROBID" when="2023-07-28T04:38+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Our abilities in scene understanding, which allow us to perceive the 3D structure of our surroundings and intuitively recognise the objects we see, are things that we largely take for granted, but for robots, the task of understanding large scenes quickly remains extremely challenging. Recently, scene understanding approaches based on 3D reconstruction and semantic segmentation have become popular, but existing methods either do not scale, fail outdoors, provide only sparse reconstructions or are rather slow. In this paper, we build on a recent hash-based technique for large-scale fusion and an efficient mean-field inference algorithm for densely-connected CRFs to present what to our knowledge is the first system that can perform dense, large-scale, outdoor semantic reconstruction of a scene in (near) real time. We also present a 'semantic fusion' approach that allows us to handle dynamic objects more effectively than previous approaches. We demonstrate the effectiveness of our approach on the KITTI dataset, and provide qualitative and quantitative results showing high-quality dense reconstruction and labelling of a number of scenes.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>I. INTRODUCTION</head><p>As we navigate the world, for example when driving a car from our home to the work place, we constantly perceive the 3D structure of the environment around us and recognise objects within it. Such capabilities help us in our everyday lives and allow us free and accurate movement even in unfamiliar places.</p><p>Building a system that can automatically perform incremental real-time dense large-scale reconstruction and semantic segmentation, as illustrated in Fig. <ref type="figure">1</ref>, is a crucial prerequisite for a variety of applications, including robot navigation <ref type="bibr" target="#b0">[1]</ref>, <ref type="bibr" target="#b1">[2]</ref>, semantic mapping <ref type="bibr" target="#b2">[3]</ref>, <ref type="bibr" target="#b3">[4]</ref>, wearable and/or assistive technology <ref type="bibr" target="#b4">[5]</ref>, <ref type="bibr" target="#b5">[6]</ref>, and change detection <ref type="bibr" target="#b6">[7]</ref>. However, despite the large body of literature motivated by such applications <ref type="bibr" target="#b2">[3]</ref>, <ref type="bibr" target="#b3">[4]</ref>, <ref type="bibr" target="#b7">[8]</ref>- <ref type="bibr" target="#b11">[12]</ref>, most existing approaches suffer from a variety of limitations. Offline reconstruction methods can achieve impressive results at city scale <ref type="bibr" target="#b12">[13]</ref> and beyond, but cannot be used in a real-time setting. Sparse online reconstructions <ref type="bibr" target="#b13">[14]</ref>- <ref type="bibr" target="#b16">[17]</ref> were historically favoured over dense ones due to their lower computational V. Vineet and O. Miksik assert joint first authorship. {vibhav.vineet, ondra.miksik}@gmail.com V. <ref type="bibr">Vineet</ref>  -building -vegetation -car -road -wall -pavement -pole Fig. <ref type="figure">1</ref>: Incremental reconstruction (top) and semantic segmentation (bottom) from our system, as seen from a moving platform on-the-fly (i.e. not a final mesh).</p><p>requirements and the difficulties of acquiring adequate input for dense methods, but sparse maps are not guaranteed to contain objects of interest (e.g. traffic lights, signs). Dense reconstructions working on a regular voxel grid <ref type="bibr" target="#b17">[18]</ref>- <ref type="bibr" target="#b19">[20]</ref> are limited to small volumes due to memory requirements. This has been addressed by approaches that use scalable data structures and stream data between GPU and CPU memory <ref type="bibr" target="#b20">[21]</ref>, <ref type="bibr" target="#b21">[22]</ref>, but they use Kinect-like cameras that only work indoors <ref type="bibr" target="#b8">[9]</ref>, <ref type="bibr" target="#b9">[10]</ref>. Approaches working outdoors usually take significant time to run <ref type="bibr" target="#b3">[4]</ref>, <ref type="bibr" target="#b7">[8]</ref>, <ref type="bibr" target="#b10">[11]</ref>, <ref type="bibr" target="#b22">[23]</ref>, do not work incrementally <ref type="bibr" target="#b11">[12]</ref> or rely on LIDAR data <ref type="bibr" target="#b23">[24]</ref>. Existing systems also do not cope well with moving objects. Ideally, we believe a method should 1) be able to incrementally build a dense semantic 3D map of any indoor or outdoor environment at any scale; 2) perform both tasks on-the-fly at real-time rates; 3) be amenable to handling moving objects.</p><p>In this paper, we propose an end-to-end system that can process the data incrementally and perform real-time dense stereo reconstruction and semantic segmentation of unbounded outdoor environments. The system outputs a pervoxel probability distribution instead of a single label (soft predictions are desirable in robotics, as the vision output is usually fed as input into other subsystems). Our system is also able to handle moving objects more effectively Next, we (d) fuse the depth into a common 3D map. We also (e) extract features, (f) evaluate unary potentials for each voxel and (g) perform inference over a densely-connected pairwise 3D random field to generate a high-quality labelling, which (h) controls fusion weights.</p><p>than prior approaches by incorporating knowledge of object classes into the reconstruction process. In order to achieve fast test times, we extensively use the computational power of modern GPUs.</p><p>Our goal is to incrementally build dense large-scale semantic outdoor maps. We emphasise the incremental nature of our approach, as many methods employ post-processing techniques such as surface densification, texture mapping and tone matting, etc. to produce high-quality or visuallyplausible meshes. However, in most robotics settings it is the actual output produced on-the-fly that matters (Fig. <ref type="figure">1</ref>). This consideration motivates both our reconstruction pipeline and the system as a whole.</p><p>At the core of our system (Fig. <ref type="figure" target="#fig_0">2</ref>) is a scalable fusion approach <ref type="bibr" target="#b21">[22]</ref> that allows the reconstruction of high-quality surfaces in virtually unbounded scenes. It achieves this by replacing the fixed dense 3D volumetric representation of the standard formulations <ref type="bibr" target="#b17">[18]</ref>- <ref type="bibr" target="#b19">[20]</ref> with a hash-table-driven counterpart that ignores unoccupied space in the target environment. Furthermore, whilst the standard formulations are limited by the available GPU memory, <ref type="bibr" target="#b21">[22]</ref> swaps/streams map data between device and host memories as needed. This is key for scalable dense reconstruction, and to our knowledge has thus far only been used in indoor environments.</p><p>Outdoor scenes present several challenges: 1) Kinect-like cameras are less effective outdoors, whilst LIDARs are often too large for "wearable robotics" or produce overly sparse point-clouds: we thus prefer to rely on stereo, which is suitable for both large robots and wearable glasses/headsets; 2) as a result, the estimated depth <ref type="bibr" target="#b24">[25]</ref> is usually more noisy; 3) the depth range is much larger and 4) dynamically moving objects are much more common and the camera itself may move significantly between consecutive frames (e.g. if mounted on a car, etc.). All of this makes data association for ICP camera pose estimation (as used in <ref type="bibr" target="#b19">[20]</ref>, <ref type="bibr" target="#b21">[22]</ref>) harder, so we replaced it with a more reliable visual odometry <ref type="bibr" target="#b15">[16]</ref>.</p><p>Our semantic segmentation pipeline extracts 2D features and evaluates unary potentials based on random forest classifier predictions. It transfers these into the 3D volume, where we define a densely-connected CRF. Volumetric CRFs reduce the computational burden, since multiple pixels usually correspond to the same voxel, and enforce temporal consistency, since we label actual 3D surfaces. In order to efficiently infer the approximate maximum posterior marginal (MPM) solution, we propose an online volumetric mean-field inference technique that incrementally refines the marginals of a voxel across iterations, and design a volumetric filter that is suitable for parallel implementation. This allows us to run inference each frame (a single mean-field update takes 2-6ms), so our dynamic energy landscape changes slowly and only a few mean-field update iterations are required at each time step. We use our semantic labels to reinforce the weights in the fusion step, thereby allowing us to handle moving objects more effectively than prior approaches (see §IV).</p><p>All parts of our system are implemented on a GPU, except for visual odometry and disparity estimation, but both are easily parallelisable and can hence be switched to the GPU.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>II. RELATED WORK A. Reconstruction</head><p>Recently, <ref type="bibr" target="#b25">[26]</ref> demonstrated large-scale semi-dense reconstruction using only a monocular camera. Early real-time dense approaches <ref type="bibr" target="#b17">[18]</ref>, <ref type="bibr" target="#b18">[19]</ref> were able to estimate depth from monocular input, but their use of a regular voxel grid limited reconstruction to small volumes due to memory requirements. KinectFusion <ref type="bibr" target="#b19">[20]</ref> directly sensed depth using active sensors and fused noisy depth measurements of the perceived scene over time to recover high-quality surfaces, but suffered from the same scalability issue. This drawback has since been removed by scalable approaches that use either a voxel hierarchy <ref type="bibr" target="#b20">[21]</ref> or voxel block hashing <ref type="bibr" target="#b21">[22]</ref> to avoid storing unnecessary data for free space, and stream individual trees in the hierarchy or voxel blocks between the GPU and CPU to allow scaling to unbounded scenes. The hashing approach has the advantage of supporting constanttime lookups of voxel blocks, whereas lookups even in a balanced hierarchy are logarithmic in the number of blocks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Semantic Segmentation</head><p>Many approaches have been proposed in this field <ref type="bibr" target="#b2">[3]</ref>, <ref type="bibr" target="#b3">[4]</ref>, <ref type="bibr" target="#b7">[8]</ref>- <ref type="bibr" target="#b11">[12]</ref>, <ref type="bibr" target="#b22">[23]</ref>, <ref type="bibr" target="#b23">[24]</ref>. A summary of the most relevant papers and key attributes for outdoor large-scale reconstruction is provided in Tab. I.</p><p>Hermans et al. <ref type="bibr" target="#b8">[9]</ref> use a random forest classifier and a dense 2D CRF, transfer the resulting marginals into 3D and solve a 3D CRF to refine the predictions. Other shortcomings aside (see Tab. I), a CPU implementation requires heuristic scheduling (frame-skipping, etc.) to maintain a near-real-time frame rate. Sengupta et al. <ref type="bibr" target="#b3">[4]</ref> proposed an offline method, which uses label transfer from 2D to 3D with sampling in a reversed order, which is computationally very expensive. They support streaming from RAM (CPU implementation), but not back again, i.e. they always start from scratch. Similarly, Valentin et al. <ref type="bibr" target="#b11">[12]</ref> define a CRF over a reconstructed mesh, leading to faster inference. However, their method is not incremental, i.e. they need to reconstruct the whole scene first and then label it. Kundu et al. <ref type="bibr" target="#b10">[11]</ref> proposed an offline method (based on personal communication) to integrate sparse (monocular) reconstruction with 2D semantic labels into a CRF model to determine the structure and labelling of a scene. Whilst their results are visually appealing, they do appear slightly voxelated when viewed at close range. Other methods <ref type="bibr" target="#b7">[8]</ref>, <ref type="bibr" target="#b17">[18]</ref>, <ref type="bibr" target="#b18">[19]</ref>, <ref type="bibr" target="#b22">[23]</ref> share similar issues, whilst Hu et al. <ref type="bibr" target="#b26">[27]</ref> relies on LIDAR data. In contrast to <ref type="bibr" target="#b3">[4]</ref>, <ref type="bibr" target="#b10">[11]</ref>, <ref type="bibr" target="#b11">[12]</ref>, <ref type="bibr" target="#b22">[23]</ref>, our method provides soft predictions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>III. LARGE-SCALE OUTDOOR RECONSTRUCTION</head><p>Our system relies on passive stereo cameras, so we need to estimate the depth data that we want to fuse into our reconstruction each frame. In order to fuse the depth data, we also need to know the current pose of the camera, so we run a camera pose tracker in parallel with our depth estimation process. The following subsections describe the three parts of our reconstruction system (depth estimation, camera pose estimation and large-scale fusion) in more detail.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Depth Estimation</head><p>To estimate depth from each stereo pair, we first estimate disparity and then convert it to depth using the equation z i = bf /d i , in which z i and d i are (respectively) the depth and disparity for the i'th pixel, b is the stereo camera baseline and f is the camera's focal length. For disparity estimation, we use the approach of Geiger et al. <ref type="bibr" target="#b24">[25]</ref>, which forms a triangulation on a set of support points that can be robustly matched. This reduces matching ambiguities and allows efficient exploitation of the disparity via constraints on the search space without requiring any global optimization. As a result, the method can be easily parallelised.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Camera Pose Estimation</head><p>To estimate camera pose, we use the FOVIS feature-based visual odometry method <ref type="bibr" target="#b15">[16]</ref>. First, an input pair of images is preprocessed using a Gaussian smoothing filter and a three-level image pyramid is built (each level corresponds to one octave in scale space). Then, a set of sparse local features is extracted by using a FAST corner detector with an adaptively-chosen threshold to detect a sufficient number of features. The feature extraction step is usually "biased" using bucketing to ensure that features are uniformly distributed across space and scale.</p><p>To constrain the feature matching stage to local search windows, an initial rotation of the image plane is estimated to deal with small motions in 3D. The matching stage associates the extracted features with descriptors and features are matched using a mutual-consistency check. A robust estimate is performed either by finding a maximal clique in the graph or using RANSAC, and the final transformation is estimated on the inliers. Robustness is further increased by using "keyframes", which reduces drift when the camera viewpoint does not change significantly. This can be further improved by using a full SLAM with loop closures, but this is beyond the scope of this paper.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Large-Scale Fusion</head><p>Traditionally, KinectFusion-based approaches have fused depth inside a full, dense, volumetric 3D representation, which severely limits the size of reconstruction that can be handled. However, in real-world scenarios, a large part of this volume only contains free space, which does not need to be densely stored. By focusing the representation on the useful parts of the scene, we can use memory much more efficiently, which in turn enables much larger environments to be reconstructed. This insight has acted as a catalyst for works such as the hash-based method of <ref type="bibr" target="#b21">[22]</ref> and the octree technique of <ref type="bibr" target="#b20">[21]</ref>.</p><p>We adopt the hash-based fusion method <ref type="bibr" target="#b21">[22]</ref>, which allocates space for only those voxels that fall within a small distance of the perceived surfaces in the scene. This space is organised into small voxel blocks. As with other depth fusion approaches, the dense areas are represented using an approximate truncated signed distance function (TSDF) <ref type="bibr" target="#b27">[28]</ref>. Access to individual voxel blocks is mediated by a hash table. Given a known camera pose ( §III-B), we use the following fusion pipeline: a) Allocation: We ensure that voxel blocks are allocated for each voxel visible in the depth image. This is done by (i) back-projecting all visible voxels to voxel block world coordinates; (ii) looking up each unique voxel block in the hash table to determine whether or not it is currently allocated and (iii) allocating any blocks that are currently unallocated.</p><p>b) Integration: We integrate the current depth and colour frames into the volumetric data structure, using the conventional sliding-average technique of <ref type="bibr" target="#b27">[28]</ref>.</p><p>c) Host-device streaming: Although current GPUs have several GB of device memory, it is generally not enough to store a full large-scale reconstruction. To this end, data is streamed between the device and host. We only keep parts that are in or near the frustum. To implement this approach, we actively swap parts of the map between device and host memory as they move in and out of view. Note that the scale of the reconstructions we can handle is still limited by host RAM in the current implementation. However, it would be simple to use the "swapping in and out" strategy between RAM and disk storage to achieve virtually unbounded reconstructions.</p><p>d) Raycasting: In every frame, the fused depth map is rendered from the current camera position.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>IV. SEMANTIC FUSION</head><p>In the standard fusion approach, each voxel i stores TSDF and colour measurements T t i and Ĉt i at time t, together with weights ŵt T,i and ŵt C,i that capture our confidence in these measurements. These values are updated over time using the corresponding live TDSF and colour measurements T t i and C t i , and some live weights w t T,i and w t C,i that can often be set to 1 to give simple running averages, e.g.:</p><formula xml:id="formula_0">ŵt T,i = ŵt-1 T,i + w t T,i T t i = ( ŵt-1 T,i T t-1 i + w t T,i T t i )/( ŵt-1 T,i + w t T,i )<label>(1</label></formula><p>) This fusion step generally fails when there are moving objects in the scene, since static objects can become corrupted when we fuse in depth data from moving objects. This effect can be reduced by basing the live weights w t T,i and w t C,i on object class: by using higher weights for voxels that are labelled with moving object classes (e.g. car, pedestrian, etc.), we can speed up the process of fusing new data into our TSDF in places where the scene is more likely to be changing rapidly, which allows us to avoid being left with incorrect surfaces in places that briefly contained moving objects (note that the weights for voxels increase as we fuse in moving object data, and take some time to decrease again after the objects leave the voxels again). We call this adaptation of the original scheme "semantic fusion", and update our measurements using ŵt</p><formula xml:id="formula_1">T,i = ŵt-1 T,i + w t i T t i = ( ŵt-1 T,i T t-1 i + w t i T t i )/( ŵt-1 T,i + w t i ),<label>(2)</label></formula><p>in which w t i is a per-class fixed weight corresponding to the the semantic label of voxel i at time t.</p><p>This approach temporarily decreases the smoothness of the surface of affected voxels, but it allows us to preserve moving objects in a scene and avoids corruption of static objects. An example showing the way in which our semantic fusion approach is able to handle dynamically-moving objects is shown in Figure <ref type="figure" target="#fig_4">6</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>V. VOLUMETRIC CRF AND MEAN-FIELD INFERENCE A. Model</head><p>We begin by defining a random field over random variables X = {X 1 , ..., X N }, conditioned on the 3D surface D. We assume that each discrete random variable X i is associated with a voxel V ∈ {1, ..., N } in the 3D reconstruction volume and takes a label l i from a finite label set L = {l 1 , ..., l L }, corresponding to different object classes such as car, building or road. We formulate the problem of assigning object labels to the voxels as one of solving a volumetric, denselyconnected, pairwise Conditional Random Field (CRF).</p><p>We define this CRF over the voxels in the current view frustum. Since our volumetric reconstruction is dynamically changing as new observations are captured, we have to deal with a dynamic energy function that keeps on changing in each iteration. Our CRF can be expressed as</p><formula xml:id="formula_2">P (X|D) = 1 Z(D) exp(-E(X|D)) E(X|D) = i∈V ψ u (X i ) + i&lt;j∈V ψ p (X i , X j ),<label>(3)</label></formula><p>in which E(X|D) is the energy associated with a configuration X, conditioned on the volumetric data D, Z(D) = X exp(-E(X |D)) is the (data-dependent) partition function and ψ u (•) and ψ p (•, •) are the unary potential and pairwise potential functions, respectively, both implicitly conditioned on the data D.</p><p>Unary potentials: Unary potential terms ψ u (•) correspond to the cost of voxel i taking an object label l ∈ L. In order to evaluate the per-voxel unary potentials, we first train per-pixel object class models derived from TextonForest <ref type="bibr" target="#b28">[29]</ref> using a set of per-pixel ground truth training images <ref type="bibr" target="#b3">[4]</ref>. We use the 17-dimensional filter bank suggested by Shotton et al. <ref type="bibr" target="#b28">[29]</ref>, and follow Ladický et al. <ref type="bibr" target="#b29">[30]</ref> by adding colour, histogram of oriented gradients (HOG), and pixel location features. At test time, we evaluate unary potentials in the image domain and then project them onto the voxels using the current camera pose and average them over time. Pairwise potentials: The pairwise potential function ψ p (•, •) enforces consistency over pairs of random variables and thus generally leads to a smooth output. In our application, we use the weighted Potts model, which takes the form</p><formula xml:id="formula_3">ψ ij (l, l ) = λ ij (f i , f j )[l = l ],</formula><p>where [.] is the Iverson bracket (1 iff the condition in the square bracket is satisfied and 0 otherwise) and f i , f j are the 3D features extracted from data D at the i th and j th voxels (respectively).</p><p>In the 2D segmentation domain, the cost λ ij of assigning different labels to neighbouring pixels is generally chosen such that it preserves image edges. Inspired by these edgepreserving smoothness costs, we make λ ij a weighted combination of Gaussian kernels (with unit covariance matrix) that depend on appearance and depth features:</p><formula xml:id="formula_4">λ ij = M m=1 θ m λ m ij (f i , f j ) = θ m p e -pi-pj 2 2 + θ m a e -ai-aj 2 2 + θ m n e -ni-nj 2 2<label>(4)</label></formula><p>Here, p i , a i and n i are respectively the 3D world coordinate position, RGB appearance, and surface normal vector of the reconstructed surface at voxel i, and θ p , θ a and θ n are parameters obtained by cross-validation. Note that surface normals are calculated using the TSDF values <ref type="bibr" target="#b19">[20]</ref>. In general, we obtain high-quality normals (see Fig. <ref type="figure" target="#fig_2">4</ref>), which helps in achieving very smooth output.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Efficient Mean-Field Inference</head><p>One of the most popular approaches for multi-label CRF inference has been graph-cuts based α-expansion <ref type="bibr" target="#b30">[31]</ref>, which finds the maximum a posteriori (MAP) solution. However, graph-cuts leads to slow inference and is not easily parallelisable. Given the form of the energy function defined above, we follow the mean-field based optimization method, a filter-based variant that has been shown to be very efficient for densely-connected CRFs in 2D image segmentation <ref type="bibr" target="#b31">[32]</ref>, <ref type="bibr" target="#b32">[33]</ref>.</p><p>In the mean-field framework, we approximate the true distribution P (X) by a family of Q(X) distributions that factorize as the product of all components' marginals (components are independent) Q(X) = i Q i (x i ). The meanfield inference then attempts to minimize the KL-divergence D KL (Q||P ) between the tractable distribution Q and true distribution P . Under this assumption, the fixed point solution of the KL-divergence leads to the following mean-field update for all j = i (refer to <ref type="bibr" target="#b33">[34]</ref> for more details):</p><formula xml:id="formula_5">Q i (X i = l) = 1 Z i exp{-ψ u (X i ) - l ∈L j =i Q j (X j = l )ψ p (X i , X j )}<label>(5)</label></formula><p>where</p><formula xml:id="formula_6">Z i = Xi=l∈L exp{-ψ u (X i ) -l ∈L j =i Q j ( X j = l )ψ p (X i , X j )</formula><p>} is a constant normalizing the marginal at voxel i. The complexity of the mean-field update for the volumetric data is O(N 2 ).</p><p>Next, we discuss our online volumetric mean-field approach, which has been adapted from the 2D filteringbased mean-field approach we described above. Although this online mean-field approach has previously been applied in 2D <ref type="bibr" target="#b34">[35]</ref>, we believe this is the first time it has been applied in a 3D setting.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Volumetric filtering-based mean-field</head><p>The most time-consuming step in the mean-field inference is the pairwise update, whose complexity is O(N 2 ). Now we will discuss how we reduce this complexity to O(N ) for pairwise potentials taking the form of a weighted combination of Gaussian kernels. Our work is motivated by <ref type="bibr" target="#b31">[32]</ref>, <ref type="bibr" target="#b32">[33]</ref>, who show that fast approximate MPM inference can be achieved by applying cross bilateral filtering techniques.</p><p>First, we show why the mean-field update from Eq. 5 can be interpreted as filtering. To this end, we apply the transformation <ref type="bibr" target="#b5">(6)</ref> in which G m is the Gaussian kernel corresponding to the m th component and ⊗ is the convolution operator. Since j =i Q j (x j = l )ψ p (x i , x j ) can be written as m w (m) Q(m) i (l ), and approximate Gaussian convolution is O(N ), parallel updates can be efficiently approximated in O(M N L) time for the Potts model. The algorithm is run for a fixed number of iterations, and the MPM solution extracted by choosing X i ∈ argmax l Q i (x i = l) from soft predictions at the final iteration. We use high-dimensional filtering on the 3D volumetric data, where the filtering is a simple extension of the 2D permutohedral lattice-based filtering shown in <ref type="bibr" target="#b31">[32]</ref> to 3D.</p><formula xml:id="formula_7">Q(m) i (l) = j =i λ m (f i , f j )Q j (l) = [G m ⊗ Q(l)](f i ) -Q i (l),</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D. Online mean-field</head><p>Given unlimited computation, one might run multiple update iterations until convergence. However, in our online system, we assume that the next frame's updates to the volume (and thus to the energy function) are not too radical, and so we can make the assumption that the Q i distributions can be temporally propagated from one frame to the next, rather than re-initialized (e.g. to uniform) at each frame. Thus, running even a single iteration of mean-field updates per frame effectively allows us to amortize an otherwise expensive inference operation over multiple frames and maintain real-time speeds.  As described above, the output of the classifier responses is used to update the unary potentials, which will, over several frames, impact the final segmentation that results from the online mean-field inference. However, to speed up convergence, rather than simply propagating the Q t-1 i s from the previous frame, we instead provide the next iteration of mean-field updates with a weighted combination of Q t-1 i and the classifier prediction P u (x i = l | D). We thus use</p><formula xml:id="formula_8">Qt-1 i (l) = γQ t-1 i (l) + (1 -γ)P u (X i = l | D) (7) in place of Q t-1 i</formula><p>, where γ is a weighting parameter.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>VI. EXPERIMENTS</head><p>We demonstrate the effectiveness of our approach for both 3D semantic segmentation and reconstruction. We evaluate our system on the KITTI dataset <ref type="bibr" target="#b35">[36]</ref>, which contains a variety of outdoor sequences, including a city, road and campus. All sequences were captured at a resolution of 1241×376 pixels using stereo cameras (with baseline 0.54m) mounted on the roof of a car. The car was also equipped with a Velodyne HDL-64E laser scanner (LIDAR). The KITTI dataset is very challenging since it contains many moving objects such as cars, pedestrians and bikes, and numerous changes in lighting conditions.</p><p>For both voxel labelling and reconstruction, we show our results on both static and dynamic scenes. This enables us to properly evaluate how well our approach handles motion. For static scenes, we used the dataset of Sengupta et al. <ref type="bibr" target="#b3">[4]</ref>, which consists of 45 training and 25 test images that are labelled with the following classes: road, building, vehicle, pedestrian, pavement, tree, sky, signage, Fig. <ref type="figure">7:</ref> A high-quality mesh recovered from the long (1000 images) sequence 5 of the KITTI odometry dataset, superimposed over the corresponding Google Earth image. This shows the ability of our method to reconstruct and label large scenes. See Fig. <ref type="figure">1</ref> for colour coding.</p><p>post/pole and wall/fence. For dynamic scenes, we manually annotated sequences from the KITTI dataset that contained many moving objects. We compare the timings and accuracy achieved by our voxel-labelling approach against two baselines, Ladický et al. <ref type="bibr" target="#b29">[30]</ref> and Sengupta et al. <ref type="bibr" target="#b3">[4]</ref>. To evaluate our reconstruction results, we compare them with the depth data generated using Geiger et al.'s approach <ref type="bibr" target="#b24">[25]</ref>, using LIDAR data from the Velodyne scanner as ground truth.</p><p>To perform qualitative and quantitative evaluation, we backproject the voxel labels and reconstructed surfaces onto the camera's image plane, ignoring those that are farther than 25 metres from the camera. A. Qualitative KITTI Results First, we show some qualitative results for our semantic reconstruction approach. In Fig. <ref type="figure" target="#fig_3">5</ref>, we highlight the ability of our approach not only to reconstruct and label entire outdoor scenes that include roads, pavements and buildings, but also to accurately recover thin objects such as lamp posts and trees. In Fig. <ref type="figure" target="#fig_4">6</ref>, we show the advantages of our semantic fusion approach in handling moving objects (in this case, a car). Note in particular that with semantic fusion turned on, the static scene is far less corrupted by moving objects than it would be otherwise. Fig. <ref type="figure">7</ref> shows a high-quality mesh recovered from a long KITTI sequence (1000 images), superimposed over the corresponding Google Earth image. This shows the ability of our method to reconstruct and label large scenes. In Fig. <ref type="figure">8</ref>, we show a close-up view of a semantic model produced using our method, in which the arrows indicate the image locations and their corresponding positions in the 3D model, and colours indicate the object labels. This shows that even though our approach is an incremental one, we are able to achieve smooth surfaces for outdoor scenes.</p><p>TABLE II: Quantitative results for our segmantic segmentation approach on the KITTI dataset. We compare global accuracy and intersection/union on both (a) static and (b) moving scenes. For static scenes, we compare our approach without semantic fusion [Ours <ref type="bibr" target="#b0">(1)</ref>] against the state-of-the-art approaches of Ladický et al. <ref type="bibr" target="#b29">[30]</ref> and Sengupta et al. <ref type="bibr" target="#b3">[4]</ref>. For moving scenes, we compare our approach with semantic fusion [Ours <ref type="bibr" target="#b1">(2)</ref>] against [Ours(1)] and <ref type="bibr" target="#b29">[30]</ref> Fig. <ref type="figure">8</ref>: A close-up view of a semantic model produced using our method, in which the arrows indicate the image locations and their corresponding positions in the 3D model, and colours indicate the object labels. This shows that even though our approach is an incremental one, we are able to achieve smooth surfaces for outdoor scenes. See Fig. <ref type="figure">1</ref> for colour coding.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Quantitative KITTI Results</head><p>Semantic Segmentation: Next, we quantitatively evaluate the speed and accuracy of our mean-field-based volumetric labelling approach. Mean-field updates take roughly 20ms. Although the timings change as a function e.g. of the number of visible voxels, in all tests we performed we observed real-time performance. We assess the overall percentage of correctly-labelled voxels (global accuracy) and the intersection/union (I/U) score defined in terms of the true/false positives/negatives for a given class, i.e. TP/(TP+FP+FN).</p><p>Quantitative results for static scenes are shown in Tab. II(a). In comparison to the 2D approach of Ladický et al. <ref type="bibr" target="#b29">[30]</ref>, we achieve a 0.49% improvement in global accuracy and a 0.84% improvement in I/U score. We also significantly improve upon the 3D approach of Sengupta et al. <ref type="bibr" target="#b3">[4]</ref>, achieving a 10.8% improvement in global accuracy and a 6.7% improvement in I/U. More importantly, our approach achieves encouraging improvements in global accuracy and I/U for thin objects (e.g. poles).</p><p>In Tab. II(b), we evaluate the accuracy of our labellings on sequences containing many moving cars. We observe that our non-semantic fusion approach reduces accuracy by over 10% in comparison to <ref type="bibr" target="#b29">[30]</ref>; however, our semantic fusion approach improves overall accuracy by 1.5%. For cars, we observe an improvement of 2.2% in global accuracy and 5.5% in I/U. Note that our semantic fusion approach significantly improves both the global accuracy and I/U of our method, in both cases by over 10%. The improvements for cars are even more significant, highlighting the importance of using semantic fusion for scenes containing moving objects. Reconstruction: Next, we quantitatively evaluate the efficiency and accuracy of our reconstruction approach. Camera tracking takes roughly 20ms, stereo estimation takes around 40ms (on our 12 core systems) and fusion takes 14ms. In order to evaluate accuracy, we follow the approach of Sengupta et al. <ref type="bibr" target="#b3">[4]</ref>, who measure the number of pixels whose distance (in terms of depth) from the ground truth (in our case the Velodyne data) after projection to the image plane is less than a fixed threshold.</p><p>Quantitative results for depth evaluation are summarised in Fig. <ref type="figure" target="#fig_5">9</ref> for both static and dynamic scenes. We observe that for static scenes, our non-semantic fusion approach itself achieves almost 90% and 95% accuracy when the thresholds are 1m and 4m respectively. We therefore achieve an improvement of almost 20% over the initial depth estimated using the stereo output from Geiger et al.'s approach <ref type="bibr" target="#b24">[25]</ref>. However, for sequences in which there are many moving objects, non-semantic fusion does not perform that well and leads to a decrease in accuracy of almost 5% compared to Geiger et al.'s method. By contrast, our semantic fusion approach achieves an almost 5% improvement in accuracy.</p><p>We would like to highlight that the real-time aspect of our semantic reconstruction pipeline does not include the feature evaluation time. However, features can be implemented on GPU to provide real-time performance, as shown in <ref type="bibr" target="#b36">[37]</ref>. C. Other Qualitative Results Finally, we show additional qualitative results on four new, challenging sequences that we captured using a headmounted stereo camera. Fig. <ref type="figure" target="#fig_6">10</ref> shows the final smooth semantic reconstructions obtained by running our mean-field inference procedure. The images clearly indicate the sharp boundaries that we manage to achieve between different conflicting semantic classes. For example, observe the extremely accurate boundary between the pavement and the road in the sequence in the third column. More results are provided in the supplementary video. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>VII. CONCLUSION</head><p>We have presented a robust and accurate approach for incremental dense large-scale semantic reconstruction of outdoor environments in real time from a stereo camera. At the core of our algorithm is a hash-based fusion approach for 3D reconstruction and a volumetric mean-field inference approach for object labelling. By performing reconstruction and recognition in tandem, we capture the synergy between the two tasks. By harnessing the processing power of modern GPUs, we can perform semantic reconstruction at real-time rates, even for large-scale environments. We have demonstrated our system's effectiveness for both high-quality dense reconstruction and scene labelling on the KITTI dataset.</p><p>Our paper offers many interesting avenues for further work. One area that we would like to explore is the enforcement of object-specific shape priors for 3D reconstruction. Currently, feature generation and learning of the class models have been done in an offline fashion. We would like to implement the online aspects of these tasks on GPU.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 2 :</head><label>2</label><figDesc>Fig. 2: Overview of our system: (a) given stereo image pairs, we (b) generate depth and (c) estimate 6 DoF camera pose using visual odometry in parallel.Next, we (d) fuse the depth into a common 3D map. We also (e) extract features, (f) evaluate unary potentials for each voxel and (g) perform inference over a densely-connected pairwise 3D random field to generate a high-quality labelling, which (h) controls fusion weights.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 3 :</head><label>3</label><figDesc>Fig. 3: Labelled mesh (output of our algorithm) for sequence 95 from the KITTI residential dataset, consisting of 268 stereo pairs. The close-up views show snapshots of the scene at several places along the route. See Fig. 1 for colour coding.</figDesc><graphic coords="4,104.40,50.08,403.23,145.83" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 4 :</head><label>4</label><figDesc>Fig.4: An example of the normals we generate from the TSDF surfaces. These provide a lot of information about surface orientation and curvature that we use in pairwise potentials.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Fig. 5 :</head><label>5</label><figDesc>Fig.5: Our approach not only reconstructs and labels entire outdoor scenes that include roads, pavements and buildings, but also accurately recovers thin objects such as lamp posts and trees. See Fig.1for colour coding.</figDesc><graphic coords="6,54.00,50.08,244.81,79.04" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Fig. 6 :</head><label>6</label><figDesc>Fig. 6: Our semantic fusion technique enables us to avoid corrupting a static scene with data from moving objects. First row: input image; second row: reconstructed scene without semantic fusion; third row: reconstructed scene with semantic fusion. Note the way in which semantic fusion helps suppress the trail of spurious voxels that moving objects would normally leave behind. See Fig. 1 for colour coding.</figDesc><graphic coords="6,54.00,168.66,244.81,123.10" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Fig. 9 :</head><label>9</label><figDesc>Fig. 9: Quantitative results for depth evaluation for static (left) and moving (right) scenes.</figDesc><graphic coords="7,54.00,191.94,244.81,123.11" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Fig. 10 :</head><label>10</label><figDesc>Fig. 10: Final labelling surfaces for four reconstructed sequences (the last two columns belong to the same sequence). See Fig. 1 for colour coding.</figDesc><graphic coords="8,61.94,124.09,90.72,68.34" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>and M. Nießner are with Stanford, California, US. O. Miksik, M. Lidegaard, S. Golodetz, V. Prisacariu, O. Kähler, D. Murray and P. Torr are with the University of Oxford, UK.</figDesc><table /><note><p>S. Izadi is with Microsoft Research, Redmond, Washington, US. P. Pérez is with Technicolor R&amp;I, Cesson Sévigné, FR. This research was supported by Technicolor, EPSRC, the Leverhulme Trust and the ERC grant ERC-2012-AdG 321162-HELIOS. SG was funded via a Royal Society Brian Mercer Award for Innovation awarded to S. Hicks.</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>TABLE I</head><label>I</label><figDesc></figDesc><table><row><cell cols="8">: Comparison with some related work: O = outdoor, C = camera</cell></row><row><cell cols="8">only, I = incremental, SDT = sparse data structures, S = host-device</cell></row><row><cell cols="5">streaming, RT = real-time, MV = moving objects</cell><cell></cell><cell></cell><cell></cell></row><row><cell>Method</cell><cell>O</cell><cell>C</cell><cell>I</cell><cell>SDT</cell><cell>S</cell><cell>RT</cell><cell>MV</cell></row><row><cell>Sengupta et al. [4]</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>out only</cell><cell></cell><cell></cell></row><row><cell>Valentin et al. [12]</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Häne et al. [8]</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>N/A</cell><cell></cell></row><row><cell>Kundu et al. [11]</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Hermans et al. [9]</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Hu et al. [27]</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Ours</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head></head><label></label><figDesc>.</figDesc><table><row><cell></cell><cell></cell><cell></cell><cell>(a) Static</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>(b) Moving</cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell cols="3">Global Accuracy</cell><cell cols="3">Intersection/Union</cell><cell></cell><cell></cell><cell cols="2">Global Accuracy</cell><cell cols="3">Intersection/Union</cell></row><row><cell>Class</cell><cell>[30]</cell><cell>[4]</cell><cell>Ours(1)</cell><cell>[30]</cell><cell>[4]</cell><cell>Ours(1)</cell><cell>Class</cell><cell>[30]</cell><cell cols="2">Ours(1) Ours(2)</cell><cell>[30]</cell><cell cols="2">Ours(1) Ours(2)</cell></row><row><cell>building</cell><cell cols="2">97.0 96.1</cell><cell>97.2</cell><cell cols="2">86.1 83.8</cell><cell>88.3</cell><cell>building</cell><cell>90.9</cell><cell>89.1</cell><cell>93.1</cell><cell>82.1</cell><cell>81.9</cell><cell>82.7</cell></row><row><cell>vegetation</cell><cell cols="2">93.4 86.9</cell><cell>94.1</cell><cell cols="2">82.8 74.3</cell><cell>83.2</cell><cell>vegetation</cell><cell>89.2</cell><cell>66.9</cell><cell>92.1</cell><cell>77.6</cell><cell>64.3</cell><cell>79.0</cell></row><row><cell>car</cell><cell cols="2">93.9 88.5</cell><cell>94.1</cell><cell cols="2">78.0 63.5</cell><cell>79.5</cell><cell>car</cell><cell>92.1</cell><cell>78.5</cell><cell>94.3</cell><cell>72.0</cell><cell>56.4</cell><cell>77.5</cell></row><row><cell>road</cell><cell cols="2">98.3 97.8</cell><cell>98.7</cell><cell>94.3</cell><cell>96.3</cell><cell>94.7</cell><cell>road</cell><cell>98.6</cell><cell>87.8</cell><cell>97.7</cell><cell>91.3</cell><cell>86.3</cell><cell>92.1</cell></row><row><cell>wall</cell><cell>48.5</cell><cell>46.1</cell><cell>47.8</cell><cell>47.5</cell><cell>45.2</cell><cell>46.3</cell><cell>wall</cell><cell>46.7</cell><cell>42.1</cell><cell>48.1</cell><cell>49.5</cell><cell>42.2</cell><cell>50.3</cell></row><row><cell>pavement</cell><cell cols="2">91.3 46.1</cell><cell>91.8</cell><cell cols="2">73.4 68.4</cell><cell>73.8</cell><cell>pavement</cell><cell>93.3</cell><cell>84.5</cell><cell>94.8</cell><cell>72.4</cell><cell>63.4</cell><cell>75.8</cell></row><row><cell>pole</cell><cell cols="2">49.3 38.2</cell><cell>51.4</cell><cell cols="2">39.5 28.9</cell><cell>41.7</cell><cell>pole</cell><cell>46.2</cell><cell>36.7</cell><cell>47.4</cell><cell>34.1</cell><cell>24.6</cell><cell>36.7</cell></row><row><cell>Average</cell><cell cols="2">81.7 71.4</cell><cell>82.2</cell><cell cols="2">71.7 65.8</cell><cell>72.5</cell><cell>Average</cell><cell>79.6</cell><cell>69.4</cell><cell>81.1</cell><cell>68.4</cell><cell>59.9</cell><cell>70.6</cell></row></table></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Selfsupervised Monocular Road Detection in Desert Terrain</title>
		<author>
			<persName><forename type="first">H</forename><surname>Dahlkamp</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Kaehler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Stavens</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Thrun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Bradski</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Robotics: Science and Systems</title>
		<imprint>
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Autonomous driving in urban environments: Boss and the urban challenge</title>
		<author>
			<persName><forename type="first">C</forename><surname>Urmson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">JFR</title>
		<imprint>
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Automatic Dense Visual Semantic Mapping from Street-Level Imagery</title>
		<author>
			<persName><forename type="first">S</forename><surname>Sengupta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Sturgess</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Ladický</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">H S</forename><surname>Torr</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IROS</title>
		<imprint>
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Urban 3D Semantic Modelling Using Stereo Vision</title>
		<author>
			<persName><forename type="first">S</forename><surname>Sengupta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Greveson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Shahrokni</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">H S</forename><surname>Torr</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ICRA</title>
		<imprint>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">ATAP Project Tango Google</title>
		<author>
			<persName><surname>Google</surname></persName>
		</author>
		<ptr target="http://www.google.com/atap/projecttango/" />
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">A Depth-Based Head-Mounted Visual Display to Aid Navigation in Partially Sighted Individuals</title>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">L</forename><surname>Hicks</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Wilson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Muhammed</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Worsfold</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">M</forename><surname>Downes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Kennard</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">PLoS ONE</title>
		<imprint>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">City-Scale Change Detection in Cadastral 3D Models using Images</title>
		<author>
			<persName><forename type="first">A</forename><surname>Taneja</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Ballan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Pollefeys</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Joint 3D Scene Reconstruction and Class Segmentation</title>
		<author>
			<persName><forename type="first">C</forename><surname>Häne</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Zach</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Cohen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Angst</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Pollefeys</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Dense 3D Semantic Mapping of Indoor Scenes from RGB-D Images</title>
		<author>
			<persName><forename type="first">A</forename><surname>Hermans</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Floros</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Leibe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ICRA</title>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Semantic Labeling of 3D Point Clouds for Indoor Scenes</title>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">S</forename><surname>Koppula</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Anand</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Joachims</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Saxena</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">NIPS</title>
		<imprint>
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Joint Semantic Segmentation and 3D Reconstruction from Monocular Video</title>
		<author>
			<persName><forename type="first">A</forename><surname>Kundu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Dellaert</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">M</forename><surname>Rehg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Mesh Based Semantic Modelling for Indoor and Outdoor Scenes</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">P C</forename><surname>Valentin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Sengupta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Warrell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Shahrokni</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">H S</forename><surname>Torr</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Building Rome in a Day</title>
		<author>
			<persName><forename type="first">S</forename><surname>Agarwal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Furukawa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Snavely</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Simon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Curless</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">M</forename><surname>Seitz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Szeliski</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">CACM</title>
		<imprint>
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">MonoSLAM: Real-Time Single Camera SLAM</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">J</forename><surname>Davison</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><forename type="middle">D</forename><surname>Reid</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><forename type="middle">D</forename><surname>Molton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Stasse</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">PAMI</title>
		<imprint>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="issue">6</biblScope>
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Parallel Tracking and Mapping for Small AR Workspaces</title>
		<author>
			<persName><forename type="first">G</forename><surname>Klein</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Murray</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ISMAR</title>
		<imprint>
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">Visual Odometry and Mapping for Autonomous Flight Using an RGB-D Camera</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">S</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Bachrach</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Henry</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Krainin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Maturana</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Fox</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Roy</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
	<note>in ISRR</note>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">SVO: Fast Semi-Direct Monocular Visual Odometry</title>
		<author>
			<persName><forename type="first">C</forename><surname>Forster</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Pizzoli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Scaramuzza</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ICRA</title>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">Real-time dense geometry from a handheld camera</title>
		<author>
			<persName><forename type="first">J</forename><surname>Stühmer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Gumhold</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Cremers</surname></persName>
		</author>
		<editor>DAGM</editor>
		<imprint>
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">DTAM: Dense Tracking and Mapping in Real-Time</title>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">A</forename><surname>Newcombe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">J</forename><surname>Lovegrove</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">J</forename><surname>Davison</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">KinectFusion: Real-Time Dense Surface Mapping and Tracking</title>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">A</forename><surname>Newcombe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Izadi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Hilliges</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Molyneaux</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">J</forename><surname>Davison</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Kohli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Shotton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Hodges</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Fitzgibbon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ISMAR</title>
		<imprint>
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Scalable Real-time Volumetric Surface Reconstruction</title>
		<author>
			<persName><forename type="first">J</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Bautembach</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Izadi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">TOG</title>
		<imprint>
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="issue">4</biblScope>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Real-time 3D Reconstruction at Scale using Voxel Hashing</title>
		<author>
			<persName><forename type="first">M</forename><surname>Nießner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Zollhöfer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Izadi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Stamminger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">TOG</title>
		<imprint>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Joint 2D-3D Temporally Consistent Semantic Segmentation of Street Scenes</title>
		<author>
			<persName><forename type="first">G</forename><surname>Floros</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Leibe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Contextual Classification with Functional Max-Margin Markov Networks</title>
		<author>
			<persName><forename type="first">D</forename><surname>Munoz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">A</forename><surname>Bagnell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Vandapel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Hebert</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Efficient Large-Scale Stereo Matching</title>
		<author>
			<persName><forename type="first">A</forename><surname>Geiger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Roser</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Urtasun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACCV</title>
		<imprint>
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">LSD-SLAM: Large-Scale Direct Monocular SLAM</title>
		<author>
			<persName><forename type="first">J</forename><surname>Engel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Schöps</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Cremers</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Efficient 3-d scene analysis from streaming data</title>
		<author>
			<persName><forename type="first">H</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Munoz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">A</forename><surname>Bagnell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Hebert</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ICRA</title>
		<imprint>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">A Volumetric Method for Building Complex Models from Range Images</title>
		<author>
			<persName><forename type="first">B</forename><surname>Curless</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Levoy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SIGGRAPH</title>
		<imprint>
			<date type="published" when="1996">1996</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Semantic texton forests for image categorization and segmentation</title>
		<author>
			<persName><forename type="first">J</forename><surname>Shotton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Cipolla</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2008 IEEE CVPR</title>
		<meeting><address><addrLine>Anchorage, Alaska, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2008-06">June 2008. 2008</date>
			<biblScope unit="page" from="24" to="26" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Associative Hierarchical Random Fields</title>
		<author>
			<persName><forename type="first">L</forename><surname>Ladický</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Russell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Kohli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">H S</forename><surname>Torr</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">PAMI</title>
		<imprint>
			<biblScope unit="volume">36</biblScope>
			<biblScope unit="issue">6</biblScope>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Fast Approximate Energy Minimization via Graph Cuts</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Boykov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Veksler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Zabih</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">PAMI</title>
		<imprint>
			<biblScope unit="volume">23</biblScope>
			<biblScope unit="issue">11</biblScope>
			<date type="published" when="2001">2001</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Efficient Inference in Fully Connected CRFs with Gaussian Edge Potentials</title>
		<author>
			<persName><forename type="first">P</forename><surname>Krähenbühl</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Koltun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">NIPS</title>
		<imprint>
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Filter-based Mean-Field Inference for Random Fields with Higher-Order Terms and Product Label-Spaces</title>
		<author>
			<persName><forename type="first">V</forename><surname>Vineet</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Warrell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">H S</forename><surname>Torr</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
		<title level="m" type="main">Probabilistic Graphical Models -Principles and Techniques</title>
		<author>
			<persName><forename type="first">D</forename><surname>Koller</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Friedman</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2009">2009</date>
			<publisher>MIT Press</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<monogr>
		<title level="m" type="main">Mean field approach for tracking similar objects</title>
		<author>
			<persName><forename type="first">C</forename><surname>Medrano</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Herrero</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Martnez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Orrite</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2009">2009</date>
		</imprint>
		<respStmt>
			<orgName>CVIU</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Are we ready for Autonomous Driving? The KITTI Vision Benchmark Suite</title>
		<author>
			<persName><forename type="first">A</forename><surname>Geiger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Lenz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Urtasun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<monogr>
		<title level="m" type="main">fasthog -a real-time gpu implementation of hog</title>
		<author>
			<persName><forename type="first">V</forename><forename type="middle">A</forename><surname>Prisacariu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Reid</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2009">2009</date>
		</imprint>
		<respStmt>
			<orgName>University of Oxford, Tech</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">Report</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
