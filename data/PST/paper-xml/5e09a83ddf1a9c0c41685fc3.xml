<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Bridging the Gap Between Relevance Matching and Semantic Matching for Short Text Similarity Modeling</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author role="corresp">
							<persName><forename type="first">Jinfeng</forename><surname>Rao</surname></persName>
							<email>raojinfeng@fb.com</email>
						</author>
						<author>
							<persName><forename type="first">Linqing</forename><surname>Liu</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">David R. Cheriton School of Computer Science</orgName>
								<orgName type="institution">University of Waterloo</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Yi</forename><surname>Tay</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution">Nanyang Technological University</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Wei</forename><surname>Yang</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">David R. Cheriton School of Computer Science</orgName>
								<orgName type="institution">University of Waterloo</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Peng</forename><surname>Shi</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">David R. Cheriton School of Computer Science</orgName>
								<orgName type="institution">University of Waterloo</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Jimmy</forename><surname>Lin</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">David R. Cheriton School of Computer Science</orgName>
								<orgName type="institution">University of Waterloo</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Facebook</forename><surname>Assistant</surname></persName>
						</author>
						<title level="a" type="main">Bridging the Gap Between Relevance Matching and Semantic Matching for Short Text Similarity Modeling</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.2" ident="GROBID" when="2022-12-25T13:15+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>A core problem of information retrieval (IR) is relevance matching, which is to rank documents by relevance to a user's query. On the other hand, many NLP problems, such as question answering and paraphrase identification, can be considered variants of semantic matching, which is to measure the semantic distance between two pieces of short texts. While at a high level both relevance and semantic matching require modeling textual similarity, many existing techniques for one cannot be easily adapted to the other. To bridge this gap, we propose a novel model, HCAN (Hybrid Co-Attention Network), that comprises (1) a hybrid encoder module that includes ConvNet-based and LSTM-based encoders, (2) a relevance matching module that measures soft term matches with importance weighting at multiple granularities, and (3) a semantic matching module with co-attention mechanisms that capture context-aware semantic relatedness. Evaluations on multiple IR and NLP benchmarks demonstrate state-ofthe-art effectiveness compared to approaches that do not exploit pretraining on external data. Extensive ablation studies suggest that relevance and semantic matching signals are complementary across many problem settings, regardless of the choice of underlying encoders.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Neural networks have achieved great success in many NLP tasks, such as question answering <ref type="bibr" target="#b22">(Rao et al., 2016;</ref><ref type="bibr" target="#b2">Chen et al., 2017a)</ref>, paraphrase detection <ref type="bibr" target="#b34">(Wang et al., 2017)</ref>, and textual semantic similarity modeling <ref type="bibr" target="#b9">(He and Lin, 2016)</ref>. Many of these tasks can be treated as variants of a semantic matching (SM) problem, where two pieces of texts are jointly modeled through distributed representations for similarity learning. Various neural network architectures, e.g., Siamese networks <ref type="bibr" target="#b9">(He et al., 2016)</ref> and attention <ref type="bibr" target="#b26">(Seo et al., 2017;</ref><ref type="bibr" target="#b31">Tay et al., 2019b)</ref>, have been proposed to model semantic similarity using diverse techniques.</p><p>A core problem of information retrieval (IR) is relevance matching (RM), where the goal is to rank documents by relevance to a user's query. Though at a high level semantic and relevance matching both require modeling similarities in pairs of texts, there are fundamental differences. Semantic matching emphasizes "meaning" correspondences by exploiting lexical information (e.g., words, phrases, entities) and compositional structures (e.g., dependency trees), while relevance matching focuses on keyword matching. It has been observed that existing approaches for textual similarity modeling in NLP can produce poor results for IR tasks <ref type="bibr" target="#b8">(Guo et al., 2016)</ref>, and vice versa <ref type="bibr" target="#b10">(Htut et al., 2018)</ref>.</p><p>Specifically, <ref type="bibr" target="#b8">Guo et al. (2016)</ref> point out three distinguishing characteristics of relevance matching: exact match signals, query term importance, and diverse matching requirements. In particular, exact match signals play a critical role in relevance matching, more so than the role of term matching in, for example, paraphrase detection. Furthermore, in document ranking there is an asymmetry between queries and documents in terms of length and the richness of signals that can be extracted; thus, symmetric models such as Siamese architectures may not be entirely appropriate.</p><p>To better demonstrate these differences, we present examples from relevance and semantic matching tasks in Table <ref type="table">1</ref>. Column 'Label' denotes whether sentence A and B are relevant or duplicate. The first example from tweet search shares many common keywords and is identified as relevant, while the second pair from Quora shares all words except for the subject and is not considered a duplicate pair. An approach based on keyword matching alone is unlikely to be able to distinguish is worth $469,000 .</p><p>Table <ref type="table">1</ref>: Sample sentence pairs from TREC <ref type="bibr">Microblog 2013, Quora, and TrecQA.</ref> between these cases. In contrast, the third example is judged as a relevant QA pair because different terms convey similar semantics. These divergences motivate different architectural choices. Since relevance matching is fundamentally a matching task, most recent neural architectures, such as DRMM <ref type="bibr" target="#b8">(Guo et al., 2016)</ref> and Co-PACRR <ref type="bibr" target="#b13">(Hui et al., 2018)</ref>, adopt an interaction-based design. They operate directly on the similarity matrix obtained from products of query and document embeddings and build sophisticated modules on top to capture additional n-gram matching and term importance signals. On the other hand, many NLP problems, such as question answering and textual similarity measurement, require more semantic understanding and contextual reasoning rather than specific term matches. Context-aware representation learning, such as co-attention methods <ref type="bibr" target="#b26">(Seo et al., 2017)</ref>, has been proved effective in many benchmarks. Though improvements have been shown from adding exact match signals into representation learning, for example, the Dr.QA model of <ref type="bibr" target="#b2">Chen et al. (2017a)</ref> concatenates exact match scores to word embeddings, it remains unclear to what extent relevance matching signals can further improve models primarily designed for semantic matching.</p><p>To this end, we examine two research questions: (1) Can existing approaches to relevance matching and semantic matching be easily adapted to the other? (2) Are signals from relevance and semantic matching complementary? We present a novel neural ranking approach to jointly model both the relevance matching process and the semantic matching process. Our model, HCAN (Hybrid Co-Attention Network), comprises three major components:</p><p>1. A hybrid encoder module that explores three types of encoders: deep, wide, and contextual, to obtain contextual sentence representations.</p><p>2. A relevance matching module that measures soft term matches with term weightings between pairs of texts, starting from word-level to phrase-level, and finally to sentence-level.</p><p>3. A semantic matching module with co-attention mechanisms applied at each encoder layer to enable context-aware representation learning at multiple semantic levels.</p><p>Finally, all relevance and semantic matching signals are integrated using a fully-connected layer to yield the final classification score.</p><p>Contributions. We see our work as making the following contributions:</p><p>• We highlight and systematically explore important differences between relevance matching and semantic matching on short texts, which lie at the core of many of IR and NLP problems.</p><p>• We propose a novel model, HCAN (Hybrid Co-Attention Network), to combine best practices in neural modeling for both relevance and semantic matching.</p><p>• Evaluations on multiple IR and NLP tasks, including answer selection, paraphrase identification, semantic similarity measurement, and tweet search, demonstrate state-of-the-art effectiveness compared to approaches that do not exploit pretraining on external data. Ablation studies show that relevance and semantic matching signals are complementary in many problems, and combining them can be more data efficient.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">HCAN: Hybrid Co-Attention Network</head><p>The overview of our model is shown in Figure <ref type="figure">1</ref>. It is comprised of three major components: (1) a hybrid encoder module that explores three types of encoders: deep, wide, and contextual (Sec. 2.1); (2) a relevance matching module with external weights for learning soft term matching signals (Sec. 2.2); (3) a semantic matching module with co-attention mechanisms for context-aware representation learning (Sec. 2.3). Note that the relevance and semantic matching modules are applied at each encoder layer, and all signals are finally aggregated for classification.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Hybrid Encoders</head><p>Without loss of generality, we assume that inputs to our model are sentence pairs (q, c), where (q, c) can refer to a (query, document) pair in a search setting, a (question, answer) pair in a QA setting, etc. The query q and context c are denoted by their words, {w q 1 , w q 2 , ..., w q n } and {w c 1 , w c 2 , ..., w c m }, respectively, where n and m are the number of words in the query and the context. A word embedding layer converts both into their embedding representations Q ∈ R n×L and C ∈ R m×L , where L is the dimension of the embeddings.</p><p>To learn effective phrase-level representations, we explore three different types of encoders: deep, wide, and contextual, detailed below. Deep Encoder: This design consists of multiple convolutional layers stacked in a hierarchical manner to obtain higher-level k-gram representations. A convolutional layer applies convolutional filters to the text, which is represented by an embedding matrix U (Q or C). Each filter is moved through the input embedding incrementally as a sliding window (with window size k) to capture the compositional representation of k neighboring terms. Assuming a convolutional layer has F filters, this CNN layer (with padding) produces an output matrix</p><formula xml:id="formula_0">U o ∈ R U ×F .</formula><p>For notational simplicity, we drop the superscript o from all output matrices and add a superscript h to denote the output of the h-th convolutional layer. Stacking N CNN layers therefore corresponds to obtaining the output matrix of the h-th layer U h ∈ R U ×F h via:</p><formula xml:id="formula_1">U h = CNN h (U h−1 ), h = 1, . . . , N,</formula><p>where U h−1 is the output matrix of the (h − 1)-th convolutional layer. Note that U 0 = U denotes the input matrix (Q or C) obtained directly from the word embedding layer. The parameters of each CNN layer are shared by the query and the context. Wide Encoder: Unlike the deep encoder that stacks multiple convolutional layers hierarchically, the wide encoder organizes convolutional layers in parallel, with each convolutional layer having a different window size k to obtain the corresponding k-gram representations. Given N convolutional layers, the window sizes of the CNN layers will be in</p><formula xml:id="formula_2">[k, k + 1, ..., k + N − 1].</formula><p>Contextual Encoder: Different from both the deep and wide encoders that capture k-gram patterns with convolutions, the contextual encoder leverages Bi-directional LSTMs to extract longrange contextual features. Given N BiLSTM layers, the output at the h-th layer is computed as:</p><formula xml:id="formula_3">U h = BiLSTM h (U h−1 ), h = 1, . . . , N,</formula><p>The three encoders represent different tradeoffs. The deep and wide encoders are easier for performing inference in parallel and are much faster to train than the contextual encoder. Additionally, the use of CNN layers allows us to explicitly control the window size for phrase modeling, which has been shown to be critical for relevance matching <ref type="bibr" target="#b5">(Dai et al., 2018;</ref><ref type="bibr" target="#b25">Rao et al., 2019)</ref>. On the other hand, the contextual encoder enables us to obtain long-distance contextual representations for each token. Comparing the deep and wide encoders, the deep encoder saves more parameters by reusing representations from the previous layer. The effectiveness of each encoder is an empirical question we will experimentally answer.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Relevance Matching</head><p>This section describes our efforts to capture keyword matching signals for relevance matching. We calculate the relevance score between the query and the context at each encoder layer by multiplying the query representation matrix U q and the context representation matrix U c :</p><formula xml:id="formula_4">S = U q U c T , S ∈ R n×m ,</formula><p>where S i,j can be considered the similarity score by matching the query phrase vector U q [i] with the context phrase vector U c [j]. Since the query and the context share the same encoder layers, similar phrases will be placed closer in a highdimensional embedding space and their product will produce larger scores. Next, we obtain a normalized similarity matrix S by applying a softmax function over the context columns of S to normalize the similarity scores into the [0, 1] range.</p><p>For each query phrase i, the above softmax function normalizes its matching scores over all phrases in the context and helps discriminate matches with higher scores. An exact match will dominate others and contribute a similarity score close to 1.0. We then apply max and mean pooling to the similarity matrix to obtain discriminative feature vectors:</p><formula xml:id="formula_5">Max(S) = [max( S1,: ), ..., max( Sn,: )],</formula><p>Mean(S) = [mean( S1,: ), ..., mean( Sn,: )],</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Max(S), Mean(S) ∈ R n</head><p>Each score generated from pooling can be viewed as matching evidence for a specific query phrase in the context, where the value denotes the significance of the relevance signal. Compared to Max pooling, Mean pooling is beneficial for cases where a query phrase is matched to multiple relevant terms in the context.</p><p>It's worth noting that term importance modeling can be important for some search tasks <ref type="bibr" target="#b8">(Guo et al., 2016)</ref>; therefore, we inject external weights as priors to measure the relative importance of different query terms and phrases. We multiply the score after pooling with the weights of that specific query term/phrase. These are provided as feature inputs to the final classification layer, denoted by O RM :</p><formula xml:id="formula_6">O RM = {wgt(q) Max(S), wgt(q) Mean(S)}, O RM ∈ 2 • R n ,<label>(1)</label></formula><p>where is an element-wise product between the weights of the query terms/phrases with the pooling scores, and wgt(q) i denotes the weight of the i-th term/phrase in the query; its value changes in the intermediate encoder layers since deeper/wider encoder layers capture longer phrases. We choose inverse document frequency (IDF) as our weighting function. A higher IDF weight implies a rarer occurrence in the collection and thus greater discriminative power. The weighting method also allows us to reduce the impact of large matching scores for common words like stopwords.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Semantic Matching</head><p>In addition to relevance matching, we aim to capture semantic matching signals via co-attention mechanisms on intermediate query and context representations. Our semantic matching method behaves similarly to the transformer <ref type="bibr" target="#b32">(Vaswani et al., 2017)</ref>, which also uses attention (specifically, self-attention) over hierarchical blocks to capture semantics at different granularities.</p><p>Given U q ∈ R n×F and U c ∈ R m×F generated by an intermediate encoder layer, we first calculate the bilinear attention as follows:</p><formula xml:id="formula_7">A = REP(U q W q ) + REP(U c W c ) + U q W b U T c A =softmax col (A) A ∈ R n×m where W q , W c ∈ R F , W b ∈ R F ×F ,</formula><p>and the REP operator converts the input vector to a R n×m matrix by repeating elements in the missing dimen-sions. Softmax col is the column-wise softmax operator. Similar to <ref type="bibr" target="#b26">Seo et al. (2017)</ref>, we perform co-attention from two directions: query-to-context and context-to-query, as follows:</p><formula xml:id="formula_8">Ũq = A T U q Ũc = REP(max col (A)U c ) Ũq ∈ R m×F , Ũc ∈ R m×F</formula><p>where max col is the column-wise max-pooling operator. Ũq denotes query-aware context embeddings by attending the raw query representations to the attention weights, while Ũc indicates the weighted sum of the most important words in the context with respect to the query.</p><p>We then take an enhanced concatenation to explore the interaction between Ũq and Ũc , as in Equation <ref type="formula">2</ref>. Finally, we apply an additional Bi-LSTM to the concatenated contextual embeddings H to capture contextual dependencies in the sequence, and use the last hidden state (with dimension d) as the output features of the semantic matching module O SM :</p><formula xml:id="formula_9">H = [U c ; Ũq ; U c ⊗ Ũq ; Ũc ⊗ Ũq ] O SM = BiLSTM(H) H ∈ R m×4F , O SM ∈ R d (2)</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.4">Final Classification</head><p>Given the relevance and semantic matching features {O l RM , O l SM } (from Equations 1 and 2) learned at each encoder layer l, we concatenate them together and use a two-layer fully-connected layer with ReLU activation to generate the final prediction vector o. During training, we minimize the negative log likelihood loss L summed over all samples (o i , y i ) below:</p><formula xml:id="formula_10">o =softmax(MLP({O l RM , O l SM })), l =1, 2, ..., N and o ∈ R class L = − (o i ,y i ) log o i [y i ],</formula><p>where N is the number of encoder layers.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Experimental Setup</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Benchmarks and Metrics</head><p>We evaluated our proposed HCAN model on three NLP tasks and two IR datasets, as follows: Answer Selection. This task is to rank candidate answer sentences based on their similarity to the question. We use the TrecQA <ref type="bibr" target="#b33">(Wang et al., 2007)</ref> dataset (raw version)<ref type="foot" target="#foot_0">1</ref> with 56k question-answer pairs. We report mean average precision (MAP) and mean reciprocal rank (MRR). Paraphrase Identification. This task is to identify whether two sentences are paraphrases of each other. We use the TwitterURL <ref type="bibr" target="#b15">(Lan et al., 2017)</ref> dataset with 50k sentence pairs. We report the unweighted average of F1 scores on the positive and negative classes (macro-F1). Semantic Textual Similarity (STS). This task is to measure the degree of semantic equivalence between pairs of texts. We use the Quora <ref type="bibr" target="#b14">(Iyer et al., 2017)</ref> dataset with 400k question pairs collected from the Quora website. We report class prediction accuracy. Tweet Search. This task is to rank candidate tweets by relevance with respect to a short query. We use the TREC Microblog 2013-2014 datasets <ref type="bibr" target="#b17">(Lin and Efron, 2013;</ref><ref type="bibr" target="#b18">Lin et al., 2014)</ref>, as prepared by <ref type="bibr" target="#b25">Rao et al. (2019)</ref>, where each dataset contains around 50 queries and 40k query-tweet pairs. We report MAP and precision at rank 30 (P@30).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Baselines and Implementations</head><p>For the answer selection, paraphrase identification, and STS tasks, we compared against the following baselines: InferSent <ref type="bibr" target="#b4">(Conneau et al., 2017)</ref>, ESIM <ref type="bibr" target="#b3">(Chen et al., 2017b)</ref>, DecAtt <ref type="bibr" target="#b21">(Parikh et al., 2016)</ref>, and PWIM <ref type="bibr" target="#b9">(He and Lin, 2016)</ref>. Additionally, we report state-of-the-arts results on each dataset from published literature. We also include the current state-of-the-art BERT <ref type="bibr" target="#b6">(Devlin et al., 2019)</ref> results on each dataset.</p><p>For the tweet search task, we mostly follow the experimental setting in <ref type="bibr" target="#b25">Rao et al. (2019)</ref>. Baselines include the classic query likelihood (QL) method, RM3 query expansion <ref type="bibr" target="#b0">(Abdul-Jaleel et al., 2004)</ref>, learning to rank (L2R), as well as a number of neural ranking models: DRMM <ref type="bibr" target="#b8">(Guo et al., 2016)</ref>, <ref type="bibr">DUET (Mitra et al., 2017)</ref>, K-NRM <ref type="bibr" target="#b36">(Xiong et al., 2017b)</ref>, and PACRR <ref type="bibr" target="#b12">(Hui et al., 2017)</ref>. For the neural baselines, we used implementations in MatchZoo.<ref type="foot" target="#foot_1">2</ref> For L2R, we used LambdaMART (Burges, 2010) on the same feature sets as <ref type="bibr" target="#b25">Rao et al. (2019)</ref> In our experiments, we use trainable 300d word2vec <ref type="bibr" target="#b19">(Mikolov et al., 2013)</ref> embeddings with the SGD optimizer. For out-of-vocabulary words, we initialize their word embeddings with a uniform distribution from [0, 0.1]. Since our model also uses external weights, we calculate the IDF values from the training corpus. The number of convolutional layers N is set to 4, and the convolutional filter size k is set to 2. Hidden dimension d is set to 150. We tune the learning rate in [0.05, 0.02, 0.01], the number of convolutional filters F in <ref type="bibr">[128,</ref><ref type="bibr">256,</ref><ref type="bibr">512]</ref>, batch size in <ref type="bibr">[64,</ref><ref type="bibr">128,</ref><ref type="bibr">256]</ref>, and the dropout rate between 0.1 and 0.5. Our code and datasets are publicly available. 3</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Results</head><p>Our main results on the TrecQA, TwitterURL, and Quora datasets are shown in Table <ref type="table" target="#tab_2">2 and</ref>  In these experiments, we use the deep encoder.</p><p>From Table <ref type="table" target="#tab_2">2</ref>, we can see that on all three datasets, relevance matching (RM) achieves significantly higher effectiveness than semantic  In Table <ref type="table" target="#tab_4">3</ref>, we observe that the query expansion method (RM3) outperforms most of the neural ranking models except for BERT, which is consistent with <ref type="bibr" target="#b37">Yang et al. (2019a)</ref>. We suggest two reasons: (1) tweets are much shorter and the informal text is "noisier" than longer documents in the web or newswire settings, which are what the previous neural models were designed for; (2) most neural baselines build directly on top of the embedding similarity matrix without any representation learning, which can be less effective.</p><p>Comparing our proposed approaches in Table <ref type="table" target="#tab_4">3</ref>, RM achieves fairly good scores while SM is not effective at all, affirming our hypothesis that term matching signals are essential to IR tasks. This finding further supports our motivation for bridging SM and RM. Indeed, semantic matching methods alone are ineffective when queries are comprised of only a few keywords, without much semantic information to exploit. However, the  context-aware representations learned from SM do contribute to RM, leading to the superior results of our complete HCAN model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Encoder Comparisons</head><p>report results with the three different encoders from Sec 2.1 in Table <ref type="table" target="#tab_5">4</ref>. Overall, the effectiveness of the deep and wide encoders are quite close, given that the two encoders capture the same types of n-gram matching signals. The contextual encoder performs worse than the other two on TrecQA, but is comparable on all other datasets. This finding is consistent with <ref type="bibr" target="#b23">Rao et al. (2017a)</ref>, which shows that keyword matching signals are important for TrecQA. Also, we notice that the gaps between RM and SM are smaller for all encoders on Quora. We suspect that SM is more data-hungry than RM given its larger parameter space (actually, the RM module has no learnable parameters) and Quora is about 10× larger than the other datasets. For all encoders, combing RM and SM consistently improves effectiveness, af-firming that relevance and semantic matching signals are complementary regardless of the underlying encoder choice.</p><p>To better understand the different encoder mechanisms, we vary the number of encoder layers for the deep and contextual encoders in Figure 2 (since the wide encoder behaves similarly to the deep encoder, we omit the analysis here). Our complete HCAN model has N = 4. In Figure <ref type="figure" target="#fig_2">2a</ref>, we can see overall increases in effectiveness for the <ref type="bibr">RM and HCAN (comb)</ref> as N increases, showing that long-range phrase modeling is critical. However, increasing context window lengths don't help SM on the TrecQA and TREC-2013 datasets, likely because of dominant bigram matching signals (N = 1). Also, the complete HCAN model is consistently better than SM and RM alone in most settings, affirming its superior effectiveness, consistent with results in the above tables. In contrast, increasing the number of Bi-LSTM layers can sometimes even hurt, as shown in Figure <ref type="figure" target="#fig_1">2b</ref>. This is not a surprise since a sin-  gle BiLSTM layer (N = 1) can already capture long-range contextual information and increasing the number of layers can introduce more parameters and lead to overfitting.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Learning Efficiency</head><p>We also designed two experiments to examine whether the complete HCAN model is learning more efficiently than SM or RM alone. In these experiments, we used the deep encoder. First, we show validation losses for different numbers of batches in Figure <ref type="figure" target="#fig_3">3a</ref>. Second, we vary the training data size by randomly selecting different percentages (from 20% to 100%) of the original training set, shown in Figure <ref type="figure" target="#fig_3">3b</ref>.</p><p>In Figure <ref type="figure" target="#fig_3">3a</ref>, we can see that the validation loss for the complete model drops much faster than RM and SM alone, especially on TwitterURL and Quora. In Figure <ref type="figure" target="#fig_3">3b</ref>, we can see that, as expected, all methods in general achieve higher scores when more data are used for training. An exception is SM on the TREC-2013 Twitter dataset, which we see is not effective in Table <ref type="table" target="#tab_4">3</ref>. Another important finding is that both RM and HCAN are more data efficient: for TREC-2013 and TwitterURL, both can achieve effectiveness comparable to the full training set with only 20% data.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Qualitative Sample Analysis</head><p>We present sample outputs in Table <ref type="table" target="#tab_6">5</ref> to gain more insight into model behavior. For space considerations, we only show the Quora dataset, but our analysis reveals similar findings on the other datasets. The column "label" denotes the label of the sentence pair: 1 means semantically equiva-lent and 0 means not equivalent. For each model, we output its predicted label along with its confidence score; phrases with large attention weights are highlighted in orange and red.</p><p>In the first example, SM is able to correctly identify that the two sentences convey the same meaning with high confidence, while RM fails as the two sentences have no influential phrase matches (with high IDF weights). The sentence pair in the second example has a large text overlap. It is no surprise that RM would predict a high relevance score, while SM fails to capture their relatedness. In both examples, HCAN is able to integrate SM and RM to make correct predictions. Since the third example presents a similar pattern, we omit a detailed explanation. Overall, our quantitative and qualitative analyses show that relevance matching is better at capturing overlapbased signals, while combining semantic matching signals improve representation learning.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Related Work</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Neural Relevance Matching</head><p>Recently, deep learning has achieved great success in many NLP and IR applications <ref type="bibr" target="#b9">(He and Lin, 2016;</ref><ref type="bibr" target="#b29">Sutskever et al., 2014;</ref><ref type="bibr" target="#b39">Yin et al., 2016;</ref><ref type="bibr" target="#b24">Rao et al., 2017b)</ref>. Current neural models for IR can be divided into representation-based and interactionbased approaches, discussed below:</p><p>Early neural IR models mainly focus on representation-based modeling between the query and documents, such as DSSM <ref type="bibr" target="#b11">(Huang et al., 2013)</ref>, C-DSSM <ref type="bibr">(Shen et al., 2014)</ref>, and SM-CNN <ref type="bibr" target="#b27">(Severyn and Moschitti, 2015)</ref>. These meth-Label SM Score RM Score HCAN Score Sample Pair 1 1, 0.9119 0, 0.9353 1, 0.5496 -How does it feel to kill a human ? -How does it feel to be a murderer ? 1 0, 0.9689 1, 0.8762 1, 0.8481 -What are the time dilation effects on the ISS ? -According to the theory of relativity , time runs slowly under the influence of gravity . Is there any time dilation experienced on the ISS ? 0 0, 0.9927 1, 0.8473 1, 0.7280 -Does RBI send its employees for higher education such as MBA , like sponsoring the education or allowing paid / unpaid leaves ? -Does EY send its employees for higher education such as MBA , like sponsoring the education or allowing paid / unpaid leaves ? ods directly learn from query and document representations, and have been found to be ineffective when data is scarce.</p><p>Interaction-based approaches build on the similarity matrix computed from word pairs between the query and the document, often with countbased techniques to address data sparsity. For example, DRMM <ref type="bibr" target="#b8">(Guo et al., 2016)</ref> introduced a pyramid pooling technique to convert the similarity matrix into histogram representations, on top of which a term gating network aggregates weighted matching signals from different query terms. Inspired by DRMM, <ref type="bibr" target="#b36">Xiong et al. (2017b)</ref> proposed K-NRM, which introduced a differentiable kernelbased pooling technique to capture matching signals at different strength levels. Sharing similarities with our architecture is <ref type="bibr" target="#b25">Rao et al. (2019)</ref>, who developed a multi-perspective relevance matching method with a hierarchical convolutional encoder to capture character-level to sentence-level relevance signals from heterogeneous sources.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Neural Semantic</head><p>Semantic matching is a fundamental problem for a variety of NLP tasks. For example, in paraphrase identification, SM is used to determine whether two sentences or phrases convey the same meaning. In question answering or reading comprehension <ref type="bibr" target="#b35">(Xiong et al., 2017a;</ref><ref type="bibr" target="#b30">Tay et al., 2019a)</ref>, SM can help identify the correct answer span given a question. Semantic understanding and reasoning for two pieces of texts lie at the core of SM. Existing state-of-the-art techniques for SM usually comprise three major components: (1) sequential sentence encoders that incorporate word context and sentence order for better sentence representations; (2) interaction and attention mecha-nisms <ref type="bibr" target="#b31">(Tay et al., 2019b;</ref><ref type="bibr" target="#b26">Seo et al., 2017;</ref><ref type="bibr" target="#b21">Parikh et al., 2016;</ref><ref type="bibr" target="#b4">Conneau et al., 2017;</ref><ref type="bibr" target="#b7">Gong et al., 2018)</ref> to emphasize salient word pair interactions;</p><p>(3) structure modeling <ref type="bibr" target="#b3">(Chen et al., 2017b)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Conclusion</head><p>In this work, we examine the relationship between relevance matching and semantic matching, and highlight a few important differences between them. This is an important problem that lies at the core of many NLP and IR tasks. We propose the HCAN model with a relevance matching module to capture weighted n-gram matching signals and a semantic matching module for contextaware representation learning.</p><p>Thorough experiments show that relevance matching alone performs reasonably well for many NLP tasks, while semantic matching alone is not effective for IR tasks. We show that relevance matching and semantic matching are complementary, and HCAN combines the best of both worlds to achieve competitive effectiveness across a large number of tasks, in some cases, achieving the state of the art for models that do not exploit large-scale pretraining. We also find that our model can learn in a data efficient manner, further demonstrating the complementary nature of relevance matching and semantic matching signals.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head></head><label></label><figDesc>results on TREC Microblog 2013-2014 are shown in Table 3. The best numbers for each dataset (besides BERT) are bolded. We compare to three variants of our HCAN model: (1) only relevance matching signals (RM), (2) only semantic matching signals (SM), and (3) the complete model (HCAN).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: Model effectiveness with different numbers of encoder layers.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>( a )</head><label>a</label><figDesc>Validation losses w.r.t. number of batches. Effectiveness w.r.t. different percentage of training data.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: Experiments exploring learning efficiency.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2 :</head><label>2</label><figDesc>: text-based, URL-Results on TrecQA, TwitterURL, and Quora. The best scores except for BERT are bolded. In these experiments, all our approaches use the deep encoder in Sec. 2.1. RM and SM denote that only relevance and semantic matching signals are used, respectively. HCAN denotes the complete HCAN model.</figDesc><table><row><cell>Model</cell><cell cols="2">TrecQA MAP MRR</cell><cell cols="2">TwitterURL Quora macro-F1 Acc</cell></row><row><cell>InferSent</cell><cell cols="2">0.521 0.559</cell><cell>0.797</cell><cell>0.866</cell></row><row><cell>DecAtt</cell><cell cols="2">0.660 0.712</cell><cell>0.785</cell><cell>0.845</cell></row><row><cell>ESIM seq</cell><cell cols="2">0.771 0.795</cell><cell>0.822</cell><cell>0.850</cell></row><row><cell>ESIM tree</cell><cell cols="2">0.698 0.734</cell><cell>-</cell><cell>0.755</cell></row><row><cell>ESIM seq+tree</cell><cell cols="2">0.749 0.768</cell><cell>-</cell><cell>0.854</cell></row><row><cell>PWIM</cell><cell cols="2">0.739 0.795</cell><cell>0.809</cell><cell>0.834</cell></row><row><cell cols="4">State-of-the-Art Models</cell><cell></cell></row><row><cell>Rao et al. (2016)</cell><cell cols="2">0.780 0.834</cell><cell>-</cell><cell>-</cell></row><row><cell>Gong et al. (2018)</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>0.891</cell></row><row><cell>BERT</cell><cell cols="2">0.838 0.887</cell><cell>0.852</cell><cell>0.892</cell></row><row><cell></cell><cell cols="2">Our Approach</cell><cell></cell><cell></cell></row><row><cell>RM</cell><cell cols="2">0.756 0.812</cell><cell>0.790</cell><cell>0.842</cell></row><row><cell>SM</cell><cell cols="2">0.663 0.725</cell><cell>0.708</cell><cell>0.817</cell></row><row><cell>HCAN</cell><cell cols="2">0.774 0.843</cell><cell>0.817</cell><cell>0.853</cell></row></table><note>based, and hashtag-based. Finally, we include the BERT results from<ref type="bibr" target="#b38">Yang et al. (2019b)</ref>.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 3 :</head><label>3</label><figDesc>Results on TREC Microblog 2013-2014, organized in the same manner as Table 2.</figDesc><table><row><cell>matching (SM). It beats other competitive base-</cell></row><row><cell>lines (InferSent, DecAtt and ESIM) by a large</cell></row><row><cell>margin on the TrecQA dataset, and is still compa-</cell></row><row><cell>rable to those baselines on TwitterURL and Quora.</cell></row><row><cell>This finding suggests that soft term matching sig-</cell></row><row><cell>nals alone are fairly effective for many textual sim-</cell></row><row><cell>ilarity modeling tasks. However, SM performs</cell></row><row><cell>much worse on TrecQA and TwitterURL, while</cell></row><row><cell>the gap between SM and RM is reduced on Quora.</cell></row><row><cell>By combining SM and RM signals, we observe</cell></row><row><cell>consistent effectiveness gains in HCAN across</cell></row><row><cell>all three datasets, establishing new state-of-the-art</cell></row><row><cell>(non-BERT) results on TrecQA.</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 4 :</head><label>4</label><figDesc>Evaluation of different encoders in Sec. 2.1 (best numbers on each dataset are bolded).</figDesc><table><row><cell>Encoder</cell><cell>Model</cell><cell cols="3">TrecQA MAP MRR macro-F1 TwitURL Quora Acc</cell><cell>TREC-2013 MAP P@30 MAP P@30 TREC-2014</cell></row><row><cell></cell><cell>RM</cell><cell>0.756 0.812</cell><cell>0.790</cell><cell cols="2">0.842 0.282 0.522 0.430 0.630</cell></row><row><cell>Deep</cell><cell>SM</cell><cell>0.663 0.725</cell><cell>0.708</cell><cell cols="2">0.817 0.137 0.241 0.241 0.328</cell></row><row><cell></cell><cell cols="2">HCAN 0.774 0.843</cell><cell>0.817</cell><cell cols="2">0.853 0.292 0.533 0.437 0.649</cell></row><row><cell></cell><cell>RM</cell><cell>0.758 0.806</cell><cell>0.790</cell><cell cols="2">0.830 0.278 0.510 0.421 0.617</cell></row><row><cell>Wide</cell><cell>SM</cell><cell>0.673 0.727</cell><cell>0.719</cell><cell cols="2">0.811 0.138 0.247 0.247 0.336</cell></row><row><cell></cell><cell cols="2">HCAN 0.770 0.847</cell><cell>0.795</cell><cell cols="2">0.843 0.285 0.524 0.435 0.642</cell></row><row><cell></cell><cell>RM</cell><cell>0.690 0.736</cell><cell>0.811</cell><cell cols="2">0.804 0.272 0.503 0.417 0.613</cell></row><row><cell>Contextual</cell><cell>SM</cell><cell>0.668 0.735</cell><cell>0.730</cell><cell cols="2">0.805 0.133 0.256 0.242 0.324</cell></row><row><cell></cell><cell cols="2">HCAN 0.739 0.790</cell><cell>0.815</cell><cell cols="2">0.826 0.285 0.524 0.434 0.635</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 5 :</head><label>5</label><figDesc>Sample pairs from Quora. Phrases with large attention weights are highlighted in orange and red.</figDesc><table /></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0">The leaderboard can be found in https: //aclweb.org/aclwiki/Question_Answering_(State_of_the_art)   </note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_1">https://github.com/NTMC-Community/ MatchZoo</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgments</head><p>This research was supported in part by the Natural Sciences and Engineering Research Council (NSERC) of Canada.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">UMass at TREC 2004: Novelty and HARD</title>
		<author>
			<persName><forename type="first">Nasreen</forename><surname>Abdul-Jaleel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">James</forename><surname>Allan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">Bruce</forename><surname>Croft</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fernando</forename><surname>Diaz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Leah</forename><surname>Larkey</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaoyan</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Donald</forename><surname>Metzler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mark</forename><forename type="middle">D</forename><surname>Smucker</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Trevor</forename><surname>Strohman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Howard</forename><surname>Turtle</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Courtney</forename><surname>Wade</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Thirteenth Text REtrieval Conference (TREC 2004)</title>
				<meeting>the Thirteenth Text REtrieval Conference (TREC 2004)<address><addrLine>Gaithersburg, Maryland</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">From RankNet to LambdaRank to LambdaMART: An overview</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">C</forename><surname>Christopher</surname></persName>
		</author>
		<author>
			<persName><surname>Burges</surname></persName>
		</author>
		<idno>MSR-TR-2010-82</idno>
		<imprint>
			<date type="published" when="2010">2010</date>
			<publisher>Microsoft Research</publisher>
		</imprint>
	</monogr>
	<note type="report_type">Technical Report</note>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Reading Wikipedia to answer opendomain questions</title>
		<author>
			<persName><forename type="first">Danqi</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Adam</forename><surname>Fisch</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jason</forename><surname>Weston</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Antoine</forename><surname>Bordes</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/P17-1171</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics</title>
		<title level="s">Long Papers</title>
		<meeting>the 55th Annual Meeting of the Association for Computational Linguistics<address><addrLine>Vancouver, Canada</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2017">2017a</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="1870" to="1879" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Enhanced LSTM for natural language inference</title>
		<author>
			<persName><forename type="first">Qian</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaodan</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhen-Hua</forename><surname>Ling</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Si</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hui</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Diana</forename><surname>Inkpen</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/P17-1152</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics</title>
				<meeting>the 55th Annual Meeting of the Association for Computational Linguistics<address><addrLine>Vancouver, Canada</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2017">2017b</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="1657" to="1668" />
		</imprint>
	</monogr>
	<note>Long Papers)</note>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Supervised learning of universal sentence representations from natural language inference data</title>
		<author>
			<persName><forename type="first">Alexis</forename><surname>Conneau</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Douwe</forename><surname>Kiela</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Holger</forename><surname>Schwenk</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Loïc</forename><surname>Barrault</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Antoine</forename><surname>Bordes</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/D17-1070</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing</title>
				<meeting>the 2017 Conference on Empirical Methods in Natural Language Processing<address><addrLine>Copenhagen, Denmark</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="670" to="680" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Convolutional neural networks for soft-matching n-grams in ad-hoc search</title>
		<author>
			<persName><forename type="first">Zhuyun</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chenyan</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jamie</forename><surname>Callan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhiyuan</forename><surname>Liu</surname></persName>
		</author>
		<idno type="DOI">10.1145/3159652.3159659</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Eleventh ACM International Conference on Web Search and Data Mining, WSDM &apos;18</title>
				<meeting>the Eleventh ACM International Conference on Web Search and Data Mining, WSDM &apos;18<address><addrLine>Marina Del Rey, CA, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="126" to="134" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">BERT: Pre-training of deep bidirectional transformers for language understanding</title>
		<author>
			<persName><forename type="first">Jacob</forename><surname>Devlin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ming-Wei</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kenton</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kristina</forename><surname>Toutanova</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/N19-1423</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2019 Conference of the North American Chapter</title>
		<title level="s">the Association for Computational Linguistics: Human Language Technologies</title>
		<meeting>the 2019 Conference of the North American Chapter<address><addrLine>Minneapolis, Minnesota</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="4171" to="4186" />
		</imprint>
	</monogr>
	<note>Long and Short Papers</note>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Natural language inference over interaction space</title>
		<author>
			<persName><forename type="first">Yichen</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Heng</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jian</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Sixth International Conference on Learning Representations</title>
				<meeting>the Sixth International Conference on Learning Representations</meeting>
		<imprint>
			<date type="published" when="2018">2018. 2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Pairwise word interaction modeling with deep neural networks for semantic similarity measurement</title>
		<author>
			<persName><forename type="first">Jiafeng</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yixing</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qingyao</forename><surname>Ai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">Bruce</forename><surname>Croft</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/N16-1108</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 25th ACM International on Conference on Information and Knowledge Management, CIKM &apos;16</title>
				<meeting>the 25th ACM International on Conference on Information and Knowledge Management, CIKM &apos;16<address><addrLine>Indianapolis, Indiana, USA; San Diego, California</addrLine></address></meeting>
		<imprint>
			<publisher>Human Language Technologies</publisher>
			<date type="published" when="2016">2016. 2016</date>
			<biblScope unit="page" from="937" to="948" />
		</imprint>
	</monogr>
	<note>Proceedings of the 2016 Conference of the North American Chapter</note>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">UMD-TTIC-UW at SemEval-2016 task 1: Attention-based multi-perspective convolutional neural networks for textual similarity measurement</title>
		<author>
			<persName><forename type="first">Hua</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">John</forename><surname>Wieting</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kevin</forename><surname>Gimpel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jinfeng</forename><surname>Rao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jimmy</forename><surname>Lin</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/S16-1170</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 10th International Workshop on Semantic Evaluation (SemEval-2016)</title>
				<meeting>the 10th International Workshop on Semantic Evaluation (SemEval-2016)<address><addrLine>San Diego, California</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="1103" to="1108" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Training a ranking function for open-domain question answering</title>
		<author>
			<persName><forename type="first">Samuel</forename><surname>Phu Mon Htut</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kyunghyun</forename><surname>Bowman</surname></persName>
		</author>
		<author>
			<persName><surname>Cho</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/N18-4017</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Student Research Workshop</title>
				<meeting>the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Student Research Workshop<address><addrLine>New Orleans, Louisiana</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="120" to="127" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Learning deep structured semantic models for web search using clickthrough data</title>
		<author>
			<persName><forename type="first">Po-Sen</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaodong</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jianfeng</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Li</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alex</forename><surname>Acero</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Larry</forename><surname>Heck</surname></persName>
		</author>
		<idno type="DOI">10.1145/2505515.2505665</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 22nd ACM International Conference on Information &amp; Knowledge Management, CIKM &apos;13</title>
				<meeting>the 22nd ACM International Conference on Information &amp; Knowledge Management, CIKM &apos;13<address><addrLine>San Francisco, California, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2013">2013</date>
			<biblScope unit="page" from="2333" to="2338" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">PACRR: A position-aware neural IR model for relevance matching</title>
		<author>
			<persName><forename type="first">Kai</forename><surname>Hui</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrew</forename><surname>Yates</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Klaus</forename><surname>Berberich</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gerard</forename><surname>De</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Melo</forename></persName>
		</author>
		<idno type="DOI">10.18653/v1/D17-1110</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing</title>
				<meeting>the 2017 Conference on Empirical Methods in Natural Language Processing<address><addrLine>Copenhagen, Denmark</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="1049" to="1058" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Co-PACRR: A context-aware neural IR model for ad-hoc retrieval</title>
		<author>
			<persName><forename type="first">Kai</forename><surname>Hui</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrew</forename><surname>Yates</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Klaus</forename><surname>Berberich</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gerard</forename><surname>De</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Melo</forename></persName>
		</author>
		<idno type="DOI">10.1145/3159652.3159689</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Eleventh ACM International Conference on Web Search and Data Mining, WSDM &apos;18</title>
				<meeting>the Eleventh ACM International Conference on Web Search and Data Mining, WSDM &apos;18<address><addrLine>Marina Del Rey, CA, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="279" to="287" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<author>
			<persName><forename type="first">Shankar</forename><surname>Iyer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nikhil</forename><surname>Dandekar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kornél</forename><surname>Csernai</surname></persName>
		</author>
		<title level="m">First Quora Dataset Release: Question Pairs</title>
				<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">A continuously growing dataset of sentential paraphrases</title>
		<author>
			<persName><forename type="first">Wuwei</forename><surname>Lan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Siyu</forename><surname>Qiu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hua</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wei</forename><surname>Xu</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/D17-1126</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing</title>
				<meeting>the 2017 Conference on Empirical Methods in Natural Language Processing<address><addrLine>Copenhagen, Denmark</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="1224" to="1234" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Neural network models for paraphrase identification, semantic textual similarity, natural language inference, and question answering</title>
		<author>
			<persName><forename type="first">Wuwei</forename><surname>Lan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wei</forename><surname>Xu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 27th International Conference on Computational Linguistics</title>
				<meeting>the 27th International Conference on Computational Linguistics<address><addrLine>Santa Fe, New Mexico, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="3890" to="3902" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Overview of the TREC-2013 Microblog Track</title>
		<author>
			<persName><forename type="first">Jimmy</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Miles</forename><surname>Efron</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Twenty-Second Text REtrieval Conference</title>
				<meeting>the Twenty-Second Text REtrieval Conference</meeting>
		<imprint>
			<date type="published" when="2013">2013. 2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Overview of the TREC-2014 Microblog Track</title>
		<author>
			<persName><forename type="first">Jimmy</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Miles</forename><surname>Efron</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yulu</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Garrick</forename><surname>Sherman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Twenty-Third Text REtrieval Conference (TREC 2014)</title>
				<meeting>the Twenty-Third Text REtrieval Conference (TREC 2014)<address><addrLine>Gaithersburg, Maryland</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Distributed representations of words and phrases and their compositionality</title>
		<author>
			<persName><forename type="first">Tomas</forename><surname>Mikolov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kai</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Greg</forename><surname>Corrado</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jeffrey</forename><surname>Dean</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
				<imprint>
			<date type="published" when="2013">2013</date>
			<biblScope unit="volume">26</biblScope>
			<biblScope unit="page" from="3111" to="3119" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Learning to match using local and distributed representations of text for web search</title>
		<author>
			<persName><forename type="first">Fernando</forename><surname>Bhaskar Mitra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nick</forename><surname>Diaz</surname></persName>
		</author>
		<author>
			<persName><surname>Craswell</surname></persName>
		</author>
		<idno type="DOI">10.1145/3038912.3052579</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 26th International Conference on World Wide Web, WWW &apos;17</title>
				<meeting>the 26th International Conference on World Wide Web, WWW &apos;17<address><addrLine>Perth, Australia</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="1291" to="1299" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">A decomposable attention model for natural language inference</title>
		<author>
			<persName><forename type="first">Ankur</forename><surname>Parikh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Oscar</forename><surname>Täckström</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dipanjan</forename><surname>Das</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jakob</forename><surname>Uszkoreit</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/D16-1244</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing</title>
				<meeting>the 2016 Conference on Empirical Methods in Natural Language Processing<address><addrLine>Austin, Texas</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="2249" to="2255" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Noisecontrastive estimation for answer selection with deep neural networks</title>
		<author>
			<persName><forename type="first">Jinfeng</forename><surname>Rao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hua</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jimmy</forename><surname>Lin</surname></persName>
		</author>
		<idno type="DOI">10.1145/2983323.2983872</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 25th ACM International on Conference on Information and Knowledge Management, CIKM &apos;16</title>
				<meeting>the 25th ACM International on Conference on Information and Knowledge Management, CIKM &apos;16<address><addrLine>Indianapolis, Indiana, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="1913" to="1916" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Experiments with convolutional neural network models for answer selection</title>
		<author>
			<persName><forename type="first">Jinfeng</forename><surname>Rao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hua</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jimmy</forename><surname>Lin</surname></persName>
		</author>
		<idno type="DOI">10.1145/3077136.3080648</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 40th International ACM SIGIR Conference on Research and Development in Information Retrieval, SIGIR &apos;17</title>
				<meeting>the 40th International ACM SIGIR Conference on Research and Development in Information Retrieval, SIGIR &apos;17<address><addrLine>Shinjuku, Tokyo, Japan</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2017">2017a</date>
			<biblScope unit="page" from="1217" to="1220" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Talking to your TV: Contextaware voice search with hierarchical recurrent neural networks</title>
		<author>
			<persName><forename type="first">Jinfeng</forename><surname>Rao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ferhan</forename><surname>Ture</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hua</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Oliver</forename><surname>Jojic</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jimmy</forename><surname>Lin</surname></persName>
		</author>
		<idno type="DOI">10.1145/3132847.3132893</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2017 ACM on Conference on Information and Knowledge Management, CIKM &apos;17</title>
				<meeting>the 2017 ACM on Conference on Information and Knowledge Management, CIKM &apos;17<address><addrLine>Singapore</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2017">2017b</date>
			<biblScope unit="page" from="557" to="566" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Multi-perspective relevance matching with hierarchical ConvNets for social media search</title>
		<author>
			<persName><forename type="first">Jinfeng</forename><surname>Rao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wei</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuhao</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ferhan</forename><surname>Ture</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jimmy</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Thirty-Third AAAI Conference on Artificial Intelligence (AAAI-19)</title>
				<meeting>the Thirty-Third AAAI Conference on Artificial Intelligence (AAAI-19)<address><addrLine>Honolulu, Hawaii</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="232" to="240" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Bidirectional attention flow for machine comprehension</title>
		<author>
			<persName><forename type="first">Minjoon</forename><surname>Seo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aniruddha</forename><surname>Kembhavi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ali</forename><surname>Farhadi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hannaneh</forename><surname>Hajishirzi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Fifth International Conference on Learning Representations</title>
				<meeting>the Fifth International Conference on Learning Representations</meeting>
		<imprint>
			<date type="published" when="2017">2017. 2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Learning semantic representations using convolutional neural networks for web search</title>
		<author>
			<persName><forename type="first">Aliaksei</forename><surname>Severyn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alessandro Moschitti ; Jianfeng</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Li</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Grégoire</forename><surname>Mesnil</surname></persName>
		</author>
		<idno type="DOI">10.1145/2567948.2577348</idno>
		<idno>WWW &apos;14</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 38th International ACM SIGIR Conference on Research and Development in Information Retrieval, SIGIR &apos;15</title>
				<meeting>the 38th International ACM SIGIR Conference on Research and Development in Information Retrieval, SIGIR &apos;15<address><addrLine>Santiago, Chile</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2014">2015. 2014</date>
			<biblScope unit="page" from="373" to="382" />
		</imprint>
	</monogr>
	<note>Proceedings of the 23rd International Conference on World Wide Web</note>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title/>
	</analytic>
	<monogr>
		<title level="j">Companion</title>
		<imprint>
			<biblScope unit="page" from="373" to="374" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Sequence to sequence learning with neural networks</title>
		<author>
			<persName><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Oriol</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Quoc</surname></persName>
		</author>
		<author>
			<persName><surname>Le</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
				<imprint>
			<date type="published" when="2014">2014</date>
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="page" from="3104" to="3112" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Simple and effective curriculum pointer-generator networks for reading comprehension over long narratives</title>
		<author>
			<persName><forename type="first">Yi</forename><surname>Tay</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shuohang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Anh</forename><forename type="middle">Tuan</forename><surname>Luu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jie</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Minh</forename><forename type="middle">C</forename><surname>Phan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xingdi</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jinfeng</forename><surname>Rao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Siu</forename><forename type="middle">Cheung</forename><surname>Hui</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aston</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics</title>
				<meeting>the 57th Annual Meeting of the Association for Computational Linguistics<address><addrLine>Florence, Italy</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2019">2019a</date>
			<biblScope unit="page" from="4922" to="4931" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Lightweight and efficient neural natural language processing with quaternion networks</title>
		<author>
			<persName><forename type="first">Yi</forename><surname>Tay</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aston</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Anh</forename><forename type="middle">Tuan</forename><surname>Luu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jinfeng</forename><surname>Rao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shuai</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shuohang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jie</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Siu</forename><forename type="middle">Cheung</forename><surname>Hui</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics</title>
				<meeting>the 57th Annual Meeting of the Association for Computational Linguistics<address><addrLine>Florence, Italy</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2019">2019b</date>
			<biblScope unit="page" from="1494" to="1503" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Attention is all you need</title>
		<author>
			<persName><forename type="first">Ashish</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Noam</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Niki</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jakob</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Llion</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aidan</forename><forename type="middle">N</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Łukasz</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Illia</forename><surname>Polosukhin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
				<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="5998" to="6008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">What is the Jeopardy model? A quasi-synchronous grammar for QA</title>
		<author>
			<persName><forename type="first">Mengqiu</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Noah</forename><forename type="middle">A</forename><surname>Smith</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Teruko</forename><surname>Mitamura</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2007 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning (EMNLP-CoNLL)</title>
				<meeting>the 2007 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning (EMNLP-CoNLL)<address><addrLine>Prague, Czech Republic</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2007">2007</date>
			<biblScope unit="page" from="22" to="32" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Bilateral multi-perspective matching for natural language sentences</title>
		<author>
			<persName><forename type="first">Zhiguo</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wael</forename><surname>Hamza</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Radu</forename><surname>Florian</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Twenty-Sixth International Joint Conference on Artificial Intelligence (IJCAI 2017)</title>
				<meeting>the Twenty-Sixth International Joint Conference on Artificial Intelligence (IJCAI 2017)<address><addrLine>Melbourne, Australia</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="4144" to="4150" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Dynamic coattention networks for question answering</title>
		<author>
			<persName><forename type="first">Caiming</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Victor</forename><surname>Zhong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Fifth International Conference on Learning Representations</title>
				<meeting>the Fifth International Conference on Learning Representations</meeting>
		<imprint>
			<date type="published" when="2017">2017a. 2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">End-to-end neural ad-hoc ranking with kernel pooling</title>
		<author>
			<persName><forename type="first">Chenyan</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhuyun</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jamie</forename><surname>Callan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhiyuan</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Russell</forename><surname>Power</surname></persName>
		</author>
		<idno type="DOI">10.1145/3077136.3080809</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 40th International ACM SIGIR Conference on Research and Development in Information Retrieval, SIGIR &apos;17</title>
				<meeting>the 40th International ACM SIGIR Conference on Research and Development in Information Retrieval, SIGIR &apos;17<address><addrLine>Shinjuku, Tokyo, Japan</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2017">2017b</date>
			<biblScope unit="page" from="55" to="64" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Critically examining the &quot;neural hype&quot;: Weak baselines and the additivity of effectiveness gains from neural ranking models</title>
		<author>
			<persName><forename type="first">Wei</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kuang</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Peilin</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jimmy</forename><surname>Lin</surname></persName>
		</author>
		<idno type="DOI">10.1145/3331184.3331340</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 42nd International ACM SIGIR Conference on Research and Development in Information Retrieval, SIGIR&apos;19</title>
				<meeting>the 42nd International ACM SIGIR Conference on Research and Development in Information Retrieval, SIGIR&apos;19<address><addrLine>Paris, France</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2019">2019a</date>
			<biblScope unit="page" from="1129" to="1132" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<monogr>
		<author>
			<persName><forename type="first">Wei</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Haotian</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jimmy</forename><surname>Lin</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1903.10972</idno>
		<title level="m">Simple applications of BERT for ad hoc document retrieval</title>
				<imprint>
			<date type="published" when="2019">2019b</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">ABCNN: Attention-based convolutional neural network for modeling sentence pairs</title>
		<author>
			<persName><forename type="first">Wenpeng</forename><surname>Yin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hinrich</forename><surname>Schütze</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bing</forename><surname>Xiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bowen</forename><surname>Zhou</surname></persName>
		</author>
		<idno type="DOI">10.1162/tacl_a_00097</idno>
	</analytic>
	<monogr>
		<title level="j">Transactions of the Association for Computational Linguistics</title>
		<imprint>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page" from="259" to="272" />
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
