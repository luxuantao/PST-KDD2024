<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">VLSI Placement Parameter Optimization using Deep Reinforcement Learning</title>
				<funder ref="#_hqDeMbR">
					<orgName type="full">National Science Foundation</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author role="corresp">
							<persName><forename type="first">Anthony</forename><surname>Agnesina</surname></persName>
							<email>agnesina@gatech.edu</email>
							<affiliation key="aff0">
								<orgName type="department">School of ECE</orgName>
								<orgName type="institution">Georgia Institute of Technology</orgName>
								<address>
									<settlement>Atlanta</settlement>
									<region>GA</region>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Kyungwook</forename><surname>Chang</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">School of ECE</orgName>
								<orgName type="institution">Georgia Institute of Technology</orgName>
								<address>
									<settlement>Atlanta</settlement>
									<region>GA</region>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Sung</forename><forename type="middle">Kyu</forename><surname>Lim</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">School of ECE</orgName>
								<orgName type="institution">Georgia Institute of Technology</orgName>
								<address>
									<settlement>Atlanta</settlement>
									<region>GA</region>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">VLSI Placement Parameter Optimization using Deep Reinforcement Learning</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="DOI">10.1145/3400302.3415690</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-01-03T09:32+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>The quality of placement is essential in the physical design flow. To achieve PPA goals, a human engineer typically spends a considerable amount of time tuning the multiple settings of a commercial placer (e.g. maximum density, congestion effort, etc.). This paper proposes a deep reinforcement learning (RL) framework to optimize the placement parameters of a commercial EDA tool. We build an autonomous agent that learns to tune parameters optimally without human intervention and domain knowledge, trained solely by RL from self-search. To generalize to unseen netlists, we use a mixture of handcrafted features from graph topology theory along with graph embeddings generated using unsupervised Graph Neural Networks. Our RL algorithms are chosen to overcome the sparsity of data and latency of placement runs. Our trained RL agent achieves up to 11% and 2.5% wirelength improvements on unseen netlists compared with a human engineer and a state-of-the-art tool auto-tuner, in just one placement iteration (20? and 50? less iterations).</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">INTRODUCTION</head><p>In the recent years, the combination of deep learning techniques with reinforcement learning (RL) principles has resulted in the creation of self-learning agents achieving superhuman performance at the game of Go, Shogi and Chess <ref type="bibr" target="#b12">[13]</ref>. Deep RL is also used with large success in real-world applications such as robotics, finance, self-driving cars, etc.</p><p>The quality of VLSI placement is essential for the subsequent steps of physical design with influential repercussions on design quality and design closure. Recent studies <ref type="bibr" target="#b9">[10]</ref> however show that existing placers cannot produce near optimal solutions. The goal of a placement engine is to assign locations for the cells inside the chip's area. The most common target of state-of-the-art placers is to minimize the total interconnect length, i.e. the estimated halfperimeter wire length (HPWL) from the placed cells locations.</p><p>The algorithms implemented inside the EDA tools have parameter settings that users can modify to achieve the desired powerperformance-area (PPA). In the authors' experience, more time is spent on tuning and running a commercial placer than on creating a first version of the design. Tools and flows have steadily increased in complexity, with modern place and route tools offering more than 10,000 parameter settings. Expert users are required in particular for the latest technology nodes, with increased cost and risk. Indeed, as the design space of the parameters is too big and complex to be explored by a human engineer alone, one usually relies on expertise and domain knowledge when tuning. However, the correlations between the different parameters and the resulting PPA may be complex or nonintuitive. Placement engines may exhibit nondeterministic behaviors as they heavily rely on handcrafted rules and metaheurisitics. Moreover, the advertised goal of a parameter may not always directly translate onto the targeted metric.</p><p>A state-of-the-art tool auto-tuner <ref type="bibr" target="#b0">[1]</ref> is used in EDA such as in <ref type="bibr" target="#b17">[18]</ref> and <ref type="bibr" target="#b18">[19]</ref> to optimize Quality of Results (QoR) in the FPGA and high-level synthesis (HLS) compilation flows. It leverages Multi-Armed Bandit (MAB) to organize a set of classical optimization techniques and efficiently explore the design space. However, these techniques rely on heuristics that are too general and do not consider the specificities of each netlist. Therefore, each new netlist requires to start over parameter exploration. We overcome this limitation in our RL agent by first encoding the netlist information using a mixture of graph handcrafted features and graph neural network embeddings. This helps generalize the tuning process from netlist to netlist, saving long and costly placement iterations.</p><p>The goal of our RL framework is to learn an optimization process that finds placement parameters minimizing wirelength after placement. The main contributions of this paper are:</p><p>? We reduce the significant time expense of VLSI development by application of deep RL to pre-set the placement parameters of a commercial EDA tool. To the best of our knowledge, this is the first work on RL applied to placement parameters optimization. ? Our RL algorithm overcomes the sparsity of data and the latency of design tool runs using multiple environments collecting experiences in parallel. ? We use a mixture of features relative to graph topological characteristics along with graph embedding generated by a graph neural network to train an RL agent capable of generalizing its tuning process to unseen netlists. ? We build an autonomous agent that iteratively learns to tune parameter settings to optimize placement, without supervised samples. We achieve better wirelengths on unseen netlists than a state-of-the-art auto-tuner, without any additional training and in just one placement iteration.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">RL ENVIRONMENT 2.1 Overview</head><p>We build an RL agent that tunes the parameter settings of the placement tool autonomously, with the objective of minimizing wirelength. Our RL problem consists of the following four key elements:</p><p>? States: the set of all netlists in the world and all possible parameter settings combinations (P) from the placement tool (e.g. Cadence Innovus or Synopsys ICC2). A single state ? consists of a unique netlist and a current parameter set. ? Actions: set of actions that the agent can use to modify the current parameters. An action ? changes the setting of a subset of parameters. As depicted in Figure <ref type="figure" target="#fig_0">1</ref>, in RL an agent learns from interacting with its environment over a number of discrete time steps. At each time step ? the agent receives a state ? ? , and selects an action ? ? from a set of possible actions A according to its policy ?, where ? maps states to actions. In return, the agent receives a reward signal ? ? and transitions to the next state ? ? +1 . This process continues until the agent reaches a terminal state after which the process restarts. The goal of the agent is to maximize its long-term return:</p><formula xml:id="formula_0">? ? = ? ?=0 ? ? ? ? +?+1<label>(1)</label></formula><p>where ? is a factor discounting future rewards.</p><p>An optimal policy is one that maximizes the expected returns or values. The value function ? ? (? ? ) is the expected return starting from state ? ? when following policy ?:</p><formula xml:id="formula_1">? ? (? ? ) = E ? [? ? |? ? ] = E ? ? ?=0 ? ? ? ? +?+1 |? ? .</formula><p>(2)</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Our RL Settings</head><p>We formulate the placement parameters optimization task led by an RL agent as follows:</p><p>RL Placement Parameter Optimization Problem Goal Given a netlist, find arg min ? ? P ??? ?(?) where P is the set of all parameter combinations and ??? ? is given by the tool. How? (1) Define Environment as placement tool black-box.</p><p>(2) Define state ? ? current set of parameters ? ???? ? P and target netlist. (3) Define actions to modify ? ???? . (4) Define a reward ? ? -??? ? so that the agent's goal is to decrease wirelength. (5) Select a discount factor ? &lt; 1 to force the agent reduce wirelength in as few steps as possible.</p><p>current  This is a combinatorial optimization problem where P is very large and exhaustive search is infeasible. For an agent to correctly select an action, we must first define a good representation of its environment. In our case, the representation of the environment is given by a human expert as presented.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Our States</head><p>We define our state as the joint values of 12 placement parameters from Cadence Innovus used to perform the current placement (Table <ref type="table" target="#tab_2">1</ref>), along with information metrics on the netlist being placed. The netlist information consists of a mixture of metadata knowledge (number of cells, floorplan area, etc.) with graph topological features (Table <ref type="table" target="#tab_3">2</ref>) along with unsupervised features extracted using a graph neural network. Netlist characteristics are important to transfer the knowledge accross very different netlists so that our agent generalizes its tuning process to unseen netlists. Indeed, the optimal policy is likely to be related to the particularities of each netlist. Our state can be written as a concatenation of one-hot encoded categorical parameters (Booleans or enumerates), integer parameters and integer and float netlist features.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3.1">Topological Graph Features.</head><p>In order to learn rich graph representations that uniquely classify each graph, we borrow concepts from graph theory. We use Boost and BLAS optimized C++ libraries to extract efficiently our features on large graphs (all collected in less than 1hr for the larger netlists). Let ? = (? , ?) be the directed cyclic graph representing the netlist, obtained using a fully-connected clique model. Global signals such as reset, clock or VDD/VSS are not considered when building the graph. Multiple connections between two vertices in the graph are merged into one and self-loops are eliminated. We consider the following graph features to capture complex topology characteristics (e.g. connections and spectral) from the netlist:</p><formula xml:id="formula_2">? Strongly Connected Components (SCC): A strong component of</formula><p>? is a subgraph ? of ? if for any pair of vertices ?, ? ? ? there is a directed cycle in ? containing ? and ?. We compute them in linear time using directed depth-first search with an algorithm due to Tarjan <ref type="bibr" target="#b13">[14]</ref>. ? Clique: Given an integer ?, does ? contains ? ? (the complete graph on ? vertices) as a subgraph? We use the Bron-Kerbosch algorithm <ref type="bibr" target="#b2">[3]</ref> to find the maximal cliques. ? k-Colorability: Given an integer ?, is there a coloring of ? with ? or fewer colors? A coloring is a map ? : ? ? ? such as  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>? (?, ?).</head><p>? Rich Club Coefficient: How well do high degree (rich) nodes know each other? Let ? ? = (? ? , ? ? ) be the filtered graph of ? with only nodes of degree &gt; ?, then</p><formula xml:id="formula_3">??? ? = 2|? ? | |? ? | ( |? ? |-1) [4].</formula><p>? Clustering coefficient: A measure of the cliquishness of nodes neighborhoods <ref type="bibr" target="#b16">[17]</ref>:</p><formula xml:id="formula_4">?? = 1 |? | ? ?? |? ?? : ? ? , ? ? ? Neighbors(?), ? ?? ? ?| deg(?)(deg(?) -1)</formula><p>.</p><p>(3)</p><p>? Spectral characteristics: Using the implicitly restarted Arnoldi method <ref type="bibr" target="#b7">[8]</ref>, we extract from the Laplacian matrix of ? the Fiedler value (second smallest eigenvalue) deeply related to the connectivity properties of ?, as well as the spectral radius (largest eigenvalue) relative to the regularity of ?.</p><p>These features give important information about the netlist. For example, connectivity features such as SCC, maximal clique and RCC are important to capture congestion considerations (considered during placement refinement) while logic levels translate indirectly the difficulty of meeting timing by extracting the longest logic path.</p><formula xml:id="formula_5">1 2 3 5 4 G = (V, E) ENC(G) ENC(v, v V) Graph SAGE ENC(3)</formula><p>ENC( <ref type="formula" target="#formula_7">5</ref>)</p><p>ENC( <ref type="formula" target="#formula_0">1</ref>) ENC( <ref type="formula">2</ref>) ENC(4) mean aggregator </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3.2">Features from Graph Neural Network (GNN).</head><p>Starting from simple node features including degree, fanout, area and encoded gate type, we generate node embeddings (enc(?)) using unsupervised GraphSAGE <ref type="bibr" target="#b6">[7]</ref> with convolutional aggregation, dropout, and output size of 32. The GNN algorithm iteratively propagates information from a node to its neighbors. The GNN is trained on each graph individually. Then the graph embedding (enc(?)) is obtained from the node embeddings with a permutation invariant aggregator as shown in Figure <ref type="figure">2</ref>: The t-SNE projection in two dimensions <ref type="bibr" target="#b14">[15]</ref> of the vector of graph features is displayed in Figure <ref type="figure">3</ref>. We see that all netlists points are far apart, indicating that the combination of our handcrafted and learned graph features distinguish well the particularities of each netlist.</p><formula xml:id="formula_6">enc(?) = mean enc(?)|? ? ? .<label>(4)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.4">Our Actions</head><p>We define our own deterministic actions to change the setting of a subset of parameters. They render the state markovian, i.e. given state-action pair (? ? , ? ? ), the resulting state ? ? +1 is unique. An advantage of fully-observed determinism is that it allows planning. Starting from state ? 0 and following a satisfactory policy ?, the trajectory</p><formula xml:id="formula_7">? 0 ? (? 0 ) ----? ? 1 ? (? 1 ) ----? ... ? (? ?-1 ) ------? ? ?<label>(5)</label></formula><p>leads to a parameter set ? ? of good quality. If ? has been learned, ? ? can be computed directly in O (1) time without performing any placement.</p><p>Defining only two actions per placement parameter would result in 24 different actions, which is too many for the agent to learn well. Thus, we decide to first group tuning variables per type (Boolean, Enumerate, Numeric) and per placement "focus" (Global, Detailed, Effort). On each of these groups, we define simple yet expressive actions such as flip (for booleans), up, down, etc. For integers, we define prepared ranges where up ? "put in range", while for enumerates down ? "pass from high to medium" for example. We also add one arm that does not modify the current set of parameters. It serves as a trigger to reset the environment, in case it gets picked multiple times in a row. This leads to the 11 different actions A presented in Table <ref type="table" target="#tab_4">3</ref>. Our action space is designed to be as simple as possible in order to help neural network training, but also expressive enough so that any parameter settings can be reached by such transformations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.5">Our Reward Structure</head><p>In order to learn with a single RL agent across various netlists with different wirelengths, we cannot define a reward directly linear with HPWL. Thus, to help convergence, we adopt a normalized reward function which renders the magnitude of the value approximations similar among netlists:</p><formula xml:id="formula_8">? ? := ??? ? Human Baseline -??? ? ? ??? ? Human Baseline .<label>(6)</label></formula><p>While defining rewards in this manner necessitates knowing ??? ? Human Baseline , an expected baseline wirelength per design, this only requires one placement to be completed by an engineer.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.6">Extensions in Physical Design Flow</head><p>The environment description and in particular the action definition can be applied to Parameter Optimization of any stage in the physical design flow, such as routing, clock tree synthesis, etc. As our actions act on abstracted representations of tool parameters, we can perform rapid design space exploration. The reward function can be easily adjusted to target PPA such as routed wirelength or congestion, in order to optimize the design for different trade-offs. For example, a reward function combining various attributes into a single numerical value can be:</p><formula xml:id="formula_9">? = exp ? ? ? QoR ? -1. (<label>7</label></formula><formula xml:id="formula_10">)</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">RL PLACEMENT AGENT 3.1 Overview</head><p>Using the definition of the environment presented in the previous section, we train an agent to autonomously tune the parameters of the placement tool. Here is our approach:</p><p>? The agent learns the optimal action for a given state. This action is chosen based on its policy network probability outputs. ? To train the policy network effectively, we adopt an actor-critic framework which brings the benefits of value-based and policybased optimization algorithms together. ? To solve the well-known shortcomings of RL in EDA that are latency and sparsity, we implement multiple environments collecting different experiences in parallel. ? To enable the learning of a recursive optimization process with complex dependencies, our agent architecture utilizes a deep neural network comprising a recurrent layer with attention mechanism.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Goal of Learning</head><p>From the many ways of learning how to behave in an environment, we choose to use what is called policy-based reinforcement learning. We state the formal definition of this problem as follows:</p><p>Policy Based RL Problem Goal Learn the optimal policy ? * (?|?) How? (1) Approximate a policy by parameterized ? ? (?|?).</p><p>(2) Define objective</p><formula xml:id="formula_11">? (? ) = E ? ? [? ? ? (?)]. (3) Find arg max ? ? (? ) with Stochastic Gradient.</formula><p>The goal of this optimization problem is to learn directly which action ? to take in a specific state ?. We represent the parametrized policy ? ? by a deep neural network. The main reasons for choosing this framework are as follows:</p><p>? It is model-free which is important as the placer tool environment is very complex and may be hard to model.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">How to Learn: the Actor-Critic Framework</head><p>In our chosen architecture we learn a policy that optimizes the value while learning the value simultaneously. For learning, it is often beneficial to use as much knowledge observed from the environment as possible and hang off other rather than solely predicting the policy. This type of framework called actor-critic is shown in Figure <ref type="figure" target="#fig_2">4</ref>. The policy is known as the actor, because it is used to select actions, and the estimated value function is known as the critic, because it criticizes the actions made by the actor.</p><p>Actor-critic algorithms combine value-based and policy-based methods. Value-based algorithms learn to approximate values ? w (?) ? ? ? (?) by exploiting the the Bellman equation:</p><formula xml:id="formula_12">? ? (?) = E[? ? +1 + ?? ? (? ? +1 )|? ? = ?]<label>(8)</label></formula><p>which is used in temporal difference (TD):</p><formula xml:id="formula_13">?w ? = (? ? +1 + ?? w (? ? +1 ) -? w (? ? ))? w ? w (? ? ).<label>(9)</label></formula><p>On the other hand, policy-based algorithms update a parameterized policy ? ? (? ? |? ? ) directly through stochastic gradient ascent in the direction of the value:</p><formula xml:id="formula_14">?? ? = ? ? ? ? log ? ? (? ? |? ? ).<label>(10)</label></formula><p>In actor-critic, the policy updates are computed from incomplete episodes by using truncated returns that bootstrap on the value estimate at state ? ? +? according to ? w :</p><formula xml:id="formula_15">? (?) ? = ?-1 ?=0 ? ? ? ? +?+1 + ? ? ? w (? ? +? ).<label>(11)</label></formula><p>This reduces the variance of the updates and propagates rewards faster. The variance can be further reduced using state-values as a baseline in policy updates, as in advantage actor-critic updates:</p><formula xml:id="formula_16">?? ? = (? (?) ? -? w (? ? ))? ? log ? ? (? ? |? ? ).<label>(12)</label></formula><p>The critic updates parameters w of ? w by ?-step TD (Eq. 9) and the actor updates parameters ? of ? ? in direction suggested by critic by policy gradient (Eq. 12). In this work we use the advantage actor-critic method, called A2C <ref type="bibr" target="#b11">[12]</ref>, which was shown to produce excellent results on diverse environments. As shown in Equation <ref type="formula" target="#formula_16">12</ref>, an advantage function formed as the difference between returns and baseline state-action estimate is used instead of raw returns. The advantage can be thought of as a measure of how good a given action is compared to some average.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Synchronous Actor/Critic Implementation</head><p>The main issues plaguing the use of RL in EDA are the latency of tool runs (it takes minutes to hours to perform one placement) as well as the sparsity of data (there is no database of millions of netlists, placed designs or layouts). To solve both issues, we implement a parallel version of A2C, as depicted in Figure <ref type="figure" target="#fig_3">5</ref>. In this implementation, an agent learns from experiences of multiple Actors interacting in parallel with their own copy of the environment. This configuration increases the throughput of acting and learning and helps decorrelate samples during training for data efficiency <ref type="bibr" target="#b5">[6]</ref>.</p><p>In parallel training setups, the learning updates may be applied synchronously or asynchronously. We use a synchronous version, i.e. a deterministic implementation that waits for each Actor to finish its segment of experience (according to the current policy provided by the step model) before performing a single batch update to the weights of the network. One advantage is that it provides larger batch sizes, which is more effectively used by computing resources.</p><p>The parallel training setup does not modify the equations presented before. The gradients are just accumulated among all the environments' batches.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.5">A Two-Head Network Architecture</head><p>The actor-critic framework uses both policy and value models. The full agent network can be represented as a deep neural network (? ? , ? w ) = ? (?). This neural network takes the state ? = (? ? ?) made of parameter values ? and netlist features ? and outputs a vector of action probabilities with components ? ? (?) for each action ?, and a scalar value ? w (?) estimating the expected cumulative reward ? from state ?.</p><p>The policy tells how to modify a placement parameter setting and the value network tells us how good this current setting is. We share the body of the network to allow value and policy predictions to inform one another. The parameters are adjusted by gradient ascent on a loss function that sums over the losses of the policy and the value plus a regularization term, whose gradient is defined Figure <ref type="figure">6</ref>: Overall network architecture of our agent. The combination of an LSTM with an Attention mechanism enables the learning of a complex recurrent optimization process. Table <ref type="table" target="#tab_7">4</ref> provides the details of the sub-networks used here.</p><p>as follows as in <ref type="bibr" target="#b15">[16]</ref>: </p><formula xml:id="formula_17">(?<label>(</label></formula><p>The entropy regularization pushes entropy up to encourage exploration, and ? and ? are hyper-parameters that balance the importance of the loss components.</p><p>The complete architecture of our deep neural network is shown in Figure <ref type="figure">6</ref>. To compute value and policy, the concatenation of placement parameters with graph extracted features are first passed through two feed-forward fully-connected (FC) layers with ???? activations, followed by a FC linear layer. This is followed by a Long Short-Term Memory (LSTM) module with layer normalization and with 16 hidden standard units with forget gate. The feed-forward FC layers have no memory. Introducing an LSTM in the network, which is a recurrent layer, the model can base its actions on previous states. This is motivated by the fact that traditional optimization methods are based on recurrent approaches. Moreover, we add a sequence-toone global attention mechanism <ref type="bibr" target="#b8">[9]</ref> inspired from state-of-the-art </p><formula xml:id="formula_19">? ? (?) = exp score(? ? , ? ? ) ? ? exp score(? ? , ? ? ? )<label>(14)</label></formula><p>where the alignment score function is:</p><formula xml:id="formula_20">score(? ? , ? ? ) = ? ? ? ? ? ? ? .<label>(15)</label></formula><p>The global context vector:</p><formula xml:id="formula_21">? ? = ? ? ? (?)? ?<label>(16)</label></formula><p>is combined with the hidden state to produce an attentional hidden state as follows:</p><formula xml:id="formula_22">h? = tanh ? ? [? ? ? ? ? ] .<label>(17)</label></formula><p>This hidden state is then fed to the two heads of the network, both composed of two FC layers with an output softmax layer for the policy and an output linear layer for the value. The parameters of our network are summarized in Table <ref type="table" target="#tab_7">4</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.6">Our Self-Play Strategy</head><p>Inspired from AlphaZero <ref type="bibr" target="#b12">[13]</ref>, our model learns without any supervised samples. We do not use expert knowledge to pre-train the network using good known parameter sets or actions. While the agent makes random moves at first, the idea is that by relying on zero human bias, the agent may learn counter-intuitive moves and achieve superhuman tuning capabilities.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">EXPERIMENTAL RESULTS</head><p>To train and test our agent, we select 15 benchmarks designs from OpenCores, ISPD 2012 contest and two RISC-V single cores, presented in Table <ref type="table" target="#tab_8">5</ref>. We use the first eleven for training and last four for testing. We synthesize the RTL netlists using Synopsys Design Compiler. We use TSMC 28nm technology node. The placements are done with Cadence Innovus 17.1. Aspect ratio of the floorplans is fixed to 1 and appropriate fixed clock frequencies are selected. Memory macros of RocketTile and OpenPiton Core benchmarks are pre-placed by hand. For successful placements, a lower bound of total cell area divided by floorplan area is set on parameter max density. IO pins are placed automatically by the tool between metals 4 and 6.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">RL Network Training Setting</head><p>We define our environment using OpenAI Gym interface <ref type="bibr" target="#b1">[2]</ref> and implement our RL agent network in Tensorflow. We use 16 parallel environments (16 threads) in our synchronous A2C framework.</p><p>We perform tuning of the hyperparameters of our network using Bayesian Optimization, which results in stronger agents. The learning curve of our A2C agent in our custom Innovus environment is shown in Figure <ref type="figure" target="#fig_5">7</ref>. We observe that the mean reward accross all netlists converges asymptotically to a value of 6.8%, meaning wirelength is reduced in average by 6.8%.</p><p>Training over 150 iterations (= 14, 400 placements) takes about 100 hours. Note that 99% of that time is spent to perform the placements while updating the neural network weights takes less than 20min. Without parallelization, training over the same number of placements would take 16 ? 100 hr = 27 days. An explained variance of 0.67 shows the value function explains relatively well the observed returns. We use a discount factor ? = 0.997, coefficient for the value loss ? = 0.25, entropy cost of ? = 0.01, and a learning rate of 0.0008. We use a standard noncentered RMSProp as gradient ascent optimizer. The weights are initialized using orthogonal initialization. The learning updates are batched across rollouts of 6 actor steps for 16 parallel copies of the environment, totalling a mini-batch size of 96. All experiments use gradient clipping by norm to avoid exploding gradients (phenomenom common with LSTMs), with a maximum norm of 0.5. Note that with 14, 400 placements, we only explore 10 -6 % of the total parameter space.</p><p>For a given environment, we select a random netlist and we always start with the corresponding human parameter set to form an initial state for each episode. Each environment is reset for episodes of 16 steps. Training on ? environments in parallel, each performing a placement on a different netlist, the reward signal is averaged on the ? netlist for the network updates, which decreases the reward variance and ultimately help the network generalize to unseen netlists as prescribed in <ref type="bibr" target="#b10">[11]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Netlist Training Results</head><p>For comparison, we use the state-of-the-art tool auto-tuner Open-Tuner <ref type="bibr" target="#b0">[1]</ref> that we adapt for Innovus as baseline. In this framework, a Multi-Armed Bandit (MAB) selects at each iteration a search technique among Genetic Algorithm, Simulated Annealing, Torczon, Nelder-Mead and Partial Swarm Optimization, based on a score that forces a trade-off between exploitation (use arm that worked best) and exploration (use a more rarely used arm). We run the search techniques in parallel, evaluating 16 candidate parameter sets at the same time. We run the tuner on the eleven training netlists and record the best achieved wirelength, performing 1, 300 placements per netlist so that the total number of placements equals those of the RL agent training.</p><p>Table <ref type="table" target="#tab_9">6</ref> shows the best wirelengths found by the MAB as well as RL agent during training. The human baseline is set by an experienced engineer who tuned the parameters for a day. We see that the RL agent outperforms MAB on most netlists, reducing HPWL by 9.8% on Rocket Core benchmark. All in all, both methods improve quite well on the human baseline.    </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Unseen Netlist Testing Results</head><p>To verify the ability of our agent to generalize we check its performance on the four unseen test netlists. Without any additional training (the network parameters are fixed), the RL agent iteratively improves a random initial parameter set by selecting action ? with highest predicted ? ? (?) value, as described in Equation <ref type="formula" target="#formula_7">5</ref>.</p><p>Because our actions are deterministic, the resulting set of parameters is known, and fed back to the network. We repeat this process until the estimated value decreases for 3 consecutive updates and backtrack to the settings with highest value. This way a "good" candidate parameter set is found without performing any placement.</p><p>We then perform a unique placement with that parameter set and record the obtained wirelength.</p><p>In comparison, the MAB needs the reward signal to propose a new set of parameters and therefore needs actual placements to be performed by the tool. We track the best wirelength found, allotting 50 sequential iterations to the MAB.</p><p>The best wirelength found by our RL agent and the MAB on all four test netlists is show in Table <ref type="table" target="#tab_10">7</ref>. We observe our RL agent achieves superior wirelengths consistently, performing only one placement. Table <ref type="table" target="#tab_11">8</ref> shows the best parameter settings found by MAB and RL agent on Netcard. Interestingly enough, we can see how the two optimizers found separate ways to minimize HPWL: WL driven vs. congestion driven.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">PPA Comparison After Routing</head><p>To confirm improvement in HPWL after placement is translated into one in final routed wirelength, we perform routing of placed designs. They are all routed with same settings where metal layers 1 to 6 are used. The layouts of OpenPiton Core are shown in Figure <ref type="figure" target="#fig_7">8</ref>. We verify target frequency is achieved and routing succeeded without congestion issues or DRC violations. PPA of routed designs is summarized in Table <ref type="table" target="#tab_12">9</ref>. We observe that HPWL reduction after placement is conserved after routing on all test designs, reaching 7.3% and 11% wirelength savings on LDPC and Netcard compared with the human baseline. Footprints are 74,283 ?? 2 for LDPC, 1,199,934 ?? 2 for OpenPiton, 728,871 ?? 2 for Netcard and 894,115 ?? 2 for Leon3.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">CONCLUSIONS</head><p>Our placement parameter optimizer based on deep RL provides preset of improved parameter settings without human intervention. We believe this is an important step to shift from the "CAD" mindset to the "Design Automation" mindset. We use a novel representation to formulate states and actions applied to placement optimization. Our experimental results show our agent generalizes well to unseen netlists and consistently reduces wirelength compared with a state-of-the-art tool auto-tuner, in only one iteration without any additional training.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: Reinforcement learning agent-environment interaction in the proposed methodology.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :Figure 3 :</head><label>23</label><figDesc>Figure2: Graph embedding using a graph neural network package GraphSAGE. In our experiments, we first extract 32 features for each node in the graph. Next, we calculate the mean among all nodes for each feature. In the end, we obtain 32 features for the entire graph.GNN + handcrafted features</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 4 :</head><label>4</label><figDesc>Figure4: Actor-critic framework. The critic learns about and critiques the policy currently being followed by the actor.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 5 :</head><label>5</label><figDesc>Figure 5: Synchronous parallel learner. The global network sends actions to the actors through the step model. Each actor gathers experiences from their own environment.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>7 :</head><label>7</label><figDesc>Training our agent for 150 iterations (= 14,400 placements). The reward is an aggregate reward from all training netlists. Training time is within 100 hours. Human baseline: reward = 0.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head></head><label></label><figDesc>(a) human design (took 7hrs) (b) Multi-Armed Bandit (took 16hrs) (c) reinforcement learning (took 20min)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure 8 :</head><label>8</label><figDesc>Figure 8: 28nm full-chip GDSII layouts of OpenPiton.</figDesc><graphic url="image-3.png" coords="8,135.18,53.24,75.04,90.06" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>? State transition: given a state (? ? ) and an action, the next state (? ? +1 ) is the same netlist with updated parameters. ? Reward: minus the HPWL output from the commercial EDA placement tool. The reward increases if the action improved the parameter settings in terms of minimizing wirelength.</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 1 :</head><label>1</label><figDesc>12 placement parameters we are targeting. The solution space is 6x10 9 .</figDesc><table><row><cell>Name</cell><cell>Objective</cell><cell>Type</cell><cell>Groups</cell><cell># val</cell></row><row><cell>eco max distance</cell><cell>maximum distance allowed during placement legalization</cell><cell>integer</cell><cell>detail</cell><cell>[0, 100]</cell></row><row><cell>legalization gap</cell><cell>minimum sites gap between instances</cell><cell>integer</cell><cell>detail</cell><cell>[0, 100]</cell></row><row><cell>max density</cell><cell>controls the maximum density of local bins</cell><cell>integer</cell><cell>global</cell><cell>[0, 100]</cell></row><row><cell>eco priority</cell><cell>instance priority for refine place</cell><cell>enum</cell><cell>detail</cell><cell>3</cell></row><row><cell cols="2">activity power driven level of effort for activity power driven placer</cell><cell cols="2">enum detail + effort</cell><cell>3</cell></row><row><cell>wire length opt</cell><cell>optimizes wirelength by swapping cells</cell><cell cols="2">enum detail + effort</cell><cell>3</cell></row><row><cell>blockage channel</cell><cell cols="2">creates placement blockages in narrow channels between macros enum</cell><cell>global</cell><cell>3</cell></row><row><cell>timing effort</cell><cell>level of effort for timing driven placer</cell><cell cols="2">enum global + effort</cell><cell>2</cell></row><row><cell>clock power driven</cell><cell>level of effort for clock power driven placer</cell><cell cols="2">enum global + effort</cell><cell>3</cell></row><row><cell>congestion effort</cell><cell>the effort level for relieving congestion</cell><cell cols="2">enum global + effort</cell><cell>3</cell></row><row><cell>clock gate aware</cell><cell>specifies that placement is aware of clock gate cells in the design</cell><cell>bool</cell><cell>global</cell><cell>2</cell></row><row><cell>uniform density</cell><cell>enables even cell distribution</cell><cell>bool</cell><cell>global</cell><cell>2</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 2 :</head><label>2</label><figDesc>Our 20 handcrafted netlist features.</figDesc><table><row><cell>Metadata (10)</cell><cell></cell><cell cols="2">Topological (10)</cell></row><row><cell>Name</cell><cell>Type</cell><cell>Name</cell><cell>Type</cell></row><row><cell># cells</cell><cell cols="2">integer average degree</cell><cell>float</cell></row><row><cell># nets</cell><cell cols="2">integer average fanout</cell><cell>float</cell></row><row><cell># cell pins</cell><cell>integer</cell><cell>largest SCC</cell><cell>integer</cell></row><row><cell># IO</cell><cell>integer</cell><cell>max. clique</cell><cell>integer</cell></row><row><cell cols="2"># nets w. fanout ?]5, 10[ integer</cell><cell cols="2">chromatic nb. integer</cell></row><row><cell># nets w. fanout ? 10</cell><cell cols="3">integer max. logic level integer</cell></row><row><cell># FFs</cell><cell>integer</cell><cell>RCC</cell><cell>float</cell></row><row><cell>total cell area (?? 2 )</cell><cell>integer</cell><cell>??</cell><cell>float</cell></row><row><cell># hardmacros</cell><cell>integer</cell><cell>Fiedler value</cell><cell>float</cell></row><row><cell>macro area (?? 2 )</cell><cell cols="2">integer spectral radius</cell><cell>float</cell></row></table><note><p><p><p><p>two adjacent vertices have the same color; i.e., if (?, ?) ? ? then ? (?) ? ? (?). Minimum ? (the chromatic number) is computed using a technique proposed in</p><ref type="bibr" target="#b4">[5]</ref></p>.</p>? Logic Levels: What is the maximum distance (# gates traversed) between two flip-flops? ?? = max ?,? ?? ??</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 3 :</head><label>3</label><figDesc>Our 11 actions.    </figDesc><table><row><cell>1. FLIP Booleans</cell></row><row><cell>2. UP Integers</cell></row><row><cell>3. DOWN Integers</cell></row><row><cell>4. UP Efforts</cell></row><row><cell>5. DOWN Efforts</cell></row><row><cell>6. UP Detailed</cell></row><row><cell>7. DOWN Detailed</cell></row><row><cell>8. UP Global (does not touch the bool)</cell></row><row><cell>9. DOWN Global (does not touch the bool)</cell></row><row><cell>10. INVERT-MIX timing vs. congestion vs. WL efforts</cell></row><row><cell>11. DO NOTHING</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>?</head><label></label><figDesc>Our intuition is that the optimal policy may be simple to learn and represent (e.g. keep increasing the effort) while the value of</figDesc><table><row><cell></cell><cell cols="4">current state s t</cell></row><row><cell></cell><cell cols="2">netlist</cell><cell cols="2">param.</cell></row><row><cell>placement engine environment</cell><cell>reward R t =-HPWL</cell><cell cols="2">action a t TD err.</cell><cell>agent</cell><cell>update cur. state</cell></row><row><cell></cell><cell cols="4">netlist new param.</cell></row><row><cell></cell><cell cols="4">new state s t+1</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 4 :</head><label>4</label><figDesc>Neural network parameters used in our RL agent architecture in Figure6. The number of inputs of the first FC layer is as follows: 32 from GNN, 20 from Table2, 24 one-hot encoding for the enum/bool types from Table1, and 3 integer types from Table1. RNN) focus on important parts of the recursion. Let ? ? be the hidden state of the RNN. Then the attention alignment weights ? ? with each source hidden state ? ? are defined as:</figDesc><table><row><cell>Part</cell><cell>Input</cell><cell>Hidden</cell><cell>Output</cell></row><row><cell>1. Shared Body</cell><cell>79</cell><cell cols="2">(64, 32) (tanh) 16 (linear)</cell></row><row><cell>2. LSTM (6 unroll)</cell><cell>16</cell><cell>16</cell><cell>16 ? 6</cell></row><row><cell>3. Attention</cell><cell>16 ? 6</cell><cell>? ? , ? ?</cell><cell>16</cell></row><row><cell>4. Policy</cell><cell>16</cell><cell cols="2">(32, 32) (tanh) 11 (softmax)</cell></row><row><cell>5. Value</cell><cell>16</cell><cell>(32, 16) (tanh)</cell><cell>1 (linear)</cell></row><row><cell cols="4">Natural Language Processing architectures, to help the Recurrent</cell></row><row><cell>Layer (</cell><cell></cell><cell></cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>Table 5 :</head><label>5</label><figDesc>Benchmark statistics based on a commercial 28nm technology. RCC is the Rich Club Coefficient (? -4 ), LL is the maximum logic level and Sp. R. denotes the Spectral Radius. RT is the average placement runtime using Innovus (in minutes).</figDesc><table><row><cell>Name</cell><cell cols="7">#cells #nets #IO ??? 3 LL Sp. R. RT</cell></row><row><cell></cell><cell></cell><cell cols="2">training set</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>PCI</cell><cell cols="3">1.2K 1.4K 361</cell><cell cols="2">510 17</cell><cell cols="2">25.6 0.5</cell></row><row><cell>DMA</cell><cell>10K</cell><cell>11K</cell><cell>959</cell><cell>65</cell><cell>25</cell><cell>26.4</cell><cell>1</cell></row><row><cell>B19</cell><cell>33K</cell><cell>34K</cell><cell>47</cell><cell>19</cell><cell>86</cell><cell>36.1</cell><cell>2</cell></row><row><cell>DES</cell><cell>47K</cell><cell>48K</cell><cell>370</cell><cell>14</cell><cell>16</cell><cell>25.6</cell><cell>2</cell></row><row><cell>VGA</cell><cell>52K</cell><cell>52K</cell><cell>184</cell><cell>15</cell><cell>25</cell><cell>26.5</cell><cell>3</cell></row><row><cell>ECG</cell><cell>83K</cell><cell cols="2">84K 1.7K</cell><cell>7.5</cell><cell>23</cell><cell>26.8</cell><cell>4</cell></row><row><cell>Rocket</cell><cell>92K</cell><cell>95K</cell><cell>377</cell><cell>8.1</cell><cell cols="2">42 514.0</cell><cell>6</cell></row><row><cell>AES</cell><cell cols="3">112K 112K 390</cell><cell>5.8</cell><cell cols="2">14 102.0</cell><cell>6</cell></row><row><cell>Nova</cell><cell cols="3">153K 155K 174</cell><cell>4.6</cell><cell cols="3">57 11,298 9</cell></row><row><cell>Tate</cell><cell cols="3">187K 188K 1.9K</cell><cell>3.2</cell><cell>21</cell><cell>25.9</cell><cell>10</cell></row><row><cell>JPEG</cell><cell cols="2">239K 267K</cell><cell>67</cell><cell>2.8</cell><cell cols="3">30 287.0 12</cell></row><row><cell></cell><cell cols="4">test set (unseen netlist)</cell><cell></cell><cell></cell><cell></cell></row><row><cell>LDPC</cell><cell>39K</cell><cell cols="2">41K 4.1K</cell><cell>18</cell><cell cols="2">19 328.0</cell><cell>2</cell></row><row><cell cols="4">OpenPiton 188K 196K 1.6K</cell><cell>3.9</cell><cell cols="3">76 3940 19</cell></row><row><cell>Netcard</cell><cell cols="3">300K 301K 1.8K</cell><cell>2.9</cell><cell>32</cell><cell>27.3</cell><cell>24</cell></row><row><cell>Leon3</cell><cell cols="3">326K 327K 333</cell><cell>2.4</cell><cell>44</cell><cell>29.5</cell><cell>26</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head>Table 6 :</head><label>6</label><figDesc>Comparison of half-perimeter bounding box (HPWL) after placement on training netlists among human design, Multi-Armed Bandit (MAB)<ref type="bibr" target="#b0">[1]</ref>, and our RL-based method. HPWL is reported in ?. ? denotes percentage negative improvement over human design.</figDesc><table><row><cell>Netlist</cell><cell cols="2">human MAB [1] (?%)</cell><cell>RL (?%)</cell></row><row><cell>PCI</cell><cell>0.010</cell><cell>0.0092 (-8.0%)</cell><cell>0.0092 (-8.0%)</cell></row><row><cell>DMA</cell><cell>0.149</cell><cell>0.139 (-6.7%)</cell><cell>0.135 (-9.4%)</cell></row><row><cell>B19</cell><cell>0.30</cell><cell>0.28 (-6.7%)</cell><cell>0.28 (-6.7%)</cell></row><row><cell>DES</cell><cell>0.42</cell><cell>0.37 (-11.9%)</cell><cell>0.36 (-14.3%)</cell></row><row><cell>VGA</cell><cell>1.52</cell><cell>1.40 (-7.9%)</cell><cell>1.41 (-7.2%)</cell></row><row><cell>ECG</cell><cell>0.72</cell><cell>0.65 (-9.7%)</cell><cell>0.68 (-5.5%)</cell></row><row><cell>Rocket</cell><cell>1.33</cell><cell>1.27 (-4.5%)</cell><cell>1.20 (-9.8%)</cell></row><row><cell>AES</cell><cell>1.49</cell><cell>1.44 ( -2.7%)</cell><cell>1.40 (-6.0%)</cell></row><row><cell>AVC-Nova</cell><cell>1.59</cell><cell>1.49 (-6.3%)</cell><cell>1.46 (-8.2%)</cell></row><row><cell>Tate</cell><cell>1.53</cell><cell>1.42 (-7.2%)</cell><cell>1.45 (-5.2%)</cell></row><row><cell>JPEG</cell><cell>2.14</cell><cell>1.96 (-8.4%)</cell><cell>1.88 (-12.2%)</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_10"><head>Table 7 :</head><label>7</label><figDesc>Comparison on test netlists of best wirelength found (one iteration = one placement performed). HPWL is reported in ?.</figDesc><table><row><cell cols="3">Netlist human #iter. MAB [1] #iter.</cell><cell>RL</cell><cell>#iter.</cell></row><row><cell>LDPC</cell><cell>1.14</cell><cell cols="2">20 1.04 (-8.8%) 50 1.02 (-10.5%) 1</cell></row><row><cell cols="2">OpenPt 5.26</cell><cell cols="2">20 5.11 (-2.9%) 50 4.99 (-5.1%) 1</cell></row><row><cell cols="2">Netcard 4.88</cell><cell cols="2">20 4.45 (-8.8%) 50 4.34 (-11.1%) 1</cell></row><row><cell>Leon3</cell><cell>3.52</cell><cell cols="2">20 3.37 (-4.3%) 50 3.29 (-6.5%) 1</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_11"><head>Table 8 :</head><label>8</label><figDesc>Best placement parameters found for Netcard benchmark.</figDesc><table><row><cell>Name</cell><cell cols="2">MAB [1] RL</cell></row><row><cell>eco max distance</cell><cell>54</cell><cell>81</cell></row><row><cell>legalization gap</cell><cell>1</cell><cell>5</cell></row><row><cell>max density</cell><cell>0.92</cell><cell>0.94</cell></row><row><cell>eco priority</cell><cell>eco</cell><cell>fixed</cell></row><row><cell>activity power driven</cell><cell>none</cell><cell>none</cell></row><row><cell>wire length opt</cell><cell>high</cell><cell>none</cell></row><row><cell>blockage channel (for macros)</cell><cell>none</cell><cell>soft</cell></row><row><cell>timing effort</cell><cell cols="2">medium high</cell></row><row><cell>clock power driven</cell><cell>none</cell><cell>none</cell></row><row><cell>congestion effort</cell><cell>low</cell><cell>high</cell></row><row><cell>clock gate aware</cell><cell>true</cell><cell>true</cell></row><row><cell>uniform density</cell><cell>false</cell><cell>false</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_12"><head>Table 9 :</head><label>9</label><figDesc>PPA comparison after routing on test set. The target frequencies are 1GHz, 500MHz, 833MHz, 666MHz from top to bottom.</figDesc><table><row><cell>ckt</cell><cell>metric</cell><cell cols="2">human MAB [1]</cell><cell>RL</cell></row><row><cell></cell><cell>WL (?)</cell><cell>1.65</cell><cell>1.57</cell><cell>1.53</cell></row><row><cell>LDPC</cell><cell>WNS (??)</cell><cell>-0.005</cell><cell>-0.001</cell><cell>-0.001</cell></row><row><cell></cell><cell>Power (?? )</cell><cell>162.10</cell><cell>156.49</cell><cell>153.77</cell></row><row><cell></cell><cell>WL (?)</cell><cell>6.31</cell><cell>6.24</cell><cell>6.10</cell></row><row><cell>OpenPiton</cell><cell>WNS (??)</cell><cell>-0.003</cell><cell>-0.001</cell><cell>0</cell></row><row><cell></cell><cell>Power (?? )</cell><cell>192.08</cell><cell>190.95</cell><cell>189.72</cell></row><row><cell></cell><cell>WL (?)</cell><cell>8.01</cell><cell>7.44</cell><cell>7.15</cell></row><row><cell>NETCARD</cell><cell>WNS (??)</cell><cell>-0.006</cell><cell>-0.007</cell><cell>-0.004</cell></row><row><cell></cell><cell>Power (?? )</cell><cell>174.05</cell><cell>170.51</cell><cell>167.70</cell></row><row><cell></cell><cell>WL (?)</cell><cell>5.66</cell><cell>5.53</cell><cell>5.41</cell></row><row><cell>LEON3</cell><cell>WNS (??)</cell><cell>-0.005</cell><cell>-0.001</cell><cell>-0.003</cell></row><row><cell></cell><cell>Power (?? )</cell><cell>156.83</cell><cell>156.00</cell><cell>155.51</cell></row></table></figure>
		</body>
		<back>

			<div type="funding">
<div><p>This material is based upon work supported by the <rs type="funder">National Science Foundation</rs> under Grant No. <rs type="grantNumber">CNS 16-24731</rs> and the industry members of the <rs type="institution">Center for Advanced Electronics in Machine Learning</rs>.</p></div>
			</div>
			<listOrg type="funding">
				<org type="funding" xml:id="_hqDeMbR">
					<idno type="grant-number">CNS 16-24731</idno>
				</org>
			</listOrg>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">OpenTuner: An Extensible Framework for Program Autotuning</title>
		<author>
			<persName><forename type="first">J</forename><surname>Ansel</surname></persName>
		</author>
		<imprint>
			<biblScope unit="page">14</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title/>
		<author>
			<persName><forename type="first">G</forename><surname>Brockman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">OpenAI Gym</title>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Algorithm 457: Finding All Cliques of an Undirected Graph</title>
		<author>
			<persName><forename type="first">C</forename><surname>Bron</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Kerbosch</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Commun. ACM</title>
		<imprint>
			<date type="published" when="1973">1973</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Detecting rich-club ordering in complex networks</title>
		<author>
			<persName><forename type="first">V</forename><surname>Colizza</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nature Physics</title>
		<imprint>
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">Exact Coloring Of Real-life Graphs Is Easy</title>
		<author>
			<persName><forename type="first">O</forename><surname>Coudert</surname></persName>
		</author>
		<imprint>
			<biblScope unit="page">97</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">IMPALA: Scalable Distributed Deep-RL with Importance Weighted Actor-Learner Architectures</title>
		<author>
			<persName><forename type="first">L</forename><surname>Espeholt</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">Inductive Representation Learning on Large Graphs</title>
		<author>
			<persName><forename type="first">W</forename><surname>Hamilton</surname></persName>
		</author>
		<imprint>
			<biblScope unit="page">17</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Deflation Techniques for an Implicitly Restarted Arnoldi Iteration</title>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">B</forename><surname>Lehoucq</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">C</forename><surname>Sorensen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">SIAM J. Matrix Analysis Applications</title>
		<imprint>
			<biblScope unit="volume">17</biblScope>
			<biblScope unit="page" from="789" to="821" />
			<date type="published" when="1996">1996</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">Effective Approaches to Attention-based Neural Machine Translation</title>
		<author>
			<persName><forename type="first">M.-T</forename><surname>Luong</surname></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">Progress and Challenges in VLSI Placement Research</title>
		<author>
			<persName><forename type="first">I</forename><forename type="middle">L</forename><surname>Markov</surname></persName>
		</author>
		<imprint>
			<biblScope unit="page">12</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">Chip Placement with Deep Reinforcement Learning</title>
		<author>
			<persName><forename type="first">A</forename><surname>Mirhoseini</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">Asynchronous Methods for Deep Reinforcement Learning</title>
		<author>
			<persName><forename type="first">V</forename><surname>Mnih</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">Mastering Chess and Shogi by Self-Play with a General Reinforcement Learning Algorithm</title>
		<author>
			<persName><forename type="first">D</forename><surname>Silver</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Depth-First Search and Linear Graph Algorithms</title>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">E</forename><surname>Tarjan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">SIAM J. Comput</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="146" to="160" />
			<date type="published" when="1972">1972</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">Visualizing Data using t-SNE</title>
		<author>
			<persName><forename type="first">L</forename><surname>Van Der Maaten</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<author>
			<persName><forename type="first">O</forename><surname>Vinyals</surname></persName>
		</author>
		<title level="m">StarCraft II: A New Challenge for Reinforcement Learning</title>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Collective dynamics of &apos;small-world&apos; networks</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">J</forename><surname>Watts</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">H</forename><surname>Strogatz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nature</title>
		<imprint>
			<date type="published" when="1998">1998</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">A Parallel Bandit-Based Approach for Autotuning FPGA Compilation</title>
		<author>
			<persName><forename type="first">C</forename><surname>Xu</surname></persName>
		</author>
		<imprint>
			<biblScope unit="page">17</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">S2FA: An Accelerator Automation Framework for Heterogeneous Computing in Datacenters</title>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">H</forename><surname>Yu</surname></persName>
		</author>
		<idno>DAC &apos;18</idno>
		<imprint/>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
