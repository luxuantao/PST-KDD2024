<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Black-Box Adversarial Attacks on Graph Neural Networks with Limited Node Access</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date type="published" when="2020-06-09">9 Jun 2020</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Jiaqi</forename><surname>Ma</surname></persName>
							<email>jiaqima@umich.edu</email>
							<affiliation key="aff0">
								<orgName type="department">School of Information</orgName>
								<orgName type="institution">University of Michigan</orgName>
								<address>
									<settlement>Ann Arbor</settlement>
									<region>Michigan</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Qiaozhu</forename><surname>Mei</surname></persName>
							<email>qmei@umich.edu</email>
							<affiliation key="aff0">
								<orgName type="department">School of Information</orgName>
								<orgName type="institution">University of Michigan</orgName>
								<address>
									<settlement>Ann Arbor</settlement>
									<region>Michigan</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="department">Department of EECS</orgName>
								<orgName type="institution">University of Michigan</orgName>
								<address>
									<settlement>Ann Arbor</settlement>
									<region>Michigan</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Black-Box Adversarial Attacks on Graph Neural Networks with Limited Node Access</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2020-06-09">9 Jun 2020</date>
						</imprint>
					</monogr>
					<idno type="arXiv">arXiv:2006.05057v1[cs.LG]</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.2" ident="GROBID" when="2022-12-25T13:41+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>We study the black-box attacks on graph neural networks (GNNs) under a novel and realistic constraint: attackers have access to only a subset of nodes in the network, and they can only attack a small number of them. A node selection step is essential under this setup. We demonstrate that the structural inductive biases of GNN models can be an effective source for this type of attacks. Specifically, by exploiting the connection between the backward propagation of GNNs and random walks, we show that the common gradient-based white-box attacks can be generalized to the black-box setting via the connection between the gradient and an importance score similar to PageRank. In practice, we find attacks based on this importance score indeed increase the classification loss by a large margin, but they fail to significantly increase the mis-classification rate. Our theoretical and empirical analyses suggest that there is a discrepancy between the loss and mis-classification rate, as the latter presents a diminishing-return pattern when the number of attacked nodes increases. Therefore, we propose a greedy procedure to correct the importance score that takes into account of the diminishing-return pattern. Experimental results show that the proposed procedure can significantly increase the mis-classification rate of common GNNs on real-world data without access to model parameters nor predictions.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Graph neural networks (GNNs) <ref type="bibr" target="#b19">[20]</ref>, the family of deep learning models on graphs, have shown promising empirical performance on various applications of machine learning to graph data, such as recommender systems <ref type="bibr" target="#b24">[25]</ref>, social network analysis <ref type="bibr" target="#b10">[11]</ref>, and drug discovery <ref type="bibr" target="#b14">[15]</ref>. Like other deep learning models, GNNs have also been shown to be vulnerable under adversarial attacks <ref type="bibr" target="#b27">[28]</ref>, which has recently attracted increasing research interest <ref type="bibr" target="#b7">[8]</ref>. Indeed, adversarial attacks have been an efficient tool to analyze both the theoretical properties as well as the practical accountability of graph neural networks. As graph data have more complex structures than image or text data, researchers have come up with diverse adversarial attack setups. For example, there are different tasks (node classification and graph classification), assumptions of attacker's knowledge (white-box, grey-box, and black-box), strategies (node feature modification and graph structure modification), and corresponding budget or other constraints (norm of feature changes or number of edge changes). Despite these research efforts, there is still a considerable gap between the existing attack setups and the reality. It is unreasonable to assume that an attacker can alter the input of a large proportion of nodes, and even if there is a budget limit, it is unreasonable to assume that they can attack any node as they wish. For example, in a real-world social network, the attackers usually only have access to a few bot accounts, and they are unlikely to be among the top nodes in the network; it is difficult for 2 Related Work</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Adversarial Attack on GNNs</head><p>The study of adversarial attacks on graph neural networks has surged recently. A taxonomy of existing work has been summarized by Jin et al. <ref type="bibr" target="#b7">[8]</ref>, and we give a brief introduction here. First, there are two types of machine learning tasks on graphs that are commonly studied, node-level classification and graph-level classification. We focus on the node-level classification in this paper. Next, there are a couple of choices of the attack form. For example, the attack can happen either during model training (poisoning) or during model testing (evasion); the attacker may aim to mislead the prediction on specific nodes (targeted attack) <ref type="bibr" target="#b27">[28]</ref> or damage the overall task performance (untargeted attack) <ref type="bibr" target="#b26">[27]</ref>; the adversarial perturbation can be done by modifying node features, adding or deleting edges, or injecting new nodes <ref type="bibr" target="#b15">[16]</ref>. Our work belongs to untargeted evasion attacks. For the adversarial perturbation, most existing works of untargeted attacks apply global constraints on the proportion of node features or the number of edges to be altered. Our work sets a novel local constraint on node access, which is more realistic in practice: perturbation on top (e.g., celebrity) nodes is prohibited and only a small number of nodes can be perturbed. Finally, depending on the attacker's knowledge about the GNN model, existing work can be split into three categories: white-box attacks <ref type="bibr" target="#b20">[21,</ref><ref type="bibr" target="#b3">4,</ref><ref type="bibr" target="#b18">19]</ref> have access to full information about the model, including model parameters, input data, and labels; grey-box attacks <ref type="bibr" target="#b26">[27,</ref><ref type="bibr" target="#b27">28,</ref><ref type="bibr" target="#b15">16]</ref> have partial information about the model and the exact setups vary in a range; in the most challenging setting, black-box attacks <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b0">1,</ref><ref type="bibr" target="#b2">3]</ref> can only access the input data and sometimes the black-box predictions of the model. In this work, we consider an even more strict black-box attack setup, where model predictions are invisible to the attackers. As far as we know, the only existing works that conduct untargeted black-box attacks without access to model predictions are those by Bojchevski and Günnemann <ref type="bibr" target="#b0">[1]</ref> and Chang et al. <ref type="bibr" target="#b2">[3]</ref>. However both of them require the access to embeddings of nodes, which are prohibited as well in our setup.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Structural Inductive Bias of GNNs</head><p>While having an extremely restricted black-box setup, we demonstrate that effective adversarial attacks are still possible due to the strong and explicit structural inductive biases of GNNs.</p><p>Structural inductive biases refer to the structures encoded by various neural architectures, such as the weight sharing mechanisms in convolution kernels of convolutional neural networks, or the gating mechanisms in recurrent neural networks. Such neural architectures have been recognized as a key factor for the success of deep learning models <ref type="bibr" target="#b25">[26]</ref>, which (partially) motivate some recent developments of neural architecture search <ref type="bibr" target="#b25">[26]</ref>, Bayesian deep learning <ref type="bibr" target="#b17">[18]</ref>, Lottery Ticket Hypothesis <ref type="bibr" target="#b5">[6]</ref>, etc. The natural graph structure and the heavy weight sharing mechanism grant GNN models even more explicit structural inductive biases. Indeed, GNN models have been theoretically shown to share similar behaviours as Weisfeiler-Lehman tests <ref type="bibr" target="#b12">[13,</ref><ref type="bibr" target="#b21">22]</ref> or random walks <ref type="bibr" target="#b22">[23]</ref>. On the positive side, such theoretical analyses have led to better GNN model designs <ref type="bibr" target="#b22">[23,</ref><ref type="bibr" target="#b9">10]</ref>.</p><p>Our work instead studies the negative impact of the structural inductive biases in the context of adversarial attacks: when the graph structure is exposed to the attacker, such structural information can turn into the knowledge source for an attack. While most existing attack strategies more-or-less utilize some structural properties of GNNs, they are utilized in a data-driven manner which requires querying the GNN model, e.g., learning to edit the graph via a trial-and-error interaction with the GNN model <ref type="bibr" target="#b4">[5]</ref>. We formally establish connections between the structural properties and attack strategies without any queries to the GNN model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Principled Black-Box Attack Strategies with Limited Node Access</head><p>In this section, we derive principled strategies to attack GNNs under the novel black-box setup with limited node access. We first analyze the corresponding white-box attack problem in Section 3.2 and then adapt the theoretical insights from the white-box setup to the black-box setup and propose a black-box attack strategy in Section 3.3. Finally, in Section 3.4, we correct the proposed strategy by taking into account of the diminishing-return effect for the mis-classification rate.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Preliminary Notations</head><p>We first introduce necessary notations. We denote a graph as G = (V, E), where V = {1, 2, . . . , N } is the set of N nodes, and E ⊆ V × V is the set of edges. For a node classification problem, the nodes of the graph are collectively associated with node features X ∈ R N ×D and labels y ∈ {1, 2, . . . , K} N , where D is the dimensionality of the feature vectors and K is the number of classes. Each node i's local neighborhood including itself is denoted as N i = {j ∈ V | (i, j) ∈ E} ∪ {i}, and its degree as d i = |N i |. To ease the notation, for any matrix A ∈ R D1×D2 in this paper, we refer A j to the transpose of the j-th row of the matrix, i.e., A j ∈ R D2 . GNN models. Given the graph G, a GNN model is a function f G : R N ×D → R N ×K that maps the node features X to output logits of each node. We denote the output logits of all nodes as a matrix H ∈ R N ×K and H = f G (X). A GNN f G is usually built by stacking a certain number (L) of layers, with the l-th layer, 1 ≤ l ≤ L, taking the following form:</p><formula xml:id="formula_0">H (l) i = σ   j∈Ni α ij W l H (l−1) j   ,<label>(1)</label></formula><p>where H (l) ∈ R N ×D l is the hidden representation of nodes with D l dimensions, output by the l-th layer; W l is a learnable linear transformation matrix; σ is an element-wise nonlinear activation function; and different GNNs have different normalization terms α ij . For instance, α ij = 1/ d i d j or α ij = 1/d i in Graph Convolutional Networks (GCN) <ref type="bibr" target="#b8">[9]</ref>. In addition, H (0) = X and H = H (L) .</p><p>Random walks. A random walk <ref type="bibr" target="#b11">[12]</ref> on G is specified by the matrix of transition probabilities, M ∈ R N ×N , where</p><formula xml:id="formula_1">M ij = 1/d i , if (i, j) ∈ E or j = i, 0,</formula><p>otherwise. Each M ij represents the probability of transiting from i to j at any given step of the random walk. And powering the transition matrix by t gives us the t-step transition matrix M t .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">White-Box Adversarial Attacks with Limited Node Access</head><p>Problem formulation. Given a classification loss L : R N ×K × {1, . . . , K} N → R, the problem of white-box attack with limited node access can be formulated as an optimization problem as follows:</p><formula xml:id="formula_2">max S⊆V L(H, y)<label>(2)</label></formula><formula xml:id="formula_3">subject to |S| ≤ r, d i ≤ m, ∀i ∈ S H = f (τ (X, S)),</formula><p>where r, m ∈ Z + respectively specify the maximum number of nodes and the maximum degree of nodes that can be attacked. Intuitively, we treat high-degree nodes as a proxy of celebrity accounts in a social network. For simplicity, we have omitted the subscript G of the learned GNN classifier f G . The function τ : R N ×D × 2 V → R N ×D perturbs the feature matrix X based on the selected node set S (i.e., attack set). Under the white-box setup, theoretically τ can also be optimized to maximize the loss. However, as our goal is to study the node selection strategy under the black-box setup, we set τ as a pre-determined function. In particular, we define the j-th row of the output of</p><formula xml:id="formula_4">τ as τ (X, S) j = X j + 1[j ∈ S]</formula><p>, where ∈ R D is a small constant noise vector constructed by attackers' domain knowledge about the features. In other words, the same small noise vector is added to the features of every attacked node.</p><p>We use the Carlili-Wagner loss for our analysis, a close approximation of cross-entropy loss and has been used in the analysis of adversarial attacks on image classifiers <ref type="bibr" target="#b1">[2]</ref>:</p><formula xml:id="formula_5">L(H, y) N j=1 L j (H j , y j ) N j=1 max k∈{1,...,K} H jk − H jyj .<label>(3)</label></formula><p>The change of loss under perturbation. Next we investigate how the overall loss changes when we select and perturb different nodes. We define the change of loss when perturbing the node i as a function of the perturbed feature vector x:</p><formula xml:id="formula_6">∆ i (x) = L(f (X ), y) − L(f (X), y), where X i = x and X j = X j , ∀j = i.</formula><p>To concretize the analysis, we consider the GCN model with α ij = 1 di in our following derivations. Suppose f is an L-layer GCN. With the connection between GCN and random walk <ref type="bibr" target="#b22">[23]</ref> and Assumption 1 on the label distribution, we can show that, in expectation, the first-order Taylor approximation</p><formula xml:id="formula_7">∆ i (x) ∆ i (X i ) + (∇ x ∆ i (X i )) T (x − X i</formula><p>) is related to the sum of the i-th column of the L-step random walk transition matrix M L . We formally summarize this finding in Proposition 1. Assumption 1 (Label Distribution). Assume the distribution of the labels of all nodes follows the same constant categorical distribution, i.e.,</p><formula xml:id="formula_8">Pr[y j = k] = q k , ∀j = 1, 2, . . . , N,</formula><p>where 0 &lt; q k &lt; 1 for k = 1, 2, . . . , K and K k=1 q k = 1. Moreover, since the classifier f has been well-trained and fixed, the prediction of f should capture certain relationships among the K classes. Specifically, we assume the chance for f predicting any node j as any class k ∈ {1, . . . , K}, conditioned on the node label y j = l ∈ {1, . . . , K}, confines to a certain distribution p(k | l), i.e., Pr argmax c∈{1,...,K}</p><formula xml:id="formula_9">H jc = k | y j = l = p(k | l).</formula><p>Proposition 1. For an L-layer GCN model, if Assumption 1 and a technical assumption about the GCN<ref type="foot" target="#foot_0">4</ref> hold, then</p><formula xml:id="formula_10">δ i E ∆ i (x) | x=τ (X,{i})i = C N j=1 [M L ] ji ,</formula><p>where C is a constant independent of i.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Adaptation from the White-Box Setup to the Black-Box Setup</head><p>Now we turn to the black-box setup where we have no access to the model parameters or predictions. This means we are no longer able to evaluate the objective function L(H, y) of the optimization problem (2). Proposition 1 shows that the relative ratio of δ i /δ j between different nodes i = j only depends on the random walk transition matrix, which we can easily calculate based on the graph G. This implies that we can still approximately optimize the problem (2) in the black-box setup.</p><p>Node selection with importance scores. Consider the change of loss under the perturbation of a set of nodes S. If we write the change of loss as a function of the perturbed features and take the first order Taylor expansion, which we denote as δ, we have δ = i∈S δ i . Therefore δ is maximized by the set of r nodes with degrees less than m and the largest possible δ i , where m, r are the limits of node access defined in the problem <ref type="bibr" target="#b1">(2)</ref>. Therefore, we can define an importance score for each node i as the sum of the i-th column of M L , i.e.,</p><formula xml:id="formula_11">I i = N j=1 [M L</formula><p>] ji , and simply select the nodes with the highest importance scores to attack. We denote this strategy as RWCS (Random Walk Column Sum). We note that RWCS is similar to PageRank. The difference between RWCS and PageRank is that the latter uses the stationary transition matrix M ∞ for a random walk with restart.</p><p>Empirically, RWCS indeed significantly increases the classification loss (as shown in Section 4.2). The nonlinear loss actually increases linearly w.r.t. the perturbation strength (the norm of the perturbation noise ) for a wide range, which indicates that ∆ i is a good approximation of ∆ i . Surprisingly, RWCS fails to continue to increase the mis-classification rate (which matters more in real applications) when the perturbation strength becomes larger. Details of this empirical finding are shown in Figure <ref type="figure" target="#fig_0">1</ref> in Section 4.2. We conduct additional formal analyses on the mis-classification rate in the following section and find a diminishing-return effect of adding more nodes to the attack set when the perturbation strength is adequate.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Diminishing-Return of Mis-classification Rate and its Correction</head><p>Analysis of the diminishing-return effect. Our analysis is based on the investigation that each target node i ∈ V will be mis-classified as we increase the attack set.</p><p>To assist the analysis, we first define the concepts of vulnerable function and vulnerable set below. Definition 1 (Vulnerable Function). We define the vulnerable function g i : 2 V → {0, 1} of a target node i ∈ V as, for a given attack set S ⊆ V ,</p><formula xml:id="formula_12">g i (S) = 1, if i is mis-classified when attacking S, 0, if i is correctly-classified when attacking S.</formula><p>Definition 2 (Vulnerable Set). We define the vulnerable set of a target node i ∈ V as a set of all attack sets that could lead i to being mis-classified:</p><formula xml:id="formula_13">A i {S ⊆ V | g i (S) = 1}.</formula><p>We also make the following assumption about the vulnerable function. Assumption 2. g i is non-decreasing for all i ∈ V , i.e., if T ⊆ S ⊆ V , then g i (T ) ≤ g i (S).</p><p>With the definitions above, the mis-classification rate can be written as the average of the vulnerable functions: h(S) = 1 N N i=1 g i (S). By Assumption 2, h is also clearly non-decreasing. We further define the basic vulnerable set to characterize the minimal attack sets that can lead a target node to being mis-classified.</p><formula xml:id="formula_14">Definition 3 (Basic Vulnerable Set). ∀i ∈ V , we call B i ⊆ A i a basic vulnerable set of i if, 1) ∅ / ∈ B i ; if ∅ ∈ A i , B i = ∅; 2) if ∅ / ∈ A i , for any nonempty S ∈ A i , there exists a T ∈ B i s.t. T ⊆ S; 3) for any distinct S, T ∈ B i , |S ∩ T | &lt; min(|S|, |T |).</formula><p>And the existence of such a basic vulnerable set is guaranteed by Proposition 2. Proposition 2. For any i ∈ V , there exists a unique B i .</p><p>The distribution of the sizes of the element sets of B i is closely related to the perturbation strength on the features. When the perturbation is small, we may have to perturb multiple nodes before the target node is mis-classified, and thus the element sets of B i will be large. When perturbation is relatively large, we may be able to turn a target node to be mis-classified by perturbing a single node, if chosen wisely. In this case B i will have a lot of singleton sets.</p><p>Our following analysis (Proposition 3) shows that h has a diminishing-return effect if the vulnerable sets of nodes on the graph present homophily (Assumption 3), which is common in real-world networks, and the perturbation on features becomes considerably large (Assumption 4). Assumption 3 (Homophily). ∀S ∈ ∪ N i=1 A i and |S| &gt; 1, there are b(S) ≥ 1 nodes s.t., for any node j among these nodes, S ∈ A j .</p><p>Intuitively, the vulnerable sets present strong homophily if b(S)'s are large. Assumption 4 (Considerable Perturbation). ∀S ∈ ∪ N i=1 A i and if |S| &gt; 1, then there are p(S)•b(S) nodes s.t., for any node j among these nodes, there exists a set T ⊆ S, |T | = 1, and T ∈ A j . And r r+1 &lt; p(S) ≤ 1. Proposition 3. If Assumptions 3 and 4 hold, h is γ-approximately submodular for some 0 &lt; γ &lt; 1 r , i.e., there exists a non-decreasing submodular function h :</p><formula xml:id="formula_15">2 V → R + , s.t. ∀S ⊆ V , (1 − γ) h(S) ≤ h(S) ≤ (1 + γ) h(S).</formula><p>As greedy methods are guaranteed to enjoy a constant approximation ratio for such approximately submodular functions <ref type="bibr" target="#b6">[7]</ref>, Proposition 3 motivates us to develop a greedy correction procedure to compensate the diminishing-return effect when calculating the importance scores.</p><p>The greedy correction procedure. We propose an iterative node selection procedure and apply two greedy correction steps on top of the RWCS strategy, motivated by Assumption 3 and 4.</p><p>To accommodate Assumption 3, after each node is selected into the attack set, we exclude a k-hop neighborhood of the selected node for next iteration, for a given constant integer k. The intuition is that nodes in a local neighborhood may contribute to similar target nodes due to homophily. To accommodate Assumption 4, we adopt an adaptive version of RWCS scores. First, we binarize the L-step random walk transition matrix M L as M , i.e.,</p><formula xml:id="formula_16">M ij = 1, if [M L ] ij is among Top-l of [M L ] i and [M L ] ij = 0, 0, otherwise,<label>(4)</label></formula><p>where l is a given constant integer. Next, we define a new adaptive influence score as a function of a matrix Q:</p><formula xml:id="formula_17">I i (Q) = N j=1 [Q] ji .</formula><p>In the iterative node selection procedure, we initialize Q as M . We select the node with highest score I i (Q) subsequently. After each iteration, suppose we have selected the node i in this iteration, we will update Q by setting to zero for all the rows where the elements of the i-th column are 1. The underlying assumption of this operation is that, adding i to the selected set is likely to mis-classify all the target nodes corresponding to the aforementioned rows, which complies Assumption 4. We name this iterative procedure as the GC-RWCS (Greedily Corrected RWCS) strategy, and summarize it in Algorithm 1 in Appendix A.3.</p><p>Finally, we want to mention that, while the derivation of RWCS and GC-RWCS requires the knowledge of the number of layers L for GCN, we find that the empirical performance of the proposed attack strategies are not sensitive w.r.t. the choice of L. Therefore, the proposed methods are applicable to the black-box setup where we do not know the exact L of the model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experiments</head><p>4.1 Experiment Setup GNN models. We evaluate the proposed attack strategies on two common GNN models, GCN <ref type="bibr" target="#b8">[9]</ref> and JK-Net <ref type="bibr" target="#b22">[23]</ref>. For JK-Net, we test on its two variants, JKNetConcat and JKNetMaxpool, which apply concatenation and element-wise max at last layer respectively. We set the number of layers for GCN as 2 and the number of layers for both JK-Concat and JK-Maxpool as 7. The hidden size of each layer is 32. For the training, we closely follow the hyper-parameter setup in Xu et al. <ref type="bibr" target="#b22">[23]</ref>.</p><p>Datasets. We adopt three citation networks, Citeseer, Cora, and Pubmed, which are standard node classification benchmark datasets <ref type="bibr" target="#b23">[24]</ref>. Following the setup of JK-Net <ref type="bibr" target="#b22">[23]</ref>, we randomly split each dataset by 60%, 20%, and 20% for training, validation, and testing. And we draw 40 random splits.</p><p>Baseline methods for comparison. As we summarized in Section 2.1, our proposed black-box adversarial attack setup is by far the most restricted, and none of existing attack strategies for GNN can be applied. We compare the proposed attack strategies with baseline strategies by selecting nodes with top centrality metrics. We compare with three well-known network metrics capturing different aspects of node centrality: Degree, Betweenness, and PageRank and name the attack strategies correspondingly. In classical network analysis literature <ref type="bibr" target="#b13">[14]</ref>, real-world networks are shown to be fragile under attacks to high-centrality nodes. Therefore we believe these centrality metrics serve as reasonable baselines under our restricted black-box setup. For the purpose of sanity check, we also include a trivial baseline Random, which randomly selects the nodes to be attacked.</p><p>Hyper-parameters for GC-RWCS. For the proposed GC-RWCS strategy, we fix the number of step L = 4, the neighbor-hop parameter k = 1 and the parameter l = 30 for the binarized M in Eq. ( <ref type="formula" target="#formula_16">4</ref>) for all models on all datasets. Note that L = 4 is different from the number of layers of both GCN and JK-Nets in our experiments. But we achieve effective attack performance. We also conduct a sensitivity analysis in Appendix A.5 and demonstrate the proposed method is not sensitive w.r.t. L.</p><p>Nuisance parameters of the attack procedure. For each dataset, we fix the limit on the number of nodes to attack, r, as 1% of the graph size. After the node selection step, we also need to specify how to perturb the node features, i.e., the design of in τ function in the optimization problem <ref type="bibr" target="#b1">(2)</ref>. In a real-world scenario, should be designed with domain knowledge about the classification task, without access to the GNN models. In our experiments, we have to simulate the domain knowledge due to the lack of semantic meaning of each individual feature in the benchmark datasets. Formally, we construct the constant perturbation ∈ R D as follows, for j = 1, 2, . . . , D,</p><formula xml:id="formula_18">j =    λ • sign( N i=1 ∂L(H,y) ∂Xij ), if j ∈ arg top-J N i=1 ∂L(H,y) ∂X il l=1,2,...,D , 0, otherwise,<label>(5)</label></formula><p>where λ is the magnitude of modification. We fix J = 0.02D for all datasets. While gradients of the model are involved, we emphasize that we only use extremely limited information of the gradients: determining a few number of important features and the binary direction to perturb for each selected feature, only at the global level by averaging gradients on all nodes. We believe such coarse information is usually available from domain knowledge about the classification task. The perturbation magnitude for each feature is fixed as a constant λ and is irrelevant to the model. In addition, the same perturbation vector is added to the features of all the selected nodes. The construction of the perturbation is totally independent of the selected nodes.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Experiment Results</head><p>Verifying the discrepancy between the loss and the mis-classification rate. We first provide empirical evidence for the discrepancy between classification loss (cross-entropy) and mis-classification rate. We compare the RWCS strategy to baseline strategies with varying perturbation strength as measured by λ in Eq. ( <ref type="formula" target="#formula_18">5</ref>). The results shown in Figure <ref type="figure" target="#fig_0">1</ref>  Full experiment results. We then provide the full experiment results of attacking GCN, JKNetConcat, and JKNetMaxpool on all three datasets in Table <ref type="table" target="#tab_0">1</ref>. The perturbation strength is set as λ = 1.</p><p>The thresholds 10% and 30% indicate that we set the limit on the maximum degree m as the lowest degree of the top 10% and 30% nodes respectively.</p><p>The results clearly demonstrate the effectiveness of the proposed GC-RWCS strategy. GC-RWCS achieves the best attack performance on almost all experiment settings, and the difference to the second-best strategy is significant in almost all cases. It is also worth noting that the proposed GC-RWCS strategy is able to decrease the node classification accuracy by up to 33.5%, and GC-RWCS achieves a 70% larger decrease of the accuracy than the Random baseline in most cases (see Table <ref type="table">4</ref> in Appendix A.5). And this is achieved by merely adding the same constant perturbation vector to the features of 1% of the nodes in the graph. This verifies that the explicit structural inductive biases of GNN models make them vulnerable even in the extremely restricted black-box attack setup. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusion</head><p>In this paper, we propose a novel black-box adversarial attack setup for GNN models with constraint of limited node access, which we believe is by far the most restricted and realistic black-box attack setup. Nonetheless, through both theoretical analyses and empirical experiments, we demonstrate that the strong and explicit structural inductive biases of GNN models make them still vulnerable to this type of adversarial attacks. We also propose a principled attack strategy, GC-RWCS, based on our theoretical analyses on the connection between the GCN model and random walk, which corrects the diminishing-return effect of the mis-classification rate. Our experimental results show that the proposed strategy significantly outperforms competing attack strategies under the same setup.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Broader Impact</head><p>For the potential positive impacts, we anticipate that the work may raise the public attention about the security and accountability issues of graph-based machine learning techniques, especially when they are applied to real-world social networks. Even without accessing any information about the model training, the graph structure alone can be exploited to damage a deep learning framework with a rather executable strategy.</p><p>On the potential negative side, as our work demonstrates that there is a chance to attack existing GNN models effectively without any knowledge but a simple graph structure, this may expose a serious alert to technology companies who maintain the platforms and operate various applications based on the graphs. However, we believe making this security concern transparent can help practitioners detect potential attack in this form and better defend the machine learning driven applications.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A Appendix</head><p>A.1 Proof of Proposition 1</p><p>We first remind the reader for some notations, a GCN model is denoted as a function f , the feature matrix is X ∈ R N ×D , and the output logits H = f (X) ∈ R N ×K . The L-step random walk transition matrix is M L . More details can be found in in Section 3.1</p><p>We give in Lemma 1 the connection between GCN models and random walks. Lemma 1 relies on a technical assumption about the GCN model (Assumption 5) and the proof can be found in Xu et al. <ref type="bibr" target="#b22">[23]</ref>. Assumption 5 (Xu et al. <ref type="bibr" target="#b22">[23]</ref>). All paths in the computation graph of the given GCN model are independently activated with the same probability of success ρ. Lemma 1. (Xu et al. <ref type="bibr" target="#b22">[23]</ref>.) Given an L-layer GCN with averaging as α i,j = 1/d i in Eq. 1, assume that all path in the computation graph of the model are activated with the same probability of success ρ (Assumption 5). Then, for any node i, j ∈ V ,</p><formula xml:id="formula_19">E ∂H j ∂X i = ρ • 1 l=L W l [M L ] ji ,<label>(6)</label></formula><p>where W l is the learnable parameter at l-th layer.</p><p>Then we are able to prove Proposition 1 below.</p><p>Proof. First, we derive the gradient of the loss L(H, y) w.r.t. the feature X i of node i,</p><formula xml:id="formula_20">∇ Xi L(H, y) = ∇ Xi   N j=1 L j (H j , y j )   = N j=1 ∇ Xi L j (H j , y j ) = N j=1 ∂H j ∂X i T ∂L j (H j , y j ) ∂H j ,<label>(7)</label></formula><p>where H j is the jth row of H but being transposed as column vectors and y j is the true label of node j. Note that ∂Lj (Hj ,yj ) ∂Hj ∈ R K , and</p><formula xml:id="formula_21">∂Hj ∂Xi ∈ R K×D .</formula><p>Next, we plug Eq. 7 into ∆ i (x) | x=τ (X,{i})i . For simplicity, We write ∆ i (x) | x=τ (X,{i})i as ∆ i in the rest of the proof. </p><formula xml:id="formula_22">∆ i = (∇ Xi L(H, y)) T = N j=1 ∂L j (H j , y j ) ∂H j T ∂H j ∂X i .<label>(8)</label></formula><formula xml:id="formula_23">[a j k ] = −q k (1 − p(k | k)) + K w=1,w =k p(k | w)q w , k = 1, 2, . . . , K</formula><p>which is a constant independent of H j and y j . Therefore, we can write</p><formula xml:id="formula_24">E[a j ] = c, ∀j = 1, 2, . . . , N,</formula><p>where c ∈ R K is a constant vector independent of j.</p><p>Taking expectation of Eq. ( <ref type="formula" target="#formula_22">8</ref>) and plug in the result of Lemma 1,</p><formula xml:id="formula_25">E ∆ i ≈ E   N j=1 ∂L j (H j , y j ) ∂H j T ∂H j ∂X i   = N j=1 E[a j ] T ρ 1 l=L W l [M L ] ji = ρc T 1 l=L W l N j=1 [M L ] ji = C N j=1 [M L ] ji ,</formula><p>where C = ρc T 1 l=L W l is a constant scalar independent of i.</p><p>A.2 Proofs for Propositions in Section 3.4</p><p>Proof of Proposition 2.</p><p>Proof.</p><formula xml:id="formula_26">If A i = ∅, B i ⊆ A i so B i = ∅.</formula><p>The three conditions of Definition 3 are also trivially true.</p><p>Below we investigate the case A i = ∅.</p><p>The existence can be given by a constructive proof. We check the nonempty elements in A i one by one with any order. If this element is a super set of any other element in A i , we skip it. Otherwise, we put it into B i . Then we verify that the resulted B i is a basic vulnerable set for i. B i ⊆ A i . For condition 1), clearly, ∅ / ∈ B i and if ∅ ∈ A i , all nonempty elements in A i are skipped so B i = ∅. For condition 2), given ∅ / ∈ A i , for any nonempty S ∈ A i , if S ∈ B i , the condition holds. If S / ∈ B i , by construction, there exists a nonempty strict subset S 1 ⊂ S and S 1 ∈ A i . If S 1 ∈ B i , the condition holds. If S 1 / ∈ B i , we can similarly find a nonempty strict subset S 2 ⊂ S and S 2 ∈ A i . Recursively, we can get a series S ⊃ S 1 ⊃ S 2 ⊃ • • • . As S is finite, we will have a set S k that no longer has strict subset so S k ∈ B i . Therefore the condition holds. Condition 3) means any set in B i is not a subset of another set in B i . This condition holds by construction. Now we prove the uniqueness. Suppose there are two distinct basic vulnerable sets</p><formula xml:id="formula_27">B i = C i . Without loss of generality, we assume S ∈ B i but S / ∈ C i . B i = ∅ so ∅ / ∈ A i . Further S ∈ A i , hence C i = ∅. As S ∈ B i ⊆ A i , S = ∅,</formula><p>and C i satisfies condition 2), there will be a nonempty T ∈ C i s.t. T ⊂ S. If T ∈ B i , then condition 3) is violated for B i . If T / ∈ B i , there will be a nonempty T ∈ B i s.t. T ⊂ T . But T ⊂ S also violates condition 3). By contradiction we prove the uniqueness.</p><p>In order to prove Proposition 3, we first would like to construct a submodular function that is close to h, with the help of Lemma 2 below. Lemma 2. If ∀i ∈ V , B i is either empty or only contains singleton sets, then h is submodular.</p><p>Proof. We first prove the case when ∀i ∈ V, A i = ∅.</p><p>First, we show that ∀i ∈ V , if A i = ∅, for any nonempty S ⊆ V, g i (S) = 1 if and only if B i = ∅ or ∃T ∈ B i , T ⊆ S. On one hand, if g i (S) = 1, then S ∈ A i . If ∅ ∈ A i , B i = ∅. If ∅ / ∈ A i , by condition 2) of the basic vulnerable set, ∃T ∈ B i , T ⊆ S. On the other hand, if ∃T ∈ B i , T ⊆ S, g i (T ) = 1, by Assumption 2, g i (S) ≥ g i (T ), so g i (S) = 1. If B i = ∅, as A i = ∅, if ∅ / ∈ A i , the condition 2) of Definition 3 will be violated. Therefore ∅ ∈ A i so g i (∅) = 1. Still by Assumption 2, g i (S) ≥ g i (∅), so g i (S) = 1.</p><p>Define a function e : V → 2 V s.t. for any node i ∈ V , e(i) = {j ∈ V | {i} ∈ B j }. Given B i is either empty or only contains singleton sets for any i ∈ V , for any nonempty S ⊆ V The case of allowing some nodes to have empty vulnerable sets can be easily proved by removing such nodes in Eq. ( <ref type="formula">9</ref>) as their corresponding vulnerable functions always equal to zero.</p><p>Proof of Proposition 3. For simplicity, we assume A i = ∅ for any i ∈ V . The proof below can be easily adapted to the general case without this assumption, by removing the nodes with empty vulnerable sets similarly as the proof for Lemma 2.</p><p>Proof. ∀i ∈ V , define B i {S ∈ B i | |S| = 1}. We can then define a new group of vulnerable sets A i on V for i ∈ V . Let</p><formula xml:id="formula_28">A i =    2 V , if B i = ∅, ∅, B i = ∅ but B i = ∅, {S ⊆ V | ∃T ∈ B i , T ⊆ S}, otherwise.</formula><p>Then it is clear that B i is a valid basic vulnerable set corresponding to A i , for i ∈ V . If we define gi : 2 V → {0, 1} as gi (S) = 1, if B i = ∅ or ∃T ∈ B i , T ⊆ S, 0, otherwise, we can easily verify that gi is a valid vulnerable function corresponding to A i , for i ∈ V . Further let h : 2 V → R + as</p><formula xml:id="formula_29">h(S) = 1 N N i=1</formula><p>gi (S).</p><p>By Lemma 2, as ∀i ∈ V, B i is either empty or only contains singleton sets, we know h is submodular.</p><p>Next we investigate the difference between h and h. </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: Experiments of attacking GCN on Citeseer with increasing perturbation strength λ. Results are averaged over 40 random trials and error bars indicate standard error of mean.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>1 N 1 N</head><label>11</label><figDesc>| B j = ∅ or ∃T ∈ B j , T ⊆ S}| = |{j ∈ V | B j = ∅ or ∃{i} ∈ B j , i ∈ S}| = |{j ∈ V | B j = ∅ or ∃i ∈ S, {i} ∈ B j }| = 1 N (|∪ i∈S e(i)| + |{j ∈ V | B j = ∅}|) . |{j ∈ V | B j = ∅}| is a constant independent of S.Therefore, maximizing h(S) over S with |S| ≤ r is equivalent to maximizing |∪ i∈S e(i)| over S with |S| ≤ r, which is a maximum coverage problem. Therefore h is submodular.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head></head><label></label><figDesc>First, for any S ⊆ V , if S / ∈ ∪ N i=1 A i , clearly h(S) = h(S) = 0; if |S| ≤ 1,it's easy to show h(S) = h(S). Second, for any S ∈ ∪ N i=1 A i and |S| &gt; 1, by Assumption 3, there are exactly b (omitting the S in b(S)) nodes whose vulnerable set contains S. Without loss of generality, let us assume the indexes of b nodes are 1, 2, . . . , b. Then, for any node i &gt; b, g i (S) = 0, gi (S) = 0. For node i = 1, 2, . . . , b, g i (S) = 1, and gi (S) = 1, if B i = ∅ or ∃T ⊆ S, |T | = 1 and T ∈ B i , 0, otherwise. By Assumption 4, there are at least pb (omitting the S in p(S)) nodes like j s.t. gj (S) = 1. Therefore, h(S) = b N and pb N ≤ h(S) ≤ b N . Hence 1 − 1 r &lt; 1 ≤ h(S) h(S) ≤ 1 p &lt; 1 + 1 r .</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 :</head><label>1</label><figDesc>Summary of the attack performance. The lower the accuracy (in %) the better the attacks. The bold marker denotes the best performance. The asterisk (*) means the difference between the best strategy and the second-best strategy is statistically significant by a t-test at significance level 0.05. The error bar (±) denotes the standard error of the mean by 40 independent trials.</figDesc><table><row><cell></cell><cell></cell><cell>Cora</cell><cell></cell><cell></cell><cell>Citeseer</cell><cell></cell><cell></cell><cell>Pubmed</cell></row><row><cell>Method</cell><cell>GCN</cell><cell cols="3">JKNetConcat JKNetMaxpool GCN</cell><cell cols="3">JKNetConcat JKNetMaxpool GCN</cell><cell cols="2">JKNetConcat JKNetMaxpool</cell></row><row><cell>None</cell><cell cols="2">85.6 ± 0.3 86.2 ± 0.2</cell><cell>85.8 ± 0.3</cell><cell>75.1 ± 0.2</cell><cell>72.9 ± 0.3</cell><cell>73.2 ± 0.3</cell><cell>85.7 ± 0.1</cell><cell>85.8 ± 0.1</cell><cell>85.7 ± 0.1</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">Threshold 10%</cell><cell></cell><cell></cell><cell></cell></row><row><cell>Random</cell><cell cols="2">81.3 ± 0.3 68.8 ± 0.8</cell><cell>68.8 ± 1.3</cell><cell>71.3 ± 0.3</cell><cell>60.8 ± 0.8</cell><cell>61.7 ± 0.9</cell><cell>82.0 ± 0.3</cell><cell>75.9 ± 0.7</cell><cell>75.4 ± 0.7</cell></row><row><cell>Degree</cell><cell cols="2">78.2 ± 0.4 60.7 ± 1.0</cell><cell>59.9 ± 1.5</cell><cell>67.5 ± 0.4</cell><cell>52.5 ± 0.8</cell><cell>53.7 ± 1.0</cell><cell>78.9 ± 0.5</cell><cell>63.4 ± 1.0</cell><cell>63.3 ± 1.2</cell></row><row><cell>Pagerank</cell><cell cols="2">79.4 ± 0.4 71.6 ± 0.6</cell><cell>70.0 ± 1.0</cell><cell>70.1 ± 0.3</cell><cell>61.5 ± 0.5</cell><cell>62.6 ± 0.6</cell><cell>80.3 ± 0.3</cell><cell>71.3 ± 0.8</cell><cell>71.2 ± 0.8</cell></row><row><cell cols="3">Betweenness 79.7 ± 0.4 60.5 ± 0.9</cell><cell>60.3 ± 1.6</cell><cell>68.9 ± 0.3</cell><cell>53.5 ± 0.8</cell><cell>55.1 ± 1.0</cell><cell>78.5 ± 0.6</cell><cell>67.1 ± 1.1</cell><cell>66.2 ± 1.1</cell></row><row><cell>RWCS</cell><cell cols="2">79.5 ± 0.3 71.2 ± 0.5</cell><cell>69.9 ± 1.0</cell><cell>69.9 ± 0.3</cell><cell>60.8 ± 0.6</cell><cell>62.2 ± 0.7</cell><cell>79.8 ± 0.3</cell><cell>70.7 ± 0.8</cell><cell>70.7 ± 0.8</cell></row><row><cell>GC-RWCS</cell><cell cols="2">78.5 ± 0.5 52.7 ± 1.0*</cell><cell>53.3 ± 1.9*</cell><cell cols="2">65.1 ± 0.5* 46.6 ± 0.8*</cell><cell>48.2 ± 1.1*</cell><cell>77.3 ± 0.7</cell><cell>62.1 ± 1.2</cell><cell>60.6 ± 1.4*</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">Threshold 30%</cell><cell></cell><cell></cell><cell></cell></row><row><cell>Random</cell><cell cols="2">82.6 ± 0.4 70.7 ± 1.1</cell><cell>71.8 ± 1.1</cell><cell>72.6 ± 0.3</cell><cell>62.7 ± 0.8</cell><cell>63.9 ± 0.8</cell><cell>82.6 ± 0.2</cell><cell>77.3 ± 0.4</cell><cell>77.4 ± 0.5</cell></row><row><cell>Degree</cell><cell cols="2">80.7 ± 0.4 64.9 ± 1.4</cell><cell>67.0 ± 1.5</cell><cell>70.4 ± 0.4</cell><cell>56.9 ± 0.8</cell><cell>58.7 ± 0.9</cell><cell>81.5 ± 0.4</cell><cell>72.4 ± 0.7</cell><cell>72.3 ± 0.7</cell></row><row><cell>Pagerank</cell><cell cols="2">82.6 ± 0.3 79.6 ± 0.4</cell><cell>79.7 ± 0.4</cell><cell>72.9 ± 0.2</cell><cell>70.2 ± 0.3</cell><cell>70.3 ± 0.3</cell><cell>83.0 ± 0.2</cell><cell>79.3 ± 0.3</cell><cell>79.6 ± 0.3</cell></row><row><cell cols="3">Betweenness 81.8 ± 0.4 64.1 ± 1.3</cell><cell>65.9 ± 1.4</cell><cell>70.7 ± 0.3</cell><cell>56.3 ± 0.8</cell><cell>58.3 ± 0.9</cell><cell>81.3 ± 0.3</cell><cell>74.1 ± 0.5</cell><cell>74.6 ± 0.5</cell></row><row><cell>RWCS</cell><cell cols="2">82.8 ± 0.3 79.3 ± 0.5</cell><cell>79.5 ± 0.4</cell><cell>72.9 ± 0.2</cell><cell>69.8 ± 0.3</cell><cell>70.1 ± 0.3</cell><cell>82.1 ± 0.2</cell><cell>77.8 ± 0.3</cell><cell>78.4 ± 0.3</cell></row><row><cell>GC-RWCS</cell><cell cols="2">80.7 ± 0.5 59.1 ± 1.6*</cell><cell>61.1 ± 1.6*</cell><cell cols="2">67.8 ± 0.5* 49.0 ± 0.9*</cell><cell>50.7 ± 1.1*</cell><cell cols="2">80.3 ± 0.5* 69.2 ± 0.7*</cell><cell>70.0 ± 0.7*</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head></head><label></label><figDesc>= 1, 2, . . . , K. Under Assumption 1, the expectation of each element of a j is E</figDesc><table><row><cell>Denote a j</cell><cell>∂Lj (Hj ,yj ) ∂Hj</cell><cell cols="3">∈ R K . From the definition of loss</cell></row><row><cell></cell><cell></cell><cell></cell><cell>N</cell></row><row><cell></cell><cell></cell><cell>L j (H j , y j ) =</cell><cell>j=1</cell><cell>max k∈{1,...,K}</cell><cell>H jk − H jyj ,</cell></row><row><cell>we have</cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>a j k =</cell><cell></cell><cell></cell></row></table><note> −1, if k = y j and y j = argmax c∈{1,...,K} H jc , 1, if k = y j and k = argmax c∈{1,...,K} H jc , 0, otherwise, for k</note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4" xml:id="foot_0">This is an assumption made by Xu et al.<ref type="bibr" target="#b22">[23]</ref>, which we list as Assumption</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="5" xml:id="foot_1">in Appendix A.1.</note>
		</body>
		<back>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.3 Algorithm Details of GC-RWCS</head><p>We summarize the GC-RWCS strategy in Algorithm 1.</p><p>Algorithm 1: The GC-RWCS Strategy for Node Selection. Input: number of nodes limit r; maximum degree limit m; neighbor hops k; binarized transition matrix M ; the adaptive influence score function I i , ∀i ∈ V . Output: the set S to be attacked. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.4 Additional Experiment Details</head><p>Datasets. We adopt the Deep Graph Library <ref type="bibr" target="#b16">[17]</ref> version of Cora, Citeseer, and Pubmed in our experiments. The summary statistics of the datasets are summarized in Table <ref type="table">2</ref>. The number of edges does not include self-loops. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.5 Additional Experiment Results</head><p>In this section, we provide results of more experiment setups and conduct a sensitivity analysis of the hyper-parameter L in GC-RWCS in Table <ref type="table">3</ref>. We provide a setup of 20% threshold in addition to the 10% and 30% thresholds shown in Section 4.2, to give a better resolution of the results. And the results of threshold 20% are consistent with other setups. We also show the results of GC-RWCS with L = 3, 4, 5, 6, 7. Note that GCN has 2 layers and the JK-Nets have 7 layers. The variations of GC-RWCS results with the provided range of L are typically within 2%, indicating that the proposed GC-RWCS strategy does not rely on the exact knowledge of number of layers in the GNN models to be effective.</p><p>Further, we also compare the relative decrease of accuracy between the proposed GC-RWCS strategy (L = 4) and the Random strategy in Table <ref type="table">4</ref>. GC-RWCS is able to decrease the node classification accuracy by up to 33.5%, and achieves a 70% larger decrease of the accuracy than the Random baseline in most cases. As the GC-RWCS and Random use exactly the same feature perturbation and the node selection step of Random does not include any information of the graph structure, this relative comparison can be roughly viewed as an indicator of the attack effectiveness attributed to the structural inductive biases of the GNN models. </p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">Adversarial attacks on node embeddings via graph poisoning</title>
		<author>
			<persName><forename type="first">Aleksandar</forename><surname>Bojchevski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stephan</forename><surname>Günnemann</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1809.01093</idno>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Towards evaluating the robustness of neural networks</title>
		<author>
			<persName><forename type="first">Nicholas</forename><surname>Carlini</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><surname>Wagner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2017 ieee symposium on security and privacy (sp)</title>
				<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="39" to="57" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">A restricted black-box adversarial framework towards attacking graph embedding models</title>
		<author>
			<persName><forename type="first">Heng</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yu</forename><surname>Rong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tingyang</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wenbing</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Honglei</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Peng</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wenwu</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Junzhou</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI Conference on Artificial Intelligence</title>
				<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<author>
			<persName><forename type="first">Jinyin</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yangyang</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xuanheng</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yixian</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Haibin</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qi</forename><surname>Xuan</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1809.02797</idno>
		<title level="m">Fast gradient attack on network embedding</title>
				<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">Adversarial attack on graph structured data</title>
		<author>
			<persName><forename type="first">Hanjun</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hui</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tian</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xin</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lin</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jun</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Le</forename><surname>Song</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1806.02371</idno>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">The lottery ticket hypothesis: Finding sparse, trainable neural networks</title>
		<author>
			<persName><forename type="first">Jonathan</forename><surname>Frankle</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><surname>Carbin</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1803.03635</idno>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Maximization of approximately submodular functions</title>
		<author>
			<persName><forename type="first">Thibaut</forename><surname>Horel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yaron</forename><surname>Singer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
				<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="3045" to="3053" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">Adversarial attacks and defenses on graphs: A review and empirical study</title>
		<author>
			<persName><forename type="first">Wei</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yaxin</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Han</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yiqi</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiliang</forename><surname>Tang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2003.00653</idno>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">Semi-supervised classification with graph convolutional networks</title>
		<author>
			<persName><forename type="first">N</forename><surname>Thomas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Max</forename><surname>Kipf</surname></persName>
		</author>
		<author>
			<persName><surname>Welling</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1609.02907</idno>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">Predict then propagate: Graph neural networks meet personalized pagerank</title>
		<author>
			<persName><forename type="first">Johannes</forename><surname>Klicpera</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aleksandar</forename><surname>Bojchevski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stephan</forename><surname>Günnemann</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1810.05997</idno>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Deepcas: An end-to-end predictor of information cascades</title>
		<author>
			<persName><forename type="first">Cheng</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiaqi</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaoxiao</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qiaozhu</forename><surname>Mei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 26th international conference on World Wide Web</title>
				<meeting>the 26th international conference on World Wide Web</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="577" to="586" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Random walks on graphs: A survey</title>
		<author>
			<persName><forename type="first">László</forename><surname>Lovász</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Combinatorics, Paul erdos is eighty</title>
				<imprint>
			<date type="published" when="1993">1993</date>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="1" to="46" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Weisfeiler and leman go neural: Higher-order graph neural networks</title>
		<author>
			<persName><forename type="first">Christopher</forename><surname>Morris</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Martin</forename><surname>Ritzert</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Matthias</forename><surname>Fey</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jan</forename><surname>William L Hamilton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gaurav</forename><surname>Eric Lenssen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Martin</forename><surname>Rattan</surname></persName>
		</author>
		<author>
			<persName><surname>Grohe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the AAAI Conference on Artificial Intelligence</title>
				<meeting>the AAAI Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page" from="4602" to="4609" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title/>
		<author>
			<persName><forename type="first">Mark</forename><surname>Newman</surname></persName>
		</author>
		<author>
			<persName><surname>Networks</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018">2018</date>
			<publisher>Oxford university press</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">Graphaf: a flow-based autoregressive model for molecular graph generation</title>
		<author>
			<persName><forename type="first">Chence</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Minkai</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhaocheng</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Weinan</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ming</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jian</forename><surname>Tang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2001.09382</idno>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">Node injection attacks on graphs via reinforcement learning</title>
		<author>
			<persName><forename type="first">Yiwei</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Suhang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xianfeng</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tsung-Yu</forename><surname>Hsieh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vasant</forename><surname>Honavar</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1909.06543</idno>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">Deep graph library: Towards efficient and scalable deep learning on graphs</title>
		<author>
			<persName><forename type="first">Minjie</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lingfan</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Da</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Quan</forename><surname>Gan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yu</forename><surname>Gai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zihao</forename><surname>Ye</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mufei</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jinjing</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qi</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chao</forename><surname>Ma</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1909.01315</idno>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<author>
			<persName><forename type="first">Andrew</forename><surname>Gordon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wilson</forename></persName>
		</author>
		<idno type="arXiv">arXiv:2001.10995</idno>
		<title level="m">The case for bayesian deep learning</title>
				<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Adversarial examples for graph data: Deep insights into attack and defense</title>
		<author>
			<persName><forename type="first">Huijun</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chen</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuriy</forename><surname>Tyshetskiy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrew</forename><surname>Docherty</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kai</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Liming</forename><surname>Zhu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IJCAI</title>
				<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">A comprehensive survey on graph neural networks</title>
		<author>
			<persName><forename type="first">Zonghan</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shirui</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fengwen</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Guodong</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chengqi</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Philip</forename><surname>Yu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Neural Networks and Learning Systems</title>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">Topology attack and defense for graph neural networks: An optimization perspective</title>
		<author>
			<persName><forename type="first">Kaidi</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hongge</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sijia</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pin-Yu</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tsui-Wei</forename><surname>Weng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mingyi</forename><surname>Hong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xue</forename><surname>Lin</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1906.04214</idno>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<author>
			<persName><forename type="first">Keyulu</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Weihua</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jure</forename><surname>Leskovec</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stefanie</forename><surname>Jegelka</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1810.00826</idno>
		<title level="m">How powerful are graph neural networks?</title>
				<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">Representation learning on graphs with jumping knowledge networks</title>
		<author>
			<persName><forename type="first">Keyulu</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chengtao</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yonglong</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tomohiro</forename><surname>Sonobe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ken-Ichi</forename><surname>Kawarabayashi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stefanie</forename><surname>Jegelka</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1806.03536</idno>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">Revisiting semi-supervised learning with graph embeddings</title>
		<author>
			<persName><forename type="first">Zhilin</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">William</forename><forename type="middle">W</forename><surname>Cohen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ruslan</forename><surname>Salakhutdinov</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1603.08861</idno>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Graph convolutional neural networks for web-scale recommender systems</title>
		<author>
			<persName><forename type="first">Rex</forename><surname>Ying</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ruining</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kaifeng</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pong</forename><surname>Eksombatchai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">William</forename><forename type="middle">L</forename><surname>Hamilton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jure</forename><surname>Leskovec</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 24th ACM SIGKDD International Conference on Knowledge Discovery &amp; Data Mining</title>
				<meeting>the 24th ACM SIGKDD International Conference on Knowledge Discovery &amp; Data Mining</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="974" to="983" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<author>
			<persName><forename type="first">Barret</forename><surname>Zoph</surname></persName>
		</author>
		<author>
			<persName><surname>Quoc</surname></persName>
		</author>
		<author>
			<persName><surname>Le</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1611.01578</idno>
		<title level="m">Neural architecture search with reinforcement learning</title>
				<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Adversarial attacks on graph neural networks via meta learning</title>
		<author>
			<persName><forename type="first">Daniel</forename><surname>Zügner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stephan</forename><surname>Günnemann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations (ICLR)</title>
				<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Adversarial attacks on neural networks for graph data</title>
		<author>
			<persName><forename type="first">Daniel</forename><surname>Zügner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Amir</forename><surname>Akbarnejad</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stephan</forename><surname>Günnemann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 24th ACM SIGKDD International Conference on Knowledge Discovery &amp; Data Mining</title>
				<meeting>the 24th ACM SIGKDD International Conference on Knowledge Discovery &amp; Data Mining</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="2847" to="2856" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
