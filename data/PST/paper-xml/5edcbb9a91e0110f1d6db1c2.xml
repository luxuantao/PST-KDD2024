<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Hierarchical Bipartite Graph Neural Networks: Towards Large-Scale E-commerce Applications</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Zhao</forename><surname>Li</surname></persName>
							<affiliation key="aff0">
								<orgName type="laboratory">Alibaba Group</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Xin</forename><surname>Shen</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution">Zhejiang University</orgName>
								<address>
									<settlement>Hangzhou</settlement>
									<region>Zhejiang</region>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Yuhang</forename><surname>Jiao</surname></persName>
							<email>jiaoyuhang@email.cufe.edu.cn</email>
							<affiliation key="aff2">
								<orgName type="department">School of Information</orgName>
								<orgName type="institution">Central University of Finance and Economics</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Xuming</forename><surname>Pan</surname></persName>
							<email>xuming.panxm@alibaba-inc.com</email>
							<affiliation key="aff0">
								<orgName type="laboratory">Alibaba Group</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Pengcheng</forename><surname>Zou</surname></persName>
							<affiliation key="aff0">
								<orgName type="laboratory">Alibaba Group</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Xianling</forename><surname>Meng</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution">Zhejiang University</orgName>
								<address>
									<settlement>Hangzhou</settlement>
									<region>Zhejiang</region>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Chengwei</forename><surname>Yao</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution">Zhejiang University</orgName>
								<address>
									<settlement>Hangzhou</settlement>
									<region>Zhejiang</region>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Jiajun</forename><surname>Bu</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution">Zhejiang University</orgName>
								<address>
									<settlement>Hangzhou</settlement>
									<region>Zhejiang</region>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Hierarchical Bipartite Graph Neural Networks: Towards Large-Scale E-commerce Applications</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="DOI">10.1109/ICDE48307.2020.00149</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.2" ident="GROBID" when="2022-12-25T14:10+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Hierarchical Representation</term>
					<term>Graph Neural Network</term>
					<term>Bipartite Graph</term>
					<term>E-commerce Recommendations</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>The e-commerce appeals to a multitude of online shoppers by providing personalized experiences and becomes indispensable in our daily life. Accurately predicting user preference and making a recommendation of favorable items plays a crucial role in improving several key tasks such as Click Through Rate (CTR) and Conversion Rate (CVR) in order to increase commercial value. Some state-of-the-art collaborative filtering methods exploiting non-linear interactions on a useritem bipartite graph are able to learn better user and item representations with Graph Neural Networks (GNNs), which do not learn hierarchical representations of graphs because they are inherently flat. Hierarchical representation is reportedly favorable in making more personalized item recommendations in terms of behaviorally similar users in the same community and a context of topic-driven taxonomy. However, some advanced approaches, in this regard, are either only considering linear interactions, or adopting single-level community, or computationally expensive. To address these problems, we propose a novel method with Hierarchical bipartite Graph Neural Network (HiGNN) to handle large-scale e-commerce tasks. By stacking multiple GNN modules and using a deterministic clustering algorithm alternately, HiGNN is able to efficiently obtain hierarchical user and item embeddings simultaneously, and effectively predict user preferences on a larger scale. Extensive experiments on some real-world e-commerce datasets demonstrate that HiGNN achieves a significant improvement compared to several popular methods. Moreover, we deploy HiGNN in Taobao, one of the largest e-commerces with hundreds of million users and items, for a series of large-scale prediction tasks of item recommendations. The results also illustrate that HiGNN is arguably promising and scalable in real-world applications.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>I. INTRODUCTION</head><p>The e-commerce era is witnessing a rapid development of online retailers who have attracted increasing people favoring online shopping, which conveniently provides items of interest with personalized experiences in our daily life. Leading ecommerce companies nowadays generate hundreds of millions of interactions (e.g. browsers, clicks, add-to-favorites, purchases, and comments) between tens of millions of users and a huge amount of items every day for a series of prediction ยง is the corresponding author. tasks including Click Through Rate (CTR), Conversion Rate (CVR), personalized recommendation list, and so on <ref type="bibr" target="#b0">[1]</ref>- <ref type="bibr" target="#b4">[5]</ref>. Precisely predicting a user preference in such a complex environment of e-commerce is very vital for improving user experience, and quite challenging for increasing business volumes. However, unexploited information in these numerous interactions are valuable user preferences and item attractiveness in a collaborative manner, which is arguably beneficial to improving the performance of top-K recommendation and preference ranking <ref type="bibr" target="#b5">[6]</ref>. More specific, collaborative filtering assumes that behaviorally similar users would exhibit similar preference on items, and vice versa <ref type="bibr" target="#b6">[7]</ref>. As a result, users and items are vectorized as embeddings to reconstruct historical interactions for efficiently predicting user preference. Recently, graph neural networks (GNNs) have gained a high reputation of obtaining state-of-the-art results through effectively learned node embeddings of non-linear interactions in tasks such as node classification and link prediction <ref type="bibr" target="#b7">[8]</ref>- <ref type="bibr" target="#b16">[17]</ref>. In particular, <ref type="bibr" target="#b17">[18]</ref> proposes a neural graph collaborative filtering method to explicitly integrate the user-item interactions into the embedding process, which is able to encode collaborative signal in the interaction graph structure by exploiting the high-order connectivity from user-item interactions and lead to a better performance of recommendation systems. Intuitively, collaborative filtering is an indication of the effect of community generalization on individual preference. However, in this regard, most state-of-the-art methods including neural graph collaborative filtering do not consider underlying usercommunity interactions or user hierarchy which have shown an advantageous performance over paradigms using user-item interactions alone <ref type="bibr" target="#b18">[19]</ref>- <ref type="bibr" target="#b20">[21]</ref>.</p><p>Generally speaking, GNN methods are inherently flat and do not learn hierarchical representations of graphs. On one hand, it demonstrates in <ref type="bibr" target="#b19">[20]</ref> that hierarchical representations of graphs can be combined with various graph neural network architectures in an end-to-end fashion to achieve prevailing results on graph classification benchmarks. Nonetheless, generating a hierarchical representation involves extensive and unscalable computation with the adjacent matrix of the graph. <ref type="bibr" target="#b18">[19]</ref> learns a hierarchical representation of graphs by decomposing user information into two orthogonal spaces, each of which represents information captured by community level and individualized user preference respectively, which improves the prediction accuracy with promising scalability. But it neglects multi-level effects of hierarchical clustering and item hierarchy information which limits its application in unsupervised learning for computing meaningful and interpretable clusters on input graphs. On the other hand, <ref type="bibr" target="#b21">[22]</ref> proposes an approach that automatically constructs an easyto-interpret taxonomy on a large-scale bi-partite graph in a unsupervised manner, facilitating an efficient browsing navigation that enhances user search experiences with inherent highorder connections, and the resulting descriptive hierarchical tree is also reported in favor of making more personalized item recommendations to users within the same cluster (community) they belong to. For example, a user, who has bought beach dresses and sunglasses, may prefer the topic of "trip to beach", in which an item under this topic such as sunblock may be clicked. Also, dress and sunglasses may indicate a more extensive topic like "outdoor activities" at a higher level, in which an item such as sneakers may be clicked. Likewise, an item, such as sneakers, may attract some users who prefer "sports", in which a user such as a sports enthusiast may click. Also, sneakers may attract more extensive users who prefer "outdoor activities" at a higher level, in which a user such as an outdoor enthusiast may click on. In spite of flexible topicdriven taxonomy capturing user's intention in many scenarios, by performing parallel hierarchical agglomerative clustering, it is not sufficient to yield satisfactory embeddings for exploiting the user-item non-linear interactions.</p><p>In this paper, motivated by the above pioneering work, we aim to learn hierarchical representations on bi-partite graphs to not only exploit the hierarchical high-order connections but also capture non-linear interactions for the purpose of applying to a series of tasks, such as, user preference prediction, and personalized browsing navigation, in large-scale e-commerce scenarios. Therefore, we propose a Hierarchical bipartite Graph Neural Network (HiGNN) which allows one to stack multiple GNN modules in a hierarchical fashion. It builds a coarsened graph as input to the next GNN layer by performing general clustering algorithms on embeddings obtained from the previous GNN layer. The whole process repeats several times until a stopping criteria, i.e., a specified number of levels, are satisfied. Although HiGNN is a two-stage hierarchical representation learning by combining GNN with deterministic clustering algorithms, which is able to preserve non-linear interactions and hierarchical high-order connections as well. More importantly, it is easy to scale to a very large realworld application without involving computationally expensive matrix operations. We summarize the major contributions of this paper as follows:</p><p>1) First, we introduce a large-scale Hierarchical bipartite Graph Neural Network (HiGNN), which effectively and efficiently addresses the problem of utilizing high-order connections and non-linear interactions through hierarchical representation learning on bi-partite graphs. Moreover, it is scalable to large-scale sparse graph data related applications. 2) Second, from perspective of supervised learning, by stacking multiple GNN in a hierarchical fashion, HiGNN is able to obtain hierarchical user preferences and the hierarchical item attractiveness through the learned hierarchical structure for depicting users and items precisely. Extensive experiments on large-scale e-commerce datasets, both online and offline, show its prevailing performances in a couple of prediction tasks including Click Through Rate (CTR) and Conversion Rate (CVR). 3) Third, from perspective of unsupervised learning, we apply HiGNN to automatically generate a topic-driven taxonomy of a large-scale real-world e-commerce for providing browsing navigation with personalized recommendation lists to enhance user experiences. It not only demonstrates the superiority of HiGNN compared to existing methods for placing favorable items into right topics, but also boosts the user preference prediction in terms of precision, which sheds a light on its promising applications in large-scale real-world e-commerce.</p><p>The remainder of this paper is organized as follows: in Section II, we investigate most related works. Section III gives a detailed description of the proposed HiGNN approach. Experimental results on the real world e-commerce applications for supervised learning are shown in Section IV, while the demonstrations of building a concept-driven taxonomy from unsupervised learning perspective are displayed in Section V. At last, Section VI concludes the paper.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>II. RELATED WORKS</head><p>Our work builds upon a rich line of recent research on graph neural networks (GNNs) with the aim of applying to a serials of e-commerce prediction tasks such as click through rate, conversion Rate, and topic-driven taxonomy. In particular, the GNNs with collaborative filtering and GNNs with hierarchical representation are most relevant literature regarding e-commerce applications and investigated elaborately here.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Graph Neural Networks</head><p>In recent years, graph neural networks (GNNs) have exerted a tremendous fascination on research community dedicating to effectively learn node embeddings over graph structured data, such as social network data or graph-based representations. GNNs treat the underlying graph as a computation graph and generate individual node embeddings by passing, transforming, and aggregating node feature information across the graph <ref type="bibr" target="#b22">[23]</ref>- <ref type="bibr" target="#b25">[26]</ref>. The generated node embeddings are widely used as input to any prediction tasks, e.g., for node classification, link prediction, and item recommendation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Graph-based Collaborative Filtering</head><p>Another line of research <ref type="bibr" target="#b26">[27]</ref>, <ref type="bibr" target="#b27">[28]</ref> exploits the user-item interaction graph to infer user preference in a collaborative fashion, assuming behaviorally similar users would exhibit similar preference on items. Intuitively, integrating user-item interactions into the embedding function could contribute to making better user preference prediction. An approach named HOP-Rec in <ref type="bibr" target="#b28">[29]</ref> performs random walks to enrich the interactions of a user with multi-hop connected items, which is beneficial to obtain better embeddings by partially capturing the collaborative effect of user-item interactions. The recently proposed neural graph collaborative filtering method is designed to propagate embeddings recursively on the graph for modeling the high-order connectivity information in the embedding function, a natural way that encodes collaborative signal in the interaction graph structure. Most of graph-based collaborative filtering methods are highly depended on matrix operations, such as matrix factorization or matrix multiplication, which makes it less scalable on large-scale graphs <ref type="bibr" target="#b29">[30]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Hierarchical Graph Representation</head><p>General GNN based methods are inherently flat as they only propagate information across edges of a graph and generate individual node embeddings, which is problematic or inefficient for predicting the label associate with the entire graph. However, learning hierarchical representations of graph enjoys its outstanding features in graph classification and clustering, and becomes prevailing in several scenarios such as link prediction, e-commerce recommendation, etc, <ref type="bibr" target="#b18">[19]</ref>, <ref type="bibr" target="#b21">[22]</ref>. There are some recent works that learn hierarchical graph representations by combining GNNs with different clustering processes. In particular, the recently proposed approach DIFFPOOL <ref type="bibr" target="#b19">[20]</ref>, a differentiable graph pooling module that can generate hierarchical representations of graphs and can be combined with various graph neural network architectures in an end-to-end fashion. It hierarchically learns a differentiable soft assignment at each layer of a deep GNN, mapping nodes to a set of clusters based on their learned embeddings. DIFFPOOL obtains favorable hierarchical representation and computes meaningful and interpretable clusters on the input graphs, while requiring explicitly expressing with the adjacent matrix of the graph. Consequently, it is computationally expensive that make it less popular in handling large-scale graphs <ref type="bibr" target="#b29">[30]</ref>. On the other hand, some researchers <ref type="bibr" target="#b30">[31]</ref>, <ref type="bibr" target="#b31">[32]</ref> illustrate a user's community-level embedding to be effective in graph classification tasks, in addition to a user's individual embedding. In <ref type="bibr" target="#b19">[20]</ref>, authors make some efforts in effectively co-training two embeddings by decomposing user information into two orthogonal spaces, each of which represents information captured by community level and individualized user preference respectively. Another intriguing application of hierarchical graph representation is e-commerce taxonomy for offering a personalized dynamic shopping navigation. <ref type="bibr" target="#b21">[22]</ref> illstrates a topic-driven hierarchical taxonomy based on user-item bi-partite graph in presence of query interactions effectively expressing user intention. It establishes correlation between categories of ontology-driven taxonomy, and offers an explainable recommendation with a noticeable prediction accuracy. While it is shown advantageous in some applications, it is not sufficient to capture the user-item non-linear interactions as it perform a traditional hierarchical agglomerative clustering to explore the hierarchical structure.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>III. HIERARCHICAL GRAPH NEURAL NETWORKS</head><p>In this section, we present the proposed HiGNN framework. First, we introduce bipartite GraphSAGE on a user-item graph to project user vertices and item vertices into two different feature spaces, i.e., user embedding and item embedding. Then, we elaborate on HiGNN implementation regarding constructing the hierarchical structure.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Preliminaries</head><p>The user-item graph is a quadruple G = (U, I, E, S), where users U = {u 1 , u 2 , . . . , u M } and items I = {i 1 , i 2 , . . . , i N } are two sets of vertices, E is the set of edges and each edge {e = (u m , i n )|u m โ U, i n โ I} is associated with a weight S(e) to denote the connection strength. An edge (u m , i n ) exists if user u m clicks item i n in the behavior history. In this graph, there are no edges between users or between items. The CTR prediction task is to learn a function between a user u m and an item i n , which can be used to predict the probability i n is clicked by u m .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Bipartite GraphSAGE</head><p>The intuition behind bipartite GraphSAGE is that at each iteration, a user aggregates information from local neighbor items, and an item aggregates information from local neighbor users. As this process iterates, vertices incrementally gain more and more information from further reaches of the graph.</p><p>We first denote AGGREGATE p u , โp โ {1, 2, . . . , P } and AGGREGATE p i , โp โ {1, 2, . . . , P } as the aggregator functions for users and items, which aggregate information from neighbors respectively. Denote sets of weight matrices W p u , โp โ {1, 2, . . . , P } and W p i , โp โ {1, 2, . . . , P } for users and items, which are used to propagate information between different steps of the model.</p><p>In the bipartite GraphSAGE method, the entire bipartite graph G = (U, I, E, S) and features for all users X u = {x u , โu โ U } and all items X i = {x i , โi โ I} are provided as input, where x u โ R du and x i โ R di . Denote h p u and h p i as the embedding of user and item at step p, and h 0 u = x u and h 0 i = x i for all users and items. In step p, each user u aggregates item embeddings in its immediate neighborhood, h pโ1 i , โi โ N (u), and transforms item embeddings into the corresponding user embedding h p N (u) by multiplying a transformation matrix M u i . This process is described as</p><formula xml:id="formula_0">h p N (u) โ M u i โข AGGREGATE p u ({h pโ1 i , โi โ N (u)}).<label>(1)</label></formula><p>With the same consideration, the aggregated item embedding is derived from</p><formula xml:id="formula_1">h p N (i) โ M i u โข AGGREGATE p i ({h pโ1 u , โu โ N (i)}),<label>(2)</label></formula><p>where M i u is the transformation matrix from user to item. Any type of aggregator is available and we adopt mean aggregator in our demonstration.</p><p>After aggregating the neighboring vertex embedding, bipartite GraphSAGE concatenates the vertex (both users and items) current embedding with the aggregated neighborhood embedding, and feeds the concatenated embedding through a full connection layer with nonlinear activation function ฯ, in order to transform the embedding to be used at the next step of the method. The method is expressed as</p><formula xml:id="formula_2">h p u โ ฯ W p u โข CONCAT(h pโ1 u , h p N (u) )<label>(3)</label></formula><p>for users and</p><formula xml:id="formula_3">h p i โ ฯ W p i โข CONCAT(h pโ1 i , h p N (i) )<label>(4)</label></formula><p>for items. Denote z u โก h P u , โu โ U and z i โก h P i , โi โ I as the final embedding output at step P .</p><p>In order to learn useful, predictive embeddings in a fully unsupervised setting for bipartite graphs, we apply a bipartite graph-based loss function to the output embeddings, z u , โu โ U and z i , โi โ I, and tune the weight matrices, W p u , โp โ {1, 2, . . . , P }, u โ U and W p i , โp โ {1, 2, . . . , P }, i โ I, the transformation matrices, M u i and M i u , and parameters of the aggregator functions via stochastic gradient descent. The bipartite graph-based loss function encourages nearby users and items have similar embeddings, while enforcing that embeddings of disparate users and items are highly distinct:</p><formula xml:id="formula_4">J BG = โ log ฯ f [CONCAT(z u , z i ), S((u, i))] โ Q u โข E unโผPn(u) log ฯ f [CONCAT(z un , z i ), ฮณ] โ Q i โข E inโผPn(i) log ฯ f [CONCAT(z u , z in ), ฮณ] ,</formula><p>(5) where (u, i) is a pair of user and item if an edge exists between them. f is a full connection network for generating similarity based on the concatenation of user embedding and item embedding, and the corresponding edge weight. ฯ is the sigmoid function. P n is a negative sampling distribution. Q u and Q i are defined as the number of negative samples for users and items, respectively. ฮณ is a hyper-parameter for denoting the weight of negative samples.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. HiGNN implementation</head><p>In this subsection, we present HiGNN, a network that allows one to stack multiple GNN modules, i.e., bipartite GraphSAGE, in order to construct a hierarchical structure in an end-to-end fashion.</p><p>Denote Z u = {z u , โu โ U } and Z i = {z i , โi โ I} as the sets of user embedding and item embedding, respectively. For brevity, we denote (Z u , Z i ) โ BG(G, X u , X i ) as the implementation of bipartite GraphSAGE with input bipartite graph G, user features X u and item features X i in the following article.</p><p>Given (Z u , Z i ) and the origin user-item graph, we adopt some clustering approach, i.e., K-means, to cluster similar users and similar items together in their own feature spaces, respectively. We consider user clusters C u and item clusters C i clustered by K-means as new users and items in a new coarsened user-item graph. The user cluster feature X Cu is able to be expressed as the average user embedding of users who belong to the cluster. With a similar method, the item cluster feature X Ci can be expressed. The edge weight of (C u , C i ) in the new coarsened graph is calculated as <ref type="bibr" target="#b5">(6)</ref> where u โ C u means user u belongs to user cluster C u , and i โ C i has the similar meaning. An edge is existed between</p><formula xml:id="formula_5">S(C u , C i ) = e S(e), โe = (u, i) โ G, u โ C u , i โ C i ,</formula><formula xml:id="formula_6">C u and C i if and only if S(C u , C i ) &gt; 0.</formula><p>Based on all information above (vertices, edges and edge weights), we are able to construct a new coarsened user-item graph. This graph is able to be used as input to the bipartite GraphSAGE at the next level. For clearly, denote K u (Z l u ), K i (Z l i ) as the K-means process at level l for users and items respectively. Denote F (C l u , C l i , G lโ1 ) as the coarsened graph construction process at level l. The hierarchical structure is able to be constructed by repeating bipartite GraphSAGE L times. We summarize the implementation of HiGNN in Algorithm 1. Then, the learned hierarchical user preference and hierarchical item attractiveness from the hierarchical structure can be utilized for the subsequent prediction tasks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D. Algorithm Complexity Analysis</head><p>As we can see, the user/item aggregator in bipartite Graph-SAGE and clusting are the main operations. For the two kind of operations, the computational complexity of the first layer is dominant. For the first layer of GNN, the computational complexity of aggregator is O((M + N )(K 1 * K 2 )), where M is the number of users, N is the number of items and K 1 and K 2 is the number of neighbors sampled at the depth of 1 and 2 respectively. For the first layer of Kmeans, we use the singlepass version which estimates the cluster centers with a single pass over all data and is appropriate for large-scale clustering. Thus, the computational complexity is O(M * K u + N * K i ), where K u and K i is the specified cluster number of user and item respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Algorithm 1: HiGNN implementation</head><p>Input: User-item graph G(U, I, E, S), user features X u and item features X i Output: Hierarchical structure</p><formula xml:id="formula_7">G โ {G 0 , G 1 , . . . , G L }, Z u โ {Z 1 u , . . . , Z L u } and Z i โ {Z 1 i , . . . , Z L i } 1 G, Z u , Z i โ {G 0 }, {}, {}; 2 l โ 1; 3 while l โค L do 4 (Z l u , Z l i ) โ BG(G lโ1 , X lโ1 u , X lโ1 i ); 5 C l u , C l i โ K u (Z l u ), K i (Z l i ); 6 (G l , X l u , X l i ) โ F (C l u , C l i , G lโ1 ); 7 G, Z u , Z i โ G โช G l , Z u โช Z l u , Z i โช Z l i ; 8 l โ l + 1; 9 end</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>IV. HIERARCHICAL BIPARTITE GRAPH NEURAL NETWORK FOR E-COMMERCE PREDICTION</head><p>HiGNN obtains hierarchical user preferences and hierarchical item attractiveness by stacking multiple GNNs in a hierarchical fashion. To utilise the learned user embeddings and item embeddings to precisely predict e-commerce tasks, we develop a deep neural network with HiGNN. Extensive experiments on large-scale e-commerce datasets, including offline and online, show our method outperforms other popular compared algorithms.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Supervised Deep Neural Network with HiGNN for Ecommerce Predictions</head><p>As shown in Figure <ref type="figure" target="#fig_1">2</ref>, a supervised deep neural network with HiGNN for e-commerce predictions is proposed to solve a series of prediction tasks, including CTR, CVR, and personalized recommendation. In this section, we extract the hierarchical user preference and hierarchical item attractiveness from the hierarchical structure and utilize it for CVR prediction.</p><p>Considering user and item embeddings obtained from the user-item graph, z u describes the user preference on different items and z i describes the item attractiveness on different kinds of users. In a CVR prediction task, for a user u and a candidate item i, if the user preference of u and the item attractiveness of i are matched, we predict that user i will purchase the item i.</p><p>In our proposed hierarchical structure, the user cluster embedding z l Cu at level l presents the level l user preference for all users in this cluster. Thus, we derive the hierarchical user preference of user u by concatenating the user cluster embedding (user embedding at the first level) z H u = CONCAT(z 1 u , z 2 u , . . . , z L u ), which synthesizes different-grained user preferences together. With the similar consideration, we obtain hierarchical item attractiveness of item i as z H i = CONCAT(z 1 i , z 2 i , . . . , z L i ). Given the hierarchical user preference, hierarchical item attractiveness, user profile (gender, purchasing power, etc.) and item statistic (click count, purchase count, etc.) as input, full connection layers are used to learn the combination of features automatically. The loss function of CVR prediction problem is defined as</p><formula xml:id="formula_8">J CV R = โ 1 N T (x,y) y log p(x) + (1 โ y) log(1 โ p(x)) , (<label>7</label></formula><formula xml:id="formula_9">)</formula><p>where N T is the size of the training set, with x as the input of the network and y โ {0, 1} as the label, p(x) is the output of the network after sigmoid function, indicating the predicted probability of sample x be purchased.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Offline Experiments and Results</head><p>We design our offline experiments to demonstrate whether the hierarchical user preference and hierarchical item attractiveness help to improve CVR prediction accuracy, and discuss the sensitivity of hyper-parameters in HiGNN.</p><p>1) Datasets and Metrics: We use Taobao dataset, a realworld industry dataset, to evaluate the performances of different methods. Table <ref type="table" target="#tab_0">I</ref> summarizes the statistics of our experimental datasets. Taobao datasets contain user-item click behaviors and transactions on Taobao, one of the largest online e-commerce platforms in the world. One week's logs are used for training and logs of the following day for testing. Specifically, Taobao #1 utilizes click and transaction logs in one week as the training set, and click and transaction logs on the next day as the testing set. Taobao #2 utilizes click and transaction logs about new arrival products in one week as the training set, and click and transaction logs about new arrival products on the next day as the testing set. Taobao #2 dataset concerns cold-start scenario, which focuses on the new items published within 2 months. Thus, Taobao #2's data density is relatively smaller than Taobao #1's.</p><p>We consider purchase behaviors as positive samples, and click behaviors without purchasing as negative samples. Because the number of positive samples is relatively small, to achieve better performance, we adopt a replicate sampling strategy to make the ratio of positive samples to negative samples as 1:3 in Taobao #1 dataset. However, to keep the real cold-start scenario in the e-commerce system, and test different algorithms in relatively sparse and unbalanced data, we utilize original records in Taobao #2 dataset. Table VI summarizes the statistics of samples in datasets.</p><p>We adopt the area under the receiver operator curve (AUC) to evaluate the performance of all the methods <ref type="bibr" target="#b32">[33]</ref>. AUC is the most popular evaluation metric on prediction tasks in both research and industry area. Larger AUC means better performance.  2) Compared Algorithms and Settings: To the best of our knowledge, no existing algorithm can deal efficiently with hierarchical user preferences and hierarchical item attractiveness to predict real-world e-commerce tasks of such large scale, including <ref type="bibr" target="#b29">[30]</ref> and <ref type="bibr" target="#b19">[20]</ref>. Our baseline algorithms are as follows:</p><p>โข CGNN: A graph neural network method learns two user embeddings for prediction by decomposing user information into two orthogonal spaces, each of which represents information captured by community level and individualized user preference respectively. CGNN can be considered as a special case of our proposed method, which fixes the number of user levels to 2. The parameter of CGNN refers to <ref type="bibr" target="#b18">[19]</ref>. โข DIN: A popular deep neural network method without graph structure information and hierarchical information in the e-commerce system. DIN can be regarded as a special case of our proposed method at level 0 (L = 0). The parameter of DIN refers to <ref type="bibr" target="#b0">[1]</ref>.</p><p>โข GE: Single level Graph Embedding-based (GE) method, which is our proposed method using only one level, without hierarchical information. โข HUP-only: Submodel of our proposed method, which considers Hierarchical User Preference only, without item attractiveness.</p><p>โข HIA-only: Submodel of our proposed method, which considers Hierarchical Item Attractiveness only, without user preference. We deploy our algorithm on Alibaba's server clusters comprising of 300 computing workers with 3000 CPUs. Empirically, both the dimension of user embedding d u and item embedding d i in bipartite GraphSAGE equal to 32. To keep consistency and simplicity, in this paper, we set the level number of a hierarchical structure L = 3 and the K-means parameter K l at level l satisfies K l = K lโ1 /ฮฑ, ฮฑ = 5 in Taobao datasets for achieving better performance. Furthermore, we will discuss the parameters' sensitiveness in this section later. We also set sizes of fully connected layers as 256, 128 and 64, learning rate as 0.001, batch size as 1024. L2-norm is used for regularization. Leaky ReLu is utilized as the activation function, and Sigmoid is utilized as the loss function.</p><p>3) Performance Comparison: Table <ref type="table" target="#tab_2">III</ref> lists the performance results of all compared methods. We evaluate our proposed method, HiGNN, with five popular and state-of-theart methods in two different Taobao datasets.  outperforms the baselines in all two datasets in terms of AUC.</p><p>In particular, GE is better than DIN, which indicates that the graph embedding method is able to represent the user preference and the item attractiveness more precisely. Both HUP-only and CGNN consider user hierarchical embedding without item hierarchical embedding. Because CGNN fixes the level to 2, it is relatively worse than HUP-only. Our proposed method performs better than HUP-only and HIA-only, which indicates that either the user hierarchical embedding item hierarchical embedding is effective for the CVR prediction model. Moreover, the proposed method performs best among all compared methods, which indicates that the combination of hierarchical embedding of user and item can further improve the CVR prediction accuracy. A more important thing we observed is that HiGNN still works on very sparse dataset, i.e. the cold-start scenario. Notice that HiGNN outperforms DIN by 3.08% and 3.33% in Taobao #1 and #2 respectively, hierarchical information works more effectively when the graph is sparse.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>4) Sensitivity Analysis:</head><p>There are two key hyper-parameters having the most influence on HiGNN, i.e., the level number L and the K-means parameter K. We conduct experiments for investigating the influence of different L and different K update strategy. The results are shown in Fig. <ref type="figure" target="#fig_2">3</ref>.</p><p>In this figure, we also plot one compared algorithm, DIN, which can be regarded as a special case of our proposed method at level 0 (L = 0). It can be observed that adding hierarchical information can achieve better performance. In most cases, AUC increases with an increase of L when L is less than or equal to 3, which demonstrates the hierarchical information is able to improve AUC performance. Obtaining more levels of user embeddings and item embeddings can better learn user preferences and item attractiveness.</p><p>Different K strategies lead to different clustering results, and the influence of clustering is determined as the distribution and scale of datasets. In particular, we set K l = K lโ1 ฮฑ from ฮฑ = 5 to ฮฑ = 20 on Taobao #1 dataset, which are shown in Fig. <ref type="figure" target="#fig_2">3</ref>. Larger ฮฑ can decrease the scale of hierarchical bi-partite graphs quickly, which reduces the running time. However, Larger ฮฑ also means more information loss. So smaller ฮฑ can achieve better performance, specifically, ฮฑ = 5 in the best in this parameter sensitiveness experiment.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Online Experiments and Results</head><p>This subsection shows the results of the online evaluation in the large-scale real-world e-commerce platform, Taobao system, with a standard A/B testing configuration. We mainly concern crucial e-commerce evaluation metrics as follows:</p><p>โข Unique Visitor (UV): the number of different clicked visitors, indicating whether our recommendation can attract users to click. โข transaction CouNT (CNT): the number of transactions, indicating whether our recommendation can increase sales volume.</p><p>โข CTR: the ratio of click number to visit number, one popular evaluation metric in an e-commerce system, especially in search and recommendation scenarios. โข CVR: the ratio of transaction number to click number, which directly shows the recommendation effectiveness on sales volume. We apply our model on the real Taobao e-commerce online system for new arrival products to solve the cold-start problem. Table <ref type="table" target="#tab_3">IV</ref> reports the results of improvement of UV, CNT, CTR, and CVR on two testing days. The results show that our proposed method increases commercial volumes under all e-commerce evaluation metrics. Both CNT and CVR, two crucial commercial metrics reflecting the sales volume, are improved by more than 2% on two A/B testing days. Moreover, HiGNN also increases UV and CTR, which indicates our proposed method can achieve user preferences and item attractiveness more precisely, and attract more visitors to click their interested items.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>V. HIERARCHICAL BIPARTITE GRAPH NEURAL</head><p>NETWORKS FOR E-COMMERCE TAXONOMY In this section, we first briefly revisit the background about taxonomy and the basic definition of topic-driven taxonomy based on query-item graphs. Then we introduce the Hierarchical Bi-partite Graph Neural Network (HiGNN) on a queryitem graph, which is an indication of users' search intention and somewhat different from the method on a user-item graph. Next we compare our proposed HiGNN method with the current taxonomy solution of Taobao, giving the quantitative experiments and analysis. Finally, we demonstrate the cases of constructing a topic-driven taxonomy structure from large-scale real-world e-commerce data using our proposed approach.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Brief Background</head><p>In addition to the specific e-commerce prediction applications, taxonomy construction is another crucial task in e-commerce scenarios. Dictionary-based ontology taxonomy <ref type="bibr" target="#b33">[34]</ref> is a widely used method to organize items into categorical structures in most existing e-commerce platforms, because the hierarchical conceptual knowledge behind the items can be naturally distilled into the ontology dictionary. Take Fig. <ref type="figure">4</ref> for example, "Beach Dress" is a leaf category and belongs to a parent category "Dress" and a grandparent category "Women's Clothing". The ontology taxonomy follows the pre-defined dictionary rules to manage items, which ignores the correlations between items in the shopping history and results in low coverage of the taxonomy. What's more, the terms in the dictionary are likely to be highly redundant since the concept of one item can be expressed in many different ways. Thus the ontology taxonomy in many cases can't capture the user's search intention. However, helping users explore items with categories sharing the same topic, in other words, the same search intention, in massive number of data is one of the most important characteristics required for e-commerce systems.</p><p>Noticeably, user's search queries can effectively reflect the topics of interest to the user, to bridge the gap between item taxonomy and user search intention, we apply our HiGNN on the query-item graph to automatically generate a new topicdriven taxonomy structure.</p><p>1) Query-Item Graph: Similar to the user-item graph, a query-item graph can be represented by the quadruple G = (Q, I, E, S), where queries Q = {q 1 , q 2 , . . . , q M } and items I = {i 1 , i 2 , . . . , i N } are two sets of vertices, E = {e = (q m , i n ) | q m โ Q, i n โ I} is the set of edges and S is the weights set where S(e) denotes the connection strength of edge e. Note that an edge (q m , i n ) exists if and only if a user clicked item i n from the resulting items of the query q m . In addition, there is no edge between query-query pair or item-item pair in this graph, hence it is a bi-partite graph.</p><p>2) Topic-Driven Taxonomy: The taxonomy construction task in e-commerce scenarios is to organize hundreds of millions of items into hierarchical topics-based structures, in which each topic consists of a group of items sharing the same user's search intention and a set of descriptions extracted from search queries. In particular, the taxonomy structure in this paper is constructed from the query-item graph, which not only contains the conceptual information, but also considers the conceptual shopping relationship on the query-item graph.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. HiGNN on Query-Item Graphs</head><p>Different from the user-item graph in e-commerce prediction task, the query-item graph has features embedded into the same space.</p><p>Specifically, the e-commerce prediction task is to measure the user's preference for different items and the attractiveness of an item to different users, which requires a lot of user information, such as age, gender, education level etc., and Fig. <ref type="figure">4</ref>: An example of ontology-driven taxonomy in Ecommerce, each node of which represents a category. item information, e.g., price, sales, brand, etc., those are orthogonally distributed in different feature spaces. However, taxonomy task is to construct a hierarchical topics-driven structure from the historical interactions between queries and items, especially from those keywords and titles, first capturing the hierarchical topics preserved in items and then assigning descriptions extracted from search queries to each topic. Note that the original keywords and titles of both queries and items in taxonomy task are composed of texts, which allows us to exploit the widely used natural language processing technique, word2vec <ref type="bibr" target="#b34">[35]</ref>, to embed the original features of queries and items into the same latent space. Thus, the Bipartite Graph Neural Network applied on query-item graphs is slightly different from the method on the user-item graphs.</p><p>First, we denote AGGREGATE as the aggregate operation for both queries and items, and W as the weight matrix used to propagate information between query-item vertices and their local neighbors. For each aggregate operation step p โ {1, 2, . . . , P }, the aggregate operations and the weight matrices can be represented by AGGREGATE p and W p . The inputs of this Bi-partite Graph Neural Network are the entire bipartite query-item graph G = (Q, I, E, S) and features for all queries X Q = {x q , โq โ Q} and all items X I = {x i , โi โ I}, where x q , x i โ R dw since the features are in the same word-embedding latent space.</p><p>1) Aggregator: We denote h p q and h p i as the embedding of query and item at the aggregation step p, and h 0 q = x q and h 0 i = x i . In step p, each query q aggregates information from its one-hop neighborhood, denotes as h pโ1 i , โi โ N (q), and transforms the features into the corresponding query features h p N (q) by multiplying a transformation matrix M p . This process is described as</p><formula xml:id="formula_10">h p N (q) โ M p โข AGGREGATE p ({h pโ1 i , โi โ N (q)}).<label>(8)</label></formula><p>Note that the item and query features are embedded into the same space, with the shared transformation matrix M p , the aggregated item embedding is derived from</p><formula xml:id="formula_11">h p N (i) โ M p โข AGGREGATE p ({h pโ1 q , โq โ N (i)}),<label>(9)</label></formula><p>similar to the supervised part, we adopt mean aggregator in this paper.</p><p>2) Dense Layer: After aggregating the neighboring information, we concatenate the target vertex embedding with the aggregated neighborhood embedding, then feed the concatenated embedding through dense layer with a nonlinear activation function ฯ. The method is expressed as</p><formula xml:id="formula_12">h p q โ ฯ W p โข CONCAT(h pโ1 q , h p N (q) )<label>(10)</label></formula><p>for queries and</p><formula xml:id="formula_13">h p i โ ฯ W p โข CONCAT(h pโ1 i , h p N (i) )<label>(11)</label></formula><p>for items. Denote z q = h P q and z i = h P i as the final embedding output at step P .</p><p>3) Unsupervised Loss: As we mentioned above, the input embeddings of queries and items are in the same latent space, and the differentiable weight matrices for queries and items are shared, which suggests that the output embeddings of both queries and items are distributed in the same feature space. Following the same rules, the unsupervised query-item graphbased loss function can be rewritten as:</p><formula xml:id="formula_14">J BG = โ log ฯ f [CONCAT(z(e)), S(e)] โ N n โข E enโผPn(e) log ฯ f [CONCAT(z(e n )), ฮณ] , (<label>12</label></formula><p>) where e = (q, i) is a pair of query and item if e exists in set E. f is Multi-Layer Perceptron (MLP) for generating similarity based on the concatenation of query-item embeddings and edge weight S(e). ฯ is the activation function. P n is a negative sampling distribution. N n is defined as the number of negative samples for edges. ฮณ is a hyper-parameter for denoting the weight of negative samples.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Topic-Driven Taxonomy Construction</head><p>In this subsection, we show how to generate topic-driven taxonomy with the unsupervised HiGNN, and to obtain a set of meaningful topic descriptions.</p><p>1) Hierarchical Taxonomy: The intuition behind HiGNN for taxonomy is to enhance the connections between queries and items that share the same search intention, i.e., the same topic, by clustering vertices which have the similar learned embeddings. The new coarsened graph consisting of the centroid of each cluster will be considered as the input of HiGNN at the next level. After repeating this coarsening procedure a few times, we can construct a hierarchical topicbased structures from the query-item graph. Obviously, the taxonomy results are very sensitive to the number of clusters that we set to be coarsened at each level. In order to generate a better clustering result, we exploit the Calinski-Harabasz Index <ref type="bibr" target="#b35">[36]</ref> to maximize the between-cluster variance and minimize the within-cluster variance, the objective function can be formulated as</p><formula xml:id="formula_15">max CH = D B (k) D W (k) ร N โ k k โ 1 , (<label>13</label></formula><formula xml:id="formula_16">)</formula><p>where k is the number of clusters, D B (k) denotes the betweencluster variance, D W (k) denotes the within-cluster variance and N is the number of point data.</p><p>2) Topic Description Matching: Once the topic-driven taxonomy structure is generated, the topic is probably associated with a set of items and each item will be connected with a number of queries. To make the topic more interpretive, we follow the similar strategy described in <ref type="bibr" target="#b36">[37]</ref> to find the most representative query as the description for a specific topic. The topic description matching method mainly considers two factors for calculating the representativeness of a query q for a topic t k , which are popularity and concentration. Specifically, the popularity stands for the frequency at which a query q appears in the topic t k , and the concentration represents the relevance of the topic t k and the query q compared with other topics. The representativeness of a query q for a topic t k can be derived from r(q, t k ) = pop(q, t k ) โข con(q, t k ), <ref type="bibr" target="#b13">(14)</ref> in which pop(q, t k ) and con(q, t k ) are the popularity and concentration scores of q for t k . Denote I k as the items in the cluster of topic t k , pop(q, t k ) can be calculated as</p><formula xml:id="formula_17">pop(q, t k ) = log tf (q, I k ) + 1 log tf (I k )<label>(15)</label></formula><p>where tf (I k ) is the number of tokens in I k and tf (q, I k ) denotes the number of tokens from the items that in the same cluster of topic t k with query q.</p><p>The con(q, t k ) is defined as</p><formula xml:id="formula_18">con(q, t k ) = exp(rel(q, D k )) 1 + 1โคjโคK exp(rel(q, D j ))<label>(16)</label></formula><p>rel(:, :) is the BM25 relevance of two elements, D k denotes the concatenation of the titles of all items belonging to the same topic t k .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D. Experiments and Results</head><p>SHOAL <ref type="bibr" target="#b21">[22]</ref> is Alibaba's current topic-driven taxonomy solution deployed on Taobao platform, which also considers a hierarchical graph-based strategy but only uses a well-defined metric to calculate the query-item embeddings. SHOAL doesn't apply a trainable graph neural network to learn the non-linear interactions between queries and items, which results in the inability to generate more meaningful query-item embeddings. We compare our proposed method with SHOAL in both online and offfline environments.</p><p>1) Offline Datasets and Metrics: We design our quantitative experiments to demonstrate whether the hierarchical embeddings which contain the non-linear query-item interactions will help to construct the topic-driven taxonomy that captures users search intention as much as possible. We use a realworld large-scale Taobao dataset, which contains hundreds of millions of items and query-click history in the last seven days, to evaluate the performances of different methods. Table <ref type="table" target="#tab_4">V</ref> shows the information of our experimental dataset. Taobao #3 dataset contains query-item click behaviors on Taobao. Over seven days' logs are used for training the neural network and generating the topic-driven taxonomy. Specifically, each existing query-item click log indicates that a user clicked the item from the result of the search query, a query-item edge exists if and only if the corresponding click appears in the logs, and the weight of the edge is the number of the specific query-item clicks. The purpose of the unsupervised loss function of this graph neural network is to encourage the nearby queries and items to share the similar embeddings, while enforcing the embeddings of disparate users and items are highly distinct. We consider all the existing query-item edges as positive samples, and items without being clicked in the search query results as negative query-item samples. We follow the same ratio of positive samples and negative samples in the e-commerce prediction experiments, which is set to 1:3.  We report the results of taxonomy to the domain experts to evaluate the performance of all methods. For each taxonomy results, the experts pick 100 topics from the taxonomy and randomly select 100 items under each topic to calculate the accuracy. The larger accuracy, the better result. Furthermore, to demonstrate the hierarchical separating capacity of methods, we use a new metric, the diversity. To define the diversity, we first introduce the term, "qualified topic". Items belonging to a qualified topic should cover more than two different categories. We define diversity as the ratio of the number of qualified topics to the number of all topics discovered by the algorithm. Then larger diversity means better hierarchical separating capacity.</p><p>2) Offline Experimental Results: To investigate the model effectiveness, we compare our proposed method with Alibaba's current topic-driven taxonomy solution SHOAL <ref type="bibr" target="#b21">[22]</ref>. In the parameter setting, we set the level number of the hierarchical structure L = 4 according to the observation of natural ontology level of items in the e-commerce platform. The dimension of all embeddings for both query and item are set to 32, which is the same as the previous experiments. To keep consistency, we set SHOAL's number of clusters as same as HiGNN's.</p><p>Table VII lists the performance results of SHOAL and HiGNN, in which the number of clusters are user specified and set to be the same for fair comparisons. It is noted that SHOAL's average numbers of final levels is 4.31, which is similar to ours. It is also observed that our proposed method outperforms the Alibaba's current topic-driven taxonomy solution, which is improved by more than 4% of the accuracy. That's because our method not only considers the graph-based non-linear interactions between queries and items but also applies a trainable neural network architecture to learn a hierarchical features which embeds the user search intention successfully. The diversity, our proposed new metric, is also used to evaluate SHOAL and HiGNN. As Table <ref type="table" target="#tab_7">VII</ref> shown, HiGNN outperforms Alibaba's existing method by 6% in diversity. Under the same cluster number, HiGNN can discover more qualified topics, which demonstrates its strong hierarchical separating capacity.</p><p>3) Offline Case Study: We show a taxonomy case generated by our method from the real-world Taobao dataset in Figure <ref type="figure" target="#fig_4">5</ref>. We apply our method to generate a four-level taxonomy, and each parent topic is split into several child topics. Figure <ref type="figure" target="#fig_4">5</ref> shows parts of the taxonomy generated by HiGNN. As shown in Figure <ref type="figure" target="#fig_4">5</ref>(a), HiGNN splits the root topic 'Healthy Home' into five sub-topics: 'Environmental Test', 'Beauty Products', 'Smart Home', 'Kitchen Equipment' and 'Disposable Items'. The description of those topics are generated automatically by selecting the term that is most representative for a topic. We find those topics are of good quality and precisely summarize the major topics in our daily home life. In Figure <ref type="figure" target="#fig_4">5</ref>(a) and 5(b), we also show how HiGNN splits level two topics 'Beauty Products' and 'Disposable Items' into more finegrained topics. Taking 'Beauty Products' as an example: (1) at level three, HiGNN can successfully find five child topics in 'Beauty Products': 'Massage Treatment', 'Health Care', 'Cosmetics', 'Male Care', and 'Sports Health Care'; (2) at level four, HiGNN splits the 'Cosmetics' topic into more finegrained topics: 'Basic Care', 'Facial Products', 'Hair Care', 'Eye Makeup Tool' and 'Hydration Product'. Similarly for the 'Disposable Items' topic (Figure <ref type="figure" target="#fig_4">5</ref>(b)), HiGNN can discover level-three topics like 'Chinese Medicine Supplies' and levelfour topic like 'Household Appliance Cleaning'. Moreover, the description of child topics for each topic are of good quality and they are semantically coherent and cover different aspects of the same topic. We also find some interesting topics, such as 'Quit Smoking Clean lungs' in 'Male Care' topic and 'Baby bathroom' in 'Clean Care' topic, which are not existed in the taxonomy generated by the compared method.</p><p>4) Online Evaluation: In order to verify the effectiveness of our HiGNN method on taxonomy construction tasks, we design an online A/B test with more than 3 million users in a Taobao's real recommendation application. In the control group, the taxonomy structure with the matched recommendations are generated by SHOAL. While in the experiment group,  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>VI. CONCLUSIONS</head><p>In this paper, we introduce a large-scale Hierarchical Bipartite Graph Neural Network (HiGNN) with the aim of applying to a series of e-commerce scenarios such as user preference prediction, item recommendations, and so on. Although state-of-the-art methods armed with Graph Neural Network (GNN) considerately utilize high-order connections, non-linear interactions, and hierarchical representation on a user-item bipartite graph to bring collaborative filtering signal into fullplay, they are inadequate to display their abilities in large-scale e-commerce applications. However, by stacking multiple GNN modules and using a deterministic clustering algorithm HiGNN is able to efficiently obtain user and item embeddings simultaneously in a hierarchical fashion, and scalable to large-scale bipartite graphs. To evaluate the performance of HiGNN, we conduct extensive experiments from two perspectives: supervised learning for predicting on user preference, and unsupervised learning for constructing topic-driven taxonomy. The experimental results demonstrate that HiGNN is able to effectively and efficiently obtain the hierarchical structure, and achieve a significant improvement compared against state-of-the-art baselines. Moreover, HiGNN is also deployed into Taobao, one of the largest real-world e-commerce platforms, and is potential to capture increasing attention in various applications.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 1 :</head><label>1</label><figDesc>Fig. 1: An example of topic-driven taxonomy in E-commerce. The virtual arrows of a bipartite graph (on the left) indicate a user's potential preference on items, based on which the hierarchical topic tree (on the right) is constructed to represent conceptual shopping scenarios.</figDesc><graphic url="image-1.png" coords="1,314.08,267.98,244.33,138.42" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 2 :</head><label>2</label><figDesc>Fig. 2: Supervised Deep Neural Network with HiGNN for E-commerce Predictions</figDesc><graphic url="image-3.png" coords="6,112.19,69.68,359.01,239.08" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 3 :</head><label>3</label><figDesc>Fig. 3: AUC comparisons with different K and L strategies.</figDesc><graphic url="image-5.png" coords="7,78.94,60.00,195.87,147.08" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head></head><label></label><figDesc>(a) The sub-topics under the topics 'Healthy Home', 'Beauty Products', and 'Cosmetics'. (b) The sub-topics under the topics 'Disposable Items' and 'Clean Care'.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Fig. 5 :</head><label>5</label><figDesc>Fig. 5: An example of a topic-driven taxonomy generated by HiGNN.</figDesc><graphic url="image-11.png" coords="11,112.65,299.51,195.76,132.83" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>TABLE I :</head><label>I</label><figDesc>Statistical Information of Datasets</figDesc><table><row><cell>Dataset</cell><cell>Users</cell><cell>Items</cell><cell cols="2">User-Item Clicks Density</cell></row><row><cell cols="3">Taobao #1 34,519,150 13,296,702</cell><cell>280,522,717</cell><cell>6.11e-7</cell></row><row><cell cols="2">Taobao #2 11,727,217</cell><cell>3,053,149</cell><cell>1,109,274</cell><cell>3.10e-8</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>TABLE II :</head><label>II</label><figDesc>Samples Information of Datasets</figDesc><table><row><cell>Dataset</cell><cell></cell><cell>Training Set</cell><cell></cell><cell>Testing Set</cell></row><row><cell></cell><cell>Positive</cell><cell>Negative</cell><cell>Total</cell><cell>Total</cell></row><row><cell cols="4">Taobao #1 78,988,312 223,612,179 302,600,491</cell><cell>40,824,588</cell></row><row><cell>Taobao #2</cell><cell>2,074,792</cell><cell>28,689,261</cell><cell>30,764,053</cell><cell>3,986,179</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>TABLE III :</head><label>III</label><figDesc>Performance Evaluation (AUC).</figDesc><table><row><cell>Dataset</cell><cell>CGNN</cell><cell>DIN</cell><cell>GE</cell><cell cols="3">HUP-o HIA-o HiGNN</cell></row><row><cell>Taobao #1</cell><cell>0.829</cell><cell cols="2">0.844 0.863</cell><cell>0.853</cell><cell>0.855</cell><cell>0.870</cell></row><row><cell>Taobao #2</cell><cell>0.875</cell><cell cols="2">0.870 0.893</cell><cell>0.881</cell><cell>0.881</cell><cell>0.899</cell></row></table><note>It can be observed that our proposed method significantly</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>TABLE IV :</head><label>IV</label><figDesc>Online A/B Testing of Performance Evaluation</figDesc><table><row><cell>Date</cell><cell>Day 1</cell><cell>Day 2</cell></row><row><cell>UV</cell><cell cols="2">43,514 โ 44,341 (+1.90%) 48,531 โ 49,522 (+2.04%)</cell></row><row><cell>CNT</cell><cell cols="2">54,438 โ 55,940 (+2.76%) 60,717 โ 62,001 (+2.11%)</cell></row><row><cell>CTR</cell><cell cols="2">0.3569 โ 0.3581 (+0.34%) 0.3469 โ 0.3492 (+0.66%)</cell></row><row><cell cols="3">CVR 0.1226 โ 0.1253 (+2.25%) 0.1206 โ 0.1231 (+2.09%)</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>TABLE V :</head><label>V</label><figDesc>Statistical Information of Taxonomy Dataset</figDesc><table><row><cell>Dataset</cell><cell>Queries</cell><cell>Items</cell><cell>Q-I Edges</cell><cell>Density</cell></row><row><cell cols="2">Taobao #3 76,218,663</cell><cell>138,514,439</cell><cell cols="2">1,000,947,908 9.481e-8</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head></head><label></label><figDesc>Table VI summarizes the statistics of samples in dataset.</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>TABLE VI :</head><label>VI</label><figDesc>Sample Information of Taxonomy Dataset</figDesc><table><row><cell>Dataset</cell><cell>Positive</cell><cell>Negative</cell><cell>Total</cell></row><row><cell cols="2">Taobao #3 1,000,947,908</cell><cell>3,002,843,724</cell><cell>4,003,791,632</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>TABLE VII :</head><label>VII</label><figDesc>Taxonomy Quality Evaluation</figDesc><table><row><cell>Algorithm</cell><cell>#Level</cell><cell cols="2">Accuracy Diversity</cell></row><row><cell>SHOAL</cell><cell>4.31 (on average)</cell><cell>85%</cell><cell>66%</cell></row><row><cell>HiGNN</cell><cell>4</cell><cell>89%</cell><cell>70%</cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" xml:id="foot_0">Authorized licensed use limited to: Murdoch University. Downloaded on June 14,2020 at 20:06:29 UTC from IEEE Xplore. Restrictions apply.</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>ACKNOWLEDGMENT This work was supported by Zhejiang Lab under No. 2019KE0AB01, the National Key RD Program of China under Grant 2018YFB1403202, and the Alibaba-ZJU Joint Research Institute of Frontier Technologies.</p></div>
			</div>

			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Authorized licensed use limited to: Murdoch University. Downloaded on June 14,2020 at 20:06:29 UTC from IEEE Xplore. Restrictions apply.</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Deep interest network for click-through rate prediction</title>
		<author>
			<persName><forename type="first">G</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Gai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 24th ACM SIGKDD International Conference on Knowledge Discovery &amp; Data Mining</title>
				<meeting>the 24th ACM SIGKDD International Conference on Knowledge Discovery &amp; Data Mining</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="1059" to="1068" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Deep graph embedding for ranking optimization in e-commerce</title>
		<author>
			<persName><forename type="first">C</forename><surname>Chu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Xin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Rohs</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Zhou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 27th ACM International Conference on Information and Knowledge Management</title>
				<meeting>the 27th ACM International Conference on Information and Knowledge Management</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="2007" to="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Interactive paths embedding for semantic proximity search on heterogeneous graphs</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><forename type="middle">W</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Ying</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 24th ACM SIGKDD International Conference on Knowledge Discovery &amp;#38; Data Mining</title>
				<meeting>the 24th ACM SIGKDD International Conference on Knowledge Discovery &amp;#38; Data Mining</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="1860" to="1869" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Tissa: A time slice self-attention approach for modeling sequential user behaviors</title>
		<author>
			<persName><forename type="first">C</forename><surname>Lei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The World Wide Web Conference</title>
				<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="2964" to="2970" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Personalized bundle list recommendation</title>
		<author>
			<persName><forename type="first">J</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Qu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>An</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Gao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The World Wide Web Conference</title>
				<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="60" to="71" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Unifying knowledge graph learning and recommendation: Towards a better understanding of user preferences</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T.-S</forename><surname>Chua</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The World Wide Web Conference</title>
				<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="151" to="161" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Neural collaborative filtering</title>
		<author>
			<persName><forename type="first">X</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Liao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Nie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T.-S</forename><surname>Chua</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 26th International Conference on World Wide Web</title>
				<meeting>the 26th International Conference on World Wide Web</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="173" to="182" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Inductive representation learning on large graphs</title>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">L</forename><surname>Hamilton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Ying</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Leskovec</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 31st International Conference on Neural Information Processing Systems</title>
				<meeting>the 31st International Conference on Neural Information Processing Systems</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="1025" to="1035" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">N</forename><surname>Kipf</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Welling</surname></persName>
		</author>
		<title level="m">Proceedings of the 5th International Conference on Learning Representations</title>
				<meeting>the 5th International Conference on Learning Representations</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Deriving neural architectures from sequence and graph kernels</title>
		<author>
			<persName><forename type="first">T</forename><surname>Lei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Barzilay</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Jaakkola</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 34th International Conference on Machine Learning</title>
				<meeting>the 34th International Conference on Machine Learning</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="volume">70</biblScope>
			<biblScope unit="page" from="2024" to="2033" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Learning convolutional neural networks for graphs</title>
		<author>
			<persName><forename type="first">M</forename><surname>Niepert</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Ahmed</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Kutzkov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 33rd International Conference on International Conference on Machine Learning</title>
				<meeting>the 33rd International Conference on International Conference on Machine Learning</meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="volume">48</biblScope>
			<biblScope unit="page" from="2014" to="2023" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Heterogeneous graph attention network</title>
		<author>
			<persName><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Ye</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">C</forename></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Yu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The World Wide Web Conference</title>
				<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="2022" to="2032" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Convolutional neural networks on graphs with fast localized spectral filtering</title>
		<author>
			<persName><forename type="first">M</forename><surname>Defferrard</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Bresson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Vandergheynst</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 30th International Conference on Neural Information Processing Systems</title>
				<meeting>the 30th International Conference on Neural Information Processing Systems</meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="3844" to="3852" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Spectral networks and locally connected networks on graphs</title>
		<author>
			<persName><forename type="first">J</forename><surname>Bruna</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Zaremba</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Szlam</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2nd International Conference on Learning</title>
				<meeting>the 2nd International Conference on Learning</meeting>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Convolutional networks on graphs for learning molecular fingerprints</title>
		<author>
			<persName><forename type="first">D</forename><surname>Duvenaud</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Maclaurin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Aguilera-Iparraguirre</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Gรณmez-Bombarelli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Hirzel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Aspuru-Guzik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">P</forename><surname>Adams</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 28th International Conference on Neural Information Processing Systems</title>
				<meeting>the 28th International Conference on Neural Information Processing Systems</meeting>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="2224" to="2232" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Inductive representation learning on large graphs</title>
		<author>
			<persName><forename type="first">W</forename><surname>Hamilton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Ying</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Leskovec</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
				<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="1024" to="1034" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Addgraph: Anomaly detection in dynamic graph using attention-based temporal gcn</title>
		<author>
			<persName><forename type="first">L</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Gao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Twenty-Eighth International Joint Conference on Artificial Intelligence</title>
				<meeting>the Twenty-Eighth International Joint Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page" from="4419" to="4425" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Neural graph collaborative filtering</title>
		<author>
			<persName><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T.-S</forename><surname>Chua</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 42Nd International ACM SIGIR Conference on Research and Development in Information Retrieval</title>
				<meeting>the 42Nd International ACM SIGIR Conference on Research and Development in Information Retrieval</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="165" to="174" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Hierarchical representation learning for bipartite graphs</title>
		<author>
			<persName><forename type="first">C</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">R</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Twenty-Eighth International Joint Conference on Artificial Intelligence, IJCAI-19</title>
				<meeting>the Twenty-Eighth International Joint Conference on Artificial Intelligence, IJCAI-19</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page" from="2873" to="2879" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Hierarchical graph representation learning with differentiable pooling</title>
		<author>
			<persName><forename type="first">R</forename><surname>Ying</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>You</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Morris</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">L</forename><surname>Hamilton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Leskovec</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 32Nd International Conference on Neural Information Processing Systems</title>
				<meeting>the 32Nd International Conference on Neural Information Processing Systems</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="4805" to="4815" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Dynamic edge-conditioned filters in convolutional neural networks on graphs</title>
		<author>
			<persName><forename type="first">M</forename><surname>Simonovsky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Komodakis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2017 IEEE Conference on Computer Vision and Pattern Recognition</title>
				<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="29" to="38" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Shoal: Large-scale hierarchical taxonomy via graph-based query coalition in e-commerce</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Zou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Yu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. VLDB Endow</title>
				<meeting>VLDB Endow</meeting>
		<imprint>
			<date type="published" when="2019-08">Aug. 2019</date>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="page" from="1858" to="1861" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Discriminative embeddings of latent variable models for structured data</title>
		<author>
			<persName><forename type="first">H</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Song</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 33rd International Conference on International Conference on Machine Learning</title>
				<meeting>the 33rd International Conference on International Conference on Machine Learning</meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="volume">48</biblScope>
			<biblScope unit="page" from="2702" to="2711" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Neural message passing for quantum chemistry</title>
		<author>
			<persName><forename type="first">J</forename><surname>Gilmer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">S</forename><surname>Schoenholz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">F</forename><surname>Riley</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">E</forename><surname>Dahl</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 34th International Conference on Machine Learning</title>
				<meeting>the 34th International Conference on Machine Learning</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="volume">70</biblScope>
			<biblScope unit="page" from="1263" to="1272" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Schnet: A continuous-filter convolutional neural network for modeling quantum interactions</title>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">T</forename><surname>Schรผtt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P.-J</forename><surname>Kindermans</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">E</forename><surname>Sauceda</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Chmiela</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Tkatchenko</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K.-R</forename><surname>Mรผller</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 31st International Conference on Neural Information Processing Systems</title>
				<meeting>the 31st International Conference on Neural Information Processing Systems</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="992" to="1002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Gated graph sequence neural networks</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Zemel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Brockschmidt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Tarlow</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 3rd International Conference on Learning</title>
				<meeting>the 3rd International Conference on Learning</meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Birank: Towards ranking on bipartite graphs</title>
		<author>
			<persName><forename type="first">X</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M.-Y</forename><surname>Kan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. on Knowl. and Data Eng</title>
		<imprint>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="57" to="71" />
			<date type="published" when="2017-01">Jan. 2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Recwalk: Nearly uncoupled random walks for top-n recommendation</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">N</forename><surname>Nikolakopoulos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Karypis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Twelfth ACM International Conference on Web Search and Data Mining</title>
				<meeting>the Twelfth ACM International Conference on Web Search and Data Mining</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="150" to="158" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Hop-rec: high-order proximity for implicit recommendation</title>
		<author>
			<persName><forename type="first">J</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Tsai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 12th ACM Conference on Recommender Systems</title>
				<meeting>the 12th ACM Conference on Recommender Systems</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="140" to="144" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Parallel matrix factorization for recommender systems</title>
		<author>
			<persName><forename type="first">H.-F</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C.-J</forename><surname>Hsieh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Si</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><forename type="middle">S</forename><surname>Dhillon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Knowl. Inf. Syst</title>
		<imprint>
			<biblScope unit="volume">41</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="793" to="819" />
			<date type="published" when="2014-12">Dec. 2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Deep matrix factorization models for recommender systems</title>
		<author>
			<persName><forename type="first">H.-J</forename><surname>Xue</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X.-Y</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 26th International Joint Conference on Artificial Intelligence</title>
				<meeting>the 26th International Joint Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="3203" to="3209" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Deep autoencoder-like nonnegative matrix factorization for community detection</title>
		<author>
			<persName><forename type="first">F</forename><surname>Ye</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Zheng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 27th ACM International Conference on Information and Knowledge Management</title>
				<meeting>the 27th ACM International Conference on Information and Knowledge Management</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="1393" to="1402" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Click-through prediction for advertising in twitter timeline</title>
		<author>
			<persName><forename type="first">C</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Mei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Pandey</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 21th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining</title>
				<meeting>the 21th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="1959" to="1968" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Ontology construction for information classification</title>
		<author>
			<persName><forename type="first">S</forename><surname>Weng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Tsai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Hsu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Expert Syst. Appl</title>
		<imprint>
			<biblScope unit="volume">31</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1" to="12" />
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Efficient estimation of word representations in vector space</title>
		<author>
			<persName><forename type="first">T</forename><surname>Mikolov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Corrado</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Dean</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 1st International Conference on Learning</title>
				<meeting>the 1st International Conference on Learning<address><addrLine>Scottsdale, Arizona, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2013">May 2-4, 2013. 2013</date>
		</imprint>
	</monogr>
	<note>Workshop Track Proceedings</note>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">A dendrite method for cluster analysis</title>
		<author>
			<persName><forename type="first">T</forename><surname>Caliลski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Harabasz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Communications in Statistics-theory and Methods</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1" to="27" />
			<date type="published" when="1974">1974</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Taxogen: Unsupervised topic taxonomy construction by adaptive term embedding and clustering</title>
		<author>
			<persName><forename type="first">C</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Tao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">M</forename><surname>Sadler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Vanni</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Han</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 24th ACM SIGKDD International Conference on Knowledge Discovery &amp; Data Mining</title>
				<meeting>the 24th ACM SIGKDD International Conference on Knowledge Discovery &amp; Data Mining</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="2701" to="2709" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
