<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Video Saliency Prediction using Spatiotemporal Residual Attentive Networks</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Qiuxia</forename><surname>Lai</surname></persName>
							<email>qxlai@cse.cuhk.edu.hk</email>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science and Engineering</orgName>
								<orgName type="institution">The Chinese University of Hong Kong</orgName>
								<address>
									<settlement>Hong Kong</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><roleName>Member, IEEE</roleName><forename type="first">Wenguan</forename><surname>Wang</surname></persName>
							<email>wenguanwang.ai@gmail.com</email>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science and Engineering</orgName>
								<orgName type="institution">The Chinese University of Hong Kong</orgName>
								<address>
									<settlement>Hong Kong</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><roleName>Member, IEEE</roleName><forename type="first">Hanqiu</forename><surname>Sun</surname></persName>
							<email>hanqiu@cse.cuhk.edu.hk</email>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science and Engineering</orgName>
								<orgName type="institution">The Chinese University of Hong Kong</orgName>
								<address>
									<settlement>Hong Kong</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><roleName>Senior Member, IEEE</roleName><forename type="first">Jianbing</forename><surname>Shen</surname></persName>
							<email>shenjianbingcg@gmail.com</email>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science and Engineering</orgName>
								<orgName type="institution">The Chinese University of Hong Kong</orgName>
								<address>
									<settlement>Hong Kong</settlement>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="department">School of Computer Science</orgName>
								<orgName type="laboratory">Beijing Laboratory of Intelligent Information Technology</orgName>
								<orgName type="institution">Beijing Institute of Technology</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Video Saliency Prediction using Spatiotemporal Residual Attentive Networks</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">CF1ADDB46AE5EA843F19416847357C7D</idno>
					<idno type="DOI">10.1109/TIP.2019.2936112</idno>
					<note type="submission">This article has been accepted for publication in a future issue of this journal, but has not been fully edited. Content may change prior to final publication. Citation information: DOI 10.1109/TIP.2019.2936112, IEEE Transactions on Image Processing IEEE TRANSACTIONS ON IMAGE PROCESSING 1 This article has been accepted for publication in a future issue of this journal, but has not been fully edited. Content may change prior to final publication. Citation information: DOI 10.1109/TIP.2019.2936112, IEEE Transactions on Image Processing IEEE TRANSACTIONS ON IMAGE PROCESSING 2</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.3" ident="GROBID" when="2023-07-28T12:19+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Dynamic eye-fixation prediction</term>
					<term>residual attentive learning</term>
					<term>attention mechanism</term>
					<term>deep learning</term>
					<term>video saliency</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>This paper proposes a novel residual attentive learning network architecture for predicting dynamic eye-fixation maps. The proposed model emphasizes two essential issues, i.e., effective spatiotemporal feature integration and multi-scale saliency learning. For the first problem, appearance and motion streams are tightly coupled via dense residual cross connections, which integrate appearance information with multi-layer, comprehensive motion features in a residual and dense way. Beyond traditional two-stream models learning appearance and motion features separately, such design allows early, multi-path information exchange between different domains, leading to a unified and powerful spatiotemporal learning architecture. For the second one, we propose a composite attention mechanism that learns multi-scale local attentions and global attention priors endto-end. It is used for enhancing the fused spatiotemporal features via emphasizing important features in multi-scales. A lightweight convolutional Gated Recurrent Unit (convGRU), which is flexible for small training data situation, is used for long-term temporal characteristics modeling. Extensive experiments over four benchmark datasets clearly demonstrate the advantage of the proposed video saliency model over other competitors and the effectiveness of each component of our network. Our code and all the results will be available at https://github.com/ashleylqx/STRA-Net.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>models <ref type="bibr" target="#b4">[5]</ref>- <ref type="bibr" target="#b11">[12]</ref> were proposed for imitating the mechanism of visual attention allocation in static scenes. Those deep static saliency models are able to learn flexible and powerful saliency representations from large-scale eye-tracking data and generally obtain a large margin of performance gain compared with previous non-deep learning static models <ref type="bibr" target="#b12">[13]</ref>- <ref type="bibr" target="#b15">[16]</ref>.</p><p>In striking contrast to the significant advance of static visual attention prediction in recent years (back to <ref type="bibr" target="#b4">[5]</ref>), there are only very few deep learning based visual attention models <ref type="bibr" target="#b16">[17]</ref>- <ref type="bibr" target="#b20">[21]</ref> that are specially designed for modeling eye fixation during dynamic free-viewing. Different from static visual attention prediction, in dynamic videos, both spatial and temporal information are essential factors that guide human attention orientation, as suggested by many studies in cognitive science and computer vision. Although rich motion information in the extra temporal domain bring more cues for predicting visual attention, complex motion patterns from the background, inconsistent movements among different foreground patterns, camera motions, and the optical flow computation error, all make video saliency prediction more difficult. Hence, how to effectively integrate saliency features from different domains and scales is one essential but long-time unsolved problem for dynamic visual attention prediction.</p><p>Recent deep dynamic visual attention models have shown improved performance, however, none of them well handle the above challenges. Although appearance and motion information are learned by an architecture consisting of convolutional neural network and Long-Short Term Memory (CNN-LSTM) <ref type="bibr" target="#b16">[17]</ref>, or more informative cues such as optical flow <ref type="bibr" target="#b17">[18]</ref> and objectness <ref type="bibr" target="#b18">[19]</ref> are explicitly captured by different network streams, those touched few (1) how to fuse saliency features from different domains; and (2) how to learn saliency representations from multi-scales. For two-stream models <ref type="bibr" target="#b17">[18]</ref>, <ref type="bibr" target="#b18">[19]</ref>, saliency information from spatial and temporal streams are fused only once (in early or late fusion strategy), which decreases the representative power and cannot fully leverage rich information encoded in appearance and motion network streams. For CNN-LSTM architecture based model <ref type="bibr" target="#b16">[17]</ref>, where motion information is captured by a Recurrent Neural Network (RNN), it requires a lot of static and dynamic eyetracking data for decoupling spatial and temporal saliency information, due to the limited representation power of LSTM. Additionally, they largely neglect the importance of learning saliency information from multi-scales, while only considering saliency information within one single network layer.</p><p>Aiming for alleviating the above challenges in video saliency detection and remedying the shortages of previous deep dynamic attention models, we propose a spatiotemporal residual attentive network, which emphasizes comprehensive fusion of spatial and temporal saliency features, and multi-scale saliency representation enhancement simultaneously. More specially, inspired by the recent advance of residual network <ref type="bibr" target="#b21">[22]</ref> and video classification <ref type="bibr" target="#b22">[23]</ref>, we design a spatiotemporal residual network with dense residual cross connections between the layers from appearance and motion streams, for encouraging information flow between different stream layers and deeper integration of saliency cues from different domains. For further emphasizing multi-scale saliency representation learning, we introduce a composite attention module that learns attentions from multiple scales, namely a stack of local attentions as well as global attention priors. The local attentions act as a network attention mechanism that suppresses irrelevant saliency features, while the global attentions capture the center bias of visual attention allocation in free-viewing. Considering the relatively small amount of dynamic eye-tracking data, we introduce convolutional Gated Recurrent Unit (convGRU) <ref type="bibr" target="#b23">[24]</ref>, a lightweight recurrent network, for learning the temporal attention transitions across time. Compared with traditional RNN, it could better capture long-term dependencies and handle gradient vanishing/expoding problem. Its architecture is also relatively simple compared with LSTM, thus it is more applicable for small training data situation.</p><p>To summarize, the contributions of this work are three-fold:</p><p>• A spatiotemporal residual neural network, which consists of two tightly coupled streams for capturing and fusing saliency features in spatial and temporal domains, is proposed for dynamic visual attention prediction. With the dense residual cross connections among different layers of the network streams, spatial and temporal features are integrated in a more comprehensive way, leading to a powerful spatiotemporal saliency representation. • A composite attention module is integrated for enhancing spatiotemporal saliency representation with multi-scale information. The composite attention mechanism learns local attentions as well as global attention priors for emphasizing the informative saliency features and filtering out useless information, thus improving the spatiotemporal saliency representation efficiently. • ConvGRU, as a lightweight recurrent network, is introduced for modeling the attention transitions across video frames, which allows more efficient saliency learning with relative paucity of dynamic eye-tracking data. To the best of our knowledge, this is the first application of convGRU in the task of human visual attention prediction. Extensive experiments on four large-scale video saliency datasets (i.e., DHF1K <ref type="bibr" target="#b16">[17]</ref>, Hollywood2 <ref type="bibr" target="#b24">[25]</ref>, UCF-sports <ref type="bibr" target="#b24">[25]</ref> and DIEM <ref type="bibr">[26]</ref>) clearly demonstrate that the proposed video saliency model outperforms other competitors.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>II. RELATED WORK</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Computational Models for Visual Attention Prediction</head><p>The concept of visual saliency in computer vision is typically modeled as two tasks, namely visual attention prediction which aims at predicting human eye fixation, and salient object detection (SOD) that highlights salient regions in images. The SOD task is driven by object-level applications <ref type="bibr" target="#b26">[27]</ref>, and covers a wide range of sub-tasks including RGB saliency <ref type="bibr" target="#b27">[28]</ref>, <ref type="bibr" target="#b28">[29]</ref>, RGBD saliency <ref type="bibr" target="#b29">[30]</ref>, video SOD <ref type="bibr" target="#b30">[31]</ref>- <ref type="bibr" target="#b35">[36]</ref>, and cosaliency <ref type="bibr" target="#b36">[37]</ref>, <ref type="bibr" target="#b37">[38]</ref>. We refer interested readers to <ref type="bibr" target="#b38">[39]</ref>, <ref type="bibr" target="#b39">[40]</ref> for more detailed introduction of SOD. In this paper, we focus on the task of predicting visual attention for videos.</p><p>The work of Itti <ref type="bibr" target="#b0">[1]</ref> represents an early attempt that builds a computational model for predicting human eye fixation. After that, a rich set of saliency models are developed in computer vision community. Here, we mainly focus on discussing representative works, especially recent deep learning based visual attention models. According to the input format, these models can be classified as either static or dynamic method.</p><p>1) Static models: Earlier image attention models were inspired by cognitive theories of visual attention, and typically based on bottom-up, stimuli-driven mechanism. They leverage and analyse various biologically inspired features such as color, edge and orientations on different spatial scales <ref type="bibr" target="#b40">[41]</ref>- <ref type="bibr" target="#b45">[46]</ref> or frequency domain <ref type="bibr" target="#b46">[47]</ref>. Then the final attention map is built bottom-up by fusing the saliency estimates from these features, where the locations with distinct features over their neighborhoods usually come out with higher saliency values.</p><p>Recently, several deep saliency models have been proposed. Compared with traditional methods, these models achieved significantly better results, benefited from large-scale datasets and the strong learning ability of neural network. Vig et al. <ref type="bibr" target="#b4">[5]</ref> search for the optimal ensemble of CNNs as deep feature extractor, and train an SVM for predicting the probability of an image region being salient. This is the first work that applies deep learning for visual attention. After that, some models like DeepFix <ref type="bibr" target="#b9">[10]</ref>, DeepNet <ref type="bibr" target="#b7">[8]</ref> and SALICON <ref type="bibr" target="#b5">[6]</ref> are built via fine-tuning existing top-performance image classification models (e.g., VGG-16 <ref type="bibr" target="#b47">[48]</ref>) on eye-tracking data. Mr-CNN <ref type="bibr" target="#b8">[9]</ref> applies a multi-stream network to capture multi-scale saliency. DVA <ref type="bibr" target="#b10">[11]</ref> fuses multi-layer features and thus incorporates multi-level saliency predictions within a single network.</p><p>2) Dynamic models: Although a huge amount of static saliency models have been proposed in the last two decades, there are far less effort in mimicking eye fixation behavior in dynamic scenes. Traditional dynamic models <ref type="bibr" target="#b12">[13]</ref>- <ref type="bibr" target="#b14">[15]</ref> leverage hand-crafted features in both spatial and temporal domains, and most of them can be viewed as extensions of static models by incorporating extra motion features <ref type="bibr" target="#b48">[49]</ref>- <ref type="bibr" target="#b51">[52]</ref>. Some other methods resort to features by video compression since those features are more related to visual attention and can be highly predictive of human eye fixations <ref type="bibr" target="#b52">[53]</ref>- <ref type="bibr" target="#b55">[56]</ref>.</p><p>So far, there are only very few deep dynamic saliency models. Bak et al. <ref type="bibr" target="#b17">[18]</ref> first propose to utilize a two-stream CNN architecture that learns both appearance and motion saliency information. Later, Bazzani et al. <ref type="bibr" target="#b56">[57]</ref> assume human attention distribution as Gaussian Mixture Model (GMM) and use LSTM to learn the GMM. Jiang et al. <ref type="bibr" target="#b18">[19]</ref> propose a multistream model that considers appearance, motion, and objectness together. More recently, Wang et al. <ref type="bibr" target="#b16">[17]</ref> incorporated a supervised static attention mechanism into a CNN-LSTM network and achieved promising results. All above works show the advantages of applying deep learning to this problem, while they largely neglect the issue of effectively fusing multi- Fig. <ref type="figure">1</ref>: Architectures of (a) two-stream network <ref type="bibr" target="#b17">[18]</ref>, (b) OM-CNN + 2C-LSTM network <ref type="bibr" target="#b18">[19]</ref>, (c) ACLNet model <ref type="bibr" target="#b16">[17]</ref>, and (d) our proposed spatiotemporal residual attentive network. See Section II-C for more details.</p><p>domain features and learning saliency representations from multi-scales. In Section II-C, we will offer more detailed discussion for the differences with previous models.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Related Network Design</head><p>Next we give a brief overview of related network architectures, which inspire the design of our model.</p><p>1) Two-stream structure: Two-stream network was first proposed by Simonyan et al. <ref type="bibr" target="#b57">[58]</ref> for video action recognition, and then used in many spatiotemporal tasks <ref type="bibr" target="#b22">[23]</ref>, <ref type="bibr" target="#b58">[59]</ref>- <ref type="bibr" target="#b60">[61]</ref>. It has two parallel streams that take video frames and optical flows as inputs, for capturing appearance and motion information, respectively. Inspired by <ref type="bibr" target="#b61">[62]</ref>, we propose a spatiotemporal residual network that fuses the spatial and temporal information using dense residual cross connections between the corresponding layers of the appearance and motion streams, which allows more effective interaction during the training process. The motion feature serves as gating signal to the appearance feature, which is able to filter out indeterminant spatial information and inject rich temporal information.</p><p>2) Attention mechanism in neural network: It is observed a popular trend to incorporate attention mechanism into network design. Such attention mechanism lets the network focus more on the most important and task-relevant contents. It has shown great successes in computer vision <ref type="bibr" target="#b62">[63]</ref>- <ref type="bibr" target="#b64">[65]</ref> and natural language processing communities <ref type="bibr" target="#b65">[66]</ref>, <ref type="bibr" target="#b66">[67]</ref>. Such attention can be learned in an implicitly way and enhance network features.</p><p>Here we introduce a composite attention module that learns a stack of local attentions and global human attention bias, for emphasizing multi-scale saliency representation learning.</p><p>3) Gated Recurrent Unit (GRU): The GRU <ref type="bibr" target="#b67">[68]</ref> is lighterweighted over LSTM, while achieving similar performance in capturing long-term dependencies. Also, it can better handle gradient vanishing and exploding problems compared with classical RNN. The convGRU inherits the advantages of fully connected GRU and is further able to preserve spatial information, which is essential for pixel-level saliency prediction task. So far, convGRU was only applied in limited applications such as learning video representation features <ref type="bibr" target="#b23">[24]</ref> and video segmentation <ref type="bibr" target="#b68">[69]</ref>. In this work, we introduce convGRU for learning attention transitions across video frames.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Comparison with Previous Works</head><p>In this section, we discuss three representative network architectures of current top-performing deep video saliency models <ref type="bibr" target="#b16">[17]</ref>- <ref type="bibr" target="#b18">[19]</ref>, and detail the differences between these models and our method. This would be better to highlight our contributions. A schematic illustration of these network architectures is shown in Fig. <ref type="figure">1</ref>.</p><p>1) Two-stream network <ref type="bibr" target="#b17">[18]</ref>: Bak et al. propose a twostream network for video saliency prediction, where two separated branches take video frames and optical flows as inputs, and learn appearance and motion information respectively (see Fig. <ref type="figure">1 (a)</ref>). However, the appearance and motion features are only fused lately, which is not efficient enough to learn comprehensive spatiotemporal features due to the lack of information exchange between the two streams.</p><p>2) OM-CNN + 2C-LSTM network <ref type="bibr" target="#b18">[19]</ref>: Based on the observation that human attention is normally attracted by objects, Jiang et al. 3) Attentive CNN-convLSTM Network (ACLNet) <ref type="bibr" target="#b16">[17]</ref>: ACLNet <ref type="bibr" target="#b16">[17]</ref> is based on a typical CNN-LSTM architecture which combines the convolutional and recurrent networks in a single stream to learn the spatial and temporal information, as shown in Fig. <ref type="figure">1 (c</ref>). It incorporates a supervised attention mechanism to enhance intra-frame salient features with encoded static saliency information learned explicitly from image saliency dataset. However, ACLNet cannot efficiently capture the motion dynamics due to the limited learning power of LSTM and lack of explicit motion representation.</p><p>4) The proposed model: Our model is significantly differentiated in three aspects. First, by adding multiple residual cross connections between the two stream layers, our model allows multi-domain features fusion in a more efficient and earlier way. Such design learns a more powerful spatiotemporal model, rather than previous methods fusing appearance and motion in a shallow manner. Second, our model is equipped with a composite attention mechanism for enhancing spatiotemporal features in multi-scales. Instead of ACLNet learning attention mechanism from one single layer, the proposed composite attention mechanism emphasizes multi-scales and captures human attention bias as global prior. Third, to 1057-7149 (c) 2019 IEEE. Personal use is permitted, but republication/redistribution requires IEEE permission. See http://www.ieee.org/publications_standards/publications/rights/index.html for more information. model the attention transitions over time, we augment our model with a lightweight recurrent layer, i.e., convGRU, which is more flexible for learning from relatively small dynamic fixation datasets, compared with complex LSTM structure.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>III. OUR APPROACH</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Architecture Overview</head><p>Video carries both spatial and temporal information in the form of the individual frame appearance and the motion across frames, respectively. Such nature inspires us to build our model upon a two-stream architecture to better process the appearance and motion information, as shown in Fig. <ref type="figure" target="#fig_2">2</ref>.</p><p>Two parallel DNN streams are used to handle the spatial and temporal information of an input video, i.e. the RGB frames and the optical flows, respectively. To learn the comprehensive spatiotemporal saliency representation, we fuse the saliency features from two streams by incorporating dense residual cross connections among different layers of two DNNs, in Section III-B. To further enhance the spatiotemporal saliency representations with multi-scale information, in Section III-C, we incorporate a composite attention module to learn the local and global attention priors. To learn the temporal attention transitions more efficiently from limited data, we introduce convGRU, a lightweight RNN structure, into our network. More details can be found in Section III-D. In Section III-F, we present the implementation details and training protocols.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Spatiotemporal Residual Network</head><p>1) Residual Learning: Before going deep into our spatiotemporal residual learning framework, we first give a brief introduction of residual learning <ref type="bibr" target="#b21">[22]</ref>, which is used as the building block of our model. Let x and H(x) denote the input feature and the desired underlying mapping, residual learning is learn the residual of identity mapping F(x) = H(x) -x, instead of the original, unreferenced mapping H(x) (see Fig. <ref type="figure" target="#fig_3">3  (a)</ref>). Thus H(x) can be computed as:</p><formula xml:id="formula_0">H(x) = x + F(x).<label>(1)</label></formula><p>Such kind of residual design can ease the training of relatively deep network architecture.</p><p>2) Appearance and Motion Streams: Our appearance and motion streams contain several residual blocks (borrowed from the five convolution blocks of ResNet-50 <ref type="bibr" target="#b21">[22]</ref>). The l-th (l ∈ {1, . . . , L}) residual block is defined as:</p><formula xml:id="formula_1">x l = x l-1 + F (x l-1 ; W l ) ,<label>(2)</label></formula><p>where x l-1 and x l are the input and output of the l-th block, respectively, and F is the nonlinear mapping with weights W l . Given a set of video frames {I t } T t=1 and the corresponding optical flows {O t } T t=1 . The appearance stream takes one single video frame I t as input, and produce the appearance feature. The optical flows {O t , O t+1 , . . . , O t+S } from the neighboring S frames are fed into the motion stream for obtaining corresponding motion features.</p><p>3) Dense Residual Connection across Two Streams: Previous methods largely build the two streams separately; only adopting late fusion 'outside' of the two streams. There is no information exchange between the two streams. Differently, in our model the two streams are more tightly incorporated to learn more comprehensive spatial and temporal features. This is achieved via adding dense residual cross connections between the corresponding layers (see Fig. <ref type="figure" target="#fig_2">2</ref> and Fig. <ref type="figure" target="#fig_3">3 (c)</ref>).</p><p>Considering the fact that spatial stream tends to dominate the motion stream during training <ref type="bibr" target="#b61">[62]</ref>, we first inject the motion features x m into the appearance stream, resulting in a cross connection strategy (see Fig. <ref type="figure" target="#fig_3">3 (d)</ref>). Intuitively, the appearance feature x a l in l-th residual layer of appearance stream can be improved via: where ' ' denotes the Hadamard product and x m l is the motion signal from the corresponding layer of motion stream, serving as a gate mechanism to enhance the appearance information. However, some tiny values in x m l may greatly suppress the appearance information. To fix ideas, we improve above equation with a residual form:</p><formula xml:id="formula_2">x a l ← x a l x m l ,<label>(3)</label></formula><formula xml:id="formula_3">x a l ← x a l + x a l x m l .<label>(4)</label></formula><p>With above residual cross connection design (Fig. <ref type="figure" target="#fig_3">3</ref> (e)), potentially useful information in the original appearance feature can be preserved even if the motion signal closes to zero. Further, we densely connect appearance feature in a certain layer l with a stack of motion features from previous lower levels: {1, . . . , l}, yielding a dense residual cross connection architecture (Fig. <ref type="figure" target="#fig_3">3 (f)</ref>), which can be formulated as:</p><formula xml:id="formula_4">xl = x a l + x a l C({x m i } l i=1 ; W c l ),<label>(5)</label></formula><p>where C is a nonlinear mapping with weight W c l which concatenates and squeezes the motion signals from different blocks. The x represents the improved feature. The resulted spatiotemporal saliency representation with richer temporal information are more robust w.r.t. complex scenes.</p><p>Fig. <ref type="figure" target="#fig_4">4</ref> shows the predictions w. and w/o. spatiotemporal feature fusions. As seen, the model which effectively fuses saliency information from different domains can focus precisely on where human tend to look at. We further quantitatively demonstrate the effectiveness of the dense residual cross connections in Section IV-C.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Composite Attention Mechanism</head><p>The involvement of attention mechanism in neural network shows effectiveness in many tasks <ref type="bibr" target="#b62">[63]</ref>- <ref type="bibr" target="#b66">[67]</ref>, since attention mechanism imposes the model to attend to the most taskrelevant part of the inputs or features. In this section, we propose a composite attention mechanism to emphasize multiscale saliency representation learning. It has a local attention module for enhancing spatiotemporal features in multi-scales, Let xl ∈ R W l ×H l ×C l be the spatiotemporal saliency feature at the l-th layer of the appearance branch. The goal of the local attention module is to learn an attention mask M l ∈ [0, 1] W l ×H l to softly weight xl based on the saliency information it carries. Specially, to reflect the importance of different regions with the attention mask, we apply a softmax </p><formula xml:id="formula_5">(M l ) i,j = exp M (x l ; W att l ) i,j W l w=1 H l h=1 exp M (x l ; W att l ) w,h ,<label>(6)</label></formula><p>where W att l is the weight of non-linear mapping M that maps the spatiotemporal feature xl to a single channel mask, and the footnote i, j denotes the 2D spatial coordinate (i ∈ {1, . . . , W l }, j ∈ {1, . . . , H l }). In this way, the local attention module learns the normalized probability of each region of current spatiotemporal feature being important. The attention masks from different levels highlight multi-scale important areas and guide the network to gradually focus on the salient part. The enhanced feature can be given as:</p><formula xml:id="formula_6">(x l ) c ← M l (x l ) c ,<label>(7)</label></formula><p>where c ∈ {1, . . . , C l } is the channel index. However, directly multiplying the mask with feature may mistakenly discard important information at earlier training stage when the local attention module has not been well trained with selectiveness.</p><p>In view of this, we introduce an identity mapping again, and re-formulate the local attention module in Eq. ( <ref type="formula" target="#formula_6">7</ref>) as:</p><formula xml:id="formula_7">(x l ) c ← (x l ) c + M l (x l ) c .<label>(8)</label></formula><p>The local attention module with residual structure avoids introducing drastic changes into the original features, thus allowing more effective learning. To further guarantee the discriminative ability of the enhanced spatiotemporal representations, we deeply supervise <ref type="bibr" target="#b72">[73]</ref> the side-outputs of the multi-scale saliency features with ground truth fixation data, as illustrated in Fig. <ref type="figure" target="#fig_2">2</ref>. Fig. <ref type="figure" target="#fig_5">5</ref> (b) visualizes the learned attentions at different scales, where the higher level attention gradually focus on the area where human tends to look at. With the enhancement of multi-scale attention, our model is able to make more precise predictions (see Fig. <ref type="figure" target="#fig_5">5 (c)</ref> and<ref type="figure">(d)</ref>).</p><p>The enhanced saliency feature xl is then fed to the appearance stream and contributes to the appearance feature of next residual block. The calculation of the appearance features in Eq. ( <ref type="formula" target="#formula_1">2</ref>) can be re-written as:</p><formula xml:id="formula_8">x a l = xl-1 + F (x l-1 ; W a l ) .<label>(9)</label></formula><p>2) Global attention module: Photographers tend to position the important objects around the center when taking photos or recording videos, which encourages people to look for objects of interest at center when they are used to watching such kind of photos or videos <ref type="bibr" target="#b73">[74]</ref>. Previous studies also showed that people tend to watch the center when there are no highly attractive contents in the images or videos <ref type="bibr" target="#b74">[75]</ref>. Here we propose a global attention module to directly learn the nature bias of human fixation from the dynamic visual attention data.</p><p>The prior statistics of visual attention can be approximately modeled as a mixture of Gaussians <ref type="bibr" target="#b56">[57]</ref>. Each Gaussian can be modeled by a 2D Gaussian function:</p><formula xml:id="formula_9">f (x, y) = 1 2πρ x ρ y exp - (x -µ x ) 2 2ρ 2 x + (y -µ y ) 2 2ρ 2 y ,<label>(10)</label></formula><p>where µ x , µ y and ρ x , ρ y are means and standard deviations along two axes, respectively.</p><p>To learn the parameters of the mixture of Gaussians, a prior learning module <ref type="bibr" target="#b75">[76]</ref> is incorporated which can be trained endto-end with the dynamic fixation data. The corresponding prior maps are generated using the learned parameters, and then concatenated to the spatiotemporal features to further enhance the features with the global attention that are directly learned from training data. The visualized prior maps under different training settings are shown in Fig. <ref type="figure" target="#fig_6">6</ref>, where different training data may result in different global priors.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D. ConvGRU</head><p>RNN is suitable for handling sequential data and is able to model the temporal relation within the sequence. Each recurrent cell has a hidden unit to maintain a dynamic memory according to the current hidden state and the new input: where x, y, h represent the input, output, and hidden states, respectively, Ws are the learnable weights and φ is the activation function. For video related tasks, a natural thought is to resort to RNN for modeling the temporal transitions. However, such vanilla RNN easily suffers from gradient vanishing/exploding. One promising solution is a newly proposed recurrent unit, GRU <ref type="bibr" target="#b67">[68]</ref>, which alleviates the above problem by introducing gating units to control the information flow across time steps:</p><formula xml:id="formula_10">h t = W h h φ(h t-1 ) + W h x x t , y t = W y h φ(h t ),<label>(11</label></formula><formula xml:id="formula_11">z t = σ(W z h h t-1 + W z x x t ), r t = σ(W r h h t-1 + W r x x t ), ht = tanh(W h h (r t h t-1 ) + W h x x t ), h t = z t ht + (1 -z t )h t-1 , (<label>12</label></formula><formula xml:id="formula_12">)</formula><p>where the update gate z t controls how much previous hidden state h t-1 to be kept, and the reset gate r t defines how to combine current input with the previous hidden state. Compared with another popular candidate, i.e., LSTM, GRU can model the long-term relationships with simpler architecture, thus is more applicable when the training datasets are relatively small.</p><p>To better formulate the visual attention transitions across video frames, we adapt GRU for handling multi-dimensional inputs by replacing the matrix multiplications with convolution operations, which is formulated as:</p><formula xml:id="formula_13">z t = σ(W z h * h t-1 + W z x * x t ), r t = σ(W r h * h t-1 + W r x * x t ), ht = tanh(W h h * (r t h t-1 ) + W h x * x t ), h t = z t ht + (1 -z t )h t-1 . (13)</formula><p>where' * ' is the convolution operator and all the parameters, i.e., x, z, r, h, Ws, are 3D tensors. An illustration of convGRU architecture can be found in Fig. <ref type="figure" target="#fig_2">2</ref>. The convGRU module in our network takes the sequence of the top-level spatiotemporal features {x t L } T t=1 as inputs to learn the dynamic fixation transitions. The output sequence of features accord with the spatiotemporal pattern of dynamic visual attention and are temporally smoother in modeling the attention transitions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E. Loss Function</head><p>Similar to <ref type="bibr" target="#b5">[6]</ref>, our loss function is a linear combination of four loss terms adapted from saliency evaluation metrics, which can comprehensively measure the quality of the predictions. The proposed loss function is defined as:</p><formula xml:id="formula_14">L(P, Q, G) = L kl (P, G) + ω 1 L cc (P, G) + ω 2 L nss (P, Q) + ω 3 L sim (P, G), (<label>14</label></formula><formula xml:id="formula_15">)</formula><p>where P is the predicted attention map, Q is the ground-truth binary fixation map, and G is the continuous ground-truth attention map. ωs are scalars to balance the four loss terms. L kl is derived from the Kullback-Leibler (KL) divergence metric. It views P, Q as probability distributions and measures the information loss of using P to approximate G: L cc is adopted from the Linear Correlation Coefficient (CC) metric which measures the linear relationship between two random variables. The calculation is defined as follows:</p><formula xml:id="formula_16">L kl (P, G) = i G i log G i P i . (<label>15</label></formula><formula xml:id="formula_17">)</formula><formula xml:id="formula_18">L cc (P, G) = - cov(P, G) ρ(P)ρ(G) ,<label>(16)</label></formula><p>where cov(P, G) is the covariance of P and G, and ρ(•) represents the standard deviation. L nss is from the Normalized Scanpath Saliency (NSS) metric calculating the average saliency value at positive positions:</p><formula xml:id="formula_19">L nss (P, Q) = - 1 N i P i -µ(P) ρ(P) • Q i ,<label>(17)</label></formula><p>where µ(•) and ρ(•) are the mean and the standard deviation, respectively. N is the number of positive pixels in Q.</p><p>L sim is adopted from the Similarity (SIM) metric concerning the intersection between two probability distributions. Larger interaction indicates stronger similarity:</p><formula xml:id="formula_20">L sim (P, G) = - i min(P i , G i ). (<label>18</label></formula><formula xml:id="formula_21">)</formula><p>In the calculation of L sim , P and G are normalized to be probability distributions P i and G i satisfying i P i = 1 and i G i = 1. Note that CC, NSS and SIM are similarity metrics, hence their negatives are adopted as the loss terms.</p><p>Considering the supervisions applied at multi-scale spatiotemporal features and the final output, the overall loss is: <ref type="bibr" target="#b18">(19)</ref> where P out is the final prediction, P l is the side output of the enhanced spatiotemporal feature xl . Note that the incorporation of spatiotemporal feature starts from the output of the 3rd block and L = 5. The α out and {α l } L l=3 are the weights of final-output loss and side-output losses, respectively.</p><formula xml:id="formula_22">L total = α out L(P out , Q, G) + L l=3 α l L(P l , Q, G),</formula><p>F. Implementation Details 1) Network Details: We build our spatiotemporal residual network based on two ResNet-50 <ref type="bibr" target="#b21">[22]</ref> for handling appearance and motion information, respectively. To obtain feature maps with finer resolution, for each stream we remove the fullyconnected layer and change the strides of the last two blocks to 1. To preserve the receptive field, we further modify the dilation rates of the last two blocks to 2 following <ref type="bibr" target="#b75">[76]</ref>.</p><p>The composite attention mechanism includes local attentions and global attention priors. The local attention module is built as:</p><formula xml:id="formula_23">Conv(3 × 3, C 2 ) → ReLU → Batch Normal- ization (BN) → Conv(1 × 1, 1) → Softmax,</formula><p>where C is the channel number of the input feature. Multiple local attention modules are hierarchically embedded to enhance multi-scale spatiotemporal features from different levels. The side output of each enhanced spatiotemporal feature is obtained through a Conv(1×1, 1) layer with Sigmoid activation. To capture the global attention priors, we incorporate a prior module <ref type="bibr" target="#b75">[76]</ref> for learning a set of Gaussian parameters {µ k</p><p>x , µ k y , ρ k x , ρ k y } K k=1 from the training data, and set K = 16 in our experiments. The convGRU layer contains 256 filters of kernel size 3. As shown in Fig. <ref type="figure" target="#fig_2">2</ref>, given an input video sequence {I t ∈ [0, 255] 224×224×3 } T t=1 with typical 224×224 spatial resolution, the convGRU takes the top-level enhanced spatiotemporal feature sequence {x t L ∈ R 28×28×2048 } T t=1 as inputs for learning dynamic attention transitions. The output feature maps are concatenated with K global attention maps generated with the parameters learned by global attention module. The concatenated features are fed to a Conv(1 × 1, 1) layer with Sigmoid activation to generate the final attention predictions {P t out ∈ [0, 1] 28×28 } T t=1 . The outputs are bilinearly up-sampled to the size of the ground truths for quantitative evaluation.</p><p>2) Training Details: We train the spatiotemporal residual module and our convGRU in tandem. We first initialized the spatiotemporal feature learning part (the two streams) with the weights of ResNet-50 pre-trained on ImageNet, and trained it under certain settings, with deep supervision on the three sideoutputs. Then we randomly initialized the temporal transition learning part and trained the whole network, where the spatiotemporal feature extraction part has been well trained.</p><p>The video batch size is 1, and the frame batch size is 5, i.e. we choose 5 frames from a single video as a training data batch. The optical flows are generated before training using Flownet 2.0 <ref type="bibr" target="#b77">[78]</ref>, and the absolute values of the magnitudes are confined between 0.1 and 20 to avoid noise. The number of stacked optical flows, S, is set to be 5, i.e. we feed 5 consecutive optical flows into the motion stream each time a single frame is fed into the appearance stream. Our model is implemented using Keras with Tensorflow backend. We choose Adam <ref type="bibr" target="#b78">[79]</ref> with initial learning rate 10 -4 to minimize the total loss in Eq. <ref type="bibr" target="#b18">(19)</ref>. We set α out = α l = 1 to equally weight the final-output loss and side-output losses. We set ω 1 = 0.2, ω 2 = ω 3 = 0.1 in Eq. ( <ref type="formula" target="#formula_14">14</ref>) to emphasize the importance of L kl .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>IV. EXPERIMENTS</head><p>In Section IV-A, we review the datasets on which we train our models and make comparisons with other models. The qualitative and quantitative comparisons can be found in Section IV-B, and the ablation study is shown in Section IV-C.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Datasets</head><p>There are four widely used datasets for dynamic visual attention, which differ in number of video samples, number of participants, experimental settings, viewing purpose etc. 1) DHF1K <ref type="bibr" target="#b16">[17]</ref>: includes 1K videos with diverse contents of 7 main categories, such as human activities, arts, animals etc. It also covers varied motion patterns and various foreground objects for studying human attention allocation in dynamic viewing. The eye-tracking data come from 17 observers performing free-viewing. These videos are randomly split into 600/100/300 training/validation/test samples, respectively.</p><p>2) Hollywood-2 <ref type="bibr" target="#b24">[25]</ref>: consists of 1,707 videos from the Hollywood-2 action recognition dataset <ref type="bibr" target="#b92">[93]</ref>, containing 12 actions such as eating, running, kissing, etc. The groundtruth fixations are the eye-tracking data from 19 observers. The training and testing sets include 823 and 884 videos, respectively, and can be further divided into multiple shots following the ground-truth shot boundaries.</p><p>3) UCF sports <ref type="bibr" target="#b24">[25]</ref>: contains 150 videos from the UCF sports action dataset <ref type="bibr" target="#b93">[94]</ref> with 9 sports actions such as diving, golfing etc. The participants were instructed to identify the actions during experiments, which inevitably bias the fixation data with special viewing purpose. Statistics reveals that most All the metrics are widely used and accepted for evaluating a visual attention model. See <ref type="bibr" target="#b38">[39]</ref> for detailed introductions. 1) Performance on DHF1K: We evaluate our model on the testing set of DHF1K containing 300 videos. The qualitative comparison over 3 frames, which are selected to show the temporal transitions of visual attention in Fig. <ref type="figure" target="#fig_7">7</ref> (a). There exist inconsistent movements of foreground objects at different time points, which raises challenges for attention prediction. Our can precisely predict the visual attention locations and comprehensively model dynamic attention transitions across frames. This can be attributed to the efficient spatiotemporal feature fusion, the composite attention mechanism and the con-vGRU which models long-term dependencies. The quantitative evaluation results are shown in Table <ref type="table" target="#tab_1">I</ref>. Our model consistently outperforms other methods in all the metrics.</p><p>2) Performance on Hollywood-2: Hollywood-2 is a large dataset with 884 testing video sequences, all of which are movie scenes. Fig. <ref type="figure" target="#fig_7">7</ref> (b) shows an outdoor scene where a woman is hurrying on a bustling street. Despite various shooting environments and complex action patterns, our model consistently provides better predictions w.r.t. all other methods. Table <ref type="table" target="#tab_1">II</ref> shows the quantitative evaluation results. Again, our model achieves better results over most of the metrics.</p><p>3) Performance on UCF sports: The testing set of UCF sports contains 47 sports videos. The visual comparisons are </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Ablation Study</head><p>In this section, we show the effectiveness of each main component described in Section III by experimenting on several model variants under training setting (iii), and evaluate these models on the testing set of UCF sports.</p><p>1) Spatiotemporal Feature Fusion: We incorporate dense residual cross connections between two streams to fuse the spatial and temporal features (Section III-B). To validate the effectiveness of the feature fusion for learning better spatiotemporal representations, we experiment on four variants, namely w/o. cross connection, w/o. dense connection and w/o. residual mapping, meanwhile preserving the composite attention mechanism, the convGRU and the deep supervision at the features of 3rd-5th blocks unless otherwise specified.</p><p>The w/o. cross connection refers to the model without any early interactions between two streams. The feature fusion only happens at the last block where the outputs from two streams are concatenated and squeezed. To adapt the composite attention mechanism for this situation, we double the number of local attention modules, and embed these modules into both streams to separately enhance the spatial and temporal features, i.e. {x a l }  nections are only added between the equivalent blocks of the two streams in the model. For w/o. residual mapping, the fusion is performed by directly multiplying a condensed stack of motion features from several previous blocks to the appearance feature of current block. In the last two cases, all the fusions start from the output of the 3rd block, as described in Section III-F. Table <ref type="table" target="#tab_4">V</ref> shows that gating appearance feature using motion information can result in better spatiotemporal saliency representations. The dense and the residual mapping structures further boost the prediction precision.</p><p>2) Composite Attention Mechanism: Our composite attention mechanism consists of the local and global attention mod-ule, in Section III-C. We experiment on four variants w.r.t. the attention mechanism, without modifying the spatiotemporal feature fusion strategy and the convGRU. The first case w/o. composite attention is obtained by disabling both the local and global attention modules in the complete model. The second case w/o. global attention only disables the global attention module, while keeping embedded local attention modules functioning. The third case w/o. local attention disables all the local attention modules in the complete model, and only learns attention using the global attention module. To further verify the residual structure in the local attention module, the fourth case w/o. local residual structure removes the identity mappings of the local attention modules from the complete model, which corresponds to the case in Eq. <ref type="bibr" target="#b6">(7)</ref>.</p><p>The evaluation results are shown in Table <ref type="table" target="#tab_4">V</ref>. By comparing w/o. composite attention and w/o. global attention, or w/o. local attention and Ours, we may conclude that the involvement of local attention module can remarkably improve almost all the five metrics except AUC-J. The AUC-J can be further enhanced after incorporating the global attention module, as seen from w/o. composite attention and w/o. local attention, or w/o. global attention and Ours. From w/o. local residual structure and Ours, we see that the identity mapping in the local attention module is essential for precise prediction.</p><p>3) ConvGRU: The convGRU is a lightweight RNN which is able to model the temporal transitions of dynamic visual attention, and can be well trained with relatively small datasets due to its simple structure. The ability of modeling temporal visual attention trainsition is significant for preidicting the saliency shifts when human is faced with multiple saliency targets <ref type="bibr" target="#b94">[95]</ref> or is observing dynamic scenes <ref type="bibr" target="#b35">[36]</ref>.</p><p>To varify the effectiveness of the convGRU module in the complete model, we first replace the convGRU with a layer which has the same number of filters and kernel size (w. convolution). To compare convGRU with convLSTM, we replace the convGRU with a convLSTM of the same number of filters and kernel size (w. convLSTM).</p><p>As shown in Table <ref type="table" target="#tab_4">V</ref>, convGRU outperforms the other two variants in almost all the metrics, suggesting that convGRU would be the better choice for video saliency prediction compared with feed-forward convolution and convLSTM. The example in Fig. <ref type="figure" target="#fig_8">8</ref> demonstrates the ability of the convGRU module to model the saliency shifts, e.g. from the ball to the head. The w. convolution keeps focusing on the head, while the w. convLSTM focus on the head instead of the ball in the frame and it pays almost equal attention on the body when it is expected to look at the head in the 4th frame.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>4) Side outputs:</head><p>We study the discriminative property of the enhanced spatiotemporal features by evaluating the side outputs. Three additional baselines are introduced, namely x3 side output, x4 side output and x5 side output, which correspond to the side outputs from three enhanced spatiotemporal representations. As shown in Table V, the side outputs gradually becomes better since higher layers can make finer predictions. The final prediction achieves the best performance. </p><note type="other">Input Ground truth CNN ConvLSTM ConvGRU</note></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head></head><label></label><figDesc><ref type="bibr" target="#b18">[19]</ref> propose to predict the attention upon both objectness and object motion. Their feature extraction network, i.e., OM-CNN, consists of two branches, one is based on YOLO<ref type="bibr" target="#b69">[70]</ref>, a well-known object detection network, and the other one is modified from FlowNet<ref type="bibr" target="#b70">[71]</ref>, a CNN-based optical flow estimation network. As shown in Fig.1(b), multilayer features from both two branches are fed into a temporal correlation learning network (2C-LSTM) comprised of two convolutional LSTM (convLSTM)<ref type="bibr" target="#b71">[72]</ref> layers.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 2 :</head><label>2</label><figDesc>Fig. 2: Architecture overview. Two tightly coupled streams take RGB frames and stacked optical flows as inputs, respectively, and output the comprehensive spatiotemporal saliency representation. The spatiotemporal saliency representations are further enhanced with multi-scale information through a composite attention mechanism. A lightweight recurrent unit, convGRU, is incorporated to facilitate learning the temporal transition of visual attention.</figDesc><graphic coords="4,176.45,141.23,144.31,72.24" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Fig. 3 :</head><label>3</label><figDesc>Fig. 3: (a) Residual learning is to learn the residual mapping F(x) instead of the original mapping H(x). The appearance and motion streams consist of several residual blocks, and are tightly coupled by (b) dense residual cross connections between corresponding layers. (c) Dense residual cross connection integrates appearance feature with multi-layer motion information for robust spatiotemporal saliency learning. To clarify, the naïve (d) cross connection between two streams directly using the motion feature as a gate mechanism to enhance appearance feature, which can be further improved by introducing residual structure, resulting in (e) residual cross connection design, and incorporating more comprehensive motion features from previous layers, yielding a (f) dense residual cross connection architecture. See Section III-B for more details.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Fig. 4 :</head><label>4</label><figDesc>Fig.4: Model w. effective spatiotemporal feature fusion (last column) is able to locate at human attention focuses more precisely than the one w/o. such information integration (3rd column). See Section III-B for more details.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Fig. 5 :</head><label>5</label><figDesc>Fig. 5: Illustration of multi-scale local attention module. (a) Input video frame. (b) Multi-scale local attentions. (c) Saliency predictions w/o. local attention. (d) Saliency predictions w. local attention. (e) Ground-truth saliency maps.</figDesc><graphic coords="6,341.48,292.48,68.70,54.96" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Fig. 6 :</head><label>6</label><figDesc>Fig. 6: Visualized prior maps using different training sets from (a) DHF1K, (b) Hollywood-2, (c) UCF sports, and (d) DHF1K+Hollywood-2+UCF sports. See Section IV-A for details about the training settings. For each sub-figure, the left shows the learned gaussian function maps, and the right shows the integrated prior map under certain network parameters.</figDesc><graphic coords="7,499.91,124.29,53.83,53.83" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Fig. 7 :</head><label>7</label><figDesc>Fig. 7: Qualitative comparisons with other methods on: (a) DHF1K; (b) Hollywood-2; (c) UCF sports; (d) DIEM. For each dataset, we present one example video with three representative frames for demonstration. Best viewed in color.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Fig. 8 :</head><label>8</label><figDesc>Fig. 8: Visual comparisons for temporal transition learning using CNN, convLSTM and convGRU. Best viewed in color.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>TABLE I :</head><label>I</label><figDesc>Quantitative results on DHF1K. (The best scores are marked in bold. This note is the same for other tables.)</figDesc><table><row><cell></cell><cell cols="2">Methods AUC-J↑ SIM↑ s-AUC↑ CC↑</cell><cell>NSS↑</cell></row><row><cell>Static Models</cell><cell>*  ITTI [1] 0.774 0.162 0.553  *  GBVS [49] 0.828 0.186 0.554 SALICON [6] 0.857 0.232 0.590 Shallow-Net [8] 0.833 0.182 0.529 Deep-Net [8] 0.855 0.201 0.592 DVA [11] 0.860 0.262 0.595</cell><cell cols="2">0.233 1.207 0.283 1.474 0.327 1.901 0.295 1.509 0.331 1.775 0.358 2.013</cell></row><row><cell></cell><cell>Training setting (i) 0.890 0.323 0.634</cell><cell cols="2">0.427 2.305</cell></row><row><cell>Ours</cell><cell>Training setting (ii) 0.885 0.332 0.635 Training setting (iii) 0.866 0.307 0.655</cell><cell cols="2">0.433 2.400 0.378 2.108</cell></row><row><cell></cell><cell>Training setting (iv) 0.895 0.355 0.663</cell><cell cols="2">0.458 2.558</cell></row><row><cell></cell><cell></cell><cell></cell></row></table><note><p>* Non-deep learning model.</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>TABLE V :</head><label>V</label><figDesc>Ablation study on UCF sports dataset. See Section IV-C for details.</figDesc><table><row><cell>Aspects</cell><cell>Variants</cell><cell>AUC-J↑ SIM↑ s-AUC↑ CC↑ NSS↑</cell></row><row><cell></cell><cell>Ours Training setting (iii)</cell><cell>0.914 0.535 0.790 0.645 3.472</cell></row><row><cell>Spatiotemporal Feature Fusion</cell><cell cols="2">w/o. cross connection 0.896 0.512 0.775 0.613 3.110 w/o. dense connection 0.909 0.518 0.772 0.626 3.384 w/o. residual mapping 0.906 0.496 0.764 0.591 3.151</cell></row><row><cell>Composite</cell><cell>w/o. composite attention</cell><cell>0.910 0.515 0.772 0.620 3.321</cell></row><row><cell>Attention</cell><cell>w/o. global attention</cell><cell>0.908 0.521 0.773 0.627 3.382</cell></row><row><cell>Mechanism</cell><cell>w/o. local attention</cell><cell>0.912 0.514 0.766 0.619 3.355</cell></row><row><cell>vary competitive</cell><cell>w/o. local residual structure</cell><cell>0.898 0.501 0.754 0.603 3.296</cell></row><row><cell>ConvGRU</cell><cell>w. convolution w. convLSTM</cell><cell>0.907 0.524 0.774 0.639 3.403 0.911 0.522 0.782 0.629 3.376</cell></row><row><cell></cell><cell>x3 side output</cell><cell>0.893 0.461 0.752 0.558 2.963</cell></row><row><cell>Side Outputs</cell><cell>x4 side output</cell><cell>0.906 0.516 0.775 0.621 3.340</cell></row><row><cell></cell><cell>x5 side output</cell><cell>0.912 0.529 0.788 0.640 3.354</cell></row></table></figure>
		</body>
		<back>

			<div type="funding">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>This work was supported in part by the Beijing Natural Science Foundation under Grant 4182056, the CCF-Tencent Open Fund, the Zhijiang Lab's International Talent Fund for Young Professionals, and the Fok Ying-Tong Education Foundation for Young Teachers. Specialized Fund for Joint Building Program of Beijing Municipal Education Commission.</p></div>
			</div>

			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Static</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Models</head><p>* ITTI <ref type="bibr" target="#b0">[1]</ref>  of the fixations are located at human body regions <ref type="bibr" target="#b16">[17]</ref>. It contains 103 videos for training and 47 videos for testing.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>4) DIEM [26]</head><p>: has 84 videos with varying styles, in which the gaze data is collected from 50 participants per video. The videos are from professional movies which are intentionally edited to attract human attention to certain objects in the scene. Following <ref type="bibr" target="#b83">[84]</ref>, the testing set contains 20 selected videos, and the remaining videos belong to the training set.</p><p>We use the standard experiment protocol in <ref type="bibr" target="#b16">[17]</ref> for fare comparison, i.e., considering 4 training settings with the training set(s) from (i) DHF1K, (ii) Hollywood-2, (iii) UCF sports, and (iv) DHF1K + Hollywood-2 + UCF sports . The performance is evaluated on the testing sets of DHF1K, Hollywood-2 and UCF sports. To further evaluate the generalization ability of the proposed method, we also conduct experiments on the DIEM, whose videos are not used for training.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Comparison Results</head><p>We quantitatively compare our model with 22 saliency models, including 17 dynamic models (PQFT <ref type="bibr" target="#b79">[80]</ref>, Seo et al. <ref type="bibr" target="#b80">[81]</ref>, Rudoy et al. <ref type="bibr" target="#b81">[82]</ref>, Hou et al. <ref type="bibr" target="#b82">[83]</ref>, Fang et al. <ref type="bibr" target="#b54">[55]</ref>, OBDL <ref type="bibr" target="#b83">[84]</ref>, AWS-D <ref type="bibr" target="#b84">[85]</ref>, PMES <ref type="bibr" target="#b85">[86]</ref>, PIM-ZEN <ref type="bibr" target="#b87">[88]</ref>, MAM <ref type="bibr" target="#b86">[87]</ref>, PIM-MCS <ref type="bibr" target="#b88">[89]</ref>, MCSDM <ref type="bibr" target="#b89">[90]</ref>, MSM-SM <ref type="bibr" target="#b90">[91]</ref>, PNSP-CS <ref type="bibr" target="#b91">[92]</ref>, OM-CNN <ref type="bibr" target="#b18">[19]</ref>, Two-stream <ref type="bibr" target="#b17">[18]</ref> 2 , and ACLNet <ref type="bibr" target="#b16">[17]</ref>) and 6 static models (ITTI <ref type="bibr" target="#b0">[1]</ref>, GBVS <ref type="bibr" target="#b48">[49]</ref>, SALICON <ref type="bibr" target="#b5">[6]</ref>, DVA <ref type="bibr" target="#b10">[11]</ref>, Shallow-Net <ref type="bibr" target="#b7">[8]</ref>, and Deep-Net <ref type="bibr" target="#b7">[8]</ref>), among which, OM-CNN,</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">A model of saliency-based visual attention for rapid scene analysis</title>
		<author>
			<persName><forename type="first">L</forename><surname>Itti</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Koch</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Niebur</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="1254" to="1259" />
			<date type="published" when="1998">1998</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Higher-order energies for image segmentation</title>
		<author>
			<persName><forename type="first">J</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Shao</surname></persName>
		</author>
		<author>
			<persName><surname>Porikli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">on Image Processing</title>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="volume">26</biblScope>
			<biblScope unit="page" from="4911" to="4922" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Inferring salient objects from human fixations</title>
		<author>
			<persName><forename type="first">W</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Borji</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Multimodal saliency and fusion for movie summarization based on aural, visual, and textual attention</title>
		<author>
			<persName><forename type="first">G</forename><surname>Evangelopoulos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Zlatintsi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Potamianos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Maragos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Rapantzikos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Skoumas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Avrithis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. on Multimedia</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="1553" to="1568" />
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Large-scale optimization of hierarchical features for saliency prediction in natural images</title>
		<author>
			<persName><forename type="first">E</forename><surname>Vig</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Dorr</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Cox</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="2798" to="2805" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Salicon: Reducing the semantic gap in saliency prediction by adapting deep neural networks</title>
		<author>
			<persName><forename type="first">X</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Boix</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Zhao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="262" to="270" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Predicting eye fixations using convolutional neural networks</title>
		<author>
			<persName><forename type="first">N</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Wen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="362" to="370" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Shallow and deep convolutional networks for saliency prediction</title>
		<author>
			<persName><forename type="first">J</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Sayrol</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Giro-I Nieto</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Mcguinness</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><forename type="middle">E</forename><surname>O'connor</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="598" to="606" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Learning to predict eye fixations via multiresolution convolutional neural networks</title>
		<author>
			<persName><forename type="first">N</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. on Neural Networks and Learning Systems</title>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Deepfix: A fully convolutional neural network for predicting human eye fixations</title>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">S</forename><surname>Kruthiventi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Ayush</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">V</forename><surname>Babu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. on Image Processing</title>
		<imprint>
			<biblScope unit="volume">26</biblScope>
			<biblScope unit="issue">9</biblScope>
			<biblScope unit="page" from="4446" to="4456" />
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Deep visual attention prediction</title>
		<author>
			<persName><forename type="first">W</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Shen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. on Image Processing</title>
		<imprint>
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="2368" to="2378" />
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">A deep spatial contextual long-term recurrent convolutional network for saliency detection</title>
		<author>
			<persName><forename type="first">N</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Han</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. on Image Processing</title>
		<imprint>
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="3264" to="3274" />
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Realistic avatar eye and head animation using a neurobiological model of visual attention</title>
		<author>
			<persName><forename type="first">L</forename><surname>Itti</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Dhavale</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Pighin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Applications and Science of Neural Networks, Fuzzy Systems, and Evolutionary Computation VI</title>
		<imprint>
			<date type="published" when="2003">2003</date>
			<biblScope unit="volume">5200</biblScope>
			<biblScope unit="page" from="64" to="79" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Sunday: Saliency using natural statistics for dynamic analysis of scenes</title>
		<author>
			<persName><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">H</forename><surname>Tong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">W</forename><surname>Cottrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of AAAI Annual Cognitive Science Conference</title>
		<meeting>AAAI Annual Cognitive Science Conference</meeting>
		<imprint>
			<date type="published" when="2009">2009</date>
			<biblScope unit="page" from="2944" to="2949" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Regularized feature reconstruction for spatio-temporal saliency detection</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L.-T</forename><surname>Chia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Rajan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. on Image Processing</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="3120" to="3132" />
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Correspondence driven saliency transfer</title>
		<author>
			<persName><forename type="first">W</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Shao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Porikli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. on Image Processing</title>
		<imprint>
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="5025" to="5034" />
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Revisiting video saliency: A large-scale benchmark and a new model</title>
		<author>
			<persName><forename type="first">W</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M.-M</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Borji</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Spatio-temporal saliency networks for dynamic saliency prediction</title>
		<author>
			<persName><forename type="first">C</forename><surname>Bak</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Kocak</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Erdem</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Erdem</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. on Multimedia</title>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">DeepVS: A deep learning based video saliency prediction approach</title>
		<author>
			<persName><forename type="first">L</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Qiao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Saliency detection for unconstrained videos using superpixel-level graph and spatiotemporal propagation</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Ye</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Shen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Trans. on Circuits and Systems for Video Technology</title>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="page" from="2527" to="2542" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Improving video saliency detection via localized estimation and spatiotemporal refinement</title>
		<author>
			<persName><forename type="first">X</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. on Multimedia</title>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="770" to="778" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Spatiotemporal residual networks for video action recognition</title>
		<author>
			<persName><forename type="first">C</forename><surname>Feichtenhofer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Pinz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Wildes</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="3468" to="3476" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Delving deeper into convolutional networks for learning video representations</title>
		<author>
			<persName><forename type="first">N</forename><surname>Ballas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Pal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Courville</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Actions in the eye: dynamic gaze datasets and learnt saliency models for visual recognition</title>
		<author>
			<persName><forename type="first">S</forename><surname>Mathe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Sminchisescu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">37</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="1408" to="1424" />
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Clustering of gaze during dynamic scene viewing is predicted by motion</title>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">K</forename><surname>Mital</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">J</forename><surname>Smith</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">L</forename><surname>Hill</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">M</forename><surname>Henderson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Cognitive Computation</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="5" to="24" />
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Learning Unsupervised Video Object Segmentation Through Visual Attention</title>
		<author>
			<persName><forename type="first">W</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">C H</forename><surname>Hoi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Ling</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="3064" to="3074" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Salient objects in clutter: Bringing salient object detection to the foreground</title>
		<author>
			<persName><forename type="first">D.-P</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M.-M</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J.-J</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S.-H</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Hou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Borji</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="186" to="202" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Salient Object Detection With Pyramid Attention and Salient Edges</title>
		<author>
			<persName><forename type="first">W</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">C H</forename><surname>Hoi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Borji</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="1448" to="1457" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Contrast prior and fluid pyramid integration for rgbd salient object detection</title>
		<author>
			<persName><forename type="first">J.-X</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D.-P</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M.-M</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X.-Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Consistent video saliency using local gradient flow optimization and global refinement</title>
		<author>
			<persName><forename type="first">W</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Shao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. on Image Processing</title>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="4185" to="4196" />
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Saliency-aware video object segmentation</title>
		<author>
			<persName><forename type="first">W</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Porikli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">40</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="20" to="33" />
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Video Saliency Detection via Sparsity-based Reconstruction and Propagation</title>
		<author>
			<persName><forename type="first">R</forename><surname>Cong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Lei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Porikli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Hou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. on Image Processing</title>
		<imprint>
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page" from="4819" to="4831" />
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Video salient object detection via fully convolutional networks</title>
		<author>
			<persName><forename type="first">W</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Shao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. on Image Processing</title>
		<imprint>
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="38" to="49" />
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Pyramid dilated deeper ConvLSTM for video salient object detection</title>
		<author>
			<persName><forename type="first">H</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K.-M</forename><surname>Lam</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="715" to="731" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Shifting more attention to video salient object detection</title>
		<author>
			<persName><forename type="first">D.-P</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M.-M</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Shen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="8554" to="8564" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Co-saliency detection for rgbd images based on multi-constraint feature matching and cross label propagation</title>
		<author>
			<persName><forename type="first">R</forename><surname>Cong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Lei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Hou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. on Image Processing</title>
		<imprint>
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="568" to="579" />
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Video co-saliency guided co-segmentation</title>
		<author>
			<persName><forename type="first">W</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Shao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. on Circuits and Systems for Video Technology</title>
		<imprint>
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="1727" to="1736" />
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<monogr>
		<title level="m" type="main">Salient object detection in the deep learning era: An in-depth survey</title>
		<author>
			<persName><forename type="first">W</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Lai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Ling</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1904.09146</idno>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Salient object detection: A benchmark</title>
		<author>
			<persName><forename type="first">A</forename><surname>Borji</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M.-M</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. on Image Processing</title>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page" from="5706" to="5722" />
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">A coherent computational approach to model bottom-up visual attention</title>
		<author>
			<persName><forename type="first">O</forename><surname>Le Meur</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">Le</forename><surname>Callet</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Barba</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Thoreau</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="802" to="817" />
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Sun: A bayesian framework for saliency using natural statistics</title>
		<author>
			<persName><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">H</forename><surname>Tong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">K</forename><surname>Marks</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Shan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">W</forename><surname>Cottrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Vision</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="32" to="32" />
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Discriminant saliency for visual recognition from cluttered scenes</title>
		<author>
			<persName><forename type="first">D</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Vasconcelos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2005">2005</date>
			<biblScope unit="page" from="481" to="488" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Saliency based on information maximization</title>
		<author>
			<persName><forename type="first">N</forename><surname>Bruce</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Tsotsos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2006">2006</date>
			<biblScope unit="page" from="155" to="162" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Global contrast based salient region detection</title>
		<author>
			<persName><forename type="first">M.-M</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><forename type="middle">J</forename><surname>Mitra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">H</forename><surname>Torr</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S.-M</forename><surname>Hu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">37</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="569" to="582" />
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Learning to predict saliency on face images</title>
		<author>
			<persName><forename type="first">M</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="3907" to="3915" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Saliency detection: A spectral residual approach</title>
		<author>
			<persName><forename type="first">X</forename><surname>Hou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2007">2007</date>
			<biblScope unit="page" from="1" to="8" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Very deep convolutional networks for large-scale image recognition</title>
		<author>
			<persName><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Graph-based visual saliency</title>
		<author>
			<persName><forename type="first">J</forename><surname>Harel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Koch</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Perona</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2007">2007</date>
			<biblScope unit="page" from="545" to="552" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Video saliency detection via dynamic consistent spatio-temporal attention modelling</title>
		<author>
			<persName><forename type="first">S.-H</forename><surname>Zhong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Ren</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI Conference on Artificial Intelligence</title>
		<imprint>
			<date type="published" when="2013">2013</date>
			<biblScope unit="page" from="1063" to="1069" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Time-mapping using spacetime saliency</title>
		<author>
			<persName><forename type="first">F</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">Bing</forename><surname>Kang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">F</forename><surname>Cohen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="3358" to="3365" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Submodular trajectories for better motion segmentation in videos</title>
		<author>
			<persName><forename type="first">J</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Shao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. on Image Processing</title>
		<imprint>
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="2688" to="2700" />
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">How many bits does it take for a stimulus to be salient?</title>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">H</forename><surname>Khatoonabadi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Vasconcelos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><forename type="middle">V</forename><surname>Bajic</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Shan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">Salient motion detection in compressed domain</title>
		<author>
			<persName><forename type="first">K</forename><surname>Muthuswamy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Rajan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Signal Processing Letters</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page" from="996" to="999" />
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">Video saliency incorporating spatiotemporal cues and uncertainty weighting</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Fang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. on Image Processing</title>
		<imprint>
			<biblScope unit="volume">23</biblScope>
			<biblScope unit="issue">9</biblScope>
			<biblScope unit="page" from="3910" to="3921" />
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<analytic>
		<title level="a" type="main">Learning to detect video saliency with hevc features</title>
		<author>
			<persName><forename type="first">M</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Ye</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. on Image Processing</title>
		<imprint>
			<biblScope unit="volume">26</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="369" to="385" />
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<analytic>
		<title level="a" type="main">Recurrent mixture density network for spatiotemporal visual attention</title>
		<author>
			<persName><forename type="first">L</forename><surname>Bazzani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Larochelle</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Torresani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<analytic>
		<title level="a" type="main">Two-stream convolutional networks for action recognition in videos</title>
		<author>
			<persName><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="568" to="576" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b58">
	<analytic>
		<title level="a" type="main">Beyond short snippets: Deep networks for video classification</title>
		<author>
			<persName><forename type="first">J</forename><surname>Yue-Hei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Ng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Hausknecht</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Vijayanarasimhan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Monga</surname></persName>
		</author>
		<author>
			<persName><surname>Toderici</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="4694" to="4702" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b59">
	<analytic>
		<title level="a" type="main">Modeling spatialtemporal clues in a hybrid deep learning framework for video classification</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y.-G</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Ye</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Xue</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ACM International Conference on Multimedia</title>
		<meeting>ACM International Conference on Multimedia</meeting>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="461" to="470" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b60">
	<analytic>
		<title level="a" type="main">Convolutional two-stream network fusion for video action recognition</title>
		<author>
			<persName><forename type="first">C</forename><surname>Feichtenhofer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Pinz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="1933" to="1941" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b61">
	<analytic>
		<title level="a" type="main">Spatiotemporal multiplier networks for video action recognition</title>
		<author>
			<persName><forename type="first">C</forename><surname>Feichtenhofer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Pinz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">P</forename><surname>Wildes</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="7445" to="7454" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b62">
	<analytic>
		<title level="a" type="main">Look and think twice: Capturing topdown visual attention with feedback convolutional neural networks</title>
		<author>
			<persName><forename type="first">C</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Xu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="2956" to="2964" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b63">
	<analytic>
		<title level="a" type="main">Stacked attention networks for image question answering</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Smola</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="21" to="29" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b64">
	<analytic>
		<title level="a" type="main">Residual attention network for image classification</title>
		<author>
			<persName><forename type="first">F</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Qian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="3156" to="3164" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b65">
	<analytic>
		<title level="a" type="main">A neural attention model for abstractive sentence summarization</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">M</forename><surname>Rush</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Chopra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Weston</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Conference on Empirical Methods on Natural Language Processing</title>
		<meeting>the Conference on Empirical Methods on Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="379" to="389" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b66">
	<analytic>
		<title level="a" type="main">A decomposable attention model for natural language inference</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">P</forename><surname>Parikh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Täckström</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Das</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Uszkoreit</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Conference on Empirical Methods on Natural Language Processing</title>
		<meeting>the Conference on Empirical Methods on Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="2249" to="2255" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b67">
	<monogr>
		<title level="m" type="main">On the properties of neural machine translation: Encoder-decoder approaches</title>
		<author>
			<persName><forename type="first">K</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Van Merriënboer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Bahdanau</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1409.1259</idno>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b68">
	<analytic>
		<title level="a" type="main">Convolutional gated recurrent networks for video segmentation</title>
		<author>
			<persName><forename type="first">M</forename><surname>Siam</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Valipour</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Jagersand</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Ray</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Conference on Image Processing</title>
		<meeting>the International Conference on Image Processing</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="3090" to="3094" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b69">
	<analytic>
		<title level="a" type="main">You only look once: Unified, real-time object detection</title>
		<author>
			<persName><forename type="first">J</forename><surname>Redmon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Divvala</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Farhadi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="779" to="788" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b70">
	<analytic>
		<title level="a" type="main">Flownet: Learning optical flow with convolutional networks</title>
		<author>
			<persName><forename type="first">A</forename><surname>Dosovitskiy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Fischer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Ilg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Hausser</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Hazirbas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Golkov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Van Der Smagt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Cremers</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Brox</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="2758" to="2766" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b71">
	<analytic>
		<title level="a" type="main">Convolutional LSTM network: A machine learning approach for precipitation nowcasting</title>
		<author>
			<persName><forename type="first">S</forename><surname>Xingjian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D.-Y</forename><surname>Yeung</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W.-K</forename><surname>Wong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W.-C</forename><surname>Woo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="802" to="810" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b72">
	<analytic>
		<title level="a" type="main">Deeply-supervised nets</title>
		<author>
			<persName><forename type="first">C.-Y</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Gallagher</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Tu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Artificial Intelligence and Statistics</title>
		<imprint>
			<biblScope unit="page" from="562" to="570" />
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b73">
	<analytic>
		<title level="a" type="main">Quantifying center bias of observers in free viewing of dynamic natural scenes</title>
		<author>
			<persName><forename type="first">P.-H</forename><surname>Tseng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Carmi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><forename type="middle">G</forename><surname>Cameron</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">P</forename><surname>Munoz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Itti</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Vision</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="4" to="4" />
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b74">
	<analytic>
		<title level="a" type="main">Biologically inspired object tracking using center-surround saliency mechanisms</title>
		<author>
			<persName><forename type="first">V</forename><surname>Mahadevan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Vasconcelos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="541" to="554" />
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b75">
	<analytic>
		<title level="a" type="main">Predicting human eye fixations via an LSTM-based saliency attentive model</title>
		<author>
			<persName><forename type="first">M</forename><surname>Cornia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Baraldi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Serra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Cucchiara</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. on Image Processing</title>
		<imprint>
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page" from="5142" to="5154" />
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b76">
	<analytic>
		<title level="a" type="main">Realtime superpixel segmentation by DBSCAN clustering algorithm</title>
		<author>
			<persName><forename type="first">J</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Hao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Shao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. on Image Processing</title>
		<imprint>
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page" from="5933" to="5942" />
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b77">
	<analytic>
		<title level="a" type="main">Flownet 2.0: Evolution of optical flow estimation with deep networks</title>
		<author>
			<persName><forename type="first">E</forename><surname>Ilg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Mayer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Saikia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Keuper</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Dosovitskiy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Brox</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="volume">2</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b78">
	<analytic>
		<title level="a" type="main">Adam: A method for stochastic optimization</title>
		<author>
			<persName><forename type="first">D</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Ba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b79">
	<analytic>
		<title level="a" type="main">A novel multiresolution spatiotemporal saliency detection model and its applications in image and video compression</title>
		<author>
			<persName><forename type="first">C</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. on Image Processing</title>
		<imprint>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="185" to="198" />
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b80">
	<analytic>
		<title level="a" type="main">Static and space-time visual saliency detection by self-resemblance</title>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">J</forename><surname>Seo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Milanfar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of vision</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page" from="15" to="15" />
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b81">
	<analytic>
		<title level="a" type="main">Learning video saliency from human gaze using candidate selection</title>
		<author>
			<persName><forename type="first">D</forename><surname>Rudoy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">B</forename><surname>Goldman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Shechtman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Zelnik-Manor</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2013">2013</date>
			<biblScope unit="page" from="1147" to="1154" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b82">
	<analytic>
		<title level="a" type="main">Dynamic visual attention: Searching for coding length increments</title>
		<author>
			<persName><forename type="first">X</forename><surname>Hou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b83">
	<analytic>
		<title level="a" type="main">How many bits does it take for a stimulus to be salient</title>
		<author>
			<persName><forename type="first">S</forename><surname>Hossein Khatoonabadi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Vasconcelos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><forename type="middle">V</forename><surname>Bajic</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Shan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="5501" to="5510" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b84">
	<analytic>
		<title level="a" type="main">Dynamic whitening saliency</title>
		<author>
			<persName><forename type="first">V</forename><surname>Leboran</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Garcia-Diaz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><forename type="middle">R</forename><surname>Fdez-Vidal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><forename type="middle">M</forename><surname>Pardo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">39</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="893" to="907" />
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b85">
	<analytic>
		<title level="a" type="main">A new perceived motion based shot content representation</title>
		<author>
			<persName><forename type="first">Y.-F</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H.-J</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Conference on Image Processing</title>
		<meeting>the International Conference on Image Processing</meeting>
		<imprint>
			<date type="published" when="2001">2001</date>
			<biblScope unit="page" from="426" to="429" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b86">
	<analytic>
		<title level="a" type="main">A model of motion attention for video skimming</title>
		<author>
			<persName><forename type="first">Y.-F</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H.-J</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Conference on Image Processing</title>
		<meeting>the International Conference on Image Processing</meeting>
		<imprint>
			<date type="published" when="2002">2002</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b87">
	<analytic>
		<title level="a" type="main">A fast algorithm to find the regionof-interest in the compressed mpeg domain</title>
		<author>
			<persName><forename type="first">G</forename><surname>Agarwal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Anbu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Sinha</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Multimedia and Expo</title>
		<meeting>the IEEE International Conference on Multimedia and Expo</meeting>
		<imprint>
			<date type="published" when="2003">2003</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b88">
	<analytic>
		<title level="a" type="main">Region-of-interest based compressed domain video transcoding scheme</title>
		<author>
			<persName><forename type="first">A</forename><surname>Sinha</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Agarwal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Anbu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Acoustics, Speech, and Signal Processing</title>
		<meeting>the IEEE International Conference on Acoustics, Speech, and Signal Processing</meeting>
		<imprint>
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b89">
	<analytic>
		<title level="a" type="main">A motion attention model based rate control algorithm for h.264/avc</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/ACIS International Conference on Computer and Information Science</title>
		<meeting>the IEEE/ACIS International Conference on Computer and Information Science</meeting>
		<imprint>
			<date type="published" when="2009">2009</date>
			<biblScope unit="page" from="568" to="573" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b90">
	<analytic>
		<title level="a" type="main">Salient motion detection in compressed domain</title>
		<author>
			<persName><forename type="first">K</forename><surname>Muthuswamy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Rajan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Signal Processing Letters</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page" from="996" to="999" />
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b91">
	<analytic>
		<title level="a" type="main">A video saliency detection model in compressed domain</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">M</forename><surname>Tsai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">W</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. on Circuits and Systems for Video Technology</title>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="27" to="38" />
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b92">
	<analytic>
		<title level="a" type="main">Actions in context</title>
		<author>
			<persName><forename type="first">M</forename><surname>Marszalek</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Laptev</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Schmid</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2009">2009</date>
			<biblScope unit="page" from="2929" to="2936" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b93">
	<analytic>
		<title level="a" type="main">Action mach a spatiotemporal maximum average correlation height filter for action recognition</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">D</forename><surname>Rodriguez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Ahmed</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Shah</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2008">2008</date>
			<biblScope unit="page" from="1" to="8" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b94">
	<analytic>
		<title level="a" type="main">Attention shifts to salient targets</title>
		<author>
			<persName><forename type="first">H.-C</forename><surname>Nothdurft</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Vision Research</title>
		<imprint>
			<biblScope unit="volume">42</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page" from="1287" to="1306" />
			<date type="published" when="2002">2002</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b95">
	<analytic>
		<title level="a" type="main">degrees in the School of Automation from Huazhong University of Science and Technology in 2013 and 2016, respectively. She is currently pursuing the Ph.D. degree with the Department of</title>
	</analytic>
	<monogr>
		<title level="m">His current research interests include visual relation understanding and graph neural networks</title>
		<editor>
			<persName><surname>Prof</surname></persName>
		</editor>
		<editor>
			<persName><surname>Song-Chun Zhu</surname></persName>
		</editor>
		<imprint>
			<publisher>UAE</publisher>
		</imprint>
		<respStmt>
			<orgName>Computer Science and Engineering, The Chinese University of Hong Kong ; D. candidate in Department of Statistics, University of California</orgName>
		</respStmt>
	</monogr>
	<note>From 2016 to 2018, he was a joint Ph</note>
</biblStruct>

<biblStruct xml:id="b96">
	<monogr>
		<title level="m" type="main">Hanqiu Sun is an associate professor at the Chinese University of Hong Kong. Her research interests include virtual reality, interactive graphics/animation, real-time hypermedia, virtual surgery, mobile image/video synopsis and navigation, touch-enhanced and dynamics simulations. Sun has an MS in electrical engineering from</title>
		<imprint>
			<pubPlace>Alberta, Canada</pubPlace>
		</imprint>
		<respStmt>
			<orgName>University of British Columbia, and PhD in computer science from University of</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct xml:id="b97">
	<analytic>
		<title level="a" type="main">M&apos;11-SM&apos;12) is the Lead Scientist at the Inception Institute of Artificial Intelligence (IIAI), UAE, and also an adjunct Honorary Professor at Beijing Institute of Technology. He has published about 100 journal and conference papers, and eight papers are selected as the ESI Hightly Cited Papers. His research interests include computer vision, deep learning, autonomous driving and medical image analysis. He is an Associate Editor of IEEE Trans. on Image Processing</title>
		<author>
			<persName><forename type="first">Jianbing</forename><surname>Shen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. on Neural Networks and Learning Systems, and other journals</title>
		<imprint/>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
