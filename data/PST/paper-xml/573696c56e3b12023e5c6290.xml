<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Multiple Social Network Learning and Its Application in Volunteerism Tendency Prediction</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Xuemeng</forename><surname>Song</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">National University of Singapore</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Liqiang</forename><surname>Nie</surname></persName>
							<email>nieliqiang@gmail.com</email>
							<affiliation key="aff0">
								<orgName type="institution">National University of Singapore</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Luming</forename><surname>Zhang</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">National University of Singapore</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Mohammad</forename><surname>Akbari</surname></persName>
							<email>akbari@nus.edu.sg</email>
							<affiliation key="aff0">
								<orgName type="institution">National University of Singapore</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Tat-Seng</forename><surname>Chua</surname></persName>
							<email>chuats@comp.nus.edu.sg</email>
							<affiliation key="aff0">
								<orgName type="institution">National University of Singapore</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Multiple Social Network Learning and Its Application in Volunteerism Tendency Prediction</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">220CD2386205B8B15C4A80ECE8883064</idno>
					<idno type="DOI">10.1145/2766462.2767726</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.3" ident="GROBID" when="2023-07-27T08:52+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>H.3.1 [Information Storage and Retrieval]: Content Analysis and Indexing Multiple Social Network Learning</term>
					<term>Missing Data Completion</term>
					<term>Volunteerism Tendency Prediction</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>We are living in the era of social networks, where people throughout the world are connected and organized by multiple social networks. The views revealed by different social networks may vary according to the different services they offer. They are complimentary to each other and comprehensively characterize a specific user from different perspectives. As compared to the scare knowledge conveyed by a single source, appropriate aggregation of multiple social networks offers us a better opportunity for deep user understanding. The challenges, however, co-exist with opportunities. The first challenge lies in the existence of block-wise missing data, caused by the fact that some users may be very active in certain social networks while inactive in others. The second challenge is how to collaboratively integrate multiple social networks. Towards this end, we first proposed a novel model for data missing completion by seamlessly exploring the knowledge from multiple sources. We then developed a robust multiple social network learning model, and applied it to the application of volunteerism tendency prediction. Extensive experiments on real world dataset verify the effectiveness of our scheme. The proposed scheme is applicable to many other domains, such as demographic inference and interest prediction.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">INTRODUCTION</head><p>With the explosion of social network services, more and more people are involved in multiple social networks for various purposes at the same time. It is reported that 52% of online adults concurrently use multiple social media services <ref type="foot" target="#foot_0">1</ref> . Different aspects of users are disclosed on different social networks due to their different emphasis. In fact, these views are complementary to each other and essentially characterize the same user from different perspectives. As compared with single social network, appropriate aggregation of multiple social networks provides us a potential to comprehensively understand the given users <ref type="bibr" target="#b1">[1,</ref><ref type="bibr" target="#b29">29]</ref>. For example, we can learn descriptive user representation, build predictive models for user profiles, and recommend prescriptive actions based on complete historical behaviors. Hence, an effective technique for multiple social network learning (MSNL) is highly desired. Distinguished from multi-view learning which maximizes the agreement between views using unlabeled data, MSNL works towards supervised learning.</p><p>However, integration of multiple sources is non-trivial <ref type="bibr" target="#b27">[27]</ref>.</p><p>The first tough challenge lies in how to fuse users' heterogeneous distributed data from multiple social networks effectively. One naive approach is to concatenate the feature spaces generated from different sources into a unified feature space. Thereby, traditional machine learning models can be further applied. However, this method simply treats the confidence of all data sources equally and may also lead to the curse of dimensionality. Moreover, it ignores two important facts: 1) different aspects of users are revealed in different social networks and are thus distributed in different feature spaces; and 2) all these aspects tend to characterize the same users. In particular, data from multi-sources describes the same user and thus the results predicted by different sources should be similar. Therefore, it is expected to take the source confidence and source consistency into consideration. Another challenge we are facing is the data missing problem. Although some users have social accounts on multiple social networks, generally they are active on only a few of them. One simple approach to address this challenge is to discard all incomplete subjects. It is apparent that this method will dramatically reduce the training size, thereby result in overfitting in the model learning stage. Therefore, accurately completing missing data by jointly utilizing multiple sources is a necessity to enhance the learning performance.</p><p>To address these problems, we present a scheme for MSNL, which co-regulates the source confidence and source consistency. Figure <ref type="figure" target="#fig_0">1</ref> shows our proposed scheme comprising of three components. Given a set of users, we first crawl Figure <ref type="figure" target="#fig_0">1</ref>: Illustration of our proposed scheme. We first collect and align users' distributed data from multiple social networks. We then jointly infer the block-wise missing data based on the available data. We finally apply MSNL on the complete data. SNi, xj, and y l refer to the i-th social network, j-th user sample, and the l-th corresponding label, respectively. their historical contents and all social connections. The first component extracts the multi-faceted information cues to describe a given user, including demographic information, practical behaviors, historical posts, and profiles of social connections. To deal with the block-wise missing data, the second component attempts to infer the block-wise missing data by learning a latent space shared by different social networks, achieving a complete input to the next component. We finally use the last component to conduct MSNL on the complete data. Particularly, we model the confidence of different data sources and the consistency among them by unifying two regularization terms into our model.</p><p>Based upon our proposed scheme, we introduce one application scenario: volunteerism tendency prediction. Volunteerism was defined in <ref type="bibr" target="#b18">[18]</ref> as long-term, planned, prosocial behaviors that occur within organizational settings and can benefit strangers. Persons exhibiting volunteerism are the so-called volunteers, who serve as an important work force in modern society. Traditionally, it is intractable for nonprofit organizations (NPOs) to aimlessly recruit volunteers from the huge crowd. It is thus necessary to develop an automatic volunteerism tendency prediction system to alleviate the dilemma that NPOs are facing. In particular, we take the advantage of users' casually distributed online data, especially from multiple social networks, which can comprehensively reveal users' personal concerns, interests <ref type="bibr" target="#b5">[5,</ref><ref type="bibr" target="#b22">22]</ref> and even personality traits <ref type="bibr" target="#b20">[20,</ref><ref type="bibr" target="#b21">21]</ref>. On the other hand, the real word dataset may contain block-wise missing data due to some users' inactivity in social networks. Moreover, the specific task also brings us another issue in terms of data collection and ground truth construction. Therefore, the proposed scheme naturally fits this application scenario.</p><p>Our main contributions can be summarized in threefold:</p><p>• We propose a novel MSNL model, which is able to model both the source confidence and source consistency. Specifically, we can obtain a closed-form solution by taking the inverse of a linear system, which has been mathematically proven to be invertible. • We propose an approach to deal with missing data in multiple social networks, which first learns a common latent subpace shared by different sources <ref type="bibr" target="#b12">[12]</ref> and the original missing data can then be derived in turn.</p><p>• We empirically evaluate our proposed scheme on the application of volunteerism tendency prediction.</p><p>In addition, we develop a set of volunteer-oriented features to characterize users' volunteerism tendency.</p><p>We have released our compiled dataset<ref type="foot" target="#foot_2">2</ref> to facilitate other researchers to repeat our experiments and verify their proposed approaches.</p><p>The remainder of this paper is structured as follows, Section 2 briefly reviews the related work. Section 3 describes the proposed MSNL model. Missing data completion is introduced in Section 4. Section 5 presents the set of volunteer-oriented features we developed. Section 6 details the experimental results and analysis, followed by our concluding remarks in Section 7.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">RELATED WORK</head><p>Although our work is distinguished from multi-view learning, we can still benefit from their efforts. Zhang et al. <ref type="bibr" target="#b28">[28]</ref> proposed an inductive multi-view multi-task learning model (regMVMT). regMVMT penalizes the disagreement of models learned from different sources over the unlabeled samples. Besides, the authors also studied the structured missing data, which is completely missing for a source in terms of a task. In other words, if a source is available for a task, then all samples will have data from this source. However, they overlooked the source weights and did not pay attention to the partially structured missing data, which are both what we are concerned with. Yuan et al. <ref type="bibr" target="#b25">[25]</ref> introduced an incomplete multi-source feature learning method, avoiding the direct inference of block-wise missing data. Particularly, the authors split the incomplete data into disjoint groups, where they conducted feature learning independently. However, such a mechanism constrains us to conduct source level analysis. Later, Xiang et al. <ref type="bibr" target="#b24">[24]</ref> investigated multi-source learning with block-wise missing data with an application of Alzheimer's Disease prediction and proposed the iSFS model. Apart from feature-level analysis, the authors also conducted source-level analysis by introducing the weights of models obtained from different sources. However, ignoring the consistency relationships among different models seems inappropriate. In addition, the authors also adapted the model to handle cases where block-wise missing data exists. Different from their work, we infer the missing data by making full use of the available data before applying MSNL, which is more generalizable to other applications.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">MULTIPLE SOCIAL NETWORK LEAR-NING</head><p>This section details our proposed MSNL model and derives an analytic solution by solving the inverse of a linear system, whose invertibility is proved rigorously.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Notation</head><p>We first declare some notations. In particular, we use bold capital letters (e.g. X) and bold lowercase letters (e.g. x) to denote matrices and vectors, respectively. We employ nonbold letters (e.g. x) to represent scalars, and Greek letters (e.g. λ) as parameters. If not clarified, all vectors are in column forms.</p><p>Suppose we have a set of N labeled data samples and S ≥ 2 social networks. We compile the S social networks with an index set C = {1, 2, • • • , S}. Let Ds and Ns denote the number of features and samples in the s-th social network, s ∈ C, respectively. Let Xs ∈ R N ×Ds denote the feature matrix extracted from the s-th social network. Each row represents a user sample. Then the dimension of features extracted from all these social networks is D = ∑ S s=1 Ds. The whole feature matrix can be written as</p><formula xml:id="formula_0">X = {X1, X2, • • • , XS} ∈ R N ×D and y = {y1, y2, • • • , yN } T ∈ {1, -1} N ×1</formula><p>is the corresponding label vector.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Problem Formulations</head><p>Based on a set of data samples with S social networks, we can learn S predictive models, where each model is individually and independently trained on a social network. The final predictive model can be strengthened via linear combination of these S models. Mathematically, we learn one linear mapping function fs for the s-th social network. In addition, we assume that the mapping functions learned from all social networks agree with one another as much as possible. Particularly, we can formalize this assumption using regularization function. Using the least square loss function, we have the following objective function,</p><formula xml:id="formula_1">min fs 1 2N y -f (X) 2 + µ 2N S ∑ s=1 ∑ s ′ ̸ =s fs(Xs) -f s ′ (X s ′ ) 2 + λ 2 f 2 , (<label>1</label></formula><formula xml:id="formula_2">)</formula><p>where f (X) is the final predictive model. fs(Xs) is the prediction results generated from data Xs. λ and µ are the nonnegative regularization parameters that regulate the sparsity of the solution regarding fs and the disagreement among models learned from different social networks, respectively. If we just treat the confidence of different social networks equally, the final predictive model can be formalized as follows,</p><formula xml:id="formula_3">f (X) = 1 S S ∑ s=1 fs(Xs).<label>(2)</label></formula><p>However, in reality, different social networks always have different confidence to the final prediction, and we consider modeling the weights of multiple sources instead of treating all sources equally by introducing the weight vector:</p><formula xml:id="formula_4">α = [α1, α2, • • • , αS] T ∈ R S×1</formula><p>, where αs controls the weight of model learned from s-th social network. Then the final model is defined as follows,</p><formula xml:id="formula_5">f (X) = S ∑ s=1 αsfs(Xs) subject to e T α = 1,<label>(3)</label></formula><p>where e = [1, 1, </p><formula xml:id="formula_6">• • • , 1] T ∈ R S×1 .</formula><formula xml:id="formula_7">+ µ 2N S ∑ s=1 ∑ s ′ ̸ =s Xsws -X s ′ w s ′ 2 + λ 2 S ∑ s=1 ws 2 + β 2 α 2 , (<label>4</label></formula><formula xml:id="formula_8">)</formula><p>where e T α = 1 and β is the regularization parameter, controlling the sparsity of the solution regarding α.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Optimization</head><p>We adopt the alternating optimization strategy to solve the two variables α and ws in Eqn. <ref type="bibr" target="#b4">(4)</ref>. In particular, we optimize one variable while fixing the other one in each iteration. We keep this iterative procedure until the objective function converges.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.1">Computing α with ws fixed</head><p>We denote the objective function as Γ. For simplicity, we replace y in Eqn. ( <ref type="formula" target="#formula_7">4</ref>) by ye T α, as e T α = 1. With the help of Lagrangian, Γ can be rewritten as follows,</p><formula xml:id="formula_9">min α 1 2N ye T α -XWα 2 + β 2 α 2 + δ(1 -e T α), (<label>5</label></formula><formula xml:id="formula_10">)</formula><p>where δ is the nonnegative Lagrange multiplier and</p><formula xml:id="formula_11">W = diag(w1, w2, • • • , wS) ∈ R D×S .</formula><p>Taking derivative of Γ with respect to α, we have,</p><formula xml:id="formula_12">∂Γ ∂α = 1 N (ye T -XW) T (ye T -XW)α + βα -δe. (<label>6</label></formula><formula xml:id="formula_13">)</formula><p>Setting Eqn. ( <ref type="formula" target="#formula_12">6</ref>) to zero, it can be derived that,</p><formula xml:id="formula_14">α = δM -1 e,<label>(7)</label></formula><p>where</p><formula xml:id="formula_15">M = 1 N (ye T -XW) T (ye T -XW) + βI. (<label>8</label></formula><formula xml:id="formula_16">)</formula><p>Since e T α = 1, we can obtain that,</p><formula xml:id="formula_17">δ = 1 e T M -1 e , α = M -1 e e T M -1 e . (<label>9</label></formula><formula xml:id="formula_18">)</formula><p>Obviously, M ∈ R S×S is positive definite and invertible, according to the definition. We thus can obtain the analytic solution of α as Eqn. <ref type="bibr" target="#b9">(9)</ref>. Moreover, we note that when the prediction results learned from all social networks are equal, where X1w1 = X2w2 = • • • = XSwS, then same weights will be assigned, i.e., α1 = α2 = • • • = αS. In addition, Eqn. (9) tends to assign higher weight αs, if smaller difference exists between y and Xsws.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.2">Computing ws with α fixed</head><p>When α is fixed, we compute the derivative of Γ regarding ws as follows,</p><formula xml:id="formula_19">∂Γ ∂ws = 1 N αsX T s ( S ∑ s=1 αsXsws -y) + µ N X T s S ∑ s=1 ∑ s ′ ̸ =s (Xsws -X s ′ w s ′ ) + λws = [ λI + α 2 s N X T s Xs + µ(S -1) N X T s Xs ] ws + S ∑ s=1 ∑ s ′ ̸ =s 1 N (αsα s ′ -µ)X T s X s ′ w s ′ - αs N X T s y, (<label>10</label></formula><formula xml:id="formula_20">)</formula><p>where I is a Ds × Ds identity matrix. Setting Eqn. <ref type="bibr" target="#b10">(10)</ref> to zero and rearranging the terms, all ws's can be learned jointly by the following linear system,</p><formula xml:id="formula_21">Lw = t        L11 L12 L13 • • • L1S L21 L22 L23 • • • L2S L31 L32 L33 • • • L3S . . . . . . . . . . . . . . . LS1 LS2 LS3 • • • LSS               w1 w2 w3 . . . wS        =        t1 t2 t3 . . . tS        , (<label>11</label></formula><formula xml:id="formula_22">)</formula><p>where</p><formula xml:id="formula_23">L ∈ R D×D is a sparse block matrix with S × S blocks, w = [w T 1 , w T 2 , • • • , w T S ] T ∈ R D×1 and t = [t T 1 , t T 2 , • • • , t T S ] T ∈ R D×1</formula><p>are both sparse block vectors with S × 1 blocks. ts, Lss and L ss ′ are defined as follows,</p><formula xml:id="formula_24">     ts = αs N X T s y, Lss = λI + α 2 s -µ N X T s Xs + µS N X T s Xs, L ss ′ = αsα s ′ -µ N X T s X s ′ . (<label>12</label></formula><formula xml:id="formula_25">)</formula><p>Technically, t can be treated as a constant matrix as α is fixed. It is worth noting that L is symmetric as L ss ′ = L T s ′ s . If we can prove that L is invertible, then we can derive the closed-form solution of w as follows,</p><formula xml:id="formula_26">w = L -1 t. (<label>13</label></formula><formula xml:id="formula_27">)</formula><p>We now show L is invertible by proving that L is a positivedefinite matrix. α, w 1: Initialize (w) 0 by fitting each source individually on the available data. Initialize (α</p><formula xml:id="formula_28">Let h = [h T 1 , h T 2 , • • • , h T S ] T ∈ R D×1 ̸ = 0 be an arbitrary block vector, where hi ∈ R D i ×1 , i ∈ C. Then</formula><formula xml:id="formula_29">) 0 = [ 1 S , 1 S , • • • , 1 S ]. 2: for k = 1, 2, • • • do 3:</formula><p>Compute each (α) k according to Eqn. ( <ref type="formula" target="#formula_17">9</ref>). 4:</p><p>Update (w) k according to Eqn. ( <ref type="formula" target="#formula_26">13</ref>). 5:</p><p>if the objective value stops decreasing then 6:</p><p>return α = (α) k and w = (w) k 7:</p><p>end if 8: end for</p><formula xml:id="formula_30">we need to prove that h T Lh = S ∑ i=1 S ∑ j=1 h T i Lijhj = λ h 2 + 1 N [ S ∑ i=1 αiXihi 2 + µ(S -1) S ∑ i=1 Xihi 2 + S ∑ i=1 ∑ j̸ =i αih T i X T i αjXjhj -µ S ∑ i=1 ∑ j̸ =i h T i X T i Xjhj ] ,<label>(14)</label></formula><p>is always larger than zero. In fact, given an arbitrary vector bi, we have,</p><formula xml:id="formula_31">b1 -b2 2 + • • • + b (S-1) -bS 2 + bS -b1 2 ≥ 0 S ∑ i=1 bi 2 ≥ S ∑ i=1 ∑ j̸ =i b T i bj.<label>(15)</label></formula><p>Therefore, as S ≥ 2, we have the following inequality,</p><formula xml:id="formula_32">µ(S -1) S ∑ i=1 Xihi 2 ≥ µ S ∑ i=1 Xihi 2 ≥ µ S ∑ i=1 ∑ j̸ =i (Xihi) T Xjhj.<label>(16)</label></formula><p>Besides, we know that,</p><formula xml:id="formula_33">S ∑ i=1 αiXihi 2 + S ∑ i=1 ∑ j̸ =i αih T i X T i αjXjhj = 1 2 S ∑ i=1 αiXihi 2 + 1 2 S ∑ i=1 αiXihi 2 ≥ 0. (<label>17</label></formula><formula xml:id="formula_34">)</formula><p>Based upon Eqn. ( <ref type="formula" target="#formula_32">16</ref>) and Eqn. ( <ref type="formula" target="#formula_33">17</ref>), we have that,</p><formula xml:id="formula_35">h T Lh ≥ λ h 2 . (<label>18</label></formula><formula xml:id="formula_36">)</formula><p>As h ̸ = 0, h T Lh is always larger than zero. Consequently, L is invertible. The overall procedures for alternating optimization are summarized in Algorithm 1. As each iteration can decrease Γ, whose lower bound is zero, we can guarantee the convergence of Algorithm 1 <ref type="bibr" target="#b7">[7,</ref><ref type="bibr" target="#b16">16]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">MISSING DATA COMPLETION</head><p>In this section, we deal with a more challenging and realistic situation, where block-wise missing data exists, and propose an approach for multiple social network data completion (MSNDC). In such situations, user samples may not be active in all social networks, which leads to the blockwise data missing.</p><p>Suppose we have S data sources in total and each sample has at least one data source available. We employ the subset Ci ⊆ C to indicate the presence of each source and the signature of a specific social network combination. Based on these combinations, all the data samples can be split into multiple exclusive sets, where each set corresponds to a combination. Figure <ref type="figure" target="#fig_1">2</ref> illustrates the incomplete data in our dataset. As can be seen, all users have complete features from SN1, while some users miss data in SN2 or SN3. Therefore, our dataset can be split by four exclusive social network combinations:</p><formula xml:id="formula_37">C1 = {1, 2}, C2 = {1, 2, 3}, C3 = {1, 3}, C4 = {1}.</formula><p>Inspired by <ref type="bibr" target="#b10">[10]</ref>, we use Non-negative Matrix Factorization (NMF) to explore the latent spaces that are shared by different social networks, and further infer the missing data based upon these latent spaces. It is reasonable to assume that the data from different social networks about the same user shares certain latent features. We employ X C i s ∈ R N C i ×Ds to denote the samples generated from the s-th social network. It only contains samples that are available in the set of social networks Ci, where NC i stands for the number of these samples. We use Us ∈ R z×Ds to represent the latent basis matrix for the s-th social network, and P C i s ∈ R N C i ×z to denote the corresponding latent representation of feature matrix X C i s . z is the dimension of the shared latent space of different social networks. The intuitive assumption is that for the samples available in both the s-th and s ′ -th social networks, their corresponding latent representations should also be quite similar. In particular, we impose this constraint to NMF as follows,</p><formula xml:id="formula_38">P C i s = P C i s ′ = P C i , (<label>19</label></formula><formula xml:id="formula_39">)</formula><p>where s ̸ = s ′ , s ∈ Ci, and s ′ ∈ Ci. We thus learn the shared subspaces by the following objective function,</p><formula xml:id="formula_40">min Us≥0 Ps≥0      X {1} 1 X {1,2} 1 X {1,3} 1 X {1,2,3} 1      -     P {1} P {1,2} P {1,3} P {1,2,3}     U1 2 +ν P1 1 + η U1 1 + [ X {1,2} 2 X {1,2,3} 2 ] - [ P {1,2} P {1,2,3} ] U2 2 +ν P2 1 + η U2 1 + [ X {1,3} 3 X {1,2,3} 3 ] - [ P {1,3} P {1,2,3} ] U3 2 +ν P3 1 + η U3 1 , (<label>20</label></formula><formula xml:id="formula_41">)</formula><p>where ν and η are the nonnegative tradeoff parameters for the regularizations. Similarly, we employ the alternating optimization strategy to solve the optimization in Eqn. <ref type="bibr" target="#b20">(20)</ref>.</p><p>To be more specific, we first initialize Us and compute the optimal Ps. Afterwards, Ps is updated based on the computed Us. We keep this iterative procedure until the objective function converges.</p><p>The proposed approach differs from <ref type="bibr" target="#b10">[10]</ref> in the following three aspects. First, MSNDC is generalized to handle the more challenging scenario where data samples are extracted from more than two social networks.</p><p>Second, apart from regulating the latent representation matrix, we also incorporate the regularization on the latent basis matrix. Third, we further derive the original missing data from the latent representation, where the authors in <ref type="bibr" target="#b10">[10]</ref> just apply cluster algorithms directly to the latent representation of data instead of the original data. This is due to two considerations. One is that we believe the value of original known data is higher than the latent representation. The other one is that we need to preserve the heterogeneity among data from different sources to fit the MSNL model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Optimization</head><p>In order to increase the efficiency of the iterative procedure, we initialize Us by optimizing the following objective function,</p><formula xml:id="formula_42">min Us≥0 X {1,2,3} 1 -P {1,2,3} U1 2 +ν P {1,2,3} 1 + η U1 1 + X {1,2,3} 2 -P {1,2,3} U2 2 +η U2 1 + X {1,2,3} 3 -P {1,2,3} U3 2 +η U3 1 . (<label>21</label></formula><formula xml:id="formula_43">)</formula><p>We then alternatively optimize Us and Ps until the objective function converges. Specifically, we employ the greedy coordinate descent (GCD) approach <ref type="bibr" target="#b9">[9]</ref>, which has been proven to be tremendously fast to solve NMF decomposition with L1-norm regularization. Finally, we obtain Ps, Us, s ∈ C, based on which we can infer the missing data as follows,</p><formula xml:id="formula_44">XC i s = P C i Us, ∀s / ∈ Ci. (<label>22</label></formula><formula xml:id="formula_45">)</formula><p>Algorithm 2 summarizes the overall procedures for alternating optimization.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">APPLICATION: VOLUNTEERISM TEN-DENCY PREDICTION</head><p>In this work, we apply the proposed scheme to an application scenario: volunteerism tendency prediction. In modern society, volunteers are extremely crucial to NPOs to sustain their continuing operations.</p><p>The discovery of users' volunteerism tendency can significantly facilitate the recruitment of volunteers for NPOs, which can save considerable cost to find the potential volunteers. In particular, we cast the problem of volunteerism tendency prediction as a user binary classification. If the predicted tendency score of a given user is larger than a pre-defined end for 19: end for threshold γ, we regard this user as a volunteer. In this work, we explore three popular social networks: Twitter, Facebook and LinkedIn, as they are representative of a public, private, and professional social network, respectively. Besides, it is known that users exhibit different aspects on different social networks <ref type="bibr" target="#b1">[1]</ref>, and the combination of these three social networks would help to better characterize user behaviors on social platforms.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Multiple Social Accounts Mappings</head><p>To represent the same users with multiple sources, we need to first tackle the problem of "Social Account Mapping", which aims to align the same users across different social networks by linking their multiple social accounts <ref type="bibr" target="#b1">[1]</ref>. To accurately establish this mapping, we employ the emerging social services such as About.me 3 and Quora 4 , where they encourage users to explicitly list their multiple social accounts on one profile.</p><p>We proposed two strategies to collect data from About.me.</p><p>• Keyword search: We searched About.me with the keyword "volunteer" and obtained 4, 151 volunteer candidates. • Random select: We employed Random API 5 , provided by About.me, to collect non-volunteers. This API returns a specified number of random user profiles. Finally, we harvested 1, 867 non-volunteer candidates.</p><p>It is worth mentioning that volunteers may be present in these random users.</p><p>To enlarge our dataset, we also collected candidates from Quora by the breadth-first-search method. Particularly, we took advantage of both the follower and followee provided by Quora. Initially, we selected two popular users as the seed users and then explored all their neighboring connected users. We applied similar exploration approach to all other non-seed users. In the end, we collected 172, 235 users' profiles and only retained those who have accounts in Facebook, Twitter and LinkedIn.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Ground Truth Construction</head><p>Based on these candidates, we launched a crawler to collect their historical social contents, including their basic profiles, social posts and relations. However, the traditional web-based crawler is not applicable to Facebook due to its dynamic loading mechanism. We thus resorted to the Selenium<ref type="foot" target="#foot_3">7</ref> to simulate users' click and scroll operations on a FireFox browser and load users' publicly available information. We limited the access rate to one request per second to avoid being blocked by the robot checkers. It is worth mentioning that the data we collected is all publicly available. On the other hand, due to the privacy constraint, we could not access uses' social relations in Facebook and LinkedIn. We hence only collected users' followee relations in Twitter.</p><p>In order to improve the quality of our dataset, we employed three annotators to finalize our ground truth. As users tend to provide more complete and reliable profiles in LinkedIn, we guided the annotators to study the LinkedIn profiles of candidate users, and determine whether they are "volunteers" by majority votes. To ensure a uniformly labeling procedure, we provided them a piece of guideline. Given a user's LinkedIn profile, we classified the user as a volunteer if and only if this user lists his/her volunteer experiences in the section "Volunteer experience &amp; Causes" or section "Experience". Candidates who do not satisfy the above two criteria were tagged as nonvolunteers. We focused on LinkedIn to determine whether users are volunteers because the volunteer experiences in LinkedIn are the most straightforward evidence to identify volunteers. It should be noted that those who do not mention their volunteer experiences in LinkedIn are not necessarily classified as "non-volunteers". However, the absence of these mentions, at least, reveals their limited interests and low enthusiasm in volunteerism. Therefore, in our work, we broadly defined users as "non-volunteers" if they do not mention their relevant volunteerism experiences in LinkedIn.</p><p>Table <ref type="table" target="#tab_3">1</ref> lists the statistics of our dataset. We obtained the data for 1, 425 volunteers and 4, 011 non-volunteers according to the aforementioned strategies. The crawling was conducted between 22nd August to 11th September, 2013. Here we only selected a subset of non-volunteer data and made the dataset balanced to avoid the training bias. To facilitate this line of research, this dataset has been released after certain privacy preservation processing.</p><p>However, in reality, not all users are active enough on all social networks. To ensure the data quality, we treated those inactive users as missing with respect to a specific social network. Therefore, there exists block-wise missing data in our dataset. In particular, we treated a user as missing in Twitter or Facebook, if this user has less than 10 historical social posts. In addition, due to the absence of social post mechanism in LinkedIn, we treated a user as missing 8 in LinkedIn if the word count of this user's profile is less than 50. Figure <ref type="figure" target="#fig_2">3</ref> shows the statistics of our incomplete data. As can be seen, about 50% of users have complete data from all three social networks. 1% and 47% of users only miss the data either from Facebook and LinkedIn, while 2% of users miss the data from both of them. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3">Features</head><p>To capture users' volunteerism tendency, we extracted a rich set of volunteer-oriented features.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3.1">Demographic Characteristics</head><p>The study in <ref type="bibr" target="#b19">[19]</ref> reported that some demographic characteristics, such as education and income level, are strong indicators for volunteerism. This study inspires us to extract demographic characteristics from users' profiles, especially the Facebook and LinkedIn profiles. In our work, we explored users' demographic characteristics, including Gender, Relationship status, Education level, and Number of social connections.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3.2">Linguistic Features</head><p>We also extracted linguistic features, including Linguistic Inquiry and Word Count (LIWC) features, user topics and contextual topics.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>LIWC features.</head><p>LIWC is widely-used to analyze the psycho-linguistic transparent lexicon. It plays an important role in predicting users' personality <ref type="bibr" target="#b2">[2,</ref><ref type="bibr" target="#b15">15]</ref>. The main component of LIWC is a directory which contains the 8 Here we exclude the contents of section "Volunteer experience &amp; Causes" and section "Experience". mapping from words to 72 categories <ref type="foot" target="#foot_4">9</ref> . Given a document, LIWC computes the percentage of words in each category and represents it as a vector of 72 dimensions.</p><p>To capture the key aspects of LIWC features, we selected the top 5 dimensions as the representative LIWC features according to the information gain ratio.</p><p>Considering that the emotions for individuals may also affect users' volunteerism tendency, we additionally selected two categories from LIWC: positive emotion and negative emotion. Besides, we also utilized the positive-negative emotion ratio to further reflect users' emotional states. Let L(•) represent the percentage of users' words in certain LIWC category. The positive-negative emotion ratio is defined as,</p><formula xml:id="formula_46">P Nemo = L(pos)log L(pos) + ξp L(neg) + ξn , (<label>23</label></formula><formula xml:id="formula_47">)</formula><p>where ξp and ξn are introduced to avoid the situation: individuals have no positive or negative emotional word. They are both set as 0.0001. In total, we have 16 dimension LIWC features, extracted from Twitter and Facebook. User topics. According to our observation, volunteers may have, on average, a higher probability of talking about topics such as social caring or giving back, while the non-volunteers may mention other topics more often. This motivates us to explore the topic distributions of users' social posts to identify volunteers. We generated topic distributions using Latent Dirichlet Allocation (LDA) <ref type="bibr" target="#b4">[4]</ref>, which has been widely found to be useful in latent topic modeling <ref type="bibr">[8,</ref><ref type="bibr" target="#b23">23]</ref>. Based on perplexity <ref type="bibr" target="#b14">[14]</ref> metric frequently utilized to find the optimal number of hidden topics, we ultimately obtained 52, 26, 42 dimensional topic-level features over users' Twitter, Facebook and LinkedIn data, respectively. Contextual topics. We define users' contextual topics as the topics of users' connections. We believe that the contextual topics intuitively reflect the contexts of users.</p><p>"He that lies down with dogs must rise up with fleas" tells us that the context significantly affects a user's tendency.</p><p>Particularly, we studied followees and retweeting<ref type="foot" target="#foot_5">10</ref> connections on Twitter because of their intuitive reflection of topics that users concern. As the bio descriptions are usually provided by users to briefly introduce themselves and may indicate users' summarized interests, we integrated the bios of a user's followees or those whose tweets are retweeted by this user into two kinds of bio documents, on which we further applied LDA model. We utilized the perplexity to fix the dimensions of topiclevel features over followees' bio documents and retweetings' bio documents as 40 and 20, respectively. In this work, we only explored the contextual topics in Twitter, since we were unable to crawl the connections' profiles in LinkedIn and the bio descriptions are usually missing in Facebook.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3.3">Behavior-based features</head><p>This kind of features is characterized by users' posting behavior patterns and networking behavior patterns. The former focuses on the written style of users' social posts, while the latter captures their egocentric network features.</p><p>Posting behavior patterns. Posting behavior patterns have been investigated in many scenarios, spanning from age estimation to social spammers discovery <ref type="bibr" target="#b3">[3,</ref><ref type="bibr" target="#b13">13]</ref>. These patterns can be used to depict users' participation in information diffusion, which correlates with volunteerism tendency much.</p><p>On one hand, we employed the fraction of users' posts containing certain behaviors, including emoticons, slang words, acronyms, hashtags, URLs, and user mentions, to intuitively reflect users' engagement in topic discussion and social interaction. On the other hand, we observed that users' posting behaviors in social networks can be classified into a few categories. For example, posts in Twitter can be classified into two categories, Ctw = {tweets, retweets}, while posts in Facebook can roughly be split into eight types: C f b = {share link, share sideo, share status, share photo, change photo, repost, post, tagged}. The distributions over users' posts on these categories also reflect their participation in information diffusion, revealing whether a given user tend to share information in social networks. When it comes to Linkedin, we utilized the profile completeness to characterize users' behaviors. Based on our observation, we found that volunteers tend to provide more information for all the sections. This not only reflects volunteers' active participation in LinkedIn but also signals their selfconfidence and openness to public. Profile completeness is defined as a boolean vector over six dimensions to denote the presence of the six common sections in LinkedIn profiles: summary, interest, language, education, skill and honor. We excluded the sections on experience and volunteer experience &amp; causes, because the ground truth is built on these two sections. Egocentric network patterns. We also studied users' social behaviors from their egocentric networks. Intuitively, we believe that users belong to certain class tend to be connected with several class-specific accounts, as it goes for that "birds of a feather flock together". Therefore, volunteers should interact with some typical accounts in social media. The set of typical accounts is denoted as F V . Inspired by <ref type="bibr" target="#b17">[17]</ref>, we measured the degree of a user's correlation with volunteerism by three features: the frequency and fraction of a user's "friends" that belong to F V as well as the total number of "friends". In particular, we treated both the followees and retweetings as the "friends" of users in Twitter.</p><p>To construct the F V , we utilized the Twitter profile repository Wefollow 11 , which allows us to find the most prominent people given a particular category. By crawling prominent users falling into categories of Nonprofit, Charity, Volunteer, NGO, Community Service, Social Welfare and Christian from Wefollow, we obtained 23, 285 accounts.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">EXPERIMENTS</head><p>We conducted extensive experiments to comparatively verify our proposed scheme from various angles over a system equipped with Intel i72.60 GHz CPU, and 8 GB memory. In particular, we launched 10-fold cross validation for each experiment, and reported the average performance. Each fold involves 2, 249 training and 250 testing samples.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.1">On Model Comparison</head><p>We compared MSNL with four baselines. Before that, the data was completed by MSNDC. We also performed significant test to validate the effectiveness of MSNL.</p><p>11 http://wefollow.com/ SVM: We chose the learning formulation with the kernel of radial-basis function. We implemented this method based on LIBSVM <ref type="bibr" target="#b6">[6]</ref>.</p><p>RLS: Regularized least squares model <ref type="bibr" target="#b11">[11]</ref> aims to minimize the objective function of 1 2N y -Xw 2 + λ 2 w 2 .</p><p>In fact, the RLS model can be deduced from MSNL via the settings of α = [ 1 S , 1 S , • • • , 1 S ] T , µ = 0 and β = 0. iSFS: The third baseline is the incomplete source-feature selection model proposed in <ref type="bibr" target="#b24">[24]</ref>. This model only assigns weights to models learned from different social networks but ignores the relationships among them. We can derive iSFS from MSNL by making µ = 0.</p><p>regMVMT: The fourth baseline is the regularized multiview multi-task learning model <ref type="bibr" target="#b28">[28]</ref>. This model only regulates the relationships among different views but fails to take the source confidence into account. We can derive regMVMT from MSNL by making α =  <ref type="table" target="#tab_1">2</ref> shows the performance comparison between baselines and our proposed MSNL. We noticed that MSNL significantly outperforms the SVM and RLS. This implies that the information on multiple social networks are complementary and characterize users' volunteerism tendency consistently. This also proves that the correlations of different social networks with the task of volunteerism tendency prediction cannot be treated equally. In addition, MSNL achieves better performance, as compared with iSFS and regMVMT, which are the derivations of MSNL. This demonstrates that both the source confidence and the source consistency deserve particular attention.</p><formula xml:id="formula_48">[ 1 S , 1 S , • • • , 1 S ] T .</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.2">On Data Completion Comparison</head><p>We further evaluated the component for missing data completion with the following three baseline methods.</p><p>Remove: This method eliminates all data samples that are not complete.</p><p>Average: This method imputes the missing features with the average values of the corresponding feature items.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>KNN:</head><p>The missing data is inferred by averaging its Knearest neighbors. K is experimentally set as 1. Table <ref type="table" target="#tab_6">3</ref> shows the performance of different models over different data completion strategies. It can be seen that MSNDC outperforms the other strategies. Additionally, removing all incomplete data samples achieves the worst performance, which may be caused by the fact that it introduces training bias, making the dataset unbalanced and reduces the size of training dataset. We found that the percentage of volunteer samples decreases from 50% to 40% after filtering out all incomplete data samples.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.3">On Feature Comparison</head><p>To examine the discriminative features we extracted, we conducted experiments over different kinds of features using MSNL. We also performed significant test to validate the advantage of combining multiple social networks. Table <ref type="table" target="#tab_7">4</ref> comparatively shows the performance of MSNL in terms of different feature configurations. It can be seen that the linguistic features achieves the best performance, as compared against demographic characteristics and behaviorbased features. This reveals that volunteerism tendency is better reflected by their social contents, including their own social posts and the self-descriptions of their social connections. This also implies that users with volunteerism tendency may talk about related topics and follow or retweet related social accounts. In addition, we found that contextual topics are more discriminative as compared to users' own topics. This may be due to the fact that users' self-descriptions are of more value and contain less noise than users' tweets. Some hot topics discussed by volunteers given in Table <ref type="table" target="#tab_8">5</ref>. Besides, the egocentric network also play a dominant role in our task. This implies that one's social connections indeed reflect the user's personal concerns to a large extent.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.4">On Source Comparison</head><p>To demonstrate the descriptiveness of multiple social network integration, we conducted experiments over various source combinations. Notably, data from Facebook and LinkedIn is incomplete and we need to infer the block-wise missing data first taking advantage of the complete data samples from Twitter.</p><p>Table <ref type="table" target="#tab_9">6</ref> shows the performance of MSNL over different social network combinations. We noted that the more sources we incorporate, the better the performance can be achieved. This implies the complementary relationships rather than mutual conflicting relationships among the sources. Moreover, we found that aggregating data from all these three social networks can achieve significantly better performance as compared to each of the single source. Additionally, as the performance obtained from different single social networks are not the same, this validates that incorporating the confidence of different social networks to MSNL is reasonable. Interestingly, we observed that MSNL over Twitter alone achieves the much better performance, as compared to that over LinkedIn or Facebook alone. This may be caused by the fact that the </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.5">Size Varying of Positive Samples</head><p>In order to verify the usefulness of our model on real world dataset, where the volunteers should account for a minority portion of user population, we tuned the fraction of volunteer samples in our dataset. In particular, we fed x%, x ∈ <ref type="bibr" target="#b5">[5,</ref><ref type="bibr">50]</ref>, of volunteer samples to our model with stepsize 5%. Figure <ref type="figure" target="#fig_3">4</ref> shows the F1-measure with respect to different fraction of volunteer samples of different models. As can be seen, our model can achieve satisfactory performance even when volunteer samples only accounts for 5% of the whole samples. This demonstrates that the proposed MSNL model is not sensitive to the percentage of positive samples. Whereas, SVM and RLS are relatively more sensitive to the fraction of volunteer samples in dataset. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.6">Complexity Discussion</head><p>In order to analyze the complexity of MSNL, we need to solve the time complexity in terms of constructing M, L and t as defined in Eqn. <ref type="bibr">(8)</ref> and Eqn. <ref type="bibr" target="#b12">(12)</ref>, and computing the inverse of M and L. Assume D ≫ S, the construction of matrix M has a time complexity of O(N DS), and the construction of matrix L has a time complexity of O(N D 2 ). Due to the fact that the cost of matrix multiplications (X T s X s ′ ) and that of constructing t involved in Eqn. <ref type="bibr" target="#b12">(12)</ref> remain the same for all iterations and L is symmetric, we can save much practical time cost. Also, using the standard method, computing the inverse of two core matrices, M and L, has the complexity of O(S 3 ) and O(D 3 ), respectively. Furthermore, using the method of Coppersmith and Winogard, the time cost can be bounded by O(S 2.376 ) and O(D 2.376 ) <ref type="bibr" target="#b26">[26]</ref>, respectively. We note that the speed bottleneck lies in the number of features and the number of social networks instead of the number of data samples. As S and D are usually small, especially S, MSNL should be efficient in time complexity.</p><p>To validate the practical efficiency of the proposed MSNL model, we conducted a set of experiments. The comparison of average time consumption of different models is shown in Table <ref type="table" target="#tab_10">7</ref>. As can be seen, MSNL shows superiority over SVM in terms of the time cost, which takes only 19% of the time that SVM uses. By careful observation, we observed that MSNL converges very fast, which on average takes about 20 iterations. Even though MSNL takes more time than RLS and regMVMT due to the consideration of source consistency and source confidence, it improves the performance in terms of F1-measure. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.">CONCLUSIONS AND FUTURE WORK</head><p>This paper presented a novel scheme for multiple social network learning. This scheme takes the source confidence and source consistency into consideration by introducing regularization to the objective function.</p><p>We further demonstrated that the proposed scheme, designed for complete data, is also able to handle the real and more challenging cases where there exists block-wise missing data. In particular, before feeding the data into the proposed MSNL model, we inferred the missing data via NMF technique. Furthermore, we practically evaluated the proposed scheme in an interesting scenario of volunteerism tendency prediction. We developed a set of volunteeroriented features to characterize users' volunteerism tendency. Experimental results demonstrated the effectiveness of our proposed scheme and verified the advantages of utilizing multiple social network over a single source. Currently, we only consider solving a single task in the proposed scheme. In the future, we will extend our work to the context of multiple task learning.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Algorithm 1</head><label>1</label><figDesc>Alternative optimization for solving Eqn. (4) Input: X, y, λ, β, µ Output:</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: Illustration of the incomplete data from three sources. X C i s denotes the samples generated from social network s that are only available in the social network combination of Ci.</figDesc><graphic coords="5,323.08,39.81,223.56,157.68" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: Statistics of the incomplete data. Tw: Users with Twitter data only; Tw+Fb: Users with Twitter and Facebook data only; Tw+In: Users with Twitter and Linkedin data only; Tw+Fb+In: Users without missing data.</figDesc><graphic coords="7,57.79,235.69,228.16,140.38" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 4 :</head><label>4</label><figDesc>Figure 4: F1-measure at different fraction of volunteer samples.</figDesc><graphic coords="9,327.79,508.11,213.98,113.55" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0"><head></head><label></label><figDesc></figDesc><graphic coords="2,54.70,53.93,500.28,184.31" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Algorithm 2</head><label>2</label><figDesc>Alternative optimization for solving Eqn.<ref type="bibr" target="#b20">(20)</ref> </figDesc><table><row><cell cols="2">Input:</cell><cell cols="3">X1, X2, X3, ν, η</cell></row><row><cell cols="2">Output:</cell><cell>X</cell><cell></cell></row><row><cell cols="3">1: Initialize U</cell><cell cols="2">(0) s</cell><cell>according to Eqn. (21).</cell></row><row><cell cols="5">2: for k = 1, 2, • • • do</cell></row><row><cell>3:</cell><cell cols="4">for s = 1, 2, • • • , S do</cell></row><row><cell>4:</cell><cell cols="4">Compute each P</cell><cell>(k) s</cell><cell>according to Eqn. (20) via GCD</cell></row><row><cell></cell><cell cols="4">approach.</cell></row><row><cell>5:</cell><cell cols="4">Update U</cell><cell>(k) s</cell><cell>according to Eqn. (20) via GCD</cell></row><row><cell></cell><cell cols="4">approach.</cell></row><row><cell>6:</cell><cell cols="4">if the objective value stops decreasing then</cell></row><row><cell>7:</cell><cell></cell><cell cols="3">return Us = U (k) s</cell><cell>(k) and Ps = P s</cell></row><row><cell>8:</cell><cell cols="2">end if</cell><cell></cell></row><row><cell>9:</cell><cell cols="2">end for</cell><cell></cell></row><row><cell cols="3">10: end for</cell><cell></cell></row><row><cell cols="5">11: for j = 1, 2, • • • , S do</cell></row><row><cell>12:</cell><cell cols="4">for Cq ⊆ C do</cell></row><row><cell>13:</cell><cell cols="4">if j ∈ Cq then</cell></row><row><cell>14:</cell><cell></cell><cell cols="3">XCq j = X Cq j .</cell></row><row><cell>15:</cell><cell cols="2">else</cell><cell></cell></row><row><cell>16:</cell><cell></cell><cell cols="2">Infer</cell><cell>XCq j according to Eqn. (22).</cell></row><row><cell>17:</cell><cell cols="2">end if</cell><cell></cell></row><row><cell>18:</cell><cell></cell><cell></cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 1 : Statistics of our dataset.</head><label>1</label><figDesc></figDesc><table><row><cell>Data</cell><cell>Volunteer</cell><cell>Non-volunteer</cell></row><row><cell>Twitter profiles</cell><cell>∼1.5k</cell><cell>∼4k</cell></row><row><cell>Twitter posts</cell><cell>∼559k</cell><cell>∼1m</cell></row><row><cell>Twitter followees' profiles</cell><cell>∼902k</cell><cell>∼3m</cell></row><row><cell>Facebook profiles</cell><cell>∼1.5k</cell><cell>∼4k</cell></row><row><cell>Facebook posts</cell><cell>∼83k</cell><cell>∼338k</cell></row><row><cell>LinkedIn profiles</cell><cell>∼1.5k</cell><cell>∼4k</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 2 : Performance of different models(%).</head><label>2</label><figDesc></figDesc><table><row><cell>Approaches</cell><cell>F1-measure</cell><cell>P-value</cell></row><row><cell>SVM</cell><cell>83.11</cell><cell>0.038</cell></row><row><cell>RLS</cell><cell>82.82</cell><cell>0.025</cell></row><row><cell>regMVMT</cell><cell>84.07</cell><cell>0.173</cell></row><row><cell>iSFS</cell><cell>84.72</cell><cell>0.281</cell></row><row><cell>MSNL</cell><cell>85.59</cell><cell>-</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table</head><label></label><figDesc></figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 3 : Performance of different models over different data completion strategies.</head><label>3</label><figDesc></figDesc><table><row><cell>Approaches</cell><cell>SVM</cell><cell>RLS</cell><cell>MSNL</cell></row><row><cell>Remove</cell><cell>74.91</cell><cell>74.66</cell><cell>81.81</cell></row><row><cell>Average</cell><cell>82.09</cell><cell>81.99</cell><cell>85.43</cell></row><row><cell>KNN</cell><cell>82.60</cell><cell>82.22</cell><cell>85.55</cell></row><row><cell>MSNDC</cell><cell>83.11</cell><cell>82.82</cell><cell>85.59</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 4 : Performance of different features(%).</head><label>4</label><figDesc></figDesc><table><row><cell>Features</cell><cell>F1-measure</cell></row><row><cell>Demographic characteristics</cell><cell>68.43</cell></row><row><cell>Linguistic features</cell><cell>80.06</cell></row><row><cell>User topics</cell><cell>75.04</cell></row><row><cell>Contextual topics</cell><cell>78.14</cell></row><row><cell>LIWC</cell><cell>68.48</cell></row><row><cell>Behavior-based features</cell><cell>78.52</cell></row><row><cell>Posting behavior patterns</cell><cell>69.83</cell></row><row><cell>Egocentric network patterns</cell><cell>75.91</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>Table 5 : Hot topics discussed by volunteers. Followee and retweeting: contextual topics; Self: user topics.</head><label>5</label><figDesc></figDesc><table><row><cell>Data source</cell><cell>Topic words</cell></row><row><cell>Followee</cell><cell>• public, politics, rights, development • editor, global, journalist, university</cell></row><row><cell>Retweeting</cell><cell>• global, nonprofit, change, community • health, education, learning, university</cell></row><row><cell>Self</cell><cell>• woman, help, education, child • volunteer, nonprofit, support</cell></row><row><cell cols="2">most discriminative features evaluated by Section 6.3 are all</cell></row><row><cell cols="2">extracted from Twitter.</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head>Table 6 : Performance of different social network combinations(%). Facebook * and LinkedIn * both refer to the complete data, whose missing data is pre-inferred. F1: F1-measure.</head><label>6</label><figDesc></figDesc><table><row><cell>Social network combinations</cell><cell>F1</cell><cell>p-value</cell></row><row><cell>Twitter</cell><cell>82.35</cell><cell>4.2e-2</cell></row><row><cell>Facebook  *</cell><cell>73.53</cell><cell>5.0e-7</cell></row><row><cell>LinkedIn  *</cell><cell>74.49</cell><cell>3.1e-7</cell></row><row><cell>Twitter+Facebook  *</cell><cell>83.67</cell><cell>1.1e-1</cell></row><row><cell>Twitter+LinkedIn  *</cell><cell>83.84</cell><cell>1.4e-1</cell></row><row><cell>Facebook  *  +LinkedIn  *</cell><cell>76.29</cell><cell>6.0e-6</cell></row><row><cell>Twitter+Facebook  *  +LinkedIn  *</cell><cell>85.59</cell><cell>-</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_10"><head>Table 7 : Time cost of different models (%).</head><label>7</label><figDesc></figDesc><table><row><cell>Approach</cell><cell>Total (s)</cell><cell>Train (s)</cell><cell>Test (s)</cell></row><row><cell>SVM</cell><cell>2.0550</cell><cell>1.8211</cell><cell>0.2339</cell></row><row><cell>RLS</cell><cell>0.0639</cell><cell>0.0631</cell><cell>0.0008</cell></row><row><cell>regMVMT</cell><cell>0.0605</cell><cell>0.0595</cell><cell>0.0006</cell></row><row><cell>iSFS</cell><cell>0.5565</cell><cell>0.5557</cell><cell>0.0008</cell></row><row><cell>MSNL</cell><cell>0.3936</cell><cell>0.3929</cell><cell>0.0007</cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0"><p>According Paw Research Internet Project's Social Media Update</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" xml:id="foot_1"><p>2014: http://www.pewinternet.org/</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_2"><p>The compiled dataset is currently publicly accessible via: http://multiplesocialnetworklearning.azurewebsites.net/</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="7" xml:id="foot_3"><p>http://docs.seleniumhq.org/download/</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="9" xml:id="foot_4"><p>http://www.liwc.net/</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="10" xml:id="foot_5"><p>If A broadcasts a tweet posted by B, then B is A's a retweeting user.</p></note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgments</head><p>This research is supported by the Singapore National Research Foundation under its International Research Centre @ Singapore Funding Initiative and administered by the IDM Programme Office.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title/>
		<author>
			<persName><surname>References</surname></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<author>
			<persName><forename type="first">F</forename><surname>Abel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Herder</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G.-J</forename><surname>Houben</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Henze</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Krause</surname></persName>
		</author>
		<title level="m">Cross-system user modeling and personalization on the social web. UMUAI</title>
		<imprint>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">On the personality traits of stackoverflow users</title>
		<author>
			<persName><forename type="first">B</forename><surname>Bazelli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Hindle</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Stroulia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICSM</title>
		<imprint>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Detecting spammers and content promoters in online video social networks</title>
		<author>
			<persName><forename type="first">F</forename><surname>Benevenuto</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Rodrigues</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Almeida</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Almeida</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Gonçalves</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SIGIR</title>
		<imprint>
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Latent dirichlet allocation</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">M</forename><surname>Blei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">Y</forename><surname>Ng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">I</forename><surname>Jordan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">JMLR</title>
		<imprint>
			<date type="published" when="2003">2003</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Personalized social search based on the user&apos;s social network</title>
		<author>
			<persName><forename type="first">D</forename><surname>Carmel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Zwerdling</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Guy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Ofek-Koifman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Har'el</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Ronen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Uziel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Yogev</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Chernov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CIKM</title>
		<imprint>
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Libsvm: a library for support vector machines</title>
		<author>
			<persName><forename type="first">C.-C</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C.-J</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">TIST</title>
		<imprint>
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Visual-textual joint relevance learning for tag-based social image search</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z.-J</forename><surname>Zha</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Wu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">TIP</title>
		<imprint>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Named entity recognition in query</title>
		<author>
			<persName><forename type="first">J</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SIGIR</title>
		<imprint>
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Fast coordinate descent methods with variable selection for non-negative matrix factorization</title>
		<author>
			<persName><forename type="first">C.-J</forename><surname>Hsieh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><forename type="middle">S</forename><surname>Dhillon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SIGKDD</title>
		<imprint>
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Partial multi-view clustering</title>
		<author>
			<persName><forename type="first">S.-Y</forename><forename type="middle">L Y</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z.-H</forename><surname>Zhou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">A method for large-scale l1-regularized least squares problems with applications in signal processing and statistics</title>
		<author>
			<persName><forename type="first">S</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Koh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Lustig</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Boyd</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Gorinevsky</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J-STSP</title>
		<imprint>
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Learning the parts of objects by non-negative matrix factorization</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">D</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">S</forename><surname>Seung</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nature</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Uncovering social spammers: social honeypots+ machine learning</title>
		<author>
			<persName><forename type="first">K</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Caverlee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Webb</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SIGIR</title>
		<imprint>
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Community-based topic modeling for social tagging</title>
		<author>
			<persName><forename type="first">D</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Sugimoto</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Qin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Dong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CIKM</title>
		<imprint>
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Mining facebook data for predictive personality modeling</title>
		<author>
			<persName><forename type="first">D</forename><surname>Markovikj</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Gievska</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Kosinski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Stillwell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICWSM</title>
		<imprint>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Learning to recommend descriptive tags for questions in social forums</title>
		<author>
			<persName><forename type="first">L</forename><surname>Nie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y.-L</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T.-S</forename><surname>Chua</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">TOIS</title>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Democrats, republicans and starbucks afficionados: user classification in twitter</title>
		<author>
			<persName><forename type="first">M</forename><surname>Pennacchiotti</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A.-M</forename><surname>Popescu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SIGKDD</title>
		<imprint>
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Dispositional and organizational influences on sustained volunteerism: An interactionist perspective</title>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">A</forename><surname>Penner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">JSI</title>
		<imprint>
			<date type="published" when="2002">2002</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Volunteerism and social problems: Making things better or worse?</title>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">A</forename><surname>Penner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">JSI</title>
		<imprint>
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">Our twitter profiles, our selves: predicting personality with twitter</title>
		<author>
			<persName><forename type="first">D</forename><surname>Quercia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Kosinski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Stillwell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Crowcroft</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2011">2011</date>
			<publisher>In SocialCom</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">The personality of popular facebook users</title>
		<author>
			<persName><forename type="first">D</forename><surname>Quercia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Lambiotte</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Stillwell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Kosinski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Crowcroft</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CSCW</title>
		<imprint>
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Implicit user modeling for personalized search</title>
		<author>
			<persName><forename type="first">X</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Zhai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CIKM</title>
		<imprint>
			<date type="published" when="2005">2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Lda-based document models for ad-hoc retrieval</title>
		<author>
			<persName><forename type="first">X</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">B</forename><surname>Croft</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SIGIR</title>
		<imprint>
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Multi-source learning with block-wise missing data for alzheimer&apos;s disease prediction</title>
		<author>
			<persName><forename type="first">S</forename><surname>Xiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">M</forename><surname>Thompson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Ye</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SIGKDD</title>
		<imprint>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Multi-source learning for joint analysis of incomplete multi-modality neuroimaging data</title>
		<author>
			<persName><forename type="first">L</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">M</forename><surname>Thompson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><forename type="middle">A</forename><surname>Narayan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Ye</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SIGKDD</title>
		<imprint>
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Multiview metric learning with global consistency and local smoothness</title>
		<author>
			<persName><forename type="first">D</forename><surname>Zhai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Shan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Gao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">TIST</title>
		<imprint>
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Composite hashing with multiple information sources</title>
		<author>
			<persName><forename type="first">D</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Si</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SIGIR</title>
		<imprint>
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Inductive multi-task learning with multiple view data</title>
		<author>
			<persName><forename type="first">J</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Huan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SIGKDD</title>
		<imprint>
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Topic hierarchy construction for the organization of multi-source user generated contents</title>
		<author>
			<persName><forename type="first">X</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z.-Y</forename><surname>Ming</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T.-S</forename><surname>Chua</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SIGIR</title>
		<imprint>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
