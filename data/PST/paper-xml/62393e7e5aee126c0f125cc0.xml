<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Learning to Reason Deductively: Math Word Problem Solving as Complex Relation Extraction</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date type="published" when="2022-04-25">25 Apr 2022</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Zhanming</forename><surname>Jie</surname></persName>
							<affiliation key="aff0">
								<orgName type="laboratory">StatNLP Research Group</orgName>
								<orgName type="institution" key="instit1">University of Texas at Austin ♦</orgName>
								<orgName type="institution" key="instit2">Singapore University of Technology</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Jierui</forename><surname>Li</surname></persName>
							<email>jierui@cs.utexas.edu</email>
							<affiliation key="aff0">
								<orgName type="laboratory">StatNLP Research Group</orgName>
								<orgName type="institution" key="instit1">University of Texas at Austin ♦</orgName>
								<orgName type="institution" key="instit2">Singapore University of Technology</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Wei</forename><surname>Lu</surname></persName>
							<email>luwei@sutd.edu.sg</email>
							<affiliation key="aff0">
								<orgName type="laboratory">StatNLP Research Group</orgName>
								<orgName type="institution" key="instit1">University of Texas at Austin ♦</orgName>
								<orgName type="institution" key="instit2">Singapore University of Technology</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">A</forename><forename type="middle">I</forename><surname>Bytedance</surname></persName>
							<affiliation key="aff0">
								<orgName type="laboratory">StatNLP Research Group</orgName>
								<orgName type="institution" key="instit1">University of Texas at Austin ♦</orgName>
								<orgName type="institution" key="instit2">Singapore University of Technology</orgName>
							</affiliation>
						</author>
						<author>
							<persName><surname>Lab</surname></persName>
							<affiliation key="aff0">
								<orgName type="laboratory">StatNLP Research Group</orgName>
								<orgName type="institution" key="instit1">University of Texas at Austin ♦</orgName>
								<orgName type="institution" key="instit2">Singapore University of Technology</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Learning to Reason Deductively: Math Word Problem Solving as Complex Relation Extraction</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2022-04-25">25 Apr 2022</date>
						</imprint>
					</monogr>
					<idno type="arXiv">arXiv:2203.10316v3[cs.CL]</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.2" ident="GROBID" when="2022-12-25T14:14+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Solving math word problems requires deductive reasoning over the quantities in the text. Various recent research efforts mostly relied on sequence-to-sequence or sequence-to-tree models to generate mathematical expressions without explicitly performing relational reasoning between quantities in the given context. While empirically effective, such approaches typically do not provide explanations for the generated expressions. In this work, we view the task as a complex relation extraction problem, proposing a novel approach that presents explainable deductive reasoning steps to iteratively construct target expressions, where each step involves a primitive operation over two quantities defining their relation. Through extensive experiments on four benchmark datasets, we show that the proposed model significantly outperforms existing strong baselines. We further demonstrate that the deductive procedure not only presents more explainable steps but also enables us to make more accurate predictions on questions that require more complex reasoning.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Math word problem (MWP) solving <ref type="bibr" target="#b4">(Bobrow, 1964)</ref> is a task of answering a mathematical question that is described in natural language. Solving MWP requires logical reasoning over the quantities presented in the context <ref type="bibr" target="#b38">(Mukherjee and Garain, 2008)</ref> to compute the numerical answer. Various recent research efforts regarded the problem as a generation problem -typically, such models focus on generating the complete target mathematical expression, often represented in the form of a linear sequence or a tree structure <ref type="bibr" target="#b58">(Xie and Sun, 2019)</ref>.</p><p>Figure <ref type="figure" target="#fig_0">1</ref> (top) depicts a typical approach that attempts to generate the target expression in the 1 Our code and data are released at https://github. com/allanj/Deductive-MWP.</p><p>Question: In a division sum , the remainder is 8 and the divisor is 6 times the quotient and is obt--ained by adding 3 to the thrice of the remainder. What is the dividend? form of a tree structure, which is adopted in recent research efforts <ref type="bibr" target="#b58">(Xie and Sun, 2019;</ref><ref type="bibr" target="#b60">Zhang et al., 2020;</ref><ref type="bibr" target="#b40">Patel et al., 2021;</ref><ref type="bibr" target="#b57">Wu et al., 2021)</ref>. Specifically, the output is an expression that can be obtained from such a generated structure. We note that, however, there are several limitations with such a structure generation approach. First, such a process typically involves a particular order when generating the structure. In the example, given the complexity of the problem, the decision of generating the addition ("+") operation as the very first step could be counter-intuitive and does not provide adequate explanations that show the reasoning process when being presented to a human learner. Furthermore, the resulting tree contains identical sub-trees ("8 × 3 + 3") as highlighted in blue dashed boxes. Unless a certain specifically designed mechanism is introduced for reusing the already generated intermediate expression, the approach would need to repeat the same effort in its process for generating the same sub-expression.</p><p>Solving math problems generally requires deductive reasoning, which is also regarded as one of the important abilities in children's cognitive development <ref type="bibr" target="#b41">(Piaget, 1952)</ref>. In this work, we propose a novel approach that explicitly presents deductive reasoning steps. We make a key observation that MWP solving fundamentally can be viewed as a complex relation extraction problem -the task of identifying the complex relations among the quantities that appear in the given problem text. Each primitive arithmetic operation (such as addition, subtraction) essentially defines a different type of relation. Drawing on the success of some recent models for relation extraction in the literature <ref type="bibr" target="#b61">(Zhong and Chen, 2021)</ref>, our proposed approach involves a process that repeatedly performs relation extraction between two chosen quantities (including newly generated quantities).</p><p>As shown in Figure <ref type="figure" target="#fig_0">1</ref>, our approach directly extracts the relation ("multiplication", or "×") between 8 and 3, which come from the contexts "remainder is 8" and "thrice of the remainder". In addition, it allows us to reuse the results from the intermediate expression in the fourth step. This process naturally yields a deductive reasoning procedure that iteratively derives new knowledge from existing ones. Designing such a complex relation extraction system presents several practical challenges. For example, some quantities may be irrelevant to the question while some others may need to be used multiple times. The model also needs to learn how to properly handle the new quantities that emerge from the intermediate expressions.</p><p>Learning how to effectively search for the optimal sequence of operations (relations) and when to stop the deductive process is also important.</p><p>In this work, we tackle the above challenges and make the following major contributions:</p><p>• We formulate MWP solving as a complex relation extraction task, where we aim to repeatedly identify the basic relations between different quantities. To the best of our knowledge, this is the first effort that successfully tackles MWP solving from such a new perspective.</p><p>• Our model is able to automatically produce explainable steps that lead to the final answer, presenting a deductive reasoning process.</p><p>• Our experimental results on four standard datasets across two languages show that our model significantly outperforms existing strong baselines. We further show that the model per-forms better on problems with more complex equations than previous approaches.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Work</head><p>Early efforts focused on solving MWP using probabilistic models with handcrafted features <ref type="bibr" target="#b33">(Liguda and Pfeiffer, 2012)</ref>. <ref type="bibr" target="#b23">Kushman et al. (2014)</ref> and <ref type="bibr" target="#b45">Roy and Roth (2018)</ref> designed templates to find the alignments between the declarative language and equations. Most recent works solve the problem by using sequence or tree generation models. <ref type="bibr" target="#b35">Wang et al. (2017)</ref> proposed the Math23k dataset and presented a sequence-to-sequence (seq2seq) approach to generate the mathematical expression <ref type="bibr" target="#b6">(Chiang and Chen, 2019)</ref>. Other approaches improve the seq2seq model with reinforcement learning <ref type="bibr" target="#b16">(Huang et al., 2018)</ref>, template-based methods <ref type="bibr" target="#b52">(Wang et al., 2019)</ref>, and group attention mechanism <ref type="bibr" target="#b28">(Li et al., 2019)</ref>. <ref type="bibr" target="#b58">Xie and Sun (2019)</ref> proposed a goal-driven tree-structured (GTS) model to generate the expression tree. This sequence-to-tree approach significantly improved the performance over the traditional seq2seq approaches. Some follow-up works incorporated external knowledge such as syntactic dependency <ref type="bibr" target="#b47">(Shen and Jin, 2020;</ref><ref type="bibr" target="#b34">Lin et al., 2021)</ref> or commonsense knowledge <ref type="bibr" target="#b56">(Wu et al., 2020)</ref>. <ref type="bibr" target="#b5">Cao et al. (2021)</ref> modeled the equations as a directed acyclic graph to obtain the expression. <ref type="bibr" target="#b60">Zhang et al. (2020)</ref> and <ref type="bibr" target="#b29">Li et al. (2020)</ref> adopted a graph-to-tree approach to model the quantity relations using the graph convolutional networks (GCN) <ref type="bibr" target="#b20">(Kipf and Welling, 2017)</ref>. Applying pre-trained language models such as BERT <ref type="bibr" target="#b12">(Devlin et al., 2019)</ref> was shown to significantly benefit the tree expression generation <ref type="bibr" target="#b24">(Lan et al., 2021;</ref><ref type="bibr" target="#b49">Tan et al., 2021;</ref><ref type="bibr" target="#b32">Liang et al., 2021;</ref><ref type="bibr" target="#b30">Li et al., 2021;</ref><ref type="bibr" target="#b46">Shen et al., 2021)</ref>. Different from the tree-based generation models, our work is related to deductive systems <ref type="bibr" target="#b48">(Shieber et al., 1995;</ref><ref type="bibr" target="#b39">Nederhof, 2003)</ref> where we aim to obtain step-by-step expressions. Recent efforts have also been working towards this direction. <ref type="bibr" target="#b35">Ling et al. (2017)</ref> constructed a dataset to provide explanations for expressions at each step. <ref type="bibr" target="#b0">Amini et al. (2019)</ref> created the MathQA dataset annotated with step-by-step operations. The annotations present the expression at each intermediate step during problem-solving. Our deductive process (Figure <ref type="figure" target="#fig_0">1</ref>) attempts to automatically obtain the expression in an incremental, step-by-step manner.</p><p>Our approach is also related to relation extraction (RE) <ref type="bibr" target="#b59">(Zelenko et al., 2003)</ref>, a fundamental task in the field of information extraction that is focused on identifying the relationships between a pair of entities. Recently, Zhong and Chen (2021) designed a simple and effective approach to directly model the relations on the span pair representations. In this work, we treat the operation between a pair of quantities as the relation at each step in our deductive reasoning process. Traditional methods <ref type="bibr" target="#b31">(Liang et al., 2018)</ref> applied rule-based approaches to extract the mathematical relations.</p><formula xml:id="formula_0">input: q in Q (0) axiom: 0 ∶ ⟨q 1 , ⋯, q |Q (0) | ⟩ t ∶ ⟨q 1 , ⋯, q |Q (t−1) | ⟩ t + 1 ∶ ⟨q 1 , ⋯, q |Q (t−1) | | q |Q (t) | ∶= e (t) i,j,op ⟩ q i op − → q j :</formula><p>MWP solving is typically regarded as one of the system 2 tasks <ref type="bibr" target="#b17">(Kahneman, 2011;</ref><ref type="bibr" target="#b2">Bengio et al., 2021)</ref>, and our current approach to this problem is related to neural symbolic reasoning <ref type="bibr" target="#b3">(Besold et al., 2017)</ref>. We design differentiable modules <ref type="bibr" target="#b1">(Andreas et al., 2016;</ref><ref type="bibr" target="#b14">Gupta et al., 2020)</ref> in our model ( §3.2) to perform reasoning among the quantities.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Approach</head><p>The math word problem solving task can be defined as follows. Given a problem description S = {w 1 , w 2 , ⋯, w n } that consists of a list of n words and Q S = {q 1 , q 2 , ⋯, q m }, a list of m quantities that appear in S, our task is to solve the problem and return the numerical answer. Ideally, the answer shall be computed through a mathematical reasoning process over a series of primitive mathematical operations <ref type="bibr" target="#b0">(Amini et al., 2019)</ref> as shown in Figure <ref type="figure" target="#fig_0">1</ref>. Such operations may include "+" (addition), "−" (subtraction), "×" (multiplication), "÷" (division), and " * * " (exponentiation). In our view, each of the primitive mathematical operations above can essentially be used for describing a specific relation between quantities. Fundamentally, solving a math word problem is a problem of complex relation extraction, which requires us to repeatedly identify the relations between quantities (including those appearing in the text and those intermediate ones created by relations). The overall solving procedure requires in-voking a relation classification module at each step, yielding a deductive reasoning process.</p><p>In practice, some questions cannot be answered without relying on certain predefined constants (such as π and 1) that may not have appeared in the given problem description. We therefore also consider a set of constants C = {c 1 , c 2 , ⋯, c |C| }. Such constants are also regarded as quantities (i.e., they would be regarded as {q m+1 , q m+2 , . . . , q m+|C| }) which may play useful roles when forming the final answer expression.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">A Deductive System</head><p>As shown in Figure <ref type="figure" target="#fig_0">1</ref>, applying the mathematical relation (e.g., "+") between two quantities yields an intermediate expression e. In general, at step t, the resulting expression e (t) (after evaluation) becomes a newly created quantity that is added to the list of candidate quantities and is ready for participating in the remaining deductive reasoning process from step t + 1 onward. This process can be mathematically denoted as follows:</p><p>• Initialization:</p><formula xml:id="formula_1">Q (0) = Q S ∪ C • At step t: e (t) i,j,op = q i op − → q j q i , q j ∈ Q (t−1) Q (t) = Q (t−1) ∪ {e (t) i,j,op } q |Q (t) | ∶= e (t) i,j,op</formula><p>where e (t) i,j,op represents the expression after applying the relation op to the ordered pair (q i , q j ). Following the standard deduction systems <ref type="bibr" target="#b48">(Shieber et al., 1995;</ref><ref type="bibr" target="#b39">Nederhof, 2003)</ref>, the reasoning process can be formulated in Figure <ref type="figure" target="#fig_1">2</ref>. We start with an axiom with the list of quantities in Q (0) . The inference rule is q i op − → q j as described above to obtain the expression as a new quantity at step t.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Model Components</head><p>Reasoner Figure <ref type="figure" target="#fig_3">3</ref> shows the deductive reasoning procedure in our model for an example that involves 3 quantities. We first convert the quantities (e.g., 2, 088) into a general quantity token "&lt;quant&gt;". We next adopt a pre-trained language model such as BERT <ref type="bibr" target="#b12">(Devlin et al., 2019)</ref> or Roberta <ref type="bibr" target="#b11">(Cui et al., 2019;</ref><ref type="bibr" target="#b36">Liu et al., 2019)</ref> to obtain the quantity representation q for each quantity q.</p><p>If a machine can make 2,088 gears in 8 hours, how many gears it make in 9 hours? We show the inference procedure to obtain the expression "q 1 ÷ q 2 × q 3 " for the example question.</p><formula xml:id="formula_2">q 1 q 2 q 3 q 1 q 2 q 3 t = 1 [q 1 , q 2 , q 1 • q 2 ] FFN op="÷" FFN op="×" e 1,2,÷ e 1,2,× q ′ 1 q ′ 2 q ′ 3 q 4 t = 2 q ′ 3 , q 4 , q ′ 3 • q 4 FFN op="×" e 3,4,×</formula><p>Given the quantity representations, we consider all the possible quantity pairs, (q i , q j ). Similar to <ref type="bibr" target="#b25">Lee et al. (2017)</ref>, we can obtain the representation of each pair by concatenating the two quantity representations and the element-wise product between them. As shown in Figure <ref type="figure" target="#fig_3">3</ref>, we apply a non-linear feed-forward network (FFN) on top of the pair representation to get the representation of the newly created expression. The above procedure can be mathematically written as:</p><formula xml:id="formula_3">e i,j,op = FFN op ( q i , q j , q i • q j ), i ≤ j (1)</formula><p>where e i,j,op is the representation of the intermediate expression e and op is the operation (e.g., "+", "−") applied to the ordered pair (q i , q j ). FFN op is an operation-specific network that gives the expression representation under the particular operation op. Note that we have the constraint i ≤ j. As a result we also consider the "reverse operation" for division and subtraction <ref type="bibr" target="#b44">(Roy and Roth, 2015)</ref>. As shown in Figure <ref type="figure" target="#fig_3">3</ref>, the expression e 1,2,÷ will be regarded as a new quantity with representation q 4 at t = 1. In general, we can assign a score to a single reasoning step that yields the expression e (t) i,j,op from q i and q j with operation op. Such a score can be calculated by summing over the scores defined over the representations of the two quantities and the score defined over the expression:</p><formula xml:id="formula_4">s(e (t)</formula><p>i,j,op ) = s q (q i ) + s q (q j ) + s e (e i,j,op ) (2) where we have:</p><formula xml:id="formula_5">s q (q i ) = w q ⋅ FFN(q i )</formula><p>s e (e i,j,op ) = w e ⋅ e i,j,op</p><p>(3)</p><formula xml:id="formula_6">Rationalizer Mechanism Multi-head Self-Attention Attention(Q = [q i , e] , K = [q i , e] , V = [q i , e]) GRU cell GRU_Cell(input = q i , previous hidden = e)</formula><p>Table <ref type="table">1</ref>: The mechanism in different rationalizers. where s q (⋅) and s e (⋅) are the scores assigned to the quantity and the expression, respectively, and w q and w e are the corresponding learnable parameters.</p><formula xml:id="formula_7">q i Rationalizer q ′ i e Expression</formula><p>Our goal is to find the optimal expression sequence [e (1) , e</p><p>(2) , ⋯, e (T ) ] that enables us to compute the final numerical answer, where T is the total number of steps required for this deductive process.</p><p>Terminator Our model also has a mechanism that decides whether the deductive procedure is ready to terminate at any given time. We introduce a binary label τ , where 1 means the procedure stops here, and 0 otherwise. The final score of the expression e at time step t can be calculated as:</p><formula xml:id="formula_8">S e (t) i,j,op , τ = s(e (t) i,j,op ) + w τ ⋅ FFN(e i,j,op ) (4)</formula><p>where w τ is the parameter vector for scoring the τ .</p><p>Rationalizer Once we obtain a new intermediate expression at step t, it is crucial to update the representations for the existing quantities. We call this step rationalization because it could potentially give us the rationale that explains an outcome <ref type="bibr" target="#b27">(Lei et al., 2016)</ref>. As shown in Figure <ref type="figure" target="#fig_4">4</ref>, the intermediate expression e serves as the rationale that explains how the quantity changes from q to q ′ . Without this step, there is a potential shortcoming for the model. That is, because if the quantity representations do not get updated as we continue the deductive reasoning process, those expressions that were initially highly ranked (say, at the first step) would always be preferred over those lowly ranked ones throughout the process. <ref type="foot" target="#foot_1">3</ref> We rationalize the quantity representation using the current intermediate expression e (t) , so that the quantity is aware of the generated expressions when its representation gets updated. This procedure can be mathematically formulated as follows: Two well-known techniques we can adopt as rationalizers are multi-head self-attention <ref type="bibr" target="#b50">(Vaswani et al., 2017)</ref> and a gated recurrent unit (GRU) <ref type="bibr" target="#b7">(Cho et al., 2014)</ref> cell, which allow us to update the quantity representation, given the intermediate expression representation. Table <ref type="table">1</ref> shows the mechanism in two different rationalizers. For the first approach, we essentially construct a sentence with two token representations -quantity q i and the previous expression e -to perform self-attention. In the second approach, we use q i as the input state and e as the previous hidden state in a GRU cell.</p><formula xml:id="formula_9">q ′ i = Rationalizer(q i , e (t) ) ∀ 1 ≤ i ≤ |Q| (5)</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Training and Inference</head><p>Similar to training sequence-to-sequence models <ref type="bibr" target="#b37">(Luong et al., 2015)</ref>, we adopt the teacherforcing strategy <ref type="bibr" target="#b54">(Williams and Zipser, 1989</ref>) to guide the model with gold expressions during training. The loss<ref type="foot" target="#foot_2">4</ref> can be written as:  and terminate the process. The overall expression (formed by the resulting expression sequence) will be used for computing the final numerical answer.</p><formula xml:id="formula_10">L(θ) = T t=1 max (i,</formula><p>Declarative Constraints Our model repeatedly relies on existing quantities to construct new quantities, which results in a structure showing the deductive reasoning process. One advantage of such an approach is that it allows certain declarative knowledge to be conveniently incorporated. For example, as we can see in Equation <ref type="formula">6</ref>, the default approach considers all the possible combinations among the quantities during the maximization step. We can easily impose constraints to avoid considering certain combinations. In practice, we found in certain datasets such as SVAMP, there does not exist any expression that involve operations applied to the same quantity (such as 9 + 9 or 9 × 9, where 9 is from the same quantity in the text). Besides, we also observe that the intermediate results would not be negative. We can simply exclude such cases in the maximization process, effectively reducing the search space during both training and inference. We show that adding such declarative constraints can help improve the performance. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experiments</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>S2S</head><p>GroupAttn <ref type="bibr" target="#b28">(Li et al., 2019)</ref> 76.1 Transformer <ref type="bibr" target="#b50">(Vaswani et al., 2017)</ref> 85.6 BERT-BERT <ref type="bibr" target="#b24">(Lan et al., 2021)</ref> 86.9 Roberta-Roberta <ref type="bibr" target="#b24">(Lan et al., 2021)</ref> 88.4</p><p>S2T/G2T GTS <ref type="bibr" target="#b58">(Xie and Sun, 2019)</ref> 82.6 Graph2Tree <ref type="bibr" target="#b60">(Zhang et al., 2020)</ref> 85.6 Roberta-GTS <ref type="bibr" target="#b40">(Patel et al., 2021)</ref> 88.5 Roberta-Graph2Tree <ref type="bibr" target="#b40">(Patel et al., 2021)</ref> 88  adapt the dataset to filter out some questions that are unsolvable. We consider the operations "addition", "subtraction", "multiplication", and "division" for MAWPS and SVAMP, and an extra "exponentiation" for MathQA and Math23k.</p><p>The number of operations involved in each question can be one of the indicators to help us gauge the difficulty of a dataset. Figure <ref type="figure" target="#fig_5">5</ref> shows the percentage distribution of the number of operations involved in each question. The MathQA dataset generally contains larger portions of questions that involve more operations, while 97% of the questions in MAWPS can be answered with only one or two operations. More than 60% of the instances in MathQA have three or more operations, which likely makes their problems harder to solve. Furthermore, MathQA <ref type="bibr" target="#b0">(Amini et al., 2019)</ref> contains GRE questions in many domains including physics, geometry, probability, etc., while Math23k questions are from primary school. Different from other datasets, SVAMP <ref type="bibr" target="#b40">(Patel et al., 2021)</ref> 7 is a challenging set that is manually created to evaluate a model's robustness. They applied variations over the instances sampled from MAWPS. Such variations could be: adding extra quantities, swapping the positions between noun phrases, etc.</p><p>Baselines The baseline approaches can be broadly categorized into sequence-to-sequence (S2S), sequence-to-tree (S2T) and graph-to-tree (G2T) models. GroupAttn <ref type="bibr" target="#b28">(Li et al., 2019)</ref> designed several types of attention mechanisms such as question or quantity related attentions in the seq2seq model. <ref type="bibr" target="#b49">Tan et al. (2021)</ref> uses multilingual firmed such information with the authors of <ref type="bibr" target="#b49">Tan et al. (2021)</ref>, and make our version of this dataset publicly available. 7 There is no test split for this dataset. We strictly follow the experiment setting in <ref type="bibr" target="#b40">Patel et al. (2021)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Model</head><p>Val Acc. Test 5-fold</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>S2S</head><p>GroupAttn <ref type="bibr" target="#b28">(Li et al., 2019)</ref> 69.5 66.9 mBERT-LSTM <ref type="bibr" target="#b49">(Tan et al., 2021)</ref> 75.1 -BERT-BERT <ref type="bibr" target="#b24">(Lan et al., 2021)</ref> -76.6 Roberta-Roberta <ref type="bibr" target="#b24">(Lan et al., 2021)</ref> -76.9</p><p>S2T/G2T</p><p>GTS <ref type="bibr" target="#b58">(Xie and Sun, 2019)</ref> 75.6 74.3 KA-S2T † <ref type="bibr" target="#b56">(Wu et al., 2020)</ref> 76.3 -MultiE&amp;D <ref type="bibr" target="#b47">(Shen and Jin, 2020)</ref> 78.4 76.9 Graph2Tree <ref type="bibr" target="#b60">(Zhang et al., 2020)</ref> 77.4 75.5 NeuralSymbolic <ref type="bibr" target="#b42">(Qin et al., 2021)</ref> -75.7 NUMS2T † <ref type="bibr" target="#b57">(Wu et al., 2021)</ref> 78.1 -HMS <ref type="bibr" target="#b34">(Lin et al., 2021)</ref> 76.1 -BERT-Tree <ref type="bibr" target="#b30">(Li et al., 2021)</ref> 82 Table <ref type="table">4</ref>: Results on Math23k. †: they used their own splits (so their results may not be directly comparable).</p><p>BERT with an LSTM decoder (mBERT-LSTM). <ref type="bibr" target="#b24">Lan et al. (2021)</ref> presented two seq2seq models that use BERT/Roberta as both encoder and decoder, namely, BERT-BERT and Roberta-Roberta.</p><p>Sequence-to-tree models mainly use a tree-based decoder with GRU (GTS) <ref type="bibr" target="#b58">(Xie and Sun, 2019)</ref> or BERT as the encoder (BERT-Tree) <ref type="bibr" target="#b32">(Liang et al., 2021;</ref><ref type="bibr" target="#b30">Li et al., 2021)</ref>. NUMS2T <ref type="bibr" target="#b56">(Wu et al., 2020)</ref> and NeuralSymbolic <ref type="bibr" target="#b42">(Qin et al., 2021)</ref> solver incorporate external knowledge in the S2T architectures. Graph2Tree <ref type="bibr" target="#b60">(Zhang et al., 2020</ref>) models the quantity relations using GCN.</p><p>Training Details We adopt BERT <ref type="bibr" target="#b12">(Devlin et al., 2019)</ref> and Roberta <ref type="bibr" target="#b36">(Liu et al., 2019)</ref> for the English datasets. Chinese BERT and Chinese Roberta <ref type="bibr" target="#b11">(Cui et al., 2019)</ref> are used for Math23k. We use the GRU cell as the rationalizer. We also conduct experiments with multilingual BERT and XLM-Roberta <ref type="bibr" target="#b10">(Conneau et al., 2020)</ref>. The pre-trained models are initialized from HuggingFace's Transformers <ref type="bibr" target="#b55">(Wolf et al., 2020)</ref>. We optimize the loss with the Adam optimizer <ref type="bibr" target="#b19">(Kingma and Ba, 2014;</ref><ref type="bibr" target="#b37">Loshchilov and Hutter, 2019)</ref>. We use a learning rate of 2e-5 and a batch size of 30. The regularization coefficient λ is set to 0.01. We run our models with 5 random seeds and report the average results (with standard deviation). Following most previous works, we mainly report the value accuracy (percentage) in our experiments. In other words, a prediction is considered correct if the predicted expression leads to the same value as the gold expression. Following previous practice <ref type="bibr" target="#b60">(Zhang et al., 2020;</ref><ref type="bibr" target="#b49">Tan et al., 2021;</ref><ref type="bibr" target="#b40">Patel et al., 2021)</ref>, we report</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Model</head><p>Val Acc.</p><p>Graph2Tree <ref type="bibr" target="#b60">(Zhang et al., 2020)</ref> 69.5 BERT-Tree <ref type="bibr" target="#b30">(Li et al., 2021)</ref> 73.8 mBERT+LSTM <ref type="bibr" target="#b49">(Tan et al., 2021)</ref> 77  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>5-fold cross-validation results on both MAWPS</head><p>8 and Math23k, and also report the test set performance for Math23k, MathQA and SVAMP.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Results</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>MAWPS and Math23k</head><p>We first discuss the results on MAWPS and Math23k, two datasets that are commonly used in previous research. Table <ref type="table" target="#tab_4">3 and 4</ref> show the main results of the proposed models with different pre-trained language models. We compare with previous works that have reported results on these datasets. Among all the encoders for our model DEDUCTREASONER, the Roberta encoder achieves the best performance. In addition, DEDUCTREASONER significantly outperforms all the baselines regardless of the choice of encoder. The performance on the best S2S model (Roberta-Roberta) is on par with the best S2T model (Roberta-Graph2Tree) on MAWPS. Overall, the accuracy of Roberta-based DEDUCTREA-SONER is more than 3 points higher than Roberta-Graph2Tree (p &lt; 0.001)<ref type="foot" target="#foot_6">9</ref> on MAWPS, and more than 2 points higher than BERT-Tree (p &lt; 0.005) on Math23k. The comparisons show that our deductive reasoner is robust across different languages and datasets of different sizes.</p><p>MathQA and SVAMP As mentioned before, MathQA and SVAMP are more challenging -the former consists of more complex questions and the latter consists of specifically designed challenging questions. Table <ref type="table" target="#tab_9">5 and 6</ref> show the performance comparisons. We are able to outperform the best baseline mBERT-LSTM<ref type="foot" target="#foot_7">10</ref> by 1.5 points in accuracy on MathQA. Different from other three datasets, the performance between different language models shows larger gaps on SVAMP. As we can see</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Model</head><p>Val Acc.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>S2S</head><p>GroupAttn <ref type="bibr" target="#b28">(Li et al., 2019)</ref> 21.5 BERT-BERT <ref type="bibr" target="#b24">(Lan et al., 2021)</ref> 24.8 Roberta-Roberta <ref type="bibr" target="#b24">(Lan et al., 2021)</ref> 30.3 S2T/G2T GTS * (Xie and Sun, 2019) 30.8 Graph2Tree <ref type="bibr" target="#b60">(Zhang et al., 2020)</ref> 36.5 BERT-Tree <ref type="bibr" target="#b30">(Li et al., 2021)</ref> 32.4 Roberta-GTS <ref type="bibr" target="#b40">(Patel et al., 2021)</ref> 41.0 Roberta-Graph2Tree <ref type="bibr" target="#b40">(Patel et al., 2021)</ref> 43  from baselines and our models, the choice of encoder appear to be important for solving questions in SVAMP -the results on using Roberta as the encoder are particularly striking. Our best variant ROBERTA-DEDUCTREASONER achieves an accuracy score of 47.3 and is able to outperfrom the best baseline (Roberta-Graph2Tree) by 3.5 points (p &lt; 0.01). By incorporating the constraints from our prior knowledge (as discussed in §3.3), we observe significant improvements for all variants -up to 7.0 points for our BERT-DEDUCTREASONER.</p><p>Overall, these results show that our model is more robust as compared to previous approaches on such challenging datasets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Fine-grained Analysis</head><p>We further perform finegrained performance analysis based on questions with different numbers of operations. Table <ref type="table" target="#tab_10">7</ref> shows the accuracy scores for questions that involve different numbers of operations. It also shows the equation accuracy on all datasets 11 . We compared our ROBERTA-DEDUCTREASONER with the best performing baselines in Table <ref type="table" target="#tab_4">3</ref> (Roberta-Graph2Tree), 4 (BERT-Tree), 5 (mBERT+LSTM) and 6 (Roberta-Graph2Tree).</p><p>On MAWPS and Math23k, our ROBERTA-DEDUCTREASONER model consistently yields higher results than baselines. On MathQA, our model also performs better on questions that involve 2, 3, and 4 operations. For the other more challenging dataset SVAMP, our model 11 Equ Acc: we regard an equation as correct if and only if it matches with the reference equation (up to reordering of sub-expressions due to commutative operations, namley "+" and "×"). has comparable performance with the baseline on 1-step questions, but achieves significantly better results (+14.3 points) on questions that involve 2 steps. Such comparisons on MathQA and SVAMP show that our model has a robust reasoning capability on more complex questions.</p><p>We observe that all models (including ours and existing models) are achieving much lower accuracy scores on SVAMP, as compared to other datasets. We further investigate the reason for this. <ref type="bibr" target="#b40">Patel et al. (2021)</ref> added irrelevant information such as extra quantities in the question to confuse the models. We quantify the effect by counting the percentage of instances which have quantities unused in the equations. As we can see in Table <ref type="table" target="#tab_11">8</ref>, SVAMP has the largest proportion (i.e., 44.5%) of instances whose gold equations do not fully utilize all the quantities in the problem text. The performance also significantly drops on those questions with more than one unused quantity on all datasets. The analysis suggests that our model still suffer from extra irrelevant information in the question and the performance is severely affected when such irrelevant information appears more frequently. <ref type="table" target="#tab_12">9</ref> shows the performance comparison with different rationalizers. As described in §3.2, the rationalizer is used to update the quantity representations at each step, so as to better "prepare them" for the subsequent reasoning process given the new context. We believe this step is crucial for achieving good performance, especially for complex MWP solving. As shown in Table 9, the performance drops by 7.3 points in value Question: Xiaoli and Xiaoqiang typed a manuscript together. Their typing speed ratio was 5:3. Xiaoli typed 1,400 more words than Xiaoqiang. How many words are there in this manuscript?</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Effect of Rationalizer Table</head><p>Gold Expr: accuracy for Math23k without rationalization, confirming the importance of rationalization in solving more complex problems that involve more steps. As most of the questions in MAWPS involve only 1-step questions, the significance of using rationalizer is not fully revealed on this dataset.</p><p>It can be seen that using self-attention achieves worse performance than the GRU unit. We believe the lower performance by using multi-head attention as rationalizer may be attributed to two reasons. First, GRU comes with sophisticated internal gating mechanisms, which may allow richer representations for the quantities. Second, attention, often interpreted as a mechanism for measuring similarities <ref type="bibr" target="#b18">(Katharopoulos et al., 2020)</ref>, may be inherently biased when being used for updating quantity representations. This is because when measuring the similarity between quantities and a specific expression (Figure <ref type="figure" target="#fig_4">4</ref>), those quantities that have just participated in the construction of the expression may receive a higher degree of similarity.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Case Studies</head><p>Explainability of Output Figure <ref type="figure" target="#fig_6">6</ref> presents an example prediction from Math23k. In this question, the gold deductive process first obtains the speed difference by "5 ÷ (5 + 3) − 3 ÷ (5 + 3)" and the final answer is 1400 divided by this difference. On the other hand, the predicted deductive process offers a slightly different understanding in speed difference. Assuming speed can be measured by some abstract "units", the predicted deductive  process first performs subtraction between 5 and 3, which gives us "2 units" of speed difference. Next, we can obtain the number of words associated with each speed unit (1400÷2). Finally, we can arrive at the total number of words by multiplying the number of words per unit (700) and the total number of units (8).</p><p><ref type="foot" target="#foot_8">12</ref> Through such an example we can see that our deductive reasoner is able to produce explainable steps to understand the answers.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Question Perturbation</head><p>The model predictions also give us guidance to understand the errors. Figure <ref type="figure" target="#fig_8">7</ref> shows how we can perturb a question given the error prediction (taken from Math23k). As we can see, the first step is incorrectly predicted with the "+" relation between 255 and 35. Because the first step involves the two quantities in the first two sentences, where we can locate the possible cause for the error. The gold step has a probability of 0.062 which is somewhat lower than the incorrect prediction. We believe that the second sentence (marked in red) may convey semantics that can be challenging for the model to digest, resulting in the incorrect prediction. Thus, we perturb the second sentence to make it semantically more straightforward (marked below in blue). The probability for the sub-expression 225 − 35 becomes higher after the purtubation, leading to a correct prediction (the "−" relation). Such an analysis demonstrates the strong interpretability of our deductive reasoner, and highlights the important connection between math word problem solving and reading comprehension, a topic that has been studied in educational psychology <ref type="bibr" target="#b51">(Vilenius-Tuohimaa et al., 2008)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Practical Issues</head><p>We discuss some practical issues with the current model in this section. Similar to most previous re-search efforts <ref type="bibr" target="#b28">(Li et al., 2019;</ref><ref type="bibr" target="#b58">Xie and Sun, 2019)</ref>, our work needs to maintain a list of constants (e.g., 1 and π) as additional candidate quantities. However, a large number of quantities could lead to a large search space of expressions (i.e., H). In practice, we could select some top-scoring quantities and build expressions on top of them <ref type="bibr" target="#b26">(Lee et al., 2018)</ref>. Another assumption of our model, as shown in Figure <ref type="figure" target="#fig_3">3</ref>, is that only binary operators are considered. Actually, extending it to support unary or ternary operators can be straightforward. Handling unary operators would require the introduction of some unary rules, and a ternary operator can be defined as a composition of two binary operators.</p><p>Our current model performs the greedy search in the training and inference process, which could be improved with a beam search process. One challenge with designing the beam search algorithm is that the search space H (t) is expanding at each step t (Equation <ref type="formula">6</ref>). We empirically found the model tends to favor outputs that involve fewer reasoning steps. In fact, better understanding the behavior and effect of beam search in seq2seq models remains an active research topic <ref type="bibr" target="#b8">(Cohen and Beck, 2019;</ref><ref type="bibr" target="#b21">Koehn and Knowles, 2017;</ref><ref type="bibr" target="#b15">Hokamp and Liu, 2017)</ref>, and we believe how to perform effective beam search in our setup could be an interesting research question that is worth exploring further.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusion and Future Work</head><p>We provide a new perspective to the task of MWP solving and argue that it can be fundamentally regarded as a complex relation extraction problem.</p><p>Based on this observation, and motivated by the deductive reasoning process, we propose an end-toend deductive reasoner to obtain the answer expression in a step-by-step manner. At each step, our model performs iterative mathematical relation extraction between quantities. Thorough experiments on four standard datasets demonstrate that our deductive reasoner is robust and able to yield new state-of-the-art performance. The model achieves particularly better performance for complex questions that involve a larger number of operations. It offers us the flexibility in interpreting the results, thanks to the deductive nature of our model. Future directions that we would like to explore include how to effectively incorporate commonsense knowledge into the deductive reasoning process, and how to facilitate counterfactual reasoning <ref type="bibr" target="#b43">(Richards and Sanderson, 1999)</ref>.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: A MWP example taken from MathQA. Top: tree generation. Bottom: deductive procedure.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: Our deductive system. t is the current step. ⟨⋅⟩ denotes the quantity list.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head></head><label></label><figDesc>2</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: Model architecture for the deductive reasoner.We show the inference procedure to obtain the expression "q 1 ÷ q 2 × q 3 " for the example question.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 4 :</head><label>4</label><figDesc>Figure 4: Rationalizing quantity representation.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 5 :</head><label>5</label><figDesc>Figure 5: Percentage of questions with different operation count.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 6 :</head><label>6</label><figDesc>Figure 6: Deductive steps by our reasoner.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Question:</head><label></label><figDesc>There are 255 apple trees in the orchard. Planting another 35 pear trees makes the number exactly the same as the apple trees. If every 20 pear trees are planted in a row, how many rows can be planted in total? Gold Expr: (255 − 35) ÷ 20 Answer: 11 Predicted Expr: (255 + 35) ÷ 20 Predicted: 14.5 Deductive Scores: 255 + 35 = 290 Prob.: 0.068 &gt; 255 − 35 = 220 Prob.: 0.062 Perturbed Question: There are 255 apple trees in the orchard. The number of pear trees is 35 fewer than the apple trees. If every 20 pear trees are planted in a row, how many rows can be planted in total? 255 + 35 = 290 Prob.: 0.061 &lt; 255 − 35 = 220 Prob.: 0.067</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Figure 7 :</head><label>7</label><figDesc>Figure 7: Question perturbation in deductive reasoning.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 2 :</head><label>2</label><figDesc>Dataset statistics. †: we follow Tan et al. (2021) to do preprocessing and obtain the subset.</figDesc><table><row><cell>Dataset</cell><cell cols="3">#Train #Valid #Test</cell><cell>Avg. Sent Len</cell><cell cols="2">#Const. Lang.</cell></row><row><cell>MAWPS</cell><cell cols="3">01,589 0,199 0,199</cell><cell>30.3</cell><cell>17</cell><cell>English</cell></row><row><cell>Math23k</cell><cell cols="3">21,162 1,000 1,000</cell><cell>26.6</cell><cell>02</cell><cell>Chinese</cell></row><row><cell cols="4">MathQA † 16,191 2,411 1,605</cell><cell>39.6</cell><cell>24</cell><cell>English</cell></row><row><cell>SVAMP</cell><cell>03,138</cell><cell>-</cell><cell>1,000</cell><cell>34.7</cell><cell>17</cell><cell>English</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 3 :</head><label>3</label><figDesc>5-fold cross-validation results on MAWPS.</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 5 :</head><label>5</label><figDesc>Test accuracy comparison on MathQA.</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head>Table 6 :</head><label>6</label><figDesc>Test accuracy comparison on SVAMP.</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_10"><head>Table 7 :</head><label>7</label><figDesc>Acc. under different number of operations.</figDesc><table><row><cell>#Operation</cell><cell cols="8">MAWPS Baseline OURS Baseline OURS Baseline OURS Baseline OURS Math23k MathQA SVAMP</cell></row><row><cell>1</cell><cell>88.2</cell><cell>92.7</cell><cell>91.3</cell><cell>93.6</cell><cell>77.3</cell><cell>77.4</cell><cell>51.9</cell><cell>52.0</cell></row><row><cell>2</cell><cell>91.3</cell><cell>91.6</cell><cell>89.3</cell><cell>92.0</cell><cell>81.3</cell><cell>83.5</cell><cell>17.8</cell><cell>32.1</cell></row><row><cell>3</cell><cell>-</cell><cell>-</cell><cell>74.5</cell><cell>77.0</cell><cell>81.9</cell><cell>83.4</cell><cell>-</cell><cell>-</cell></row><row><cell>4</cell><cell>-</cell><cell>-</cell><cell>59.1</cell><cell>60.3</cell><cell>79.3</cell><cell>81.7</cell><cell>-</cell><cell>-</cell></row><row><cell>&gt;=5</cell><cell>-</cell><cell>-</cell><cell>56.5</cell><cell>69.2</cell><cell>71.5</cell><cell>71.4</cell><cell>-</cell><cell>-</cell></row><row><cell cols="2">Overall Performance</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Equ Acc.</cell><cell>80.8</cell><cell>88.6</cell><cell>71.2</cell><cell>79.0</cell><cell>74.0</cell><cell>74.0</cell><cell>40.9</cell><cell>45.0</cell></row><row><cell>Val Acc.</cell><cell>88.7</cell><cell>92.0</cell><cell>82.4</cell><cell>85.1</cell><cell>77.1</cell><cell>78.6</cell><cell>43.8</cell><cell>47.3</cell></row><row><cell></cell><cell></cell><cell></cell><cell cols="6">MAWPS Math23k MathQA SVAMP</cell></row><row><cell>Unused</cell><cell></cell><cell></cell><cell>6.5%</cell><cell cols="2">8.2%</cell><cell>20.7%</cell><cell cols="2">44.5%</cell></row><row><cell cols="3">Accuracy (unused = 0)</cell><cell>93.6</cell><cell cols="2">87.1</cell><cell>81.4</cell><cell cols="2">63.6</cell></row><row><cell cols="3">Accuracy (unused ≥ 1)</cell><cell>100.0 †</cell><cell cols="2">62.1</cell><cell>67.4</cell><cell cols="2">27.0</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_11"><head>Table 8 :</head><label>8</label><figDesc>Value accuracy with respect to the number of unused quantities. The second row shows the percentage of instances that have unused quantities. †: may not be representative as there are only 3 instances.</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_12"><head>Table 9 :</head><label>9</label><figDesc>Performance comparison on different rationalizer using the Roberta-base model.</figDesc><table><row><cell>Rationalizer</cell><cell cols="4">MAWPS Equ Acc. Val Acc. Equ Acc. Val Acc. Math23k</cell></row><row><cell>NONE</cell><cell>88.4</cell><cell>91.8</cell><cell>71.5</cell><cell>77.8</cell></row><row><cell>Self-Attention</cell><cell>88.3</cell><cell>91.7</cell><cell>77.5</cell><cell>84.8</cell></row><row><cell>GRU unit</cell><cell>88.6</cell><cell>92.0</cell><cell>79.0</cell><cell>85.1</cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_0">While we consider binary operators, extending our approach to support unary or ternary operators is possible ( §4.3).</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3" xml:id="foot_1">See the supplementary material for more details on this.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4" xml:id="foot_2">Actually, one might have noticed that this loss comes with a trivial solution at θ = 0. In practice, however, our model and training process would prevent us from reaching such a degenerate solution with proper initialization<ref type="bibr" target="#b13">(Goodfellow et al., 2016)</ref>. This is similar to the training of a structured perceptron<ref type="bibr" target="#b9">(Collins, 2002)</ref>, where a similar situation is also involved.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="5" xml:id="foot_3">The original MathQA<ref type="bibr" target="#b0">(Amini et al., 2019)</ref> dataset contains a certain number of instances that have annotated equations which cannot lead to the correct numerical</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="6" xml:id="foot_4">answer.6  Our dataset size is not exactly the same as<ref type="bibr" target="#b49">Tan et al. (2021)</ref> as they included some instances that are wrongly annotated. We only kept the part that has correct annotations. We con-</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="8" xml:id="foot_5">All previous efforts combine training/dev/test sets and perform 5-fold cross validation, which we follow.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="9" xml:id="foot_6">We conduct bootstrapping t-test to compare the results.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="10" xml:id="foot_7">We ran the their code on our adapted MathQA dataset.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="12" xml:id="foot_8">Interestingly, when we presented this question to 3 human solvers, 2 of them used the first approach and 1 of them arrived at the second approach.</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgements</head><p>We would like to thank the anonymous reviewers and our ARR action editor for their constructive comments, and Hang Li for helpful discussions and comments on this work. This work was done when Jierui Li was working as a research assistant at SUTD, and when Wei Lu was serving as a consultant at ByteDance AI Lab.</p></div>
			</div>

			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A Importance of Rationalizer</head><p>We further elaborate on the importance of the rationalizer module in this section. As mentioned in §3.2, it is crucial for us to properly update the quantity representations, especially for questions that require more than 3 operations to solve (i.e., t ≥ 3). Because if the quantity representations do not get updated as we continue the deductive reasoning process, those expressions that were initially highly ranked (say, at the first step) would always be preferred over those lowly ranked ones throughout the process.</p><p>We provide an example here to illustrate the scenario. Suppose our target expression is (1 + 2) * (3 + 4), the first step is to predict:</p><p>In order to obtain the correct intermediate expression 1 + 2 as e</p><p>( 1) , the model has to give the highest score to this expression. Note that, the score of the expression e 1,2,+ also has to be larger than the score of e 3,4,+ .</p><p>s(e</p><p>(1)</p><p>3,4,+ )</p><p>However, in order to reach the final target expression, in the next step, the model needs to construct the intermediate expression 3 + 4. Without the rationalizer, the representations for the quantities are unchanged, so we would have:</p><p>1,2,+ ) = s(e (9) From here we could see that the model would not be able to produce the intermediate expression 3 + 4 in the second step (but would still prefer to generate another 1 + 2). With the rationalizer in place, the above two equations in Equation 9 in general may not hold, which effectively prevents such an issue from happening.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B Additional Implementation Details</head><p>We implement our model with PyTorch and run all experiments using Tesla V100 GPU. The feedforward network in our model is simply linear transformation followed by the ReLU activation. We also apply layer normalization and dropout in the feed-forward network. The hidden size in the feedforward network is 768, which the is same as the hidden size used in BERT/Roberta.</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Mathqa: Towards interpretable math word problem solving with operation-based formalisms</title>
		<author>
			<persName><forename type="first">Aida</forename><surname>Amini</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Saadia</forename><surname>Gabriel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shanchuan</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rik</forename><surname>Koncel-Kedziorski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yejin</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hannaneh</forename><surname>Hajishirzi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of NAACL</title>
				<meeting>NAACL</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Neural module networks</title>
		<author>
			<persName><forename type="first">Jacob</forename><surname>Andreas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Marcus</forename><surname>Rohrbach</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Trevor</forename><surname>Darrell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dan</forename><surname>Klein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of CVPR</title>
				<meeting>CVPR</meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Deep learning for ai</title>
		<author>
			<persName><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yann</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Geoffrey</forename><surname>Hinton</surname></persName>
		</author>
		<idno type="DOI">10.1145/3448250</idno>
	</analytic>
	<monogr>
		<title level="j">Communications of the ACM</title>
		<imprint>
			<biblScope unit="volume">64</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="58" to="65" />
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<author>
			<persName><forename type="first">Artur D'avila</forename><surname>Tarek R Besold</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sebastian</forename><surname>Garcez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Howard</forename><surname>Bader</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pedro</forename><surname>Bowman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pascal</forename><surname>Domingos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kai-Uwe</forename><surname>Hitzler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Luis</forename><forename type="middle">C</forename><surname>Kühnberger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Daniel</forename><surname>Lamb</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Priscila</forename><forename type="middle">Machado</forename><surname>Lowd</surname></persName>
		</author>
		<author>
			<persName><surname>Vieira Lima</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1711.03902</idno>
		<title level="m">Neuralsymbolic learning and reasoning: A survey and interpretation</title>
				<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">Natural language input for a computer problem solving system</title>
		<author>
			<persName><forename type="first">G</forename><surname>Daniel</surname></persName>
		</author>
		<author>
			<persName><surname>Bobrow</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1964">1964</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">A bottom-up dag structure extraction model for math word problems</title>
		<author>
			<persName><forename type="first">Yixuan</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Feng</forename><surname>Hong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hongwei</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ping</forename><surname>Luo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of AAAI</title>
				<meeting>AAAI</meeting>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Semantically-aligned equation generation for solving and reasoning math word problems</title>
		<author>
			<persName><forename type="first">Ting-Rui</forename><surname>Chiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yun-Nung</forename><surname>Chen</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/N19-1272</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of NAACL</title>
				<meeting>NAACL</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">On the properties of neural machine translation: Encoder-decoder approaches</title>
		<author>
			<persName><forename type="first">Kyunghyun</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bart</forename><surname>Van Merriënboer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dzmitry</forename><surname>Bahdanau</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of SSST-8, Eighth Workshop on Syntax, Semantics and Structure in Statistical Translation</title>
				<meeting>SSST-8, Eighth Workshop on Syntax, Semantics and Structure in Statistical Translation</meeting>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Empirical analysis of beam search performance degradation in neural sequence models</title>
		<author>
			<persName><forename type="first">Eldan</forename><surname>Cohen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christopher</forename><surname>Beck</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ICML</title>
				<meeting>ICML</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Discriminative training methods for hidden markov models: Theory and experiments with perceptron algorithms</title>
		<author>
			<persName><forename type="first">Michael</forename><surname>Collins</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of EMNLP</title>
				<meeting>EMNLP</meeting>
		<imprint>
			<date type="published" when="2002">2002</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Edouard Grave Myle Ott Luke Zettlemoyer, and Veselin Stoyanov</title>
		<author>
			<persName><forename type="first">Alexis</forename><surname>Conneau</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kartikay</forename><surname>Khandelwal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Naman</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vishrav</forename><surname>Chaudhary</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Guillaume</forename><surname>Wenzek</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Francisco</forename><surname>Guzmán</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ACL</title>
				<meeting>ACL</meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note>Unsupervised cross-lingual representation learning at scale</note>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<author>
			<persName><forename type="first">Yiming</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wanxiang</forename><surname>Che</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ting</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bing</forename><surname>Qin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ziqing</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shijin</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Guoping</forename><surname>Hu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1906.08101</idno>
		<title level="m">Pre-training with whole word masking for chinese bert</title>
				<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Bert: Pre-training of deep bidirectional transformers for language understanding</title>
		<author>
			<persName><forename type="first">Jacob</forename><surname>Devlin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ming-Wei</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kenton</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kristina</forename><surname>Toutanova</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of NAACL</title>
				<meeting>NAACL</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">Deep learning</title>
		<author>
			<persName><forename type="first">Ian</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aaron</forename><surname>Courville</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016">2016</date>
			<publisher>MIT press</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Neural module networks for reasoning over text</title>
		<author>
			<persName><forename type="first">Nitish</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kevin</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dan</forename><surname>Roth</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sameer</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Matt</forename><surname>Gardner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ICLR</title>
				<meeting>ICLR</meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Lexically constrained decoding for sequence generation using grid beam search</title>
		<author>
			<persName><forename type="first">Chris</forename><surname>Hokamp</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qun</forename><surname>Liu</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/P17-1141</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ACL</title>
				<meeting>ACL</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Neural math word problem solver with reinforcement learning</title>
		<author>
			<persName><forename type="first">Danqing</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jing</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chin-Yew</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jian</forename><surname>Yin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of COLING</title>
				<meeting>COLING</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">Thinking, fast and slow</title>
		<author>
			<persName><forename type="first">Daniel</forename><surname>Kahneman</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2011">2011</date>
			<publisher>Macmillan</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Transformers are rnns: Fast autoregressive transformers with linear attention</title>
		<author>
			<persName><forename type="first">Angelos</forename><surname>Katharopoulos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Apoorv</forename><surname>Vyas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ICML</title>
				<meeting>ICML</meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note>Nikolaos Pappas, and François Fleuret</note>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<author>
			<persName><forename type="first">P</forename><surname>Diederik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jimmy</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName><surname>Ba</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1412.6980</idno>
		<title level="m">Adam: A method for stochastic optimization</title>
				<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Semisupervised classification with graph convolutional networks</title>
		<author>
			<persName><forename type="first">N</forename><surname>Thomas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Max</forename><surname>Kipf</surname></persName>
		</author>
		<author>
			<persName><surname>Welling</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ICLR</title>
				<meeting>ICLR</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Six challenges for neural machine translation</title>
		<author>
			<persName><forename type="first">Philipp</forename><surname>Koehn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rebecca</forename><surname>Knowles</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the First Workshop on Neural Machine Translation, ACL</title>
				<meeting>the First Workshop on Neural Machine Translation, ACL</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Mawps: A math word problem repository</title>
		<author>
			<persName><forename type="first">Rik</forename><surname>Koncel-Kedziorski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Subhro</forename><surname>Roy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aida</forename><surname>Amini</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nate</forename><surname>Kushman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hannaneh</forename><surname>Hajishirzi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of NAACL</title>
				<meeting>NAACL</meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Learning to automatically solve algebra word problems</title>
		<author>
			<persName><forename type="first">Nate</forename><surname>Kushman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yoav</forename><surname>Artzi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Luke</forename><surname>Zettlemoyer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Regina</forename><surname>Barzilay</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ACL</title>
				<meeting>ACL</meeting>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title level="m" type="main">Mwptoolkit: An open-source framework for deep learning-based math word problem solvers</title>
		<author>
			<persName><forename type="first">Yihuai</forename><surname>Lan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lei</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qiyuan</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yunshi</forename><surname>Lan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bing</forename><forename type="middle">Tian</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dongxiang</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ee-Peng</forename><surname>Lim</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2109.00799</idno>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">End-to-end neural coreference resolution</title>
		<author>
			<persName><forename type="first">Kenton</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Luheng</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mike</forename><surname>Lewis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Luke</forename><surname>Zettlemoyer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of EMNLP</title>
				<meeting>EMNLP</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Higher-order coreference resolution with coarse-tofine inference</title>
		<author>
			<persName><forename type="first">Kenton</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Luheng</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Luke</forename><surname>Zettlemoyer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of NAACL</title>
				<meeting>NAACL</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Rationalizing neural predictions</title>
		<author>
			<persName><forename type="first">Tao</forename><surname>Lei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Regina</forename><surname>Barzilay</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tommi</forename><surname>Jaakkola</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of EMNLP</title>
				<meeting>EMNLP</meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Modeling intra-relation in math word problems with different functional multi-head attentions</title>
		<author>
			<persName><forename type="first">Jierui</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lei</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jipeng</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bing</forename><forename type="middle">Tian</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dongxiang</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the ACL</title>
				<meeting>the ACL</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Graph-totree neural networks for learning structured inputoutput translation with applications to semantic parsing and math word problem</title>
		<author>
			<persName><forename type="first">Shucheng</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lingfei</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shiwei</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fangli</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fengyuan</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sheng</forename><surname>Zhong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of Findings of EMNLP</title>
				<meeting>Findings of EMNLP</meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
		<title level="m" type="main">Seeking patterns, not just memorizing procedures: Contrastive learning for solving math word problems</title>
		<author>
			<persName><forename type="first">Zhongli</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wenxuan</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chao</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qingyu</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chao</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hongzhi</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yunbo</forename><surname>Cao</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2110.08464</idno>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">A meaning-based statistical english math word problem solver</title>
		<author>
			<persName><surname>Chao-Chun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yu-Shiang</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yi-Chung</forename><surname>Wong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Keh-Yih</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><surname>Su</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of NAACL</title>
				<meeting>NAACL</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
		<author>
			<persName><forename type="first">Zhenwen</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jipeng</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jie</forename><surname>Shao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiangliang</forename><surname>Zhang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2107.13435</idno>
		<title level="m">Mwp-bert: A strong baseline for math word problems</title>
				<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Modeling math word problems with augmented semantic networks</title>
		<author>
			<persName><forename type="first">Christian</forename><surname>Liguda</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Thies</forename><surname>Pfeiffer</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-642-31178-9_29.pdf</idno>
	</analytic>
	<monogr>
		<title level="m">International Conference on Application of Natural Language to Information Systems</title>
				<imprint>
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Hms: A hierarchical solver with dependency-enhanced understanding for math word problem</title>
		<author>
			<persName><forename type="first">Xin</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhenya</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hongke</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Enhong</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qi</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hao</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shijin</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of AAAI</title>
				<meeting>AAAI</meeting>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Program induction by rationale generation: Learning to solve and explain algebraic word problems</title>
		<author>
			<persName><forename type="first">Wang</forename><surname>Ling</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dani</forename><surname>Yogatama</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chris</forename><surname>Dyer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Phil</forename><surname>Blunsom</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ACL</title>
				<meeting>ACL</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<monogr>
		<author>
			<persName><forename type="first">Yinhan</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Myle</forename><surname>Ott</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Naman</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jingfei</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mandar</forename><surname>Joshi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Danqi</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Omer</forename><surname>Levy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mike</forename><surname>Lewis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Luke</forename><surname>Zettlemoyer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Veselin</forename><surname>Stoyanov</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1907.11692</idno>
		<title level="m">Roberta: A robustly optimized bert pretraining approach</title>
				<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Effective approaches to attentionbased neural machine translation</title>
		<author>
			<persName><forename type="first">Ilya</forename><surname>Loshchilov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Frank</forename><surname>Hutter</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Minh-Thang</forename><surname>Luong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hieu</forename><surname>Pham</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christopher</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of EMNLP</title>
				<meeting>EMNLP</meeting>
		<imprint>
			<date type="published" when="2015">2019. 2015</date>
		</imprint>
	</monogr>
	<note>Proceedings of ICLR</note>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">A review of methods for automatic understanding of natural language mathematical problems</title>
		<author>
			<persName><forename type="first">Anirban</forename><surname>Mukherjee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Utpal</forename><surname>Garain</surname></persName>
		</author>
		<idno type="DOI">10.1007/s10462-009-9110-0</idno>
	</analytic>
	<monogr>
		<title level="j">Artificial Intelligence Review</title>
		<imprint>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="93" to="122" />
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Weighted deductive parsing and knuth&apos;s algorithm</title>
		<author>
			<persName><forename type="first">Mark-Jan</forename><surname>Nederhof</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computational Linguistics</title>
		<imprint>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="135" to="143" />
			<date type="published" when="2003">2003</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Are NLP models really able to solve simple math word problems?</title>
		<author>
			<persName><forename type="first">Arkil</forename><surname>Patel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Satwik</forename><surname>Bhattamishra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Navin</forename><surname>Goyal</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of NAACL</title>
				<meeting>NAACL</meeting>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<monogr>
		<title level="m" type="main">Child&apos;s Conception of Number</title>
		<author>
			<persName><forename type="first">Jean</forename><surname>Piaget</surname></persName>
		</author>
		<idno type="DOI">10.4324/9781315006222</idno>
		<imprint>
			<date type="published" when="1952">1952</date>
			<publisher>Routledge</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Neural-symbolic solver for math word problems with auxiliary tasks</title>
		<author>
			<persName><forename type="first">Jinghui</forename><surname>Qin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaodan</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yining</forename><surname>Hong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jianheng</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Liang</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ACL-IJCNLP</title>
				<meeting>ACL-IJCNLP</meeting>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">The role of imagination in facilitating deductive reasoning in 2-, 3-and 4-year-olds</title>
		<author>
			<persName><forename type="first">Cassandra</forename><forename type="middle">A</forename><surname>Richards</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jennifer</forename><forename type="middle">A</forename><surname>Sanderson</surname></persName>
		</author>
		<idno type="DOI">10.1016/s0010-0277(99)00037-2</idno>
	</analytic>
	<monogr>
		<title level="j">Cognition</title>
		<imprint>
			<biblScope unit="volume">72</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="B1" to="B9" />
			<date type="published" when="1999">1999</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Solving general arithmetic word problems</title>
		<author>
			<persName><forename type="first">Subhro</forename><surname>Roy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dan</forename><surname>Roth</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of EMNLP</title>
				<meeting>EMNLP</meeting>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Mapping to declarative knowledge for word problem solving</title>
		<author>
			<persName><forename type="first">Subhro</forename><surname>Roy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dan</forename><surname>Roth</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Transactions of the Association for Computational Linguistics</title>
				<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page" from="159" to="172" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Generate &amp; rank: A multi-task framework for math word problems</title>
		<author>
			<persName><forename type="first">Jianhao</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yichun</forename><surname>Yin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lin</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lifeng</forename><surname>Shang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xin</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ming</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qun</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of Findings of EMNLP</title>
				<meeting>Findings of EMNLP</meeting>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Solving math word problems with multi-encoders and multi-decoders</title>
		<author>
			<persName><forename type="first">Yibin</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Cheqing</forename><surname>Jin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of COLING</title>
				<meeting>COLING</meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Principles and implementation of deductive parsing</title>
		<author>
			<persName><forename type="first">Yves</forename><surname>Stuart M Shieber</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fernando Cn</forename><surname>Schabes</surname></persName>
		</author>
		<author>
			<persName><surname>Pereira</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The Journal of logic programming</title>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="issue">1-2</biblScope>
			<biblScope unit="page" from="3" to="36" />
			<date type="published" when="1995">1995</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<monogr>
		<title level="m" type="main">Investigating math word problems using pretrained multilingual language models</title>
		<author>
			<persName><forename type="first">Minghuan</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lei</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lingxiao</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jing</forename><surname>Jiang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2105.08928</idno>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Attention is all you need</title>
		<author>
			<persName><forename type="first">Ashish</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Noam</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Niki</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jakob</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Llion</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aidan</forename><forename type="middle">N</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Łukasz</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Illia</forename><surname>Polosukhin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of NeurIPS</title>
				<meeting>NeurIPS</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">The association between mathematical word problems and reading comprehension</title>
		<author>
			<persName><forename type="first">Maria</forename><surname>Piia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kaisa</forename><surname>Vilenius-Tuohimaa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jari-Erik</forename><surname>Aunola</surname></persName>
		</author>
		<author>
			<persName><surname>Nurmi</surname></persName>
		</author>
		<idno type="DOI">10.1080/01443410701708228</idno>
	</analytic>
	<monogr>
		<title level="j">Educational Psychology</title>
		<imprint>
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="409" to="426" />
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">Template-based math word problem solvers with recursive neural networks</title>
		<author>
			<persName><forename type="first">Lei</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dongxiang</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jipeng</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xing</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lianli</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bing</forename><forename type="middle">Tian</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Heng Tao</forename><surname>Shen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of AAAI</title>
				<meeting>AAAI</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">Deep neural solver for math word problems</title>
		<author>
			<persName><forename type="first">Yan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaojiang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shuming</forename><surname>Shi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of EMNLP</title>
				<meeting>EMNLP</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">A learning algorithm for continually running fully recurrent neural networks</title>
		<author>
			<persName><forename type="first">J</forename><surname>Ronald</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><surname>Williams</surname></persName>
		</author>
		<author>
			<persName><surname>Zipser</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural computation</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="270" to="280" />
			<date type="published" when="1989">1989</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<analytic>
		<title level="a" type="main">Transformers: State-of-theart natural language processing</title>
		<author>
			<persName><forename type="first">Thomas</forename><surname>Wolf</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Julien</forename><surname>Chaumond</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lysandre</forename><surname>Debut</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Victor</forename><surname>Sanh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Clement</forename><surname>Delangue</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Anthony</forename><surname>Moi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pierric</forename><surname>Cistac</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Morgan</forename><surname>Funtowicz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Joe</forename><surname>Davison</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sam</forename><surname>Shleifer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of EMNLP: System Demonstrations</title>
				<meeting>EMNLP: System Demonstrations</meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<analytic>
		<title level="a" type="main">A knowledge-aware sequence-to-tree network for math word problem solving</title>
		<author>
			<persName><forename type="first">Qinzhuo</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qi</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jinlan</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xuan-Jing</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of EMNLP</title>
				<meeting>EMNLP</meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<analytic>
		<title level="a" type="main">Math word problem solving with explicit numerical values</title>
		<author>
			<persName><forename type="first">Qinzhuo</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qi</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhongyu</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xuan-Jing</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ACL-IJCNLP</title>
				<meeting>ACL-IJCNLP</meeting>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b58">
	<analytic>
		<title level="a" type="main">A goal-driven tree-structured neural model for math word problems</title>
		<author>
			<persName><forename type="first">Zhipeng</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shichao</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of IJCAI</title>
				<meeting>IJCAI</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b59">
	<analytic>
		<title level="a" type="main">Kernel methods for relation extraction</title>
		<author>
			<persName><forename type="first">Dmitry</forename><surname>Zelenko</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chinatsu</forename><surname>Aone</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Anthony</forename><surname>Richardella</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of machine learning research</title>
		<imprint>
			<biblScope unit="page">3</biblScope>
			<date type="published" when="2003-02">2003. Feb</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b60">
	<analytic>
		<title level="a" type="main">Graph-totree learning for solving math word problems</title>
		<author>
			<persName><forename type="first">Jipeng</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lei</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Roy</forename></persName>
		</author>
		<author>
			<persName><forename type="first">Ka-Wei</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yi</forename><surname>Bin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jie</forename><surname>Shao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ee-Peng</forename><surname>Lim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ACL</title>
				<meeting>ACL</meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b61">
	<analytic>
		<title level="a" type="main">A frustratingly easy approach for entity and relation extraction</title>
		<author>
			<persName><forename type="first">Zexuan</forename><surname>Zhong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Danqi</forename><surname>Chen</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2021.naacl-main.5</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of NAACL</title>
				<meeting>NAACL</meeting>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
