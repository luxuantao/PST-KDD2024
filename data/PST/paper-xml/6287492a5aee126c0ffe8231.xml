<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Pre-training and Fine-tuning Neural Topic Model: A Simple yet Effective Approach to Incorporating External Knowledge</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Linhai</forename><surname>Zhang</surname></persName>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">School of Computer Science and Engineering</orgName>
								<orgName type="department" key="dep2">Ministry of Education</orgName>
								<orgName type="laboratory">Key Laboratory of Computer Network and Information Integration</orgName>
								<orgName type="institution">Southeast University</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Xumeng</forename><surname>Hu</surname></persName>
							<email>xuemenghu@seu.edu.cn</email>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">School of Computer Science and Engineering</orgName>
								<orgName type="department" key="dep2">Ministry of Education</orgName>
								<orgName type="laboratory">Key Laboratory of Computer Network and Information Integration</orgName>
								<orgName type="institution">Southeast University</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Boyu</forename><surname>Wang</surname></persName>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">School of Computer Science and Engineering</orgName>
								<orgName type="department" key="dep2">Ministry of Education</orgName>
								<orgName type="laboratory">Key Laboratory of Computer Network and Information Integration</orgName>
								<orgName type="institution">Southeast University</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Deyu</forename><surname>Zhou</surname></persName>
							<email>d.zhou@seu.edu.cn</email>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">School of Computer Science and Engineering</orgName>
								<orgName type="department" key="dep2">Ministry of Education</orgName>
								<orgName type="laboratory">Key Laboratory of Computer Network and Information Integration</orgName>
								<orgName type="institution">Southeast University</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Qian-Wen</forename><surname>Zhang</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution">Tencent Could Xiaowei</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Yunbo</forename><surname>Cao</surname></persName>
							<email>yunbocao@tencent.com</email>
							<affiliation key="aff1">
								<orgName type="institution">Tencent Could Xiaowei</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Pre-training and Fine-tuning Neural Topic Model: A Simple yet Effective Approach to Incorporating External Knowledge</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.2" ident="GROBID" when="2023-01-01T13:40+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Recent years have witnessed growing interests in incorporating external knowledge such as pre-trained word embeddings (PWEs) or pretrained language models (PLMs) into neural topic modeling. However, we found that employing PWEs and PLMs for topic modeling only achieved limited performance improvements but with huge computational overhead. In this paper, we propose a novel strategy to incorporate external knowledge into neural topic modeling where the neural topic model is pretrained on a large corpus and then fine-tuned on the target dataset. Experiments have been conducted on three datasets and results show that the proposed approach significantly outperforms both current state-of-the-art neural topic models and some topic modeling approaches enhanced with PWEs or PLMs. Moreover, further study shows that the proposed approach greatly reduces the need for the huge size of training data.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Topic models have been widely used for discovering hidden themes from a large collection of documents in an unsupervised manner. Recently, to avoid the complex and specific inference process of graph model-based method such as LDA <ref type="bibr" target="#b4">(Blei et al., 2003)</ref>, neural topic modeling that utilizes neural-network-based black-box inference has been the main research direction in this field <ref type="bibr" target="#b3">(Blei, 2012;</ref><ref type="bibr" target="#b21">Miao et al., 2016;</ref><ref type="bibr" target="#b31">Srivastava and Sutton, 2017)</ref>. Typically, neural topic models infer topics of a document by utilizing its bag-of-words (BoWs) representation to capture word co-occurrence patterns. The BoWs representation, however, fails to encode rich word semantics, leading to relatively inferior quality of topics generated by the topic models. Therefore, approaches have been proposed to address the limitation of BoWs representation by incorporating the external knowledge, such as pre-trained word embeddings (PWEs) <ref type="bibr" target="#b7">(Das et al., 2015;</ref><ref type="bibr" target="#b34">Wang et al., 2020;</ref><ref type="bibr" target="#b9">Dieng et al., 2020)</ref>.</p><p>In recent years, pre-trained language models (PLMs) <ref type="bibr" target="#b26">(Peters et al., 2018;</ref><ref type="bibr" target="#b8">Devlin et al., 2019;</ref><ref type="bibr" target="#b5">Brown et al., 2020)</ref> have achieved state-of-the-art performance on a wide range of natural language processing tasks. Different from PWEs<ref type="foot" target="#foot_0">1</ref> in which a word is mapped to a static word emebdding, PLMs generate a specific word embedding for each occurrence of a word depending on the context. It is appealing to incorporate PLMs into topic models since contextualized embeddings generated by PLMs encode richer semantics and naturally deal with word polysemy <ref type="bibr" target="#b24">(Pasini et al., 2020)</ref>. One straightforward way is to replace BoWs representation with the outputs of PLM <ref type="bibr" target="#b2">(Bianchi et al., 2020b)</ref> in existing topic models or take PLM outputs as additional inputs to topic modeling <ref type="bibr" target="#b1">(Bianchi et al., 2020a)</ref>. A more sophisticated approach is to distill the knowledge of a PLM into a topic model. For example, <ref type="bibr" target="#b13">(Hoyle et al., 2020)</ref> employed the probability estimates of a teacher PLM over a text sequence to guide the training of a student topic model.</p><p>However, the approaches mentioned above still have limitations. Firstly, using PLMs for topic model training in such ways leads to huge computational overhead. Most neural topic models are based on shallow multi-layer perceptions with few hidden units. However, most popular PLMs are based on deep Transformers <ref type="bibr" target="#b33">(Vaswani et al., 2017)</ref> where at each layer expensive self-attention operations are performed, which have a time complexity quadratic in document length. Therefore, the overall training time is dominated by PLM, and it will be worse if PLM is further fine-tuned, as shown in <ref type="bibr" target="#b13">(Hoyle et al., 2020)</ref>. Secondly, there is the gap of training objectives between PLMs and topic models, where PLMs are trained to learn the semantic and syntactic knowledge within a sentence while topic models focus on extracting main themes over whole corpus. As shown in Table <ref type="table">4</ref>, a model based on GloVe embeddings <ref type="bibr" target="#b25">(Pennington et al., 2014)</ref> performs better than PLMs-based models such as those proposed in <ref type="bibr" target="#b1">(Bianchi et al., 2020a)</ref> and <ref type="bibr" target="#b2">(Bianchi et al., 2020b)</ref>.</p><p>To overcome these challenges, we propose a simple yet effective strategy, namely Pre-trained Neural Topic Model (PT-NTM), to utilize extensive knowledge from large corpora for neural topic modeling with low computational complexity. Instead of pre-training the embeddings and acquiring knowledge indirectly, PT-NTM directly pre-trains the topic model itself on the knowledge source corpora. In specific, a neural topic model is firstly trained on a large corpus only once, which is called pre-training. Afterward, it is fine-tuned on any other dataset, which is called fine-tuning. As the architecture of the neural topic model used in pretraining and fine-tuning is the same, it incurs little computational overhead to any subsequent training. Experiments have been conducted on three datasets and the results show that the proposed approach significantly outperforms not only some state-ofthe-art neural topic models but also the topic modeling approaches using PWEs and PLMs. Moreover, it is observed that on the NYTimes dataset, the neural topic model trained on 1% of the whole dataset using the proposed approach achieves superior performance than other baseline models that are trained on the whole dataset. It further shows that the proposed approach greatly reduces the need for the huge size of training data.</p><p>The main contributions are:</p><p>• We proposed a simple yet effective strategy for training neural topic models in which the models are pre-trained on a large corpus and then fine-tuned on a specific dataset.</p><p>• We conducted extensive experiments and the results show that the pre-trained neural topic models significantly outperform baselines in terms of topic coherence and topic diversity.</p><p>• The proposed approach greatly reduces the amount of training data needed. In our experiments on the NYTimes dataset, a pretrained model fine-tuned with 1% of documents achieves superior performance than baselines that are trained on the whole dataset.</p><p>2 Related Work</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Neural Topic Modeling</head><p>Due to the flexible modeling choices and high representation capacity, neural networks have been widely used for topic modeling in recent years. Some approaches <ref type="bibr" target="#b17">(Kingma and Welling, 2013;</ref><ref type="bibr" target="#b21">Miao et al., 2016)</ref> model topics with variational autoencoders (VAEs) and view the latent variables of VAEs as document topics. However, topic models typically use Dirichlet distribution as the prior of multinomial topic distributions, while the reparameterization trick required by VAEs hinders the usage of a Dirichlet prior. Therefore, some followup works <ref type="bibr" target="#b31">(Srivastava and Sutton, 2017;</ref><ref type="bibr" target="#b6">Card et al., 2018)</ref> used logistic normal to approximate Dirichlet. Another family of neural topic models <ref type="bibr" target="#b23">(Nan et al., 2019;</ref><ref type="bibr" target="#b34">Wang et al., 2020;</ref><ref type="bibr" target="#b14">Hu et al., 2020)</ref> overcome the problem with adversarial training <ref type="bibr" target="#b11">(Goodfellow et al., 2014</ref>) by encouraging the model to generate topic distributions that are similar to samples randomly drawn from a Dirichlet prior.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Topic Modeling with External Knowledge</head><p>There are mainly two ways to incorporate external knowledge into topic modeling, namely by PWEs and PLMs. Some attempts incorporate pre-trained word representations into neural topic models. For example, <ref type="bibr" target="#b6">(Card et al., 2018;</ref><ref type="bibr" target="#b9">Dieng et al., 2020)</ref> used PWEs to initialize word embeddings of topic models. <ref type="bibr" target="#b34">(Wang et al., 2020)</ref> built a generative process that models word embeddings with per-topic Gaussian distributions.</p><p>Beyond static word embeddings, researchers also tried to utilize PLMs. <ref type="bibr">(Bianchi et al., 2020b,a)</ref> treated PLM outputs as an additional knowledge source to enhance or replace BoW-based inputs. <ref type="bibr" target="#b13">(Hoyle et al., 2020)</ref> employed knowledge distillation to guide the training of a student topic model with a PLM teacher network. Recently, <ref type="bibr" target="#b30">(Song et al., 2020)</ref> proposed TopicOcean to train LDA-based topic models on large corpora and then transfer the knowledge of accumulated topics to new corpora which can also be considered a way of pre-training.</p><p>It should be pointed out that the proposed PT-NTM differs from the previous PLMs-based topic models or TopicOcean in that the architecture of neural topic models during pre-training and finetuning are the same in PT-NTM while other methods combine the large PLM with the topic models, the two different model architectures.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Methodology</head><p>In this section, we describe the detailed processes of PT-NTM. First, we will introduce the architecture of neural topic model, which we call NTM in the following, employed in PT-NTM. Then, we will introduce how to pre-train the neural topic model on a large-scale dataset. Finally, we will introduce how to fine-tune the pre-trained neural topic model on the target dataset.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Neural Topic Model Architecture</head><p>For the architecture of NTM, we follow the encoder-decoder architecture, as employed by many neural topic models <ref type="bibr" target="#b31">(Srivastava and Sutton, 2017;</ref><ref type="bibr" target="#b20">Miao et al., 2017;</ref><ref type="bibr" target="#b23">Nan et al., 2019)</ref>. The encoder takes a document's BoW x ∈ R V as input and infers its topic distribution ẑ ∈ R K , where V is the vocabulary size and K the topic number. The decoder then reconstructs the original document from ẑ, denoted as x.</p><p>The whole architecture of NTM is shown in Figure 1. In specific, the encoder is a stack of N + 1 MLP layers. From the bottom to the top, the first N layers have an identical structure. Each layer has four sub-layers: Dropout <ref type="bibr" target="#b32">(Srivastava et al., 2014)</ref>, Linear, BatchNorm <ref type="bibr" target="#b15">(Ioffe and Szegedy, 2015)</ref>, and <ref type="bibr">LeakyReLU (Maas et al., 2013)</ref>. The final layer is a Dropout sub-layer and a Linear transformation followed by a Softmax. The decoder shares the same architecture as the encoder, though they may vary in input/output dimensions. In our experiments, we set a Dropout probability of 0.5 in the first encoder layer and 0.2 in the remaining encoder and decoder layers. All LeakyReLU sub-layers have a negative slope of 0.01.</p><p>Combining the encoder and the decoder, we now have the reconstruction loss:</p><formula xml:id="formula_0">L rec (X, X) = −E(x log x),<label>(1)</label></formula><p>which encourages the decoder outputs X = { x(i) } m i=1 to be as similar as the corresponding encoder inputs X = {x (i) } m i=1 for each training batch, where m is the batch size.</p><p>For topic distribution ẑ, what we have done above is insufficient to generate reasonable topics since ẑ's distribution Q is not well defined. To this end, we follow a similar approach proposed in <ref type="bibr" target="#b23">(Nan et al., 2019)</ref> and further impose on ẑ a Dirichlet prior P by minimizing the Maximum Mean Discrepancy (MMD) <ref type="bibr" target="#b12">(Gretton et al., 2012)</ref> between the two distributions P and Q:</p><formula xml:id="formula_1">L MMD (Z, Ẑ) = − 2 m 2 i,j k(z (i) , ẑ(j) )+ 1 m(m − 1) i̸ =j (k(z (i) , z (j) ) + k( ẑ(i) , ẑ(j) )), (2)</formula><p>where Z = {z (i) } m i=1 are topic distributions randomly drawn from the prior P , Ẑ = { ẑ(i) } m i=1 are encoder outputs, and k is the kernel function that is information diffusion kernel <ref type="bibr" target="#b17">(Lebanon and Lafferty, 2003)</ref> in our experiments following <ref type="bibr" target="#b23">(Nan et al., 2019)</ref>.</p><p>The overall training objective is:</p><formula xml:id="formula_2">L = L rec (X, X) + λrL MMD (Z, Ẑ),<label>(3)</label></formula><p>where we balance L rec and L MMD with a hyperparameter λ and another factor</p><formula xml:id="formula_3">r = ∥∇ b (N +1) L rec (X, X)∥ 2 ∥∇ b (N +1) L MMD (Z, Ẑ)∥ 2 , (<label>4</label></formula><formula xml:id="formula_4">)</formula><p>where ∥•∥ 2 denotes L2 normalization and b (N +1)  is the bias term of the last Linear sub-layer of the encoder, i.e., the one just before the Softmax sublayer. Equation (4) shows that the two losses are balanced with their relative gradient norm with respect to b (N +1) . We found in our experiments that r greatly reduces the effort of tuning λ and generally produces better results.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Pre-training</head><p>By pre-training the topic model on a large and topically diverse corpus, we expect the model would learn topic-related knowledge that is general enough to be reused on other corpora. For the proposed approach, the knowledge may include word semantics, common senses, and document encoding and decoding patterns at each layer.</p><p>The details of the pre-training procedure are presented in Algorithm 1. The pre-training corpus D is the subset00 of the OpenWebText dataset <ref type="bibr" target="#b10">(Gokaslan and Cohen, 2019)</ref>, an open-source recreation of the WebText dataset as detailed in <ref type="bibr" target="#b27">(Radford et al., 2019)</ref>. We preprocess data by tokenization, lemmatization, stopword removal, and only keeping words occurred in at least 50 documents. After preprocessing, there are about 392K documents, consisting of 45K unique words, in the resulting dataset. At each training mini-batch, we update model parameters according to Equation (3) using the Adam optimizer <ref type="bibr" target="#b16">(Kingma and Ba, 2014)</ref>. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>4:</head><p>for each X = {x (j) } m j=1 from D do 5:</p><p>Ẑ ← E(X); X ← D( Ẑ)</p><p>6:</p><p>Sample Z = {z (j) } m j=1 ∼ P (z).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>7:</head><p>Compute L by Equation (3).</p><p>8:</p><formula xml:id="formula_5">θ ← Adam(∇ θ 1 m m j=1 L (j) , θ) 9:</formula><p>end for 10: end for 3.3 Fine-tuning Fine-tuning is the process of adapting the pretrained topic model to a specific dataset. However, directly fine-tuning the pre-trained model on a new dataset does not always work and may introduce severe bias to subsequent tuning steps since the ideal number of topics might change and the corpus-wide topic distributions might be different. Therefore, our fine-tuning begins with the pre-trained model but randomly re-initializes parameters in the last encoder layer and the first decoder layer. If we fine-tune the model without any re-initialization, we find that in our experiments the corpus-wide topic distributions discovered by the fine-tuned model would be biased towards the topic distribution of the pre-training corpus, which is unexpected. The proposed fine-tuning strategy with re-initialization solves this issue. Algorithm 2 shows the fine-tuning steps. We keep the pre-trained parameters fixed for the first n 1 epochs and use a small learning rate in the remaining training epochs since they have already been well trained before fine-tuning.</p><p>Algorithm 2 Fine-tuning.</p><p>Require: D ′ , the target corpus; E, the encoder; D, the decoder; θ r , randomly initialized parameters; θ p , pre-trained parameters; m, the batch size; n, the number of training epochs; n 1 , n 1 ∈ N and 0 ≤ n 1 ≤ n; P (z), the Dirichlet prior. 1:</p><formula xml:id="formula_6">for i = 1, • • • , n do 2: Shuffle D ′ . 3: for each X = {x (j) } m j=1 from D ′ do 4: Ẑ ← E(X); X ← D( Ẑ) 5:</formula><p>Sample Z = {z (j) } m j=1 ∼ P (z).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>6:</head><p>Compute L by Equation (3).</p><p>7:</p><formula xml:id="formula_7">θ r ← Adam(∇ θr 1 m m j=1 L (j) , θ r ) 8: if i &gt; n 1 then 9: θ p ← Adam(∇ θp 1 m m j=1 L (j) , θ p ) 10:</formula><p>end if 11:</p><p>end for 12: end for By comparing Algorithm 1 with Algorithm 2, it can be observed that the fine-tuning process adds little overhead to the training stage. More importantly, the proposed method does not introduce any additional computations or parameters during inference.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experiments</head><p>We used three datasets in <ref type="bibr" target="#b14">(Hu et al., 2020)</ref>: NY-Times<ref type="foot" target="#foot_1">2</ref> , Grolier<ref type="foot" target="#foot_2">3</ref> , and 20Newsgroups<ref type="foot" target="#foot_3">4</ref> . We did not include the DBPedia dataset as it is based on Wikipedia and potentially overlaps with the dataset used for our pre-training. The dataset statistics are shown in Table <ref type="table" target="#tab_0">1</ref>.</p><p>The proposed basic model, NTM, is the one described in Section 3 without pre-training. encoder and the decoder have three layers (N = 2) and 300 neurons at each hidden layer. We have four variants:</p><p>• NTM-w2v, we initialize weights w e1 ∈ R V ×300 of the first encoder Linear sub-layer and w d3 ∈ R 300×V of the the last decoder Linear sub-layer with the corresponding 300dim Word2Vec embeddings trained on Google News.</p><p>• NTM-glv, same as NTM-w2v but utilizing 300-dim GloVe embeddings trained on Wikipedia and Gigaword 5.</p><p>• PT-NTM-w2v, pre-training from NTM-w2v initialization and then fine-tuning.</p><p>• PT-NTM-glv, pre-training from NTM-glv initialization and then fine-tuning.</p><p>The number of training epochs is 200 for pretraining, fine-tuning (PT-* models) and fresh training (NTM). We used the Dirichlet prior distribution whose parameters are all 1 K , where K is the topic number. MMD loss weight λ is 1 for all models expect the fine-tuning of *-pre models in which λ is 0.3. We will analyze the effect of λ in our experiments. During pre-training, the batch size is 1,024, the learning rate is 2e-2, and the topic number is 200. For fine-tuning, n 1 is 100, and the learning rates for reinitialized and pre-trained parameters are 2e-2 and 1e-5, respectively (Algorithm 2), showing that the pre-trained parameters are only slightly tuned. The batch size of fine-tuning and fresh training varies on different datasets depending on their sizes. Specifically, it is set to 128 for 20Newsgroups, 256 for Grolier and 512 for NY-Times. Finally, it should be noted that fine-tuning on each datasets shares the same pre-trained model checkpoint for each model variant.</p><p>We compare our models with following baselines:</p><p>• LDA <ref type="bibr" target="#b4">(Blei et al., 2003)</ref>, we used the implementation of GibbsLDA++<ref type="foot" target="#foot_4">5</ref> .</p><p>• ProdLDA <ref type="bibr" target="#b31">(Srivastava and Sutton, 2017)</ref>, a VAE-based model that employs logistic normal prior for topic distributions.</p><p>• W-LDA <ref type="bibr" target="#b23">(Nan et al., 2019)</ref>. Our model follows W-LDA loss but differs in training and implementation.</p><p>• BAT <ref type="bibr" target="#b34">(Wang et al., 2020)</ref>, an adversarially trained neural topic model.</p><p>• ToMCAT <ref type="bibr" target="#b14">(Hu et al., 2020)</ref>, an adversarial neural topic model with cycle-consistency objective.</p><p>• ZeroShotTM <ref type="bibr" target="#b2">(Bianchi et al., 2020b)</ref>, taking Sentence-BERT (Reimers and Gurevych, 2019) embeddings as input.</p><p>• CombinedTM <ref type="bibr" target="#b1">(Bianchi et al., 2020a)</ref>, same as ZeroShotTM but combining the input with BoWs.</p><p>• G-BAT <ref type="bibr" target="#b34">(Wang et al., 2020)</ref>, extending BAT to incorporate pre-trained word embeddings.</p><p>• TopicOcean <ref type="bibr" target="#b30">(Song et al., 2020)</ref>, integrating well-trained LDAs and transferring the knowledge of accumulated topics to new corpora, which is re-implemented by ourselves.</p><p>We evaluate the model performance with three topic coherence measures and one topic diversity measure. Topic coherence measures first calculate the coherence scores of pairs of top words ranked by their topic-associated probabilities for each topic and then aggregate all topic scores as the final topic coherence. The used topic coherence measures are C_A <ref type="bibr" target="#b0">(Aletras and Stevenson, 2013)</ref>, C_P <ref type="bibr" target="#b29">(Röder et al., 2015)</ref>, and NPMI <ref type="bibr" target="#b0">(Aletras and Stevenson, 2013)</ref> of top-10 topic words, implemented in Palmetto <ref type="bibr" target="#b29">(Röder et al., 2015)</ref> <ref type="foot" target="#foot_5">6</ref> . Topic coherence measures are highly correlated with human evaluation but have no penalizing mechanism for repetitive or similar topics. We remedy the problem by also evaluating topic diversity. Our topic diversity measure is calculate by TD = 1 − Nrep N total , where N total = 10 × K is the total number of topic words and N rep counts the number of repetitions in all topic words. For example, 5 identical words would add 4 to N rep . </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Topic Modeling Results</head><p>The topic modeling results are presented in Table <ref type="table" target="#tab_1">2</ref>. We report results averaged over five runs with topic number set to 20, 30, 50, 75, and 100 respectively in all our experiments unless otherwise specified.</p><p>From Table <ref type="table" target="#tab_1">2</ref>, we can observe that: 1) Among all models, PT-NTM and its variants outperform other methods by a large margin. Since PT-NTM and NTM share the identical model architecture, we attribute the improvements of PT-NTM over NTM to the pre-training strategy. 2) For PLMs-based methods, both ZeroShotTM and Com-binedTM performs badly, for some metric even worse than regular methods. We think the reason maybe the gap between the learning objectives of PLMs (word order-based) and topic models (wordcooccurrence based). 3) For PWEs-based methods, non-pretrained methods (NTM, BAT) benefits a lot from the PWEs. We think the reason maybe the PWEs are also trained based on word-cooccurrence, so the gap between PWEs and topic models is relatively small. Another interesting thing is that the benefit of using PWEs in topic modeling seems diminishing with our proposed topic model pretraining strategy. For example, PT-NTM gives similar results compared to PT-NTM-w2v and PT-NTM-glv. This shows that word semantic knowledge has somehow been captured to a certain degree by pre-training the topic model on a large corpus. 4) For pre-training-based models, PT-NTM outperforms TopicOcean, consider the performance gap between their base models (NTM for PT-NTM and LDA for TopicOcean), the improvement of PT-NTM is even larager. What's more, our method is based on neural network, which is easier to incorporated with PWEs or other information than TopicOcean, which is based on graphical models.</p><p>One concern about PT-NTM may be that the whether the f ine − tuning stage works. To get a sense of the topics extracted by our model, we list in Table <ref type="table" target="#tab_3">3</ref> top 4 topics extracted by PT-NTM on the pre − training and f ine − tuning dataset. The topic labels are assigned manually. The whole topics are presented in the attachment.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Contextualized vs. Static word embeddings</head><p>Contextualized word embeddings like those produced by BERT <ref type="bibr" target="#b8">(Devlin et al., 2019)</ref> provide richer semantic than static ones like Word2Vec <ref type="bibr" target="#b22">(Mikolov et al., 2013)</ref> or GloVe <ref type="bibr" target="#b25">(Pennington et al., 2014)</ref>. Thus we also conducted experiments to test their performance on topic modeling. The baseline models are ZeroShotTM <ref type="bibr" target="#b2">(Bianchi et al., 2020b)</ref> and CombinedTM <ref type="bibr" target="#b1">(Bianchi et al., 2020a)</ref>. Ze-roShotTM and CombinedTM both take Sentence-BERT <ref type="bibr" target="#b28">(Reimers and Gurevych, 2019)</ref> embeddings as inputs but CombinedTM additionally uses BoW. We also implement three NTM-based models, namely BERT-NTM, Word2Vec-NTM, and GloVe-NTM, according to the input embeddings they used. BERT-NTM follows the idea of ZeroShotTM, aim-   can be viewed as a form of word clustering, our results are somewhat inline with previous findings reported in <ref type="bibr" target="#b19">Meng et al. (2019)</ref> that using BERT leads to poor performance on text clustering.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Ablation Study and Further Analysis</head><p>Number of model layers We vary the number of encoder and decoder layers of pre-training and fine-tuning models, and show the results in Table <ref type="table" target="#tab_4">5</ref>. It can be observed that the four-layer and the threelayer models achieve the highest topic coherence and topic diversity respectively. Further increasing the layer number resulted in slight declines in all four metrics.</p><p>MMD loss weight λ We present the impact of λ on our model in Figure <ref type="figure" target="#fig_3">2</ref>. With λ increasing from 0.03 to 30, the NPMI of PT-NTM-glv first gradually increases, peaking at about 0.14 when λ = 1, and then gradually decreases. For Topic Diversity (TD), however, we observe a steady decline for PT-NTM-glv. PT-NTM also has a similar trend but with more drastic changes. Given these findings, it seems that there is a trade-off towards generating more coherent or diverse topics. Nevertheless, it is worth noting that in comparison to NTM, the PT-NTM-glv is very robust to the choices of λ. The NPMI values of PT-NTMglv only fluctuate in the range of [0.11, 0.14] while its TD values vary between 0.74 and 0.86. This is in contrast to NTM in which it has poor topic coherence for λ ≤ 0.1 and low topic diversity for λ ≥ 10. We attribute the advantage of the pretrained model to our proposed fine-tuning strategy. During fine-tuning, we mainly update a small set of parameters that are directly related to topics while only slightly tune others, which consequently enables more controllable data/gradient flows and thus produces more stable results.</p><p>Data efficiency With pre-training, a topic model indeed captures extensive knowledge from an external corpus. As have been shown in our experiments, the acquired knowledge can improve the performance of subsequent fine-tuning on other datasets, It would be interesting to see to what extent such knowledge can increase data efficiency. To this end, we conducted experiments that take subsets of NYTimes dataset of varying sizes as training datasets. Specifically, we used dataset sizes including 1K, 2K, 4K, • • • , 64K, and 100K. For each size, we averaged the results over five runs whose training datasets are randomly sampled from the whole dataset with different random seeds.</p><p>The results are shown in Figure <ref type="figure" target="#fig_4">3</ref>. PT-NTM-glv has a very high starting point when the document number is 1000: the NPMI and TD is about 0.15 and 0.89 respectively. While at the same time, NTM has extremely poor performance with negative NPMI and low TD. Only when the number increases to 8000, the topics generated by NTM has comparable topic diversity to topics from PT-NTM-glv. But even when the whole dataset is used by PT-NTM, i.e., the document number is 100K, NTM's NPMI is still about 0.08 lower than the 1000-document PT-NTM-glv, which indeed represents a significant difference in topic quality. In summary, pre-training the topic model greatly reduces the need for training data and helps the model achieve superior performance with only 1% of documents on the NYTimes dataset.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusion</head><p>In this paper, we proposed a simple yet effective strategy to incorporating external knowledge into neural topic modeling by pre-training topic models on a large corpus before fine-tuning them on specific datasets. By experiments, we have presented the effectiveness of the method of pre-trained neural topic model in terms of topic coherence, topic diversity, and data efficiency over other methods such as by incorporating PWEs and PLMs. Another advantage of this approach is that it introduces little overhead to the training and none to the inference. Limited by computing resources, we did not experiment pre-trainings on larger datasets, though we believe there is still room for improvement given more pre-training data. For future research, we encourage further explorations in model architectures, pre-training objectives, and fine-tuning procedures.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: The architecture of neural topic model employed in PT-NTM. Both the encoder on the left and the decoder on the right have N + 1 layers.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Algorithm 1</head><label>1</label><figDesc>Pre-training. Require: D, the pre-training corpus; E, the encoder; D, the decoder; θ, parameters of E and D; θ 0 , initial parameters; m, the batch size; n, the number of training epochs; P (z), the Dirichlet prior. 1: θ ← θ 0 2: for i = 1, • • • , n do 3: Shuffle D.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: NPMI and TD results on 20Newsgroups of PT-NTM-glv and NTM w.r.t. MMD loss weight λ.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: NPMI and TD results on NYTimes of PT-NTM-glv and NTM w.r.t. the training dataset sizes.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 :</head><label>1</label><figDesc>Dataset statistics.</figDesc><table><row><cell>Both the</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 :</head><label>2</label><figDesc>Average topic coherence (C_A, C_P, and NPMI) and topic diversity (TD) scores of 5 topic number settings(20, 30, 50, 75, 100)  on 3 datasets (NYTimes, Grolier, and 20Newsgroups). Bold values indicate best-performing models under corresponding settings. NYTimes and Grolier only have BoW data so we cannot evaluate ZeroShotTM and CombinedTM, which require word order information, on them.</figDesc><table><row><cell cols="2">Model</cell><cell>C_A</cell><cell cols="2">NYTimes C_P NPMI</cell><cell>TD</cell><cell>C_A</cell><cell cols="2">Grolier C_P NPMI</cell><cell>TD</cell><cell>C_A</cell><cell>20Newsgroups C_P NPMI</cell><cell>TD</cell></row><row><cell></cell><cell>LDA</cell><cell cols="10">0.215 0.323 0.081 0.82 0.196 0.197 0.053 0.81 0.186 0.282 0.064 0.79</cell></row><row><cell></cell><cell>ProdLDA</cell><cell cols="10">0.184 0.125 0.015 0.69 0.148 -0.065 -0.019 0.83 0.178 0.071 -0.044 0.67</cell></row><row><cell>BoWs-based</cell><cell>W-LDA BAT</cell><cell cols="10">0.225 0.335 0.078 0.79 0.235 0.258 0.073 0.86 0.229 0.341 0.062 0.72 0.236 0.375 0.095 0.80 0.211 0.231 0.061 0.73 0.199 0.296 0.055 0.69</cell></row><row><cell></cell><cell>ToMCAT</cell><cell cols="10">0.245 0.385 0.095 0.79 0.229 0.275 0.081 0.90 0.208 0.314 0.066 0.68</cell></row><row><cell></cell><cell>NTM</cell><cell cols="10">0.229 0.269 0.056 0.90 0.215 0.146 0.030 0.93 0.242 0.372 0.070 0.82</cell></row><row><cell></cell><cell>G-BAT</cell><cell cols="10">0.249 0.414 0.108 0.72 0.219 0.258 0.074 0.78 0.229 0.394 0.087 0.78</cell></row><row><cell>PWEs-based</cell><cell>NTM-w2v</cell><cell cols="10">0.238 0.404 0.096 0.93 0.236 0.273 0.087 0.92 0.258 0.482 0.113 0.82</cell></row><row><cell></cell><cell>NTM-glv</cell><cell cols="10">0.247 0.388 0.103 0.90 0.257 0.334 0.106 0.93 0.278 0.526 0.129 0.80</cell></row><row><cell>PLMs-based</cell><cell>ZeroShotTM CombinedTM</cell><cell>--</cell><cell>--</cell><cell>--</cell><cell>--</cell><cell>--</cell><cell>--</cell><cell>--</cell><cell>--</cell><cell cols="2">0.190 0.249 0.042 0.81 0.182 0.235 0.039 0.79</cell></row><row><cell></cell><cell>TopicOcean</cell><cell cols="10">0.266 0.419 0.099 0.68 0.197 0.289 0.060 0.61 0.195 0.289 0.070 0.61</cell></row><row><cell>Pretrain-based</cell><cell cols="11">PT-NTM PT-NTM-w2v 0.276 0.539 0.131 0.96 0.325 0.621 0.160 0.95 0.271 0.538 0.127 0.87 0.312 0.651 0.148 0.91 0.325 0.616 0.127 0.93 0.279 0.532 0.124 0.80</cell></row><row><cell></cell><cell>PT-NTM-glv</cell><cell cols="10">0.304 0.614 0.152 0.95 0.345 0.673 0.181 0.96 0.287 0.560 0.140 0.84</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 3 :</head><label>3</label><figDesc>Top 4 topics extracted by PT-NTM on OpenWebText, NYTimes, Grolier and 20Newsgroups dataset.</figDesc><table><row><cell>Model</cell><cell>C_A</cell><cell>C_P NPMI</cell><cell>TD</cell><cell cols="2">#Layers C_A</cell><cell>C_P NPMI</cell><cell>TD</cell></row><row><cell cols="4">ZeroShotTM CombinedTM BERT-NTM Word2Vec-NTM 0.233 0.388 0.079 0.79 0.190 0.249 0.042 0.81 0.182 0.235 0.039 0.79 0.236 0.382 0.072 0.80 GloVe-NTM 0.250 0.407 0.083 0.80</cell><cell>2 3 4 5</cell><cell cols="3">0.238 0.375 0.071 0.82 0.287 0.560 0.140 0.84 0.292 0.588 0.146 0.80 0.286 0.578 0.143 0.78</cell></row><row><cell cols="4">Table 4: Topic modeling results on 20Newsgroups.</cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="4">ing at providing a fair comparison between BERT-</cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="4">based topic models. Word2Vec-NTM only uses</cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="4">pre-trained embeddings in the encoder, which is</cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="4">different from NTM-w2v as the latter use the the</cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="4">pre-trained Word2Vec embeddings in both the first</cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="4">encoder layer and the last decoder layer. The same</cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="4">setup applies to GloVe-NTM. The experimental results on 20Newsgroups 7 are</cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="4">shown in Table 4. All the models have similar</cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="4">topic diversity. Our NTM variants outperform both</cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="4">ZeroShotTM and CombinedTM on all three topic</cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="4">coherence measures. The possible reasons could</cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="4">be: 1) Topic modeling does not quite rely on word</cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="4">order information, at least for our experimented</cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="4">dataset; and 2) Training of GloVe utilizes global</cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="4">word-word co-occurrence statistics that are also</cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="4">helpful for topic modeling. As topic modeling</cell><cell></cell><cell></cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 5 :</head><label>5</label><figDesc>The impact of the #layers on 20Newsgroups.</figDesc><table /></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0">In this paper, PWEs refer to context-free embeddings.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_1">http://archive.ics.uci.edu/ml/ datasets/Bag+of+Words</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3" xml:id="foot_2">https://cs.nyu.edu/~roweis/data</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4" xml:id="foot_3">http://qwone.com/~jason/20Newsgroups</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="5" xml:id="foot_4">http://gibbslda.sourceforge.net/</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="6" xml:id="foot_5">https://github.com/AKSW/Palmetto</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="7" xml:id="foot_6">The other two datasets only contain word counts, making it impossible to extract BERT embeddings since no word context information is present.</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgement</head><p>We would like to thank anonymous reviewers for their valuable comments and helpful suggestions and we thank Tencent for supporting this project. This work was funded by the National Natural Science Foundation of China (61772132, 62176053).</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Evaluating topic coherence using distributional semantics</title>
		<author>
			<persName><forename type="first">Nikolaos</forename><surname>Aletras</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mark</forename><surname>Stevenson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 10th International Conference on Computational Semantics (IWCS 2013) -Long Papers</title>
				<meeting>the 10th International Conference on Computational Semantics (IWCS 2013) -Long Papers<address><addrLine>Potsdam, Germany</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2013">2013</date>
			<biblScope unit="page" from="13" to="22" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<author>
			<persName><forename type="first">Federico</forename><surname>Bianchi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Silvia</forename><surname>Terragni</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dirk</forename><surname>Hovy</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2004.03974</idno>
		<title level="m">Pre-training is a hot topic: Contextualized document embeddings improve topic coherence</title>
				<imprint>
			<date type="published" when="2020">2020a</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">Cross-lingual contextualized topic models with zero-shot learning</title>
		<author>
			<persName><forename type="first">Federico</forename><surname>Bianchi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Silvia</forename><surname>Terragni</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dirk</forename><surname>Hovy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Debora</forename><surname>Nozza</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Elisabetta</forename><surname>Fersini</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2004.07737</idno>
		<imprint>
			<date type="published" when="2020">2020b</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Probabilistic topic models</title>
		<author>
			<persName><forename type="first">M</forename><surname>David</surname></persName>
		</author>
		<author>
			<persName><surname>Blei</surname></persName>
		</author>
		<idno type="DOI">10.1145/2133806.2133826</idno>
	</analytic>
	<monogr>
		<title level="j">Communications of the ACM</title>
		<imprint>
			<biblScope unit="volume">55</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page">77</biblScope>
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Latent dirichlet allocation</title>
		<author>
			<persName><forename type="first">David</forename><forename type="middle">M</forename><surname>Blei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrew</forename><forename type="middle">Y</forename><surname>Ng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><forename type="middle">I</forename><surname>Jordan</surname></persName>
		</author>
		<idno type="DOI">10.5555/944919.944937</idno>
	</analytic>
	<monogr>
		<title level="j">J. Mach. Learn. Res</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page" from="993" to="1022" />
			<date type="published" when="2003">2003</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">Language models are few-shot learners</title>
		<author>
			<persName><forename type="first">Benjamin</forename><surname>Tom B Brown</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nick</forename><surname>Mann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Melanie</forename><surname>Ryder</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jared</forename><surname>Subbiah</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Prafulla</forename><surname>Kaplan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Arvind</forename><surname>Dhariwal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pranav</forename><surname>Neelakantan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Girish</forename><surname>Shyam</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Amanda</forename><surname>Sastry</surname></persName>
		</author>
		<author>
			<persName><surname>Askell</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2005.14165</idno>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Neural models for documents with metadata</title>
		<author>
			<persName><forename type="first">Dallas</forename><surname>Card</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chenhao</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Noah</forename><forename type="middle">A</forename><surname>Smith</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/P18-1189</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 56th Annual Meeting of the Associafor Computational Linguistics</title>
		<title level="s">Long Papers</title>
		<meeting>the 56th Annual Meeting of the Associafor Computational Linguistics<address><addrLine>Melbourne, Australia</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2018">2018</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="2031" to="2040" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Gaussian LDA for topic models with word embeddings</title>
		<author>
			<persName><forename type="first">Rajarshi</forename><surname>Das</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Manzil</forename><surname>Zaheer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chris</forename><surname>Dyer</surname></persName>
		</author>
		<idno type="DOI">10.3115/v1/P15-1077</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing</title>
				<meeting>the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing<address><addrLine>Beijing, China</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2015">2015</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="795" to="804" />
		</imprint>
	</monogr>
	<note>Long Papers)</note>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">BERT: Pre-training of deep bidirectional transformers for language understanding</title>
		<author>
			<persName><forename type="first">Jacob</forename><surname>Devlin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ming-Wei</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kenton</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kristina</forename><surname>Toutanova</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/N19-1423</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</title>
		<title level="s">Long and Short Papers</title>
		<meeting>the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies<address><addrLine>Minneapolis, Minnesota</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2019">2019</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="4171" to="4186" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Topic modeling in embedding spaces</title>
		<author>
			<persName><forename type="first">B</forename><surname>Adji</surname></persName>
		</author>
		<author>
			<persName><surname>Dieng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">R</forename><surname>Francisco</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><forename type="middle">M</forename><surname>Ruiz</surname></persName>
		</author>
		<author>
			<persName><surname>Blei</surname></persName>
		</author>
		<idno type="DOI">10.1162/tacl_a_00325</idno>
	</analytic>
	<monogr>
		<title level="j">Transactions of the Association for Computational Linguistics</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page" from="439" to="453" />
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title/>
		<author>
			<persName><forename type="first">Aaron</forename><surname>Gokaslan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vanya</forename><surname>Cohen</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note>Openwebtext corpus</note>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Generative adversarial nets</title>
		<author>
			<persName><forename type="first">Ian</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jean</forename><surname>Pouget-Abadie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mehdi</forename><surname>Mirza</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bing</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><surname>Warde-Farley</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sherjil</forename><surname>Ozair</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aaron</forename><surname>Courville</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
				<imprint>
			<publisher>Curran Associates, Inc</publisher>
			<date type="published" when="2014">2014</date>
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="page" from="2672" to="2680" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">A kernel two-sample test</title>
		<author>
			<persName><forename type="first">Arthur</forename><surname>Gretton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Karsten</forename><forename type="middle">M</forename><surname>Borgwardt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Malte</forename><forename type="middle">J</forename><surname>Rasch</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bernhard</forename><surname>Schölkopf</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alexander</forename><surname>Smola</surname></persName>
		</author>
		<idno type="DOI">10.5555/2188385.2188410</idno>
	</analytic>
	<monogr>
		<title level="j">J. Mach. Learn. Res</title>
		<imprint>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="page" from="723" to="773" />
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Improving Neural Topic Models using Knowledge Distillation</title>
		<author>
			<persName><forename type="first">Alexander</forename><surname>Miserlis Hoyle</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pranav</forename><surname>Goel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Philip</forename><surname>Resnik</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2020.emnlp-main.137</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)</title>
				<meeting>the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)</meeting>
		<imprint>
			<publisher>Online. Association for Computational Linguistics</publisher>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="1752" to="1771" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Neural topic modeling with cycleconsistent adversarial training</title>
		<author>
			<persName><forename type="first">Xuemeng</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rui</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Deyu</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuxuan</forename><surname>Xiong</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2020.emnlp-main.725</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)</title>
				<meeting>the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)</meeting>
		<imprint>
			<publisher>Online. Association for Computational Linguistics</publisher>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="9018" to="9030" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Batch normalization: Accelerating deep network training by reducing internal covariate shift</title>
		<author>
			<persName><forename type="first">Sergey</forename><surname>Ioffe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christian</forename><surname>Szegedy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 32nd International Conference on Machine Learning</title>
				<meeting>the 32nd International Conference on Machine Learning<address><addrLine>Lille, France</addrLine></address></meeting>
		<imprint>
			<publisher>PMLR</publisher>
			<date type="published" when="2015">2015</date>
			<biblScope unit="volume">37</biblScope>
			<biblScope unit="page" from="448" to="456" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<author>
			<persName><forename type="first">P</forename><surname>Diederik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jimmy</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName><surname>Ba</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1412.6980</idno>
		<title level="m">Adam: A method for stochastic optimization</title>
				<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Information diffusion kernels</title>
		<author>
			<persName><forename type="first">P</forename><surname>Diederik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Max</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Guy</forename><surname>Welling</surname></persName>
		</author>
		<author>
			<persName><forename type="first">John</forename><forename type="middle">D</forename><surname>Lebanon</surname></persName>
		</author>
		<author>
			<persName><surname>Lafferty</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1312.6114</idno>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
				<editor>
			<persName><forename type="first">S</forename><surname>Becker</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">S</forename><surname>Thrun</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">K</forename><surname>Obermayer</surname></persName>
		</editor>
		<imprint>
			<publisher>MIT Press</publisher>
			<date type="published" when="2003">2013. 2003</date>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="page" from="391" to="398" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
	<note>Autoencoding variational bayes</note>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Rectifier nonlinearities improve neural network acoustic models</title>
		<author>
			<persName><forename type="first">Awni</forename><forename type="middle">Y</forename><surname>Andrew L Maas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrew</forename><forename type="middle">Y</forename><surname>Hannun</surname></persName>
		</author>
		<author>
			<persName><surname>Ng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML Workshop on Deep Learning for Audio, Speech and Language Processing</title>
				<imprint>
			<publisher>Citeseer</publisher>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Spherical text embedding</title>
		<author>
			<persName><forename type="first">Yu</forename><surname>Meng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiaxin</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Guangyuan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chao</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Honglei</forename><surname>Zhuang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lance</forename><surname>Kaplan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiawei</forename><surname>Han</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
				<imprint>
			<publisher>Curran Associates, Inc</publisher>
			<date type="published" when="2019">2019</date>
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="page" from="8208" to="8217" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Discovering discrete latent topics with neural variational inference</title>
		<author>
			<persName><forename type="first">Yishu</forename><surname>Miao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Edward</forename><surname>Grefenstette</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Phil</forename><surname>Blunsom</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 34th International Conference on Machine Learning</title>
				<meeting>the 34th International Conference on Machine Learning<address><addrLine>Sydney, Australia</addrLine></address></meeting>
		<imprint>
			<publisher>PMLR</publisher>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="2410" to="2419" />
		</imprint>
	</monogr>
	<note>Proceedings of Machine Learning Research</note>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Neural variational inference for text processing</title>
		<author>
			<persName><forename type="first">Yishu</forename><surname>Miao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lei</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Phil</forename><surname>Blunsom</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of The 33rd International Conference on Machine Learning</title>
				<meeting>The 33rd International Conference on Machine Learning<address><addrLine>New York, New York, USA</addrLine></address></meeting>
		<imprint>
			<publisher>PMLR</publisher>
			<date type="published" when="2016">2016</date>
			<biblScope unit="volume">48</biblScope>
			<biblScope unit="page" from="1727" to="1736" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Distributed representations of words and phrases and their compositionality</title>
		<author>
			<persName><forename type="first">Tomas</forename><surname>Mikolov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kai</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Greg</forename><forename type="middle">S</forename><surname>Corrado</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jeff</forename><surname>Dean</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
				<imprint>
			<publisher>Curran Associates, Inc</publisher>
			<date type="published" when="2013">2013</date>
			<biblScope unit="volume">26</biblScope>
			<biblScope unit="page" from="3111" to="3119" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Topic modeling with Wasserstein autoencoders</title>
		<author>
			<persName><forename type="first">Feng</forename><surname>Nan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ran</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ramesh</forename><surname>Nallapati</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bing</forename><surname>Xiang</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/P19-1640</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics</title>
				<meeting>the 57th Annual Meeting of the Association for Computational Linguistics<address><addrLine>Florence, Italy</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="6345" to="6381" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">CluBERT: A cluster-based approach for learning sense distributions in multiple languages</title>
		<author>
			<persName><forename type="first">Tommaso</forename><surname>Pasini</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Federico</forename><surname>Scozzafava</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bianca</forename><surname>Scarlini</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2020.acl-main.369</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics</title>
				<meeting>the 58th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<publisher>Online. Association for Computational Linguistics</publisher>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="4008" to="4018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">GloVe: Global vectors for word representation</title>
		<author>
			<persName><forename type="first">Jeffrey</forename><surname>Pennington</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christopher</forename><surname>Manning</surname></persName>
		</author>
		<idno type="DOI">10.3115/v1/D14-1162</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP)</title>
				<meeting>the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP)<address><addrLine>Doha, Qatar</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="1532" to="1543" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Deep contextualized word representations</title>
		<author>
			<persName><forename type="first">Matthew</forename><surname>Peters</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mark</forename><surname>Neumann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mohit</forename><surname>Iyyer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Matt</forename><surname>Gardner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christopher</forename><surname>Clark</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kenton</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Luke</forename><surname>Zettlemoyer</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/N18-1202</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</title>
		<title level="s">Long Papers</title>
		<meeting>the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies<address><addrLine>New Orleans, Louisiana</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2018">2018</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="2227" to="2237" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Language models are unsupervised multitask learners</title>
		<author>
			<persName><forename type="first">Alec</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jeffrey</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rewon</forename><surname>Child</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><surname>Luan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dario</forename><surname>Amodei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">OpenAI blog</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page">9</biblScope>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Sentence-BERT: Sentence embeddings using Siamese BERTnetworks</title>
		<author>
			<persName><forename type="first">Nils</forename><surname>Reimers</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Iryna</forename><surname>Gurevych</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/D19-1410</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)</title>
				<meeting>the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)<address><addrLine>Hong Kong, China</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="3982" to="3992" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Exploring the space of topic coherence measures</title>
		<author>
			<persName><forename type="first">Michael</forename><surname>Röder</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andreas</forename><surname>Both</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alexander</forename><surname>Hinneburg</surname></persName>
		</author>
		<idno type="DOI">10.1145/2684822.2685324</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Eighth ACM International Conference on Web Search and Data Mining, WSDM &apos;15</title>
				<meeting>the Eighth ACM International Conference on Web Search and Data Mining, WSDM &apos;15<address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="399" to="408" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Topicocean: An ever-increasing topic model with metalearning</title>
		<author>
			<persName><forename type="first">Yuanfeng</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yongxin</forename><surname>Tong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Siqi</forename><surname>Bao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Di</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hua</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Raymond Chi-Wing</forename><surname>Wong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2020 IEEE International Conference on Data Mining (ICDM)</title>
				<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="1262" to="1267" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
		<title level="m" type="main">Autoencoding variational inference for topic models</title>
		<author>
			<persName><forename type="first">Akash</forename><surname>Srivastava</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Charles</forename><surname>Sutton</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1703.01488</idno>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Dropout: A simple way to prevent neural networks from overfitting</title>
		<author>
			<persName><forename type="first">Nitish</forename><surname>Srivastava</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Geoffrey</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alex</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ruslan</forename><surname>Salakhutdinov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="issue">56</biblScope>
			<biblScope unit="page" from="1929" to="1958" />
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Attention is all you need</title>
		<author>
			<persName><forename type="first">Ashish</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Noam</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Niki</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jakob</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Llion</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aidan</forename><forename type="middle">N</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ł</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Illia</forename><surname>Polosukhin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
				<imprint>
			<publisher>Curran Associates, Inc</publisher>
			<date type="published" when="2017">2017</date>
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="page" from="5998" to="6008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Neural Gibbs Sampling for Joint Event Argument Extraction</title>
		<author>
			<persName><forename type="first">Xiaozhi</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shengyu</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xu</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhiyuan</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Juanzi</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Peng</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jie</forename><surname>Zhou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 1st Conference of the Asia-Pacific Chapter of the Association for Computational Linguistics and the 10th International Joint Conference on Natural Language Processing</title>
				<meeting>the 1st Conference of the Asia-Pacific Chapter of the Association for Computational Linguistics and the 10th International Joint Conference on Natural Language Processing<address><addrLine>Suzhou, China</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="169" to="180" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
