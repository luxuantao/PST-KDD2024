<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main"></title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Hu</forename><surname>Chen</surname></persName>
							<email>huchen@scu.edu.cn</email>
						</author>
						<author>
							<persName><forename type="first">Yi</forename><surname>Zhang</surname></persName>
							<email>yzhang@scu.edu.cn</email>
						</author>
						<author>
							<persName><roleName>Member, IEEE</roleName><forename type="first">Mannudeep</forename><forename type="middle">K</forename><surname>Kalra</surname></persName>
							<email>mkalra@mgh.harvard.edu</email>
						</author>
						<author>
							<persName><forename type="first">Feng</forename><surname>Lin</surname></persName>
							<email>linfeng@scu.edu.cn</email>
						</author>
						<author>
							<persName><forename type="first">Yang</forename><surname>Chen</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Peixo</forename><surname>Liao</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Jiliu</forename><surname>Zhou</surname></persName>
							<email>zhoujl@scu.edu.cn</email>
						</author>
						<author>
							<persName><forename type="first">G</forename><surname>Wang</surname></persName>
							<email>wangg6@rpi.edu</email>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="department">College of Computer Science</orgName>
								<orgName type="institution">Sichuan University</orgName>
								<address>
									<postCode>610065</postCode>
									<settlement>Chengdu</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff1">
								<orgName type="department">Department of Radiology</orgName>
								<orgName type="institution">Massachusetts General Hospital</orgName>
								<address>
									<postCode>02114</postCode>
									<settlement>Boston</settlement>
									<region>MA</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff2">
								<orgName type="department">Laboratory of Image Science and Technology</orgName>
								<orgName type="institution">Southeast University</orgName>
								<address>
									<postCode>210096</postCode>
									<settlement>Nanjing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff3">
								<orgName type="department">Ministry of Education</orgName>
								<orgName type="laboratory">Key Laboratory of Computer Network and Information Integration (Southeast University)</orgName>
								<address>
									<postCode>210096</postCode>
									<settlement>Nanjing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff4">
								<orgName type="department" key="dep1">Department of Scientific Research and Education</orgName>
								<orgName type="department" key="dep2">The Sixth People&apos;s Hospital of Chengdu</orgName>
								<address>
									<postCode>610065</postCode>
									<settlement>Chengdu</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff5">
								<orgName type="department">Department of Biomedical Engineering</orgName>
								<orgName type="institution">Rensselaer Polytechnic Institute</orgName>
								<address>
									<postCode>12180</postCode>
									<settlement>Troy</settlement>
									<region>NY</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">E9BC9F719B9E98A4A6E2851A2C32D338</idno>
					<idno type="DOI">10.1109/TMI.2017.2715284</idno>
					<note type="submission">This article has been accepted for publication in a future issue of this journal, but has not been fully edited. Content may change prior to final publication. Citation information: DOI 10.1109/TMI.2017.2715284, IEEE Transactions on Medical Imaging</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.3" ident="GROBID" when="2023-07-27T04:14+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Low-dose CT</term>
					<term>deep learning</term>
					<term>auto-encoder</term>
					<term>convolutional</term>
					<term>deconvolutional</term>
					<term>residual neural network</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Given the potential risk of X-ray radiation to the patient, low-dose CT has attracted a considerable interest in the medical imaging field. Currently, the main stream low-dose CT methods include vendor-specific sinogram domain filtration and iterative reconstruction algorithms, but they need to access raw data whose formats are not transparent to most users. Due to the difficulty of modeling the statistical characteristics in the image domain, the existing methods for directly processing reconstructed images cannot eliminate image noise very well while keeping structural details. Inspired by the idea of deep learning, here we combine the autoencoder, deconvolution network, and shortcut connections into the residual encoder-decoder convolutional neural network (RED-CNN) for low-dose CT imaging. After patch-based training, the proposed RED-CNN achieves a competitive performance relative to the-state-of-art methods in both simulated and clinical cases. Especially, our method has been favorably evaluated in terms of noise suppression, structural preservation, and lesion detection.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>I. INTRODUCTION</head><p>-RAY computed tomography (CT) has been widely utilized in clinical, industrial and other applications. Due to the increasing use of medical CT, concerns have been expressed on the overall radiation dose to a patient. The research interest has been strong in CT dose reduction under the well-known guiding principle of ALARA (as low as reasonably achievable) <ref type="bibr" target="#b0">[1]</ref>. The most common way to lower the radiation dose is to reduce the X-ray flux by decreasing the operating current and shortening the exposure time of an X-ray tube. In general, the weaker the X-ray flux, the noisier a reconstructed CT image, which degrades the signal-to-noise ratio and could compromise the diagnostic performance. To address this inherent physical problem, many algorithms were designed to improve the image quality for low-dose CT (LDCT). These algorithms can be generally categorized into three categories: (a) sinogram domain filtration, (2) iterative reconstruction, and (3) image processing.</p><p>Sinogram filtering techniques perform on either raw data or log-transformed data before image reconstruction, such as filtered backprojection (FBP). The main convenience in the data domain is that the noise characteristic has been well known. Typical methods include structural adaptive filtering <ref type="bibr" target="#b1">[2]</ref>, bilateral filtering <ref type="bibr" target="#b2">[3]</ref>, and penalized weighted least-squares (PWLS) algorithms <ref type="bibr" target="#b3">[4]</ref>. However, the sinogram filtering methods often suffer from spatial resolution loss when edges in the sinogram domain are not well preserved.</p><p>Over the past decade, iterative reconstruction (IR) algorithms have attracted much attention especially in the field of LDCT. This approach combines the statistical properties of data in the sinogram domain, prior information in the image domain, and even parameters of the imaging system into one unified objective function. With compressive sensing (CS) <ref type="bibr" target="#b4">[5]</ref>, several image priors were formulated as sparse transforms to deal with the low-dose, few-view, limited-angle and interior CT issues, such as total variation (TV) and its variants <ref type="bibr" target="#b5">[6]</ref><ref type="bibr" target="#b6">[7]</ref><ref type="bibr" target="#b7">[8]</ref><ref type="bibr" target="#b8">[9]</ref>, nonlocal means (NLM) <ref type="bibr" target="#b9">[10]</ref><ref type="bibr" target="#b10">[11]</ref><ref type="bibr" target="#b11">[12]</ref>, dictionary learning <ref type="bibr" target="#b12">[13]</ref>, low-rank <ref type="bibr" target="#b13">[14]</ref>, and other techniques. Model based iterative reconstruction (MBIR) takes into account the physical acquisition processes and has been implemented on some current CT scanners <ref type="bibr" target="#b14">[15]</ref>. Although IR methods obtained exciting results, there are two weaknesses. First, on most of modern MDCT scanners, IR techniques have replaced FBP based image reconstruction techniques for radiation dose reduction. However, these IR techniques are vendor-specific since the details of the scanner geometry and correction steps are not available to users and other vendors. Second, there are substantial computational overhead costs associated with popular IR techniques. Fully model-based iterative reconstruction techniques have greater potential for radiation dose reduction but slow reconstruction speed and changes in image appearance limit their clinical applications.</p><p>An alternative for LDCT is post-processing of reconstructed images, which does not rely on raw data. These techniques can be directly applied on LDCT images, and integrated into any CT system. In <ref type="bibr" target="#b15">[16]</ref>, NLM was introduced to take advantage of the feature similarity within a large neighborhood in a reconstructed image. Inspired by the theory of sparse representation, dictionary learning <ref type="bibr" target="#b16">[17]</ref> was adapted for LDCT denoising, and resulted in substantially improved quality abdomen images <ref type="bibr" target="#b17">[18]</ref>. Meanwhile, block-matching 3D (BM3D) was proved efficient for various X-ray imaging tasks <ref type="bibr" target="#b18">[19]</ref><ref type="bibr" target="#b19">[20]</ref><ref type="bibr" target="#b20">[21]</ref>. In contrast to the other two kinds of methods, the noise distribution in the image domain cannot be accurately determined, which prevents users from achieving the optimal tradeoff between structure preservation and noise supersession.</p><p>Recently, deep learning (DL) has generated an overwhelming enthusiasm in several imaging applications, ranging from low-level to high-level tasks from image denoising, deblurring and super resolution to segmentation, detection and recognition <ref type="bibr" target="#b21">[22]</ref>. It simulates the information processing procedure by human, and can efficiently learn highlevel features from pixel data through a hierarchical network framework <ref type="bibr" target="#b22">[23]</ref>.</p><p>Several DL algorithms have been proposed for image restoration using different network models <ref type="bibr" target="#b23">[24]</ref><ref type="bibr" target="#b24">[25]</ref><ref type="bibr" target="#b25">[26]</ref><ref type="bibr" target="#b26">[27]</ref><ref type="bibr" target="#b27">[28]</ref><ref type="bibr" target="#b28">[29]</ref><ref type="bibr" target="#b29">[30]</ref><ref type="bibr" target="#b30">[31]</ref>. As the autoencoder (AE) has a great potential for image denoising, stacked sparse denoising autoencoder (SSDA) and its variant were introduced <ref type="bibr" target="#b23">[24]</ref><ref type="bibr" target="#b24">[25]</ref><ref type="bibr" target="#b25">[26]</ref>. Convolutional neural networks are powerful tools for feature extraction and were applied for image denoising, deblurring and super resolution <ref type="bibr" target="#b26">[27]</ref><ref type="bibr" target="#b27">[28]</ref><ref type="bibr" target="#b28">[29]</ref>. Burger et al. <ref type="bibr" target="#b29">[30]</ref> analyzed the performance of multi-layer perception (MLP) as applied to image patches and obtained competitive results as compared to the state-of-the-art methods. Previous studies also applied DL for medical image analysis, such as tissue segmentation <ref type="bibr" target="#b31">[32,</ref><ref type="bibr" target="#b32">33]</ref>, organ classification <ref type="bibr" target="#b33">[34]</ref> and nuclei detection <ref type="bibr" target="#b34">[35]</ref>. Furthermore, reports started emerging on tomographic imaging topics. For example, Wang et al. incorporated a DL-based regularization term into a fast MRI reconstruction framework <ref type="bibr" target="#b35">[36]</ref>. Chen et al. presented preliminary results with a light-weight CNN-based framework for LDCT imaging <ref type="bibr" target="#b36">[37]</ref>. A deeper version using the wavelet transform as inputs was presented <ref type="bibr" target="#b37">[38]</ref> which won the second place in the "2016 NIH-AAPM-Mayo Clinic Low Dose CT Grand Challenge." The filtered back-projection (FBP) workflow was mapped to a deep CNN architecture, reducing the reconstruction error by a factor of two in the case of limitedangle tomography <ref type="bibr" target="#b38">[39]</ref>. An overall perspective was also published on deep learning, or machine learning in general, for tomographic reconstruction <ref type="bibr" target="#b39">[40]</ref>.</p><p>Despite the interesting results on CNN for LDCT, the potential of the deep CNN has not been fully realized. Although some studies involved construction of deeper networks <ref type="bibr" target="#b40">[41,</ref><ref type="bibr" target="#b41">42]</ref>, most image denoising models had limited layers (usually 2~3 layers) since image denoising is considered as a "low-level" task without intention to extract features. This is in clear contrast to high-level tasks such as recognition or detection, in which pooling and other operations are widely used to bypass image details and capture topological structures.</p><p>Inspired by the work of <ref type="bibr" target="#b30">[31]</ref>, we incorporated a deconvolution network <ref type="bibr" target="#b42">[43]</ref> and shortcut connections <ref type="bibr" target="#b40">[41,</ref><ref type="bibr" target="#b41">42]</ref> into a CNN model, which is referred to as a residual encoderdecoder convolutional neural network (RED-CNN). In the second section, the proposed network architecture is described. In the third section, the proposed model is evaluated and validated. In the final section, the conclusion is drawn.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>II. METHODS</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Noise Reduction Model</head><p>Our workflow starts with a straightforward FBP reconstruction from a low-dose scan, and the image denoising problem is restricted within the image domain <ref type="bibr" target="#b36">[37]</ref>. Since the DL-based methods are independent of the statistical distribution of image noise, the LDCT problem can be simplified to the following one. Assuming that mn   XR is a LDCT image  </p><p>where f is regarded as the optimal approximation of 1   , and can be estimated using DL techniques.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Residual Autoencoder Network</head><p>The autoencoder (AE) was originally developed for unsupervised feature learning from noisy inputs, which is also suitable for image restoration. In the context of image denoising, CNN also demonstrated an excellent performance. However, due to its multiple down-sampling operations, some image details can be missed by CNN. For LDCT, here we propose a residual network combining AE and CNN, which has an origin in the work <ref type="bibr" target="#b30">[31]</ref>. Rather than adopting fully-connected layers for encoding and decoding, we use both convolutional and deconvolutional layers in symmetry. Furthermore, different from the typical encoder-decoder structure, residual learning <ref type="bibr" target="#b40">[41]</ref> with shortcuts is included to facilitate the operations of the convolutional and corresponding deconvolutional layers. There are two modifications to the network described in <ref type="bibr" target="#b30">[31]</ref>: (a) the ReLU layers before summation with residuals have been removed to abandon the positivity constraint on learned residuals; and (b) shortcuts have been added to improve the learning process.</p><p>The overall architecture of the proposed RED-CNN network is shown in Fig. <ref type="figure" target="#fig_0">1</ref>. This network consists of 10 layers, including 5 convolutional and 5 deconvolutional layers symmetrically arranged. Shortcuts connect matching convolutional and deconvolutional layers. Each layer is followed by its rectified linear units (ReLU) <ref type="bibr" target="#b43">[44]</ref>. The details about the network are described as follows.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>1) Patch extraction</head><p>DL-based methods need a huge number of samples. This requirement cannot be easily met in practice, especially for clinical imaging. In this study, we propose to use overlapped patches in CT images. This strategy has been found to be effective and efficient, because the perceptual differences of local regions can be detected, and the number of samples are significantly boosted <ref type="bibr" target="#b23">[24,</ref><ref type="bibr" target="#b26">27,</ref><ref type="bibr" target="#b27">28]</ref>. In our experiments, we extracted patches from LDCT and corresponding NDCT images with a fixed size.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>2) Stacked encoders (Noise and artifact reduction)</head><p>Unlike the traditional stacked AE networks, we use a chain of fully-connected convolutional layers as the stacked encoders. Image noise and artifacts are suppressed from low-level to highlevel step by step in order to preserve essential information in the extracted patches. Moreover, since the pooling layer (downsampling) after a convolutional layer may discard important structural details, it is abandoned in our encoder. As a result, there are only two types of layers in our encoder: convolutional layers and ReLU units, and the stacked encoders ()</p><formula xml:id="formula_1">i ei</formula><p>C x can be formulated as ( ) ReLU( ) 0,1..., ,</p><formula xml:id="formula_2">i e i i i i C i N     Wb xx (<label>3</label></formula><formula xml:id="formula_3">)</formula><p>where N is the number of convolutional layers, i W and i b denote the weights and biases respectively,  represents the convolution operator, 0</p><p>x is the extracted patch from the input images, and ( 0) i i  x is the extracted features from the previous layers. ReLU( ) max(0, ) xx  is the activation function. After the stacked encoders, the image patches are transformed into a feature space, and the output is a feature vector N x whose size is N l .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>3) Stacked decoders (Structural detail recovery)</head><p>Although the pooling operation is removed, a serial of convolutions, which essentially act as noise filters, will still diminish the details of input signals. Inspired by the recent results on semantic segmentation <ref type="bibr" target="#b44">[45,</ref><ref type="bibr" target="#b45">46,</ref><ref type="bibr" target="#b46">47]</ref> and biomedical image segmentation <ref type="bibr" target="#b47">[48,</ref><ref type="bibr" target="#b48">49]</ref>, deconvolutional layers are integrated into our model for recovery of structural details, which can be seen as image reconstruction from extracted features. We use a chain of fully-connected deconvolutional layers to form the stacked decoders for image reconstruction. Since the encoders and decoders should appear in pair, the convolutional and deconvolutional layers are symmetric in the proposed network. To ensure the input and output of the network match exactly, the convolutional and deconvolutional layers must have the same kernel size. Note that the data flow through the convolutional and deconvolutional layers in our framework follows the rule of "FILO" (First In Last Out). As demonstrated in Fig. <ref type="figure" target="#fig_0">1</ref>, the first convolution layer corresponds to the last deconvolutional layer, the last convolution layer corresponds to the first deconvolutional layer, and so on. In other words, this architecture is featured by the symmetry of paired convolution and deconvolution layers.</p><p>There are two types of layers in our decoder network: deconvolution and ReLU. Thus, the stacked decoders ()</p><formula xml:id="formula_4">i di D y can be formulated as: '' ( ) ReLU( ) 0,1..., , i d i i i i D i N     y W y b (<label>4</label></formula><formula xml:id="formula_5">)</formula><p>where N is the number of deconvolutional layers, is the reconstructed feature vector from the previous deconvolutional layer, and 0 y is the reconstructed patch. After stacked decoding, image patches are reconstructed from features, and can be assembled to reconstruct a denoised image.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>4) Residual compensation</head><p>Like the prior art methods <ref type="bibr" target="#b23">[24,</ref><ref type="bibr" target="#b24">25]</ref>, convolution will eliminate some image details. Although the deconvolutional layers can recover some of the details, when the network goes deeper this inverse problem becomes more ill-posed, and the accumulated loss could be quite unsatisfactory for image reconstruction. In addition, when the network depth increases the gradient diffusion could make the network difficult to train.</p><p>To address the above two issues, similar to deep residual learning <ref type="bibr" target="#b40">[41,</ref><ref type="bibr" target="#b41">42]</ref> we introduce a residual compensation mechanism into the proposed network. Instead of mapping the input to the output solely by the stacked layers, we adopt a residual mapping, as shown in Fig. <ref type="figure">2</ref>. Defining the input as I and the output as O , the residual mapping can be denoted as () F I O I , and we use stacked layers to fit this mapping. Once the residual mapping is built, we can reconstruct the original mapping as ( )</p><formula xml:id="formula_6">( ) R I O F I I  </formula><p> . Consequently, we transform the direct mapping problem to a residual mapping problem.</p><p>There are two benefits associated with the residual mapping. First, it is easier to optimize the residual mapping than optimizing the direct mapping. In other words, it helps avoid the gradient vanishing during training when the network is deep. For example, it would be much easier to train an identity mapping network by pushing the residual to zero than fitting an identity mapping directly. Second, since only the residual is processed by the convolutional and deconvolutional layers, more structural and contrast details can be preserved in the output of the deconvolutional layers, which can significantly enhance the LDCT imaging performance.</p><p>The use of shortcut connections in <ref type="bibr" target="#b40">[41,</ref><ref type="bibr" target="#b41">42]</ref> was to solve the difficulty in training so that the shortcut connections were only applied across convolutional layers of the same size. In our work, shortcut connections were used for both preservation of structural details and facilitation of training deeper networks. Furthermore, the symmetric structure of convolution and deconvolution layer pairs was also utilized to keep more details while suppressing image noise and artifacts. The CNN layers in <ref type="bibr" target="#b40">[41]</ref> are essentially feedforward long short-term memories (LSTMs) without gates, while our RED-CNN network is in general not composed of the standard feedforward LSTMs.</p><p>In <ref type="bibr" target="#b46">[47]</ref> and its variants <ref type="bibr" target="#b47">[48,</ref><ref type="bibr" target="#b48">49]</ref>, both shortcut connection and deconvolution were used for segmentation. High resolution features were combined with an up-sampled output to improve the image classification. Besides shortcut connection and deconvolution, there are the following new features of the proposed RED-CNN over the networks in <ref type="bibr" target="#b46">[47]</ref><ref type="bibr" target="#b47">[48]</ref><ref type="bibr" target="#b48">[49]</ref>:</p><p>(i). The idea of the autoencoder, which was originally designed for training with noisy samples, was introduced into our model, and convolution and deconvolution layers appeared in pairs;</p><p>(ii). To avoid losing details, pooling layer was discarded; (iii). Convolution layers can be seen as noise filters in our application, but filtering leads to loss in details. Deconvolution and shortcutting in our model were used for detail preservation, and in the experiment section we will separately analyze the improvements due to each of these components. Furthermore, the strides of convolution and deconvolution layers in our model were fixed to 1 to avoid down-sampling.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>5) Training</head><p>The proposed network is an end-to-end mapping from lowdose CT images to normal-dose images. Once the network is configured, the set of parameters, of the convolutional and deconvolutional layers should be estimated to build the mapping function M . The estimation can be achieved by minimizing the loss ( ; ) FD between the estimated CT images and the reference NDCT images X . </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Given</head><formula xml:id="formula_7">N ii i F D M N      XY (5)</formula><p>In this study, the loss function was optimized by Adam <ref type="bibr" target="#b49">[50]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>III. EXPERIMENTAL DESIGN AND RESULTS</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Data Sources 1) Simulated data</head><p>The normal dose dataset included 7,015 normal-dose CT images of 256×256 pixels per image from 165 patients downloaded from the National Biomedical Imaging Archive (NBIA). Different parts of the human body were included for diversity. Some typical images are in Fig. <ref type="figure">3</ref>. The corresponding LDCT images were produced by adding Poisson noise into the sinograms simulated from the normal-dose images. With the assumed use of a monochromatic source, the projection '' ={ , , , } where i z is the measurement along the i -th ray path.  . Siddon's raydriven method <ref type="bibr" target="#b50">[51]</ref> was used to generate the projection data in fan-beam geometry. The source-to-rotation center distance was 40 cm while the detector-to-rotation center was 40 cm. The image region was set to 20 cm × 20 cm. The detector width was 41.3 cm containing 512 detector elements. There were 1,024 viewing angles uniformly distributed over a full scan range.</p><formula xml:id="formula_8">i i i i  W b W b Fig.2.</formula><p>Since direct processing of an entire CT image is intractable, RED-CNN was applied to image patches. Our method benefits from the patch extraction since patch-based processing represents the local details required for optimal denoising and the number of samples were greatly increased for the training purpose. Deep learning methods require a large amount of training samples, but collecting medical images is usually limited by the complicated formalities addressing multiple factors such as the patient's privacy.</p><p>In the training step, 200 normal-dose and corresponding simulated low-dose images were randomly selected as the training set. Also, 100 image pairs were randomly selected as the testing set. Images from the patients in the training set were not in the testing set.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>2) Clinical data</head><p>To validate the clinical performance of RED-CNN, a real clinical database was used, which was authorized by Mayo Clinics for "the 2016 NIH-AAPM-Mayo Clinic Low Dose CT Grand Challenge". The dataset contained 2,378 3mm thickness full and quarter dose 512×512 CT images from 10 patients <ref type="bibr" target="#b36">[37]</ref>. The network was trained with a subset of full dose and quarter dose image pairs. The rest of the image pairs were respectively used as the testing set and the gold standard. For fairness, crossvalidation was utilized in the testing phase. While testing on CT images from each patient, the images from the other 9 patients were involved in the training phase.</p><p>There are three reasons why we used both simulated and clinical data. First, the database from NBIA is more diverse than that at Mayo. It includes more body parts than the database at Mayo, and therefore more realistically reflects clinical imaging applications. Second, different from the clinical dataset, which has both full-dose datasets and their corresponding quarter-dose counterparts, the low-dose images from NBIA were simulated by adding Poisson noise into the simulated sinograms. By doing so, we can control the noise levels of the data to simulate different doses, and we can evaluate the robustness of our method as described in III D <ref type="bibr" target="#b4">(5)</ref>. Third, the experiments on simulated data were focused on the improvement of image quality with our model while the experiments with clinical data targeted clinical tasks, such as low contrast lesion detection.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Parameter selection</head><p>The patch size  was set to 55×55 with the sliding interval of 4 pixels. After the patch extraction, the number of training patch pairs reached 10 6 . Furthermore, three kinds of transformation operations, including rotation (by 45 degrees), flipping (vertical and horizontal) and scaling (scale factors were 2 and 0.5), were used for data augmentation. The network was implemented in Caffe. In our experiments, we evaluated several parameter combinations and finalized the parameter settings as follows. The base learning rate was set to 10 -4 , and slowly decreased down to 10 -5 . The convolution and deconvolution kernels were initialized with random Gaussian distributions with zero mean and standard deviation 0.01. The filter number of last layer was set to 1 and the others were set to 96. The kernel size of all layers was set to 5×5. The strides of convolution and deconvolution were set to 1 with no padding. All the experiments were performed with MATLAB 2015b on a PC (Intel i7 6700K CPU and 16 GB RAM). The training stage is time-consuming for traditional CPU implementation. A common way for acceleration is to work in a parallel manner. In our work, the training of RED-CNN was performed on a graphic processing unit card (GTX 1080). Although the training was done on patches, the proposed network can process images of arbitrary sizes. All the testing images were simply fed into the network, without decomposition.</p><p>The three metrics, including the root mean square error (RMSE), peak signal to noise ratio (PSNR) and structural similarity index measure (SSIM), were chosen for quantitative assessment of image quality.</p><p>Five different state-of-the-art methods were compared against our RED-CNN, including TV-POCS <ref type="bibr" target="#b5">[6]</ref>, K-SVD <ref type="bibr" target="#b17">[18]</ref>, BM3D <ref type="bibr" target="#b19">[20]</ref>, CNN10 <ref type="bibr" target="#b36">[37]</ref>, and KAIST-Net <ref type="bibr" target="#b37">[38]</ref>. Dictionary learning and BM3D are two most popular image-based denoising methods already applied for LDCT. ASD-POCS is a widely used iterative reconstruction method under the TV regularization. CNN10 is a simplified version of the proposed RED-CNN without shortcuts and deconvolutional layers. It also can be viewed as a deeper version of the CNN-based LDCT restoration model <ref type="bibr" target="#b36">[37]</ref>. KAIST-Net is the most recently proposed CNN-based LDCT denoising method. It can be considered as a deepened variant of the lightweight CNN model <ref type="bibr" target="#b36">[37]</ref>. The parameters of these competing methods were set per the suggestions from the original papers.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Experimental Results 1) Simulated data</head><p>Two representative slices from the testing dataset were used to demonstrate the performance of RED-CNN, which are through the chest and abdominal regions respectively. It can be seen that the normal-dose images from different scan protocols contained different noise levels. Fig. <ref type="figure" target="#fig_3">4</ref> shows our results from the chest image. In Fig. <ref type="figure" target="#fig_3">4</ref>(b), there was high image noise and streaking artifacts adjacent to structures with high attenuation coefficients such as bones. All applied methods suppressed image noise to various degrees. However, in Fig. <ref type="figure" target="#fig_3">4</ref>(c), TV-POCS suffered from a blocky effect and also smoothened some important small structures in the lungs. K-SVD and BM3D preserved more details than TV-POCS, but there were artifacts near the bones. CNN10, KAIST-Net and RED-CNN eliminated most image noise and artifacts while preserving the structural features better than the other methods. Furthermore, RED-CNN discriminated low contrast regions in the best way. Fig. <ref type="figure" target="#fig_4">5</ref> shows the zoomed images over a region of interest (ROI). Clearly, the blood vessels in the lungs, highlighted by the blue arrow, were smoothened by TV-POCS in Fig. <ref type="figure" target="#fig_4">5(c</ref>). The other methods could identify these details to different extents, and the details were retained without blurring in Fig. <ref type="figure" target="#fig_4">5</ref>(h). Meanwhile, in Fig. <ref type="figure" target="#fig_4">5(d)</ref> and (e) the streaking artifacts were evident near the bone, marked by the red arrow. To further show the merits of RED-CNN, the absolute difference images relative to the original image are in Fig. <ref type="figure" target="#fig_5">6</ref>. It can be clearly observed that RED-CNN yielded the smallest difference from the original normal-dose image, preserving all details and suppressing most noise and artifacts.</p><p>For quantitative evaluation, four ROIs were chosen as highlighted by red dotted boxes in Fig. <ref type="figure" target="#fig_3">4(a)</ref>. The results are Fig. <ref type="figure" target="#fig_6">7</ref>. The quantitative results followed similar trends per visual inspection. The RED-CNN had the lowest RMSE and the highest PSNR/SSIM for all the ROIs. Fig. <ref type="figure" target="#fig_7">8</ref> presents the results from the abdominal image. Since the image quality of the original normal-dose image (Fig. <ref type="figure" target="#fig_7">8(a)</ref>) is worse than the chest image (Fig. <ref type="figure" target="#fig_3">4(a)</ref>), the simulated LDCT image suffered from severe deterioration and many structures cannot be distinguished in Fig. <ref type="figure" target="#fig_7">8(b)</ref>. TV-POCS and K-SVD cannot recover the image well. The blocky effect appeared in Fig. <ref type="figure" target="#fig_7">8(c</ref>). BM3D eliminated most noise but the artifacts close to the spine were evident. CNN10, KAIST-Net and RED-CNN suppressed most of the noise and artifacts but the result in Fig. <ref type="figure" target="#fig_8">9</ref>(f) and (g) suffered a bit from over-smoothing, which is consistent to the previous results with a lightweight CNN as reported in <ref type="bibr" target="#b36">[37]</ref>. The red arrows indicate several noticeable structural differences between different methods. The linear high attenuation structure in the liver likely representing a   contrast enhanced blood vessel was best retained by RED-CNN in Fig. <ref type="figure" target="#fig_8">9</ref>. The low attenuation pseudo lesions noted in the posterior aspect of the liver on other techniques were not seen on RED-CNN. The thin and subtle right adrenal gland was best appreciated on the RED-CNN image as well. Finally, margins of different tissues were also better delineated and retained on the RED-CNN image.</p><p>The quantitative results for the whole images restored using these methods are listed in Table <ref type="table" target="#tab_3">I</ref>. It can be seen that RED-CNN obtained the best scores on all the indexes.</p><p>Table <ref type="table" target="#tab_3">II</ref> gives the mean measurements for all the 100 images from the testing dataset. Again, it can be seen that the proposed RED-CNN outperformed the state-of-the-art methods in terms of each of the metrics.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>2) Clinical data</head><p>Two representative slices from real clinical CT scans were chosen to evaluate the performance of the proposed RED-CNN. The results are in Figs. <ref type="figure" target="#fig_10">10</ref><ref type="figure" target="#fig_11">11</ref><ref type="figure" target="#fig_12">12</ref><ref type="figure" target="#fig_13">13</ref>. In Figs. <ref type="figure" target="#fig_10">10</ref> and<ref type="figure" target="#fig_12">12</ref>, it is clear that RED-CNN delivered the best performance in terms of both noise suppression and structure preservation. In the zoomed parts in Figs. <ref type="figure" target="#fig_11">11</ref> and<ref type="figure" target="#fig_13">13</ref>, the low-contrast liver lesions highlighted by the red circles were processed using our and others' methods, and our proposed method gave the best image quality. TV-POCS and K-SVD generated some artifacts and lowered the detectability of lesions. BM3D blurred the lowcontrast lesions. All the methods except KAIST-Net and RED-CNN could not distinguish the contrast enhanced blood vessel marked by the red arrow in Fig. <ref type="figure" target="#fig_11">11</ref>. Although KAIST-Net had a performance similar to that of our method, KAIST-Net smoothened the low contrast lesions in Fig. <ref type="figure" target="#fig_11">11</ref>(g) while our RED-CNN preserved the edges very well in Fig. <ref type="figure" target="#fig_11">11(h)</ref>. Meanwhile, two tiny focal low attenuation lesions were hard to detect in Fig. <ref type="figure" target="#fig_12">12</ref>(f) and (g) but they can be noticed in Fig. <ref type="figure" target="#fig_12">12(h)</ref>.</p><p>Table <ref type="table" target="#tab_5">III</ref> summarizes the quantitative results from the aforementioned two images. RED-CNN gave better performance in terms of most of the metrics than the other methods. Table <ref type="table" target="#tab_6">IV</ref> shows the quantitative results on the full cross validation in terms of means ± SDs (average scores ± standard deviations). All our visual observations are supported by the quantitative evaluation as shown in Table <ref type="table" target="#tab_6">IV</ref>.  For qualitative evaluation, 10 LDCT images with lesions and the processed images using different methods were selected for a reader study. Artifact reduction, noise suppression, contrast retention, lesion discrimination, and overall quality were used as subjective indicators on the five-point scale (1 = unacceptable and 5 = excellent). Two radiologists (R1 and R2) with 6 and 8 years of clinical experience respectively evaluated these images independently to provide their scores. The NDCT images were used as the gold standard. For each set of images, the scores were reported as means ± SDs (average scores of the two radiologists ±standard deviations). The student t-test with p&lt;0.05 was performed. The statistical results are in Table <ref type="table">V</ref>. For all the five indicators, the LDCT images had the lowest scores due to their severe image quality degradation. All the LDCT methods significantly improved the scores. KAIST-Net and RED-CNN produced substantially higher scores than the other methods, and RED-CNN performed slightly better than KAIST-Net and ran significantly faster than KAIST-Net in both the training and testing processes.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D. Model and Performance Trade-Offs</head><p>In this subsection, several critical factors of the proposed RED-CNN were examined, including deconvolutional decoder, shortcut connection, number of the layers, patch size and robustness with respect to the training and testing datasets. Computational costs were also discussed. The data used to plot the curves in the following sections were randomly selected from the NBIA dataset (average values from 40 images).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>1) Deconvolutional Decoder</head><p>Different from the traditional convolutional layers, deconvolutional layers, also referred to as learnable upsampling layers, can produce multiple outputs with a single input. This architecture has been successful in sematic segmentation coupled with convolutional layers <ref type="bibr" target="#b44">[45]</ref><ref type="bibr" target="#b45">[46]</ref><ref type="bibr" target="#b46">[47]</ref><ref type="bibr" target="#b47">[48]</ref><ref type="bibr" target="#b48">[49]</ref>. With traditional fully-connected CNNs <ref type="bibr" target="#b26">[27,</ref><ref type="bibr" target="#b27">28,</ref><ref type="bibr" target="#b36">37]</ref>, some important details could be lost in the convolution. That is why the number of layers of these CNNs are usually less than 5, for low-level tasks, such as denoising, deblurring and super resolution <ref type="bibr" target="#b23">[24]</ref><ref type="bibr" target="#b24">[25]</ref><ref type="bibr" target="#b25">[26]</ref><ref type="bibr" target="#b26">[27]</ref><ref type="bibr" target="#b27">[28]</ref><ref type="bibr" target="#b28">[29]</ref><ref type="bibr" target="#b29">[30]</ref>. In our proposed model, we have balanced the conventional CNN layers with an equal number of deconvolutional layers, forming the network capable of bringing the details back to the image of the original size. In our network, pooling and unpooling operations are avoided to keep structural information in the images. We assessed the performance of the networks with and without deconvolutional layers as shown in Fig. <ref type="figure" target="#fig_3">14</ref>. It can be seen that our model with the deconvolution mechanism performed better than the fully-convolutional counterpart.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>2) Shortcut Connection</head><p>Shortcut connection is another trick we used to improve the structural preservation. It has been proven useful in the both high-and low-level tasks, such as image recognition <ref type="bibr" target="#b39">[40,</ref><ref type="bibr" target="#b40">41,</ref><ref type="bibr" target="#b46">[47]</ref><ref type="bibr" target="#b47">[48]</ref><ref type="bibr" target="#b48">[49]</ref> and image restoration <ref type="bibr" target="#b30">[31]</ref>. We evaluated the impact of shortcuts in the proposed RED-CNN. The results are in Fig. <ref type="figure" target="#fig_4">15</ref>. The model with shortcuts produced better PSNR and RMSE values and converged more rapidly.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>3) Number of Layers</head><p>Recent studies suggested that deeper network architectures, especially CNN-based models, produced better performance for image recognition <ref type="bibr" target="#b40">[41,</ref><ref type="bibr" target="#b41">42]</ref>. Here we investigated the trade-off between performance and the number of layers by testing the use of 10, 20 and 30 layers. The quantitative results are in Fig. <ref type="figure" target="#fig_5">16</ref>. It can be seen that the differences were not easily noticeable. This observation is consistent to the statement in <ref type="bibr" target="#b27">[28,</ref><ref type="bibr" target="#b30">31]</ref> that deeper networks do not always result in better performance in low-level image processing tasks. Although the utilization of shortcut connections enables much more layers than the preliminary explorations <ref type="bibr" target="#b27">[28,</ref><ref type="bibr" target="#b36">37]</ref>, it seems that the enhanced performance by adding more layers is limited, and better understanding for training dynamics of deep networks may help overcome this bottleneck.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>4) Patch Size</head><p>For CNN-based image restoration <ref type="bibr" target="#b23">[24,</ref><ref type="bibr" target="#b27">28,</ref><ref type="bibr" target="#b30">31]</ref>, in the training stage image patch pairs were used, and in the testing stage the whole images were directly fed into the trained network. Training with patches can enhance the detectability of perceptual differences of local regions, and the amount of samples are significantly boosted. Once the filters in each layer are trained well, due to the property of convolution operators,   there is no difference between different patch sizes with which the network is fed. Here we tried to sense the impact with different training patch sizes. The result is in Fig. <ref type="figure" target="#fig_6">17</ref>. We increased the patch size from 55 to 100 and there was no significant difference shown up. Based on this observation, in our experiments we fixed the patch size to 55×55 for better trade-off between training time and imaging performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>5) Performance Robustness</head><p>In our experiments, the noise level was fixed, and the corresponding network was trained under the assumption of a uniform noise level. In practice, it is impossible to train with different parameter sets subject to different noise levels. Since the IR methods usually have several parameters, it is inconvenient to explore the possible parameter space for optimal image quality. We believe that the proposed RED-CNN model is robust for different noise levels. To show the robustness of RED-CNN, several combinations of noise levels in the training and testing datasets were to generate the quantitative results in Table VI. In this table, the training dataset of CNN10, KAIST-Net and RED-CNN were made for CNN+ obtained the best performance in most of the situations, which means that if an accurate noise level cannot be determined, a good solution is to train the network with mixed data at possible noise levels. In the column 'RED-CNN', it is seen that even if training is done with a single noise level, RED-CNN is still competitive in handling the cases of inconsistent noisy data.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>6) Computational Cost</head><p>The computational cost is another advantage of deep learning. Although the training is time-consuming, it can be improved with GPU. For the dataset involved in our experiments, training took about 4 hours for about 10 6 patches and 12 hours for about 10 7 patches. CNN10 has the same number of layers as that of RED-CNN, but without the shortcut, it took more time in training. It ran 6 hours about 10 6 patches and 15 hours for about 10 7 patches. KAIST-Net has a complex architecture with 26 layers and 15 channel inputs, and as a result the training time was much longer (also implemented in Caffe). For 10 6 and 10 7 patches, it took 12 hours and 30 hours respectively. The other methods, especially for iterative reconstruction, do not need a training process, but the execution time is much longer than CNN10, KAIST-Net and RED-CNN. In this study, the average  execution times for ASD-POCS, K-SVD, BM3D, CNN10, KAIST-Net and RED-CNN are <ref type="bibr">21.36, 38.45, 4.22, 3.22, 30.</ref>22 and 3.68 seconds respectively. Actually, after the network is trained offline, the proposed model is much more efficient than any other methods in terms of execution time.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>IV. CONCLUSION</head><p>In brief, we have designed a symmetrical convolutional and deconvolutional neural network, aided by shortcut connections. Two well-known have been utilized to evaluate and validate the performance of our proposed RED-CNN in comparison the state of the art methods. The simulated and clinical results have demonstrated a great potential of deep learning for noise suppression, structural preservation, and lesion detection at a high computational speed. In the future, we plan to optimize RED-CNN, extend it to higher dimensional cases such as 3D reconstruction, dynamic/spectral CT reconstruction, and adapt the ideas to other imaging tasks or even other imaging modalities.          </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 1 .</head><label>1</label><figDesc>Fig.1. Overall architecture of our proposed RED-CNN network.</figDesc><graphic coords="2,104.64,51.12,401.28,195.60" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head></head><label></label><figDesc>photons and denoted as 5 0 10 , 1,...,</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Fig. 4 .</head><label>4</label><figDesc>Fig. 4. Results from the chest image for comparison. (a) NDCT, (b) LDCT, (c) TV-POCS, (d) K-SVD, (e) BM3D, (f) CNN10, (g) KAIST-Net, and (h) RED-CNN. The blue box indicates the region zoomed in Fig. 5. The red dotted boxes define several ROIs.</figDesc><graphic coords="6,68.40,50.76,207.24,380.04" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Fig. 5 .</head><label>5</label><figDesc>Fig. 5. Zoomed parts over the region of interest (ROI) marked by the blue box in Fig. 4(a). (a) NDCT, (b) LDCT, (c) TV-POCS, (d) K-SVD, (e) BM3D, (f) CNN10, (g) KAIST-Net, and (h) RED-CNN ((a)-(h) from Fig. 4(a)-(h)). The arrows indicate two regions for visual differences.</figDesc><graphic coords="6,314.40,49.56,250.80,126.72" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Fig. 6 .</head><label>6</label><figDesc>Fig. 6. Absolute difference images relative to the NDCT image. (a) LDCT, (b) TV-POCS, (c) K-SVD, (d) BM3D, (e) CNN10, (f) KAIST-Net, and (g) RED-CNN.</figDesc><graphic coords="6,313.56,221.16,250.44,284.76" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Fig. 7 .</head><label>7</label><figDesc>Fig. 7. Performance comparison of the six algorithms over the ROIs marked in Fig. 4(a) in terms of the selected metrics.</figDesc><graphic coords="7,69.36,50.76,208.32,396.84" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Fig. 8 .</head><label>8</label><figDesc>Fig. 8. Results from the abdominal image for comparison. (a) NDCT, (b) LDCT, (c) TV-POCS, (d) K-SVD, (e) BM3D, (f) CNN10, (g) KAIST-Net, and (h) RED-CNN. The arrows indicate three regions to observe the visual effects.</figDesc><graphic coords="7,328.44,50.76,219.96,432.00" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Fig. 9 .</head><label>9</label><figDesc>Fig. 9. Zoomed ROI images from Fig. 8. (a) NDCT, (b) LDCT, (c) TV-POCS, (d) K-SVD, (e) BM3D, (f) CNN10, (g) KAIST-Net, and (h) RED-CNN ((a)-(h) from Fig. 8(a)-(h)). The arrows indicate two regions containing features revealed differently by the competing algorithms.</figDesc><graphic coords="8,47.16,50.64,250.80,127.20" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head></head><label></label><figDesc>KAIST-Net+ and RED-CNN+ denote the same networks as CNN10, KAIST-Net and RED-CNN with randomly mixed training data at different noise levels for  . It is clear that RED-</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head>Fig. 10 .</head><label>10</label><figDesc>Fig. 10. Results from the abdominal image with a metastasis in the liver for comparison. (a) NDCT, (b) LDCT, (c) TV-POCS; (d) K-SVD, (e) BM3D, (f) CNN10, (g) KAIST-Net, and (h) RED-CNN.</figDesc><graphic coords="9,101.04,50.76,409.20,188.76" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_11"><head>Fig. 11 .</head><label>11</label><figDesc>Fig. 11. Zoomed parts from Fig. 10. (a) NDCT, (b) LDCT, (c) TV-POCS, (d) K-SVD, (e) BM3D, (f) CNN10, (g) KAIST-Net, and (h) RED-CNN. The circle indicates the lesion while the arrow points to the contrast enhanced blood vessel.</figDesc><graphic coords="9,91.92,264.00,424.80,172.20" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_12"><head>Fig. 12 .</head><label>12</label><figDesc>Fig. 12. Results from the abdominal image with two focal fatty sparings in the liver for comparison. (a) NDCT, (b) LDCT, (c) TV-POCS, (d) K-SVD, (e) BM3D, (f) CNN10, (g)KAIST-Net, and (h) RED-CNN.</figDesc><graphic coords="10,76.44,50.64,458.04,208.08" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_13"><head>Fig. 13 .</head><label>13</label><figDesc>Fig. 13. Zoomed parts from Fig. 12. (a) NDCT, (b) LDCT, (c) TV-POCS, (d) K-SVD, (e) BM3D, (f) CNN10, (g) KAIST-Net, and (h) RED-CNN.</figDesc><graphic coords="10,114.60,282.00,383.16,161.52" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_14"><head></head><label></label><figDesc>Fig. 14. PSNR and RMSE values on the testing dataset during training. Our network exhibits a better performance than CNN10. The display ranges of PSNR and RMSE are [20 45] and [0 0.04] respectively.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_15"><head></head><label></label><figDesc>Fig. 15. PSNR and RMSE values on the testing dataset during training. Our network with shortcut connections exhibits a better performance than the one without shortcuts. The display ranges of PSNR and RMSE are [20 45] and [0 0.04] respectively.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_16"><head></head><label></label><figDesc>Fig. 16. PSNR and RMSE values on the testing dataset during training based on different numbers of layers. The display ranges of PSNR and RMSE are [40 44] and [0.006 0.013] respectively.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_18"><head></head><label></label><figDesc>. PSNR and RMSE values on the testing dataset during training based on different patch sizes. The display ranges of PSNR and RMSE are [40 44] and [0.006 0.013] respectively.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>This article has been accepted for publication in a future issue of this journal, but has not been fully edited. Content may change prior to final publication. Citation information: DOI 10.1109/TMI.2017.2715284, IEEE Transactions on Medical Imaging</figDesc><table><row><cell>and</cell><cell cols="2"> YR</cell><cell>mn </cell><cell cols="2">is a corresponding normal dose CT (NDCT)</cell></row><row><cell cols="6">image, the relationship between them can be formulated as</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>() XY  </cell><cell>(1)</cell></row><row><cell>where</cell><cell></cell><cell cols="3">: m RR n  m </cell><cell>n</cell><cell>denotes the complex degradation</cell></row><row><cell cols="6">process involving quantum noise and other factors. Then, the</cell></row><row><cell cols="6">problem can be transformed to seek a function</cell><cell>f :</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">arg min || ( ) f XY </cell><cell>2 2 ||</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>f</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head></head><label></label><figDesc>This article has been accepted for publication in a future issue of this journal, but has not been fully edited. Content may change prior to final publication. Citation information: DOI 10.1109/TMI.2017.2715284, IEEE Transactions on Medical Imaging measurements from a CT scan follow the Poisson distribution, which can be expressed as</figDesc><table><row><cell>i z</cell><cell>Poisson</cell><cell> b e i</cell><cell> r i , 1,..., i   i l</cell><cell>I</cell><cell>(6)</cell></row></table><note><p><p>Shortcut in the residual compensation structure. Fig.3. Examples from the normal-dose CT image dataset.</p>0278-0062 (c) 2016 IEEE. Personal use is permitted, but republication/redistribution requires IEEE permission. See http://www.ieee.org/publications_standards/publications/rights/index.html for more information.</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>TABLE I</head><label>I</label><figDesc></figDesc><table><row><cell cols="4">QUANTITATIVE RESULTS ASSOCIATED WITH DIFFERENT</cell></row><row><cell cols="4">ALGORITHMS FOR THE ABDOMINIAL IMAGE.</cell></row><row><cell></cell><cell>PNSR</cell><cell>RMSE</cell><cell>SSIM</cell></row><row><cell>LDCT</cell><cell>34.3094</cell><cell>0.0193</cell><cell>0.8276</cell></row><row><cell>TV-POCS</cell><cell>37.5485</cell><cell>0.0133</cell><cell>0.8825</cell></row><row><cell>K-SVD</cell><cell>38.3841</cell><cell>0.0120</cell><cell>0.9226</cell></row><row><cell>BM3D</cell><cell>38.9903</cell><cell>0.0112</cell><cell>0.9295</cell></row><row><cell>CNN10</cell><cell>38.9907</cell><cell>0.0104</cell><cell>0.9288</cell></row><row><cell>KAIST-Net</cell><cell>38.9908</cell><cell>0.0102</cell><cell>0.9283</cell></row><row><cell>RED-CNN</cell><cell>39.1959</cell><cell>0.0097</cell><cell>0.9339</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>TABLE III QUANTITATIVE</head><label>III</label><figDesc>RESULTS ASSOCIATED WITH DIFFERENT ALGORITHMS FOR FIGS. 10 AND 12.</figDesc><table><row><cell></cell><cell></cell><cell>Fig. 10</cell><cell></cell><cell></cell><cell>Fig. 12</cell><cell></cell></row><row><cell></cell><cell>PNSR</cell><cell>RMSE</cell><cell>SSIM</cell><cell>PNSR</cell><cell>RMSE</cell><cell>SSIM</cell></row><row><cell>LDCT</cell><cell>40.2252</cell><cell>0.0097</cell><cell>0.9292</cell><cell>39.4243</cell><cell>0.0107</cell><cell>0.9145</cell></row><row><cell>TV-POCS</cell><cell>42.582</cell><cell>0.0074</cell><cell>0.9649</cell><cell>42.2391</cell><cell>0.0077</cell><cell>0.9624</cell></row><row><cell>K-SVD</cell><cell>43.8516</cell><cell>0.0064</cell><cell>0.9678</cell><cell>43.6871</cell><cell>0.0065</cell><cell>0.9668</cell></row><row><cell>BM3D</cell><cell>44.0152</cell><cell>0.0063</cell><cell>0.9696</cell><cell>43.7136</cell><cell>0.0065</cell><cell>0.9672</cell></row><row><cell>CNN10</cell><cell>45.1678</cell><cell>0.0057</cell><cell>0.9712</cell><cell>44.2286</cell><cell>0.0061</cell><cell>0.9698</cell></row><row><cell>KAIST-Net</cell><cell>45.1845</cell><cell>0.0053</cell><cell>0.9735</cell><cell>44.3247</cell><cell>0.0058</cell><cell>0.9708</cell></row><row><cell>RED-CNN</cell><cell>45.2616</cell><cell>0.0055</cell><cell>0.9764</cell><cell>44.6466</cell><cell>0.0059</cell><cell>0.9718</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>TABLE IV QUANTITATIVE</head><label>IV</label><figDesc>RESULTS (MEAN±SDs) ASSOCIATED WITH DIFFERENT ALGORITHMS ON THE FULL CROSS VALIDATION PNSR RMSE SSIM LDCT 39.4314±1.5206 0.0109±0.0021 0.9122±0.0280 TV-POCS 41.7496±1.1522 0.0083±0.0012 0.9535±0.0143 K-SVD 42.7203±1.4260 0.0074±0.0014 0.9531±0.0167 BM3D 42.7661±1.0471 0.0073±0.0009 0.9563±0.0125 CNN10 43.6561±1.1323 0.0066±0.0009 0.9664±0.0100 KAIST-Net 43.9668±1.2169 0.0064±0.0009 0.9688±0.0110 RED-CNN 44.</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>4187±1.2118 0.0060±0.0009 0.9705±0.0087 TABLE</head><label></label><figDesc>V STATISTICAL ANALYSIS OF SUBJECTIVE QUALITY SCORES FOR DIFFERENT ALGORITHMS (MEAN ± SDs). 55±0.26 1.88±0.33* 2.47±0.28* 2.74±0.31* 2.94±0.28* 3.39±0.24 3.45±0.24 3.50±0.23 Noise suppression 3.67±0.28 2.04±0.31* 2.59±0.32* 2.90±0.35* 3.11±0.29* 3.55±0.28 3.57±0.35 3.64±0.21 Contrast retention 3.22±0.22 1.95±0.34* 2.27±0.29* 2.77±0.35* 2.84±0.24* 3.01±0.32 3.08±0.24 3.17±0.28 Lesion discrimination 3.24±0.23 1.75±0.29* 2.24±0.30* 2.67±0.32* 2.70±0.33* 3.15±0.24 3.12±0.28 3.21±0.28 Overall quality 3.39±0.24 1.91±0.32* 2.33±0.34* 2.64±0.29* 2.72±0.29* 2.99±0.28 3.08±0.26 3.25±0.21 * indicates P &lt; 0.05, which means significantly different.</figDesc><table><row><cell></cell><cell>NDCT</cell><cell>LDCT</cell><cell>TV-POCS</cell><cell>K-SVD</cell><cell>BM3D</cell><cell>CNN10</cell><cell>KAIST-Net RED-CNN</cell></row><row><cell>Artifact reduction</cell><cell>3.</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>TABLE VI QUANTITATIVE</head><label>VI</label><figDesc>RESULTS (MEAN) ASSOCIATED WITH DIFFERENT ALGORITHMS FOR VARIOUS COMBINATIONS OF NOISE LEVELS.</figDesc><table><row><cell cols="2">Noise level of testing</cell><cell></cell><cell>TV-</cell><cell>K-SVD</cell><cell>BM3D</cell><cell cols="2">CNN10 CNN10+</cell><cell>KAIST-</cell><cell>KAIST-</cell><cell>RED-</cell><cell>RED-</cell></row><row><cell cols="2">data</cell><cell></cell><cell>POCS</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>Net</cell><cell>Net+</cell><cell>CNN</cell><cell>CNN+</cell></row><row><cell></cell><cell></cell><cell>PSNR</cell><cell>44.8030</cell><cell cols="3">44.0567 44.2798 44.4584</cell><cell>44.4755</cell><cell>44.5542</cell><cell>44.9645</cell><cell>44.8947</cell><cell>45.1584</cell></row><row><cell cols="2">5 5 10 b  0</cell><cell>RMSE</cell><cell>0.0061</cell><cell>0.0064</cell><cell>0.0063</cell><cell>0.0062</cell><cell>0.0061</cell><cell>0.0061</cell><cell>0.0058</cell><cell>0.0058</cell><cell>0.0056</cell></row><row><cell></cell><cell></cell><cell>SSIM</cell><cell>0.9735</cell><cell>0.9778</cell><cell>0.9796</cell><cell>0.9792</cell><cell>0.9791</cell><cell>0.9794</cell><cell>0.9827</cell><cell>0.9812</cell><cell>0.9825</cell></row><row><cell></cell><cell></cell><cell>PSNR</cell><cell>41.5021</cell><cell cols="3">40.8445 41.5358 41.9892</cell><cell>42.1248</cell><cell>42.2746</cell><cell>43.88781</cell><cell>43.7871</cell><cell>44.1024</cell></row><row><cell>0 b </cell><cell>5 10</cell><cell>RMSE</cell><cell>0.0087</cell><cell>0.0096</cell><cell>0.0088</cell><cell>0.0082</cell><cell>0.0081</cell><cell>0.0078</cell><cell>0.0075</cell><cell>0.0069</cell><cell>0.0065</cell></row><row><cell></cell><cell></cell><cell>SSIM</cell><cell>0.9498</cell><cell>0.9447</cell><cell>0.9509</cell><cell>0.9658</cell><cell>0.9655</cell><cell>0.9688</cell><cell>0.9765</cell><cell>0.9754</cell><cell>0.9778</cell></row><row><cell></cell><cell></cell><cell>PSNR</cell><cell>39.7729</cell><cell cols="3">38.9090 39.8928 40.5487</cell><cell>40.7954</cell><cell>40.9857</cell><cell>41.8451</cell><cell>41.4278</cell><cell>42.9892</cell></row><row><cell cols="2">4 5 10 b  0</cell><cell>RMSE</cell><cell>0.0106</cell><cell>0.0121</cell><cell>0.0106</cell><cell>0.0095</cell><cell>0.0094</cell><cell>0.0092</cell><cell>0.0084</cell><cell>0.0089</cell><cell>0.0091</cell></row><row><cell></cell><cell></cell><cell>SSIM</cell><cell>0.9221</cell><cell>0.9296</cell><cell>0.9492</cell><cell>0.9588</cell><cell>0.9592</cell><cell>0.9601</cell><cell>0.9688</cell><cell>0.9654</cell><cell>0.9727</cell></row></table></figure>
		</body>
		<back>

			<div type="funding">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>This work was supported in part by the National Natural Science Foundation of China under Grants 61671312, 61302028, 61202160, 81370040 and 81530060 and in part by the National Institute of Biomedical Imaging and Bioengineering/National Institutes of Health under Grants R01 EB016977 and U01 EB017140. Asterisk indicates corresponding author.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Computed tomography-An increasing source of radiation exposure</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">J</forename><surname>Brenner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">J</forename><surname>Hall</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Med</title>
		<imprint>
			<biblScope unit="volume">357</biblScope>
			<biblScope unit="issue">22</biblScope>
			<biblScope unit="page" from="2277" to="2284" />
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
	<note>New Eng</note>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Ray contribution masks for structure adaptive sinogram filtering</title>
		<author>
			<persName><forename type="first">M</forename><surname>Balda</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Hornegger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Heismann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Med. Imaging</title>
		<imprint>
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="1116" to="1128" />
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Projection space denoising with bilateral filtering and CT noise modeling for dose reduction in CT</title>
		<author>
			<persName><forename type="first">A</forename><surname>Manduca</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">D</forename><surname>Trzasko</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Khaylova</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">M</forename><surname>Kofler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">M</forename><surname>Mccollough</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">G</forename><surname>Fletcher</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Med. Phys</title>
		<imprint>
			<biblScope unit="page" from="4911" to="4919" />
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Penalized weighted least-squares approach to sinogram noise reduction and image reconstruction for low dose X-ray computed tomography</title>
		<author>
			<persName><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Liang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Med. Imaging</title>
		<imprint>
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page" from="1272" to="1283" />
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Compressed sensing</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">L</forename><surname>Donoho</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Inf. Theory</title>
		<imprint>
			<biblScope unit="volume">52</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="1289" to="1306" />
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Image reconstruction in circular cone-beam computed tomography by constrained, total-variation minimization</title>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">Y</forename><surname>Sidky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Pan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Phys. Med. Biol</title>
		<imprint>
			<biblScope unit="volume">53</biblScope>
			<biblScope unit="issue">17</biblScope>
			<biblScope unit="page" from="4777" to="4807" />
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Few-view image reconstruction with fractional-order total variation</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Lei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Zhou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Opt. Soc. Am. A</title>
		<imprint>
			<biblScope unit="volume">31</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="981" to="995" />
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Statistical iterative reconstruction using adaptive fractional order regularization</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Pu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Zhou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Biomed. Opt. Express</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="1015" to="1029" />
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Few-view image reconstruction combining total variation and a highorder norm</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W.-H</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M.-L</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T.-Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J.-L</forename><surname>Zhou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Int. J. Imaging Syst. Technol</title>
		<imprint>
			<biblScope unit="volume">23</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="249" to="255" />
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Bayesian statistical reconstruction for low-dose x-ray computed tomography using an adaptive weighting nonlocal prior</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Nie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Yin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Comput. Med. Imaging Graph</title>
		<imprint>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="495" to="500" />
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Iterative image reconstruction for cerebral perfusion CT using a precontrast scan induced edge-preserving prior</title>
		<author>
			<persName><forename type="first">J</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Liangm</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Phys. Med. Biol</title>
		<imprint>
			<biblScope unit="volume">57</biblScope>
			<biblScope unit="issue">22</biblScope>
			<biblScope unit="page" from="7519" to="7542" />
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Spectral CT reconstruction with image sparsity and spectral mean</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Xi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Cong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Comput. Imaging</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="510" to="523" />
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Low-dose xray CT reconstruction via dictionary learning</title>
		<author>
			<persName><forename type="first">Q</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Mou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Hsieh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Med. Imaging</title>
		<imprint>
			<biblScope unit="volume">31</biblScope>
			<biblScope unit="issue">9</biblScope>
			<biblScope unit="page" from="1682" to="1697" />
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Cine cone beam CT reconstruction using low-rank matrix factorization: algorithm and a proof-of-principle study</title>
		<author>
			<persName><forename type="first">J.-F</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">B</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Zhao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Med. Imaging</title>
		<imprint>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="1581" to="1591" />
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Model-based iterative reconstruction technique for radiation dose reduction in chest CT: comparison with the adaptive statistical iterative reconstruction techniques</title>
		<author>
			<persName><forename type="first">M</forename><surname>Katsura</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Matsuda</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Akahane</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Eur. Radiol</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="1613" to="1623" />
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Adaptive nonlocal means filtering based on local noise level for CT denoising</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">D</forename><surname>Trzasko</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">S</forename><surname>Lake</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">J</forename><surname>Blezek</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">G</forename><surname>Fletcher</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">H</forename><surname>Mccollough</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Manduca</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Med. Phys</title>
		<imprint>
			<biblScope unit="volume">41</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page">11908</biblScope>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">K-SVD: An algorithm for designing overcomplete dictionaries for sparse representation</title>
		<author>
			<persName><forename type="first">M</forename><surname>Aharon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Elad</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Bruckstein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Signal Process</title>
		<imprint>
			<biblScope unit="volume">54</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="4311" to="4322" />
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Improving abdomen tumor low-dose CT images using a fast dictionary learning based processing</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Yin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Shu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J.-L</forename><surname>Coatrieux</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Toumoulin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Phys. Med. Biol</title>
		<imprint>
			<biblScope unit="volume">58</biblScope>
			<biblScope unit="issue">16</biblScope>
			<biblScope unit="page" from="5803" to="5820" />
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Block matching 3D random noise filtering for absorption optical projection tomography</title>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">F</forename><surname>Feruglio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Vinegoni</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Gros</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Sbarbati</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Weissleder</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Phys. Med. Biol</title>
		<imprint>
			<biblScope unit="volume">55</biblScope>
			<biblScope unit="issue">18</biblScope>
			<biblScope unit="page" from="5401" to="5415" />
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Denoised and texture enhanced MVCT to improve soft tissue conspicuity</title>
		<author>
			<persName><forename type="first">K</forename><surname>Sheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Gou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">X</forename><surname>Qi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Med. Phys</title>
		<imprint>
			<biblScope unit="volume">41</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page">101916</biblScope>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Image denoising of low-radiation dose coronary CT angiography by an adaptive block-matching 3D algorithm</title>
		<author>
			<persName><forename type="first">D</forename><surname>Kang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Slomka</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Nakazato</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Woo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">S</forename><surname>Berman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C.-C</forename><forename type="middle">J</forename><surname>Kuo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Dey</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. SPIE</title>
		<meeting>SPIE</meeting>
		<imprint>
			<date type="published" when="2013">2013</date>
			<biblScope unit="volume">8669</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title/>
		<author>
			<persName><forename type="first">I</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Courville</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Deep</forename><surname>Learning</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016">2016</date>
			<publisher>The MIT Press</publisher>
			<pubPlace>Cambridge, MA</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Deep learning</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nature</title>
		<imprint>
			<biblScope unit="volume">521</biblScope>
			<biblScope unit="issue">7553</biblScope>
			<biblScope unit="page" from="436" to="444" />
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Image denoising and inpainting with deep neural networks</title>
		<author>
			<persName><forename type="first">J</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Xun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc</title>
		<meeting>null</meeting>
		<imprint>
			<date type="published" when="2012">2012</date>
			<biblScope unit="page" from="350" to="352" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Adaptive multi-column deep neural networks with application to robust image denoising</title>
		<author>
			<persName><forename type="first">F</forename><surname>Agostinelli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">R</forename><surname>Anderson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc</title>
		<meeting>null</meeting>
		<imprint>
			<date type="published" when="2013">2013</date>
			<biblScope unit="page" from="1493" to="1501" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Non-local autoencoder with collaborative stabilization for image restoration</title>
		<author>
			<persName><forename type="first">R</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Tao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Image Process</title>
		<imprint>
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="2117" to="2129" />
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Natural image denoising with convolutional networks</title>
		<author>
			<persName><forename type="first">V</forename><surname>Jain</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Seung</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc</title>
		<meeting>null</meeting>
		<imprint>
			<date type="published" when="2008">2008</date>
			<biblScope unit="page" from="769" to="776" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Image super-resolution using deep convolutional networks</title>
		<author>
			<persName><forename type="first">C</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">C</forename><surname>Loy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Pattern Anal. Mach. Intell</title>
		<imprint>
			<biblScope unit="volume">38</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="295" to="307" />
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Deep convolutional neural network for image deconvolution</title>
		<author>
			<persName><forename type="first">L</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">S J</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Jia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc</title>
		<meeting>null</meeting>
		<imprint>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="1790" to="1798" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Image denoising: can plain neural networks compete with BM3D?</title>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">C</forename><surname>Burger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">J</forename><surname>Schuler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Harmeling</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conf. Comp. Vis. Patt. Recogn. (CVPR)</title>
		<meeting>IEEE Conf. Comp. Vis. Patt. Recogn. (CVPR)</meeting>
		<imprint>
			<date type="published" when="2012">2012</date>
			<biblScope unit="page" from="2392" to="2399" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Image restoration using very deep convolutional encoder-decoder networks with symmetric skip connections</title>
		<author>
			<persName><forename type="first">X.-J</forename><surname>Mao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y.-B</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="s">Proc. Adv. Neural Inf. Process. Syst</title>
		<imprint>
			<biblScope unit="page">2016</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Urinary bladder segmentation in CT urography using deeplearning convolutional neural network and level sets</title>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">H</forename><surname>Cha</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Hadjiiski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">K</forename><surname>Samala</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H.-P</forename><surname>Chan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">M</forename><surname>Caoili</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">H</forename><surname>Cohan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Med. Phys</title>
		<imprint>
			<biblScope unit="volume">43</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="1882" to="1896" />
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Unsupervised deep learning applied to breast density segmentation and mammographic risk scoring</title>
		<author>
			<persName><forename type="first">M</forename><surname>Kallenberg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Petersen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Nielsen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">Y</forename><surname>Ng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Diao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Igel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">M</forename><surname>Vachon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Holland</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">R</forename><surname>Winkel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Karssemeijer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Lillholm</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Med. Imaging</title>
		<imprint>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="1322" to="1331" />
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Stacked autoencoders for unsupervised feature learning and multiple organ detection in a pilot study using 4D patient data</title>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">C</forename><surname>Shin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">R</forename><surname>Orton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">J</forename><surname>Collins</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">J</forename><surname>Doran</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">O</forename><surname>Leach</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Pattern Anal. Mach. Intell</title>
		<imprint>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="1930" to="1943" />
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Stacked sparse autoencoder (SSDA) for nuclei detection on breast cancer histopathology images</title>
		<author>
			<persName><forename type="first">J</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Xiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Gilmore</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Madabhushi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Med. Imaging</title>
		<imprint>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="119" to="130" />
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Accelerating magnetic resonance imaging via deep learning</title>
		<author>
			<persName><forename type="first">S</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Ying</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Liang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Int. Symp. Biomed. Imag. (ISBI)</title>
		<meeting>IEEE Int. Symp. Biomed. Imag. (ISBI)</meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="514" to="517" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Low-dose CT via convolutional neural network</title>
		<author>
			<persName><forename type="first">H</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Liao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Biomed. Opt. Express</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="679" to="694" />
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<monogr>
		<title level="m" type="main">A deep convolutional neural network using directional wavelets for low-dose X-ray CT reconstruction</title>
		<author>
			<persName><forename type="first">E</forename><surname>Kang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Min</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">C</forename><surname>Ye</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1610.09736</idno>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Deep learning computed tomography</title>
		<author>
			<persName><forename type="first">T</forename><surname>Wurfl</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><forename type="middle">C</forename><surname>Ghesu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Christlein</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Maier</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Int. Conf. Med. Image Comput. Comput. Assist. Interv. (MICCAI)</title>
		<imprint>
			<biblScope unit="page" from="432" to="440" />
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">A perspective on deep imaging</title>
		<author>
			<persName><forename type="first">G</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Access</title>
		<imprint>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page" from="8914" to="8924" />
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conf. Comp. Vis. Patt. Recogn. (CVPR)</title>
		<meeting>IEEE Conf. Comp. Vis. Patt. Recogn. (CVPR)</meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="770" to="778" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Training very deep networks</title>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">K</forename><surname>Srivastava</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Greff</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Schmidhuber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Advances in Neural Inf. Process. Syst. (NIPS)</title>
		<meeting>Advances in Neural Inf. ess. Syst. (NIPS)</meeting>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Adaptive deconvolutional networks for mid and high level feature learning</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">D</forename><surname>Zeiler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">W</forename><surname>Taylor</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Fergus</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Int. Conf. Comp. Vis. (ICCV)</title>
		<meeting>IEEE Int. Conf. Comp. Vis. (ICCV)</meeting>
		<imprint>
			<date type="published" when="2011">2011</date>
			<biblScope unit="page" from="2018" to="2025" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Rectified linear units improve restricted Boltzmann machines</title>
		<author>
			<persName><forename type="first">V</forename><surname>Nair</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Int. Conf. Mach. Learn. (ICML)</title>
		<meeting>IEEE Int. Conf. Mach. Learn. (ICML)</meeting>
		<imprint>
			<date type="published" when="2010">2010</date>
			<biblScope unit="page" from="807" to="814" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Learning deconvolution network for semantic segmentation</title>
		<author>
			<persName><forename type="first">H</forename><surname>Noh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Hong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Han</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Int. Conf. Comp. Vis. (ICCV)</title>
		<meeting>IEEE Int. Conf. Comp. Vis. (ICCV)</meeting>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="1520" to="1528" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Decoupled deep neural network for semisupervised semantic segmentation</title>
		<author>
			<persName><forename type="first">S</forename><surname>Hong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Noh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Han</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Advances in Neural Inf. Process. Syst. (NIPS)</title>
		<meeting>Advances in Neural Inf. ess. Syst. (NIPS)</meeting>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Fully Convolutional Networks for Semantic Segmentation</title>
		<author>
			<persName><forename type="first">J</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Shelhamer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conf. Comp. Vis. Patt. Recogn. (CVPR)</title>
		<meeting>IEEE Conf. Comp. Vis. Patt. Recogn. (CVPR)</meeting>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="3431" to="3440" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">The Importance of Skip Connections in Biomedical Image Segmentation</title>
		<author>
			<persName><forename type="first">E</forename><surname>Drozdzal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Vorontsov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Chartrand</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Kadoury</surname></persName>
		</author>
		<author>
			<persName><surname>Pal</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Int. Workshop on Deep Learning in Medical Image Analysis</title>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="234" to="241" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">U-Net: Convolutional Networks for Biomedical Image Segmentation</title>
		<author>
			<persName><forename type="first">O</forename><surname>Ronneberger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Fischer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Brox</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Int. Conf. Med. Image Comput. Comput. Assist. Interv. (MICCAI)</title>
		<imprint>
			<biblScope unit="page" from="432" to="440" />
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Adam: A method for stochastic optimization</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">P</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Ba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Int. Conf. Learning Representations (ICLR)</title>
		<meeting>Int. Conf. Learning Representations (ICLR)</meeting>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Fast calculation of the exact radiological path for a threedimensional CT array</title>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">L</forename><surname>Siddon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Med. Phys</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="252" to="255" />
			<date type="published" when="1985">1985</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
