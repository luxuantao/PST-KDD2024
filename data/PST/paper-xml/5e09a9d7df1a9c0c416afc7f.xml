<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Mask-Predict: Parallel Decoding of Conditional Masked Language Models</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Marjan</forename><surname>Ghazvininejad</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Luke Zettlemoyer Facebook AI Research Seattle</orgName>
								<address>
									<region>WA</region>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Omer</forename><surname>Levy</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Luke Zettlemoyer Facebook AI Research Seattle</orgName>
								<address>
									<region>WA</region>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Yinhan</forename><surname>Liu</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Luke Zettlemoyer Facebook AI Research Seattle</orgName>
								<address>
									<region>WA</region>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Mask-Predict: Parallel Decoding of Conditional Masked Language Models</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.2" ident="GROBID" when="2022-12-25T13:00+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Most machine translation systems generate text autoregressively from left to right. We, instead, use a masked language modeling objective to train a model to predict any subset of the target words, conditioned on both the input text and a partially masked target translation. This approach allows for efficient iterative decoding, where we first predict all of the target words non-autoregressively, and then repeatedly mask out and regenerate the subset of words that the model is least confident about. By applying this strategy for a constant number of iterations, our model improves state-of-the-art performance levels for nonautoregressive and parallel decoding translation models by over 4 BLEU on average. It is also able to reach within about 1 BLEU point of a typical left-to-right transformer model, while decoding significantly faster. 1 * * Equal contribution, sorted alphabetically.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Most machine translation systems use sequential decoding strategies where words are predicted one-by-one. In this paper, we present a model and a parallel decoding algorithm which, for a relatively small sacrifice in performance, can be used to generate translations in a constant number of decoding iterations.</p><p>We introduce conditional masked language models (CMLMs), which are encoder-decoder architectures trained with a masked language model objective <ref type="bibr" target="#b0">(Devlin et al., 2018;</ref><ref type="bibr" target="#b6">Lample and Conneau, 2019)</ref>. This change allows the model to learn to predict, in parallel, any arbitrary subset of masked words in the target translation. We use transformer CMLMs, where the decoder's self attention <ref type="bibr" target="#b16">(Vaswani et al., 2017)</ref> can attend to the entire sequence (left and right context) to predict each masked word. We train with a simple masking scheme where the number of masked target tokens is distributed uniformly, presenting the model with both easy (single mask) and difficult (completely masked) examples. Unlike recently proposed insertion models <ref type="bibr" target="#b4">(Gu et al., 2019;</ref><ref type="bibr" target="#b15">Stern et al., 2019)</ref>, which treat each token as a separate training instance, CMLMs can train from the entire sequence in parallel, resulting in much faster training.</p><p>We also introduce a new decoding algorithm, mask-predict, which uses the order-agnostic nature of CMLMs to support highly parallel decoding. Mask-predict repeatedly masks out and repredicts the subset of words in the current translation that the model is least confident about, in contrast to recent parallel decoding translation approaches that repeatedly predict the entire sequence <ref type="bibr" target="#b7">(Lee et al., 2018)</ref>. Decoding starts with a completely masked target text, to predict all of the words in parallel, and ends after a constant number of mask-predict cycles. This overall strategy allows the model to repeatedly reconsider word choices within a rich bi-directional context and, as we will show, produce high-quality translations in just a few cycles.</p><p>Experiments on benchmark machine translation datasets show the strengths of mask-predict decoding for transformer CMLMs. With just 4 iterations, BLEU scores already surpass the performance of the best non-autoregressive and parallel decoding models. <ref type="foot" target="#foot_0">2</ref>With 10 iterations, the approach outperforms the current state-of-the-art parallel decod-ing model <ref type="bibr" target="#b7">(Lee et al., 2018)</ref> by gaps of 4-5 BLEU points on the WMT'14 English-German translation benchmark, and up to 3 BLEU points on WMT'16 English-Romanian, but with the same model complexity and decoding speed. When compared to standard autoregressive transformer models, CMLMs with mask-predict offer a tradeoff between speed and performance, trading up to 2 BLEU points in translation quality for a 3x speed-up during decoding.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Conditional Masked Language Models</head><p>A conditional masked language model (CMLM) predicts a set of target tokens Y mask given a source text X and part of the target text Y obs . It makes the strong assumption that the tokens Y mask are conditionally independent of each other (given X and Y obs ), and predicts the individual probabilities P (y|X, Y obs ) for each y ∈ Y mask . Since the number of tokens in Y mask is given in advance, the model is also implicitly conditioning on the length of the target sequence N = |Y mask | + |Y obs |.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Architecture</head><p>We adopt the standard encoder-decoder transformer for machine translation <ref type="bibr" target="#b16">(Vaswani et al., 2017)</ref>: a source-language encoder that does selfattention, and a target-language decoder that has one set of attention heads over the encoder's output and another set for the target language (selfattention). In terms of parameters, our architecture is identical to the standard one. We deviate from the standard decoder by removing the selfattention mask that prevents left-to-right decoders from attending on future tokens. In other words, our decoder is bi-directional, in the sense that it can use both left and right contexts to predict each token.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Training Objective</head><p>During training, we randomly select Y mask among the target tokens. We first sample the number of masked tokens from a uniform distribution between one and the sequence's length, and then randomly choose that number of tokens. Following <ref type="bibr" target="#b0">Devlin et al. (2018)</ref>, we replace the inputs of the tokens Y mask with a special MASK token.</p><p>We optimize the CMLM for cross-entropy loss over every token in Y mask . This can be done in parallel, since the model assumes that the tokens in Y mask are conditionally independent of each other.</p><p>While the architecture can technically make predictions over all target-language tokens (including Y obs ), we only compute the loss for the tokens in Y mask .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Predicting Target Sequence Length</head><p>In traditional left-to-right machine translation, where the target sequence is predicted token by token, it is natural to determine the length of the sequence dynamically by simply predicting a special EOS (end of sentence) token. However, for CMLMs to predict the entire sequence in parallel, they must know its length in advance. This problem was recognized by prior work in nonautoregressive translation, where the length is predicted with a fertility model <ref type="bibr" target="#b3">(Gu et al., 2018)</ref> or by pooling the encoder's outputs into a length classifier <ref type="bibr" target="#b7">(Lee et al., 2018)</ref>.</p><p>We follow <ref type="bibr" target="#b0">Devlin et al. (2018)</ref> and add a special LENGTH token to the encoder, akin to the CLS token in BERT. The model is trained to predict the length of the target sequence N as the LENGTH token's output, similar to predicting another token from a different vocabulary, and its loss is added to the cross-entropy loss from the target sequence.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Decoding with Mask-Predict</head><p>We introduce the mask-predict algorithm, which decodes an entire sequence in parallel within a constant number of cycles. At each iteration, the algorithm selects a subset of tokens to mask, and then predicts them (in parallel) using an underlying CMLM. Masking the tokens where the model has doubts while conditioning on previous highconfidence predictions lets the model re-predict the more challenging cases, but with more information. At the same time, the ability to make large parallel changes at each step allows mask-predict to converge on a high quality output sequence in a sub-linear number of decoding iterations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Formal Description</head><p>Given the target sequence's length N (see Section 3.3), we define two variables: the target sequence (y 1 , . . . , y N ) and the probability of each token (p 1 , . . . , p N ). The algorithm runs for a predetermined number of iterations T , which is either a constant or a simple function of N . At each iteration, we perform a mask operation, followed by predict.  Mask For the first iteration (t = 0), we mask all the tokens. For later iterations, we mask the n tokens with the lowest probability scores:</p><formula xml:id="formula_0">Y (t) mask = arg min i (p i , n) Y (t) obs = Y \ Y (t) mask</formula><p>The number of masked tokens n is a function of the iteration t; specifically, we use linear decay n = N • T −t T , where T is the total number of iterations. For example, if T = 10, we will mask 90% of the tokens at t = 1, 80% at t = 2, and so forth.</p><p>Predict After masking, the CMLM predicts the masked tokens Y (t) mask , conditioned on the source text X and the unmasked target tokens Y (t) obs . We select the prediction with the highest probability for each masked token y i ∈ Y (t) mask and update its probability score accordingly:</p><formula xml:id="formula_1">y (t) i = arg max w P (y i = w|X, Y (t) obs ) p (t) i = max w P (y i = w|X, Y (t)</formula><p>obs )</p><p>The values and the probabilities of unmasked tokens Y (t) obs remain unchanged:</p><formula xml:id="formula_2">y (t) i = y (t−1) i p (t) i = p (t−1) i</formula><p>We tried updating or decaying these probabilities in preliminary experiments, but found that this heuristic works well despite the fact that some probabilities are stale.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Example</head><p>Figure <ref type="figure" target="#fig_1">1</ref> illustrates how mask-predict can generate a good translation in just three iterations.</p><p>In the first iteration (t = 0), the entire target sequence is masked (Y</p><formula xml:id="formula_3">(0) mask = Y and Y (0) obs = ∅),</formula><p>and is thus generated by the CMLM in a purely non-autoregressive process:</p><formula xml:id="formula_4">P (Y (0) mask |X, Y (0) obs ) = P (Y |X)</formula><p>This produces an ungrammatical translation with repetitions ("completed completed"), which is typical of non-autoregressive models due to the multi-modality problem <ref type="bibr" target="#b3">(Gu et al., 2018)</ref>.</p><p>In the second iteration (t = 1), we select 8 of the 12 tokens generated in the previous step; these token were predicted with the lowest probabilities at t = 0. We mask them and repredict with the CMLM, while conditioning on the 4 unmasked tokens Y</p><p>(1) obs = {"The", "20", "November", "."}. This results in a more grammatical and accurate translation. Our analysis shows that this second iteration removes most repetitions, perhaps because conditioning on even a little bit of the target sequence is enough to collapse the multi-modal target distribution into a single output (Section 5.1).</p><p>In the last iteration (t = 2), we select the 4 of the 12 tokens that had the lowest probabilities. Two of those tokens were predicted at the first step (t = 0), and not repredicted at the second step (t = 1). It is quite common for earlier predictions to be masked at later iterations because they were predicted with less information and thus tend to have lower probabilities. Now that the model is conditioning on 8 tokens, it is able to produce an more fluent translation; "withdrawal" is a better fit for describing troop movement, and "November 20th" is a more common date format in English.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Deciding Target Sequence Length</head><p>When generating, we first compute the CMLM's encoder, and then use the LENGTH token's encoding to predict a distribution over the target sequence's length (see Section 2.3). Since much of the CMLM's computation can be batched, we select the top length candidates with the highest probabilities, and decode the same example with different lengths in parallel. We then select the sequence with the highest average log-probability as our result:</p><formula xml:id="formula_5">1 N log p (T ) i</formula><p>Our analysis reveals that translating multiple candidate sequences of different lengths can improve performance (see Section 5.3).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experiments</head><p>We evaluate CMLMs with mask-predict decoding on standard machine translation benchmarks. We find that our approach significantly outperforms prior parallel decoding machine translation methods and even approaches the performance of standard autoregressive models (Section 4.2), while decoding significantly faster (Section 4.3).  <ref type="bibr" target="#b11">(Papineni et al., 2002)</ref> for all language pairs, except from EN to ZH, where we use SacreBLEU <ref type="bibr" target="#b12">(Post, 2018)</ref>. 3   Hyperparameters We follow most of the standard hyperparameters for transformers in the base configuration <ref type="bibr" target="#b16">(Vaswani et al., 2017)</ref>: 6 layers per stack, 8 attention heads per layer, 512 model dimensions, 2048 hidden dimensions. We also experiment with 512 hidden dimensions, for comparison with previous parallel decoding models <ref type="bibr" target="#b3">(Gu et al., 2018;</ref><ref type="bibr" target="#b7">Lee et al., 2018)</ref>. We follow the weight initialization scheme from BERT <ref type="bibr" target="#b0">(Devlin et al., 2018)</ref>, which samples weights from N (0, 0.02), initializes biases to zero, and sets layer normalization parameters to β = 0, γ = 1.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Experimental Setup</head><p>For regularization, we use 0.3 dropout, 0.01 L 2 weight decay, and smoothed cross validation loss with ε = 0.1. We train batches of 128k tokens using Adam (Kingma and Ba, 2015) with β = (0.9, 0.999) and ε = 10 −6 . The learning rate warms up to a peak of 5 • 10 −4 within 10,000 steps, and then decays with the inverse squareroot schedule. We trained all models for 300k steps, measured the validation loss at the end of each epoch, and averaged the 5 best checkpoints 3 SacreBLEU hash: BLEU+case.mixed+lang.en-zh +numrefs.1+smooth.exp+test.wmt17+tok.zh+version. <ref type="bibr">1.3.7</ref> to create the final model. During decoding, we use a beam size of b = 5 for autoregressive decoding, and similarly use = 5 length candidates for mask-predict decoding. We trained with mixed precision floating point arithmetic on two DGX-1 machines, each with eight 16GB Nvidia V100 GPUs interconnected by Infiniband <ref type="bibr" target="#b9">(Micikevicius et al., 2018)</ref>.</p><p>Model Distillation Following previous work on non-autoregressive and insertion-based machine translation <ref type="bibr" target="#b3">(Gu et al., 2018;</ref><ref type="bibr" target="#b7">Lee et al., 2018;</ref><ref type="bibr" target="#b15">Stern et al., 2019)</ref>, we train CMLMs on translations produced by a standard left-to-right transformer model (large for EN-DE and EN-ZH, base for EN-RO). For a fair comparison, we also train standard left-to-right base transformers on translations produced by large transformer models for EN-DE and EN-ZH, in addition to the standard baselines. We analyze the impact of distillation in Section 5.4.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Translation Quality</head><p>We compare our approach to three other parallel decoding translation methods: the fertility-based sequence-to-sequence model of <ref type="bibr" target="#b3">Gu et al. (2018)</ref>, the CTC-loss transformer of <ref type="bibr" target="#b8">Libovický and Helcl (2018)</ref>, and the iterative refinement approach of <ref type="bibr" target="#b7">Lee et al. (2018)</ref>. The first two methods are purely non-autoregressive, while the iterative refinement approach is only non-autoregressive in the first decoding iteration, similar to our approach. In terms of speed, each mask-predict iteration is virtually equivalent to a refinement iteration.</p><p>Table <ref type="table" target="#tab_1">1</ref> shows that among the parallel decoding methods, our approach yields the highest BLEU scores by a considerable margin. When controlling for the number of parameters (i.e. considering only the smaller CMLM configuration), CMLMs score roughly 4 BLEU points higher than the previous state of the art on WMT'14 EN-DE, in both directions. Another striking result is that a CMLM with only 4 mask-predict iterations yields higher scores than 10 iterations of the iterative refinement model; in fact, only 3 mask-predict iterations are necessary for achieving a new state of the art on both directions of WMT'14 EN-DE (not shown).</p><p>The translations produced by CMLMs with mask-predict also score competitively when compared to strong transformer-based autoregressive models. In all 4 benchmarks, our base CMLM reaches within 0.5-1.2 BLEU points from a welltuned base transformer, a relative decrease of less than 4% in translation quality. In many scenarios, this is an acceptable price to pay for a significant speedup from parallel decoding.</p><p>Table <ref type="table" target="#tab_2">2</ref> shows that these trends also hold for English-Chinese translation, in both directions, despite major linguistic differences between the two languages.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Decoding Speed</head><p>Because CMLMs can predict the entire sequence in parallel, mask-predict can translate an entire sequence in a constant number of decoding iterations. Does this appealing theoretical property translate into a wall-time speed-up in practice? By comparing the actual decoding times, we show that, for some sacrifice in performance, our parallel method can translate much faster than standard sequential transformers.</p><p>Setup As the baseline system, we use the base transformer with beam search (b = 5) to translate WMT'14 EN-DE; we also use greedy search (b = 1) as a faster but less accurate baseline. For CMLMs, we vary the number of mask-predict iterations (T = 4, . . . , 10) and length candidates ( = 1, 2, 3). For both models, we decode batches of 10 sentences. <ref type="foot" target="#foot_1">4</ref> For each decoding run, we measure the performance (BLEU) and wall time (seconds) from when the model and data have been loaded until the last example has been translated, and calculate the relative decoding speed-up (CMLM time / baseline time) to assess the speedperformance trade-off.</p><p>The implementation of both the baseline transformer and our CMLM is based on fairseq <ref type="bibr" target="#b1">(Gehring et al., 2017)</ref>, which efficiently decodes left-to-right transformers by caching the state. Caching reduces the baseline's decoding speed from 210 seconds to 128.5; CMLMs do not use cached decoding. All experiments used exactly the same machine and the same single GPU. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Results</head><p>Figure <ref type="figure" target="#fig_2">2</ref> shows the speed-performance trade-off. We see that mask-predict is versatile; on one hand, we can translate over 3 times faster than the baseline at a cost of 2 BLEU points (T = 4, = 2), or alternatively retain a high quality of 27.03 BLEU while gaining a 30% speed-up (T = 4, = 2). Surprisingly, this latter configuration outperforms an autoregressive transformer with greedy decoding (b = 1) in both quality and speed. We also observe that more balanced configurations (e.g. T = 8, = 2) yield similar performance to the single-beam autoregressive transformer, but decode much faster.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Analysis</head><p>To complement the quantitative results in Section 4, we present qualitative analysis that provides some intuition as to why our approach works and where future work could potentially improve it.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Why Are Multiple Iterations Necessary?</head><p>Various non-autoregressive translation models, including our own CMLM, make the strong assumption that the individual token predictions are conditionally independent of each other. Such a model might consider two or more possible translations, A and B, but because there is no coordination mechanism between the token predictions, it could predict one token from A and another token from B. This problem, known as the multimodality problem <ref type="bibr" target="#b3">(Gu et al., 2018)</ref>, often manifest as token repetitions in the output when the model has multiple hypotheses that predict the same word w with high confidence, but at different positions.</p><p>We hypothesize that multiple mask-predict iterations alleviate the multi-modality problem by allowing the model to condition on parts of the input, thus collapsing the multi-modal distribution into a sharper uni-modal distribution. To test our  hypothesis, we measure the percentage of repetitive tokens produced by each iteration of maskpredict as a proxy metric for the multi-modality problem.</p><p>Table <ref type="table" target="#tab_4">3</ref> shows that, indeed, the proportion of repetitive tokens drops drastically during the first 2-3 iterations. This finding suggests that the first few iterations are critical for converging into a uni-modal distribution. The decrease in repetitions also correlates with the steep rise in translation quality (BLEU), supporting the conjecture of <ref type="bibr" target="#b3">Gu et al. (2018)</ref> that multi-modality is a major roadblock for purely non-autoregressive machine translation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Do Longer Sequences Need More</head><p>Iterations?</p><p>A potential concern with using a constant amount of decoding iterations is that it may be effective for short sequences (where the number of iterations T is closer to the output's length N ), but insufficient for longer sequences. To determine whether this is the case, we use compare-mt <ref type="bibr" target="#b10">(Neubig et al., 2019)</ref> to bucket the evaluation data by target sentence length and compute the performance with different values of T . Table <ref type="table">4</ref> shows that increasing the number of decoding iterations (T ) appears to mainly improve the performance on longer sequences. Having said that, the performance differences across length buckets are not very large, and it seems that even 4 mask-predict iterations are enough to produce decent translations for long sequences (40 ≤ N ).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3">Do More Length Candidates Help?</head><p>Traditional autoregressive models can dynamically decide the length of the target sequence by generating a special END token when they are done, but that is not true for models that decode multiple tokens in parallel, such as CMLMs.</p><p>To address this problem, our model predicts the length of the target sequence (Section 2.3) and decodes multiple length candidates in parallel (Section 3.3). We compare our model's performance with a varying number of length candidates to its performance when conditioned on the reference (gold) target length in order to determine how accurate it is at predicting the correct length and assess the relative contribution of decoding with multiple length candidates.</p><formula xml:id="formula_6">T = 4 T = 10 T = N 1 ≤ N &lt;</formula><p>Table <ref type="table" target="#tab_5">5</ref> shows that having multiple candidates can increase performance almost as much as conditioning on the gold length. Surprisingly, adding too many candidates can even degrade performance. We suspect that because CMLMs are implicitly conditioned on the target length, producing a translation that is too short (i.e. high precision, low recall) will have a high average log probability. In preliminary experiments, we tried to address this issue by weighting the different candidates according to the model's length prediction, but this approach gave too much weight to the top candidate and resulted in lower performance.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.4">Is Model Distillation Necessary?</head><p>Previous work on non-autoregressive and insertion-based machine translation reported that it was necessary to train their models on text generated by an autoregressive teacher model, a process known as distillation. To determine CMLM's dependence on this process, we train a models on both raw and distilled data, and compare their performance.</p><p>Table <ref type="table" target="#tab_7">6</ref> shows that in every case, training with model distillation substantially outperforms training on raw data. The gaps are especially large when decoding with a single iteration (purely nonautoregressive). Overall, it appears as though CMLMs are heavily dependent on model distillation.</p><p>On the English-Romanian benchmark, the differences are much smaller, and after 10 iterations the raw-data model can perform comparably with the distilled model. A possible explanation is that our teacher model was weaker for this dataset due to insufficient hyperparameter tuning. Alternatively, it could also be the case that the English-German dataset is much noisier than the English-Romanian one, and that the teacher model essentially cleans the training data. Unfortunately, we do not have enough evidence to support or refute either hypothesis at this time.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Related Work</head><p>Training Masked Language Models with Translation Data Recent work by <ref type="bibr" target="#b6">Lample and Conneau (2019)</ref> shows that training a masked language model on sentence-pair translation data, as a pre-training step, can improve performance on cross-lingual tasks, including autoregressive machine translation. Our training scheme builds on their work, with the following differences: we use separate model parameters for source and target texts (encoder and decoder), and we also use a different masking scheme. Specifically, we mask a varying percentage of tokens, only from the target, and do not replace input tokens with noise. Most importantly, the goal of our work is different; we do not use CMLMs for pre-training, but to directly generate text with mask-predict decoding.</p><p>Concurrently with our work, Song et al. ( <ref type="formula">2019</ref>) extend the approach of Lample and Conneau (2019) by using separate encoder and decoder parameters (as in our model) and pre-training them jointly in an autoregressive version of masked language modeling, although with monolingual data. While this work demonstrates that pretraining CMLMs can improve autoregressive machine translation, it does not try to leverage the parallel and bi-directional nature of CMLMs to generate text in a non-left-to-right manner.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Generating from Masked Language Models</head><p>One such approach for generating text from a masked language model casts BERT <ref type="bibr" target="#b0">(Devlin et al., 2018)</ref>, a non-conditional masked language model, as a Markov random field <ref type="bibr" target="#b17">(Wang and Cho, 2019)</ref>. By masking a sequence of length N and then iteratively sampling a single token at each time from the model (either sequentially or in arbitrary order), one can produce grammatical examples. While this sampling process has a theoretical justification, it also requires N forward passes of the model; mask-predict decoding, on the other hand, can produce text in a constant number of iterations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Parallel Decoding for Machine Translation</head><p>There have been several advances in parallel decoding machine translation by training nonautoregressive models. <ref type="bibr" target="#b3">Gu et al. (2018)</ref> introduce a transformer-based approach with explicit word fertility, and identify the multi-modality problem. <ref type="bibr" target="#b8">Libovický and Helcl (2018)</ref> approach the multimodality problem by collapsing repetitions with the Connectionist Temporal Classification training objective <ref type="bibr" target="#b2">(Graves et al., 2006)</ref>. Perhaps most similar to our work is the iterative refinement approach of <ref type="bibr" target="#b7">Lee et al. (2018)</ref>, in which the model corrects the original non-autoregressive prediction by passing it multiple times through a denoising autoencoder. A major difference is that <ref type="bibr" target="#b7">Lee et al. (2018)</ref> train their noisy autoencoder to deal with corrupt inputs by applying stochastic corruption heuristics on the training data, while we simply mask a random number of input tokens. We also show that our approach outperforms all of these models by wide margins.</p><p>Arbitrary Order Language Generation Finally, recent work has developed insertion-based transformers for arbitrary, but fixed, word order generation <ref type="bibr" target="#b4">(Gu et al., 2019;</ref><ref type="bibr" target="#b15">Stern et al., 2019)</ref>. While they do not decode in a constant number of iterations, <ref type="bibr" target="#b15">Stern et al. (2019)</ref> show strong results in logarithmic time. Both models treat each token insertion as a separate training example, which cannot be computed in parallel with every other insertion in the same sequence. This makes training significantly more expensive that standard transformers (which use causal attention masking) and our CMLMs (which can predict all of the masked tokens in parallel).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">Conclusion</head><p>This work introduces conditional masked language models and a novel mask-predict decoding algorithm that leverages their parallelism to generate text in a constant number of decoding iterations. We show that, in the context of machine translation, our approach substantially outperforms previous parallel decoding methods, and can approach the performance of sequential autoregressive models while decoding much faster. While there are still open problems, such as the need to condition on the target's length and the dependence on knowledge distillation, our results provide a significant step forward in nonautoregressive and parallel decoding approaches to machine translation. In a broader sense, this paper shows that masked language models are useful not only for representing text, but also for generating text efficiently.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>src</head><label></label><figDesc>Der Abzug der franzsischen Kampftruppen wurde am 20. November abgeschlossen . t = 0 The departure of the French combat completed completed on 20 November . t = 1 The departure of French combat troops was completed on 20 November . t = 2 The withdrawal of French combat troops was completed on November 20th .</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: An example from the WMT'14 DE-EN validation set that illustrates how mask-predict generates text. At each iteration, the highlighted tokens are masked and repredicted, conditioned on the other tokens in the sequence.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: The trade-off between speed-up and translation quality of a base CMLM with mask-predict, compared to the standard sequentially-decoded base transformer on the WMT'14 EN-DE test set, with beam sizes b = 1 (orange triangle) and b = 5 (red triangle). Each blue circle represents a mask-predict decoding run with a different number of iterations (T = 4, . . . , 10) and length candidates ( = 1, 2, 3).</figDesc><graphic url="image-1.png" coords="6,72.00,62.81,453.57,340.92" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 1 :</head><label>1</label><figDesc>The performance (BLEU) of CMLMs with mask-predict, compared to other parallel decoding machine translation methods. The standard (sequential) transformer is shown for reference. Bold numbers indicate stateof-the-art performance among parallel decoding methods.</figDesc><table><row><cell>Model</cell><cell cols="2">Dimensions</cell><cell>Iterations</cell><cell cols="2">WMT'14</cell><cell cols="2">WMT'16</cell></row><row><cell></cell><cell cols="2">(Model/Hidden)</cell><cell></cell><cell cols="4">EN-DE DE-EN EN-RO RO-EN</cell></row><row><cell>NAT w/ Fertility (Gu et al., 2018)</cell><cell>512/512</cell><cell></cell><cell>1</cell><cell>19.17</cell><cell>23.20</cell><cell>29.79</cell><cell>31.44</cell></row><row><cell>CTC Loss (Libovický and Helcl, 2018)</cell><cell>512/4096</cell><cell></cell><cell>1</cell><cell>17.68</cell><cell>19.80</cell><cell>19.93</cell><cell>24.71</cell></row><row><cell>Iterative Refinement (Lee et al., 2018)</cell><cell>512/512</cell><cell></cell><cell>1</cell><cell>13.91</cell><cell>16.77</cell><cell>24.45</cell><cell>25.73</cell></row><row><cell></cell><cell>512/512</cell><cell></cell><cell>10</cell><cell>21.61</cell><cell>25.48</cell><cell>29.32</cell><cell>30.19</cell></row><row><cell>(Dynamic #Iterations)</cell><cell>512/512</cell><cell></cell><cell>?</cell><cell>21.54</cell><cell>25.43</cell><cell>29.66</cell><cell>30.30</cell></row><row><cell>Small CMLM with Mask-Predict</cell><cell>512/512</cell><cell></cell><cell>1</cell><cell>15.06</cell><cell>19.26</cell><cell>20.12</cell><cell>20.36</cell></row><row><cell></cell><cell>512/512</cell><cell></cell><cell>4</cell><cell>24.17</cell><cell>28.55</cell><cell>30.00</cell><cell>30.43</cell></row><row><cell></cell><cell>512/512</cell><cell></cell><cell>10</cell><cell>25.51</cell><cell>29.47</cell><cell>31.65</cell><cell>32.27</cell></row><row><cell>Base CMLM with Mask-Predict</cell><cell>512/2048</cell><cell></cell><cell>1</cell><cell>18.05</cell><cell>21.83</cell><cell>27.32</cell><cell>28.20</cell></row><row><cell></cell><cell>512/2048</cell><cell></cell><cell>4</cell><cell>25.94</cell><cell>29.90</cell><cell>32.53</cell><cell>33.23</cell></row><row><cell></cell><cell>512/2048</cell><cell></cell><cell>10</cell><cell>27.03</cell><cell>30.53</cell><cell>33.08</cell><cell>33.31</cell></row><row><cell>Base Transformer (Vaswani et al., 2017)</cell><cell>512/2048</cell><cell></cell><cell>N</cell><cell>27.30</cell><cell>--</cell><cell>--</cell><cell>--</cell></row><row><cell>Base Transformer (Our Implementation)</cell><cell>512/2048</cell><cell></cell><cell>N</cell><cell>27.74</cell><cell>31.09</cell><cell>34.28</cell><cell>33.99</cell></row><row><cell>Base Transformer (+Distillation)</cell><cell>512/2048</cell><cell></cell><cell>N</cell><cell>27.86</cell><cell>31.07</cell><cell>--</cell><cell>--</cell></row><row><cell>Large Transformer (Vaswani et al., 2017)</cell><cell>1024/4096</cell><cell></cell><cell>N</cell><cell>28.40</cell><cell>--</cell><cell>--</cell><cell>--</cell></row><row><cell>Large Transformer (Our Implementation)</cell><cell>1024/4096</cell><cell></cell><cell>N</cell><cell>28.60</cell><cell>31.71</cell><cell>--</cell><cell>--</cell></row><row><cell>Model</cell><cell cols="3">Dimensions</cell><cell>Iterations</cell><cell cols="2">WMT'17</cell></row><row><cell></cell><cell cols="3">(Model/Hidden)</cell><cell></cell><cell cols="2">EN-ZH ZH-EN</cell></row><row><cell>Base CMLM with Mask-Predict</cell><cell></cell><cell cols="2">512/2048</cell><cell>1</cell><cell>24.23</cell><cell>13.64</cell></row><row><cell></cell><cell></cell><cell cols="2">512/2048</cell><cell>4</cell><cell>32.63</cell><cell>21.90</cell></row><row><cell></cell><cell></cell><cell cols="2">512/2048</cell><cell>10</cell><cell>33.19</cell><cell>23.21</cell></row><row><cell cols="2">Base Transformer (Our Implementation)</cell><cell cols="2">512/2048</cell><cell>N</cell><cell>34.31</cell><cell>23.74</cell></row><row><cell>Base Transformer (+Distillation)</cell><cell></cell><cell cols="2">512/2048</cell><cell>N</cell><cell>34.44</cell><cell>23.99</cell></row><row><cell cols="2">Large Transformer (Our Implementation)</cell><cell cols="2">1024/4096</cell><cell>N</cell><cell>35.01</cell><cell>24.65</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2 :</head><label>2</label><figDesc></figDesc><table /><note>The performance (BLEU) of CMLMs with mask-predict, compared to the standard (sequential) transformer on WMT' 17 EN-ZH.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 3 :</head><label>3</label><figDesc>The performance (BLEU) and percentage of repeating tokens when decoding with a different number of mask-predict iterations (T ).</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 5 :</head><label>5</label><figDesc>The performance (BLEU) of base CMLM with 10 mask-predict iterations (T = 10), varied by the number of length candidates ( ), compared to decoding with the reference target length (Gold). Length precision (LP) is the percentage of examples that contain the correct length as one of their candidates.</figDesc><table><row><cell></cell><cell>10</cell><cell>21.8</cell><cell>22.4</cell><cell>22.4</cell></row><row><cell cols="2">10 ≤ N &lt; 20</cell><cell>24.6</cell><cell>25.9</cell><cell>26.0</cell></row><row><cell cols="2">20 ≤ N &lt; 30</cell><cell>24.9</cell><cell>26.7</cell><cell>27.1</cell></row><row><cell cols="2">30 ≤ N &lt; 40</cell><cell>24.9</cell><cell>26.7</cell><cell>27.6</cell></row><row><cell>40 ≤ N</cell><cell></cell><cell>25.0</cell><cell>27.5</cell><cell>28.1</cell></row><row><cell cols="5">Table 4: The performance (BLEU) of base CMLM</cell></row><row><cell cols="5">with different amounts of mask-predict iterations (T )</cell></row><row><cell cols="5">on WMT'14 EN-DE, bucketed by target sequence</cell></row><row><cell cols="5">length (N ). Decoding with = 1 length candidates.</cell></row><row><cell>Length</cell><cell cols="4">WMT'14 EN-DE WMT'16 EN-RO</cell></row><row><cell cols="2">Candidates BLEU</cell><cell>LP</cell><cell>BLEU</cell><cell>LP</cell></row><row><cell>= 1</cell><cell>26.56</cell><cell>16.1%</cell><cell>32.75</cell><cell>13.8%</cell></row><row><cell>= 2</cell><cell>27.03</cell><cell>30.6%</cell><cell>33.06</cell><cell>26.1%</cell></row><row><cell>= 3</cell><cell>27.09</cell><cell>43.1%</cell><cell>33.11</cell><cell>39.6%</cell></row><row><cell>= 4</cell><cell>27.09</cell><cell>53.1%</cell><cell>32.13</cell><cell>49.2%</cell></row><row><cell>= 5</cell><cell>27.03</cell><cell>62.2%</cell><cell>33.08</cell><cell>57.5%</cell></row><row><cell>= 6</cell><cell>26.91</cell><cell>69.5%</cell><cell>32.91</cell><cell>64.3%</cell></row><row><cell>= 7</cell><cell>26.71</cell><cell>75.5%</cell><cell>32.75</cell><cell>70.4%</cell></row><row><cell>= 8</cell><cell>26.59</cell><cell>80.3%</cell><cell>32.50</cell><cell>74.6%</cell></row><row><cell>= 9</cell><cell>26.42</cell><cell>83.8%</cell><cell>32.09</cell><cell>78.3%</cell></row><row><cell>Gold</cell><cell>27.27</cell><cell>-</cell><cell>33.20</cell><cell>-</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 6 :</head><label>6</label><figDesc>The performance (BLEU) of base CMLM, trained with either raw data (Raw) or knowledge distillation from an autoregressive model (Dist).</figDesc><table /></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_0">We use the term "parallel decoding" to refer to the family of approaches that can generate the entire target sequence in parallel. These are often referred to as "non-autoregressive" approaches, but both iterative refinement<ref type="bibr" target="#b7">(Lee et al., 2018)</ref> and our mask-predict approach condition on the model's past predictions.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4" xml:id="foot_1">The batch size was chosen arbitrarily; mask-predict can scale up to much larger batch sizes.</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgements</head><p>We thank Abdelrahman Mohamed for sharing his expertise on non-autoregressive models, and our colleagues at FAIR for valuable feedback.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<author>
			<persName><forename type="first">Jacob</forename><surname>Devlin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ming-Wei</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kenton</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kristina</forename><surname>Toutanova</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1810.04805</idno>
		<title level="m">Pre-training of deep bidirectional transformers for language understanding</title>
				<meeting><address><addrLine>Bert</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<author>
			<persName><forename type="first">Jonas</forename><surname>Gehring</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><surname>Auli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><surname>Grangier</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Denis</forename><surname>Yarats</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yann</forename><forename type="middle">N</forename><surname>Dauphin</surname></persName>
		</author>
		<title level="m">Convolutional Sequence to Sequence Learning</title>
				<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
	<note>ArXiv e-prints</note>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Connectionist temporal classification: Labelling unsegmented sequence data with recurrent neural networks</title>
		<author>
			<persName><forename type="first">Alex</forename><surname>Graves</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Santiago</forename><surname>Fernández</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Faustino</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jürgen</forename><surname>Schmidhuber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 23rd international conference on Machine learning</title>
				<meeting>the 23rd international conference on Machine learning</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2006">2006</date>
			<biblScope unit="page" from="369" to="376" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Non-autoregressive neural machine translation</title>
		<author>
			<persName><forename type="first">Jiatao</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">James</forename><surname>Bradbury</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Caiming</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><forename type="middle">K</forename><surname>Victor</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Richard</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><surname>Socher</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
				<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">Insertion-based decoding with automatically inferred generation order</title>
		<author>
			<persName><forename type="first">Jiatao</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qi</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kyunghyun</forename><surname>Cho</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1902.01370</idno>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Adam: A method for stochastic optimization</title>
		<author>
			<persName><forename type="first">P</forename><surname>Diederik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jimmy</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName><surname>Ba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference for Learning Representations</title>
				<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<author>
			<persName><forename type="first">Guillaume</forename><surname>Lample</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alexis</forename><surname>Conneau</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1901.07291</idno>
		<title level="m">Crosslingual language model pretraining</title>
				<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Deterministic non-autoregressive neural sequence modeling by iterative refinement</title>
		<author>
			<persName><forename type="first">Jason</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Elman</forename><surname>Mansimov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kyunghyun</forename><surname>Cho</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing</title>
				<meeting>the 2018 Conference on Empirical Methods in Natural Language Processing<address><addrLine>Brussels, Belgium</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="1173" to="1182" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">End-toend non-autoregressive neural machine translation with connectionist temporal classification</title>
		<author>
			<persName><forename type="first">Jindřich</forename><surname>Libovický</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jindřich</forename><surname>Helcl</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing</title>
				<meeting>the 2018 Conference on Empirical Methods in Natural Language Processing<address><addrLine>Brussels, Belgium</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="3016" to="3021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Mixed precision training</title>
		<author>
			<persName><forename type="first">Paulius</forename><surname>Micikevicius</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sharan</forename><surname>Narang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jonah</forename><surname>Alben</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gregory</forename><surname>Diamos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Erich</forename><surname>Elsen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><surname>Garcia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Boris</forename><surname>Ginsburg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><surname>Houston</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Oleksii</forename><surname>Kuchaiev</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ganesh</forename><surname>Venkatesh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hao</forename><surname>Wu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
				<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">compare-mt: A tool for holistic comparison of language generation systems</title>
		<author>
			<persName><forename type="first">Graham</forename><surname>Neubig</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zi-Yi</forename><surname>Dou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Junjie</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Paul</forename><surname>Michel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Danish</forename><surname>Pruthi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xinyi</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Meeting of the North American Chapter of the Association for Computational Linguistics (NAACL) Demo Track</title>
				<meeting><address><addrLine>Minneapolis, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Bleu: a method for automatic evaluation of machine translation</title>
		<author>
			<persName><forename type="first">Kishore</forename><surname>Papineni</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Salim</forename><surname>Roukos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Todd</forename><surname>Ward</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wei-Jing</forename><surname>Zhu</surname></persName>
		</author>
		<idno type="DOI">10.3115/1073083.1073135</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 40th Annual Meeting of the Association for Computational Linguistics</title>
				<meeting>the 40th Annual Meeting of the Association for Computational Linguistics<address><addrLine>Philadelphia, Pennsylvania, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2002">2002</date>
			<biblScope unit="page" from="311" to="318" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">A call for clarity in reporting BLEU scores</title>
		<author>
			<persName><forename type="first">Matt</forename><surname>Post</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/W18-6319</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Third Conference on Machine Translation: Research Papers</title>
				<meeting>the Third Conference on Machine Translation: Research Papers<address><addrLine>Belgium, Brussels</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="186" to="191" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Neural machine translation of rare words with subword units</title>
		<author>
			<persName><forename type="first">Rico</forename><surname>Sennrich</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Barry</forename><surname>Haddow</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alexandra</forename><surname>Birch</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/P16-1162</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics</title>
		<title level="s">Long Papers</title>
		<meeting>the 54th Annual Meeting of the Association for Computational Linguistics<address><addrLine>Berlin, Germany</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2016">2016</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="1715" to="1725" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">Mass: Masked sequence to sequence pre-training for language generation</title>
		<author>
			<persName><forename type="first">Kaitao</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xu</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tao</forename><surname>Qin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jianfeng</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tie-Yan</forename><surname>Liu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1905.02450</idno>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">Insertion transformer: Flexible sequence generation via insertion operations</title>
		<author>
			<persName><forename type="first">Mitchell</forename><surname>Stern</surname></persName>
		</author>
		<author>
			<persName><forename type="first">William</forename><surname>Chan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jamie</forename><surname>Kiros</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jakob</forename><surname>Uszkoreit</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1902.03249</idno>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Attention is all you need</title>
		<author>
			<persName><forename type="first">Ashish</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Noam</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Niki</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jakob</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Llion</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aidan</forename><forename type="middle">N</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Łukasz</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Illia</forename><surname>Polosukhin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
				<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="5998" to="6008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Pay less attention with lightweight and dynamic convolutions</title>
		<author>
			<persName><forename type="first">Alex</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kyunghyun</forename><surname>Cho</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1902.04094</idno>
	</analytic>
	<monogr>
		<title level="m">Bert has a mouth, and it must speak: Bert as a markov random field language model</title>
				<editor>
			<persName><forename type="first">Felix</forename><surname>Wu</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Angela</forename><surname>Fan</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Alexei</forename><surname>Baevski</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Michael</forename><surname>Yann N Dauphin</surname></persName>
		</editor>
		<editor>
			<persName><surname>Auli</surname></persName>
		</editor>
		<imprint>
			<date type="published" when="2019">2019. 2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
	<note>International Conference on Learning Representations</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
