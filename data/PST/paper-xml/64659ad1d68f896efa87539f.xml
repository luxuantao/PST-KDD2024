<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">PMC-VQA: Visual Instruction Tuning for Medical Visual Question Answering</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date type="published" when="2023-05-17">17 May 2023</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Xiaoman</forename><surname>Zhang</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Shanghai Jiao Tong University</orgName>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="laboratory">Shanghai AI Laboratory</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Chaoyi</forename><surname>Wu</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Shanghai Jiao Tong University</orgName>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="laboratory">Shanghai AI Laboratory</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Ziheng</forename><surname>Zhao</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Shanghai Jiao Tong University</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Weixiong</forename><surname>Lin</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Shanghai Jiao Tong University</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Ya</forename><surname>Zhang</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Shanghai Jiao Tong University</orgName>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="laboratory">Shanghai AI Laboratory</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Yanfeng</forename><surname>Wang</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Shanghai Jiao Tong University</orgName>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="laboratory">Shanghai AI Laboratory</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Weidi</forename><surname>Xie</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Shanghai Jiao Tong University</orgName>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="laboratory">Shanghai AI Laboratory</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">PMC-VQA: Visual Instruction Tuning for Medical Visual Question Answering</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2023-05-17">17 May 2023</date>
						</imprint>
					</monogr>
					<idno type="arXiv">arXiv:2305.10415v1[cs.CV]</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-01-03T09:22+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Radiology</term>
					<term>Pathology</term>
					<term>Microscopy</term>
					<term>Signals</term>
					<term>Generic biomedical illlustrations</term>
					<term>etc</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>In this paper, we focus on the problem of Medical Visual Question Answering (MedVQA), which is crucial in efficiently interpreting medical images with vital clinic-relevant information. Firstly, we reframe the problem of MedVQA as a generation task that naturally follows the human-machine interaction, we propose a generative-based model for medical visual understanding by aligning visual information from a pre-trained vision encoder with a large language model. Secondly, we establish a scalable pipeline to construct a large-scale medical visual question-answering dataset, named PMC-VQA, which contains 227k VQA pairs of 149k images that cover various modalities or diseases. Thirdly, we pre-train our proposed model on PMC-VQA and then fine-tune it on multiple public benchmarks, e.g., VQA-RAD and SLAKE, outperforming existing work by a large margin. Additionally, we propose a test set that has undergone manual verification, which is significantly more challenging, even the best models struggle to solve.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Large language models (LLMs), such as ChatGPT <ref type="bibr" target="#b0">[1]</ref>, PaLM <ref type="bibr" target="#b7">[8]</ref>, LLaMA <ref type="bibr" target="#b34">[35]</ref>, have recently demonstrated remarkable progress in a wide range of natural language processing (NLP) tasks, such as question answering, text classification, and interactive dialog. Notably, even in domains where expert knowledge is supposed to play a critical role, like medical diagnosis, these language models have also achieved impressive success, passing the United States Medical Licensing Examination (USMLE) <ref type="bibr" target="#b12">[13,</ref><ref type="bibr" target="#b16">17,</ref><ref type="bibr" target="#b25">26,</ref><ref type="bibr" target="#b31">32]</ref>. While recent LLMs excel in language understanding in the medical domain, they are essentially "blind" to visual modalities such as images and videos, hindering the utilization of visual content as a means of communication with these models.</p><p>In this paper, we focus on the problem of Medical Visual Question Answering (MedVQA), which aims to develop models that can comprehend text-based queries and produce accurate answers by leveraging medical visual content <ref type="bibr" target="#b20">[21]</ref>. Existing MedVQA methods <ref type="bibr" target="#b24">[25,</ref><ref type="bibr" target="#b21">22,</ref><ref type="bibr" target="#b5">6,</ref><ref type="bibr" target="#b19">20]</ref> typically treat the problem as a retrieval task with a limited answer base and train multi-modal vision-language models with contrastive or classification objectives. Consequently, they are only useful for limited use cases where a finite set of outcomes is provided beforehand. We propose to develop the first open-ended MedVQA system with a generative model as the backend, capable of handling diverse questions that arise in clinical practice, generating answers in free form without being constrained by the vocabulary. While there has been promising research in visual-language representation learning, such as Flamingo <ref type="bibr" target="#b1">[2]</ref> and BLIP <ref type="bibr" target="#b18">[19]</ref>, these models have primarily been trained on natural language Table 1: Comparison of existing medical VQA datasets with PMC-VQA, demonstrating the significant increase in size and diversity achieved by our dataset.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Dataset</head><p>Modality Source Images QA pairs VQA-RAD <ref type="bibr" target="#b17">[18]</ref> Radiology MedPix database 0.3k 3.5k PathVQA <ref type="bibr" target="#b11">[12]</ref> Pathology PEIR Digital Library <ref type="bibr" target="#b13">[14]</ref> 5k 32.8k SLAKE <ref type="bibr" target="#b22">[23]</ref> Radiology MSD <ref type="bibr" target="#b2">[3]</ref>, ChestX-ray8 <ref type="bibr" target="#b35">[36]</ref>, CHAOS <ref type="bibr" target="#b14">[15]</ref> 0.7k 14k VQA-Med-2021 <ref type="bibr" target="#b4">[5]</ref> Radiology MedPix database 5k 5k</p><p>PMC-VQA Mixture * PubMed Central 149k 227k</p><p>Figure <ref type="figure">1</ref>: The top 20 figure types in PMC-VQA, cover a wide range of diagnostic procedures.</p><p>and images, with very limited application in medical domain, due to the complex and nuanced visual concepts often found in medical scenarios.</p><p>To this end, we introduce a novel paradigm for MedVQA that harnesses the power of generative learning. Specifically, our proposed models start from the foundation models in medical domain, and train a bridge to align the pre-trained vision encoder and large language model via visual instruction tuning, we term the model as MedVInT (Medical Visual Instruction Tuning). To accommodate different architectures, we offer two variants, named as MedVInT-TE and MedVInT-TD, that are tailored for encoder-based and decoder-based language models, respectively.</p><p>In order to effectively train the generative-based MedVQA models, our study reveals that existing datasets are limited in size, making them insufficient for training high-performing models. To overcome this challenge, we leverage well-established medical visual-language datasets <ref type="bibr" target="#b19">[20]</ref> and initiate a scalable, automatic pipeline for constructing a new large-scale medical visual questionanswering dataset. This new dataset, termed as PMC-VQA, contains 227k VQA pairs of 149k images, covering various modalities or diseases (Fig. <ref type="figure">1</ref>), surpassing existing datasets in terms of both amount and diversity, as illustrated in Tab. 1. In our experiments, we pre-trained MedVInT on the collected PMC-VQA dataset and fine-tuned it on the existing MedVQA datasets, e.g., VQA-RAD <ref type="bibr" target="#b17">[18]</ref> and SLAKE <ref type="bibr" target="#b22">[23]</ref>, outperforming existing models by a large margin, achieving over 80% accuracy on multi-choice selection. However, while evaluating on our proposed challenging benchmark, even the state-of-the-art models struggle, showing that there is still ample room for development in this field.</p><p>In summary, our contributions are as follows: (i) We reframe the problem of MedVQA as a generative learning task and propose MedVInT, a model obtained by aligning a pre-trained vision encoder with large language model through visual instruction tuning; (ii) We introduce a scalable pipeline and construct a large-scale MedVQA dataset, PMC-VQA, which far exceeds the size and diversity of existing datasets, covering various modalities and diseases; (iii) We pre-train MedVInT on PMC-VQA and fine-tune it on VQA-RAD <ref type="bibr" target="#b17">[18]</ref> and SLAKE <ref type="bibr" target="#b22">[23]</ref>, achieving state-of-the-art performance and significantly outperforming existing models; (iv) We propose a new test set and present a more challenging benchmark for MedVQA, to evaluate the performance of VQA methods thoroughly.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Method</head><p>Here, we start with an introduction to the problem of generative medical visual question answering in Sec. 2.1; then we present the architecture detail in Sec. 2.2.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Problem Formulation</head><p>MedVQA is a task of answering natural language questions about medical visual content, typically images or videos obtained from medical devices like X-ray, CT, MRI, or microscopy, etc. Specifically, our goal is to train a model that can output the corresponding answer for a given question, which can be expressed as:</p><formula xml:id="formula_0">?i = ? MedVQA (I i , q i ; ?) = ? dec (? vis (I i ; ? vis ), ? text (q i ; ? text ); ? dec )<label>(1)</label></formula><p>Here, ?i refers to the predicted answer, I i ? R H?W ?C refers to the visual image, H, W, C are height, width, channel respectively. The posed question and corresponding ground-truth answer in the form of natural language are denoted as q i and a i , respectively. ? = {? vis , ? text , ? dec } denote the trainable parameters.</p><p>Existing approaches have primarily treated medical VQA as a classification problem, with the goal of selecting the correct answer from a candidate set, i.e., a i ? ? = {a 1 , a 2 , . . . , a N }, where N represents the total number of answers within the dataset. Consequently, this approach limits the system's utility to predefined outcomes, hampering its free-form user-machine interaction potential.</p><p>In this paper, we take an alternative approach, with the goal to generate an open-ended answer in natural language. Specifically, we train the system by maximizing the probability of generating the ground-truth answer given the input image and question. The loss function used to train the model is typically the negative log-likelihood of the correct next token in the sequence, summed over all time steps, which can be expressed as :</p><formula xml:id="formula_1">L(?) = - T t=1</formula><p>log p(a t |I, q 1:T , a 1:t-1 ; ?)</p><p>where T is the length of the ground-truth answer, and p(a t |I, q 1:T , a 1:t-1 ; ?) is the probability of generating the t-th token in the answer sequence given the input image I, the question sequence q 1:T , and the previous tokens in the answer sequence a 1:t-1 . This formulation allows the model to generate diverse and informative answers, which can be useful in a wider range of scenarios than traditional classification-based methods.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Architecture</head><p>In this section, we introduce our proposed architecture for generative MedVQA (Fig. <ref type="figure" target="#fig_0">2(a)</ref>). Specifically, we offer two model variants, which are tailored to encoder-based and decoder-based language models, respectively, denoted as MedVInT-TE (Sec. 2.2.1) and MedVInT-TD (Sec. 2.2.2).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.1">MedVInT-TE</head><p>Visual Encoder. Given one specific image I, we can obtain the image embedding, i.e., v = ? vis (I) ? R n?d , where d denotes the embedding dimension, n denotes the patch number. The vision encoder is based on a pre-trained ResNet-50 adopted from PMC-CLIP <ref type="bibr" target="#b19">[20]</ref>, with a trainable projection module. To produce a fixed shape of visual output, we add a trainable projection module on top of the ResNet-50, with the aim of bridging the gap between the pre-trained visual and language embeddings. More specifically, in the case of MedVInT-TE, the image feature is pooled and then fed into a two-layer Multilayer Perceptron (MLP), yielding v ? R 1?d .</p><p>Language Encoder. Given one question on the image, to guide the language model with desirable output, we append a fixed prompt with the question, i.e., "Question: q, the answer is:", and encode it with the language encoder: q = ? text (q) ? R l?d , where q refers to the text embedding, l represents the sequential length for the question, and q is the prompted question. ? text is initialized with the pre-trained language model. Note that our model can also be applied to multiple-choice tasks, by providing options and training it to output the right choice as "A/B/C/D". The prompt is then modified as "Question: q, the options are: a 1 , a 2 , a 3 , a 4 , the answer is:", where a i refers to the i-th option.</p><p>Multimodal Decoder. With encoded visual embeddings (v) and question embeddings (q), we concatenate them as the input to the multimodal decoder (? dec ). The multimodal decoder is initialized from scratch with a 4-layer transformer structure. Additionally, acknowledging that the encoder-based language models lack casual masking, we reform the generation task as a mask language modeling task, i.e., the question input is padded with several '[MASK]' token and the decoder module learns to generate the prediction for the masked token.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.2">MedVInT-TD</head><p>Visual Encoder. The visual encoder is the same as MedVInT-TE except for the projection module, the projection module is a 12-layer transformer decoder with several learnable vectors as query input.</p><p>It takes a variable number of image features from the visual encoder as key and value input and produces a fixed number of visual outputs.</p><p>Text Encoder. We design ? text as a simple embedding layer similar to the primary GPT-like LLMs and initialized with their parameters. Same with MedVInT-TE, it also encodes the question input into embedding features q and can perform multi-choice or blank through different prompts.</p><p>Multimodal Decoder. For the Transformer decoder-based language model, with its output format already being free-form text, we directly use its architecture as the multimodal decoder initialized with the pre-train weights. Specifically, we concatenate the image and text features as the input. However, directly using the text decoder as a multimodal decoder, may lead to significant mismatching between the image encoding space and the decoder input space. Therefore, to further fill the gap between the image embedding space, here, we pre-train the whole network using the PMC-OA <ref type="bibr" target="#b19">[20]</ref> dataset in a caption-based manner, which is similar to BLIP-2 <ref type="bibr" target="#b18">[19]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">The PMC-VQA Dataset</head><p>Our study has identified the lack of large-scale, multi-modal MedVQA datasets as a significant obstacle to the development of effective generative MedVQA models. To address this issue, we present a scalable and automatic pipeline for creating a new large MedVQA dataset. In this section, we provide a detailed description of our dataset collection process, starting with the source data and continuing with the question-answer generation and data filtering procedures. Finally, we analyze the collected data from various perspectives to gain insights into its properties and potential applications.</p><p>Source Data. We start from PMC-OA <ref type="bibr" target="#b19">[20]</ref>, which is a comprehensive biomedical dataset comprising 1.6 million image-text pairs collected from PubMedCentral (PMC)'s OpenAccess subset <ref type="bibr" target="#b30">[31]</ref>, which covers 2.4 million papers. In order to maintain the diversity and complexity of PMC-VQA, we have used a version of 381K image-caption pairs obtained from the first stage of the medical figure collection process without subfigure auto-separation. We have opted not to use the final released version of the dataset, which only includes subfigure separation, subcaption separation, and alignment, in order to maintain a certain level of complexity and avoid oversimplifying the dataset.</p><p>Question-Answer Generation. To automatically generate high-quality question-answer pairs within the constraints of an academic budget, we leverage the power of ChatGPT by inputting the image captions of PMC-OA as the content to the model. We use the following prompt to generate 5 question-answer pairs for each caption.  To answer questions related to these images, the network must acquire sufficient medical knowledge, for example, for the first two images, it is essential to recognize the anatomy structure and modalities; for the third image, recognizing the X-ray image pattern of pathologies is necessary; for the final two images, apart from the basic biomedical knowledge, the model is also required to discern colors, differentiate subfigures, and perform Optical Character Recognition (OCR).</p><p>Ask 5 questions about the content and generate four options for each question. The questions should be answerable with the information provided in the caption, and the four options should include one correct and three incorrect options, with the position of the correct option randomized. The output should use the following template: i:'the question index' question:'the generate question' choice: 'A:option content B:option content C:option content D:option content' answer: The correct option(A\B\C\D).</p><p>This approach allows us to generate a large volume of diverse and high-quality questions that cover a wide range of medical topics. After generating the question-answer pairs using ChatGPT, we applied a rigorous filtering process to ensure that the pairs met our formatting requirements. As a result, we obtained 1,497,808 question-answer pairs, and since the original captions are linked with images, the pairs can naturally find corresponding images, resulting in an average of 3.93 pairs per image.</p><p>Data Filtering. As the questions are sourced from image captions, some questions can be answered correctly using biomedical knowledge alone without the need for a specific image, for example, question: "which type of MRI sequence shows high signal in the marrow edema?". To address this issue, we trained a question-answer model using LLaMA-7B <ref type="bibr" target="#b34">[35]</ref> with text data only and eliminated all questions that could be potentially answerable by the language model. This filtering process resulted in 848,433 high-quality question-answer pairs.</p><p>Furthermore, some questions in our data rely on additional information in the caption that cannot be answered using only the corresponding image, such as "How many patients were classified into the middle stage?" To identify these questions, we trained a question classification model to determine whether a question is answerable given the image alone. Specifically, we manually annotated 2192 question-answer pairs and randomly split them into a training set of 1752 pairs and a testing set of 440 pairs. We fine-tuned LLaMA-7B <ref type="bibr" target="#b34">[35]</ref> on this training set, and our model achieved an accuracy of 81.77% on the test set. We then used this model for data cleaning, resulting in a total of 226,946 question-answer pairs corresponding to 149,075 images. From this cleaned dataset, we randomly selected 50,000 image-question pairs to create our test set, namely, PMC-VQA-test. Additionally, we also provided a small clean test set of 2,000 samples, which were manually verified for quality, termed as PMC-VQA-test-clean. During this manual verification procedure, we have estimated that over 80% of PMC-VQA-test can be retained.</p><p>Data Analysis. This section provides an analysis on images, questions, and answers in the PMC-VQA dataset. In detail, the dataset comprises 227k image-question pairs, some examples are presented in Fig. <ref type="figure" target="#fig_2">3</ref>, which demonstrates the wide diversity of images within our dataset. As indicated in Table1, PMC-VQA outperforms existing MedVQA datasets in terms of data size and modality diversity. The questions in our dataset cover a range of difficulties, from simple questions such as identifying image modalities, perspectives, and organs to challenging questions that require specialized knowledge and judgment. Additionally, our dataset includes difficult questions that demand the ability to identify the specific target sub-figure from the compound figure. types based on the words that start the question, as shown in Fig. <ref type="figure" target="#fig_3">4</ref>. We found a surprising variety of question types, including "What is the difference...", "What type of imaging...", and "Which image shows...". Most questions range from 5 to 15 words, and detailed information about the distribution of question lengths is shown in the supplementary materials. (iii) Answers: The words in answers primarily encompass positional descriptions, image modalities, and specific anatomical regions. Detailed information about the top 50 words that appeared in the answers is provided in the supplementary materials. Most answers are around 5 words, which is much shorter than the questions. The correct options were distributed as follows: A (24.07%), B (30.87%), C (29.09%), D (15.97 %).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experiments</head><p>In this section, we first introduce two existing primary MedVQA datasets, namely VQA-RAD and SLAKE (Sec. 4.1). We then provide a detailed description of our proposed dataset, PMC-VQA, which can be used for both multiple-choice and open-ended answering tasks (Sec. 4.2). Finally, we discuss the primary pre-trained models we use for ablation in Sec. 4.3. The implementation details is provided in the supplementary materials.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Existing MedVQA Datasets</head><p>VQA-RAD <ref type="bibr" target="#b17">[18]</ref> is a VQA dataset specifically designed for radiology, consisting of 315 images and 3,515 questions with 517 possible answers. The questions in VQA-RAD are categorized as either close-ended or open-ended, depending on whether answer choices are limited or not. We follow the official dataset split for our evaluation.</p><p>SLAKE <ref type="bibr" target="#b22">[23]</ref> is an English-Chinese bilingual VQA dataset composed of 642 images and 14k questions. The questions are categorized as close-ended if answer choices are limited, otherwise openended. There are 224 possible answers in total. We only use the "English" part, and follow the official split.</p><p>Baselines and Metrics. We compare with various baselines on these two MedVQA datasets, namely, MEVF-BAN <ref type="bibr" target="#b24">[25]</ref>, CPRD-BAN <ref type="bibr" target="#b21">[22]</ref>, M3AE <ref type="bibr" target="#b5">[6]</ref>, PMC-CLIP <ref type="bibr" target="#b19">[20]</ref>. PMC-CLIP <ref type="bibr" target="#b19">[20]</ref> is the existing SOTA method on these two datasets. For evaluation, ACC scores are used. Note that, since our model is generative-based, we calculate ACC by matching the generative output with the options using difflib.SequenceMatcher and choosing the most similar one as the choice of the model, which is more difficult than the evaluation for retrieval-based methods due to the larger output space.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">PMC-VQA Dataset</head><p>The PMC-VQA dataset consists of a train set with 177K samples and a test set with 50K samples, which are respectively denoted as PMC-VQA-train and PMC-VQA-test. Additionally, the smaller clean test set with 2K samples that have been manually verified, is referred to as PMC-VQA-test-clean. The dataset can be used for both open-ended and multiple-choice tasks.</p><p>Multi-choice MedVQA. Four candidate answers are provided for each question as the prompt. The model is then trained to select the correct option among them. The accuracy (ACC) score can be used to evaluate the performance of the model on this task.</p><p>Open-ended MedVQA. The total possible answers for PMC-VQA are over 100K, which makes the traditional retrieval-based approach limited in usefulness for the answer set of such a level. Therefore, we provide another training style, called "blank", where the network is not provided with options in input and is required to directly generate answers based on the questions. For evaluation, we adopt two metrics. The first is Bleu scores, which are widely used to evaluate the quality of generated text against a set of references. The second is ACC scores, which can be computed by comparing the generated answer with the ground-truth answer using sentence similarity, as introduced in Sec. 4.1.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Pre-trained Backbones</head><p>In this section, we introduce the pre-trained models used in our experiments. We separate them into language and vision backbones. Notably, while all the following models can be used in our architecture, by default, we use the "PMC-LLaMA" (or "PMC-LLaMA-ENC") and "PMC-CLIP" as backbones, since they are known to be more suitable for medical data according to previous works.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.1">Language Backbone</head><p>LLaMA <ref type="bibr" target="#b34">[35]</ref> is a state-of-the-art large-scale language model, pre-trained on trillions of tokens and widely used in the research community. We adopt the 7B version, which consists of 32 transformer layers, as our language backbone. PMC-LLaMA <ref type="bibr" target="#b36">[37]</ref> is an open-source language model that is acquired by fine-tuning LLaMA-7B on a total of 4.8 million biomedical academic papers with auto-regressive loss. Compared to LLaMA, PMC-LLaMA demonstrates stronger fitting capabilities and better performance on medical tasks.</p><p>PubMedBERT <ref type="bibr" target="#b10">[11]</ref> is an encoder-based BERT-like model that is trained from scratch using abstracts from PubMed and full-text articles from PubMedCentral in the corpus "The Pile" <ref type="bibr" target="#b9">[10]</ref>. It has 12 transformer layers and 100 million parameters. Such domain-specific models proved to yield excellent text embedding capability before the era of large language models.</p><p>LLaMA-ENC and PMC-LLaMA-ENC. While LLaMA and PMC-LLaMA are known for their performance in text generation tasks, we also experiment with them as encoder models by passing a full attention mask and sampling the embedding from the last token. This allows for a direct comparison to be made with the aforementioned BERT-like models, which are also encoder-based.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.2">Vision Backbone</head><p>CLIP <ref type="bibr" target="#b29">[30]</ref> is a model trained from scratch on a dataset of 400 million image-text pairs collected from the internet with contrastive loss. We use its "ViT-base-patch32" version as our visual encoder with 12 transformer layers, which has been pre-trained on natural images.</p><p>PMC-CLIP <ref type="bibr" target="#b19">[20]</ref> is a medical-specific visual model based on CLIP architecture, which was trained on a dataset of 1.6 million biomedical image-text pairs collected from PubMed open-access papers using cross-modality contrastive loss. Compared to the pre-trained visual model on natural images, PMC-CLIP is specifically designed to handle medical images and text.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Results</head><p>In this section, we begin by evaluating our model on two publicly-available datasets, VQA-RAD and SLAKE, and compare it with existing MedVQA models, showing state-of-the-art performance. However, these datasets have limited diversity and scope, which led us to propose a more challenging MedVQA benchmark in Sec. 5.2. Our benchmark covers significantly more diverse modalities and diseases, and we demonstrate that even state-of-the-art methods struggle to perform well on it.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Comparison on Existing Datasets</head><p>As shown in Tab. 2, comparing our model to existing ones, we can draw the following observations:</p><p>State-of-the-art Performance of Generative MedVQA. As shown in Tab. 2, our MedVInT model outperforms the previous state-of-the-art (SOTA) methods on both the VQA-RAD and SLAKE datasets, regardless of whether the "MedVInT-TE" or "MedVInT-TD" variant is used. We improved the overall accuracy (ACC) scores from 77.6% to 81.6% on VQA-RAD and from 84.3% to 88.0% on SLAKE. Notably, since our model generates answers rather than retrieving one from a pre-defined answer basis, the evaluation metric is more challenging, further demonstrating our superiority.</p><p>Pre-training on PMC-VQA is Essential for Generative MedVQA. Comparing results using the same architecture, with and without PMC-VQA, it is clear that pre-training with PMC-VQA significantly outperforms. Specifically, "MedVInT-TE" boosts the final results by approximately 11% on VQA-RAD and 4% on SLAKE compared to "MedVInT-TE-S" that refers to training the model from scratch without pre-trained on PMC-VQA. Similar improvements are observed with 'MedVInT-TD'. These results highlight the critical role that our PMC-VQA plays in addressing the major challenges that hinder the development of a generative MedVQA system.</p><p>Both MedVInT-TE and MedVInT-TD Perform Well. The gap between the two training styles mainly exists in open-ended questions, with "MedVInT-TD" performing better on VQA-RAD and "MedVInT-TE" being more effective on SLAKE. This difference can be attributed to the fact that the VQA-RAD answers are typically longer than those in SLAKE, making the "MedVInT-TD" model more suitable for generating such answers. Conversely, SLAKE questions often require short and concise responses, making the MedVInT-TE" model more appropriate for such retrieve-like tasks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Benchmark on PMC-VQA</head><p>In this section, we introduce our new MedVQA benchmark on PMC-VQA. We evaluate different methods for both open-ended and multiple-choice tasks. The results are summarized in Tab. 3 (See supplementary for more qualitative comparisons.).We can draw the following observations:</p><p>Multimodal Understanding is Essential. As shown in Tab. 3, when using only language, the model struggles to provide accurate answers and produces nearly random outcomes, with accuracies of only 26.1% in Blanking and 30.6% in Choice. It is worth noting that around 30% of the questions have "B" answers, making the 30.6% score nearly equivalent to the highest possible score attainable through guessing. These observations highlight the crucial role of multimodal understanding in our dataset and emphasize the strong relationship between the images and the questions posed.</p><p>General Visual-language Models Struggle on MedVQA. We evaluated the zero-shot performance of existing SOTA multimodal models, BLIP-2 and open-source version of Flamingo <ref type="bibr" target="#b18">[19,</ref><ref type="bibr" target="#b3">4]</ref>. As shown, even the best-performing models in natural images struggle to answer our MedVQA questions, demonstrating the challenging nature of our dataset and its strong biomedical relevance.</p><p>PMC-VQA-test Presents a Significantly More Challenging Benchmark. Notably, the previous SOTA multimodal model for MedVQA, PMC-CLIP <ref type="bibr" target="#b19">[20]</ref>, struggles on our dataset. Not only does it fail to solve the blanking task, but it also significantly underperforms on multi-choice questions, with accuracy close to random. These findings underline the difficulty of our dataset and its potential to serve as a more robust benchmark for evaluating VQA models.</p><p>Comparing Generative Model Backbones on PMC-VQA-test. To further assess the effectiveness of our proposed method, we compared it against various baselines that use different generative model backbones. Our results show that replacing the general visual backbone with a specialized medical one leads to improved performance, highlighting the importance of visual understanding in MedVQA. Additionally, we observed that replacing the language backbone with a domain-specific model also leads to some improvements, although not as significant as those achieved in the visual domain.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Related Works</head><p>Instruction Tuning with Large-language Models. Large Language Models (LLMs) have recently achieved tremendous success <ref type="bibr" target="#b27">[28,</ref><ref type="bibr" target="#b26">27,</ref><ref type="bibr" target="#b0">1]</ref> in generating high-quality text for various tasks such as language translation, summarization, and question answering. Open-source models, e.g., Alpaca <ref type="bibr" target="#b33">[34]</ref>, have proposed instruction tuning to train models using examples generated from ChatGPT <ref type="bibr" target="#b0">[1]</ref>, effectively improving the performance of language models. In the visual-language domain, concurrent work to ours, Mini-GPT4 <ref type="bibr" target="#b38">[39]</ref> generates a high-quality image-text dataset by prompting ChatGPT with well-designed inputs. In this paper, we focus on visual instruction tuning for MedVQA, which poses unique challenges due to the complexity of medical texts and the variability of medical images.</p><p>Medical Visual Question Answering. The field of MedVQA has gained significant interest in recent years, with a growing number of studies <ref type="bibr" target="#b20">[21]</ref>. Despite the increasing attention, building a robust and reliable MedVQA system remains challenging due to the complexity and variability of medical images, as well as the lack of large-scale and diverse MedVQA datasets. Existing publicly available MedVQA datasets have limitations on diversity, or dataset scale, for example, RadVisDial <ref type="bibr" target="#b15">[16]</ref> only contains samples on chest x-ray images, VQA-Med <ref type="bibr" target="#b4">[5]</ref>, VQA-RAD <ref type="bibr" target="#b17">[18]</ref>, and SLAKE <ref type="bibr" target="#b22">[23]</ref> have less than 10K images. To address these limitations, we propose the PMC-VQA dataset that includes 227k image-question pairs with various image modalities and question types.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">Conclusion</head><p>In conclusion, this paper addresses the challenge of MedVQA, where even the strongest VQA models trained on natural images yield results that closely resemble random guesses. To overcome this, we propose MedVInT, a generative model tailored to advance this crucial medical task. MedVInT is trained by aligning visual data from a pre-trained vision encoder with language models. Additionally, we present a scalable pipeline for constructing PMC-VQA, a comprehensive MedVQA dataset comprising 227k VQA pairs across 149k images, spanning diverse modalities and diseases. Our proposed model delivers state-of-the-art performance on existing MedVQA datasets, providing a new and reliable benchmark for evaluating different methods in this field.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B Implementation Details</head><p>Our models are trained using the AdamW optimizer <ref type="bibr" target="#b23">[24]</ref> with a learning rate 2e-5. The max context length is set as 512, and the batch size is 128. To improve the training speed of our models, we adopt the Deepspeed acceleration strategy, together with Automatic Mixed Precision (AMP) and gradient checkpointing <ref type="bibr" target="#b8">[9]</ref>. All models are implemented in PyTorch and trained on NVIDIA A100 GPU with 80 GB memory</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C Social Impact</head><p>In an era where the digitization of healthcare is rapidly advancing, and medical data is proliferating, multimodal tools such as Medical Visual Question Answering (MedVQA) present significant potential to revolutionize patient care, empower clinicians, and bolster research. Our contribution in this field is twofold: First, we introduce a scalable pipeline for the creation of a MedVQA dataset. This scalability ensures a continuous evolution and expansion of the dataset, maintaining its relevance in the everchanging landscape of healthcare. Second, we present the PMC-VQA dataset, crafted to overcome the limitations inherent in existing datasets. By encompassing a larger, more diverse selection of medical images, complemented by sophisticated questions and answers, we aim to significantly enhance the reliability and precision of medical multimodal models. This innovation holds the promise of equipping these models with the necessary tools to effectively navigate real-world scenarios.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D Limitation</head><p>The proposed PMC-VQA has several limitations:</p><p>Inherent Biases: Despite efforts to construct a comprehensive MedVQA dataset with PMC-VQA, it is important to acknowledge the potential presence of biases in the dataset. Biases might arise from the data collection process, annotation methodology, or underlying distribution of the medical images and questions. Understanding and addressing these biases is crucial for ensuring fair and unbiased performance evaluation.</p><p>Potential Annotation Biases: Despite efforts to ensure quality and accuracy during the annotation process of PMC-VQA-test-clean, the dataset may still be susceptible to annotation biases. The subjective nature of question-answer pairs and the involvement of human annotators introduces the possibility of inconsistencies or subjective interpretations, which could impact the dataset's reliability.</p><p>Lacking Comprehensive Evaluation Metrics: Although both the ACC score and Bleu score are utilized in our benchmark for assessing open-ended blanking results, these two metrics fail to capture the fluency of the generated sentence since they measure string similarity irrespective of word order. As exhibited in the third case of Fig. <ref type="figure">5</ref>, the encoder-based model significantly underperforms compared to the decoder-based model in this regard, a fact not reflected in the quantitative results. Indeed, finding an objective way to evaluate generative results comprehensively poses a significant challenge in the entire generative model community <ref type="bibr" target="#b6">[7]</ref>. To address this issue, we plan to explore more evaluation metrics in our benchmark in future work.</p><p>Need for Continual Dataset Expansion and Updates: The medical field is dynamic, with ongoing advancements and new findings. To ensure the dataset's relevance and coverage of emerging medical knowledge, continual expansion and updates to the PMC-VQA dataset are necessary.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: (a) The proposed architecture of MedVInt, mainly consists of three components: a visual encoder to extract visual features, a language encoder to encode textual context, and a multimodal decoder to generate the answer; (b) The proposed question-answer pairs generation pipeline.</figDesc><graphic url="image-9.png" coords="3,309.89,141.73,181.57,54.36" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head></head><label></label><figDesc>Question: What is the view of the brain used in the image? A: Axial B: Coronal C: Sagittal D: Oblique Question: What medical imaging technique was used to obtain the image? A: MRI B: PET scan C: CT scan D: X-ray Question: Which part of the lung is affected by the pneumothorax in the image? A: Right middle lobe B: Left lower lobe C: Right apical lobe D: Left apical lobe Question: What is the color of the actin cap in the images? A: Green B: Red C: Yellow D: Blue Question: What does the circle in image D surround? A: Abnormal mitotic figures B: Central keratinization C: Frank atypia D: Areas of necrosis</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: Several examples of challenging questions and answers along with their respective images.To answer questions related to these images, the network must acquire sufficient medical knowledge, for example, for the first two images, it is essential to recognize the anatomy structure and modalities; for the third image, recognizing the X-ray image pattern of pathologies is necessary; for the final two images, apart from the basic biomedical knowledge, the model is also required to discern colors, differentiate subfigures, and perform Optical Character Recognition (OCR).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 4 :</head><label>4</label><figDesc>Figure 4: Question distribution of the training set by their first four words. From left to right are all questions, questions started with "What" and questions started with "Which". The ordering of the words starts towards the center and radiates outwards.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 6 :</head><label>6</label><figDesc>Figure 6: Answer distribution of training set.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 7 :</head><label>7</label><figDesc>Figure 7: Percentage of questions and answers with different word lengths.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 2 :</head><label>2</label><figDesc>Comparison of ACC to SOTA approaches on VQA-RAD and SLAKE. We use the blank model for evaluation. Pre-training data indicates whether the model is pre-trained on the medical multi-modal dataset before training on the target dataset. The best result is in red, the second-best result is in blue. "Overal" refers to the micro-average ACC of all the Open and Close questions.</figDesc><table><row><cell>Method</cell><cell>Pre-training Data</cell><cell cols="6">VQA-RAD Open Close Overall Open Close Overall SLAKE</cell></row><row><cell cols="2">MEVF-BAN [25] -</cell><cell>49.2</cell><cell>77.2</cell><cell>66.1</cell><cell>77.8</cell><cell>79.8</cell><cell>78.6</cell></row><row><cell cols="2">CPRD-BAN [22] -</cell><cell>52.5</cell><cell>77.9</cell><cell>67.8</cell><cell>79.5</cell><cell>83.4</cell><cell>81.1</cell></row><row><cell>M3AE [6]</cell><cell>ROCO [29], MedICaT [33]</cell><cell>67.2</cell><cell>83.5</cell><cell>77.0</cell><cell>80.3</cell><cell>87.8</cell><cell>83.3</cell></row><row><cell>PMC-CLIP [20]</cell><cell>PMC-OA [20]</cell><cell>67.0</cell><cell>84.0</cell><cell>77.6</cell><cell>81.9</cell><cell>88.0</cell><cell>84.3</cell></row><row><cell>MedVInT-TE-S</cell><cell>-</cell><cell>53.6</cell><cell>76.5</cell><cell>67.4</cell><cell>84.0</cell><cell>85.1</cell><cell>84.4</cell></row><row><cell>MedVInT-TD-S</cell><cell>-</cell><cell>55.3</cell><cell>80.5</cell><cell>70.5</cell><cell>79.7</cell><cell>85.1</cell><cell>81.8</cell></row><row><cell>MedVInT-TE</cell><cell>PMC-VQA</cell><cell>69.3</cell><cell>84.2</cell><cell>78.2</cell><cell>88.2</cell><cell>87.7</cell><cell>88.0</cell></row><row><cell>MedVInT-TD</cell><cell>PMC-VQA</cell><cell>73.7</cell><cell>86.8</cell><cell>81.6</cell><cell>84.5</cell><cell>86.3</cell><cell>85.2</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 3 :</head><label>3</label><figDesc>Comparison of baseline models using different pre-trained models on both open-ended and multiple-choice tasks. We reported the results on PMC-VQA-test / PMC-VQA-test-clean. "Scratch" means to train the vision model from scratch with the same architecture as "PMC-CLIP".</figDesc><table><row><cell>Method</cell><cell>Language Backbone</cell><cell>Vision Backbone</cell><cell cols="2">Blanking ACC Bleu-1</cell><cell>Choice ACC</cell></row><row><cell>Zero-shot</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>PMC-CLIP [20]</cell><cell>PMC-CLIP [20]</cell><cell>PMC-CLIP [20]</cell><cell>-</cell><cell>-</cell><cell>24.0 / 24.7</cell></row><row><cell>BLIP-2[19]</cell><cell>OPT-2.7B [38]</cell><cell>CLIP [30]</cell><cell>22.5 / 21.8</cell><cell>5.2 / 7.6</cell><cell>24.6 / 24.3</cell></row><row><cell cols="2">Open-Flamingo [4] LLaMA[35]</cell><cell>CLIP [30]</cell><cell>26.1 / 26.5</cell><cell>4.1 / 4.1</cell><cell>25.0 / 26.4</cell></row><row><cell cols="2">Trained on PMC-VQA</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>LLaMA [35]</cell><cell>LLaMA [35]</cell><cell>-</cell><cell cols="3">26.1 / 27.2 14.2 / 14.6 30.6 / 30.8</cell></row><row><cell></cell><cell></cell><cell>Scratch</cell><cell cols="3">33.7 / 34.2 20.4 / 20.9 34.4 / 34.9</cell></row><row><cell></cell><cell>PubMedBERT [11]</cell><cell>CLIP [30]</cell><cell cols="3">33.7 / 34.4 20.4 / 20.8 34.5 / 34.3</cell></row><row><cell></cell><cell></cell><cell cols="4">PMC-CLIP [20] 35.2 / 36.4 22.0 / 23.2 37.1 / 37.6</cell></row><row><cell>MedVInT-TE</cell><cell>LLaMA-ENC [35]</cell><cell>Scratch CLIP [30]</cell><cell cols="3">32.5 / 32.5 15.3 / 15.9 35.2 / 35.1 32.3 / 33.4 15.6 / 15.1 35.3 / 36.1</cell></row><row><cell></cell><cell></cell><cell cols="4">PMC-CLIP [20] 35.4 / 36.8 18.2 / 18.4 36.9 / 37.1</cell></row><row><cell></cell><cell></cell><cell>Scratch</cell><cell cols="3">32.6 / 35.0 16.2 / 17.0 37.0 / 38.0</cell></row><row><cell></cell><cell>PMC-LLaMA-ENC [37]</cell><cell>CLIP [30]</cell><cell cols="3">33.0 / 34.4 16.6 / 16.5 37.1 / 38.5</cell></row><row><cell></cell><cell></cell><cell cols="4">PMC-CLIP [20] 34.8 / 35.3 18.1 / 18.6 38.2 / 39.2</cell></row><row><cell></cell><cell></cell><cell>Scratch</cell><cell cols="3">29.1 / 30.2 17.4 / 18.0 36.2 / 37.9</cell></row><row><cell></cell><cell>LLaMA[35]</cell><cell>CLIP [30]</cell><cell cols="3">31.3 / 32.2 19.5 / 20.0 38.2 / 38.3</cell></row><row><cell>MedVInT-TD</cell><cell></cell><cell cols="4">PMC-CLIP [20] 31.9 / 33.4 20.0 / 21.3 37.3 / 39.5</cell></row><row><cell></cell><cell></cell><cell>Scratch</cell><cell cols="3">28.6 / 29.8 16.8 / 17.4 36.8 / 36.9</cell></row><row><cell></cell><cell>PMC-LLaMA [37]</cell><cell>CLIP [30]</cell><cell cols="3">31.4 / 32.6 19.5 / 20.4 36.8 / 36.9</cell></row><row><cell></cell><cell></cell><cell cols="4">PMC-CLIP [20] 32.7 / 33.6 20.3 / 21.5 39.4 / 40.3</cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" xml:id="foot_0"><p>https://docs.python.org/3/library/difflib.html</p></note>
		</body>
		<back>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A PMC-VQA Dataset A.1 Examples</head><p>In order to provide a more comprehensive understanding of the dataset, we offer additional examples illustrated in Fig. <ref type="figure">5</ref>. This figure showcases random instances of the original image and corresponding captions, along with multiple-choice questions generated from them. Additionally, we present the predictions of MedVInT-TE and MedVInT-TD models, with PMC-CLIP and PMC-LLAMA as their vision and language backbones.  </p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title/>
		<ptr target="https://openai.com/blog/chatgpt/,2023.1" />
	</analytic>
	<monogr>
		<title level="j">Openai. introducing chatgpt</title>
		<imprint>
			<biblScope unit="page">9</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Flamingo: a visual language model for few-shot learning</title>
		<author>
			<persName><forename type="first">Jean-Baptiste</forename><surname>Alayrac</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jeff</forename><surname>Donahue</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pauline</forename><surname>Luc</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Antoine</forename><surname>Miech</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Iain</forename><surname>Barr</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yana</forename><surname>Hasson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Karel</forename><surname>Lenc</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Arthur</forename><surname>Mensch</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Katherine</forename><surname>Millican</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Malcolm</forename><surname>Reynolds</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in Neural Information Processing Systems</title>
		<imprint>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="23716" to="23736" />
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">The medical segmentation decathlon</title>
		<author>
			<persName><forename type="first">Michela</forename><surname>Antonelli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Annika</forename><surname>Reinke</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Spyridon</forename><surname>Bakas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Keyvan</forename><surname>Farahani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Annette</forename><surname>Kopp-Schneider</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bennett</forename><forename type="middle">A</forename><surname>Landman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Geert</forename><surname>Litjens</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bjoern</forename><surname>Menze</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Olaf</forename><surname>Ronneberger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ronald</forename><forename type="middle">M</forename><surname>Summers</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nature Communications</title>
		<imprint>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page">4128</biblScope>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title/>
		<author>
			<persName><forename type="first">Anas</forename><surname>Awadalla</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Irena</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Joshua</forename><surname>Gardner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jack</forename><surname>Hessel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yusuf</forename><surname>Hanafy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wanrong</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yonatan</forename><surname>Kalyani Marathe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Samir</forename><surname>Bitton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jenia</forename><surname>Gadre</surname></persName>
		</author>
		<author>
			<persName><surname>Jitsev</surname></persName>
		</author>
		<ptr target="https://github.com/mlfoundations/open_flamingo" />
		<imprint>
			<date type="published" when="2023">2023</date>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page">9</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Overview of the vqa-med task at imageclef 2021: Visual question answering and generation in the medical domain</title>
		<author>
			<persName><forename type="first">Asma</forename><surname>Ben Abacha</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mourad</forename><surname>Sarrouti</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dina</forename><surname>Demner-Fushman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sadid</forename><forename type="middle">A</forename><surname>Hasan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Henning</forename><surname>M?ller</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the CLEF 2021 Conference and Labs of the Evaluation Forum-working notes</title>
		<meeting>the CLEF 2021 Conference and Labs of the Evaluation Forum-working notes</meeting>
		<imprint>
			<date type="published" when="2021-09">September 2021, 2021</date>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">10</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Multi-modal masked autoencoders for medical vision-and-language pre-training</title>
		<author>
			<persName><forename type="first">Zhihong</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuhao</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jinpeng</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Guanbin</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiang</forename><surname>Wan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tsung-Hui</forename><surname>Chang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Medical Image Computing and Computer Assisted Intervention</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2008">2022. 1, 6, 8</date>
			<biblScope unit="page" from="679" to="689" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">Vicuna: An open-source chatbot impressing gpt-4 with 90%* chatgpt quality</title>
		<author>
			<persName><forename type="first">Wei-Lin</forename><surname>Chiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhuohan</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zi</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ying</forename><surname>Sheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhanghao</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hao</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lianmin</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Siyuan</forename><surname>Zhuang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yonghao</forename><surname>Zhuang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Joseph</forename><forename type="middle">E</forename><surname>Gonzalez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ion</forename><surname>Stoica</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Eric</forename><forename type="middle">P</forename><surname>Xing</surname></persName>
		</author>
		<ptr target="https://lmsys.org/blog/2023-03-30-vicuna/.15" />
		<imprint>
			<date type="published" when="2023-03">March 2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<author>
			<persName><forename type="first">Aakanksha</forename><surname>Chowdhery</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sharan</forename><surname>Narang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jacob</forename><surname>Devlin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Maarten</forename><surname>Bosma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gaurav</forename><surname>Mishra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Adam</forename><surname>Roberts</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Paul</forename><surname>Barham</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hyung</forename><forename type="middle">Won</forename><surname>Chung</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Charles</forename><surname>Sutton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sebastian</forename><surname>Gehrmann</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2204.02311</idno>
		<title level="m">Scaling language modeling with pathways</title>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Optimal gradient checkpoint search for arbitrary computation graphs</title>
		<author>
			<persName><forename type="first">Jianwei</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dong</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page">14</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<author>
			<persName><forename type="first">Leo</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stella</forename><surname>Biderman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sid</forename><surname>Black</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Laurence</forename><surname>Golding</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Travis</forename><surname>Hoppe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Charles</forename><surname>Foster</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jason</forename><surname>Phang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Horace</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Anish</forename><surname>Thite</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Noa</forename><surname>Nabeshima</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shawn</forename><surname>Presser</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Connor</forename><surname>Leahy</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2101.00027</idno>
		<title level="m">The Pile: An 800gb dataset of diverse text for language modeling</title>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Domain-specific language model pretraining for biomedical natural language processing</title>
		<author>
			<persName><forename type="first">Yu</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Robert</forename><surname>Tinn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hao</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><surname>Lucas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Naoto</forename><surname>Usuyama</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaodong</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tristan</forename><surname>Naumann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jianfeng</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hoifung</forename><surname>Poon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Transactions on Computing for Healthcare (HEALTH)</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page">8</biblScope>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">Towards visual question answering on pathology images</title>
		<author>
			<persName><forename type="first">Xuehai</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yichen</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Luntian</forename><surname>Mou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Eric</forename><surname>Xing</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pengtao</forename><surname>Xie</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="708" to="718" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">What disease does this patient have? a large-scale open domain question answering dataset from medical exams</title>
		<author>
			<persName><forename type="first">Di</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Eileen</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nassim</forename><surname>Oufattole</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wei-Hung</forename><surname>Weng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hanyi</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Peter</forename><surname>Szolovits</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Applied Sciences</title>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="issue">14</biblScope>
			<biblScope unit="page">6421</biblScope>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Peir digital library: Online resources and authoring system</title>
		<author>
			<persName><forename type="first">Kristopher</forename><forename type="middle">N</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dwain</forename><forename type="middle">E</forename><surname>Woode</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kristina</forename><surname>Panizzi</surname></persName>
		</author>
		<author>
			<persName><surname>Peter G Anderson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the AMIA Symposium</title>
		<meeting>the AMIA Symposium</meeting>
		<imprint>
			<date type="published" when="2001">2001</date>
			<biblScope unit="page">1075</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Chaos challengecombined (ct-mr) healthy abdominal organ segmentation</title>
		<author>
			<persName><forename type="first">N</forename><surname>Emre Kavur</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mustafa</forename><surname>Sinem Gezer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sinem</forename><surname>Bar??</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pierre-Henri</forename><surname>Aslan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vladimir</forename><surname>Conze</surname></persName>
		</author>
		<author>
			<persName><surname>Groza</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Duy</forename><surname>Duc</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Soumick</forename><surname>Pham</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Philipp</forename><surname>Chatterjee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sava?</forename><surname>Ernst</surname></persName>
		</author>
		<author>
			<persName><surname>?zkan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Medical Image Analysis</title>
		<imprint>
			<biblScope unit="volume">69</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page">101950</biblScope>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Towards visual dialog for radiology</title>
		<author>
			<persName><forename type="first">Olga</forename><surname>Kovaleva</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chaitanya</forename><surname>Shivade</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Satyananda</forename><surname>Kashyap</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Karina</forename><surname>Kanjaria</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Joy</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Deddeh</forename><surname>Ballah</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Adam</forename><surname>Coy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alexandros</forename><surname>Karargyris</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yufan</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><surname>Beymer Beymer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 19th SIGBioMed Workshop on Biomedical Language Processing</title>
		<meeting>the 19th SIGBioMed Workshop on Biomedical Language Processing</meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="60" to="69" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Performance of chatgpt on usmle: Potential for ai-assisted medical education using large language models</title>
		<author>
			<persName><forename type="first">Tiffany</forename><forename type="middle">H</forename><surname>Kung</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Morgan</forename><surname>Cheatham</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Arielle</forename><surname>Medenilla</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Czarina</forename><surname>Sillos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lorie</forename><forename type="middle">De</forename><surname>Leon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Camille</forename><surname>Elepa?o</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Maria</forename><surname>Madriaga</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rimel</forename><surname>Aggabao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Giezel</forename><surname>Diaz-Candido</surname></persName>
		</author>
		<author>
			<persName><forename type="first">James</forename><surname>Maningo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">PLoS digital health</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page">198</biblScope>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">A dataset of clinically generated visual questions and answers about radiology images</title>
		<author>
			<persName><forename type="first">Jason</forename><forename type="middle">J</forename><surname>Lau</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Soumya</forename><surname>Gayen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Asma</forename><surname>Ben Abacha</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dina</forename><surname>Demner-Fushman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Scientific data</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page">10</biblScope>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">Blip-2: Bootstrapping languageimage pre-training with frozen image encoders and large language models</title>
		<author>
			<persName><forename type="first">Junnan</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dongxu</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Silvio</forename><surname>Savarese</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Steven</forename><surname>Hoi</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2301.12597</idno>
		<imprint>
			<date type="published" when="2009">2023. 1, 4, 8, 9</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">Pmc-clip: Contrastive language-image pre-training using biomedical documents</title>
		<author>
			<persName><forename type="first">Weixiong</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ziheng</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaoman</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chaoyi</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ya</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yanfeng</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Weidi</forename><surname>Xie</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2303.07240</idno>
		<imprint>
			<date type="published" when="2009">2023. 1, 2, 3, 4, 6, 7, 8, 9</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<author>
			<persName><forename type="first">Zhihong</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Donghao</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qingyi</forename><surname>Tac</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Danli</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gholamreza</forename><surname>Haffari</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qi</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mingguang</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zongyuan</forename><surname>Ge</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2111.10056</idno>
		<title level="m">Medical visual question answering: A survey</title>
		<imprint>
			<date type="published" when="2022">2022</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">9</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Contrastive pre-training and representation distillation for medical visual question answering based on radiology images</title>
		<author>
			<persName><forename type="first">Bo</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Li-Ming</forename><surname>Zhan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiao-Ming</forename><surname>Wu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In Medical Image Computing and Computer Assisted Intervention</title>
		<imprint>
			<biblScope unit="page" from="210" to="220" />
			<date type="published" when="2008">2021. 1, 6, 8</date>
			<publisher>Springer</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Slake: A semanticallylabeled knowledge-enhanced dataset for medical visual question answering</title>
		<author>
			<persName><forename type="first">Bo</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Li-Ming</forename><surname>Zhan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Li</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lin</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yan</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiao-Ming</forename><surname>Wu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2021 IEEE 18th International Symposium on Biomedical Imaging (ISBI)</title>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">10</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<author>
			<persName><forename type="first">Ilya</forename><surname>Loshchilov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Frank</forename><surname>Hutter</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1711.05101</idno>
		<title level="m">Decoupled weight decay regularization</title>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page">14</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Overcoming data limitation in medical visual question answering</title>
		<author>
			<persName><forename type="first">Thanh-Toan</forename><surname>Binh D Nguyen</surname></persName>
		</author>
		<author>
			<persName><surname>Do</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tuong</forename><surname>Binh X Nguyen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Erman</forename><surname>Do</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Quang D</forename><surname>Tjiputra</surname></persName>
		</author>
		<author>
			<persName><surname>Tran</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In Medical Image Computing and Computer Assisted Intervention</title>
		<imprint>
			<biblScope unit="page" from="522" to="530" />
			<date type="published" when="2008">2019. 1, 6, 8</date>
			<publisher>Springer</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<author>
			<persName><forename type="first">Harsha</forename><surname>Nori</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nicholas</forename><surname>King</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Scott</forename><forename type="middle">Mayer</forename><surname>Mckinney</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dean</forename><surname>Carignan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Eric</forename><surname>Horvitz</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2303.13375</idno>
		<title level="m">Capabilities of gpt-4 on medical challenge problems</title>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<idno type="arXiv">arXiv:2303.08774</idno>
		<title level="m">OpenAI. Gpt-4 technical report</title>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Training language models to follow instructions with human feedback</title>
		<author>
			<persName><forename type="first">Long</forename><surname>Ouyang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jeffrey</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xu</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Diogo</forename><surname>Almeida</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Carroll</forename><surname>Wainwright</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pamela</forename><surname>Mishkin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chong</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sandhini</forename><surname>Agarwal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Katarina</forename><surname>Slama</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alex</forename><surname>Ray</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in Neural Information Processing Systems</title>
		<imprint>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="issue">9</biblScope>
			<biblScope unit="page" from="27730" to="27744" />
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Radiology objects in context (roco): a multimodal image dataset</title>
		<author>
			<persName><forename type="first">Obioma</forename><surname>Pelka</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sven</forename><surname>Koitka</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Johannes</forename><surname>R?ckert</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Felix</forename><surname>Nensa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christoph</forename><forename type="middle">M</forename><surname>Friedrich</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">MICCAI Workshop on Large-scale Annotation of Biomedical Data and Expert Label Synthesis (LABELS) 2018</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="180" to="189" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Learning transferable visual models from natural language supervision</title>
		<author>
			<persName><forename type="first">Alec</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jong</forename><forename type="middle">Wook</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chris</forename><surname>Hallacy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aditya</forename><surname>Ramesh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gabriel</forename><surname>Goh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sandhini</forename><surname>Agarwal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Girish</forename><surname>Sastry</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Amanda</forename><surname>Askell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pamela</forename><surname>Mishkin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jack</forename><surname>Clark</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International conference on machine learning</title>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page">8</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Pubmed central: The genbank of the published literature</title>
		<author>
			<persName><forename type="first">J</forename><surname>Richard</surname></persName>
		</author>
		<author>
			<persName><surname>Roberts</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">National Acad Sciences</title>
		<imprint>
			<biblScope unit="volume">98</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="381" to="382" />
			<date type="published" when="2001">2001</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
		<title level="m" type="main">Large language models encode clinical knowledge</title>
		<author>
			<persName><forename type="first">Karan</forename><surname>Singhal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shekoofeh</forename><surname>Azizi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tao</forename><surname>Tu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sara</forename><surname>Mahdavi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jason</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hyung</forename><forename type="middle">Won</forename><surname>Chung</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nathan</forename><surname>Scales</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ajay</forename><surname>Tanwani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Heather</forename><surname>Cole-Lewis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stephen</forename><surname>Pfohl</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2212.13138</idno>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Medicat: A dataset of medical images, captions, and textual references</title>
		<author>
			<persName><forename type="first">Sanjay</forename><surname>Subramanian</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Findings of EMNLP</title>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
		<author>
			<persName><forename type="first">Rohan</forename><surname>Taori</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ishaan</forename><surname>Gulrajani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tianyi</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yann</forename><surname>Dubois</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xuechen</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Carlos</forename><surname>Guestrin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Percy</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tatsunori</forename><forename type="middle">B</forename><surname>Hashimoto</surname></persName>
		</author>
		<ptr target="https://github.com/tatsu-lab/stanford_alpaca" />
		<title level="m">Stanford alpaca: An instruction-following llama model</title>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<monogr>
		<title level="m" type="main">Llama: Open and efficient foundation language models</title>
		<author>
			<persName><forename type="first">Hugo</forename><surname>Touvron</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Thibaut</forename><surname>Lavril</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gautier</forename><surname>Izacard</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xavier</forename><surname>Martinet</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Marie-Anne</forename><surname>Lachaux</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Timoth?e</forename><surname>Lacroix</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Baptiste</forename><surname>Rozi?re</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Naman</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Eric</forename><surname>Hambro</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Faisal</forename><surname>Azhar</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2302.13971</idno>
		<imprint>
			<date type="published" when="2008">2023. 1, 5, 7, 8</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Chestx-ray8: Hospital-scale chest x-ray database and benchmarks on weaklysupervised classification and localization of common thorax diseases</title>
		<author>
			<persName><forename type="first">Xiaosong</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yifan</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Le</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhiyong</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mohammadhadi</forename><surname>Bagheri</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ronald</forename><forename type="middle">M</forename><surname>Summers</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="2097" to="2106" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<monogr>
		<title level="m" type="main">Pmc-llama: Further finetuning llama on medical papers</title>
		<author>
			<persName><forename type="first">Chaoyi</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaoman</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ya</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yanfeng</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Weidi</forename><surname>Xie</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2304.14454</idno>
		<imprint>
			<date type="published" when="2023">2023</date>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page">8</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b37">
	<monogr>
		<author>
			<persName><forename type="first">Susan</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stephen</forename><surname>Roller</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Naman</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mikel</forename><surname>Artetxe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Moya</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shuohui</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christopher</forename><surname>Dewan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mona</forename><surname>Diab</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xian</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xi</forename><surname>Victoria Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Todor</forename><surname>Mihaylov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Myle</forename><surname>Ott</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sam</forename><surname>Shleifer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kurt</forename><surname>Shuster</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Daniel</forename><surname>Simig</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Punit</forename><surname>Singh Koura</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Anjali</forename><surname>Sridhar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tianlu</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Luke</forename><surname>Zettlemoyer</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2205.01068</idno>
		<title level="m">Opt: Open pre-trained transformer language models</title>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b38">
	<monogr>
		<title level="m" type="main">Minigpt-4: Enhancing vision-language understanding with advanced large language models</title>
		<author>
			<persName><forename type="first">Deyao</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jun</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaoqian</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiang</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mohamed</forename><surname>Elhoseiny</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2304.10592</idno>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
