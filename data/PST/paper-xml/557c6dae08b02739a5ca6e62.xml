<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">A Neuroevolution Approach to General Atari Game Playing</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date type="published" when="2013-12-08">December 8, 2013</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Matthew</forename><surname>Hausknecht</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science</orgName>
								<orgName type="institution">University of Texas at Austin Austin</orgName>
								<address>
									<postCode>78712</postCode>
									<region>TX</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Joel</forename><surname>Lehman</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science</orgName>
								<orgName type="institution">University of Texas at Austin Austin</orgName>
								<address>
									<postCode>78712</postCode>
									<region>TX</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Risto</forename><surname>Miikkulainen</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science</orgName>
								<orgName type="institution">University of Texas at Austin Austin</orgName>
								<address>
									<postCode>78712</postCode>
									<region>TX</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author role="corresp">
							<persName><forename type="first">Peter</forename><surname>Stone</surname></persName>
							<email>pstone@cs.utexas.edu</email>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science</orgName>
								<orgName type="institution">University of Texas at Austin Austin</orgName>
								<address>
									<postCode>78712</postCode>
									<region>TX</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">A Neuroevolution Approach to General Atari Game Playing</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2013-12-08">December 8, 2013</date>
						</imprint>
					</monogr>
					<idno type="MD5">1A82BB5DCF99020BC2D1C7C7656C45AC</idno>
					<idno type="DOI">10.1109/TCIAIG.2013.2294713</idno>
					<note type="submission">This article has been accepted for publication in a future issue of this journal, but has not been fully edited. Content may change prior to final publication. Citation information: DOI 10.1109/TCIAIG.2013.2294713, IEEE Transactions on Computational Intelligence and AI in Games This article has been accepted for publication in a future issue of this journal, but has not been fully edited. Content may change prior to final publication. Citation information: DOI 10.1109/TCIAIG.2013.2294713, IEEE Transactions on Computational Intelligence and AI in Games This article has been accepted for publication in a future issue of this journal, but has not been fully edited. Content may change prior to final publication. Citation information: DOI 10.1109/TCIAIG.2013.2294713, IEEE Transactions on Computational Intelligence and AI in Games 1943-068X This article has been accepted for publication in a future issue of this journal, but has not been fully edited. Content may change prior to final publication. Citation information: DOI 10.1109/TCIAIG.2013.2294713, IEEE Transactions on Computational Intelligence and AI in Games</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.3" ident="GROBID" when="2023-07-27T08:14+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>This article addresses the challenge of learning to play many different video games with little domainspecific knowledge. Specifically, it introduces a neuro-evolution approach to general Atari 2600 game playing. Four neuro-evolution algorithms were paired with three different state representations and evaluated on a set of 61 Atari games. The neuro-evolution agents represent different points along the spectrum of algorithmic sophistication -including weight evolution on topologically fixed neural networks (Conventional Neuro-evolution), Covariance Matrix Adaptation Evolution Strategy (CMA-ES), evolution of network topology and weights (NEAT), and indirect network encoding (HyperNEAT). State representations include an object representation of the game screen, the raw pixels of the game screen, and seeded noise (a comparative baseline). Results indicate that direct-encoding methods work best on compact state representations while indirect-encoding methods (i.e. HyperNEAT) allow scaling to higher-dimensional representations (i.e. the raw game screen). Previous approaches based on temporaldifference learning had trouble dealing with the large state spaces and sparse reward gradients often found in Atari games. Neuro-evolution ameliorates these problems and evolved policies achieve state-of-the-art results, even surpassing human high scores on three games. These results suggest that neuro-evolution is a promising approach to general video game playing.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>A major challenge for AI is to develop agents that can learn to perform many different tasks. To this end, this article describes a class of agents capable of learning to play a large number of Atari 2600 games with little domain-specific knowledge.</p><p>Atari 2600 games represent a middle ground between classic board games and newer, graphically intensive video games. The Atari 2600 includes many different games spanning a number of genres, including board games such as chess and checkers, action-adventure games like Pitfall, shooting games like Space Invaders and Centipede, rudimentary 3-D games like Battlezone, and arcade classics such as Frogger and Pac-Man. A few example games are shown in Figure <ref type="figure" target="#fig_1">1</ref>. From the perspective of an AI practitioner, Atari games are similar to traditional board games in that an AI agent may benefit from cunning planning and a solid understanding of the game's dynamics. They are also similar to modern video games in that they provide opportunities for an agent to process and learn from the action taking place on the game screen. Despite the variability of game dynamics, all Atari games share a standard interface designed for humans to use. Game state is conveyed to the player through a 2D game screen, and in response, the player controls game elements by manipulating a joystick and pressing a single button. The diversity of games, applicability of fundamental AI techniques, and standardized interface make the Atari 2600 an enticing platform for the development of a general video game playing agent.</p><p>In this article four neuro-evolution algorithms using three different state representations are applied to 61 Atari games. This work builds upon HyperNEAT-GGP, a HyperNEAT-based general Atari game  playing agent <ref type="bibr" target="#b17">[18]</ref> demonstrated to learn on two different Atari games: Freeway and Asterix. HyperNEAT-GGP required game-specific knowledge in order to initialize the associated neural network and to select actions. These restrictions are removed in this article, extending HyperNEAT-GGP to many more Atari games. Additionally, this approach is evaluated in the context of a wider class of neuro-evolutionary learning algorithms with the intent of investigating algorithmic performance as a factor of state representation and algorithmic sophistication. Specifically the following questions are addressed: (1) How does performance change as a factor of algorithm sophistication? (2) How does neuro-evolution performance scale as a function of state representation? (3) How do the evolutionary algorithms presented in this article compare to previous benchmarks including temporal-difference learning algorithms, planning algorithms, and human experts?</p><p>The article is organized as follows: Section 2 outlines the merits of the Atari 2600 as a research platform, and Section 3 reviews related work in this area. Next, Section 4 covers the four different neuro-evolution algorithms, while Section 5 describes the state representations used by these algorithms. Finally, Sections 6-8 describe the specifics of interfacing these algorithms with the Atari domain and running experiments, and Sections 9 and 10 present results and their analysis, in particular answering the questions above.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Atari for Research</head><p>The Atari 2600 video game console was released in October 1977. It was the first console to create game cartridges that decoupled game code from console hardware (previous devices all contained dedicated hardware with games already built in). Selling over 30 million consoles <ref type="bibr" target="#b11">[12]</ref>, Atari was considered wildly successful as an entertainment device. Today, while Atari is no longer at the forefront of entertainment, the console has good research potential for four main reasons.</p><p>First, there are 418 original Atari games with more being continually developed by an active homebrew community. Many games support a second player, opening the possibility of multiagent learning. The large number of games creates an ideal testbed for general game playing agents by allowing a learning agent to be quickly deployed in a variety of domains.</p><p>Second, there exists a high quality Atari emulator, ALE (Arcade Learning Environment 1 ), which is designed specifically to accommodate learning agents. Furthermore, since the Atari 2600 CPU runs at 1.19 megahertz, ALE, on modern processors, is many times faster -running at high speeds of up to 2000 frames per second, expediting the evaluation of agents and algorithms and thus making sophisticated learning simulations practical.</p><p>Third, the Atari state and action interface is simple enough for learning agents, but complex enough to control many different games. The state of an Atari game can be described relatively simply by its 2D graphics (containing between 8 or 256 colors depending on the color mode and a native resolution of 160 × 210), elementary sound effects, and 128 bytes of console RAM. The discrete action space for Atari consists of eight directions of movement for the joystick (up, down, left, right, up-left, up-right, down-left, down-right) as well as a single button. This button can be pressed alone or simultaneously with any of the joystick movements. Including NO-OP (no action), this setup yields a total of 18 possible actions.</p><p>Fourth, the games have complex enough dynamics to be interesting yet at the same time are relatively short and have a limited 2D world with relatively few agents.</p><p>One inconvenient aspect of the Atari 2600 is that it is necessary to acquire ROM files for the different games in order to play them. In practice this is not a major problem because Atari 2600 enthusiast websites such as http://www.atarimania.com/ offer links to ROM packages.</p><p>In summary, the large number of games, availability of high quality emulators, and the standardized interface make the Atari 2600 a great stepping stone from board games and declarative games (like those used in the GGP competition -see Section 3) to more modern video games.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Related Work</head><p>The Atari domain was first used as a research platform <ref type="bibr" target="#b10">[11]</ref>, then modified to interface with learning agents <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b32">33]</ref>, and released as the Atari Learning Environment (ALE). ALE has been subsequently used as a domain for reinforcement learning and search agents <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b2">3]</ref> and to investigate aspects of prediction and self-detection <ref type="bibr" target="#b3">[4]</ref>. The work presented in this article differs from previous Atari research in its application of neuro-evolution algorithms rather than temporal difference (TD) algorithms to this domain.</p><p>Broadly, this work is motivated by the idea of general-purpose intelligence <ref type="bibr" target="#b20">[21,</ref><ref type="bibr" target="#b24">25]</ref>, applied to the domain of video game playing <ref type="bibr" target="#b36">[37]</ref>. Research in this area has focused on learning to play single games using perceptual imagery <ref type="bibr" target="#b51">[52]</ref>, human demonstration <ref type="bibr" target="#b8">[9]</ref>, discovery of objective functions <ref type="bibr" target="#b47">[48]</ref>, and use of raw visual inputs <ref type="bibr" target="#b29">[30,</ref><ref type="bibr" target="#b33">34,</ref><ref type="bibr" target="#b34">35,</ref><ref type="bibr" target="#b48">49]</ref>. The work in this article differs because of its emphasis on general video game playing.</p><p>In a more traditional board game setting, the General Game Playing (GGP) <ref type="bibr" target="#b15">[16]</ref> competition has been run since 2005. In this competition, agents are given a declarative description of an arbitrary game (including a complete description of the game dynamics) and without prior knowledge must formulate strategies to play this game on the fly. Thus, unlike specialized game players, general game playing agents cannot rely on algorithms designed in advance for specific games. Recently there has been a push to bring general game playing into the realm of video games <ref type="bibr" target="#b26">[27]</ref>, creating the area of General Video Game Playing (GVGP) <ref type="bibr" target="#b27">[28,</ref><ref type="bibr" target="#b35">36]</ref>. The GGP competition differs from the scenario considered in this article in four ways: (1) While learning techniques can be used in principle, there is little opportunity to do so in the competition. (2) While the GGP competition provides a description of game dynamics, algorithms studied in this article must learn dynamics exclusively by playing the game. (3) While the GGP competition is based on declarative representations, this article focuses on visual representations (as suggested by other recent work in GGP <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b21">22]</ref>) (4) While GGP features board-like games, this article takes a step towards playing video games.</p><p>Neuro-evolution algorithms <ref type="bibr" target="#b12">[13,</ref><ref type="bibr" target="#b31">32]</ref> have been successful on a variety of tasks including quadruped locomotion <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b49">50]</ref>, evolution of morphology <ref type="bibr" target="#b5">[6]</ref>, design of robot morphology and control <ref type="bibr" target="#b28">[29]</ref>, checkers <ref type="bibr" target="#b13">[14]</ref>, Unreal Tournament <ref type="bibr" target="#b37">[38]</ref>, and game playing <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b46">47]</ref>. Of the algorithms used in this article, HyperNEAT has been previously applied to game of Robocup Keepaway Soccer <ref type="bibr" target="#b44">[45,</ref><ref type="bibr" target="#b50">51]</ref>, checkers <ref type="bibr" target="#b14">[15]</ref>, multi-agent predator prey <ref type="bibr" target="#b9">[10]</ref>, and quadruped locomotion <ref type="bibr" target="#b6">[7]</ref>. NEAT has been applied to evolving agents for the NERO video game <ref type="bibr" target="#b38">[39]</ref> and the game of Go <ref type="bibr" target="#b43">[44]</ref>. CMA-ES has been applied to the task of walk-optimization in a robot soccer simulator <ref type="bibr" target="#b30">[31]</ref>. None of these algorithms has been previously applied to the task of general video game playing.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Algorithms</head><p>The four different neuro-evolution algorithms applied to the Atari domain are simple weight evolution over a topologically static neural network (CNE), Covariance Matrix Adaptation Evolutionary Strategy (CMA-ES), evolution of weights and topology (NEAT <ref type="bibr" target="#b42">[43]</ref>), and indirect encoding of network weights (HyperNEAT <ref type="bibr" target="#b14">[15]</ref>). All algorithms encode Artificial Neural Networks (ANNs) which are represented by weights and connectivity (also called topology). The first two algorithms search over just the weights of the ANN while the latter two have the capacity to also modify the topology. Specifics of ANN topology are discussed in Section 6.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Conventional Neuro-evolution (CNE)</head><p>In the simplest of the algorithms, Conventional Neuro-evolution (CNE), the topology of the ANN is fixed and only the weights of the network are evolved. Crossover and mutation, typical of genetic algorithms, are used to generate offspring. As such, CNE represents a baseline of performance. In addition, speciation (similar to that in NEAT) is used to maintain diversity in the population. Thus the only difference between CNE and NEAT is that NEAT also evolves network topology. CNE is able to fine tune each weight independently; thus given enough time it is capable of finding high performing policies if they exist within the policy space <ref type="bibr" target="#b7">[8]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">CMA-ES</head><p>The second method to be evaluated in the experiments consists of a more sophisticated way to evolve fixed topology networks. Instead of crossover and mutation, the weights of the network are evolved through Covariance Matrix Adaptation Evolution Strategy (CMA-ES) <ref type="bibr" target="#b16">[17]</ref>, a policy search algorithm that successively generates and evaluates sets of candidates sampled from a multivariate Gaussian distribution. Once CMA-ES generates a set of candidate policies, the fitness of each policy is evaluated with respect to the game score it accrues. When all candidates in the group have been evaluated, the mean of the multivariate Gaussian distribution is recalculated as a weighted average of the candidates with the highest fitness. The covariance matrix is also updated to bias the generation of the next set of candidates towards directions of previously successful search steps. The topology of the networks evolved by CMA-ES is identical to those of CNE.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">NEAT</head><p>Neuro-Evolution of Augmenting Topologies (NEAT) is an evolutionary computation method that evolves the topology and weights of an Artificial Neural Network (ANN) <ref type="bibr" target="#b42">[43]</ref>. As such it represents an advance from methods that evolve only the weights of topologically fixed networks. Allowing NEAT to control the topology of the ANN gives it the power to scale the complexity of the network to match that of the problemincreasing the number of nodes for more complex problems and reducing it for less complex ones, in addition to simply adjusting connection weights. Additionally, NEAT attempts to balance the fitness of evolved solutions and their diversity. It is based on applying three key techniques: tracking genes with history markers to allow crossover among topologies, applying speciation to preserve innovations, and developing topologies incrementally from simple initial structures (i.e. complexification) <ref type="bibr" target="#b41">[42]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">HyperNEAT</head><p>HyperNEAT <ref type="bibr" target="#b14">[15,</ref><ref type="bibr" target="#b39">40,</ref><ref type="bibr" target="#b40">41]</ref> evolves an indirect encoding called a Compositional Pattern Producing Network (CPPN). The CPPN is used to define the weights of an ANN that produces a solution for the problem. This encoding differs from direct encodings (e.g. CNE, CMA-ES, and NEAT) that evolve the ANN itself. As shown in Figure <ref type="figure" target="#fig_2">2</ref>, the ANN substrate has an input layer consisting of nodes spanning an (X 1 , Y 1 ) plane. The weights between this input layer and the next layer in the network (X 2 , Y 2 ) are determined by the CPPN. This CPPN is said to be geometrically aware because each network weight it outputs is a function of the (x 1 , y 1 ), (x 2 , y 2 ) coordinates of the nodes in the corresponding layers of the ANN. By ranging the inputs of the CPPN over all pairs of nodes in the first and second layers of the ANN, the weights in the network are fully determined. This geometric awareness allows the CPPN to encode weights that are functions of domain geometry, resulting in an ANN that implicitly contains knowledge about geometric relationships in the domain. In comparison to standard NEAT, HyperNEAT's encoding potentially allows it to take advantage of geometric regularities present in Atari 2600 games. In addition, its indirect encoding allows it to express very large neural networks while evolving only small, compact CPPNs. Gaussians and sinusoids (chosen because they reflect the patterns and regularities observed in nature and therefore also in real-world domains) connected in a weighted topology (Figure <ref type="figure" target="#fig_2">2</ref>).</p><p>2. The CPPN is used to determine the weights for every pair of (input,output) nodes in adjacent layers of the ANN.</p><p>3. With fully specified weights, the ANN is applied to the problem of interest. The performance of the ANN determines the fitness of the CPPN that generates it.</p><p>4. Based on fitness scores, the population of CPPNs is maintained, evaluated, and evolved via NEAT.</p><p>Having discussed the selected neuro-evolution algorithms, the next section describes the three state representations paired with these algorithms.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">State Representations</head><p>This section presents the state representations used by the algorithms in Section 4. Three specific state representations are investigated: object representation, raw-pixel representation, and the noise-screen representation. These state representations convey information to the agent about the state of the game. This information is translated into activations of the input level of an ANN. The architecture of this ANN will be discussed in Section 6, but the translation of state information to input activations is a topic of this section.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Object Representation</head><p>The object representation relies on a set of manually identified object images categorized into object classes. These images were manually created by the authors playing each game, saving images of encountered objects, and categorizing them into classes. Subsequently, these objects may be automatically identified at runtime by checking if any of objects on the current screen matches any of the saved object images. In order to capture animated sprites, multiple images for each object may be saved. Object matching must be exact and objects that are partially occluded are not identified unless an image of the partial occlusion was saved.</p><p>One problem with the manual object detection approach is that for level-based games new objects arise at more advanced game levels. This behavior can be a problem when algorithm performance exceeds human performance, leading to the algorithm encountering objects for which there have been no saved imagesrendering them invisible. This happens in several games including Asterix, Beam Rider, and Phoenix. In such cases the agent will play on, blind but confident.</p><p>Information about the objects on screen is conveyed to the input layer of an ANN via N substrates -one for each class of object (Figure <ref type="figure" target="#fig_4">3a</ref>). At every game screen, the relevant on-screen entities in each object class are identified and transformed into substrate activations in the substrate corresponding to that object class and at the node corresponding to that object's location on the game screen. For example, substrate N may be assigned the object class of cars. Each car in the current game screen is given an activation in substrate N at the corresponding location (subject to downscaled resolution). Thus each activated substrate mirrors the locations of a single class of objects on the screen. Together the substrates represent a decomposition of on-screen entities.</p><p>The object representation is a clean and informative characterization of the visual information on the game screen. As such it represents the highest level and least general features that are used in this article. Correspondingly, algorithms using the object representation are the least general game players as they require relevant game objects to be identified before the start of the game. It may be possible to do this recognition automatically; this possibility is discussed in Section 11.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Raw-Pixel State Representation</head><p>In contrast to the object representation, the raw-pixel representation is a low-level representation of the information on the game screen. Quite simply it provides the downsampled pixel values as input to the agent. In order to limit the dimensionality of the screen, it was necessary to minimize the number of colors present. Fortunately, Atari has support for three different color modes: NTSC (containing 128 unique colors), PAL (104 colors), and SECAM (8 colors). Thus, the SECAM mode was used to minimize the color palette.</p><p>The raw-pixel representation (shown in Figure <ref type="figure" target="#fig_4">3b</ref>) has eight input substrates -one to represent each color. Colors from the screen were transformed into substrate activations by activating a node in a substrate if the corresponding color was present in the screen at that geometric location.</p><p>Individual pixels of the game screen are low-level, uninformative features, yet together they encode a great deal of information (2 2688 unique game screens can be represented). The raw-pixel representation is the most general of the representations used and mirrors what a human would use when playing an Atari game. Therefore this representation provides a true test of general game playing.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3">Noise-Screen Representation</head><p>In order to investigate how much of the learning is based on memorization and how much on learning general concepts, noise-screens were used as the third category of input representation. Noise-screens consist of a single substrate with seeded random activations. While learning may seem hopeless at first glance, Atari games are deterministic, meaning that the start state for each game is the same and there is no stochasticity in the transition and reward functions. <ref type="foot" target="#foot_0">2</ref> Thus it is possible for HyperNEAT-GGP to evolve a policy that takes advantage of a deterministic sequence of random activations. In some sense the noise-screen representation tests the memorization capability of an algorithm -how well can it learn to associate the correct actions with state inputs that are deterministic but entirely uncorrelated with the action taking place on-screen. Additionally, the noise-screen representation represents a sanity check to make sure the neuro-evolution algorithms using the object and pixel representations are doing more than just memorizing sequences of moves. The noise-screen representation uses a single input substrate as shown in Figure <ref type="figure" target="#fig_4">3c</ref>.</p><p>Having described the algorithms and state representations, the next section describes the topology the Artificial Neural Networks and how they are interfaced with the Atari game screen.  In (c) random noise is provided as substrate activation. In all cases, networks are feed-forward and input activations are propagated upwards through the processing level to the output layer, which consists of a 3× 3 directional substrate (D-pad) and a fire button, mirroring the physical controls of the Atari 2600.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Neural Network Architecture</head><p>The ANN consists of three layers (Figure <ref type="figure" target="#fig_4">3</ref>): a substrate layer in which information from the game screen (raw pixels, objects, or noise) is given as input to the network, a processing layer which adds a non-linear internal representation, and a non-linear output layer from which actions are read and conveyed to the Atari emulator. Both the input and output layers are fully connected to the processing layer. The substrate dimensionality of the input and processing layers is 8 × 10 in the case of the object representation and 16 × 21 for the pixel and noise representations. <ref type="foot" target="#foot_1">3</ref> The output layer consists of a 3 × 3 substrate mirroring the nine possible directions of the Atari 2600 joystick and a single node representing the Fire button. A sigmoid non-linearity was used in the processing and output units.</p><p>In the output layer, the 3 × 3 directional substrate contains the four primary directions of movement in the North, South, West, and East positions. Combinations of these directions (North-East, etc) are found at four corners of the substrate, and the center node corresponds to no joystick movement. Actions are read from the output layer by first selecting the node with the highest activation from the directional substrate, then pairing it with the activity of the fire button. If the activation of the fire button exceeds a threshold (0.5) then it is considered active, otherwise inactive. By pairing the joystick direction and the fire button, all eighteen actions can be created in a manner isomorphic to the physical Atari controls.</p><p>Non-HyperNEAT algorithms do not require geometric output layers and instead used an output layer consisting of a single node for each action available in the current game for a dimensionality of 1 × A where A is eighteen or less. <ref type="foot" target="#foot_2">4</ref> The action corresponding to the output node with the highest activation is selected as the agent's action.</p><p>The ANNs execute as follows: At the beginning of each timestep, the current game screen is acquired. Next the on-screen objects, pixels, or noise are transformed into activations in their respective input substrates. These activations are propagated upwards in the ANN through the processing layer and to the output layer. Finally, an action is selected at the output layer and fed into the emulator. This cycle continues until the game has terminated. ANN weights are set at the beginning of each episode and do not change throughout the episode.</p><p>Although all of the neuro-evolution algorithms in Section 4 can evolve recurrent networks, experiments in this article consider only feed-forward architectures because most Atari games are MDPs given the state representations discussed in Section 5.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">Experimental Setup</head><p>As mentioned in the introduction, the experiments in this article evaluate the performance of four different neuro-evolution algorithms paired with the three different state representations on a set of 61 Atari games. The 61 games were selected as a representative sample of different game genres including action-adventure, shooting, 3-D, and arcade classics. Additionally these games represent a super-set of the 55 games used by Bellemare et at. <ref type="bibr" target="#b2">[3]</ref>, allowing direct score comparisons on that subset. Each of the algorithms was paired with each of the different representations, unless the pairing was found to be computationally intractable.</p><p>A separate policy was evolved for each game. This policy consisted of 150 generations with a population size of 100 individuals per generation, a total of 15,000 episodes of play per game. <ref type="foot" target="#foot_3">5</ref> All episodes were terminated after 50,000 frames or about thirteen minutes of game time. Each member of the population earned a fitness score equal to the game score accrued from a single episode of play (e.g. until the gameover or the frame limit is reached). The reported score for each neuro-evolution agent corresponds to the champion fitness at generation 150. Appendix A lists other experimental parameters. Source code necessary to replicate experiments can be found online at https://github.com/mhauskn/HyperNEAT.</p><p>The performance of the neuro-evolution algorithms was compared to other previously published benchmarks <ref type="bibr" target="#b2">[3]</ref> including SARSA(λ), planning, human play, and random play. SARSA(λ) refers to a class of agents using model-free learning with linear function approximation, replacing traces, and -greedy exploration. Bellemare et al. <ref type="bibr" target="#b2">[3]</ref> paired SARSA(λ) with the following state representations: discretized NTSC (128 colored) screen, discretized SECAM (8-colored) screen, on-screen objects, RAM content of the game, and locally-sensitive hashes of the game screen. The SARSA column in Appendix B represents the maximum reported performance of any of the five SARSA(λ) agents on each game.</p><p>Planning is accomplished in the Atari environment by using the emulator as a generative model of future states. This approach reduces the problem of playing an Atari game to a search problem over future states. Bellemare et al. <ref type="bibr" target="#b2">[3]</ref> investigated two planning algorithms: a breadth-first search and an Upper Confidence Bound for Trees (UCT <ref type="bibr" target="#b22">[23]</ref>) based search. Due to the size of the action space, the breadth-first search could only search approximately one third of a second into the future given 100,000 search steps. While such search depth may be sufficient to avoid immediate death, it is unable to perform any sort of long-term planning. UCT uses a more sophisticated sampling strategy, applying the bandit algorithm UCB1 to guide sampling of actions and future states. Monte-Carlo rollouts are performed when this sampling phase reaches a leaf node in the game tree. This sampling strategy allows UCT to follow promising trajectories further into the future than breadth-first search. The planning column in Appendix B represents the maximum reported performance of either of the planning algorithms on each game.</p><p>Human expert high scores were collected from http://www.jvgs.net/2600/top50.htm. These scores represent the best reported human scores for a number of Atari 2600 games. Finally, the random play algorithm takes a random action at each timestep. These two scores represent upper and lower bounds for performance expected from neuro-evolution algorithms. run in a feed-forward manner, the crossover operation became computationally intractable for any of the direct encoding algorithms. In all cases, algorithms performed worse as the representations became more general (and less informative). The object representation for both HyperNEAT and NEAT significantly outperformed the noise representation. CNE shows the same trend but without statistical significance. The pixel representations for HyperNEAT tend to perform better than noise-screens, but the difference was not significant.</p><p>The noise-screen encoding is challenging for all the algorithms. However, the resulting policies still show performance comparable to the SARSA(λ) agents and far better than random play. This level of performance shows the ability of evolutionary-algorithms to discover a policy that represents a simple mapping from noisescreens to game actions. At the same time, the performance gap between the noise-screen representation and the object representation shows that it is possible to discover general concepts for these games, and such concepts are superior to simply memorizing a deterministic sequence of actions. The fact that HyperNEAT with raw-pixel representations tends to perform better than with noise-screens suggests that it is beginning to discover and use such concepts as well.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="9.2">Results With Other Methods</head><p>Figure <ref type="figure" target="#fig_5">5a</ref> compares neuro-evolution against previously published results for the SARSA(λ) and planning agents on the 55 game subset that both shared. In this comparison, HyperNEAT with the pixel representation is included because it is the most general algorithm and NEAT with the object representation is included because it is the highest performing. All comparisons in this plot are statistically significant: planning (with access to future states) performs the best followed by the learning methods NEAT and HyperNEAT, SARSA(λ), all significantly better than random play.</p><p>Similarly, in Figure <ref type="figure" target="#fig_5">5b</ref>, the neuro-evolutionary, planning, and random algorithms were compared against human high scores on the set of 33 games for which human high scores were known. Human experts significantly outperformed all algorithms on average. Interestingly, neuro-evolution was able to beat the best human score in several games, as described in Section 10.  On average, humans perform significantly better, although neuro-evolution was able to exceed human high scores on three games and discover infinite scoring opportunities on three others. Comparison and z-score normalization done on the 33 games for which there were human high scores. Videos of evolved policies can be viewed at http://www.cs.utexas.edu/users/ai-lab/?atari.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="10">Discussion</head><p>The performance of the neuro-evolution algorithms described in this article represents the state-of-the-art for the problem of learning to play Atari games. There is still a long way to go before neuro-evolutionary performance reaches the level of planners (which have access to future game states) or human experts. However, there are striking successes as well as insightful failures in these results as will be reviewed in this section.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="10.1">Successes and Failures of Neuro-evolution Algorithms</head><p>Neuro-evolution algorithms exceeded human expert performance on the games Bowling, Kung Fu Master, and Video Pinball. CNE evolved a bowler capable of scoring 252 points in comparison to 237, the top human score, 99,800 points on Kung Fu Master in comparison to the human best of 65,130, and 407,876 points in Video Pinball, far outperforming the human score of 56,851. Counter-intuitively, CNE, the least sophisticated of all of the neuro-evolution algorithms achieved these high scores even though its average game scores were some of the worst overall. Future analysis of actual games played is necessary to uncover why. A possible explanation is that for these three games, unexpectedly good strategies exist as extensions of simple-minded strategies. Evolved policies are superior to planning algorithms in many games, including Amidar, Assault, Asteroids, Berzerk, Bowling, Carnival, Elevator Action, Freeway, Frostbite, James Bond, Journey Escape, Kangaroo, Krull, Kung Fu Master, Montezuma's Revenge, Private Eye, Riverraid, Star Gunner, Venture, and Video Pinball. In general, evolved policies can be superior to planned ones if they are capable of exploiting a sequence of actions longer than the planning horizon. For example, in Krull, evolved policies exploited a repeatable, multi-screen sequence of play.</p><p>The weak point of evolved policies is that in many cases their play differs greatly from intended game play. For example, in the high scoring Kung Fu Master policy evolved by CNE, rather than exploring the map and defeating enemies, the agent simply sits still and attacks enemies that come to it. Policies of this type are in some sense an expected result from a learning algorithm that exclusively optimizes the final game score. In addition to the Kung Fu Master policy described above, several other such exploitative strategies were learned. For example, on the game Beam Rider, HyperNEAT learned how to quickly move the joystick left and right to stay in-between lanes, which made the agent invulnerable to attacks. The caveat of this strategy is that the player must leave this invincible half-space in order to fire back at enemies, thus briefly exposing itself to their fire as well. It therefore did not perform as well as the best human score, but it nonetheless exploits the game environment in a way unintended by the designers.</p><p>Infinite score loops were found on the games Gopher, Elevator Action, and Krull. The score remained finite only because of the 50,000 frame cap on any episode. The score loop in Gopher, discovered by HyperNEAT, depends on quick reactions and would likely be very hard for a human to duplicate for any extended period of time. Similarly, the loop in Elevator Action requires a repeated sequence of timed jumps and ducks to dodge bullets and defeat enemies. The loop in Krull, discovered by HyperNEAT, seems more likely to be a design flaw as the agent is rewarded an extra life after completing a repeatable sequence of play. Most Atari games take the safer approach and reward extra lives in accordance with (exponentially) increasing score thresholds. The reader is encouraged to view the videos associated with these policies and others at http://www.cs.utexas.edu/users/ai-lab/?atari.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="10.2">Successes and Failures of SARSA(λ) Learners</head><p>SARSA(λ) agents perform extraordinarily well on the racing game Enduro, but otherwise generally perform worse than their neuro-evolutionary counterparts. When applying temporal difference-based learners to the complex environments embodied by many Atari 2600 games there are three main hurdles to overcome: the enormously large state space, the frequently delayed effects of actions and rewards, and the initial phase with no reward gradient -which can be quite long when rewards are sparse. Whereas there are some temporal difference methods that partially address these problems, policy search methods such as neuro-evolution are better able to ameliorate them by evolving policies that are insensitive to the number of states, do not need to begin the game by exploring, and are evaluated only on accumulated reward at the end of the game (avoiding problems with delayed in-game rewards).</p><p>For example Freeway gives a positive reward only when the agent successfully crosses the Freeway. Starting at the bottom of the screen and moving upwards towards the goal, it is unlikely that an agent will select enough UP actions to cross the Freeway when using a random exploration policy. This problem is even further complicated by the agent being pushed downwards if it comes into contact with a car. The neuro-evolution algorithms in this article were able to avoid this problem by quickly evolving simple networks which execute only the UP action. These networks are then further refined in later generations to avoid cars.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="10.3">Successes and Failures of Planning Algorithms</head><p>The planning algorithms outperform humans on games like Video Pinball and Wizard of Wor, both of which benefit from quick reactions that can be selected withing the planner's search horizon. Video Pinball primarily requires reflexes to repel the ball as it approaches the paddles while Wizard of Wor takes place in a pacman-esque maze in which the player shoots at on-coming monsters.</p><p>The games least favorable for the planning algorithms are ones that rely least on quick reflexes and instead require long-term planning beyond the planner's horizon. Examples of these games are actionadventure games such as Montezuma's Revenge and Pitfall in which players control an avatar that must explore multiple rooms on different screens and collect objects like keys to open doorways. In the case of Montezuma's Revenge, none of the algorithms come anywhere close to the human score, however the neuroevolution algorithms perform far better than the planners. In general however, action-adventure games often feature very sparse reward gradients and are difficult for all algorithms.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="10.4">Additional Experiments</head><p>Several variations of neuro-evolution experimental parameters were explored. Specifically, different substrate resolutions, generations of evolution, and substrate configurations were attempted. These investigations revealed that the neuro-evolution methods presented (and HyperNEAT in particular) were robust to changes in substrate design, resolution, and evolution time. This result is supported by previous work <ref type="bibr" target="#b50">[51]</ref> which shows that HyperNEAT can scale to different substrate resolutions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="11">Future Work</head><p>Since most Atari 2600 games are MDPs, the networks evolved in this article were feed-forward, without recurrent connections. Such networks represent reactive controllers i.e. the same input to the network will produce the same output actions, regardless of context or history. Recurrent networks, on the other hand, have an internal state that can act as a type of memory. Such memory is likely to be useful for exploration games that are POMDPs such as Montezuma's Revenge, in which it would be helpful to remember where the player came from in order to decide where to go. Recurrent networks can readily be evolved with all the methods tested in this article and it should be interesting to do so in future work.</p><p>Another interesting direction is deep learning which has shown promise in the fields of image processing and object recognition <ref type="bibr" target="#b19">[20,</ref><ref type="bibr" target="#b23">24,</ref><ref type="bibr" target="#b45">46]</ref>. It would be interesting to explore the space of features learned by the application of deep learning to the Atari game screens. The learned features could be used as an alternative form of state representation, removing the need for manual identification of on-screen entities.</p><p>Some games such as Pitfall are still too complicated and have too sparse reward signals for any of the algorithms to learn to play effectively. In Pitfall, the first possibility of collecting treasure takes place many screens away from the start of the game and only after the player has jumped over crocodiles and swung on vines. One potentially fruitful avenue would be to apply intrinsically motivated exploration algorithms such as novelty search <ref type="bibr" target="#b25">[26]</ref> or targeted exploration <ref type="bibr" target="#b18">[19]</ref> to these games so that the intrinsic reward signal could act as a stand-in until the first real reward is encountered.</p><p>Finally, it is possible to perform automatic object recognition in order to avoid having to identify objects manually for each game. The problem with automatic object recognition is that the substrate topology of the ANN depends on knowing the number of input substrate layers (and hence object classes). Networks whose topology could adapt as new object classes are discovered would possibly allow neuro-evolution algorithms to take advantage of automatic object recognition, and constitute a most interesting direction of future work.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="12">Conclusion</head><p>This article extends prior work on HyperNEAT-GGP <ref type="bibr" target="#b17">[18]</ref> into a fully general Atari 2600 video game player. In addition it contributes a detailed empirical analysis of a range of representations and algorithms in this domain and presents the best learning results reported to date. Significantly, evolved policies beat human high scores on three different Atari games -Bowling, Kung Fu Master, and Video Pinball, and discover opportunities for infinite scores in three others (Gopher, Elevator Action, and Krull).</p><p>Comparing the different neuro-evolution methods reveals that direct encoding algorithms such as NEAT outperform indirect encoding methods such as HyperNEAT on low-dimensional, pre-processed object and noise-screen representations. However, HyperNEAT was the only algorithm capable of learning based on the fully-general raw-pixel representation. Thus, the algorithmic sophistication of HyperNEAT's indirect encoding is justified by its ability to learn large networks, which are necessary for dealing with raw-pixel input as well as possible vision and image processing tasks more generally.</p><p>While no single Atari game, if studied in isolation and given extensive feature engineering, likely poses too great a challenge for modern AI techniques, the full collection of over 400 Atari games still presents a daunting task for a video game playing agent. The results presented in this article suggest that neuro-evolution is a promising approach towards that goal.  <ref type="bibr" target="#b2">[3]</ref>. Evolved agents show champion fitness from a single run. The skiing game in ALE gives a reward of -1 for each frame resulting in negative scores required to encourage an RL algorithms to quickly finish a slope. The human expert's score is listed in seconds required to complete the ski-course.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: General Video Game Playing in Atari 2600: just a few of the hundreds of games available for the Atari 2600. Several factors make the Atari an ideal research platform: a large variety of games with interesting dynamics, the availability of a high quality emulator designed for learning agents, standard interface and controls across the games, and a simple 2D representation.</figDesc><graphic coords="2,92.85,72.76,108.14,145.13" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: The HyperNEAT method of neuroevolution: NEAT evolves the weights and topology of a CPPN (right). This CPPN is subsequently used to determine all of the weights between substrate nodes in the ANN (left). Finally, the ANN is used to compute the solution to the desired problem. CPPNs are said to be geometrically aware because when they compute the weights of the associated ANN, they are given as input the x, y location of both the input and output node in the ANN. Such awareness could prove useful in Atari 2600 games. Also, the CPPN encoding allows HyperNEAT to represent very large networks compactly which makes scaling to large representations possible. (Figure replicated with permission from [51].) Specifically, HyperNEAT works in four stages: 1. The weights and topology of the CPPN are evolved. Internally a CPPN consists of functions such asGaussians and sinusoids (chosen because they reflect the patterns and regularities observed in nature and therefore also in real-world domains) connected in a weighted topology (Figure2).</figDesc><graphic coords="5,165.75,154.47,279.90,162.27" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head></head><label></label><figDesc>(a) Object Architecture (b) Raw-Pixel Architecture (c) Noise-Screen Architecture</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: ANN topology for (a) the object-level state representation, (b) the raw-pixel state representation, and (c) the noise-screen state representation. In (a) the geometric locations of objects from multiple classes (ghosts and pac-man in this case) are provided as input activations to the Substrate layer of the ANN. In (b) the geometric locations of colors present in the screen are transformed into substrate activations.In (c) random noise is provided as substrate activation. In all cases, networks are feed-forward and input activations are propagated upwards through the processing level to the output layer, which consists of a 3× 3 directional substrate (D-pad) and a fire button, mirroring the physical controls of the Atari 2600.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 5 :</head><label>5</label><figDesc>Figure5: (a): Comparison of the Neuro-evolution algorithms against the SARSA(λ) and planning algorithms on the set of 55 games reported by Bellemare et al.<ref type="bibr" target="#b2">[3]</ref>. Of the algorithms, the most general (HyperNEAT-Pixel) and highest performing (NEAT-Object) are included. (b): On average, humans perform significantly better, although neuro-evolution was able to exceed human high scores on three games and discover infinite scoring opportunities on three others. Comparison and z-score normalization done on the 33 games for which there were human high scores. Videos of evolved policies can be viewed at http://www.cs.utexas.edu/users/ai-lab/?atari.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>This article has been accepted for publication in a future issue of this journal, but has not been fully edited. Content may change prior to final publication. Citation information: DOI 10.1109/TCIAIG.2013.2294713, IEEE Transactions on Computational Intelligence and AI in Games</figDesc><table><row><cell></cell><cell>Algorithm</cell><cell>CMA-ES</cell><cell>CNE</cell><cell>HyperNEAT</cell><cell>NEAT</cell><cell>Algorithm</cell><cell>Object Noise Pixel</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>HyperNEAT</cell><cell>.34</cell><cell>-.78</cell><cell>-.46</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>NEAT</cell><cell>.67</cell><cell>.09</cell><cell>-</cell></row><row><cell></cell><cell>2</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>CNE</cell><cell>.08</cell><cell>-.12</cell><cell>-</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>CMA-ES</cell><cell>.21</cell><cell>-</cell><cell>-</cell></row><row><cell></cell><cell>1</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">Figure 4: Performance of Neuro-evolution algorithms</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">with different representations: Above: Mean Z-Scores</cell></row><row><cell>Z-Score</cell><cell>0</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">for Neuro-evolution algorithms. Algorithms missing from this table were too computationally expensive to run at the desired substrate resolution. Left: the box-</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">plot of these scores as a factor of state representation.</cell></row><row><cell></cell><cell>-1</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">NEAT had the highest mean z-scores across the ob-</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">ject and noise representations, however HyperNEAT</cell></row><row><cell></cell><cell>-2</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">was the only algorithm capable of scaling to the pixel representation. Comparisons were performed on the</cell></row><row><cell></cell><cell>Object</cell><cell></cell><cell>Noise</cell><cell>Pixel</cell><cell></cell><cell cols="2">full set of 61 games followed by z-score normalization.</cell></row><row><cell></cell><cell></cell><cell cols="3">Representation</cell><cell></cell><cell></cell></row></table><note><p>1943-068X (c) 2013 IEEE. Personal use is permitted, but republication/redistribution requires IEEE permission. See http://www.ieee.org/publications_standards/publications/rights/index.html for more information.</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head></head><label></label><figDesc>This article has been accepted for publication in a future issue of this journal, but has not been fully edited. Content may change prior to final publication. Citation information: DOI 10.1109/TCIAIG.2013.2294713, IEEE Transactions on Computational Intelligence and AI in Games</figDesc><table /><note><p>1943-068X (c) 2013 IEEE. Personal use is permitted, but republication/redistribution requires IEEE permission. See http://www.ieee.org/publications_standards/publications/rights/index.html for more information.</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 1 :</head><label>1</label><figDesc>Scores for different learning algorithms and comparison methods. All episodes are capped at 50,000 frames. Planning, SARSA, and Random agents are averaged over 30 trials. Planning and SARSA scores obtained from</figDesc><table><row><cell>Game</cell><cell>H-NEAT</cell><cell>H-NEAT</cell><cell>H-NEAT</cell><cell>NEAT</cell><cell>NEAT</cell><cell>CNE</cell><cell>CNE</cell><cell>CMA-ES</cell><cell>Random</cell><cell>SARSA</cell><cell>Planning</cell><cell>Human</cell></row><row><cell></cell><cell>Object</cell><cell>Pixel</cell><cell>Noise</cell><cell>Object</cell><cell>Noise</cell><cell>Object</cell><cell>Noise</cell><cell>Object</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>air raid</cell><cell>4335</cell><cell>7415</cell><cell>2380</cell><cell>4945</cell><cell>3450</cell><cell>4180</cell><cell>3075</cell><cell>4615</cell><cell>548.3</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell>alien</cell><cell>2246</cell><cell>1586</cell><cell>764</cell><cell>4320</cell><cell>1842</cell><cell>1988</cell><cell>1960</cell><cell>2730</cell><cell>116.7</cell><cell>939.2</cell><cell>7785</cell><cell>-</cell></row><row><cell>amidar</cell><cell>218.8</cell><cell>184.4</cell><cell>124</cell><cell>325.2</cell><cell>187.2</cell><cell>161.8</cell><cell>195</cell><cell>467.4</cell><cell>0.9</cell><cell>103.4</cell><cell>180.3</cell><cell>5342</cell></row><row><cell>assault</cell><cell>2396</cell><cell>912.6</cell><cell>764.4</cell><cell>2717.2</cell><cell>799</cell><cell>1276.8</cell><cell>693</cell><cell>2219.4</cell><cell>366.1</cell><cell>628</cell><cell>1512.2</cell><cell>-</cell></row><row><cell>asterix</cell><cell>2550</cell><cell>2340</cell><cell>1410</cell><cell>1490</cell><cell>1240</cell><cell>1310</cell><cell>1300</cell><cell>2970</cell><cell>315</cell><cell>987.3</cell><cell>290700</cell><cell>-</cell></row><row><cell>asteroids</cell><cell>220</cell><cell>1694</cell><cell>5316</cell><cell>4144</cell><cell>7204</cell><cell>4800</cell><cell>6940</cell><cell>3734</cell><cell>1394.3</cell><cell>907.3</cell><cell>4660.6</cell><cell>663000</cell></row><row><cell>atlantis</cell><cell>44200</cell><cell>61260</cell><cell>78640</cell><cell>126260</cell><cell>103120</cell><cell>96080</cell><cell>86000</cell><cell>81980</cell><cell>32063.3</cell><cell>62687</cell><cell>193858</cell><cell>372400</cell></row><row><cell>bank heist</cell><cell>1308</cell><cell>214</cell><cell>68</cell><cell>380</cell><cell>106</cell><cell>372</cell><cell>110</cell><cell>354</cell><cell>17.3</cell><cell>190.8</cell><cell>497.8</cell><cell>-</cell></row><row><cell>battle zone</cell><cell>37600</cell><cell>36200</cell><cell>48600</cell><cell>45000</cell><cell>41800</cell><cell>30600</cell><cell>20000</cell><cell>51600</cell><cell>2000</cell><cell>15819.7</cell><cell>70333.3</cell><cell>238000</cell></row><row><cell>beam rider</cell><cell>1443.2</cell><cell>1412.8</cell><cell>1360.8</cell><cell>1900</cell><cell>1437.6</cell><cell>1494.4</cell><cell>1380</cell><cell>1736.8</cell><cell>414.7</cell><cell>929.4</cell><cell>6624.6</cell><cell>23020</cell></row><row><cell>berzerk</cell><cell>1358</cell><cell>1394</cell><cell>1296</cell><cell>1202</cell><cell>1536</cell><cell>988</cell><cell>1560</cell><cell>1044</cell><cell>171.7</cell><cell>501.3</cell><cell>670</cell><cell>104450</cell></row><row><cell>bowling</cell><cell>250.4</cell><cell>135.8</cell><cell>101.4</cell><cell>231.6</cell><cell>170.6</cell><cell>223</cell><cell>147</cell><cell>168.6</cell><cell>23.8</cell><cell>43.9</cell><cell>43.9</cell><cell>237</cell></row><row><cell>boxing</cell><cell>91.6</cell><cell>16.4</cell><cell>33.8</cell><cell>92.8</cell><cell>60.8</cell><cell>45.2</cell><cell>57</cell><cell>71.8</cell><cell>-2.2</cell><cell>44</cell><cell>100</cell><cell>-</cell></row><row><cell>breakout</cell><cell>40.8</cell><cell>2.8</cell><cell>2.4</cell><cell>43.6</cell><cell>5</cell><cell>17.2</cell><cell>4</cell><cell>23.2</cell><cell>0.8</cell><cell>5.2</cell><cell>364.4</cell><cell>825</cell></row><row><cell>carnival</cell><cell>3532</cell><cell>2544</cell><cell>3048</cell><cell>4920</cell><cell>4338</cell><cell>5514</cell><cell>3880</cell><cell>3064</cell><cell>915.3</cell><cell>2323.9</cell><cell>5132</cell><cell>824580</cell></row><row><cell>centipede</cell><cell>33326.6</cell><cell>25275.2</cell><cell>20309.8</cell><cell>22469.6</cell><cell>24831.6</cell><cell>22866</cell><cell>21849</cell><cell>27293</cell><cell>2711.3</cell><cell>8803.8</cell><cell>125123</cell><cell>163278</cell></row><row><cell>chopper</cell><cell>8120</cell><cell>3960</cell><cell>2680</cell><cell>4580</cell><cell>3360</cell><cell>5960</cell><cell>3500</cell><cell>13360</cell><cell>836.7</cell><cell>1581.5</cell><cell>34018.8</cell><cell>201600</cell></row><row><cell>command</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>crazy climber</cell><cell>12840</cell><cell>0</cell><cell>11680</cell><cell>25060</cell><cell>23560</cell><cell>25320</cell><cell>21400</cell><cell>19980</cell><cell>2506.7</cell><cell>23410.6</cell><cell>98172.2</cell><cell>-</cell></row><row><cell>defender</cell><cell>15080</cell><cell>14620</cell><cell>22830</cell><cell>16250</cell><cell>36300</cell><cell>16410</cell><cell>35250</cell><cell>12230</cell><cell>4370</cell><cell>-</cell><cell>-</cell><cell>550000</cell></row><row><cell>demon attack</cell><cell>3082</cell><cell>3590</cell><cell>2967</cell><cell>3464</cell><cell>2707</cell><cell>3377</cell><cell>3065</cell><cell>2834</cell><cell>354.7</cell><cell>520.5</cell><cell>28158.8</cell><cell>92420</cell></row><row><cell>double dunk</cell><cell>4</cell><cell>2</cell><cell>0</cell><cell>10.8</cell><cell>10.8</cell><cell>12.4</cell><cell>10</cell><cell>2.4</cell><cell>-16</cell><cell>-13.1</cell><cell>24</cell><cell>-</cell></row><row><cell>elevator action</cell><cell>21600</cell><cell>0</cell><cell>800</cell><cell>32820</cell><cell>31120</cell><cell>32460</cell><cell>30300</cell><cell>23320</cell><cell>4026.7</cell><cell>3220.6</cell><cell>18100</cell><cell>-</cell></row><row><cell>enduro</cell><cell>112.8</cell><cell>93.6</cell><cell>90.6</cell><cell>133.8</cell><cell>99.4</cell><cell>27.4</cell><cell>35</cell><cell>95.4</cell><cell>0</cell><cell>129.1</cell><cell>286.3</cell><cell>3159.2</cell></row><row><cell>fishing derby</cell><cell>-37</cell><cell>-49.8</cell><cell>-72.6</cell><cell>-43.8</cell><cell>-47.4</cell><cell>-46.6</cell><cell>-51</cell><cell>-45.4</cell><cell>-93.5</cell><cell>-89.5</cell><cell>37.8</cell><cell>-</cell></row><row><cell>freeway</cell><cell>29.6</cell><cell>29</cell><cell>28</cell><cell>30.8</cell><cell>30</cell><cell>26.4</cell><cell>29</cell><cell>32</cell><cell>0</cell><cell>19.1</cell><cell>22.5</cell><cell>34</cell></row><row><cell>frostbite</cell><cell>2226</cell><cell>2260</cell><cell>270</cell><cell>1452</cell><cell>338</cell><cell>362</cell><cell>320</cell><cell>3786</cell><cell>66.7</cell><cell>216.9</cell><cell>270.5</cell><cell>416560</cell></row><row><cell>gopher</cell><cell>6252</cell><cell>364</cell><cell>1260</cell><cell>6092</cell><cell>2600</cell><cell>2588</cell><cell>2440</cell><cell>10428</cell><cell>213.3</cell><cell>1288.3</cell><cell>20560</cell><cell>-</cell></row><row><cell>gravitar</cell><cell>1990</cell><cell>370</cell><cell>710</cell><cell>2840</cell><cell>2530</cell><cell>2390</cell><cell>2300</cell><cell>2010</cell><cell>170</cell><cell>387.7</cell><cell>2850</cell><cell>8000</cell></row><row><cell>hero</cell><cell>3638</cell><cell>5090</cell><cell>1858</cell><cell>3894</cell><cell>5729</cell><cell>5067</cell><cell>5775</cell><cell>3546</cell><cell>834.5</cell><cell>6458.8</cell><cell>12859.5</cell><cell>159900</cell></row><row><cell>ice hockey</cell><cell>9</cell><cell>10.6</cell><cell>7.4</cell><cell>3.8</cell><cell>3</cell><cell>2.2</cell><cell>2</cell><cell>3.8</cell><cell>-9.9</cell><cell>-9.5</cell><cell>39.4</cell><cell>-</cell></row><row><cell>jamesbond</cell><cell>12730</cell><cell>5660</cell><cell>2060</cell><cell>2380</cell><cell>6900</cell><cell>410</cell><cell>250</cell><cell>1770</cell><cell>18.3</cell><cell>202.8</cell><cell>330</cell><cell>-</cell></row><row><cell>journey escape</cell><cell>20760</cell><cell>17100</cell><cell>11460</cell><cell>21840</cell><cell>18600</cell><cell>19100</cell><cell>18300</cell><cell>14660</cell><cell>-20026.7</cell><cell>-8713.5</cell><cell>7683.3</cell><cell>-</cell></row><row><cell>kangaroo</cell><cell>4880</cell><cell>800</cell><cell>720</cell><cell>12800</cell><cell>1280</cell><cell>1160</cell><cell>1400</cell><cell>8840</cell><cell>53.3</cell><cell>1622.1</cell><cell>1990</cell><cell>29700</cell></row><row><cell>krull</cell><cell>23890.2</cell><cell>12601.4</cell><cell>10005.4</cell><cell>20337.8</cell><cell>6864.2</cell><cell>6101.8</cell><cell>5688</cell><cell>25908.8</cell><cell>1780.3</cell><cell>3371.5</cell><cell>5037</cell><cell>-</cell></row><row><cell>kung fu master</cell><cell>47820</cell><cell>7720</cell><cell>20580</cell><cell>87340</cell><cell>57820</cell><cell>83080</cell><cell>54900</cell><cell>78960</cell><cell>553.3</cell><cell>19544</cell><cell>48854.5</cell><cell>65130</cell></row><row><cell>montezuma</cell><cell>0</cell><cell>0</cell><cell>0</cell><cell>340</cell><cell>60</cell><cell>340</cell><cell>100</cell><cell>340</cell><cell>3.3</cell><cell>10.7</cell><cell>10.7</cell><cell>70500</cell></row><row><cell>revenge</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>ms pacman</cell><cell>3830</cell><cell>3408</cell><cell>2378</cell><cell>4902</cell><cell>3916</cell><cell>4112</cell><cell>3980</cell><cell>3728</cell><cell>167.7</cell><cell>1691.8</cell><cell>22336</cell><cell>211480</cell></row><row><cell>name this game</cell><cell>8346</cell><cell>6742</cell><cell>4570</cell><cell>7084</cell><cell>5452</cell><cell>6396</cell><cell>5210</cell><cell>6820</cell><cell>1806</cell><cell>2500.1</cell><cell>15410</cell><cell>-</cell></row><row><cell>phoenix</cell><cell>5860</cell><cell>1762</cell><cell>4898</cell><cell>7832</cell><cell>6290</cell><cell>9178</cell><cell>7960</cell><cell>5472</cell><cell>1173.7</cell><cell>-</cell><cell>-</cell><cell>365240</cell></row><row><cell>pitfall</cell><cell>0</cell><cell>0</cell><cell>0</cell><cell>0</cell><cell>0</cell><cell>0</cell><cell>0</cell><cell>0</cell><cell>-383.3</cell><cell>-</cell><cell>-</cell><cell>109858</cell></row><row><cell>pong</cell><cell>4</cell><cell>-17.4</cell><cell>-19.6</cell><cell>15.2</cell><cell>-16</cell><cell>-10</cell><cell>-16</cell><cell>-7.4</cell><cell>-20.7</cell><cell>-19</cell><cell>21</cell><cell>-</cell></row><row><cell>pooyan</cell><cell>2900</cell><cell>1222</cell><cell>2566</cell><cell>2761</cell><cell>2782</cell><cell>2883</cell><cell>2670</cell><cell>2210</cell><cell>505.2</cell><cell>1225.3</cell><cell>17763.4</cell><cell>-</cell></row><row><cell>private eye</cell><cell>15045.2</cell><cell>10747.4</cell><cell>10692.6</cell><cell>1926.4</cell><cell>14412.8</cell><cell>9750.4</cell><cell>14385</cell><cell>880</cell><cell>-690.3</cell><cell>684.3</cell><cell>1947.3</cell><cell>-</cell></row><row><cell>qbert</cell><cell>810</cell><cell>695</cell><cell>795</cell><cell>1935</cell><cell>1275</cell><cell>2165</cell><cell>1200</cell><cell>1675</cell><cell>222.5</cell><cell>613.5</cell><cell>17343.4</cell><cell>119300</cell></row><row><cell>riverraid</cell><cell>4736</cell><cell>2616</cell><cell>2136</cell><cell>4718</cell><cell>3116</cell><cell>4120</cell><cell>3000</cell><cell>3760</cell><cell>1524.7</cell><cell>1904.3</cell><cell>4449</cell><cell>135830</cell></row><row><cell>road runner</cell><cell>14420</cell><cell>3220</cell><cell>2900</cell><cell>9600</cell><cell>7440</cell><cell>5140</cell><cell>7000</cell><cell>11320</cell><cell>43.3</cell><cell>67.7</cell><cell>38725</cell><cell>-</cell></row><row><cell>robotank</cell><cell>42.4</cell><cell>43.8</cell><cell>37.8</cell><cell>18</cell><cell>35.6</cell><cell>15</cell><cell>33</cell><cell>14.4</cell><cell>1.6</cell><cell>28.7</cell><cell>50.4</cell><cell>-</cell></row><row><cell>seaquest</cell><cell>2508</cell><cell>716</cell><cell>760</cell><cell>944</cell><cell>860</cell><cell>800</cell><cell>840</cell><cell>880</cell><cell>114</cell><cell>664.8</cell><cell>5132.4</cell><cell>205210</cell></row><row><cell>skiing</cell><cell>-9127.2</cell><cell>-7983.6</cell><cell>-8836</cell><cell>-6984</cell><cell>-9071.2</cell><cell>-8452.4</cell><cell>-9612</cell><cell>-7361.2</cell><cell>-18196.9</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell>solaris</cell><cell>0</cell><cell>160</cell><cell>11020</cell><cell>14764</cell><cell>16760</cell><cell>14768</cell><cell>16120</cell><cell>11740</cell><cell>2048</cell><cell>-</cell><cell>-</cell><cell>49640</cell></row><row><cell>space invaders</cell><cell>1481</cell><cell>1251</cell><cell>809</cell><cell>1481</cell><cell>1138</cell><cell>1246</cell><cell>945</cell><cell>1428</cell><cell>157.2</cell><cell>250.1</cell><cell>2718</cell><cell>42905</cell></row><row><cell>star gunner</cell><cell>4160</cell><cell>2720</cell><cell>1260</cell><cell>9580</cell><cell>6740</cell><cell>11300</cell><cell>5900</cell><cell>9260</cell><cell>683.3</cell><cell>1069.5</cell><cell>1345</cell><cell>-</cell></row><row><cell>tennis</cell><cell>0.2</cell><cell>0</cell><cell>0</cell><cell>1.2</cell><cell>0.6</cell><cell>1.4</cell><cell>0</cell><cell>-1</cell><cell>-24</cell><cell>-0.1</cell><cell>2.8</cell><cell>-</cell></row><row><cell>time pilot</cell><cell>15640</cell><cell>7340</cell><cell>10020</cell><cell>14320</cell><cell>14600</cell><cell>13060</cell><cell>12800</cell><cell>10180</cell><cell>3660</cell><cell>3741.2</cell><cell>63854.5</cell><cell>-</cell></row><row><cell>tutankham</cell><cell>110</cell><cell>23.6</cell><cell>20.4</cell><cell>142.4</cell><cell>126</cell><cell>163.4</cell><cell>139</cell><cell>126</cell><cell>24.3</cell><cell>114.3</cell><cell>225.5</cell><cell>-</cell></row><row><cell>up n down</cell><cell>6818</cell><cell>43734</cell><cell>11632</cell><cell>10220</cell><cell>9050</cell><cell>6466</cell><cell>7790</cell><cell>8404</cell><cell>140.3</cell><cell>3532.7</cell><cell>74473.6</cell><cell>-</cell></row><row><cell>venture</cell><cell>400</cell><cell>0</cell><cell>0</cell><cell>340</cell><cell>200</cell><cell>160</cell><cell>200</cell><cell>0</cell><cell>0</cell><cell>66</cell><cell>66</cell><cell>-</cell></row><row><cell>video pinball</cell><cell>82646</cell><cell>0</cell><cell>72326.4</cell><cell>253986</cell><cell>426194.4</cell><cell>335109</cell><cell>419666</cell><cell>234699.8</cell><cell>14130</cell><cell>15046.8</cell><cell>254748</cell><cell>56851</cell></row><row><cell>wizard of wor</cell><cell>3760</cell><cell>3360</cell><cell>6660</cell><cell>17700</cell><cell>14260</cell><cell>10720</cell><cell>15900</cell><cell>7600</cell><cell>1050</cell><cell>1981.3</cell><cell>105500</cell><cell>47100</cell></row><row><cell>yars revenge</cell><cell>24405.4</cell><cell>24096.4</cell><cell>29921</cell><cell>32836.4</cell><cell>48288.6</cell><cell>21472.2</cell><cell>44085</cell><cell>27328</cell><cell>3252.2</cell><cell>-</cell><cell>-</cell><cell>965347</cell></row><row><cell>zaxxon</cell><cell>4680</cell><cell>3000</cell><cell>600</cell><cell>6460</cell><cell>3600</cell><cell>6260</cell><cell>3000</cell><cell>6420</cell><cell>0</cell><cell>3365.1</cell><cell>22610</cell><cell>44100</cell></row><row><cell>Times Best</cell><cell>14</cell><cell>5</cell><cell>0</cell><cell>17</cell><cell>4</cell><cell>10</cell><cell>3</cell><cell>8</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_0"><p>The latest version of ALE adds an option to utilize a random starting state. Experimental work in this article pre-dates that release.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3" xml:id="foot_1"><p>These resolutions downsample the 160 × 210 native screen resolution by a factor and 20 and 10, respectively.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4" xml:id="foot_2"><p>Games vary in the number of actions that are used to play. In some games certain actions are deemed illegal because they will restart the game or otherwise change the desired MDP structure.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="5" xml:id="foot_3"><p>For comparison, Bellemare et al.<ref type="bibr" target="#b2">[3]</ref> reports that SARSA(λ) agents were trained for 5,000 episodes. Presumably, the performance of these methods had flatlined at each of these points, making it possible to compare final performance.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" xml:id="foot_4"><p>1943-068X (c) 2013 IEEE. Personal use is permitted, but republication/redistribution requires IEEE permission. See http://www.ieee.org/publications_standards/publications/rights/index.html for more information.</p></note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgments</head><p>Thanks to Jeff Clune and Ken Stanley for useful discussions. Source code for the Arcade Learning Environment (ALE) emulator environment can be found at http://www.arcadelearningenvironment.org/. Source code for the algorithms and experiments presented in this article can be found at https://github. com/mhauskn/HyperNEAT.</p><p>This work has taken place in the Learning Agents Research Group (LARG) at the Artificial Intelligence Laboratory, The University of Texas at Austin. LARG research is supported in part by grants from the National Science Foundation (IIS-0917122, IIS-0915038, and DBI-0939454), ONR (N00014-09-1-0658), and the Federal Highway Administration (DTFH61-07-H-00030).</p></div>
			</div>

			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8">Score Normalization</head><p>Scoring metrics vary from game to game. In order to compare the performance of different algorithms across multiple games it is necessary to normalize the scores of each algorithm on each game. Previously, Bellemare et al. <ref type="bibr" target="#b2">[3]</ref> proposed a normalization scheme in which an algorithm's score on a game was normalized into a [0, 1] range given how well it performed in comparison to a baseline set of all algorithm scores s on the same game:</p><p>Instead this article uses a z-score normalization scheme in which an algorithm's score is normalized against a population of scores where µ and σ are the mean and standard deviation of the set of all algorithm scores on that game:</p><p>The z-score tells how many standard deviations a given score is above or below the mean score. The advantage of using z-score normalization is that it gives a better sense of how well or poorly each algorithm is performing with respect to the mean and standard deviation of the population. This approach makes it a more informative statistic than the normalized score. For example the best scoring algorithm may have a z-score of 3 (meaning that it was three standard deviations above the mean) while the normalized score would simply be 1. 6   </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="9">Results</head><p>This section examines experimental results in order to answer the questions posed in the introduction: (1) How does performance change as a factor of algorithm sophistication? (2) How does algorithm performance scale as a function of state representation? (3) How do the neuro-evolution algorithms compare to temporal difference learning algorithms, planning algorithms, and human experts based on result in the literature? In each case, the performance of the algorithms was compared using a one-way Analysis of Variance (ANOVA) statistical test followed by a Tukey's HSD significance test. The full set of games scores can be found in Appendix B.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="9.1">Neuro-evolution Results</head><p>Figure <ref type="figure">4</ref> summarizes the results: With the object representations, NEAT statistically significantly outperformed CNE and CMA-ES and was statistically similar to HyperNEAT. 7 With the noise representations, NEAT significantly outperformed CNE and HyperNEAT.</p><p>The first main result is that NEAT had the highest mean z-score across the object and noise representations. Surprisingly, HyperNEAT did not significantly outperform the other algorithms despite the additional complexity inherent in its indirect encodings and its ability to incorporate geometric information. This result implies that many Atari games can be well-learned using a compact and well-tuned network such as the ones NEAT is capable of creating as long as the representations are low-dimensional.</p><p>The second main results is that HyperNEAT was the only neuro-evolution algorithm capable of learning to play based on the visual, raw-pixel representation. It was capable of doing so because of its indirect encoding: each individual in the population was encoded by a small CPPN rather than a massive ANN. (ANNs using the raw-pixel state representation had over 900000 weights.) While these ANNs could still be 6 One possible disadvantage of using z-scores is that they are not bounded and are therefore more difficult to compare and combine in future experiments. Also, since z-scores are population based, it is possible to inflate/deflate z-score by adding duplicate low/high scoring agents. This would not affect normalized scores. 7 References to statistical significance imply p-values of less than 0.05 unless otherwise noted. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Appendix A Experimental Parameters</head></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Learning physically-instantiated game play through visual observation</title>
		<author>
			<persName><forename type="first">A</forename><surname>Barbu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Narayanaswamy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">M</forename><surname>Siskind</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICRA</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2010">2010</date>
			<biblScope unit="page" from="1879" to="1886" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Sketch-Based Linear Value Function Approximation</title>
		<author>
			<persName><forename type="first">M</forename><surname>Bellemare</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Veness</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Bowling</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<editor>
			<persName><forename type="first">P</forename><surname>Bartlett</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">F</forename><surname>Pereira</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">C</forename><surname>Burges</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">L</forename><surname>Bottou</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">K</forename><surname>Weinberger</surname></persName>
		</editor>
		<imprint>
			<date type="published" when="2012">2012</date>
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="page" from="2222" to="2230" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">The Arcade Learning Environment: An Evaluation Platform for General Agents</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">G</forename><surname>Bellemare</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Naddaf</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Veness</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Bowling</surname></persName>
		</author>
		<idno>CoRR, abs/1207.4708</idno>
		<imprint>
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Investigating Contingency Awareness Using Atari 2600 Games</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">G</forename><surname>Bellemare</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Veness</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Bowling</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
		<editor>
			<persName><forename type="first">J</forename><surname>Hoffmann</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">B</forename><surname>Selman</surname></persName>
		</editor>
		<imprint>
			<publisher>AAAI Press</publisher>
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Evolving a Heuristic Function for the Game of Tetris</title>
		<author>
			<persName><forename type="first">N</forename><surname>Böhm</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Kókai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Mandl</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">LWA</title>
		<editor>
			<persName><forename type="first">A</forename><surname>Abecker</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">S</forename><surname>Bickel</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">U</forename><surname>Brefeld</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">I</forename><surname>Drost</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">N</forename><surname>Henze</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">O</forename><surname>Herden</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">M</forename><surname>Minor</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">T</forename><surname>Scheffer</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">L</forename><surname>Stojanovic</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">S</forename><surname>Weibelzahl</surname></persName>
		</editor>
		<imprint>
			<date type="published" when="2004">2004</date>
			<biblScope unit="page" from="118" to="122" />
		</imprint>
		<respStmt>
			<orgName>Humbold-Universität Berlin</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Morphological change in machines accelerates the evolution of robust behavior</title>
		<author>
			<persName><forename type="first">J</forename><surname>Bongard</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Proceedings of the National Academy of Sciences</title>
		<imprint>
			<biblScope unit="volume">108</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="1234" to="1239" />
			<date type="published" when="2011-01">Jan. 2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Evolving coordinated quadruped gaits with the HyperNEAT generative encoding</title>
		<author>
			<persName><forename type="first">J</forename><surname>Clune</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">E</forename><surname>Beckmann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Ofria</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">T</forename><surname>Pennock</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Eleventh conference on Congress on Evolutionary Computation, CEC&apos;09</title>
		<meeting>the Eleventh conference on Congress on Evolutionary Computation, CEC&apos;09<address><addrLine>Piscataway, NJ, USA</addrLine></address></meeting>
		<imprint>
			<publisher>IEEE Press</publisher>
			<date type="published" when="2009">2009</date>
			<biblScope unit="page" from="2764" to="2771" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">On the Performance of Indirect Encoding Across the Continuum of Regularity</title>
		<author>
			<persName><forename type="first">J</forename><surname>Clune</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">O</forename><surname>Stanley</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">T</forename><surname>Pennock</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Ofria</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Trans. Evol. Comp</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="346" to="367" />
			<date type="published" when="2011-06">June 2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Automatic State Abstraction from Demonstration</title>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">C</forename><surname>Cobo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Zang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">L I</forename><genName>Jr</genName></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">L</forename><surname>Thomaz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IJCAI/AAAI</title>
		<editor>
			<persName><forename type="first">T</forename><surname>Walsh</surname></persName>
		</editor>
		<imprint>
			<date type="published" when="2011">2011</date>
			<biblScope unit="page" from="1243" to="1248" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Generative encoding for multiagent learning</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">B</forename></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">O</forename><surname>Stanley</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">GECCO &apos;08: Proceedings of the 10th annual conference on Genetic and evolutionary computation</title>
		<meeting><address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2008">2008</date>
			<biblScope unit="page" from="819" to="826" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">An object-oriented representation for efficient reinforcement learning</title>
		<author>
			<persName><forename type="first">C</forename><surname>Diuk</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Cohen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">L</forename><surname>Littman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of 25th International Conference on Machine Learning (ICML)</title>
		<meeting>25th International Conference on Machine Learning (ICML)</meeting>
		<imprint>
			<date type="published" when="2008">2008</date>
			<biblScope unit="page" from="240" to="247" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<author>
			<persName><forename type="first">G</forename><surname>Edgers</surname></persName>
		</author>
		<ptr target="http://www.boston.com/bostonglobe/ideas/articles/2009/03/08/a_talk_with_nick_montfort/" />
		<title level="m">Atari and the deep history of video games</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Neuroevolution: From architectures to learning</title>
		<author>
			<persName><forename type="first">D</forename><surname>Floreano</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Dürr</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Mattiussi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Evolutionary Intelligence</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="47" to="62" />
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">Blondie24: Playing at the Edge of AI</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">B</forename><surname>Fogel</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2001-09">Sept. 2001</date>
			<publisher>Morgan Kaufmann Publishers</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">A case study on the critical role of geometric regularity in machine learning</title>
		<author>
			<persName><forename type="first">J</forename><surname>Gauci</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">O</forename><surname>Stanley</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 23rd National Conference on Artificial Intelligence (AAAI)</title>
		<meeting>the 23rd National Conference on Artificial Intelligence (AAAI)</meeting>
		<imprint>
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">General game playing: Overview of the AAAI competition</title>
		<author>
			<persName><forename type="first">M</forename><surname>Genesereth</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Love</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">AI Magazine</title>
		<imprint>
			<biblScope unit="volume">26</biblScope>
			<biblScope unit="page" from="62" to="72" />
			<date type="published" when="2005">2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">The cma evolution strategy: A comparing review</title>
		<author>
			<persName><forename type="first">N</forename><surname>Hansen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Towards a New Evolutionary Computation</title>
		<editor>
			<persName><forename type="first">J</forename><surname>Lozano</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">P</forename><surname>Larraaga</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">I</forename><surname>Inza</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">E</forename><surname>Bengoetxea</surname></persName>
		</editor>
		<meeting><address><addrLine>Berlin Heidelberg</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2006">2006</date>
			<biblScope unit="volume">192</biblScope>
			<biblScope unit="page" from="75" to="102" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">HyperNEAT-GGP: A HyperNEATbased Atari General Game Player</title>
		<author>
			<persName><forename type="first">M</forename><surname>Hausknecht</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Khandelwal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Miikkulainen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Stone</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Genetic and Evolutionary Computation Conference</title>
		<imprint>
			<publisher>GECCO</publisher>
			<date type="published" when="2012-07">2012. July 2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">TEXPLORE: real-time sample-efficient reinforcement learning for robots</title>
		<author>
			<persName><forename type="first">T</forename><surname>Hester</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Stone</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Machine Learning</title>
		<imprint>
			<biblScope unit="volume">90</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="385" to="429" />
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">A fast learning algorithm for deep belief nets</title>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Osindero</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><forename type="middle">W</forename><surname>Teh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural Computation</title>
		<imprint>
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="1527" to="1554" />
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">Universal Artificial Intelligence: Sequential Decisions based on Algorithmic Probability</title>
		<author>
			<persName><forename type="first">M</forename><surname>Hutter</surname></persName>
		</author>
		<ptr target="http://www.hutter1.net/ai/uaibook.htm" />
		<imprint>
			<date type="published" when="2005">2005</date>
			<publisher>Springer</publisher>
			<biblScope unit="volume">300</biblScope>
			<pubPlace>Berlin</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Learning games from videos guided by descriptive complexity</title>
		<author>
			<persName><forename type="first">L</forename><surname>Kaiser</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
		<editor>
			<persName><forename type="first">J</forename><surname>Hoffmann</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">B</forename><surname>Selman</surname></persName>
		</editor>
		<imprint>
			<publisher>AAAI Press</publisher>
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">Bandit based Monte-Carlo planning</title>
		<author>
			<persName><forename type="first">L</forename><surname>Kocsis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Szepesvari</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2006-09">Sept. 2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">Building high-level features using large scale unsupervised learning</title>
		<author>
			<persName><forename type="first">Q</forename><forename type="middle">V</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Monga</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Devin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Corrado</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Ranzato</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Dean</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">Y</forename><surname>Ng</surname></persName>
		</author>
		<idno>CoRR, abs/1112.6209</idno>
		<imprint>
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title level="m" type="main">An approximation of the universal intelligence measure</title>
		<author>
			<persName><forename type="first">S</forename><surname>Legg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Veness</surname></persName>
		</author>
		<idno>CoRR, abs/1109.5951</idno>
		<imprint>
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Novelty Seach and the Problem with Objectives</title>
		<author>
			<persName><forename type="first">J</forename><surname>Lehman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">O</forename><surname>Stanley</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Genetic Programming in Theory and Practice IX (GPTP 2011), chapter 3</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2011">2011</date>
			<biblScope unit="page" from="37" to="56" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<author>
			<persName><forename type="first">J</forename><surname>Levine</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">B</forename><surname>Congdon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Ebner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Kendall</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">M</forename><surname>Lucas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Miikkulainen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Schaul</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Thompson</surname></persName>
		</author>
		<title level="m">General video game playing</title>
		<imprint>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">General Video Game Playing</title>
		<author>
			<persName><forename type="first">J</forename><surname>Levine</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">B</forename><surname>Congdon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Ebner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Kendall</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">M</forename><surname>Lucas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Miikkulainen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Schaul</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Thompson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Artificial and Computational Intelligence in Games</title>
		<editor>
			<persName><forename type="first">S</forename><forename type="middle">M</forename><surname>Lucas</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">M</forename><surname>Mateas</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">M</forename><surname>Preuss</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">P</forename><surname>Spronck</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">J</forename><surname>Togelius</surname></persName>
		</editor>
		<meeting><address><addrLine>Dagstuhl, Germany</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2013">2013</date>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page" from="77" to="83" />
		</imprint>
		<respStmt>
			<orgName>Schloss Dagstuhl-Leibniz-Zentrum fuer Informatik</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Automatic design and manufacture of robotic lifeforms</title>
		<author>
			<persName><forename type="first">H</forename><surname>Lipson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">B</forename><surname>Pollack</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nature</title>
		<imprint>
			<biblScope unit="issue">406</biblScope>
			<biblScope unit="page" from="974" to="978" />
			<date type="published" when="2000-08-31">31 Aug. 2000</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title/>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">M</forename><surname>Lucas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Ms Pac-Man Competition. SIGEVOlution</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="37" to="38" />
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Design and optimization of an omnidirectional humanoid walk: A winning approach at the RoboCup 2011 3D simulation competition</title>
		<author>
			<persName><forename type="first">P</forename><surname>Macalpine</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Barrett</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Urieli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Vu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Stone</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Twenty-Sixth AAAI Conference on Artificial Intelligence (AAAI)</title>
		<meeting>the Twenty-Sixth AAAI Conference on Artificial Intelligence (AAAI)</meeting>
		<imprint>
			<date type="published" when="2012-07">July 2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
		<author>
			<persName><forename type="first">R</forename><surname>Miikkulainen</surname></persName>
		</author>
		<title level="m">Neuroevolution. In Encyclopedia of Machine Learning</title>
		<imprint>
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
		<author>
			<persName><forename type="first">Y</forename><surname>Naddaf</surname></persName>
		</author>
		<title level="m">Game-Independent AI Agents For Playing Atari 2600 Console Games. Master&apos;s thesis</title>
		<imprint>
			<date type="published" when="2010">2010</date>
		</imprint>
		<respStmt>
			<orgName>University of Alberta</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Backpropagation without Human Supervision for Visual Control in Quake II</title>
		<author>
			<persName><forename type="first">M</forename><surname>Parker</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Bryant</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2009 IEEE Symposium on Computational Intelligence and Games (CIG&apos;09)</title>
		<meeting>the 2009 IEEE Symposium on Computational Intelligence and Games (CIG&apos;09)</meeting>
		<imprint>
			<date type="published" when="2009">2009</date>
			<biblScope unit="page" from="287" to="293" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<monogr>
		<title level="m" type="main">Visual control in quake II with a cyclic controller</title>
		<author>
			<persName><forename type="first">M</forename><surname>Parker</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">D</forename><surname>Bryant</surname></persName>
		</author>
		<editor>P. Hingston and L. Barone, editors, CIG</editor>
		<imprint>
			<date type="published" when="2008">2008</date>
			<publisher>IEEE</publisher>
			<biblScope unit="page" from="151" to="158" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">A video game description language for model-based or interactive learning</title>
		<author>
			<persName><forename type="first">T</forename><surname>Schaul</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computational Intelligence in Games</title>
		<meeting>the IEEE Conference on Computational Intelligence in Games<address><addrLine>Niagara Falls</addrLine></address></meeting>
		<imprint>
			<publisher>IEEE Press</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<monogr>
		<title level="m" type="main">Measuring intelligence through games</title>
		<author>
			<persName><forename type="first">T</forename><surname>Schaul</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Togelius</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Schmidhuber</surname></persName>
		</author>
		<idno>CoRR, abs/1109.1314</idno>
		<imprint>
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<monogr>
		<title level="m" type="main">Humanlike Combat Behavior via Multiobjective Neuroevolution</title>
		<author>
			<persName><forename type="first">J</forename><surname>Schrum</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><forename type="middle">V</forename><surname>Karpov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Miikkulainen</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2012">2012</date>
			<publisher>Springer</publisher>
			<biblScope unit="page" from="119" to="150" />
			<pubPlace>Berlin Heidelberg</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<monogr>
		<title level="m" type="main">Evolving neural network agents in the NERO video game</title>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">O</forename><surname>Stanley</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2005">2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Compositional pattern producing networks: A novel abstraction of development</title>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">O</forename><surname>Stanley</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Genetic Programming and Evolvable Machines</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="131" to="162" />
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">A hypercube-based encoding for evolving large-scale neural networks</title>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">O</forename><surname>Stanley</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">B</forename><surname>. D'ambrosio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Gauci</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Artificial Life</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="185" to="212" />
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<monogr>
		<title level="m" type="main">Evolving neural networks through augmenting topologies</title>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">O</forename><surname>Stanley</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Miikkulainen</surname></persName>
		</author>
		<idno>AI01-290</idno>
		<imprint>
			<date type="published" when="2001-06-01">June 1 2001. 28 Apr 103 21</date>
			<biblScope unit="page">41</biblScope>
			<pubPlace>Mon</pubPlace>
		</imprint>
		<respStmt>
			<orgName>The University of Texas at Austin, Department of Computer Sciences</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">Technical Report</note>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Evolving Neural Networks Through Augmenting Topologies</title>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">O</forename><surname>Stanley</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Miikkulainen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Evolutionary Computation</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="99" to="127" />
			<date type="published" when="2002">2002</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Evolving a roving eye for go</title>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">O</forename><surname>Stanley</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Miikkulainen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">;</forename><forename type="middle">K</forename><surname>Deb</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Poli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Banzhaf</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H.-G</forename><surname>Beyer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">K</forename><surname>Burke</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">J</forename><surname>Darwen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Dasgupta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Floreano</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">A</forename><surname>Foster</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Harman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Holland</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">L</forename><surname>Lanzi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Spector</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="s">Lecture Notes in Computer Science</title>
		<editor>
			<persName><forename type="first">A</forename><surname>Tettamanzi</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">D</forename><surname>Thierens</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">A</forename><forename type="middle">M</forename><surname>Tyrrell</surname></persName>
		</editor>
		<imprint>
			<biblScope unit="volume">3103</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="1226" to="1238" />
			<date type="published" when="2004">2004</date>
			<publisher>Springer</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Scaling reinforcement learning toward RoboCup soccer</title>
		<author>
			<persName><forename type="first">P</forename><surname>Stone</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">S</forename><surname>Sutton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Eighteenth International Conference on Machine Learning</title>
		<meeting>the Eighteenth International Conference on Machine Learning<address><addrLine>San Francisco, CA</addrLine></address></meeting>
		<imprint>
			<publisher>Morgan Kaufmann</publisher>
			<date type="published" when="2001">2001</date>
			<biblScope unit="page" from="537" to="544" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Generating text with recurrent neural networks</title>
		<author>
			<persName><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Martens</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<editor>
			<persName><forename type="first">L</forename><surname>Getoor</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">T</forename><surname>Scheffer</surname></persName>
		</editor>
		<imprint>
			<publisher>Omnipress</publisher>
			<date type="published" when="2011">2011</date>
			<biblScope unit="page" from="1017" to="1024" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Learning tetris using the noisy cross-entropy method</title>
		<author>
			<persName><forename type="first">I</forename><surname>Szita</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Lörincz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural Computation</title>
		<imprint>
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page" from="2936" to="2941" />
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">The First Level of Super Mario Bros. is Easy with Lexicographic Orderings and Time Travel</title>
		<author>
			<persName><forename type="first">T</forename><surname>Murphy</surname></persName>
		</author>
		<author>
			<persName><surname>Vii</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The Association for Computational Heresy (SIGBOVIK)</title>
		<imprint>
			<date type="published" when="2013-04">April 2013</date>
			<biblScope unit="page">2013</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<monogr>
		<title level="m" type="main">Super mario evolution</title>
		<author>
			<persName><forename type="first">J</forename><surname>Togelius</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Karakovskiy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Koutník</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Schmidhuber</surname></persName>
		</author>
		<editor>P. L. Lanzi, editor, CIG</editor>
		<imprint>
			<date type="published" when="2009">2009</date>
			<publisher>IEEE</publisher>
			<biblScope unit="page" from="156" to="161" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Constructing controllers for physical multilegged robots using the enso neuroevolution approach</title>
		<author>
			<persName><forename type="first">V</forename><forename type="middle">K</forename><surname>Valsalam</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Hiller</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Maccurdy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Lipson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Miikkulainen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Evolutionary Intelligence</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1" to="12" />
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Evolving Static Representations for Task Transfer</title>
		<author>
			<persName><forename type="first">P</forename><surname>Verbancsics</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">O</forename><surname>Stanley</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Mach. Learn. Res</title>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="page" from="1737" to="1769" />
			<date type="published" when="2010-08">August 2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<monogr>
		<title level="m" type="main">Using Imagery to Simplify Perceptual Abstraction in Reinforcement Learning Agents</title>
		<author>
			<persName><forename type="first">S</forename><surname>Wintermute</surname></persName>
		</author>
		<editor>M. Fox and D. Poole</editor>
		<imprint>
			<date type="published" when="2010">2010</date>
			<publisher>AAAI Press</publisher>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
