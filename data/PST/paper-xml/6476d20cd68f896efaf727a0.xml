<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Dink-Net: Neural Clustering on Large Graphs</title>
				<funder ref="#_v6zFaFJ">
					<orgName type="full">National Key R&amp;D Program of China</orgName>
				</funder>
				<funder ref="#_wN2QVaZ #_pfJW2Bc #_TzwjDHy #_6k8FFg6">
					<orgName type="full">National Natural Science Foundation of China</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author role="corresp">
							<persName><forename type="first">Yue</forename><surname>Liu</surname></persName>
							<email>gliu@nudt.edu.cn&gt;@stanz.li</email>
							<affiliation key="aff0">
								<orgName type="institution">National University of De-fense Technology</orgName>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="institution">Westlake University</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Ke</forename><surname>Liang</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">National University of De-fense Technology</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Jun</forename><surname>Xia</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution">Westlake University</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Sihang</forename><surname>Zhou</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">National University of De-fense Technology</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Xihong</forename><surname>Yang</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">National University of De-fense Technology</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Xinwang</forename><surname>Liu</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">National University of De-fense Technology</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Stan</forename><forename type="middle">Z</forename><surname>Li</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution">Westlake University</orgName>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff2">
								<address>
									<addrLine>223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269 270 271</addrLine>
									<postBox>May 2023 002 003 004 005 006 007 008 009 010 011 012 013 014 015 016 017 018 019 020 021 022 023 024 025 026 027 028 029 030 031 032 033 034 035 036 037 038 039 040 041 042 043 044 045 046 047 048 049 050 051 052 053 054</postBox>
									<postCode>222, 272 273 274</postCode>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Dink-Net: Neural Clustering on Large Graphs</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="arXiv">arXiv:2305.18405v1[cs.LG]</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-01-03T09:28+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>? Deep graph clustering, which aims to group the nodes of a graph into disjoint clusters with deep neural networks, has achieved promising progress in recent years. However, the existing methods fail to scale to the large graph with million nodes. To solve this problem, a scalable deep graph clustering method (Dink-Net) is proposed with the idea of dilation and shrink. Firstly, by discriminating nodes, whether being corrupted by augmentations, representations are learned in a selfsupervised manner. Meanwhile, the cluster centers are initialized as learnable neural parameters. Subsequently, the clustering distribution is optimized by minimizing the proposed cluster dilation loss and cluster shrink loss in an adversarial manner. By these settings, we unify the two-step clustering, i.e., representation learning and clustering optimization, into an end-to-end framework, guiding the network to learn clustering-friendly features. Besides, Dink-Net scales well to large graphs since the designed loss functions adopt the mini-batch data to optimize the clustering distribution even without performance drops. Both experimental results and theoretical analyses demonstrate the superiority of our method. Compared to the runner-up, Dink-Net achieves 9.62% NMI improvement on the ogbn-papers100M dataset with 111 million nodes and 1.6 billion edges. The source code is released: Dink-Net I . Besides, a collection (papers, codes, and datasets) of deep graph clustering is shared on GitHub II .</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Attribute graph clustering, which aims to separate the nodes in an attribute graph into different groups, has become a fastgrowing research direction in recent years. As a pure unsupervised mission, promising achievements are made by models based on deep neural networks, especially graph neural networks (GNN) <ref type="bibr">(Kipf &amp; Welling, 2017;</ref><ref type="bibr">2016;</ref><ref type="bibr" target="#b55">Veli?kovi? et al., 2018)</ref>. Multiple models are proposed <ref type="bibr">(Wang et al., 2017;</ref><ref type="bibr">Hassani &amp; Khasahmadi, 2020;</ref><ref type="bibr" target="#b47">Peng et al., 2021;</ref><ref type="bibr">Liu et al., 2022b;</ref><ref type="bibr" target="#b10">Gong et al., 2022;</ref><ref type="bibr">Devvrit et al., 2022)</ref>, which generally first embed nodes into the hidden space and then perform clustering algorithms on them. Although proven effective, most existing models in such a manner suffer from scalability issues, i.e., poor scalability on large graphs. Such a problem is one of the most critical challenging tasks in deep graph clustering <ref type="bibr">(Liu et al., 2022c)</ref>, and our work attempts to seek a breakthrough according to it.</p><p>Based on our observations, there are three main reasons for the poor scalability of existing models on large graphs with millions of nodes. Firstly, some methods <ref type="bibr" target="#b7">(Cui et al., 2020;</ref><ref type="bibr">Hassani &amp; Khasahmadi, 2020;</ref><ref type="bibr">Liu et al., 2022b;</ref><ref type="bibr" target="#b10">Gong et al., 2022)</ref> need to process the N ? N dense graph diffusion matrix <ref type="bibr" target="#b19">(Klicpera et al., 2019)</ref>, where N is the number of nodes. Secondly, unlike other tasks like node classification or link prediction, clustering requires the method to estimate the whole sample distribution at once. Therefore, when the node number grows to a considerable value, e.g., 100 million, it easily leads to out-of-memory failure or long-running time problems. Thirdly, current attempts for this problem, such as S 3 GC <ref type="bibr">(Devvrit et al., 2022)</ref>, usually separate the optimization of representation learning and clustering, leading to sub-optimal performance.</p><p>To solve this problem, we present a scalable deep graph clustering method termed dilation shrink network (Dink-Net). The guidance idea is motivated by the dilation and shrink of many galaxies in the universe. Concretely, it mainly consists of the node discriminate module and the neural clustering module. At first, in the node discriminate module, node representations are learned by telling apart the original samples and augmented samples. After the pre-training, the cluster centers are initialized and assigned as optimizable neural parameters with gradients. Additionally, at fine-tuning stage, the neural clustering module optimizes the clustering dis-tribution by minimizing the proposed dilation and shrink loss functions. Concretely, in an adversarial manner, the dilation loss attempts to expand clustering distribution by pushing away different clusters, while the shrink loss aims to compress the clustering distribution by pulling samples back to cluster centers.</p><p>With the above designs, Dink-Net learns the clusteringfriendly representations via unifying the representation learning and clustering optimization into an end-to-end framework. In addition, the proposed loss functions effectively optimize clustering distribution on mini-batch data. Therefore, it endows the scalability of our method without performance drops. Notably, Dink-Net is scaled on a large graph like ogbn-papers100M with 111 million nodes and 1.6 billion edges. The main contributions are summarized as follows.</p><p>? A scalable deep graph clustering method named dilation shrink network (Dink-Net) is proposed to expand the deep graph clustering to large-scale graphs.</p><p>? We are the first to optimize the clustering distribution via the designed dilation and shrink loss functions in an adversarial manner. The method only relies on the mini-batch data, thus, endowing promising scalability.</p><p>? We unify the representation learning and clustering optimization procedures into an end-to-end framework for better clustering-friendly features, leading to superior clustering performance.</p><p>? Both experimental results and theoretical analyses are provided to verify the capability of Dink-Net from six aspects, i.e., superiority, effectiveness, scalability, efficiency, sensitivity, and convergence.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Work</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.">Deep Graph Clustering</head><p>Graph Neural Networks <ref type="bibr">(Kipf &amp; Welling, 2017;</ref><ref type="bibr" target="#b55">Veli?kovi? et al., 2018;</ref><ref type="bibr" target="#b17">Kipf &amp; Welling, 2016;</ref><ref type="bibr">Liu et al., 2022a)</ref> become popular in different graph scenarios <ref type="bibr">(Liang et al., 2022b;</ref><ref type="bibr">a;</ref><ref type="bibr" target="#b41">Meng et al., 2023;</ref><ref type="bibr">Liang et al., 2023b;</ref><ref type="bibr">Liu et al., 2023a;</ref><ref type="bibr">Liang et al., 2023a)</ref>. Among these, attribute graph clustering is a fundamental yet challenging task to separate the nodes in the attribute graph into different clusters without human annotations.</p><p>The early methods <ref type="bibr" target="#b52">(Tian et al., 2014;</ref><ref type="bibr" target="#b4">Cao et al., 2016)</ref> adopt auto-encoders to learn node embeddings and then perform K-Means on them. Subsequently, motivated by the success of graph neural networks (GNNs) <ref type="bibr" target="#b17">(Kipf &amp; Welling, 2016;</ref><ref type="bibr">2017)</ref>, MGAE <ref type="bibr">(Wang et al., 2017)</ref> is proposed to encode nodes with graph-auto-encoders and then group the nodes into clusters with the spectral clustering algorithm. <ref type="bibr" target="#b45">(Pan et al., 2018)</ref> propose ARGA by enforcing the latent representations to align a prior distribution. To design a clustering-directed method, they propose a unified framework termed DAEGC <ref type="bibr">(Wang et al., 2019)</ref> with the attentionbased graph encoder and clustering alignment loss adopted in deep clustering methods <ref type="bibr">(Xie et al., 2016)</ref>. SDCN <ref type="bibr" target="#b1">(Bo et al., 2020)</ref> verifies the effectiveness of integrating structural and attribute information. Then, to avoid the expensive costs of spectral clustering, <ref type="bibr" target="#b0">(Bianchi et al., 2020)</ref> formulate a continuous relaxation of the normalized minCUT problem and optimize the clustering objective with the GNNs. More recently, the contrastive learning <ref type="bibr" target="#b42">(Mo et al., 2022;</ref><ref type="bibr">2023;</ref><ref type="bibr">Zheng et al., 2022a;</ref><ref type="bibr">c)</ref> become hot research hot in deep graph clustering domain <ref type="bibr" target="#b68">(Yang et al., 2023;</ref><ref type="bibr">2022b;</ref><ref type="bibr">a)</ref>. Concretely, AGE <ref type="bibr" target="#b7">(Cui et al., 2020)</ref> filters the high-frequency noises in node attributes and then trains the encoder by adaptively contrasting the positive and negative samples. MVGRL <ref type="bibr">(Hassani &amp; Khasahmadi, 2020)</ref> generates an augmented structural view and contrasts node embeddings from one view with graph embeddings of another view and vice versa. Although the effectiveness of the contrastive learning paradigm is verified, there are still many open technical problems. Specifically, <ref type="bibr">(Zhao et al., 2021)</ref> proposes GDCL to correct sampling bias in contrastive deep graph clustering. Moreover, <ref type="bibr">(Liu et al., 2022b;</ref><ref type="bibr" target="#b30">e)</ref> design the dual correlation reduction strategy in the DCRN model to alleviate the representation collapse problem. Besides, HSAN <ref type="bibr">(Liu et al., 2023d)</ref> mines the hard sample pairs via the dynamic weighting strategy. And SCGC <ref type="bibr">(Liu et al., 2023c)</ref> simplifies the graph augmentation with parameter-unshared Siamese encoders and embedding disturbance. TGC <ref type="bibr">(Liu et al., 2023b)</ref> present a general framework for deep node clustering on temporal graphs. For more details about deep graph clustering, refer to the survey paper <ref type="bibr">(Liu et al., 2022c)</ref>. However, most previous methods fail to scale to large graphs with millions of nodes. In order to alleviate this problem, a scalable deep graph clustering method termed S 3 GC <ref type="bibr">(Devvrit et al., 2022)</ref> is proposed by contrastive learning along with GNNs. Although verifying the effectiveness, they separate the optimization of representation learning and clustering, leading to sub-optimal performance. This paper presents a new scalable method that unifies embedding and clustering into an end-to-end framework. Therefore, our method not only scales to the large graphs but also learns the clusteringfriendly representations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.">Salable Graph Neural Network</head><p>Graph Neural Networks (GNNs) <ref type="bibr">(Kipf &amp; Welling, 2017;</ref><ref type="bibr" target="#b55">Veli?kovi? et al., 2018)</ref> become one of the most effective tools for learning over graph data. Many scalable GNNs have been proposed to scale to large graphs in recent years. For example, GraphSAGE <ref type="bibr" target="#b13">(Hamilton et al., 2017)</ref> develops a general inductive framework by sampling and aggregating features from the local neighborhood of the nodes. Fast-GCN <ref type="bibr" target="#b5">(Chen et al., 2018)</ref> avoids the recursive neighborhood expansion by the layer-wise sampling to nodes in each layer independently. Additionally, SGC <ref type="bibr" target="#b60">(Wu et al., 2019)</ref> decouples the transformation and propagation in GCN <ref type="bibr">(Kipf &amp; Welling, 2017)</ref>. Besides, Graphsaint <ref type="bibr" target="#b69">(Zeng et al., 2019)</ref> and Cluster-GCN <ref type="bibr" target="#b6">(Chiang et al., 2019)</ref> are proposed to better maintain graph structure by sub-graph sampling. Moreover, <ref type="bibr" target="#b21">(Li et al., 2019;</ref><ref type="bibr">2020;</ref><ref type="bibr">2021a;</ref><ref type="bibr" target="#b31">Liu et al., 2020)</ref> aim to design a sequence of works to make GCNs deeper. And various normalization and regularization techniques like DropEdge <ref type="bibr" target="#b49">(Rong et al., 2019)</ref> and ParNorm <ref type="bibr" target="#b72">(Zhao &amp; Akoglu, 2019)</ref> are proposed to avoid over-fitting and over-smoothing. Furthermore, <ref type="bibr" target="#b2">(Bojchevski et al., 2019;</ref><ref type="bibr">2020;</ref><ref type="bibr" target="#b50">Rossi et al., 2020)</ref>  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Methodology</head><p>The methodology of Dink-Net is introduced in this section.</p><p>We first define the problem and summary the basic notation.</p><p>Then, the challenges of scaling deep graph clustering methods to large graphs are carefully illustrated. In addition, our solution to this problem is provided with the reasons.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Basic Notation</head><p>Given an attribute graph G, V = {v 1 , v 2 , ...v N } denotes a set of vertices, and E ? (x, y) (x, y) ? V 2 denotes a set of edges between vertices, where each vertex attaches the corresponding D-dimension attributes. X ? R N ?D and A ? R N ?N are defined as the node attribute matrix and adjacency matrix, separately. Here, N and D denote the number of vertices and dimension number of the attributes, respectively. The basic notation table is presented in Table <ref type="table" target="#tab_4">1</ref> of Appendix.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Problem Definition</head><p>For an attribute graph G, the deep graph clustering algorithm aims to group the vertices into disjoint clusters. Specifically, the self-supervised neural network F embeds the nodes in G into the latent space as follows.</p><formula xml:id="formula_0">H = F(G) = F(X, A),<label>(1)</label></formula><p>where H ? R N ?d denotes the node embeddings and d is the dimension number of latent features. Here, the selfsupervised network F is trained with the pre-text tasks like reconstructive task, contrastive task, discriminative task, etc. In addition to encoding, the clustering method C is designed to group the nodes into different clusters as follows.</p><formula xml:id="formula_1">? = C(H, K), (<label>2</label></formula><formula xml:id="formula_2">)</formula><p>where K is the number of clusters, which can be a hyperparameter or a learnable parameter in the clustering method C. The result ? ? R N is the clustering assignment vector.</p><p>Models suitable for large-scale graphs are always the goal researchers pursue. Unlike node classification and link prediction tasks, performing node clustering on a large-scale graph is more challenging. To this end, we aim to propose a method, which can empower the deep graph clustering algorithms to perform well on large-scale graphs, e.g., the graph with ?111 million nodes and ?10 billion edges. The detailed reasons are analyzed in the following sub-section.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Challenge Analyses</head><p>This section carefully analyzes the challenges of large-scale deep graph clustering. It begins with the differences between deep graph clustering and other tasks like node classification and link prediction. For the node classification task, instead of processing the whole graph data at once, algorithms can divide data into mini-batches <ref type="bibr" target="#b24">(Li et al., 2014)</ref> and merely classify samples in each mini-batch. Similarly, models adopt the mini-batch technique for the link prediction task for the paired nodes and predict the probability of the links between paired nodes in mini-batches. The mini-batch technique works because the predictions of each node or link are relatively independent, and they will not influence each other at the inference stage. However, in the node clustering task, the methods need to group all nodes into disjoint clusters at once. In this process, the cluster assignment of each node will influence each other, and therefore the mini-batch technique easily fails.</p><p>Due to the above concerns, most existing deep graph clustering methods fail to use the mini-batch technique and process the whole data at once in the clustering process.</p><p>Concretely, one class of methods first embeds nodes into the latent space and then directly performs the traditional clustering algorithm <ref type="bibr">(Hartigan &amp; Wong, 1979;</ref><ref type="bibr">Von Luxburg, 2007)</ref> on the learned node representations. We first analyze the complexity of traditional clustering methods. For example, the time complexity and space complexity of K-Means algorithm <ref type="bibr">(Hartigan &amp; Wong, 1979)</ref> is O(tN KD) and O(N K + N D + KD). Here, t, N , K, and D denote the iteration times, node number, cluster number, and attribute dimension number, respectively. In addition, the time complexity and space complexity of spectral clustering algorithm (Von <ref type="bibr">Luxburg, 2007)</ref> is O(N 3 ) and O(N 2 ).</p><p>Besides, the above methods separate the embedding and clustering optimization process, leading to sub-optimal performance. Differently, another class of methods <ref type="bibr">(Wang et al., 2019;</ref><ref type="bibr" target="#b1">Bo et al., 2020;</ref><ref type="bibr">Liu et al., 2022b)</ref> unify the representation learning and clustering optimization into a joint framework by minimizing the KL divergence loss <ref type="bibr">(Xie et al., 2016;</ref><ref type="bibr">Guo et al., 2017)</ref> as follows.</p><p>min L KL = min i j</p><formula xml:id="formula_3">P ij log P ij Q ij ,<label>(3)</label></formula><p>where Q ij is the original clustering distribution and P ij is the sharpened clustering distribution. This loss function optimizes the clustering distribution with the whole data. Thus, the calculation and optimization process is complex and resource-consuming, leading to O(N Kd) time complexity and O(N K + N d + Kd) space complexity.</p><p>Therefore, when the number of nodes N reaches a considerable value like ?111 million on the ogbn-papers100M dataset, the previous two types of methods lead to unacceptable running time and out-of-memory problems. Besides, some methods <ref type="bibr">(Hassani &amp; Khasahmadi, 2020;</ref><ref type="bibr">Zhao et al., 2021;</ref><ref type="bibr">Liu et al., 2022b;</ref><ref type="bibr" target="#b10">Gong et al., 2022)</ref> need to process the N ? N dense graph diffusion matrix <ref type="bibr" target="#b19">(Klicpera et al., 2019)</ref>, which also hinders the efficiency. To this end, we develop a scalable end-to-end deep graph clustering method with the guidance of divide-and-rule.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4.">Proposed Solution</head><p>Through careful analyses in the previous section, we conclude that it is challenging to expand existing deep graph clustering methods to large-scale graphs. To solve this problem, we propose a scalable end-to-end method named dilation shrink network (Dink-Net) with the idea of dilation and shrink of galaxies in the universe. Intuitively, our method optimizes the clustering distribution with the mini-batch data in an adversarial manner, therefore scaling to the large graph. Dink-Net mainly comprises the following node discriminate module and neural clustering module.</p><p>Node Discriminate Module. Given an attribute graph G, we first apply the graph data augmentations ? , like attribute disturbance and edge dropout, on the node attributions X ? R N ?D and graph structure A ? R N ?N . As the result, a augmented view graph view G ? is constructed with X ? and A ? . Subsequently, the parameter-share graph neural network encoder F(?) embeds the nodes of G, G ? to the latent embeddings H, H ? ? R N ?d . Then, a small parameter-shared neural network projection head P( </p><formula xml:id="formula_4">H = F(G), H ? = F(G ? ); 6: Representation projection: Z = P(H), H ? = P(H ? ); 7:</formula><p>Node summary: g = Z.sum(1), g ? = Z ? .sum(1); 8: Calculate discrimination loss Ldiscri. in Eq. ( <ref type="formula">4</ref> the nodes embeddings into a new latent space, where the self-supervised learning loss will be applied. It outputs Z, Z ? ? R N ?d . After that, our method pools the new node embeddings into the node summaries g, g ? ? R N ?1 by the feature aggregation operation. To train the encoder F(?) and projection head P(?), a binary cross entropy loss function is minimized to tell apart the original node and the augmented node summaries below.</p><formula xml:id="formula_5">min L discri. = min 1 N N i=1 1 ? log 1 g i + 0 ? log 1 1 -g i + 1 N N i=1 0 ? log 1 g ? i + 1 ? log 1 1 -g ? i = min 1 N N i=1 log 1 g i + log 1 1 -g ? .</formula><p>(4) where the first term aims to classify the original node summary embeddings to class 1 and the second term attempt to classify the augmented node summary embeddings to another class 0. With this discriminative pre-text task, encoder F and projection head P are trained to extract discriminative features. Besides, this discriminate loss is compatible to batch training techniques on large graphs.</p><p>Neural Clustering Module. This module aims to guide our network to learn clustering-friendly representations. Concretely, based on the learned node representations H, cluster center embeddings C ? R K?d are initialized in the K-Means++ manner <ref type="bibr">(Hartigan &amp; Wong, 1979)</ref>, where K denotes the cluster number. It is worth mentioning that the cluster center embeddings are assigned as the optimizable neural parameters with the gradients. Motivated by the dilation and shrink of galaxies in the universe, we design two loss functions to optimize the clustering distribution jointly. Firstly, since the universe is expanding, the centers of different galaxies are pushed away from each other. Similarly, we attempt to push away different cluster centers by minimizing the proposed cluster dilation loss as follows.</p><formula xml:id="formula_6">min L dilation = min -1 (K -1)K K-1 i=0 K-1 j=0,j? =i ?C i -C j ? 2 2 ,<label>(5)</label></formula><p>where K denotes the cluster number. This cluster dilation loss will not bring high time or memory costs even when the sample number N is large. The idea of dilation loss comes from the universe expansion theory <ref type="bibr" target="#b30">(Linder, 2003)</ref>. The cluster centers are like stars with huge masses, and the samples are like planets around the cluster centers. The universe is expanding, and stars are moving apart from each other. Similarly, our cluster dilation loss pushes the cluster centers away from each other.</p><p>In addition to universe dilation, the galaxy's center will pull together the planets with gravity. From this observation, a cluster shrink loss is designed to optimize clustering distribution by pulling together samples to cluster centers. Considering the considerable sample number, our shrink loss is compatible with using the mini-batch samples. It is formulated as follows.</p><formula xml:id="formula_7">min L shrink = min 1 BK B-1 i=0 K-1 j=0 ?H i -C j ? 2 2 , (<label>6</label></formula><formula xml:id="formula_8">)</formula><p>where B denotes the batch size. Also, this objective will not bring large time or memory cost since it is linear to batch size B rather than sample number N . For this cluster shrink loss, if the clustering algorithm was perfect, we should force the samples close to the nearest cluster center since the perfect clustering algorithm can group the samples with the same ground truth into one cluster. However, for the practical clustering method, this operation easily leads to the confirmation bias problem <ref type="bibr" target="#b44">(Nickerson, 1998)</ref>. To alleviate this problem, a compromise cluster shrink loss is proposed in Eq. ( <ref type="formula" target="#formula_7">6</ref>). to guide the samples to be close to all cluster centers. These two intuitive clustering losses optimize the clustering distribution with mini-batch data in shrink and dilation manners, thus endowing scalability and sample discriminative capability of Dink-Net.</p><p>The overall workflow of our proposed Dink-Net is demonstrated in Algorithm 1 and the PyTorch-style pseudo-code is given in Appendix.E. Detailed implement about Dink-Net can be found in Appendix.C. Next sub-section explores why our method works well on large-scale graphs.</p><p>3.5. Why Dink-Net Works Well on Large Graph?</p><p>By comparing with the existing methods, this section highlights the advantages of our proposed method from two aspects, including model training and model inference.</p><p>Model Training. As illustrated in Algorithm 1, the training process of Dink-Net contains the pre-training and finetuning stages. At the pre-train stage, the encoder F and the projection head P are optimized by minimizing the discriminate loss L discri. in Eq. ( <ref type="formula">4</ref>). For this loss function, the mini-batch technique can be applied since the discrimination process of each sample is independent. Therefore, given embeddings Z, Z ? , the time complexity and space complexity of calculating L discri. is O(Bd) and O(Bd), where B, d denote batch size and dimensions of latent features.</p><p>In the fine-tuning procedure, the total loss is formulated below.</p><formula xml:id="formula_9">min L = min (L dilation + L shrink + ?L discri. ) , (<label>7</label></formula><formula xml:id="formula_10">)</formula><p>where ? is the trade-off hyper-parameter. We analyze the time and space complexity at this stage as follows. Firstly, given embeddings, the time and space complexity of calculating clustering dilation loss L dilation in Eq. ( <ref type="formula" target="#formula_6">5</ref>) is O(K 2 d) and O(Kd), where K denotes the cluster number. Since the K ? N , L dilation do not expend too much time and space resource even when the sample number grows to a large value. Secondly, the clustering shrink module pulls together the samples to the cluster centers. Considering the large sample space, it optimizes the clustering distribution with mini-batch data rather than operating on all samples. Therefore, in Eq. ( <ref type="formula" target="#formula_7">6</ref> Referring to Section 3.3, it is obvious that our method's time and space costs are much less than that of the existing methods. We attribute this advantage to our proposed cluster dilation and shrink loss functions since they allow our method to optimizing the clustering distribution with mini-batch samples even without performance drops. The experimental evidence can be found in Appendix D.2.</p><p>Model Inference. In the model inference process, with the well-learned cluster center embeddings C ? R K?d , the assignment of i-th sample can be calculated as follows.</p><p>?i = arg min</p><formula xml:id="formula_11">j ?H i -C j ? 2 ,<label>(8)</label></formula><p>where ? ? R N denotes the clustering assignment vector.</p><p>Note that the inference of our method also can be carried out in a mini-batch manner. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Experiment</head><p>In this section, we comprehensively evaluate our proposed Dink-Net by answering the main questions as follows.</p><p>? Q1: Superiority. Does Dink-Net outperforms the existing state-of-the-art deep graph clustering methods?</p><p>? Q2: Effectiveness. Are the proposed node discriminate and neural clustering modules effective?</p><p>? Q3: Scalability. Can the proposed method endow the deep graph clustering method scale to large graphs?</p><p>? Q4: Efficiency. How about the time and memory efficiency of the proposed method?</p><p>? Q5: Sensitivity. What is the performance of the proposed method with different hyper-parameters?</p><p>? Q6: Convergence. Will the proposed loss function, as well as the clustering performance, converge well?</p><p>The answers of Q1-Q4 are illustrated in Section 4.2-4.5. In addition, sensitivity analyses (Q5) and convergence analyses (Q6) of Dink-Net can be found in Appendix.D.1 and D.2.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Experimental Setup</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.1.">ENVIRONMENT</head><p>Experimental results are obtained from the server with four core Intel(R) Xeon(R) Platinum 8358 CPUs @ 2.60GHZ, one NVIDIA A100 GPU (40G), and the PyTorch platform.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.2.">DATASET</head><p>To evaluate the node clustering performance, we use seven attribute graph datasets, including Cora, CiteSeer, Amazon-Photo, ogbn-arxiv, Reddit, ogbn-products, ogbn-papers100M <ref type="bibr" target="#b16">(Hu et al., 2020)</ref>. The node numbers of graphs range from ?3 kilo to ?100 million, and the edge numbers of graphs range from ?5 kilo to ?1 billion. The statistical information is summarized in Table <ref type="table" target="#tab_5">2</ref> of Appendix.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.3.">EVALUATION PROTOCOL</head><p>To evaluate the clustering methods, the predicted clustering assignment vector is firstly mapped to the ground truth by the Kuhn-Munkres algorithm <ref type="bibr" target="#b48">(Plummer &amp; Lov?sz, 1986)</ref>. Then, the clustering performance is evaluated by four widely-used metrics <ref type="bibr">(Liu et al., 2022b)</ref>, including accuracy (ACC), normalized mutual information (NMI), adjusted rand index (ARI), and F1-score (F1). All results are obtained under three runs with different random seeds.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.4.">COMPARED BASELINE</head><p>To demonstrate the superiority of the proposed method, we conduct comprehensive experiments to compare our Dink-Net with a variety of baseline methods. Concretely, the classical clustering method K-Means <ref type="bibr">(Hartigan &amp; Wong, 1979)</ref> uses the idea of exception maximum to separate samples. Additionally, the deep clustering methods <ref type="bibr">(Xie et al., 2016;</ref><ref type="bibr">Guo et al., 2017;</ref><ref type="bibr" target="#b65">Yang et al., 2017;</ref><ref type="bibr">Li et al., 2021b)</ref> apply the deep neural networks to assist clustering. Moreover, the deep graph clustering methods <ref type="bibr">(Grover &amp; Leskovec, 2016;</ref><ref type="bibr">Velickovic et al., 2019;</ref><ref type="bibr" target="#b7">Cui et al., 2020;</ref><ref type="bibr" target="#b0">Bianchi et al., 2020;</ref><ref type="bibr">Hassani &amp; Khasahmadi, 2020;</ref><ref type="bibr">Thakoor et al., 2021;</ref><ref type="bibr">Zhu et al., 2020;</ref><ref type="bibr" target="#b10">Gong et al., 2022;</ref><ref type="bibr">Liu et al., 2022b;</ref><ref type="bibr">Devvrit et al., 2022;</ref><ref type="bibr">Wang et al., 2017;</ref><ref type="bibr">2019;</ref><ref type="bibr">Pan et al., 2019;</ref><ref type="bibr" target="#b1">Bo et al., 2020;</ref><ref type="bibr">Zhao et al., 2021;</ref><ref type="bibr">Tu et al., 2020;</ref><ref type="bibr" target="#b20">Lee et al., 2021)</ref> utilize graph neural networks to reveal graph structure and then group nodes into different clusters.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Superiority</head><p>This section answers the question Q1. To illustrate the superiority of the proposed method, extensive experiments are carried out to compare Dink-Net with the existing stateof-the-art methods. Four conclusions are demonstrated by carefully analyzing the results in Table <ref type="table" target="#tab_4">1</ref>.</p><p>1) The performance of the traditional method K-Means  2) The deep clustering methods DEC <ref type="bibr">(Xie et al., 2016)</ref> and DCN <ref type="bibr" target="#b65">(Yang et al., 2017)</ref> achieve an un-promising performance because they merely extract features from node attributes but ignore the structural information. For example, on Cora dataset, our method Dink-Net outperforms DEC by about 38.74% NMI.</p><p>3) For the deep graph clustering method, node2vec <ref type="bibr">(Grover &amp; Leskovec, 2016)</ref> merely takes care of the graph structure and overlooks the node attributes. Besides, the graph representation learning methods <ref type="bibr">(Velickovic et al., 2019;</ref><ref type="bibr" target="#b7">Cui et al., 2020;</ref><ref type="bibr">Hassani &amp; Khasahmadi, 2020;</ref><ref type="bibr">Thakoor et al., 2021;</ref><ref type="bibr">Zhu et al., 2020;</ref><ref type="bibr" target="#b62">Xia et al., 2022)</ref> can not optimize embedding and clustering in a unified framework. Therefore, these methods gain the sub-optimal clustering performance compared to our proposed method. For example, on the CiteSeer dataset, compared with BGRL, our method achieves about 2.86% ACC improvement.</p><p>4) Our proposed method outperforms the recent state-ofthe-art deep graph clustering methods. For example, on the ogbn-papers100M dataset, Dink-Net achieves 9.62% NMI increment compared to the runner-up method S 3 GC <ref type="bibr">(Devvrit et al., 2022)</ref>. The reason contains two aspects as follows. Firstly, the discriminate pre-text task in the representation learning process enhances the discriminative capability of samples. Secondly, the clustering modules unify the representation learning and the clustering process, guiding models to learn clustering-friendly representations.</p><p>Moreover, in order to intuitively demonstrate the superiority of our proposed Dink-Net, we visualize the learned node representations via the t-SNE algorithm ( <ref type="bibr" target="#b54">Van der Maaten &amp; Hinton, 2008)</ref>. As shown in Figure <ref type="figure" target="#fig_1">1</ref>, we find that our proposed method better reveals cluster structure in the latent space. Due to the page limitation, the additional experimental results and analyses are presented in Appendix D.4.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Effectiveness</head><p>The question Q2 is answered in this section. To verify the effectiveness of the proposed modules, we carefully conduct ablation studies on four datasets. Specifically, as shown in Figure <ref type="figure" target="#fig_2">2</ref>, our method is denoted as "Ours". Additionally, our method without the node discriminate module and without the neural clustering module is denoted as "(w/o) NDM" and "(w/o) NCM", respectively. From the experimental results in Figure <ref type="figure" target="#fig_2">2</ref>, three observations are presented as follows. 1) "(w/o) NDM" can not achieve expected clustering performance since it lacks the strong representation learning capability of the node discriminate module.</p><p>2) The results indicate that "(w/o) NCM" becomes the runner-up. Although the node discriminate module endows strong representation capability, it can not learn clusteringfriendly features since the process of embedding, and clustering is detached.</p><p>3) Our Dink-Net achieves the best clustering performance because it unifies representation learning and clustering optimization to extract clustering-friendly features.</p><p>The above results and observations prove the effectiveness of the node discriminate and neural clustering module in Dink-Net. Both of them can boost performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.">Scalability</head><p>This section attempts to answer the question Q3. To verify the scalability of the proposed method, we conduct experiments on seven graph datasets with different scales. For instance, the Cora dataset contains 2708 nodes, and ogbn-papers100M contains ?111 million nodes. The statistics of these datasets can be found in Table <ref type="table" target="#tab_5">2</ref> in Appendix. Table <ref type="table" target="#tab_4">1</ref> provides the clustering performance on these datasets. From these results, we have four observations as follows.</p><p>1) The traditional clustering method K-Means can complete the clustering process on seven datasets. However, there are two drawbacks as follows. Firstly, it fails to achieve promising performance since it lacks the representation learning process. For example, on the Reddit dataset our method outperforms K-Means 67.13% from the aspect of ACC.</p><p>Secondly, it takes a long running time on large-scale graph datasets, e.g., ?5 days for papers100M dataset on CPU.</p><p>2) The deep clustering methods raise the out-of-memory failure on the Reddit and ogbn-papers100M datasets. The main reason for enormous memory costs is that the KL divergence loss <ref type="bibr">(Xie et al., 2016;</ref><ref type="bibr">Guo et al., 2017)</ref> estimates and sharpens the cluster distribution with all samples, leading to enormous memory costs. Also, they neglect the graph structure leading to worse performance.</p><p>3) The most of deep graph clustering methods easily lead to the out-of-memory problem because some methods <ref type="bibr">(Hassani &amp; Khasahmadi, 2020;</ref><ref type="bibr">Liu et al., 2022b;</ref><ref type="bibr" target="#b10">Gong et al., 2022)</ref> need to process N ? N dense graph diffusion matrix, which is inefficient on time and memory. For the scalable methods like node2vec <ref type="bibr">(Grover &amp; Leskovec, 2016)</ref>, DGI <ref type="bibr">(Velickovic et al., 2019)</ref>, and S 3 GC <ref type="bibr">(Devvrit et al., 2022)</ref>, they separate the embedding and clustering, leading to suboptimal performance. 4) Our proposed Dink-Net scales well to all seven datasets and achieves promising performances. The reasons and analyses are demonstrated in Section 3.5.</p><p>Through the above observations, we conclude that the existing methods easily lead to the out-of-memory problem or the long ruining time problem. But our method can endow the models with excellent scalability to large-scale graphs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5.">Efficiency</head><p>This section attempts to answer the question Q4. To verify the efficiency of Dink-Net, we test the time and memory costs of various methods on the Cora and ogbn-papers100M datasets. The main observations (See Table <ref type="table" target="#tab_6">1</ref> and<ref type="table" target="#tab_7">Table 2</ref> ) and analyses are illustrated.</p><p>1) For the time costs, on ogbn-papers100M dataset, the scalable baselines node2vec <ref type="bibr">(Grover &amp; Leskovec, 2016)</ref>, DGI <ref type="bibr">(Velickovic et al., 2019)</ref>, and S 3 GC( 2) For the memory costs on ogbn-papers100M dataset, most baseline methods raise the out-of-memory problem on 40GB GPU. But our method takes ?20GB GPU memory during training. In addition, from a theoretical perspective, calculating discriminative loss and clustering loss take O(Bd) and O(BK + Bd + Kd) space complexity, which are both linear to node number in a mini-batch.</p><p>These results and analyses demonstrate the efficiency of our proposed method in both the time and memory aspects. Due to page limitation, the additional experiments and complexity analyses can be found in Appendix.B.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Conclusion</head><p>This work aims to scale deep graph clustering to large graphs. It begins with analyzing the drawbacks of existing methods. Firstly, part of the method must process a spaceconsuming graph diffusion matrix. Secondly, some algorithms must optimize clustering distribution with all nodes, easily resulting in the out-of-memory problem. Thirdly, the scalable S 3 GC achieves sub-optimal performance since it separates representation and clustering. To solve this problem, a novel scalable deep graph clustering termed Dink-Net is proposed under the guidance idea of dilation and shrink.</p><p>Our method contains the node discriminate and neural clustering module. With these designs, we unify representation learning and clustering optimization into an end-to-end framework, guiding the network to learn clustering-friendly features. Also, the cluster dilation and shrink loss functions allow our method to optimize clustering distribution with mini-batch data. Extensive experiments and theoretical analyses verify the superiority. In the future, it is worth to extending Dink-Net to heterogeneous graphs <ref type="bibr" target="#b73">(Zheng et al., 2021)</ref>, heterophily graphs <ref type="bibr">(Liu et al., 2022d)</ref>, knowledge graphs <ref type="bibr">(Liang et al., 2022a)</ref>, and molecular graphs <ref type="bibr" target="#b63">(Xia et al., 2023)</ref>.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Time and Space Analyses</head><p>In this section, we analyze and summarize the time and space complexity of the various baseline methods, including spectral clustering <ref type="bibr">(Von Luxburg, 2007)</ref>, K-Means <ref type="bibr">(Hartigan &amp; Wong, 1979)</ref>, DEC <ref type="bibr">(Xie et al., 2016)</ref>, node2vec <ref type="bibr">(Grover &amp; Leskovec, 2016)</ref>, DGI <ref type="bibr">(Velickovic et al., 2019)</ref>, MVGRL <ref type="bibr">(Hassani &amp; Khasahmadi, 2020)</ref>, GRACE <ref type="bibr">(Zhu et al., 2020)</ref>, BGRL <ref type="bibr">(Thakoor et al., 2021)</ref>, S 3 GC <ref type="bibr">(Devvrit et al., 2022)</ref>, and our proposed Dink-Net in Table <ref type="table" target="#tab_8">3</ref>. Here, N denotes the node number in the graph, B denotes the batch size, K denotes the cluster number, E denotes the edges number of the graph, S denotes the average degree of the graph, D denotes the dimensions of node attributes, and d denotes the dimensions of latent features.</p><p>From these analyses, we find that the complexity of most methods will become unacceptable when the sample number reaches a tremendous value. Different, our method's time and memory complexity is linear to the batch size, alleviating the out-of-memory and long-running time problems. In addition, we also test the memory and time costs of these methods via </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Design Details &amp; Hyper-parameter Settings</head><p>In this section, we introduce the design details of our proposed method and summarize the hyper-parameter settings.</p><p>Following the existing works <ref type="bibr">(Zheng et al., 2022;</ref><ref type="bibr">Velickovic et al., 2019)</ref>, for the encoder F, we adopt the graph convolutional network (GCN) <ref type="bibr">(Kipf &amp; Welling, 2017)</ref>. Besides, we use multilayer perceptron (MLP) <ref type="bibr" target="#b86">(Pal &amp; Mitra, 1992)</ref> as the projector in Dink-Net. Next, we report the hyper-parameter settings of our method in  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D. Additional Experimental Result</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D.1. Compare Experiment</head><p>Due to the limited regular paper pages, the additional compare experimental results are demonstrated in Table <ref type="table" target="#tab_11">5</ref>. We further compare our proposed Dink-Net with the nine baselines, including IDEC <ref type="bibr">(Guo et al., 2017)</ref>, AdaGAE <ref type="bibr">(Li et al., 2021)</ref>, MGAE <ref type="bibr">(Wang et al., 2017)</ref>, DAEGC <ref type="bibr">(Wang et al., 2019)</ref>, ARGA <ref type="bibr">(Pan et al., 2019)</ref>, DMoN <ref type="bibr" target="#b89">(Tsitsulin et al., 2020)</ref>, SDCN <ref type="bibr" target="#b1">(Bo et al., 2020)</ref>, GDCL <ref type="bibr">(Zhao et al., 2021)</ref>, and DFCN <ref type="bibr">(Tu et al., 2020)</ref>. These additional experimental results further verify the superiority and scalability of our proposed Dink-Net.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D.2. Sensitivity Analyses</head><p>This section aims to answer Q5: Is the performance of the proposed method sensitive to hyper-parameters? This section analyzes the sensitivity of our proposed Dink-Net.  values of ?. Therefore it is not sensitive to ?. Secondly, we analyze another important hyper-parameter batch size B on the ogbn-papers100M dataset. As shown in Figure <ref type="figure" target="#fig_2">2</ref> (c), we find that the performance of Dink-Net is not sensitive to batch size B, therefore our proposed loss functions allow our method to optimizing clustering distribution with mini-batch data even without performance drops. Besides, training model with a larger batch size will bring some extent of ACC improvement.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D.3. Convergence Analyses</head><p>This section aims to answer Q6: Can the proposed loss functions and the clustering performance converge well? To verify the convergence of our proposed Dink-Net, we conduct experiments on two datasets, including Cora and CiteSeer. Specifically, as shown in Figure <ref type="figure">3</ref>  </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head></head><label></label><figDesc>), it only brings O(BKd) time complexity and O(BK + Bd + Kd) space complexity when given the embeddings. Thirdly, calculating L discri. takes O(Kd) time complexity and O(Kd) space complexity. To summarize, at the fine-tune stage, L C takes O(BKd + K 2 d + Kd) ? O(BKd + K 2 d) time and O(BK + Bd + Kd) space costs given the embeddings.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 1</head><label>1</label><figDesc>Figure 1. t-SNE visualization of seven methods on the Cora dataset.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 2 .</head><label>2</label><figDesc>Figure 2. Ablation studies of the proposed modules on four datasets. "(w/o) NDM" denotes Dink-Net without the node discriminate module. "(w/o) NDM" denotes Dink-Net without the neural clustering module. "Ours" denotes our proposed Dink-Net.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 1 .</head><label>1</label><figDesc>Figure 1. t-SNE visualization of seven methods on Cora dataset.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head></head><label></label><figDesc>Appendix of "Dink-Net: Neural Clustering on Large Graph" Algorithm 1 PyTorch-style Pseudo Code of Dink-Net. (Z.sum(-1), Z_.sum(-1))</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head></head><label></label><figDesc>); 9: Adam optimizer with learning rate ? updates parameters ? by minimizing Ldiscri.; 10: end for 11: # Model fine-tuning stage 12: Initialize the cluster center embeddings C in the K-means++ manner based on the learned node embeddings H; 13: for epoch = 1, 2, ..., T ? do</figDesc><table><row><cell>14: 15: 16: 17: 18: 19: 20: 21: 22: 23: end for Generate batched graph data B with shuffle; for G B in B do Node encoding: H = F(G B ); Calculate discrimination loss Ldiscri. in Eq. (4); Calculate dilation loss Ldilation in Eq. (5); Calculate shrink loss Lshrink in Eq. (6); Calculate the total loss L in Eq. (7); Adam optimizer with learning rate ? ? updates parameters ? and cluster centers C by minimizing L; end for 24: # Model inference stage 25: Generate batched graph data B without shuffle; 26: for G B in B do 27: Node encoding: H = F(G B ); 28: Predict cluster-ID of batch data ?B by Eq. (8); 29: end for 30: Concatenate batched cluster-ID and obtain ?; 31: Return ?</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head></head><label></label><figDesc>Therefore, when given embeddings, the time and space complexity of model inference is O(BKd) and O(BK + Bd + Kd), where B is batch size.The above complexity analyses demonstrate time and space efficiency in theory. Compared with the existing state-ofthe-art methods, the main advantages of our method are summarized as follows. 1) Dink-Net gets rid of from pro-</figDesc><table /><note><p>cessing N ? N graph diffusion matrix. 2) Our proposed loss functions allow Dink-Net to optimize the clustering distribution with mini-batch data even without performance drops. 3) Dink-Net unifies embedding learning and clustering optimization, resulting in clustering-friendly representations. Therefore, this sub-section illustrates that Dink-Net can scale well to large-scale graphs in theory. The next section aims to verify the superiority, effectiveness, scalability, and efficiency of Dink-Net by extensive experiments.</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 1 .</head><label>1</label><figDesc>Clustering performance</figDesc><table><row><cell>Dataset</cell><cell cols="14">Metric K-Means DEC DCN node2vec DGI AGE MinCutPool MVGRL BGRL GRACE ProGCL AGC-DRR DCRN S 3 GC Ours</cell></row><row><cell>Cora</cell><cell>ACC NMI ARI F1</cell><cell>33.80 14.98 8.60 30.26</cell><cell>46.50 49.38 23.54 25.65 15.13 21.63 39.23 43.71</cell><cell>61.20 44.40 32.90 62.10</cell><cell cols="2">72.60 73.50 57.10 57.58 51.10 50.10 69.20 69.28</cell><cell>49.00 41.00 30.80 51.80</cell><cell>76.30 60.80 56.60 71.60</cell><cell>74.20 58.40 53.40 69.10</cell><cell>73.90 57.00 52.70 72.50</cell><cell>57.13 41.02 30.71 45.68</cell><cell>40.62 18.74 14.80 31.23</cell><cell cols="2">61.93 74.20 78.10 45.13 58.80 62.28 33.15 54.40 61.61 49.50 72.10 72.66</cell></row><row><cell>CiteSeer</cell><cell>ACC NMI ARI F1</cell><cell>39.32 16.94 13.43 36.08</cell><cell>55.89 57.08 28.34 27.64 28.12 29.31 52.62 53.80</cell><cell>42.10 24.00 11.60 40.10</cell><cell cols="2">68.60 69.73 43.50 44.93 44.50 45.31 64.30 64.45</cell><cell>53.70 29.50 26.20 51.60</cell><cell>62.83 40.69 34.18 59.54</cell><cell>67.50 42.20 42.80 63.10</cell><cell>63.10 39.90 37.70 60.30</cell><cell>65.92 39.59 36.16 57.89</cell><cell>68.32 43.28 45.34 64.82</cell><cell cols="2">69.86 68.80 70.36 44.86 44.10 45.87 45.64 44.80 46.96 64.83 64.30 65.96</cell></row><row><cell>Amazon-Photo</cell><cell>ACC NMI ARI F1</cell><cell>27.22 13.23 5.50 23.96</cell><cell>47.22 48.25 37.35 38.76 18.59 20.80 46.71 47.87</cell><cell>27.58 11.53 4.92 21.52</cell><cell cols="2">43.03 75.98 33.67 65.38 22.15 55.89 35.17 71.74</cell><cell>54.67 50.02 34.43 53.02</cell><cell>41.07 30.28 18.77 32.88</cell><cell>66.54 60.11 44.14 63.08</cell><cell>67.66 53.46 42.74 60.30</cell><cell>51.53 39.56 34.18 31.97</cell><cell>76.81 66.54 60.15 71.03</cell><cell cols="2">79.94 75.15 81.71 73.70 59.78 74.36 63.69 56.13 68.40 73.82 72.85 73.92</cell></row><row><cell>ogbn-arXiv</cell><cell>ACC NMI ARI F1</cell><cell>18.11 22.13 7.43 12.94</cell><cell>21.25 19.91 25.14 23.81 10.28 8.25 15.57 13.06</cell><cell>29.00 40.60 19.00 22.00</cell><cell>31.40 41.20 22.30 23.00</cell><cell>OOM</cell><cell>24.20 38.00 13.90 19.80</cell><cell>OOM</cell><cell>22.70 32.10 13.00 16.60</cell><cell>OOM</cell><cell>29.86 37.51 25.74 21.79</cell><cell>OOM</cell><cell>OOM</cell><cell>35.00 43.68 46.30 43.73 27.00 35.22 23.00 26.92</cell></row><row><cell>ogbn-products</cell><cell>ACC NMI ARI F1</cell><cell>18.11 22.13 7.43 12.94</cell><cell>23.79 24.50 24.47 21.92 9.05 10.96 13.54 13.95</cell><cell>35.70 48.90 17.00 24.70</cell><cell>32.00 46.70 17.40 19.20</cell><cell>OOM</cell><cell>25.70 43.00 13.00 18.00</cell><cell>OOM</cell><cell>OOM</cell><cell>OOM</cell><cell>35.21 46.59 19.87 21.55</cell><cell>OOM</cell><cell>OOM</cell><cell>40.20 41.09 53.60 50.78 23.00 21.08 25.00 25.15</cell></row><row><cell>Reddit</cell><cell>ACC NMI ARI F1</cell><cell>8.90 11.40 2.90 6.80</cell><cell>OOM OOM</cell><cell>70.90 79.20 64.00 55.10</cell><cell>22.40 30.60 17.00 18.30</cell><cell>OOM</cell><cell>-</cell><cell>OOM</cell><cell>OOM</cell><cell>OOM</cell><cell>65.41 70.48 63.42 51.45</cell><cell>OOM</cell><cell>OOM</cell><cell>73.60 76.03 80.70 78.91 74.50 71.34 56.00 67.95</cell></row><row><cell>ogbn-papers100M</cell><cell>ACC NMI ARI F1</cell><cell>14.60 37.33 7.54 10.45</cell><cell>OOM OOM</cell><cell>17.50 38.00 11.20 11.10</cell><cell>15.10 41.60 9.60 11.10</cell><cell>OOM</cell><cell>OOM</cell><cell>OOM</cell><cell>OOM</cell><cell>OOM</cell><cell>OOM</cell><cell>OOM</cell><cell>OOM</cell><cell>17.30 26.67 45.30 54.92 11.00 18.01 11.80 19.48</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 2 .</head><label>2</label><figDesc>Devvrit et al., 2022)   Time and space analyses of various methods. The experimental GPU memory costs and time costs are obtained on Cora dataset. run in ?24 hours. Differently, our proposed method takes ?9 hours for model pre-training and ?3 hours for model fine-tuning. From the theoretical view, at the pre-training stage, the time complexity of calculating discriminative loss is O(Bd), which is linear to batch size. Moreover, at fine-tuning state, calculating the clustering loss takes O(BKd + K 2 d + Kd) time complexity, which is also linear to batch size.</figDesc><table><row><cell>Method</cell><cell>Time Complexity (per iteration)</cell><cell>Space Complexity</cell><cell cols="2">Time Cost (s) Memory Cost (MB)</cell></row><row><cell>DGI MVGRL S 3 GC Dink-Net (Ours)</cell><cell>O(ED + N d 2 ) O(N 2 d + N d 2 ) O(N Sd 2 ) O(BKd + K 2 d)</cell><cell>O(E + N d + d 2 ) O(N 2 + N d + d 2 ) O(N d + BSd + d 2 ) O(BK + Bd + Kd)</cell><cell>19.03 168.20 508.21 35.09</cell><cell>3798 9466 1474 1248</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 1 .</head><label>1</label><figDesc>Basic Notations</figDesc><table><row><cell>Dataset</cell><cell>Type</cell><cell># Nodes</cell><cell># Edges</cell><cell cols="2"># Feature Dims # Classes</cell></row><row><cell cols="4">Cora CiteSeer Amazon-Photo ogbn-arxiv Reddit ogbn-products ogbn-papers100M Attributed Graph 111,059,956 1,615,685,872 Attributed Graph 2,708 5,278 Attributed Graph 3,327 4,614 Attributed Graph 7,650 119,081 Attributed Graph 169,343 1,166,243 Attributed Graph 232,965 23,213,838 Attributed Graph 2,449,029 61,859,140</cell><cell>1,433 3,703 745 128 602 100 128</cell><cell>7 6 8 40 41 47 172</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 2 .</head><label>2</label><figDesc>The statistical information of seven datasets.</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>Table 3 .</head><label>3</label><figDesc>Cora dataset. From these experimental results, we find that our proposed Dink-Net also achieves efficient results even without batch-training techniques on a small dataset. More importantly, Dink-Net is compatible with the mini-batch training technique even without performance drops. Therefore it scales well with the large graphs. Experimental evidence can be found in Figure2. Time and space analyses of various methods. The experimental costs are obtained on the Cora dataset. "-" means ruining on CPU.</figDesc><table><row><cell>arXiv:2305.18405v1 [cs.LG] 28 May 2023</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head>Table 4 .</head><label>4</label><figDesc>Here, T is the epoch number of pre-training, T ? is the epoch number of fine-tuning, ? is the learning rate of pre-training, ? ? is the learning rate of fine-tuning, ? is the trade-off parameter, B is the batch size, and d is the dimension number of latent features.</figDesc><table><row><cell></cell><cell>T</cell><cell>?</cell><cell>T ?</cell><cell>? ?</cell><cell>?</cell><cell>B</cell><cell>d</cell></row><row><cell>Cora CiteSeer Amazon-Photo ogbn-arXiv ogbn-products Reddit ogbn-papers100M</cell><cell cols="7">200 1e-3 200 1e-2 1e-10 100 5e-4 200 1e-2 1e-10 2000 5e-4 100 1e-2 1e-10 1 1e-4 100 1e-4 1e-10 8192 1500 -512 -1536 -512 10 1e-3 10 1e-2 1e-10 8192 1024 10 1e-4 1 1e-5 1e-10 10240 512 1 1e-4 1 1e-5 1e-10 10240 256</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_10"><head>Table 4 .</head><label>4</label><figDesc>Hyper-parameter settings of our proposed method. "-" denotes that it does not use mini-batch training.</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_11"><head>Table 5 .</head><label>5</label><figDesc>Clustering performance (%) of our method and nine state-of-the-art baselines. The bold values are the best results. "OOM" indicates that the method raise the out-of-memory failure.</figDesc><table><row><cell>110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 164 163 162 161 160 159 158 157 156 155 154 153 152 151 150 149</cell><cell>ogbn-products Reddit ogbn-papers100M Raw Attribute</cell><cell>NMI ARI F1 ACC NMI ARI F1 ACC NMI ARI F1 ACC NMI ARI F1 DEC</cell><cell>27.54 12.15 17.58 20.53 22.15 9.87 12.48 OOM OOM</cell><cell>OOM OOM OOM OOM MVGRL</cell><cell>OOM OOM OOM OOM MinCutPool OOM OOM OOM OOM</cell><cell>OOM OOM OOM OOM DCRN 25.00 35.60 12.70 19.00 30.40 42.80 13.90 21.00 52.90 62.80 50.20 26.00 OOM</cell><cell>OOM OOM OOM OOM S 3 GC OOM OOM OOM OOM</cell><cell>OOM OOM OOM OOM Ours</cell><cell>43.68 43.73 35.22 26.92 41.09 50.78 21.08 25.15 76.03 78.91 71.34 67.95 26.67 54.92 18.01 19.48</cell></row></table><note><p>Firstly, we analyze the trade-off parameter ? on Cora and CiteSeer datasets. As shown in Figure 2 (a) and Figure 2(b), we find that our Dink-Net can achieve good performance with different</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_12"><head></head><label></label><figDesc>, the NMI and loss values are recorded per epoch in the training process. From these experimental results, we observe that the loss values gradually decrease and tend to converge. Meanwhile, the clustering performance NMI increases. Therefore, our proposed method converges well.</figDesc><table><row><cell>166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 276 277 278 279 280 281 282 283 284 285 286 287 288 289 290 291 292 293 294 295 296 297 298 299 300 301 302 303 304 305 306 307 308 309 310 311 312 313 314 315 316 317 318 319 320 321 322 323 324 325 326 327 328 329</cell></row></table></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div><head n="6.">Acknowledgments</head><p>This work was supported by the <rs type="funder">National Key R&amp;D Program of China</rs> (project no. <rs type="grantNumber">2020AAA0107100</rs>) and the <rs type="funder">National Natural Science Foundation of China</rs> (project no. <rs type="grantNumber">61922088</rs>, <rs type="grantNumber">61976196</rs>, <rs type="grantNumber">62006237</rs>, and <rs type="grantNumber">61872371</rs>).</p></div>
			</div>
			<listOrg type="funding">
				<org type="funding" xml:id="_v6zFaFJ">
					<idno type="grant-number">2020AAA0107100</idno>
				</org>
				<org type="funding" xml:id="_wN2QVaZ">
					<idno type="grant-number">61922088</idno>
				</org>
				<org type="funding" xml:id="_pfJW2Bc">
					<idno type="grant-number">61976196</idno>
				</org>
				<org type="funding" xml:id="_TzwjDHy">
					<idno type="grant-number">62006237</idno>
				</org>
				<org type="funding" xml:id="_6k8FFg6">
					<idno type="grant-number">61872371</idno>
				</org>
			</listOrg>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Notations &amp; Datasets</head><p>The basic notations are summarized in Table <ref type="table">1</ref>. Table <ref type="table">2</ref> lists the statistical information about seven datasets. These datasets have various scales. For example, CiteSeer has about 3.3K nodes and 4.6K edges, while ogbn-papers100M has about 111M nodes and 1.6B edges. In addition, the network densities of these graphs are various. Concretely, the density of Cora is 0.07% while the density of Amazon-Photo is 0.25%.</p><p>Sample Label Vector  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E. PyTorch-style Pseudo Code</head><p>We give the PyTorch-style pseudo code of our proposed Dink-Net in Code 1.</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Spectral clustering with graph neural networks for graph pooling</title>
		<author>
			<persName><forename type="first">F</forename><forename type="middle">M</forename><surname>Bianchi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Grattarola</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Alippi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of ICML</title>
		<meeting>of ICML</meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Structural deep clustering network</title>
		<author>
			<persName><forename type="first">D</forename><surname>Bo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Cui</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of WWW</title>
		<meeting>of WWW</meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Is pagerank all you need for scalable graph neural networks?</title>
		<author>
			<persName><forename type="first">A</forename><surname>Bojchevski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Klicpera</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Perozzi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Blais</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Kapoor</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Lukasik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>G?nnemann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACM KDD, MLG Workshop</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Scaling graph neural networks with approximate pagerank</title>
		<author>
			<persName><forename type="first">A</forename><surname>Bojchevski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Klicpera</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Perozzi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Kapoor</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Blais</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>R?zemberczki</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Lukasik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>G?nnemann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 26th ACM SIGKDD International Conference on Knowledge Discovery &amp; Data Mining</title>
		<meeting>the 26th ACM SIGKDD International Conference on Knowledge Discovery &amp; Data Mining</meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="2464" to="2473" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Deep neural networks for learning graph representations</title>
		<author>
			<persName><forename type="first">S</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Xu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of AAAI</title>
		<meeting>of AAAI</meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">Fastgcn: fast learning with graph convolutional networks via importance sampling</title>
		<author>
			<persName><forename type="first">J</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiao</forename></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename></persName>
		</author>
		<idno type="arXiv">arXiv:1801.10247</idno>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Cluster-gcn: An efficient algorithm for training deep and large graph convolutional networks</title>
		<author>
			<persName><forename type="first">W.-L</forename><surname>Chiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Si</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C.-J</forename><surname>Hsieh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 25th ACM SIGKDD international conference on knowledge discovery &amp; data mining</title>
		<meeting>the 25th ACM SIGKDD international conference on knowledge discovery &amp; data mining</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="257" to="266" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Adaptive graph encoder for attributed graph embedding</title>
		<author>
			<persName><forename type="first">G</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of KDD</title>
		<meeting>of KDD</meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">S3gc: Scalable self-supervised graph clustering</title>
		<author>
			<persName><forename type="first">F</forename><surname>Devvrit</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Sinha</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Dhillon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Jain</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Sketch-gnn: Scalable graph neural networks with sublinear training complexity</title>
		<author>
			<persName><forename type="first">M</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Rabbani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>An</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NeurIPS 2022 Workshop: New Frontiers in Graph Learning</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Attributed graph clustering with dual redundancy reduction</title>
		<author>
			<persName><forename type="first">L</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Tu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of IJCAI</title>
		<meeting>of IJCAI</meeting>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">node2vec: Scalable feature learning for networks</title>
		<author>
			<persName><forename type="first">A</forename><surname>Grover</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Leskovec</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 22nd ACM SIGKDD international conference on Knowledge discovery and data mining</title>
		<meeting>the 22nd ACM SIGKDD international conference on Knowledge discovery and data mining</meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="855" to="864" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Improved deep embedded clustering with local structure preservation</title>
		<author>
			<persName><forename type="first">X</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Yin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of IJCAI</title>
		<meeting>of IJCAI</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">Inductive representation learning on large graphs. Advances in neural information processing systems</title>
		<author>
			<persName><forename type="first">W</forename><surname>Hamilton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Ying</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Leskovec</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="volume">30</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Algorithm as 136: A k-means clustering algorithm</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">A</forename><surname>Hartigan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">A</forename><surname>Wong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of the royal statistical society. series c (applied statistics)</title>
		<imprint>
			<date type="published" when="1979">1979</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Contrastive multi-view representation learning on graphs</title>
		<author>
			<persName><forename type="first">K</forename><surname>Hassani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">H</forename><surname>Khasahmadi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of ICML</title>
		<meeting>of ICML</meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Open graph benchmark: Datasets for machine learning on graphs</title>
		<author>
			<persName><forename type="first">W</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Fey</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Zitnik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Catasta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Leskovec</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page" from="22118" to="22133" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">N</forename><surname>Kipf</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Welling</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1611.07308</idno>
		<title level="m">Variational graph auto-encoders</title>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Semi-supervised classification with graph convolutional networks</title>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">N</forename><surname>Kipf</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Welling</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of ICLR</title>
		<meeting>of ICLR</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<author>
			<persName><forename type="first">J</forename><surname>Klicpera</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Wei?enberger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>G?nnemann</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1911.05485</idno>
		<title level="m">Diffusion improves graph learning</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">Augmentation-free self-supervised learning on graphs</title>
		<author>
			<persName><forename type="first">N</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Park</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2112.02472</idno>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Deepgcns: Can gcns go as deep as cnns?</title>
		<author>
			<persName><forename type="first">G</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Muller</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Thabet</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Ghanem</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF international conference on computer vision</title>
		<meeting>the IEEE/CVF international conference on computer vision</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="9267" to="9276" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">Deepergcn: All you need to train deeper gcns</title>
		<author>
			<persName><forename type="first">G</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Thabet</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Ghanem</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2006.07739</idno>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Training graph neural networks with 1000 layers</title>
		<author>
			<persName><forename type="first">G</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>M?ller</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Ghanem</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Koltun</surname></persName>
		</author>
		<idno>PMLR</idno>
	</analytic>
	<monogr>
		<title level="m">International conference on machine learning</title>
		<imprint>
			<biblScope unit="page">2021</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Efficient minibatch training for stochastic optimization</title>
		<author>
			<persName><forename type="first">M</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">J</forename><surname>Smola</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 20th ACM SIGKDD international conference on Knowledge discovery and data mining</title>
		<meeting>the 20th ACM SIGKDD international conference on Knowledge discovery and data mining</meeting>
		<imprint>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="661" to="670" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Adaptive graph autoencoder for general data clustering</title>
		<author>
			<persName><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<author>
			<persName><forename type="first">K</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Tu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2211.10738</idno>
		<title level="m">Relational symmetry based knowledge graph contrastive learning</title>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<author>
			<persName><forename type="first">K</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Meng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Tu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Sun</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2212.05767</idno>
		<title level="m">Reasoning over different types of knowledge graphs: Static, temporal and multimodal</title>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<author>
			<persName><forename type="first">K</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Meng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Tu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Liu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2305.14074</idno>
		<title level="m">Message intercommunication for inductive relation reasoning</title>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Abslearn: a gnn-based framework for aliasing and buffer-size information retrieval</title>
		<author>
			<persName><forename type="first">K</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Tan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Pattern Analysis and Applications</title>
		<imprint>
			<date type="published" when="2023">2023</date>
			<biblScope unit="page" from="1" to="19" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Exploring the expansion history of the universe</title>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">V</forename><surname>Linder</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Physical review letters</title>
		<imprint>
			<biblScope unit="volume">90</biblScope>
			<biblScope unit="issue">9</biblScope>
			<biblScope unit="page">91301</biblScope>
			<date type="published" when="2003">2003</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Towards deeper graph neural networks</title>
		<author>
			<persName><forename type="first">M</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Ji</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 26th ACM SIGKDD international conference on knowledge discovery &amp; data mining</title>
		<meeting>the 26th ACM SIGKDD international conference on knowledge discovery &amp; data mining</meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="338" to="348" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
		<title level="m" type="main">Self-supervised temporal graph learning with temporal and structural intensity alignment</title>
		<author>
			<persName><forename type="first">M</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Tu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Liu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2302.07491</idno>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
		<author>
			<persName><forename type="first">M</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Liu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2305.10738</idno>
		<title level="m">Deep temporal graph clustering</title>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Graph self-supervised learning: A survey</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Xia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">Y</forename><surname>Philip</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Knowledge and Data Engineering</title>
		<imprint>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="5879" to="5900" />
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Deep graph clustering via dual correlation reduction</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Tu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Zhu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of AAAI</title>
		<meeting>of AAAI</meeting>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<monogr>
		<author>
			<persName><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Xia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Tu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><forename type="middle">S</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Liu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2211.12875</idno>
		<title level="m">A survey of deep graph clustering: Taxonomy, challenge, and application</title>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b37">
	<monogr>
		<title level="m" type="main">Beyond smoothing: Unsupervised graph representation learning with edge heterophily discriminating</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Pan</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2211.14065</idno>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b38">
	<monogr>
		<title level="m" type="main">Improved dual correlation reduction network</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Tu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Yang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2202.12533</idno>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Simple contrastive graph clustering</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Neural Networks and Learning Systems</title>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Hard sample aware network for contrastive deep graph clustering</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Tu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Duan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of AAAI</title>
		<meeting>of AAAI</meeting>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<monogr>
		<title level="m" type="main">Aliasing relation assisted self-supervised learning for few-shot relation reasoning</title>
		<author>
			<persName><forename type="first">L</forename><surname>Meng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><surname>Sarf</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2304.10297</idno>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Simple unsupervised graph representation learning</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Mo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Zhu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
		<imprint>
			<date type="published" when="2022">2022</date>
			<biblScope unit="page" from="7797" to="7805" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Multiplex graph representation learning via dual correlation reduction</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Mo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Lei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Zhu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Knowledge and Data Engineering</title>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Confirmation bias: A ubiquitous phenomenon in many guises</title>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">S</forename><surname>Nickerson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Review of general psychology</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="175" to="220" />
			<date type="published" when="1998">1998</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Adversarially regularized graph autoencoder for graph embedding</title>
		<author>
			<persName><forename type="first">S</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of IJCAI</title>
		<meeting>of IJCAI</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Learning graph embedding with adversarial training methods</title>
		<author>
			<persName><forename type="first">S</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S.-F</forename><surname>Fung</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE transactions on cybernetics</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Attention-driven graph clustering network</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Hou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of ACM MM</title>
		<meeting>of ACM MM</meeting>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<monogr>
		<title level="m" type="main">Recipe for a general, powerful, scalable graph transformer</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">D</forename><surname>Plummer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Lov?sz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Ramp??ek</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Galkin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><forename type="middle">P</forename><surname>Dwivedi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">T</forename><surname>Luu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Wolf</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Beaini</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2205.12454</idno>
		<imprint>
			<date type="published" when="1986">1986. 2022</date>
			<publisher>Elsevier</publisher>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
	<note>Matching theory</note>
</biblStruct>

<biblStruct xml:id="b49">
	<monogr>
		<author>
			<persName><forename type="first">Y</forename><surname>Rong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><surname>Dropedge</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1907.10903</idno>
		<title level="m">Towards deep graph convolutional networks on node classification</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b50">
	<monogr>
		<author>
			<persName><forename type="first">E</forename><surname>Rossi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Frasca</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Chamberlain</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Eynard</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Bronstein</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Monti</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2004.11198</idno>
		<title level="m">Sign: Scalable inception graph neural networks</title>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page">15</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Bootstrapped representation learning on graphs</title>
		<author>
			<persName><forename type="first">S</forename><surname>Thakoor</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Tallec</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">G</forename><surname>Azar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Munos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Veli?kovi?</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Valko</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR 2021 Workshop on Geometrical and Topological Representation Learning</title>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">Learning deep representations for graph clustering</title>
		<author>
			<persName><forename type="first">F</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T.-Y</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of AAAI</title>
		<meeting>of AAAI</meeting>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<monogr>
		<author>
			<persName><forename type="first">W</forename><surname>Tu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Cheng</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2012.09600</idno>
		<title level="m">Deep fusion clustering network</title>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">Visualizing data using t-sne</title>
		<author>
			<persName><forename type="first">L</forename><surname>Van Der Maaten</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of machine learning research</title>
		<imprint>
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<analytic>
		<title level="a" type="main">Graph attention networks</title>
		<author>
			<persName><forename type="first">P</forename><surname>Veli?kovi?</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Cucurull</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Casanova</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Romero</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Li?</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of ICLR</title>
		<meeting>of ICLR</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<monogr>
		<author>
			<persName><forename type="first">P</forename><surname>Velickovic</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Fedus</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">L</forename><surname>Hamilton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Li?</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">D</forename><surname>Hjelm</surname></persName>
		</author>
		<title level="m">Deep graph infomax. ICLR (Poster)</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<analytic>
		<title level="a" type="main">A tutorial on spectral clustering</title>
		<author>
			<persName><forename type="first">Von</forename><surname>Luxburg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">U</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Statistics and computing</title>
		<imprint>
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b58">
	<analytic>
		<title level="a" type="main">Marginalized graph autoencoder for graph clustering</title>
		<author>
			<persName><forename type="first">C</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><surname>Mgae</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2017 ACM on Conference on Information and Knowledge Management</title>
		<meeting>the 2017 ACM on Conference on Information and Knowledge Management</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b59">
	<monogr>
		<title level="m" type="main">Attributed graph clustering: A deep attentional embedding approach</title>
		<author>
			<persName><forename type="first">C</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Zhang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1906.06532</idno>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b60">
	<analytic>
		<title level="a" type="main">Simplifying graph convolutional networks</title>
		<author>
			<persName><forename type="first">F</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Souza</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Fifty</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Weinberger</surname></persName>
		</author>
		<idno>PMLR</idno>
	</analytic>
	<monogr>
		<title level="m">International conference on machine learning</title>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="6861" to="6871" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b61">
	<analytic>
		<title level="a" type="main">Nodeformer: A scalable graph structure learning transformer for node classification</title>
		<author>
			<persName><forename type="first">Q</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Wipf</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Yan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b62">
	<analytic>
		<title level="a" type="main">Progcl: Rethinking hard negative mining in graph contrastive learning</title>
		<author>
			<persName><forename type="first">J</forename><surname>Xia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">Z</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of ICML</title>
		<meeting>of ICML</meeting>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b63">
	<analytic>
		<title level="a" type="main">Mole-bert: Rethinking pre-training graph neural networks for molecules</title>
		<author>
			<persName><forename type="first">J</forename><surname>Xia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">Z</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The Eleventh International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b64">
	<analytic>
		<title level="a" type="main">Unsupervised deep embedding for clustering analysis</title>
		<author>
			<persName><forename type="first">J</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Farhadi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of ICML</title>
		<meeting>of ICML</meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b65">
	<analytic>
		<title level="a" type="main">Towards k-means-friendly spaces: Simultaneous deep learning and clustering</title>
		<author>
			<persName><forename type="first">B</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><forename type="middle">D</forename><surname>Sidiropoulos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Hong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of ICML</title>
		<meeting>of ICML</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b66">
	<monogr>
		<title level="m" type="main">Interpolation-based correlation reduction network for semi-supervised graph learning</title>
		<author>
			<persName><forename type="first">X</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Zhu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2206.02796</idno>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b67">
	<monogr>
		<author>
			<persName><forename type="first">X</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Zhu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2212.03559</idno>
		<title level="m">Contrastive deep graph clustering with learnable augmentation</title>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b68">
	<analytic>
		<title level="a" type="main">Cluster-guided contrastive graph clustering network</title>
		<author>
			<persName><forename type="first">X</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Tu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Zhu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of AAAI</title>
		<meeting>of AAAI</meeting>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b69">
	<monogr>
		<title level="m" type="main">Graphsaint: Graph sampling based inductive learning method</title>
		<author>
			<persName><forename type="first">H</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Srivastava</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Kannan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Prasanna</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1907.04931</idno>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b70">
	<analytic>
		<title level="a" type="main">Scalegcn: Efficient and effective graph convolution via channelwise scale transformation</title>
		<author>
			<persName><forename type="first">T</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Han</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Neural Networks and Learning Systems</title>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b71">
	<analytic>
		<title level="a" type="main">Graph debiased contrastive learning with joint representation clustering</title>
		<author>
			<persName><forename type="first">H</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Deng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of IJCAI</title>
		<meeting>of IJCAI</meeting>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b72">
	<monogr>
		<author>
			<persName><forename type="first">L</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Akoglu</surname></persName>
		</author>
		<author>
			<persName><surname>Pairnorm</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1909.12223</idno>
		<title level="m">Tackling oversmoothing in gnns</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b73">
	<analytic>
		<title level="a" type="main">Heterogeneous graph attention network for small and medium-sized enterprises bankruptcy prediction</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><forename type="middle">C</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Pan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Knowledge Discovery and Data Mining: 25th Pacific-Asia Conference, PAKDD 2021, Virtual Event</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2021">May 11-14, 2021. 2021</date>
			<biblScope unit="page" from="140" to="151" />
		</imprint>
	</monogr>
	<note>Proceedings, Part I, pp</note>
</biblStruct>

<biblStruct xml:id="b74">
	<analytic>
		<title level="a" type="main">Toward graph self-supervised learning with contrastive adjusted zooming</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y.-F</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Neural Networks and Learning Systems</title>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b75">
	<monogr>
		<title level="m" type="main">Rethinking and scaling up graph contrastive learning: An extremely efficient approach with group discrimination</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><forename type="middle">C</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">S</forename><surname>Yu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2206.01535</idno>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b76">
	<analytic>
		<title level="a" type="main">Unifying graph contrastive learning with flexible contextual scopes</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><forename type="middle">C</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Pan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2022 IEEE International Conference on Data Mining (ICDM)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2022">2022</date>
			<biblScope unit="page" from="793" to="802" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b77">
	<analytic>
		<title level="a" type="main">Deep Graph Contrastive Representation Learning</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML Workshop on Graph Representation Learning and Beyond</title>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b78">
	<analytic>
		<title level="a" type="main">Structural deep clustering network</title>
		<author>
			<persName><forename type="first">D</forename><surname>References Bo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Cui</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of WWW</title>
		<meeting>of WWW</meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b79">
	<monogr>
		<title level="m" type="main">S3gc: Scalable self-supervised graph clustering</title>
		<author>
			<persName><forename type="first">F</forename><surname>Devvrit</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Sinha</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Dhillon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Jain</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b80">
	<analytic>
		<title level="a" type="main">node2vec: Scalable feature learning for networks</title>
		<author>
			<persName><forename type="first">A</forename><surname>Grover</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Leskovec</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 22nd ACM SIGKDD international conference on Knowledge discovery and data mining</title>
		<meeting>the 22nd ACM SIGKDD international conference on Knowledge discovery and data mining</meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="855" to="864" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b81">
	<analytic>
		<title level="a" type="main">Improved deep embedded clustering with local structure preservation</title>
		<author>
			<persName><forename type="first">X</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Yin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of IJCAI</title>
		<meeting>of IJCAI</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b82">
	<analytic>
		<title level="a" type="main">Algorithm as 136: A k-means clustering algorithm</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">A</forename><surname>Hartigan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">A</forename><surname>Wong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of the royal statistical society. series c (applied statistics)</title>
		<imprint>
			<date type="published" when="1979">1979</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b83">
	<analytic>
		<title level="a" type="main">Contrastive multi-view representation learning on graphs</title>
		<author>
			<persName><forename type="first">K</forename><surname>Hassani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">H</forename><surname>Khasahmadi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of ICML</title>
		<meeting>of ICML</meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b84">
	<analytic>
		<title level="a" type="main">Semi-supervised classification with graph convolutional networks</title>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">N</forename><surname>Kipf</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Welling</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of ICLR</title>
		<meeting>of ICLR</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b85">
	<analytic>
		<title level="a" type="main">Adaptive graph auto-encoder for general data clustering</title>
		<author>
			<persName><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b86">
	<monogr>
		<title level="m" type="main">Multilayer perceptron, fuzzy sets, classifiaction</title>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">K</forename><surname>Pal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Mitra</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1992">1992</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b87">
	<analytic>
		<title level="a" type="main">Learning graph embedding with adversarial training methods</title>
		<author>
			<persName><forename type="first">S</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S.-F</forename><surname>Fung</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE transactions on cybernetics</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b88">
	<analytic>
		<title level="a" type="main">Bootstrapped representation learning on graphs</title>
		<author>
			<persName><forename type="first">S</forename><surname>Thakoor</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Tallec</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">G</forename><surname>Azar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Munos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Veli?kovi?</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Valko</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR 2021 Workshop on Geometrical and Topological Representation Learning</title>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b89">
	<monogr>
		<title level="m" type="main">Graph clustering with graph neural networks</title>
		<author>
			<persName><forename type="first">A</forename><surname>Tsitsulin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Palowitch</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Perozzi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>M?ller</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2006.16904</idno>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b90">
	<monogr>
		<author>
			<persName><forename type="first">W</forename><surname>Tu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Cheng</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2012.09600</idno>
		<title level="m">Deep fusion clustering network</title>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b91">
	<monogr>
		<author>
			<persName><forename type="first">P</forename><surname>Velickovic</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Fedus</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">L</forename><surname>Hamilton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Li?</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">D</forename><surname>Hjelm</surname></persName>
		</author>
		<title level="m">Deep graph infomax. ICLR (Poster)</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b92">
	<analytic>
		<title level="a" type="main">A tutorial on spectral clustering</title>
		<author>
			<persName><forename type="first">Von</forename><surname>Luxburg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">U</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Statistics and computing</title>
		<imprint>
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b93">
	<analytic>
		<title level="a" type="main">Marginalized graph autoencoder for graph clustering</title>
		<author>
			<persName><forename type="first">C</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><surname>Mgae</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2017 ACM on Conference on Information and Knowledge Management</title>
		<meeting>the 2017 ACM on Conference on Information and Knowledge Management</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b94">
	<monogr>
		<title level="m" type="main">Attributed graph clustering: A deep attentional embedding approach</title>
		<author>
			<persName><forename type="first">C</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Zhang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1906.06532</idno>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b95">
	<analytic>
		<title level="a" type="main">Unsupervised deep embedding for clustering analysis</title>
		<author>
			<persName><forename type="first">J</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Farhadi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of ICML</title>
		<meeting>of ICML</meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b96">
	<analytic>
		<title level="a" type="main">Graph debiased contrastive learning with joint representation clustering</title>
		<author>
			<persName><forename type="first">H</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Deng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of IJCAI</title>
		<meeting>of IJCAI</meeting>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b97">
	<monogr>
		<title level="m" type="main">Rethinking and scaling up graph contrastive learning: An extremely efficient approach with group discrimination</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><forename type="middle">C</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">S</forename><surname>Yu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2206.01535</idno>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b98">
	<analytic>
		<title level="a" type="main">Deep Graph Contrastive Representation Learning</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML Workshop on Graph Representation Learning and Beyond</title>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
