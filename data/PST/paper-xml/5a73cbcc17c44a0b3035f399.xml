<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Adversarial Examples: Attacks and Defenses for Deep Learning</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date type="published" when="2018-07-07">7 Jul 2018</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><roleName>Pan He,</roleName><forename type="first">Xiaoyong</forename><surname>Yuan</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">National Science Foundation Center for Big Learning</orgName>
								<orgName type="institution">University of Florida</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Qile</forename><surname>Zhu</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">National Science Foundation Center for Big Learning</orgName>
								<orgName type="institution">University of Florida</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Xiaolin</forename><surname>Li</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">National Science Foundation Center for Big Learning</orgName>
								<orgName type="institution">University of Florida</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Adversarial Examples: Attacks and Defenses for Deep Learning</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2018-07-07">7 Jul 2018</date>
						</imprint>
					</monogr>
					<idno type="arXiv">arXiv:1712.07107v3[cs.LG]</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.2" ident="GROBID" when="2022-12-25T13:12+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>deep neural network</term>
					<term>deep learning</term>
					<term>security</term>
					<term>adversarial examples</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>With rapid progress and significant successes in a wide spectrum of applications, deep learning is being applied in many safety-critical environments. However, deep neural networks have been recently found vulnerable to well-designed input samples, called adversarial examples. Adversarial examples are imperceptible to human but can easily fool deep neural networks in the testing/deploying stage. The vulnerability to adversarial examples becomes one of the major risks for applying deep neural networks in safety-critical environments. Therefore, attacks and defenses on adversarial examples draw great attention. In this paper, we review recent findings on adversarial examples for deep neural networks, summarize the methods for generating adversarial examples, and propose a taxonomy of these methods. Under the taxonomy, applications for adversarial examples are investigated. We further elaborate on countermeasures for adversarial examples and explore the challenges and the potential solutions.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>I. INTRODUCTION</head><p>Deep learning (DL) has made significant progress in a wide domain of machine learning (ML): image classification, object recognition <ref type="bibr" target="#b0">[1]</ref>, <ref type="bibr" target="#b1">[2]</ref>, object detection <ref type="bibr" target="#b2">[3]</ref>, <ref type="bibr" target="#b3">[4]</ref>, speech recognition <ref type="bibr" target="#b4">[5]</ref>, language translation <ref type="bibr" target="#b5">[6]</ref>, voice synthesis <ref type="bibr" target="#b6">[7]</ref>. The online Go Master (AlphaGo <ref type="bibr" target="#b7">[8]</ref>) beat more than 50 top go players in the world. Recently AlphaGo Zero <ref type="bibr" target="#b8">[9]</ref> surpassed its previous version without using human knowledge and a generic version, AlphaZero <ref type="bibr" target="#b9">[10]</ref>, achieved a superhuman level within 24 hours of cross domains of chess, Shogi, and Go.</p><p>Driven by the emergence of big data and hardware acceleration, deep learning requires less hand engineered features and expert knowledge. The intricacy of data can be extracted with higher and more abstract level representation from raw input features <ref type="bibr" target="#b10">[11]</ref>.</p><p>Constantly increasing number of real-world applications and systems have been powered by deep learning. For instance, companies from IT to the auto industry (e.g., Google, Telsa, Mercedes, and Uber) are testing self-driving cars, which require plenty of deep learning techniques such as object recognition, reinforcement learning, and multimodal learning. Face recognition system has been deployed in ATMs as a method of biometric authentication <ref type="bibr" target="#b11">[12]</ref>. Apple also provides face authentication to unlock mobile phones <ref type="bibr" target="#b12">[13]</ref>. Behavior-based malware detection and anomaly detection solutions are built upon deep learning to find semantic features <ref type="bibr" target="#b13">[14]</ref>- <ref type="bibr" target="#b16">[17]</ref>.</p><p>Despite great successes in numerous applications, many of deep learning empowered applications are life crucial, raising great concerns in the field of safety and security. "With great power comes great responsibility" <ref type="bibr" target="#b17">[18]</ref>. Recent studies find that deep learning is vulnerable against well-designed input samples. These samples can easily fool a well-performed deep learning model with little perturbations imperceptible to humans.</p><p>Szegedy et al. first generated small perturbations on the images for the image classification problem and fooled state-ofthe-art deep neural networks with high probability <ref type="bibr" target="#b18">[19]</ref>. These misclassified samples were named as Adversarial Examples.</p><p>Extensive deep learning based applications have been used or planned to be deployed in the physical world, especially in the safety-critical environments. In the meanwhile, recent studies show that adversarial examples can be applied to real world. For instance, an adversary can construct physical adversarial examples and confuse autonomous vehicles by manipulating the stop sign in a traffic sign recognition system <ref type="bibr" target="#b19">[20]</ref>, <ref type="bibr" target="#b20">[21]</ref> or removing the segmentation of pedestrians in an object recognition system <ref type="bibr" target="#b21">[22]</ref>. Attackers can generate adversarial commands against automatic speech recognition (ASR) models and Voice Controllable System (VCS) <ref type="bibr" target="#b22">[23]</ref>, <ref type="bibr" target="#b23">[24]</ref> such as Apple Siri <ref type="bibr" target="#b24">[25]</ref>, Amazon Alexa <ref type="bibr" target="#b25">[26]</ref>, and Microsoft Cortana <ref type="bibr" target="#b26">[27]</ref>.</p><p>Deep learning is widely regarded as a "black box" technique -we all know that it performs well, but with limited knowledge of the reason <ref type="bibr" target="#b27">[28]</ref>, <ref type="bibr" target="#b28">[29]</ref>. Many studies have been proposed to explain and interpret deep neural networks <ref type="bibr" target="#b29">[30]</ref>- <ref type="bibr" target="#b32">[33]</ref>. From inspecting adversarial examples, we may gain insights on semantic inner levels of neural networks <ref type="bibr" target="#b33">[34]</ref> and find problematic decision boundaries, which in turn helps to increase robustness and performance of neural networks <ref type="bibr" target="#b34">[35]</ref> and improve the interpretability <ref type="bibr" target="#b35">[36]</ref>.</p><p>In this paper, we investigate and summarize approaches for generating adversarial examples, applications for adversarial examples and corresponding countermeasures. We explore the characteristics and possible causes of adversarial examples. Recent advances in deep learning revolve around supervised learning, especially in the field of computer vision task. Therefore, most adversarial examples are generated against computer vision models. We mainly discuss adversarial examples for image classification/object recognition tasks in this paper. Adversarial examples for other tasks will be investigated Hessian, the second-order of derivatives KL(•)</p><p>Kullback-Leibler (KL) divergence function in Section V.</p><p>Inspired by <ref type="bibr" target="#b36">[37]</ref>, we define the Threat Model as follows:</p><p>• The adversaries can attack only at the testing/deploying stage. They can tamper only the input data in the testing stage after the victim deep learning model is trained.</p><p>Neither the trained model or the training dataset can be modified. The adversaries may have knowledge of trained models (architectures and parameters) but are not allowed to modify models, which is a common assumption for many online machine learning services. Attacking at the training stage (e.g., training data poisoning) is another interesting topic and has been studied in <ref type="bibr" target="#b37">[38]</ref>- <ref type="bibr" target="#b42">[43]</ref>. Due to the limitation of space, we do not include this topic in the paper. • We focus on attacks against models built with deep neural networks, due to their great performance achieved. We will discuss adversarial examples against conventional machine learning (e.g., SVM, Random Forest) in Section II. Adversarial examples against deep neural networks proved effective in conventional machine learning models <ref type="bibr" target="#b43">[44]</ref> (see Section VII-A). • Adversaries only aim at compromising integrity. Integrity is presented by performance metrics (e.g., accuracy, F1 score, AUC), which is essential to a deep learning model. Although other security issues pertaining to confidentiality and privacy have been drawn attention in deep learning <ref type="bibr" target="#b44">[45]</ref>- <ref type="bibr" target="#b46">[47]</ref>, we focus on the attacks that degrade the performance of deep learning models, cause an increase of false positives and false negatives. • The rest of the threat model differs in different adversarial attacks. We will categorize them in Section III. Notations and symbols used in this paper are listed in Table <ref type="table" target="#tab_0">I</ref>.</p><p>This paper presents the following contributions:</p><p>• To systematically analyze approaches for generating adversarial examples, we taxonomize attack approaches along different axes to provide an accessible and intuitive overview of these approaches. We discuss current challenges and potential solutions in Section VII. Section VIII concludes the work.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>II. BACKGROUND</head><p>In this section, we briefly introduce basic deep learning techniques and approaches related to adversarial examples. Next, we review adversarial examples in the era of conventional ML and compare the difference between adversarial examples in conventional ML and that in DL.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Brief Introduction to Deep Learning</head><p>This subsection discusses main concepts, existed techniques, popular architectures, and standard datasets in deep learning, which, due to the extensive use and breakthrough successes, have become acknowledged targets of attacks, where adversaries are usually applied to evaluate their attack methods.</p><p>1) Main concepts in deep learning: Deep learning is a type of machine learning methods that makes computers to learn from experience and knowledge without explicit programming and extract useful patterns from raw data. For conventional machine learning algorithms, it is difficult to extract wellrepresented features due to limitations, such as curse of dimensionality <ref type="bibr" target="#b47">[48]</ref>, computational bottleneck <ref type="bibr" target="#b48">[49]</ref>, and requirement of the domain and expert knowledge. Deep learning solves the problem of representation by building multiple simple features to represent a sophisticated concept. For example, a deep learning-based image classification system represents an object by describing edges, fabrics, and structures in the hidden layers. With the increasing number of available training data, deep learning becomes more powerful. Deep learning models have solved many complicated problems, with the help of hardware acceleration in computational time.</p><p>A neural network layer is composed of a set of perceptrons (artificial neurons). Each perceptron maps a set of inputs to output values with an activation function. The function of a neural network is formed in a chain:</p><formula xml:id="formula_0">f (x) = f (k) (• • • f (2) (f (1) (x))),<label>(1)</label></formula><p>where f (i) is the function of the ith layer of the network,</p><formula xml:id="formula_1">i = 1, 2, • • • k.</formula><p>Convolutional neural networks (CNNs) and Recurrent neural networks (RNNs) are two most widely used neural networks in recent neural network architectures. CNNs deploy convolution operations on hidden layers to share weights and reduce the number of parameters. CNNs can extract local information from grid-like input data. CNNs have shown incredible successes in computer vision tasks, such as image classification <ref type="bibr" target="#b0">[1]</ref>, <ref type="bibr" target="#b49">[50]</ref>, object detection <ref type="bibr" target="#b3">[4]</ref>, <ref type="bibr" target="#b50">[51]</ref>, and semantic segmentation <ref type="bibr" target="#b51">[52]</ref>, <ref type="bibr" target="#b52">[53]</ref>. RNNs are neural networks for processing sequential input data with variable length. RNNs produce outputs at each time step. The hidden neuron at each time step is calculated based on current input data and hidden neurons at previous time step. Long Short-Term Memory (LSTM) and Gated Recurrent Unit (GRU) with controllable gates are designed to avoid vanishing/exploding gradients of RNNs in long-term dependency.</p><p>2) Architectures of deep neural networks: Several deep learning architectures are widely used in computer vision tasks: LeNet <ref type="bibr" target="#b53">[54]</ref>, VGG <ref type="bibr" target="#b1">[2]</ref>, AlexNet <ref type="bibr" target="#b0">[1]</ref>, GoogLeNet [55]- <ref type="bibr" target="#b56">[57]</ref> (Inception V1-V4), and ResNet <ref type="bibr" target="#b49">[50]</ref>, from the simplest (oldest) network to the deepest and the most complex (newest) one. AlexNet first showed that deep learning models can largely surpass conventional machine learning algorithms in the ImageNet 2012 challenge and led the future study of deep learning. These architectures made tremendous breakthroughs in the ImageNet challenge and can be seen as milestones in image classification problem. Attackers usually generate adversarial examples against these baseline architectures.</p><p>3) Standard deep learning datasets: MNIST, CIFAR-10, ImageNet are three widely used datasets in computer vision tasks. The MNIST dataset is for handwritten digits recognition <ref type="bibr" target="#b57">[58]</ref>. The CIFAR-10 dataset and the ImageNet dataset are for image recognition task <ref type="bibr" target="#b58">[59]</ref>. The CIFAR-10 consists of 60,000 tiny color images (32 × 32) with ten classes. The ImageNet dataset consists 14,197,122 images with 1,000 classes <ref type="bibr" target="#b59">[60]</ref>. Because of the large number of images in the ImageNet dataset, most adversarial approaches are evaluated on only part of the ImageNet dataset. The Street View House Numbers (SVHN) dataset, similar to the MNIST dataset, consists of ten digits obtained from real-world house numbers in Google Street View images. The YoutubeDataset dataset is gained from Youtube consisting of about ten million images <ref type="bibr" target="#b53">[54]</ref> and used in <ref type="bibr" target="#b18">[19]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Adversarial Examples and Countermeasures in Machine Learning</head><p>Adversarial examples in conventional machine learning models have been discussed since decades ago. Machine learning-based systems with handcrafted features are primary targets, such as spam filters, intrusion detection, biometric authentication, fraud detection, etc. <ref type="bibr" target="#b60">[61]</ref>. For example, spam emails are often modified by adding characters to avoid detection <ref type="bibr" target="#b61">[62]</ref>- <ref type="bibr" target="#b63">[64]</ref>.</p><p>Dalvi et al. first discussed adversarial examples and formulated this problem as a game between adversary and classifier (Naïve Bayes), both of which are sensitive to cost <ref type="bibr" target="#b61">[62]</ref>. The attack and defense on adversarial examples became an iterative game. Biggio et al. first tried a gradient-based approach to generate adversarial examples against linear classifier, support vector machine (SVM), and a neural network <ref type="bibr" target="#b64">[65]</ref>. Compared with deep learning adversarial examples, their methods allow more freedom to modify the data. The MNIST dataset was first evaluated under their attack, although a human could easily distinguish the adversarial digit images. Biggio et al. also reviewed several proactive defenses and discussed reactive approaches to improve the security of machine learning models <ref type="bibr" target="#b38">[39]</ref>.</p><p>Barreno et al. presented an initial investigation on the security problems of machine learning <ref type="bibr" target="#b60">[61]</ref>, <ref type="bibr" target="#b65">[66]</ref>. They categorized attacking against machine learning system into three axes: 1) influence: whether attacks can poison the training data; 2) security violation: whether an adversarial example belongs to false positive or false negative; 3) specificity: attack is targeted to a particular instance or a wide class. We discuss these axes for deep learning area in Section III. Barreno et al. compared attacks against SpamBayes spam filter and defenses as a study case. However, they mainly focused on binary classification problem such as virus detection system, intrusion detection system (IDS), and intrusion prevention system (IPS).</p><p>Adversarial examples in conventional machine learning require knowledge of feature extraction, while deep learning usually needs only raw data input. In conventional ML, both attacking and defending methods paid great attention to features, even the previous step (data collection), giving less attention to the impact of humans. Then the target becomes a fully automatic machine learning system. Inspired by these studies on conventional ML, in this paper, we review recent security issues in the deep learning area.</p><p>[37] provided a comprehensive overview of security issues in machine learning and recent findings in deep learning. <ref type="bibr" target="#b36">[37]</ref> established a unifying threat model. A "no free lunch" theorem was introduced: the tradeoff between accuracy and robustness.</p><p>Compared to their work, our paper focuses on adversarial examples in deep learning and has a detailed discussion on recent studies and findings.</p><p>For example, adversarial examples in an image classification task can be described as follows: Using a trained image classifier published by a third party, a user inputs one image to get the prediction of class label. Adversarial images are original clean images with small perturbations, often barely recognizable by humans. However, such perturbations misguide the image classifier. The user will get a response of an incorrect image label. Given a trained deep learning model f and an original input data sample x, generating an adversarial example x can generally be described as a box-constrained optimization problem:</p><formula xml:id="formula_2">min x x − x s.t. f (x ) = l , f (x) = l, l = l , x ∈ [0, 1],<label>(2)</label></formula><p>where l and l denote the output label of x and x , • denotes the distance between two data sample. Let η = x − x be the perturbation added on x. This optimization problem minimizes the perturbation while misclassifying the prediction with a constraint of input data. In the rest of the paper, we will discuss variants of generating adversarial images and adversarial examples in other tasks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>III. TAXONOMY OF ADVERSARIAL EXAMPLES</head><p>To systematically analyze approaches for generating adversarial examples, we analyze the approaches for generating adversarial examples (see details in Section IV) and categorize them along three dimensions: threat model, perturbation, and benchmark.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Threat Model</head><p>We discuss the threat model in Section I. Based on different scenarios, assumptions, and quality requirements, adversaries decide the attributes they need in adversarial examples and then deploy specific attack approaches. We further decompose the threat model into four aspects: adversarial falsification, adversary's knowledge, adversarial specificity, and attack frequency. For example, if an adversarial example is required to be generated in real-time, adversaries should choose a onetime attack instead of an iterative attack, in order to complete the task (see Attack Frequency).</p><p>• Adversarial Falsification -False positive attacks generate a negative sample which is misclassified as a positive one (Type I Error). In a malware detection task, a benign software being classified as malware is a false positive. In an image classification task, a false positive can be an adversarial image unrecognizable to human, while deep neural networks predict it to a class with a high confidence score. Figure <ref type="figure" target="#fig_0">2</ref>   <ref type="bibr" target="#b43">[44]</ref>. We will elaborate on it in Section VII-A.</p><p>• Adversarial Specificity -Targeted attacks misguide deep neural networks to a specific class. Targeted attacks usually occur in the multiclass classification problem. For example, an adversary fools an image classifier to predict all adversarial examples as one class. In a face recognition/biometric system, an adversary tries to disguise a face as an authorized user (Impersonation) <ref type="bibr" target="#b66">[67]</ref>. Targeted attacks usually maximize the probability of targeted adversarial class. -Non-targeted attacks do not assign a specific class to the neural network output. The adversarial class of output can be arbitrary except the original one. For example, an adversary makes his/her face misidentified as an arbitrary face in face recognition system to evade detection (dodging) <ref type="bibr" target="#b66">[67]</ref>. Non-targeted attacks are easier to implement compared to targeted attacks since it has more options and space to redirect the output. Non-targeted adversarial examples are usually generated in two ways: 1) running several targeted attacks and taking the one with the smallest perturbation from the results; 2) minimizing the probability of the correct class. Some generation approaches (e.g., extended BIM, ZOO) can be applied to both targeted and non-targeted attacks.</p><p>For binary classification, targeted attacks are equivalent to non-targeted attacks. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Perturbation</head><p>Small perturbation is a fundamental premise for adversarial examples. Adversarial examples are designed to be close to the original samples and imperceptible to a human, which causes the performance degradation of deep learning models compared to that of a human. We analyze three aspects of perturbation: perturbation scope, perturbation limitation, and perturbation measurement. Adversaries do not require to change the perturbation when the input sample changes.</p><p>• Perturbation Limitation -Optimized Perturbation sets perturbation as the goal of the optimization problem. These methods aim to minimize the perturbation so that humans cannot recognize the perturbation. -Constraint Perturbation sets perturbation as the constraint of the optimization problem. These methods only require the perturbation to be small enough.</p><p>• Perturbation Measurement p measures the magnitude of perturbation by p-norm distance: </p><formula xml:id="formula_3">x p = n i=1 x i p 1 p . (<label>3</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Benchmark</head><p>Adversaries show the performance of their adversarial attacks based on different datasets and victim models. This inconsistency brings obstacles to evaluate the adversarial attacks and measure the robustness of deep learning models. Large and high-quality datasets, complex and high-performance deep learning models usually make adversaries/defenders hard to attack/defend. The diversity of datasets and victim models also makes researchers hard to tell whether the existence of adversarial examples is due to datasets or models. We will discuss this problem in Section VII-C.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>• Datasets</head><p>MNIST, CIFAR-10, and ImageNet are three most widely used image classification datasets to evaluate adversarial attacks. Because MNIST and CIFAR-10 are proved easy to attack and defend due to its simplicity and small size, ImageNet is the best dataset to evaluate adversarial attacks so far. A well-designed dataset is required to evaluate adversarial attacks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>• Victim Models</head><p>Adversaries usually attack several well-known deep learning models, such as LeNet, VGG, AlexNet, GoogLeNet, Caf-feNet, and ResNet.</p><p>In the following sections, we will investigate recent studies on adversarial examples according to this taxonomy.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>IV. METHODS FOR GENERATING ADVERSARIAL EXAMPLES</head><p>In this section, we illustrate several representative approaches for generating adversarial examples. Although many of these approaches are defeated by a countermeasure in later studies, we present these methods to show how the adversarial attacks improved and to what extent state-of-the-art adversarial attacks can achieve. The existence of these methods also requires investigation, which may improve the robustness of deep neural networks.</p><p>Table <ref type="table" target="#tab_5">II</ref> summaries the methods for generating adversarial examples in this section based on the proposed taxonomy. <ref type="bibr" target="#b18">[19]</ref>. They generated adversarial examples using a L-BFGS method to solve the general targeted problem:</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. L-BFGS Attack</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Szegedy et al. first introduced adversarial examples against deep neural networks in 2014</head><formula xml:id="formula_4">min x c η + J θ (x , l ) s.t. x ∈ [0, 1].<label>(4)</label></formula><p>To find a suitable constant c, L-BFGS Attack calculated approximate values of adversarial examples by line-searching c &gt; 0. The authors showed that the generated adversarial examples could also be generalized to different models and different training datasets. They suggested that adversarial examples are never/rarely seen examples in the test datasets.</p><p>L-BFGS Attack was also used in <ref type="bibr" target="#b79">[80]</ref>, which implemented a binary search to find the optimal c.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Fast Gradient Sign Method (FGSM)</head><p>L-BFGS Attack used an expensive linear search method to find the optimal value, which was time-consuming and impractical. Goodfellow et al. proposed a fast method called Fast Gradient Sign Method to generate adversarial examples <ref type="bibr" target="#b68">[69]</ref>. They only performed one step gradient update along the direction of the sign of gradient at each pixel. Their perturbation can be expressed as:</p><formula xml:id="formula_5">η = sign(∇ x J θ (x, l)),<label>(5)</label></formula><p>where is the magnitude of the perturbation. The generated adversarial example x is calculated as: x = x + η. This perturbation can be computed by using back-propagation.</p><p>Figure <ref type="figure" target="#fig_1">1</ref> shows an adversarial example on ImageNet. Adversary's Knowledge White-Box <ref type="bibr" target="#b18">[19]</ref>, <ref type="bibr" target="#b19">[20]</ref>, <ref type="bibr" target="#b67">[68]</ref>- <ref type="bibr" target="#b71">[72]</ref>, <ref type="bibr" target="#b73">[74]</ref>, <ref type="bibr" target="#b75">[76]</ref>, <ref type="bibr" target="#b77">[78]</ref>-[83] Black-Box <ref type="bibr" target="#b72">[73]</ref>, <ref type="bibr" target="#b74">[75]</ref>, <ref type="bibr" target="#b76">[77]</ref>, <ref type="bibr" target="#b81">[82]</ref> Adversarial Specificity Targeted <ref type="bibr" target="#b18">[19]</ref>, <ref type="bibr" target="#b67">[68]</ref>, <ref type="bibr" target="#b69">[70]</ref>, <ref type="bibr" target="#b71">[72]</ref>, <ref type="bibr" target="#b72">[73]</ref>, <ref type="bibr" target="#b74">[75]</ref>, <ref type="bibr" target="#b75">[76]</ref>, <ref type="bibr" target="#b77">[78]</ref>-[82] Non-Targeted <ref type="bibr" target="#b19">[20]</ref>, <ref type="bibr" target="#b68">[69]</ref>, <ref type="bibr" target="#b70">[71]</ref>, <ref type="bibr" target="#b72">[73]</ref>- <ref type="bibr" target="#b74">[75]</ref>, <ref type="bibr" target="#b76">[77]</ref>, <ref type="bibr" target="#b77">[78]</ref>, <ref type="bibr" target="#b81">[82]</ref>, <ref type="bibr" target="#b82">[83]</ref> Attack Frequency One-time <ref type="bibr" target="#b67">[68]</ref>, <ref type="bibr" target="#b68">[69]</ref> Iterative <ref type="bibr" target="#b18">[19]</ref>, <ref type="bibr" target="#b19">[20]</ref>, <ref type="bibr" target="#b69">[70]</ref>- <ref type="bibr" target="#b82">[83]</ref> Perturbation Perturbation Scope Individual <ref type="bibr" target="#b18">[19]</ref>, <ref type="bibr" target="#b19">[20]</ref>, <ref type="bibr" target="#b67">[68]</ref>- <ref type="bibr" target="#b72">[73]</ref>, <ref type="bibr" target="#b74">[75]</ref>- <ref type="bibr" target="#b79">[80]</ref>, <ref type="bibr" target="#b81">[82]</ref>, <ref type="bibr" target="#b82">[83]</ref> Universal <ref type="bibr" target="#b73">[74]</ref> Perturbation Limitation Optimized <ref type="bibr" target="#b18">[19]</ref>, <ref type="bibr" target="#b67">[68]</ref>, <ref type="bibr" target="#b69">[70]</ref>- <ref type="bibr" target="#b73">[74]</ref>, <ref type="bibr" target="#b76">[77]</ref>, <ref type="bibr" target="#b78">[79]</ref>, [80] Constraint <ref type="bibr" target="#b67">[68]</ref>, <ref type="bibr" target="#b74">[75]</ref>, <ref type="bibr" target="#b75">[76]</ref>, <ref type="bibr" target="#b77">[78]</ref>, <ref type="bibr" target="#b81">[82]</ref> None <ref type="bibr" target="#b19">[20]</ref>, <ref type="bibr" target="#b68">[69]</ref>, <ref type="bibr" target="#b80">[81]</ref>, <ref type="bibr" target="#b82">[83]</ref> Perturbation Measurement Element-wise <ref type="bibr" target="#b19">[20]</ref>, <ref type="bibr" target="#b68">[69]</ref>, <ref type="bibr" target="#b80">[81]</ref> p(p ≥ 0)</p><p>0 : <ref type="bibr" target="#b71">[72]</ref>, <ref type="bibr" target="#b74">[75]</ref>,</p><p>1 : [79], 2 : <ref type="bibr" target="#b18">[19]</ref>, <ref type="bibr" target="#b69">[70]</ref>- <ref type="bibr" target="#b73">[74]</ref>, <ref type="bibr" target="#b75">[76]</ref>- <ref type="bibr" target="#b77">[78]</ref>, <ref type="bibr" target="#b79">[80]</ref>, <ref type="bibr" target="#b81">[82]</ref>,</p><p>∞: <ref type="bibr" target="#b70">[71]</ref>, <ref type="bibr" target="#b71">[72]</ref>, <ref type="bibr" target="#b73">[74]</ref>, <ref type="bibr" target="#b78">[79]</ref>, <ref type="bibr" target="#b81">[82]</ref>, PASS</p><p>[68] None <ref type="bibr" target="#b82">[83]</ref> Benchmark Datasets MNIST <ref type="bibr" target="#b18">[19]</ref>, <ref type="bibr" target="#b67">[68]</ref>- <ref type="bibr" target="#b71">[72]</ref>, <ref type="bibr" target="#b76">[77]</ref>, <ref type="bibr" target="#b78">[79]</ref>, <ref type="bibr" target="#b79">[80]</ref>, <ref type="bibr">[</ref>  [68] proposed a new method, called Fast Gradient Value method, in which they replaced the sign of the gradient with the raw gradient: η = ∇ x J(θ, x, l). Fast Gradient Value method has no constraints on each pixel and can generate images with a larger local difference.</p><p>According to <ref type="bibr" target="#b80">[81]</ref>, one-step attack is easy to transfer but also easy to defend (see Section VII-A). <ref type="bibr" target="#b81">[82]</ref> applied momentum to FGSM to generate adversarial examples more iteratively. The gradients were calculated by:</p><formula xml:id="formula_6">g t+1 = µg t + ∇ x J θ (x t , l) ∇ x J θ (x t , l) ,<label>(6)</label></formula><p>then the adversarial example is derived by x t+1 = x t + signg t+1 . The authors increased the effectiveness of attack by introducing momentum and improved the transferability by applying the one-step attack and the ensembling method.</p><p>[81] extended FGSM to a targeted attack by maximizing the probability of the target class:</p><formula xml:id="formula_7">x = x − sign(∇ x J(θ, x, l )). (<label>7</label></formula><formula xml:id="formula_8">)</formula><p>The authors refer to this attack as One-step Target Class Method (OTCM).</p><p>[84] found that FGSM with adversarial training is more robust to white-box attacks than to black-box attacks due to gradient masking. They proposed a new attack, RAND-FGSM, which added random when updating the adversarial examples to defeat adversarial training:</p><formula xml:id="formula_9">x tmp = x + α • sign(N (0 d , I d )), x = x tmp + ( − α) • sign(∇ xtmp J(x tmp , l)),<label>(8)</label></formula><p>where α, are the parameters, α &lt; .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Basic Iterative Method (BIM) and Iterative Least-Likely Class Method (ILLC)</head><p>Previous methods assume adversarial data can be directly fed into deep neural networks. However, in many applications, people can only pass data through devices (e.g., cameras, sensors). <ref type="bibr">Kurakin et al.</ref> applied adversarial examples to the physical world <ref type="bibr" target="#b19">[20]</ref>. They extended Fast Gradient Sign method by running a finer optimization (smaller change) for multiple iterations. In each iteration, they clipped pixel values to avoid large change on each pixel:</p><formula xml:id="formula_10">Clip x,ξ {x } = min{255, x + ξ, max{0, x − , x }}, (9)</formula><p>where Clip x,ξ {x } limits the change of the generated adversarial image in each iteration. The adversarial examples were generated in multiple iterations:</p><formula xml:id="formula_11">x 0 = x, x n+1 = Clip x,ξ {x n + sign(∇ x J(x n , y))}. (<label>10</label></formula><formula xml:id="formula_12">)</formula><p>The authors referred to this method as Basic Iterative method.</p><p>To further attack a specific class, they chose the least-likely class of the prediction and tried to maximize the cross-entropy loss. This method is referred to as Iterative Least-Likely Class method:</p><p>x 0 = x, y LL = arg min y {p(y|x)},</p><formula xml:id="formula_13">x n+1 = Clip x, {x n − sign(∇ x J(x n , y LL ))}.<label>(11)</label></formula><p>They successfully fooled the neural network with a crafted image taken from a cellphone camera. They also found that Fast Gradient Sign method is robust to phototransformation, while iterative methods cannot resist phototransformation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D. Jacobian-based Saliency Map Attack (JSMA)</head><p>Papernot et al. designed an efficient saliency adversarial map, called Jacobian-based Saliency Map Attack <ref type="bibr" target="#b69">[70]</ref>. They first computed Jacobian matrix of given sample x, which is given by:</p><formula xml:id="formula_14">J F (x) = ∂F (x) ∂x = ∂F j (x) ∂x i i×j .<label>(12)</label></formula><p>According to <ref type="bibr" target="#b71">[72]</ref>, F denotes the second-to-last layer (logits) in <ref type="bibr" target="#b69">[70]</ref>. Carlini and Wagner modify this approach by using the output of the softmax layer as F <ref type="bibr" target="#b71">[72]</ref>. In this way, they found the input features of x that made most significant changes to the output. A small perturbation was designed to successfully induce large output variations so that change in a small portion of features could fool the neural network.</p><p>Then the authors defined two adversarial saliency maps to select the feature/pixel to be crafted in each iteration. They achieved 97% adversarial success rate by modifying only 4.02% input features per sample. However, this method runs very slow due to its significant computational cost.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E. DeepFool</head><p>Moosavi-Dezfooli et al. proposed DeepFool to find the closest distance from the original input to the decision boundary of adversarial examples <ref type="bibr" target="#b70">[71]</ref>. To overcome the non-linearity in high dimension, they performed an iterative attack with a linear approximation. Starting from an affine classifier, they found that the minimal perturbation of an affine classifier is the distance to the separating affine hyperplane F = {x : w T x + b = 0} . The perturbation of an affine classifier f can be η * (x) = − f (x) w 2 w. If f is a binary differentiable classifier, they used an iterative method to approximate the perturbation by considering f is linearized around x i at each iteration. The minimal perturbation is computed as:</p><formula xml:id="formula_15">arg min ηi η i 2 s.t. f (x i ) + ∇f (x i ) T η i = 0. (<label>13</label></formula><formula xml:id="formula_16">)</formula><p>Figure <ref type="figure" target="#fig_0">2</ref>: Unrecognizable examples to humans, but deep neural networks classify them to a class with high certainty (≥ 99.6%) <ref type="bibr" target="#b82">[83]</ref> This result can also be extended to the multi-class classifier by finding the closest hyperplanes. It can also be extended to a more general p norm, p ∈ [0, ∞). DeepFool provided less perturbation compared to FGSM and JSMA did. Compared to JSMA, DeepFool also reduced the intensity of perturbation instead of the number of selected features.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>F. CPPN EA Fool</head><p>Nguyen et al. discovered a new type of attack, compositional pattern-producing network-encoded EA (CPPN EA), where adversarial examples are classified by deep neural networks with high confidence (99%), which is unrecognizable to human <ref type="bibr" target="#b82">[83]</ref>. We categorize this kind of attack as a False positive attack. Figure <ref type="figure" target="#fig_0">2</ref> illustrates false-positive adversarial examples.</p><p>They used evolutionary algorithms (EAs) algorithm to produce the adversarial examples. To solve multi-class classification problem using EA algorithms, they applied multidimensional archive of phenotypic elites MAP-Elites <ref type="bibr" target="#b84">[85]</ref>. The authors first encoded images with two different methods: direct encoding (grayscale or HSV value) and indirect encoding (compositional pattern-producing network). Then in each iteration, MAP-Elites, like general EA algorithm, chose a random organism, mutated them randomly, and replaced with the current ones if the new ones have higher fitness (high certainty for a class of a neural network). In this way, MAP-Elites can find the best individual for each class. As they claimed, for many adversarial images, CPPN could locate the critical features to change outputs of deep neural networks just like JSMA did. Many images from same evolutionary are found similar on closely related categories. More interestingly, CPPN EA fooling images are accepted by an art contest with 35.5% acceptance rate.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>G. C&amp;W's Attack</head><p>Carlini and Wagner launched a targeted attack to defeat Defensive distillation (Section VI-A) <ref type="bibr" target="#b71">[72]</ref>. According to their further study <ref type="bibr" target="#b85">[86]</ref>, <ref type="bibr" target="#b86">[87]</ref>, C&amp;W's Attack is effective for most of existing adversarial detecting defenses. The authors made several modifications in Equation <ref type="formula" target="#formula_2">2</ref>.</p><p>They first defined a new objective function g, so that:</p><formula xml:id="formula_17">min η η p + c • g(x + η) s.t. x + η ∈ [0, 1] n ,<label>(14)</label></formula><p>where g(x ) ≥ 0 if and only if f (x ) = l . In this way, the distance and the penalty term can be better optimized. The authors listed seven objective function candidates g. One of the effective functions evaluated by their experiments can be:</p><formula xml:id="formula_18">g(x ) = max(max i =l (Z(x ) i ) − Z(x ) t , −κ),<label>(15)</label></formula><p>where Z denotes the Softmax function, κ is a constant to control the confidence (κ is set to 0 in <ref type="bibr" target="#b71">[72]</ref>). Second, instead of using box-constrained L-BFGS to find minimal perturbation in L-BFGS Attack method, the authors introduced a new variant w to avoid the box constraint, where w satisfies η = 1 2 (tanh(w)+1)−x. General optimizers in deep learning like Adam and SGD were used to generate adversarial examples and performed 20 iterations of such generation to find an optimal c by binary searching. However, they found that if the gradients of η p and g(x + η) are not in the same scale, it is hard to find a suitable constant c in all of the iterations of the gradient search and get the optimal result. Due to this reason, two of their proposed functions did not find optimal solutions for adversarial examples.</p><p>Third, three distance measurements of perturbation were discussed in the paper: 0 , 2 , and ∞ . The authors provided three kinds of attacks based on the distance metrics: 0 attack, 2 attack, and ∞ attack.</p><p>2 attack can be described by:</p><formula xml:id="formula_19">min w 1 2 (tanh(w) + 1) 2 + c • g( 1 2 tanh(w) + 1).<label>(16)</label></formula><p>The authors showed that the distillation network could not help defend 2 attack.</p><p>0 attack was conducted iteratively since 0 is not differentiable. In each iteration, a few pixels are considered trivial for generating adversarial examples and removed. The importance of pixels is determined by the gradient of 2 distance. The iteration stops if the remaining pixels can not generate an adversarial example.</p><p>∞ attack was also an iterative attack, which replaced the 2 term with a new penalty in each iteration:</p><formula xml:id="formula_20">min c • g(x + η) + i [(η i − τ ) + ].<label>(17)</label></formula><p>For each iteration, they reduced τ by a factor of 0.9, if all η i &lt; τ . ∞ attack considered τ as an estimation of ∞ .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>H. Zeroth Order Optimization (ZOO)</head><p>Different from gradient-based adversarial generating approaches, Chen et al. proposed a Zeroth Order Optimization (ZOO) based attack <ref type="bibr" target="#b72">[73]</ref>. Since this attack does not require gradients, it can be directly deployed in a black-box attack without model transferring. Inspired by <ref type="bibr" target="#b71">[72]</ref>, the authors modified g(•) in <ref type="bibr" target="#b71">[72]</ref> as a new hinge-like loss function: <ref type="bibr" target="#b17">(18)</ref> and used symmetric difference quotient to estimate the gradient and Hessian: where e i denotes the standard basis vector with the ith component as 1, h is a small constant. Through employing the gradient estimation of gradient and Hessian, ZOO does not need the access to the victim deep learning models. However, it requires expensive computation to query and estimate the gradients. The authors proposed ADAM like algorithms, ZOO-ADAM, to randomly select a variable and update adversarial examples. Experiments showed that ZOO achieved the comparable performance as C&amp;W's Attack.</p><formula xml:id="formula_21">g(x ) = max(max i =l (log[f (x)] i ) − log[f (x)] l , −κ),</formula><formula xml:id="formula_22">∂f (x) ∂x i ≈ f (x + he i ) − f (x − he i ) 2h , ∂ 2 f (x) ∂x 2 i ≈ f (x + he i ) − 2f (x) + f (x − he i ) h 2 ,<label>(19)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>I. Universal Perturbation</head><p>Leveraging their previous method on DeepFool, Moosavi-Dezfooli et al. developed a universal adversarial attack <ref type="bibr" target="#b73">[74]</ref>. The problem they formulated is to find a universal perturbation vector satisfying</p><formula xml:id="formula_23">η p ≤ , P x = f (x) ≥ 1 − δ. (<label>20</label></formula><formula xml:id="formula_24">)</formula><p>limits the size of universal perturbation, and δ controls the failure rate of all the adversarial samples.</p><p>For each iteration, they use DeepFool method to get a minimal sample perturbation against each input data and update the perturbation to the total perturbation η. This loop will not stop until most data samples are fooled (P &lt; 1 − δ). From experiments, the universal perturbation can be generated by using a small part of data samples instead of the entire dataset. Figure <ref type="figure" target="#fig_2">3</ref> illustrates a universal adversarial example can fool a group of images. The universal perturbations were shown to be generalized well across popular deep learning architectures (e.g., VGG, CaffeNet, GoogLeNet, ResNet).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>J. One Pixel Attack</head><p>To avoid the problem of measurement of perceptiveness, Su et al. generated adversarial examples by only modifying one pixel <ref type="bibr" target="#b74">[75]</ref>. The optimization problem becomes:</p><formula xml:id="formula_25">min x J(f (x ), l ) s.t. η 0 ≤ 0 ,<label>(21)</label></formula><p>where 0 = 1 for modifying only one pixel. The new constraint made it hard to optimize the problem. Su et al. applied differential evolution (DE), one of the evolutionary algorithms, to find the optimal solution. DE does not require the gradients of the neural networks and can be used in non-differential objective functions. They evaluated the proposed method on the CIFAR-10 dataset using three neural networks: All convolution network (AllConv) <ref type="bibr" target="#b87">[88]</ref>, Network in Network (NiN) <ref type="bibr" target="#b88">[89]</ref>, and VGG16. Their results showed that 70.97% of images successfully fooled deep neural networks with at least one target class with confidence 97.47% on average.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>K. Feature Adversary</head><p>Sabour et al. performed a targeted attack by minimizing the distance of the representation of internal neural network layers instead of the output layer <ref type="bibr" target="#b75">[76]</ref>. We refer to this attack as Feature Adversary. The problem can be described by:</p><formula xml:id="formula_26">min x φ k (x) − φ k (x ) s.t. x − x ∞ &lt; δ,<label>(22)</label></formula><p>where φ k denotes a mapping from image input to the output of the kth layer. Instead of finding a minimal perturbation, δ is used as a constraint of perturbation. They claimed that a small fixed value δ is good enough for human perception.</p><p>Similar to <ref type="bibr" target="#b18">[19]</ref>, they used L-BFGS-B to solve the optimization problem. The adversarial images are more natural and closer to the targeted images in the internal layers.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>L. Hot/Cold</head><p>Rozsa et al. proposed a Hot/Cold method to find multiple adversarial examples for every single image input <ref type="bibr" target="#b67">[68]</ref>. They thought small translations and rotations should be allowed as long as they were imperceptible.</p><p>They defined a new metric, Psychometric Perceptual Adversarial Similarity Score (PASS), to measure the noticeable similarity to humans. Hot/Cold neglected the unnoticeable difference based on pixels and replaced widely used p distance with PASS. PASS includes two stages: 1) aligning the modified image with the original image; 2) measuring the similarity between the aligned image and the original one.</p><p>Let φ(x , x) be a homography transform from the adversarial example x to the original example x. H is the homography matrix, with size 3 × 3. H is solved by maximizing the enhanced correlation coefficient (ECC) <ref type="bibr" target="#b89">[90]</ref> between x and x. The optimization function is:</p><formula xml:id="formula_27">arg min H x x − φ(x , x) φ(x , x) ,<label>(23)</label></formula><p>where • denotes the normalization of an image.</p><p>Structural SIMilarity (SSIM) index <ref type="bibr" target="#b90">[91]</ref> was adopted to measure the just noticeable difference of images. <ref type="bibr" target="#b67">[68]</ref> leveraged SSIM and defined a new measurement, regional SSIM index (RSSIM) as:</p><formula xml:id="formula_28">RSSIM (x i,j , x i,j ) = L(x i,j , x i,j ) α C(x i,j , x i,j ) β S(x i,j , x i,j ) γ ,</formula><p>where α, β, γ are weights of importance for luminance (L(•, •)), contrast (C(•, •)), and structure (S(•, •)). The SSIM can be calculated by averaging RSSIM:</p><formula xml:id="formula_29">SSIM (x i,j , x i,j ) = 1 n × m i,j RSSIM (x i,j , x i,j ).</formula><p>PASS is defined by combination of the alignment and the similarity measurement:</p><formula xml:id="formula_30">P ASS(x , x) = SSIM (φ * (x , x), x). (<label>24</label></formula><formula xml:id="formula_31">)</formula><p>The adversarial problem with the new distance is described as:</p><formula xml:id="formula_32">min D(x, x ) s.t. f (x ) = y , P ASS(x, x ) ≥ γ.<label>(25)</label></formula><p>D(x, x ) denotes a measure of distance (e.g., 1−P ASS(x, x ) or x − x p ). To generate a diverse set of adversarial examples, the authors defined the targeted label l as hot class, and the original label l as cold class. In each iteration, they moved toward a target (hot) class while moving away from the original (cold) class. Their results showed that generated adversarial examples are comparable to FGSM, and with more diversity.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>M. Natural GAN</head><p>Zhao et al. utilized Generative Adversarial Networks (GANs) as part of their approach to generate adversarial examples of images and texts <ref type="bibr" target="#b76">[77]</ref>, which made adversarial examples more natural to human. We name this approach Natural GAN. The authors first trained a WGAN model on the dataset, where the generator G maps random noise to the input domain. They also trained an "inverter" I to map input data to dense inner representations. Hence, the adversarial noise was generated by minimizing the distance of the inner representations like "Feature Adversary." The adversarial examples were generated using the generator: x = G(z ):</p><formula xml:id="formula_33">min z z − I(x) s.t. f (G(z)) = f (x).<label>(26)</label></formula><p>Both the generator G and the inverter I were built to make adversarial examples natural. Natural GAN was a general framework for many deep learning fields. <ref type="bibr" target="#b76">[77]</ref> applied Natural GAN to image classification, textual entailment, and machine translation. Since Natural GAN does not require gradients of original neural networks, it can also be applied to Black-box Attack. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>N. Model-based Ensembling Attack</head><formula xml:id="formula_34">α i J i (x , l )) + λ x − x , (<label>27</label></formula><formula xml:id="formula_35">)</formula><p>where k is the number of deep neural networks in the generation, f i is the function of each network, and α i is the ensemble weight ( k i α i = 1). The results showed that Model-based Ensembling Attack could generate transferable targeted adversarial images, which enhanced the power of adversarial examples for black-box attacks. They also proved that this method performs better in generating non-targeted adversarial examples than previous methods. The authors successfully conducted a black-box attack against Clarifai.com using Model-based Ensembling Attack.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>O. Ground-Truth Attack</head><p>Formal verification techniques aim to evaluate the robustness of a neural network even against zero-day attacks (Section VI-F). Carlini et al. constructed a ground-truth attack, which provided adversarial examples with minimal perturbation ( 1 , ∞ ) <ref type="bibr" target="#b78">[79]</ref>. Network Verification always checks whether an adversarial example violates a property of a deep neural network and whether there exists an example changes the label within a certain distance. Ground-Truth Attack conducted a binary search and found such an adversarial example with the smallest perturbation by invoking Reluplex <ref type="bibr" target="#b91">[92]</ref> iteratively. The initial adversarial example is found using C&amp;W's Attack <ref type="bibr" target="#b71">[72]</ref> to improve the performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>V. APPLICATIONS FOR ADVERSARIAL EXAMPLES</head><p>We have investigated adversarial examples for image classification task. In this section, we review adversarial examples against the other tasks. We mainly focus on three questions: What scenarios are adversarial examples applied in new tasks? How to generate adversarial examples in new tasks? Whether to propose a new method or to translate the problem into the image classification task and solve it by the aforementioned methods? Table <ref type="table" target="#tab_7">III</ref> summarizes the applications for adversarial examples in this section.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Reinforcement Learning</head><p>Deep neural networks have been used in reinforcement learning by training policies on raw input (e.g., images). <ref type="bibr" target="#b92">[93]</ref>, <ref type="bibr" target="#b93">[94]</ref> generated adversarial examples on deep reinforcement learning policies. Since the inherent intensive computation of reinforcement learning, both of them performed fast One-time attack.</p><p>Huang et al. applied FGSM to attack deep reinforcement learning networks and algorithms <ref type="bibr" target="#b92">[93]</ref>: deep Q network (DQN), trust region policy optimization(TRPO), and asynchronous advantage actor-critic (A3C) <ref type="bibr" target="#b105">[106]</ref>. Similarly to <ref type="bibr" target="#b68">[69]</ref>, they added small perturbations on the input of policy by calculating the gradient of the cross-entropy loss function: [94] used FGSM to attack A3C algorithm and Atari Pong task. <ref type="bibr" target="#b93">[94]</ref> found that injecting perturbations in a fraction of frames is sufficient.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Generative Modeling</head><p>Kos et al. <ref type="bibr" target="#b94">[95]</ref> and Tabacof et al. <ref type="bibr" target="#b95">[96]</ref> proposed adversarial examples for generative models. An adversary for autoencoder can inject perturbations into the input of encoder and generate a targeted class after decoding. Figure <ref type="figure" target="#fig_4">4</ref> depicts a targeted adversarial example for an autoencoder. Adding perturbations on the input image of the encoder can misguide the autoencoder by making decoder to generating a targeted adversarial output image. Kos et al. described a scenario to apply adversarial examples against autoencoder. Autoencoders can be used to compress data by an encoder and decompress by a decoder. For example, Toderici et al. use RNN-based AutoEncoder to compress image <ref type="bibr" target="#b106">[107]</ref>. Ledig et al. used GAN to superresolve images <ref type="bibr" target="#b107">[108]</ref>. Adversaries can leverage autoencoder to reconstruct an adversarial image by adding perturbation to the input of the encoder.</p><p>Tabacof et al. used Feature Adversary Attack against AE and VAE. The adversarial examples were formulated as follows <ref type="bibr" target="#b95">[96]</ref>:</p><formula xml:id="formula_36">min η D(z x , z x ) + c η s.t. x ∈ [L, U ] z x = Encoder(x ) z x = Encoder(x),<label>(28)</label></formula><p>where D(•) is the distance between latent encoding representation z x and z x . Tabacof et al. chose KL-divergence to measure D(•) in <ref type="bibr" target="#b95">[96]</ref>. They tested their attacks on the MNIST and SVHN dataset and found that generating adversarial examples for autoencoder is much harder than for classifiers. VAE is even slightly more robust than deterministic autoencoder.  (</p><formula xml:id="formula_37">)<label>29</label></formula><p>The loss function J can be cross-entropy (refer to "Classifier Attack" in <ref type="bibr" target="#b94">[95]</ref>), VAE loss function ("L V AE Attck"), and distance between the original latent vector z and modified encoded vector x ("Latent Attack", similar to Tabacof et al.'s work <ref type="bibr" target="#b95">[96]</ref>). They tested VAE and VAE-GAN <ref type="bibr" target="#b108">[109]</ref> on the MNIST, SVHN, and CelebA datasets. In their experimental results, "Latent Attack" achieved the best result.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Face Recognition</head><p>Deep neural network based Face Recognition System (FRS) and Face Detection System have been widely deployed in commercial products due to their high performance. <ref type="bibr" target="#b66">[67]</ref> first provided a design of eyeglass frames to attack a deep neural network based FRS <ref type="bibr" target="#b109">[110]</ref>, which composes 11 blocks with 38 layers and one triplet loss function for feature embedding. Based on the triplet loss function, <ref type="bibr" target="#b66">[67]</ref> designed a softmaxloss function:</p><formula xml:id="formula_38">J(x) = − log e &lt;hc x ,f (x)&gt; m c=1 e &lt;hc,f (x)&gt; ,<label>(30)</label></formula><p>where h c is a one-hot vector of class c, &lt; •, • &gt; denotes inner product. Then they used L-BFGS Attack to generate adversarial examples.</p><p>In a further step, <ref type="bibr" target="#b66">[67]</ref> implemented adversarial eyeglass frames to achieve attack in the physical world: the perturbations can only be injected into the area of eyeglass frames. They also enhanced the printability of adversarial images on the frame by adding a penalty of non-printability score (NPS) to the optimized objective. Similarly to Universal Perturbation, they optimize the perturbation to be applied to a set of face images. They successfully dodged (non-targeted attack) against FRS (over 80 % time) and misguided FRS as a specific face (targeted attack) with a high success rate (depending on the target). Figure <ref type="figure" target="#fig_5">5</ref> illustrates an example of adversarial eyeglass frames.</p><p>Leveraging the approach of printability, <ref type="bibr" target="#b20">[21]</ref> proposed an attack algorithm, Robust Physical Perturbations (RP 2 ), to modify a stop sign as a speed limit sign) <ref type="foot" target="#foot_7">8</ref> . They changed the physical road signs by two kinds of attacks: 1) overlaying an adversarial road sign over a physical sign; 2) sticking perturbations on an existing sign. <ref type="bibr" target="#b20">[21]</ref> included a non-printability score in the optimization objective to improve the printability. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D. Object Detection</head><p>The object detection task is to find the proposal of an object (bounding box), which can be viewed as an image classification task for every possible proposal. <ref type="bibr" target="#b21">[22]</ref> proposed a universal algorithm called Dense Adversary Generation (DAG) to generate adversarial examples for both object detection and semantic segmentation. The authors aimed at making the prediction (detection/segmentation) incorrect (non-targeted). Figure <ref type="figure" target="#fig_6">6</ref> illustrates an adversarial example for the object detection task.</p><p>[22] defined T = t 1 , t 2 , . . . , t N as the recognition targets. For image classification, the classifier only needs one target -entire image (N = 1); For semantic segmentation, targets consist of all pixels (N = #of pixels); For object detection, targets consist of all possible proposals (N = (#of pixels) 2 ). Then the objective function sums up the loss from all targets. Instead of optimizing the loss from all targets, the authors performed an iterative optimization and only updated the loss for the targets correctly predicted in the previous iteration. The final perturbation sums up normalized perturbations in all iterations. To deal with a large number of targets for objective detection problem, the authors used regional proposal network (RPN) <ref type="bibr" target="#b3">[4]</ref> to generate possible targets, which greatly decreases the computation for targets in object detection. DAG also showed the capability of generating images which are unrecognizable to human but deep learning could predict (false positives).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E. Semantic Segmentation</head><p>Image segmentation task can be viewed as an image classification task for every pixel. Since each perturbation is responsible for at least one pixel segmentation, this makes the space of perturbations for segmentation much smaller than that for image classification <ref type="bibr" target="#b111">[112]</ref>. <ref type="bibr" target="#b21">[22]</ref>, <ref type="bibr" target="#b96">[97]</ref>, <ref type="bibr" target="#b111">[112]</ref> generated adversarial examples against the semantic image segmentation task. However, their attacks are proposed under different scenarios. As we just discussed, <ref type="bibr" target="#b21">[22]</ref> performed a non-targeted segmentation. <ref type="bibr" target="#b96">[97]</ref>, <ref type="bibr" target="#b111">[112]</ref> both performed a targeted segmentation and tried to removed a certain class by making deep learning model to misguide it as background classes.</p><p>[97] generated adversarial examples by assigning pixels with the adversarial class that their nearest neighbor belongs to. The success rate was measured by the percentage of pixels of chosen class to be changed or of the rest classes to be preserved. <ref type="bibr" target="#b111">[112]</ref> presented a method to generate universal adversarial perturbations against semantic image segmentation task. They assigned the primary objective of adversarial examples and Targeted classes are classes to be removed. Similar to <ref type="bibr" target="#b96">[97]</ref>, pixels which belong to the targeted classes would be assigned to their nearest background classes:</p><formula xml:id="formula_39">l target ij = l pred i j ∀(i, j) ∈ I targeted , l target ij = l pred ij ∀(i, j) ∈ I background , (i , j ) = arg min (i ,j )∈I background i − i + j − j ,<label>(31)</label></formula><p>where I targeted = (i, j)|f (x ij ) = l * denotes the area to be removed. Figure <ref type="figure" target="#fig_7">7</ref> illustrates an adversarial example to hide pedestrians. They used ILLC attack to solve this problem and also extended Universal Perturbation method to get the universal perturbation. Their results showed the existence of universal perturbation for semantic segmentation task.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>F. Natural Language Processing (NLP)</head><p>Many tasks in natural language processing can be attacked by adversarial examples. People usually generate adversarial examples by adding/deleting words in the sentences.</p><p>The task of reading comprehension (a.k.a. question answering) is to read paragraphs and answer questions about the paragraphs. To generate adversarial examples that are consistent with the correct answer and do not confuse human, Jia and Liang added distracting (adversarial) sentences to the end of paragraph <ref type="bibr" target="#b98">[99]</ref>. They found that models for the reading comprehension task are overstable instead of oversensitivity, which means deep learning models cannot tell the subtle but critical difference in the paragraphs.</p><p>They proposed two kinds of methods to generate adversarial examples: 1) adding grammatical sentences similar to the question but not contradictory to the correct answer (AddSent); 2) adding a sentence with arbitrary English words (AddAny). <ref type="bibr" target="#b98">[99]</ref> successfully fooled all the models (sixteen models) they tested on Stanford Question Answering Dataset (SQuAD) <ref type="bibr" target="#b112">[113]</ref>. The adversarial examples also have the capability of transferability and cannot be improved by adversarial training. However, the adversarial sentences require manpower to fix the errors in the sentences.</p><p>[100] aimed to fool a deep learning-based sentiment classifier by removing the minimum subset of words in the given text. Reinforcement learning was used to find an approximate subset, where the reward function was proposed as 1 D when the sentiment label changes, and 0 otherwise. D denotes the number of removing word set D. The reward function also included a regularizer to make sentence contiguous.  <ref type="bibr" target="#b118">[119]</ref>, <ref type="bibr" target="#b121">[122]</ref>, <ref type="bibr" target="#b122">[123]</ref> Network Verification <ref type="bibr" target="#b91">[92]</ref>, <ref type="bibr" target="#b123">[124]</ref>, <ref type="bibr" target="#b124">[125]</ref> Proactive Network Distillation <ref type="bibr" target="#b125">[126]</ref> Adversarial (Re)Training <ref type="bibr" target="#b34">[35]</ref>, <ref type="bibr" target="#b35">[36]</ref>, <ref type="bibr" target="#b68">[69]</ref>, <ref type="bibr" target="#b80">[81]</ref>, <ref type="bibr" target="#b83">[84]</ref>,</p><p>[127] Classifier Robustifying <ref type="bibr" target="#b127">[128]</ref>, <ref type="bibr" target="#b128">[129]</ref> The changes in <ref type="bibr" target="#b98">[99]</ref>, <ref type="bibr" target="#b99">[100]</ref> can easily be recognized by humans. More natural adversarial examples for texture data was proposed by Natural GAN <ref type="bibr" target="#b76">[77]</ref> (Section IV-M).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>G. Malware Detection</head><p>Deep learning has been used in static and behavioral-based malware detection due to its capability of detecting zeroday malware <ref type="bibr" target="#b13">[14]</ref>- <ref type="bibr" target="#b16">[17]</ref>. Recent studies generated adversarial malware samples to evade deep learning-based malware detection <ref type="bibr" target="#b100">[101]</ref>- <ref type="bibr" target="#b102">[103]</ref>, <ref type="bibr" target="#b104">[105]</ref>.</p><p>[101] adapted JSMA method to attack Android malware detection model. <ref type="bibr" target="#b104">[105]</ref> evaded two PDF malware classifier, PDFrate and Hidost, by modifying PDF. <ref type="bibr" target="#b104">[105]</ref> parsed the PDF file and changed its object structure using genetic programming. The adversarial PDF file was then packed with new objects.</p><p>[104] used GAN to generate adversarial domain names to evade detection of domain generation algorithms. <ref type="bibr" target="#b102">[103]</ref> proposed a GAN based algorithm, MalGan, to generate malware examples and evade black-box detection. <ref type="bibr" target="#b102">[103]</ref> used a substitute detector to simulate the real detector and leveraged the transferability of adversarial examples to attack the real detector. MalGan was evaluated by 180K programs with API features. However, <ref type="bibr" target="#b102">[103]</ref> required the knowledge of features used in the model. <ref type="bibr" target="#b101">[102]</ref> used a large number of features <ref type="bibr" target="#b1">(2,</ref><ref type="bibr">350)</ref> to cover the required feature space of portable executable (PE) files. The features included PE header metadata, section metadata, import &amp; export table metadata.</p><p>[102] also defined several modifications to generate malware evading deep learning detection. The solution was trained by reinforcement learning, where the evasion rate is considered as a reward.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>VI. COUNTERMEASURES FOR ADVERSARIAL EXAMPLES</head><p>Countermeasures for adversarial examples have two types of defense strategies: 1) reactive: detect adversarial examples after deep neural networks are built; 2) proactive: make deep neural networks more robust before adversaries generate adversarial examples. In this section, we discuss three reactive countermeasures (Adversarial Detecting, Input Reconstruction, and Network Verification) and three proactive countermeasures (Network Distillation, Adversarial (Re)training, and Classifier Robustifying). We will also discuss an ensembling method to prevent adversarial examples. Table <ref type="table" target="#tab_8">IV</ref> summarizes the countermeasures.  <ref type="bibr" target="#b125">[126]</ref>. Network distillation was originally designed to reduce the size of deep neural networks by transferring knowledge from a large network to a small one <ref type="bibr" target="#b129">[130]</ref>, <ref type="bibr" target="#b130">[131]</ref> (Figure <ref type="figure" target="#fig_8">8</ref>). The probability of classes produced by the first DNN is used as inputs to train the second DNN. The probability of classes extracts the knowledge learned from the first DNN. Softmax is usually used to normalize the last layer of DNN and produce the probability of classes. The softmax output of the first DNN, also the input of the next DNN, can be described as:</p><formula xml:id="formula_40">q i = exp(z i /T ) j exp(z j /T ) , (<label>32</label></formula><formula xml:id="formula_41">)</formula><p>where T is a temperature parameter to control the level of knowledge distillation. In deep neural networks, temperature T is set to 1. When T is large, the output of softmax will be vague (when T → ∞, the probability of all classes → 1 m ). When T is small, only one class is close to 1 while the rest goes to 0. This schema of network distillation can be duplicated several times and connects several deep neural networks.</p><p>In <ref type="bibr" target="#b125">[126]</ref>, network distillation extracted knowledge from deep neural networks to improve robustness. The authors found that attacks primarily aimed at the sensitivity of networks and then proved that using high-temperature softmax reduced the model sensitivity to small perturbations. Network Distillation defense was tested on the MNIST and CIFAR-10 datasets and reduced the success rate of JSMA attack by 0.5% and 5% respectively. "Network Distillation" also improved the generalization of the neural networks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Adversarial (Re)training</head><p>Training with adversarial examples is one of the countermeasures to make neural networks more robust. Goodfellow et al. <ref type="bibr" target="#b68">[69]</ref> and Huang et al. <ref type="bibr" target="#b126">[127]</ref> included adversarial examples in the training stage. They generated adversarial examples in every step of training and inject them into the training set. <ref type="bibr" target="#b68">[69]</ref>, <ref type="bibr" target="#b126">[127]</ref> showed that adversarial training improved the robustness of deep neural networks. Adversarial training could provide regularization for deep neural networks <ref type="bibr" target="#b68">[69]</ref> and improve the precision as well <ref type="bibr" target="#b34">[35]</ref>.</p><p>[69] and <ref type="bibr" target="#b126">[127]</ref> were evaluated only on the MNIST dataset. A comprehensive analysis of adversarial training methods on the ImageNet dataset was presented in <ref type="bibr" target="#b80">[81]</ref>. They used half adversarial examples and half origin examples in each step of training. From the results, adversarial training increased the robustness of neural networks for one-step attacks (e.g., FGSM) but would not help under iterative attacks (e.g., BIM and ILLC methods). <ref type="bibr" target="#b80">[81]</ref> suggested that adversarial training is used for regularization only to avoid overfitting (e.g., the case in <ref type="bibr" target="#b68">[69]</ref> with the small MNIST dataset).</p><p>[84] found that the adversarial trained models on the MNIST and ImageNet datasets are more robust to white-box adversarial examples than to the transferred examples (blackbox).</p><p>[36] minimized both the cross-entropy loss and internal representation distance during adversarial training, which can be seen as a defense version of Feature Adversary.</p><p>To deal with the transferred black-box model, <ref type="bibr" target="#b83">[84]</ref> proposed Ensembling Adversarial Training method that trained the model with adversarial examples generated from multiple sources: the models being trained and also pre-trained external models.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Adversarial Detecting</head><p>Many research projects tried to detect adversarial examples in the testing stage <ref type="bibr" target="#b33">[34]</ref>, <ref type="bibr" target="#b97">[98]</ref>, <ref type="bibr" target="#b113">[114]</ref>- <ref type="bibr" target="#b118">[119]</ref>, <ref type="bibr" target="#b120">[121]</ref>.</p><p>[34], <ref type="bibr" target="#b97">[98]</ref>, <ref type="bibr" target="#b115">[116]</ref> trained deep neural network-based binary classifiers as detectors to classify the input data as a legitimate (clean) input or an adversarial example. Metzen et al. created a detector for adversarial examples as an auxiliary network of the original neural network <ref type="bibr" target="#b97">[98]</ref>. The detector is a small and straightforward neural network predicting on binary classification, i.e., the probability of the input being adversarial. SafetyNet <ref type="bibr" target="#b33">[34]</ref> extracted the binary threshold of each ReLU layer's output as the features of the adversarial detector and detects adversarial images by an RBF-SVM classifier. The authors claimed that their method is hard to be defeated by adversaries even when adversaries know the detector, since it is difficult for adversaries to find an optimal value, for both adversarial examples and new features of SafetyNet detector. <ref type="bibr" target="#b116">[117]</ref> added an outlier class to the original deep learning model. The model detected the adversarial examples by classifying it as an outlier. They found that the measurement of maximum mean discrepancy (MMD) and energy distance (ED) could distinguish the distribution of adversarial datasets and clean datasets.</p><p>[115] provided a Bayesian view of detecting adversarial examples. <ref type="bibr" target="#b114">[115]</ref> claimed that the uncertainty of adversarial examples is higher than the clean data. Hence, they deployed a Bayesian neural network to estimate the uncertainty of input data and distinguish adversarial examples and clean input data based on uncertainty estimation.</p><p>Similarly, <ref type="bibr" target="#b118">[119]</ref> used probability divergence (Jensen-Shannon divergence) as one of its detectors. <ref type="bibr" target="#b117">[118]</ref> showed that after whitening by Principal Component Analysis (PCA), adversarial examples have different coefficients in low-ranked components.</p><p>[123] trained a PixelCNN neural network <ref type="bibr" target="#b131">[132]</ref> and found that the distribution of adversarial examples is different from clean data. They calculated p-value based on the rank of PixelCNN and rejected adversarial examples using the pvalues. The results showed that this approach could detect FGSM, BIM, DeepFool, and C&amp;W attack.</p><p>[120] trained neural networks with "reverse cross-entropy" to better distinguish adversarial examples from clean data in the latent layers and then detected adversarial examples using a method called "Kernel density" in the testing stage. The "reverse cross-entropy" made the deep neural network to predict with high confidence on the true class and uniform distribution on the other classes. In this way, the deep neural network was trained to map the clean input close to a low-dimensional manifold in the layer before softmax. This brought great convenience for further detection of adversarial examples.</p><p>[121] leveraged multiple previous images to predict future input and detect adversarial examples, in the task of reinforcement learning.</p><p>However, Carlini and Wagner summarized most of these adversarial detecting methods ( <ref type="bibr" target="#b97">[98]</ref>, <ref type="bibr" target="#b113">[114]</ref>- <ref type="bibr" target="#b117">[118]</ref>) and showed that these methods could not defend against their previous attack C&amp;W's Attack with slight changes of loss function <ref type="bibr" target="#b85">[86]</ref>, <ref type="bibr" target="#b86">[87]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D. Input Reconstruction</head><p>Adversarial examples can be transformed to clean data via reconstruction. After transformation, the adversarial examples will not affect the prediction of deep learning models. Gu and Rigazio proposed a variant of autoencoder network with a penalty, called deep contractive autoencoder, to increase the robustness of neural networks <ref type="bibr" target="#b121">[122]</ref>. A denoising autoencoder network is trained to encode adversarial examples to original ones to remove adversarial perturbations. <ref type="bibr" target="#b118">[119]</ref> reconstructed the adversarial examples by 1) adding Gaussian noise or 2) encoding them with autoencoder as a plan B in MagNet <ref type="bibr" target="#b118">[119]</ref>(Section VI-G).</p><p>PixelDefend reconstructed the adversarial images back to the training distribution <ref type="bibr" target="#b122">[123]</ref> using PixelCNN. PixelDefend changed all pixels along each channel to maximize the probability distribution:</p><formula xml:id="formula_42">max x P t (x ) s.t. x − x ∞ ≤ def end ,<label>(33)</label></formula><p>where P t denotes the training distribution, def end controls the new changes on the adversarial examples. PixelDefend also leveraged adversarial detecting, so that if an adversarial example is not detected as malicious, no change will be made to the adversarial examples ( def end = 0).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E. Classifier Robustifying</head><p>[128], <ref type="bibr" target="#b128">[129]</ref> design robust architectures of deep neural networks to prevent adversarial examples.</p><p>Due to the uncertainty from adversarial examples, Bradshaw et al. leveraged Bayesian classifiers to build more robust neural networks <ref type="bibr" target="#b127">[128]</ref>. Gaussian processes (GPs) with RBF kernels were used to provide uncertainty estimation. The proposed neural networks were called Gaussian Process Hybrid Deep Neural Networks (GPDNNs). GPs expressed the latent variables as a Gaussian distribution parameterized by the functions of mean and covariance and encoded them with RBF kernels. <ref type="bibr" target="#b127">[128]</ref> showed that GPDNNs achieved comparable performance with general DNNs and more robust to adversarial examples. The authors claimed that GPDNNs "know when they do not know." <ref type="bibr" target="#b128">[129]</ref> observed that adversarial examples usually went into a small subset of incorrect classes. <ref type="bibr" target="#b128">[129]</ref> separated the classes into sub-classes and ensembled the result from all sub-classes by voting to prevent adversarial examples misclassified.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>F. Network Verification</head><p>Verifying properties of deep neural networks is a promising solution to defend adversarial examples, because it may detect the new unseen attacks. Network verification checks the properties of a neural network: whether an input violates or satisfies the property.</p><p>Katz et al. proposed a verification method for neural networks with ReLU activation function, called Reluplex <ref type="bibr" target="#b91">[92]</ref>. They used Satisfiability Modulo Theory (SMT) solver to verify the neural networks. The authors showed that within a small perturbation, there was no existing adversarial example to misclassify the neural networks. They also proved that the problem of network verification is NP-complete. Carlini et al. extended their assumption of ReLU function by presenting max(x, y) = ReLU (x−y)+y and x = ReLU (2x)−x <ref type="bibr" target="#b78">[79]</ref>. However, Reluplex runs very slow due to the large computation of verifying the networks and only works for DNNs with several hundred nodes <ref type="bibr" target="#b124">[125]</ref>. <ref type="bibr" target="#b124">[125]</ref> proposed two potential solutions: 1) prioritizing the order of checking nodes 2) sharing information of verification.</p><p>Instead of checking each point individually, Gopinath et al. proposed DeepSafe to provide safe regions of a deep neural network <ref type="bibr" target="#b123">[124]</ref> using Reluplex. They also introduced targeted robustness a safe region only regarding a targeted class.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>G. Ensembling Defenses</head><p>Due to the multi-facet of adversarial examples, multiple defense strategies can be performed together (parallel or sequential) to defend adversarial examples.</p><p>Aforementioned PixelDefend <ref type="bibr" target="#b122">[123]</ref> is composed of an adversarial detector and an "input reconstructor" to establish a defense strategy.</p><p>MagNet included one or more detectors and a reconstructor ("reformer" in the paper) as Plan A and Plan B <ref type="bibr" target="#b118">[119]</ref>. The detectors are used to find the adversarial examples which are far from the boundary of the manifold. In <ref type="bibr" target="#b118">[119]</ref>, they first measured the distance between input and encoded input and also the probability divergence (Jensen-Shannon divergence) between softmax output of input and encoded input. The adversarial examples were expected a large distance and probability divergence. To deal with the adversarial examples close to the boundary, MagNet used a reconstructor built by neural network based autoencoders. The reconstructor will map adversarial examples to legitimate examples. Figure <ref type="figure" target="#fig_9">9</ref> illustrates the workflow of the defense of two phases.</p><p>After investigating several defensive approaches, <ref type="bibr" target="#b132">[133]</ref> showed that the ensemble of those defensive approaches does not make the neural networks strong. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>VII. CHALLENGES AND DISCUSSIONS</head><p>In this section, we discuss the current challenges and the potential solutions for adversarial examples. Although many methods and theorems have been proposed and developed recently, a lot of fundamental questions need to be well explained and many challenges need to be addressed. The reason for the existence of adversarial examples is an interesting and one of the most fundamental problems for both adversaries and researchers, which exploits the vulnerability of neural networks and help defenders to resist adversarial examples. We will discuss the following questions in this section: Why do adversarial examples transfer? How to stop the transferability? Why are some defenses effective and others not? How to measure the strength of an attack as well as a defense? How to evaluate the robustness of a deep neural network against seen/unseen adversarial examples?</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Transferability</head><p>Transferability is a common property for adversarial examples. <ref type="bibr">Szegedy</ref>  We define the transferability of adversarial examples in three levels from easy to hard: 1) transfer among the same neural network architecture trained with different data; 2) transfer among different neural network architectures trained for the same task; 3) transfer among deep neural networks for different tasks. To our best knowledge, there is no existing solution on the third level yet (for instance, transfer an adversarial image from object detection to semantic segmentation).</p><p>Many studies examined transferability to show the ability of adversarial examples <ref type="bibr" target="#b18">[19]</ref>, <ref type="bibr" target="#b68">[69]</ref> Data incompletion One assumption is that adversarial examples are of low probability and low test coverage of corner cases in the testing dataset <ref type="bibr" target="#b18">[19]</ref>, <ref type="bibr" target="#b134">[135]</ref>. From training a PixelCNN, <ref type="bibr" target="#b122">[123]</ref> found that the distribution of adversarial examples was different from clean data. Even for a simple Gaussian model, a robust model can be more complicated and requires much more training data than that of a "standard" model <ref type="bibr" target="#b135">[136]</ref>.</p><p>Model capability Adversarial examples are a phenomenon not only for deep neural networks but also for all classifiers <ref type="bibr" target="#b43">[44]</ref>, <ref type="bibr" target="#b136">[137]</ref>. <ref type="bibr" target="#b68">[69]</ref> suggested that adversarial examples are the results of models being too linear in high dimensional manifolds. <ref type="bibr" target="#b137">[138]</ref> showed that in the linear case, the adversarial examples exist when the decision boundary is close to the manifold of the training data.</p><p>Contrary to <ref type="bibr" target="#b68">[69]</ref>, <ref type="bibr" target="#b136">[137]</ref> believed that adversarial examples are due to the "low flexibility" of the classifier for certain tasks. Linearity is not an "obvious explanation" <ref type="bibr" target="#b75">[76]</ref>. <ref type="bibr" target="#b79">[80]</ref> blamed adversarial examples for the sparse and discontinuous manifold which makes classifier erratic.</p><p>No robust model <ref type="bibr" target="#b35">[36]</ref> suggested that the decision boundaries of deep neural networks are inherently incorrect, which do not detect semantic objects. <ref type="bibr" target="#b138">[139]</ref> showed that if a dataset is generated by a smooth generative model with large latent space, there is no robust classifier to adversarial examples. Similarly, <ref type="bibr" target="#b139">[140]</ref> prove that if a model is trained on a sphere dataset and misclassifies a small part of the dataset, then there exist adversarial examples with a small perturbation.</p><p>In addition to adversarial examples for image classification task, as discussed in Section V, adversarial examples have been generated in various applications. Many of them deployed utterly different methods. Some applications can use the same method used in image classification task. However, some need to propose a novel method. Current studies on adversarial examples mainly focus on image classification task. No existing paper explains the relationship among different applications and existence of a universal attacking/defending method to be applied to all the applications.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Robustness Evaluation</head><p>The competition between attacks and defenses for adversarial examples becomes an "arms race": a defensive method that was proposed to prevent existing attacks was later shown to be vulnerable to some new attacks, and vice versa <ref type="bibr" target="#b78">[79]</ref>, <ref type="bibr" target="#b116">[117]</ref>. Some defenses showed that they could defend a particular attack, but later failed with a slight change of the attack <ref type="bibr" target="#b113">[114]</ref>, <ref type="bibr" target="#b114">[115]</ref>. Hence, the evaluation on the robustness of a deep neural network is necessary. For example, <ref type="bibr" target="#b140">[141]</ref> provided an upper bound of robustness for linear classifier and quadratic classifier. The following problems for robustness evaluation of deep neural networks require further exploration.</p><p>1) A methodology for evaluation on the robustness of deep neural networks: Many deep neural networks are planned to be deployed in safety-critical settings. Defending only existing attacks is not sufficient. Zero-day (new) attacks would be more harmful to deep neural networks. A methodology for evaluating the robustness of deep neural networks is required, especially for zero-day attacks, which helps people understand the confidence of model prediction and how much we can rely on them in the real world. <ref type="bibr" target="#b78">[79]</ref>, <ref type="bibr" target="#b91">[92]</ref>, <ref type="bibr" target="#b141">[142]</ref>, <ref type="bibr" target="#b142">[143]</ref> conducted initial studies on the evaluation. Moreover, this problem lies not only in the performance of deep neural network models but also in the confidentiality and privacy.</p><p>2) A benchmark platform for attacks and defenses: Most attacks and defenses described their methods without publicly available code, not to mention the parameters used in their methods. This brings difficulties for other researchers to reproduce their solutions and provide the corresponding attacks/defenses. For example, Carlini tried his best to "find the best possible defense parameters + random initialization" <ref type="foot" target="#foot_8">9</ref> . Some researchers even drew different conclusions because of different settings in their experiments. If there exists any benchmark, where both adversaries and defenders conduct experiments in a uniform way (i.e., the same threat model, dataset, classifier, attacking/defending approach), we can make a more precise comparison between different attacking and defending techniques. Cleverhans <ref type="bibr" target="#b143">[144]</ref> and Foolbox <ref type="bibr" target="#b144">[145]</ref> are open-source libraries to benchmark the vulnerability of deep neural networks against adversarial images. They build frameworks to evaluate the attacks. However, defensive strategies are missing in both tools. Providing a dataset of adversarial examples generated by different methods will make it easy for finding the blind point of deep neural networks and developing new defense strategies. This problem also occurs in other areas in deep learning.</p><p>Google Brain organized three competitions in NIPS 2017 competition track, including targeted adversarial attack, nontargeted adversarial attack, and defense against adversarial attack <ref type="bibr" target="#b145">[146]</ref>. The dataset in the competition consisted of a set of images never used before and manually labeled the images, 1,000 images for development and 5,000 images for final testing. The submitted attacks and competitions are used as benchmarks to evaluate themselves. The adversarial attacks and defenses are scored by the number of runs to fool the defenses/correctly classify images.</p><p>We present workflow of a benchmark platform for attackers and defenders (Figure <ref type="figure" target="#fig_11">10</ref>).</p><p>3) Various applications for robustness evaluation: Similar to the existence of adversarial examples for various applications, a wide range of applications make it hard to evaluate the robustness, of a deep neural network architecture. How to compare methods generating adversarial example under different threat models? Do we have a universal methodology to evaluate the robustness under all scenarios? Tackling these unsolved problems is a future direction. </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>) 0 , 2 ,</head><label>2</label><figDesc>∞ are three commonly used p metrics. 0 counts the number of pixels changed in the adversarial examples; 2 measures the Euclidean distance between the adversarial example and the original sample; ∞ denotes the maximum change for all pixels in adversarial examples. -Psychometric perceptual adversarial similarity score (PASS) is a new metric introduced in [68], consistent with human perception.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: An adversarial image generated by Fast Gradient Sign Method [69]: left: a clean image of a panda; middle: the perturbation; right: one sample adversarial image, classified as a gibbon. They claimed that the linear part of the high dimensional deep neural network could not resist adversarial examples, although the linear behavior speeded up training. Regularization approaches are used in deep neural networks such as dropout. Pre-training could not improve the robustness of networks.[68] proposed a new method, called Fast Gradient Value method, in which they replaced the sign of the gradient with the raw gradient: η = ∇ x J(θ, x, l). Fast Gradient Value method has no constraints on each pixel and can generate images with a larger local difference.According to<ref type="bibr" target="#b80">[81]</ref>, one-step attack is easy to transfer but also easy to defend (see Section VII-A).<ref type="bibr" target="#b81">[82]</ref> applied momentum to FGSM to generate adversarial examples more iteratively. The gradients were calculated by:</figDesc><graphic url="image-1.png" coords="6,58.83,387.03,231.33,86.90" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: A universal adversarial example fools the neural network on images. Left images: original labeled natural images; center image: universal perturbation; right images: perturbed images with wrong labels. [74]</figDesc><graphic url="image-3.png" coords="8,374.74,56.07,125.52,186.18" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Liu</head><label></label><figDesc>et al. conducted a study of transferability (Section VII-A) over deep neural networks on ImageNet and proposed a Model-based Ensembling Attack for targeted adversarial examples [78]. The authors argued that compared to nontargeted adversarial examples, targeted adversarial examples are much harder to transfer over deep models. Using Modelbased Ensembling Attack, they can generate transferable adversarial examples to attack a black-box model. The authors generated adversarial examples on multiple deep neural networks with full knowledge and tested them on a black-box model. Model-based Ensembling Attack was derived by the following optimization problem:</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 4 :</head><label>4</label><figDesc>Figure4: Adversarial attacks for autoencoders<ref type="bibr" target="#b95">[96]</ref>. Perturbations are added to the input the encoder. After encoding and decoding, the decoder will output an adversarial image presenting an incorrect class ∇ x J(θ, x, )). Since DQN does not have stochastic policy input, softmax of Q-values is considered to calculate the loss function. They evaluated adversarial examples on four Atari 2600 games with three norm constraints 1 , 2 , ∞ . They found Huang's Attack with 1 norm conducted a successful attack on both White-box attack and Black-box attack (no access to the training algorithms, parameters, and hyper-parameters).[94] used FGSM to attack A3C algorithm and Atari Pong task.<ref type="bibr" target="#b93">[94]</ref> found that injecting perturbations in a fraction of frames is sufficient.</figDesc><graphic url="image-4.png" coords="10,330.81,56.07,213.40,126.70" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 5 :</head><label>5</label><figDesc>Figure 5: An example of adversarial eyeglass frame against Face Recognition System [67] Kos et al. extended Tabacof et al.'s work by designing another two kinds of distances. Hence, the adversarial examples can be generated by optimizing:minη</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 6 :</head><label>6</label><figDesc>Figure 6: An adversarial example for object detection task [22]. Left: object detection on a clean image. Right: object detection on an adversarial image.</figDesc><graphic url="image-8.png" coords="12,343.36,56.07,188.30,68.92" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure 7 :</head><label>7</label><figDesc>Figure 7: Adversary examples of hiding pedestrians in the semantic segmentation task [112]. Left image: original image; Middle image: the segmentation of the original image predicted by DNN; Right image: the segmentation of the adversarial image predicted by DNN. hid the objects (e.g., pedestrians) while keeping the rest segmentation unchanged. Metzen et al. defined background classes and targeted classes (not targeted adversarial classes).Targeted classes are classes to be removed. Similar to<ref type="bibr" target="#b96">[97]</ref>, pixels which belong to the targeted classes would be assigned to their nearest background classes:</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Figure 8 :</head><label>8</label><figDesc>Figure 8: Network distillation of deep neural networks [126] A. Network Distillation Papernot et al. used network distillation to defend deep neural networks against adversarial examples<ref type="bibr" target="#b125">[126]</ref>. Network distillation was originally designed to reduce the size of deep neural networks by transferring knowledge from a large network to a small one<ref type="bibr" target="#b129">[130]</ref>,<ref type="bibr" target="#b130">[131]</ref> (Figure8). The probability of classes produced by the first DNN is used as inputs to train the second DNN. The probability of classes extracts the knowledge learned from the first DNN. Softmax is usually used to normalize the last layer of DNN and produce the probability of classes. The softmax output of the first DNN, also the input of the next DNN, can be described as:</figDesc><graphic url="image-10.png" coords="14,74.07,56.07,200.84,75.51" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>Figure 9 :</head><label>9</label><figDesc>Figure 9: MagNet workflow: one or more detectors first detects if input x is adversarial; If not, reconstruct x to x * before feeding it to the classifier. (modified from [119]) H. Summary Almost all defenses are shown to be effective only for part of attacks. They tend not to be defensive for some strong (fail to defend) and unseen attacks. Most defenses target adversarial examples in the computer vision task. However, with the development of adversarial examples in other areas, new defenses for these areas, especially for safety-critical systems, are urgently required.</figDesc><graphic url="image-11.png" coords="16,67.79,56.07,213.40,83.80" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head></head><label></label><figDesc>. Papernot et al. studied the transferability between conventional machine learning techniques (i.e., logistic regression, SVM, decision tree, kNN) and deep neural networks. They found that adversarial examples can be transferred between different parameters, training dataset of a machine learning models and even across different machine learning techniques. Liu et al. investigated transferability of targeted and nontargeted adversarial examples on complex models and large datasets (e.g., the ImageNet dataset) [78]. They found that non-targeted adversarial examples are much more transferable than targeted ones. They observed that the decision boundaries of different models aligned well with each other. Thus they proposed Model-Based Ensembling Attack to create transferable targeted adversarial examples. Tramèr et al. found that the distance to the model's decision boundary is on average larger than the distance between two models' boundaries in the same direction [134]. This may explain the existence of transferability of adversarial examples. Tramèr et al. also claimed that transferability might not be an inherent property of deep neural networks by showing a counter-example. B. The existence of Adversarial Examples The reason for the existence of adversarial examples is still an open question. Are adversarial examples an inherent property of deep neural networks? Are adversarial examples the "Achilles' heel" of deep neural networks with high performance? Many hypotheses have been proposed to explain the existence.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_11"><head>Figure 10 :</head><label>10</label><figDesc>Figure 10: Workflow of a benchmark platform for attackers and defenders: 1) attackers and defenders update/train their strategies on training dataset; 2) attackers generate adversarial examples on the clean data; 3) the adversarial examples are verified by crowdsourcing whether recognizable to human; 4) defenders generate a deep neural network as a defensive strategy; 5) evaluate the defensive strategy.</figDesc><graphic url="image-12.png" coords="17,330.81,56.07,213.40,104.22" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_12"><head></head><label></label><figDesc>VIII. CONCLUSIONIn this paper, we reviewed recent findings of adversarial examples in deep neural networks. We investigated existing methods for generating adversarial examples10 . A taxonomy of adversarial examples was proposed. We also explored the applications and countermeasures for adversarial examples.This paper attempted to cover state-of-the-art studies for adversarial examples in the deep learning domain. Compared with recent work on adversarial examples, we analyzed and10 Due to the rapid development of adversarial examples (attacks and defenses), we only considered the papers published before November 2017. We will update the survey with new methodologies and papers in our future work.discussed current challenges and potential solutions in adversarial examples.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table I :</head><label>I</label><figDesc>Notation and symbols used in this paper</figDesc><table><row><cell>Notations and Symbols</cell><cell>Description</cell></row><row><cell>x</cell><cell>original (clean, unmodified) input data</cell></row><row><cell>l</cell><cell>label of class in the classification problem. l =</cell></row><row><cell></cell><cell>1, 2, . . . , m, where m is the number of classes</cell></row><row><cell>x</cell><cell>adversarial example (modified input data)</cell></row><row><cell>l</cell><cell>label of the adversarial class in targeted adver-</cell></row><row><cell></cell><cell>sarial examples</cell></row><row><cell>f (•)</cell><cell>deep learning model (for the image classifica-</cell></row><row><cell></cell><cell>tion task, f ∈ F : R n → l)</cell></row><row><cell>θ</cell><cell>parameters of deep learning model f</cell></row><row><cell>J f (•, •)</cell><cell>loss function (e.g., cross-entropy) of model f</cell></row><row><cell>η</cell><cell>difference between original and modified input</cell></row><row><cell></cell><cell>data: η = x − x (the exact same size as the</cell></row><row><cell></cell><cell>input data)</cell></row><row><cell>• p</cell><cell>p norm</cell></row><row><cell>∇</cell><cell>gradient</cell></row><row><cell>H(•)</cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head></head><label></label><figDesc>fields of reinforcement learning, generative modeling, face recognition, object detection, semantic segmentation, natural language processing, and malware detection. Countermeasures for adversarial examples are also discussed. • We outline main challenges and potential future research directions for adversarial examples based on three main problems: transferability of adversarial examples, existence of adversarial examples, and robustness evaluation of deep neural networks. The remaining of this paper is organized as follows. Section II introduces the background of deep learning techniques, models, and datasets. We discuss adversarial examples raised in conventional machine learning in Section II. We propose a taxonomy of approaches for generating adversarial examples in Section III and elaborate on these approaches in Section IV. In Section V, we discuss applications for adversarial examples. Corresponding countermeasures are investigated in Section VI.</figDesc><table /><note>• We investigate recent approaches and their variants for generating adversarial examples and compare them using the proposed taxonomy. We show examples of se-lected applications from</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head></head><label></label><figDesc>Universal attacks only create a universal perturbation for the whole dataset. This perturbation can be applied to all clean input data. Most of the current attacks generate adversarial examples individually. However, universal perturbations make it easier to deploy adversary examples in the real world.</figDesc><table><row><cell>• Perturbation Scope</cell></row><row><cell>-Individual attacks generate different perturbations for</cell></row><row><cell>each clean input.</cell></row><row><cell>-</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table II :</head><label>II</label><figDesc>Taxonomy of Adversarial Examples</figDesc><table><row><cell>Adversarial Falsification</cell><cell>False Negative False Positive</cell><cell>[19], [20], [68]-[82] [83]</cell></row><row><cell>Threat Model</cell><cell></cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table III :</head><label>III</label><figDesc>Summary of Applications for Adversarial Examples</figDesc><table><row><cell>Architecture</cell><cell></cell><cell>DQN,</cell><cell>TRPO, A3C</cell><cell>A3C</cell><cell></cell><cell>VAE,</cell><cell>VAE-GAN</cell><cell></cell><cell>VAE, AE</cell><cell></cell><cell>VGGFace</cell></row><row><cell>Dataset</cell><cell></cell><cell>Atari</cell><cell></cell><cell>Atari Pong</cell><cell></cell><cell>MNIST,</cell><cell>SVHN,</cell><cell>CelebA</cell><cell>MNIST,</cell><cell>SVHN</cell><cell>LFW,</cell></row><row><cell>Perturbation</cell><cell>Measurement</cell><cell>1 , 2 , ∞</cell><cell></cell><cell>N/A</cell><cell></cell><cell>2</cell><cell></cell><cell></cell><cell>2</cell><cell></cell><cell>Total</cell><cell>Variation</cell></row><row><cell>Attack</cell><cell>Frequency</cell><cell>One-time</cell><cell></cell><cell>One-time</cell><cell></cell><cell>Iterative</cell><cell></cell><cell></cell><cell>Iterative</cell><cell></cell><cell>Iterative</cell></row><row><cell>Perturbation</cell><cell>Limitation</cell><cell>N/A</cell><cell></cell><cell>N/A</cell><cell></cell><cell>Optimized</cell><cell></cell><cell></cell><cell>Optimized</cell><cell></cell><cell>Optimized</cell></row><row><cell>Perturbation</cell><cell>Scope</cell><cell>Individual</cell><cell></cell><cell>Individual</cell><cell></cell><cell>Individual</cell><cell></cell><cell></cell><cell>Individual</cell><cell></cell><cell>Universal</cell></row><row><cell>Adversarial</cell><cell>Specificity</cell><cell>Non-</cell><cell>Targeted</cell><cell>Non-</cell><cell>Targeted</cell><cell>Targeted</cell><cell></cell><cell></cell><cell>Targeted</cell><cell></cell><cell>Targeted &amp;</cell><cell>Non-</cell><cell>Targeted</cell></row><row><cell>Adversary's</cell><cell>Knowledge</cell><cell>White-box &amp;</cell><cell>Black-box</cell><cell>White-box</cell><cell></cell><cell>White-box</cell><cell></cell><cell></cell><cell>White-box</cell><cell></cell><cell>white-box &amp;</cell><cell>black-box</cell></row><row><cell>Adversarial</cell><cell>Falsification</cell><cell>N/A</cell><cell></cell><cell>N/A</cell><cell></cell><cell>N/A</cell><cell></cell><cell></cell><cell>N/A</cell><cell></cell><cell>False</cell><cell>negative</cell></row><row><cell>Method</cell><cell></cell><cell>FGSM</cell><cell></cell><cell>FGSM</cell><cell></cell><cell>Feature</cell><cell>Adversary,</cell><cell>C&amp;W</cell><cell>Feature</cell><cell>Adversary</cell><cell>Impersonation</cell><cell>&amp; Dodging</cell><cell>Attack</cell></row><row><cell>Representative</cell><cell>Study</cell><cell>[93]</cell><cell></cell><cell>[94]</cell><cell></cell><cell>[95]</cell><cell></cell><cell></cell><cell>[96]</cell><cell></cell><cell>[67]</cell></row><row><cell>Applications</cell><cell></cell><cell>Reinforcement</cell><cell>Learning</cell><cell></cell><cell></cell><cell>Generative</cell><cell>Modeling</cell><cell></cell><cell></cell><cell></cell><cell>Face Recog-</cell><cell>nition</cell><cell>Object</cell><cell>Detection</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>Table IV :</head><label>IV</label><figDesc>Summary of Countermeasures for Adversarial Examples</figDesc><table><row><cell></cell><cell>Defensive Strategies</cell><cell>Representative Studies</cell></row><row><cell></cell><cell>Adversarial Detecting</cell><cell>[34], [98], [114]-[121]</cell></row><row><cell>Reactive</cell><cell>Input Reconstruction</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head></head><label></label><figDesc>et al. first found that adversarial examples generated against a neural network can fool the same neural networks trained by different datasets. Papernot et al. found that adversarial examples generated against a neural network can fool other neural networks with different architectures, even other classifiers trained by different machine learning algorithms [44]. Transferability is critical for Black-Box attacks where the victim deep learning model and the training dataset are not accessible. Attackers can train a substitute neural network model and then generate adversarial examples against substitute model. Then the victim model will be vulnerable to these adversarial examples due to transferability. From a defender's view, if we hinder transferability of adversarial examples, we can defend all white-box attackers who need to access the model and require transferability.</figDesc><table /></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0">https://aws.amazon.com/machine-learning</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_1">https://cloud.google.com/products/machine-learning</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3" xml:id="foot_2">https://bigml.com</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4" xml:id="foot_3">https://www.clarifai.com</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="5" xml:id="foot_4">https://azure.microsoft.com/en-us/services/machine-learning</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="6" xml:id="foot_5">https://console.bluemix.net/catalog/services/machine-learning</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="7" xml:id="foot_6">https://www.faceplusplus.com</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="8" xml:id="foot_7">This method was shown not effective for standard detectors (YOLO and Faster RCNN) in<ref type="bibr" target="#b110">[111]</ref>.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="9" xml:id="foot_8">Code repository used in<ref type="bibr" target="#b85">[86]</ref>: https://github.com/carlini/nn_breaking_ detection</note>
		</body>
		<back>

			<div type="funding">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>This work is partially supported by National Science Foundation (grants CNS-1842407, CNS-1747783, CNS-1624782, and OAC-1229576).</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Imagenet classification with deep convolutional neural networks</title>
		<author>
			<persName><forename type="first">A</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
				<imprint>
			<date type="published" when="2012">2012</date>
			<biblScope unit="page" from="1097" to="1105" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">Very deep convolutional networks for large-scale image recognition</title>
		<author>
			<persName><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1409.1556</idno>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">Yolo9000: Better, faster, stronger</title>
		<author>
			<persName><forename type="first">J</forename><surname>Redmon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Farhadi</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1612.08242</idno>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Faster r-cnn: Towards realtime object detection with region proposal networks</title>
		<author>
			<persName><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
				<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="91" to="99" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">The ibm 2015 english conversational telephone speech recognition system</title>
		<author>
			<persName><forename type="first">G</forename><surname>Saon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H.-K</forename><forename type="middle">J</forename><surname>Kuo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Rennie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Picheny</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1505.05899</idno>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Sequence to sequence learning with neural networks</title>
		<author>
			<persName><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><forename type="middle">V</forename><surname>Le</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
				<imprint>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="3104" to="3112" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">Wavenet: A generative model for raw audio</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">V D</forename><surname>Oord</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Dieleman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Zen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Graves</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Kalchbrenner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Senior</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Kavukcuoglu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1609.03499</idno>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Mastering the game of go with deep neural networks and tree search</title>
		<author>
			<persName><forename type="first">D</forename><surname>Silver</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">J</forename><surname>Maddison</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Guez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Sifre</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Van Den Driessche</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Schrittwieser</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Antonoglou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Panneershelvam</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Lanctot</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nature</title>
		<imprint>
			<biblScope unit="volume">529</biblScope>
			<biblScope unit="issue">7587</biblScope>
			<biblScope unit="page" from="484" to="489" />
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Mastering the game of go without human knowledge</title>
		<author>
			<persName><forename type="first">D</forename><surname>Silver</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Schrittwieser</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Antonoglou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Guez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Hubert</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Baker</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Lai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Bolton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nature</title>
		<imprint>
			<biblScope unit="volume">550</biblScope>
			<biblScope unit="issue">7676</biblScope>
			<biblScope unit="page" from="354" to="359" />
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">Mastering chess and shogi by self-play with a general reinforcement learning algorithm</title>
		<author>
			<persName><forename type="first">D</forename><surname>Silver</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Hubert</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Schrittwieser</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Antonoglou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Lai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Guez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Lanctot</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Sifre</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Kumaran</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Graepel</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1712.01815</idno>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Deep learning</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nature</title>
		<imprint>
			<biblScope unit="volume">521</biblScope>
			<biblScope unit="issue">7553</biblScope>
			<biblScope unit="page" from="436" to="444" />
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">China unveils world&apos;s first facial recognition atm</title>
		<author>
			<persName><forename type="first">C</forename><surname>Middlehurst</surname></persName>
		</author>
		<ptr target="http://www.telegraph.co.uk/news/worldnews/asia/china/11643314/China-unveils-worlds-first-facial-recognition-ATM.html" />
		<imprint>
			<date type="published" when="2015-06">Jun 2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">Face id security</title>
		<ptr target="https://images.apple.com/business/docs/FaceID_Security_Guide.pdf" />
		<imprint>
			<date type="published" when="2017-11">November 2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Large-scale malware classification using random projections and neural networks</title>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">E</forename><surname>Dahl</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">W</forename><surname>Stokes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Yu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Acoustics, Speech and Signal Processing (ICASSP)</title>
				<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2013">2013. 2013</date>
			<biblScope unit="page" from="3422" to="3426" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Droid-sec: deep learning in android malware detection</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Xue</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="s">ACM SIGCOMM Computer Communication Review</title>
		<imprint>
			<biblScope unit="volume">44</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="371" to="372" />
			<date type="published" when="2014">2014</date>
			<publisher>ACM</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Deep neural network based malware detection using two dimensional binary program features</title>
		<author>
			<persName><forename type="first">J</forename><surname>Saxe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Berlin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Malicious and Unwanted Software (MALWARE), 2015 10th International Conference on</title>
				<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="11" to="20" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">Learning fast and slow: Propedeutica for real-time malware detection</title>
		<author>
			<persName><forename type="first">R</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Gregio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Oliveira</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Xiaolin</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1712.01145</idno>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">Spider-man 2012</title>
		<author>
			<persName><forename type="first">S</forename><surname>Raimi</surname></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">Intriguing properties of neural networks</title>
		<author>
			<persName><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Zaremba</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Bruna</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Erhan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Fergus</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1312.6199</idno>
		<imprint>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">Adversarial examples in the physical world</title>
		<author>
			<persName><forename type="first">A</forename><surname>Kurakin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Bengio</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1607.02533</idno>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">Robust physical-world attacks on deep learning models</title>
		<author>
			<persName><forename type="first">I</forename><surname>Evtimov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Eykholt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Fernandes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Kohno</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Prakash</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Rahmati</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Song</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1707.08945</idno>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Adversarial examples for semantic segmentation and object detection</title>
		<author>
			<persName><forename type="first">C</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Yuille</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Computer Vision</title>
				<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Hidden voice commands</title>
		<author>
			<persName><forename type="first">N</forename><surname>Carlini</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Mishra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Vaidya</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Sherr</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Shields</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Wagner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Zhou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">USENIX Security Symposium</title>
				<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="513" to="530" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<author>
			<persName><forename type="first">G</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Xu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1708.09537</idno>
		<title level="m">Dolphinatack: Inaudible voice commands</title>
				<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title level="m" type="main">iOS -Siri -Apple</title>
		<ptr target="https://www.apple.com/ios/siri/" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title level="m" type="main">Alexa</title>
		<ptr target="https://developer.amazon.com/alexa" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<ptr target="https://www.microsoft.com/en-us/windows/cortana" />
		<title level="m">Cortana | Your Intelligent Virtual &amp; Personal Assistant | Microsoft</title>
				<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">The dark secret at the heart of ai</title>
		<author>
			<persName><forename type="first">W</forename><surname>Knight</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">MIT Technology Review</title>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Can we open the black box of AI?</title>
		<author>
			<persName><forename type="first">D</forename><surname>Castelvecchi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nature News</title>
		<imprint>
			<biblScope unit="volume">538</biblScope>
			<biblScope unit="issue">7623</biblScope>
			<biblScope unit="page">20</biblScope>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Understanding black-box predictions via influence functions</title>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">W</forename><surname>Koh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Liang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Conference on Machine Learning (ICML)</title>
				<meeting>the International Conference on Machine Learning (ICML)</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">The mythos of model interpretability</title>
		<author>
			<persName><forename type="first">Z</forename><forename type="middle">C</forename><surname>Lipton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning (ICML) Workshop</title>
				<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
		<title level="m" type="main">Opening the black box of deep neural networks via information</title>
		<author>
			<persName><forename type="first">R</forename><surname>Shwartz-Ziv</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Tishby</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1703.00810</idno>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
		<title level="m" type="main">Grad-cam: Why did you say that? visual explanations from deep networks via gradient-based localization</title>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">R</forename><surname>Selvaraju</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Das</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Vedantam</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Cogswell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Parikh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Batra</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1610.02391</idno>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
		<title level="m" type="main">Safetynet: Detecting and rejecting adversarial examples robustly</title>
		<author>
			<persName><forename type="first">J</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Issaranon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Forsyth</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Adversarial training for relation extraction</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Bamman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Russell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing</title>
				<meeting>the 2017 Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="1779" to="1784" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Towards interpretable deep neural networks by leveraging adversarial examples</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Bao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
				<meeting>the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<monogr>
		<title level="m" type="main">Sok: Towards the science of security and privacy in machine learning</title>
		<author>
			<persName><forename type="first">N</forename><surname>Papernot</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Mcdaniel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Sinha</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Wellman</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1611.03814</idno>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b37">
	<monogr>
		<title level="m" type="main">Poisoning attacks against support vector machines</title>
		<author>
			<persName><forename type="first">B</forename><surname>Biggio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Nelson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Laskov</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1206.6389</idno>
		<imprint>
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Pattern recognition systems under attack</title>
		<author>
			<persName><forename type="first">F</forename><surname>Roli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Biggio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Fumera</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Iberoamerican Congress on Pattern Recognition</title>
				<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2013">2013</date>
			<biblScope unit="page" from="1" to="8" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Is feature selection secure against training data poisoning?</title>
		<author>
			<persName><forename type="first">H</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Biggio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Brown</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Fumera</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Eckert</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Roli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
				<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="1689" to="1698" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Systematic poisoning attacks on and defenses for machine learning in healthcare</title>
		<author>
			<persName><forename type="first">M</forename><surname>Mozaffari-Kermani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Sur-Kolay</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Raghunathan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><forename type="middle">K</forename><surname>Jha</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE journal of biomedical and health informatics</title>
		<imprint>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="1893" to="1905" />
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Blind attacks on machine learners</title>
		<author>
			<persName><forename type="first">A</forename><surname>Beatson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
				<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="2397" to="2405" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Data poisoning attacks against autoregressive models</title>
		<author>
			<persName><forename type="first">S</forename><surname>Alfeld</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Barford</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
				<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="1452" to="1458" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Transferability in machine learning: from phenomena to black-box attacks using adversarial samples</title>
		<author>
			<persName><forename type="first">N</forename><surname>Papernot</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">D</forename><surname>Mcdaniel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><forename type="middle">J</forename><surname>Goodfellow</surname></persName>
		</author>
		<idno>abs/1605.07277</idno>
	</analytic>
	<monogr>
		<title level="j">CoRR</title>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Model inversion attacks that exploit confidence information and basic countermeasures</title>
		<author>
			<persName><forename type="first">M</forename><surname>Fredrikson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Jha</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Ristenpart</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 22nd ACM SIGSAC Conference on Computer and Communications Security</title>
				<meeting>the 22nd ACM SIGSAC Conference on Computer and Communications Security</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="1322" to="1333" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Deep learning with differential privacy</title>
		<author>
			<persName><forename type="first">M</forename><surname>Abadi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Chu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">B</forename><surname>Mcmahan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Mironov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Talwar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2016 ACM SIGSAC Conference on Computer and Communications Security</title>
				<meeting>the 2016 ACM SIGSAC Conference on Computer and Communications Security</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="308" to="318" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Membership inference attacks against machine learning models</title>
		<author>
			<persName><forename type="first">R</forename><surname>Shokri</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Stronati</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Shmatikov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Security and Privacy (SP), 2017 IEEE Symposium</title>
				<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="3" to="18" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Scaling learning algorithms towards ai</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Large-scale kernel machines</title>
				<imprint>
			<date type="published" when="2007">2007</date>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="page" from="1" to="41" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">A survey of modern questions and challenges in feature extraction</title>
		<author>
			<persName><forename type="first">D</forename><surname>Storcheus</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Rostamizadeh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Kumar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Feature Extraction: Modern Questions and Challenges</title>
				<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="1" to="18" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Delving deep into rectifiers: Surpassing human-level performance on imagenet classification</title>
		<author>
			<persName><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE international conference on computer vision</title>
				<meeting>the IEEE international conference on computer vision</meeting>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="1026" to="1034" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">You only look once: Unified, real-time object detection</title>
		<author>
			<persName><forename type="first">J</forename><surname>Redmon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Divvala</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Farhadi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
				<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="779" to="788" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Fully convolutional networks for semantic segmentation</title>
		<author>
			<persName><forename type="first">J</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Shelhamer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
				<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="3431" to="3440" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<monogr>
		<title level="m" type="main">Rethinking atrous convolution for semantic image segmentation</title>
		<author>
			<persName><forename type="first">L.-C</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Papandreou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Schroff</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Adam</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1706.05587</idno>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">Building high-level features using large scale unsupervised learning</title>
		<author>
			<persName><forename type="first">Q</forename><forename type="middle">V</forename><surname>Le</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Acoustics, Speech and Signal Processing (ICASSP)</title>
				<imprint>
			<date type="published" when="2013">2013. 2013</date>
			<biblScope unit="page" from="8595" to="8598" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">Going deeper with convolutions</title>
		<author>
			<persName><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Sermanet</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Reed</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Anguelov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Erhan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Vanhoucke</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Rabinovich</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
				<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="1" to="9" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<analytic>
		<title level="a" type="main">Rethinking the inception architecture for computer vision</title>
		<author>
			<persName><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Vanhoucke</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Ioffe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Shlens</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Wojna</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
				<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="2818" to="2826" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<analytic>
		<title level="a" type="main">Inception-v4, inception-resnet and the impact of residual connections on learning</title>
		<author>
			<persName><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Ioffe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Vanhoucke</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">A</forename><surname>Alemi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
				<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="4278" to="4284" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<monogr>
		<title level="m" type="main">The mnist data set</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Cortes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Burges</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1998">1998</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b58">
	<monogr>
		<title level="m" type="main">Learning multiple layers of features from tiny images</title>
		<author>
			<persName><forename type="first">A</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Hinton</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2009">2009</date>
		</imprint>
		<respStmt>
			<orgName>University of Toronto</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">Technical report</note>
</biblStruct>

<biblStruct xml:id="b59">
	<analytic>
		<title level="a" type="main">ImageNet Large Scale Visual Recognition Challenge</title>
		<author>
			<persName><forename type="first">O</forename><surname>Russakovsky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Krause</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Satheesh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Karpathy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Khosla</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Bernstein</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">C</forename><surname>Berg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Computer Vision (IJCV)</title>
		<imprint>
			<biblScope unit="volume">115</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="211" to="252" />
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b60">
	<analytic>
		<title level="a" type="main">The security of machine learning</title>
		<author>
			<persName><forename type="first">M</forename><surname>Barreno</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Nelson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">D</forename><surname>Joseph</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Tygar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Machine Learning</title>
				<imprint>
			<date type="published" when="2010">2010</date>
			<biblScope unit="volume">81</biblScope>
			<biblScope unit="page" from="121" to="148" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b61">
	<analytic>
		<title level="a" type="main">Adversarial classification</title>
		<author>
			<persName><forename type="first">N</forename><surname>Dalvi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Domingos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Sanghai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Verma</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the tenth ACM SIGKDD international conference on Knowledge discovery and data mining</title>
				<meeting>the tenth ACM SIGKDD international conference on Knowledge discovery and data mining</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2004">2004</date>
			<biblScope unit="page" from="99" to="108" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b62">
	<analytic>
		<title level="a" type="main">Adversarial learning</title>
		<author>
			<persName><forename type="first">D</forename><surname>Lowd</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Meek</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the eleventh ACM SIGKDD international conference on Knowledge discovery in data mining</title>
				<meeting>the eleventh ACM SIGKDD international conference on Knowledge discovery in data mining</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2005">2005</date>
			<biblScope unit="page" from="641" to="647" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b63">
	<analytic>
		<title level="a" type="main">Multiple classifier systems for robust classifier design in adversarial environments</title>
		<author>
			<persName><forename type="first">B</forename><surname>Biggio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Fumera</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Roli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Machine Learning and Cybernetics</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="issue">1-4</biblScope>
			<biblScope unit="page" from="27" to="41" />
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b64">
	<analytic>
		<title level="a" type="main">Evasion attacks against machine learning at test time</title>
		<author>
			<persName><forename type="first">B</forename><surname>Biggio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Corona</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Maiorca</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Nelson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Šrndić</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Laskov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Giacinto</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Roli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Joint European Conference on Machine Learning and Knowledge Discovery in Databases</title>
				<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2013">2013</date>
			<biblScope unit="page" from="387" to="402" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b65">
	<analytic>
		<title level="a" type="main">Can machine learning be secure</title>
		<author>
			<persName><forename type="first">M</forename><surname>Barreno</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Nelson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Sears</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">D</forename><surname>Joseph</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">D</forename><surname>Tygar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2006 ACM Symposium on Information, computer and communications security</title>
				<meeting>the 2006 ACM Symposium on Information, computer and communications security</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2006">2006</date>
			<biblScope unit="page" from="16" to="25" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b66">
	<analytic>
		<title level="a" type="main">Accessorize to a crime: Real and stealthy attacks on state-of-the-art face recognition</title>
		<author>
			<persName><forename type="first">M</forename><surname>Sharif</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Bhagavatula</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Bauer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">K</forename><surname>Reiter</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2016 ACM SIGSAC Conference on Computer and Communications Security</title>
				<meeting>the 2016 ACM SIGSAC Conference on Computer and Communications Security</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="1528" to="1540" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b67">
	<analytic>
		<title level="a" type="main">Adversarial diversity and hard positive generation</title>
		<author>
			<persName><forename type="first">A</forename><surname>Rozsa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">M</forename><surname>Rudd</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">E</forename><surname>Boult</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR) Workshops</title>
				<meeting>the IEEE Conference on Computer Vision and Pattern Recognition (CVPR) Workshops</meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="25" to="32" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b68">
	<monogr>
		<title level="m" type="main">Explaining and harnessing adversarial examples</title>
		<author>
			<persName><forename type="first">I</forename><forename type="middle">J</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Shlens</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1412.6572</idno>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b69">
	<analytic>
		<title level="a" type="main">The limitations of deep learning in adversarial settings</title>
		<author>
			<persName><forename type="first">N</forename><surname>Papernot</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Mcdaniel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Jha</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Fredrikson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><forename type="middle">B</forename><surname>Celik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Swami</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Security and Privacy</title>
				<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2016">2016. 2016</date>
			<biblScope unit="page" from="372" to="387" />
		</imprint>
	</monogr>
	<note>EuroS&amp;P)</note>
</biblStruct>

<biblStruct xml:id="b70">
	<analytic>
		<title level="a" type="main">Deepfool: a simple and accurate method to fool deep neural networks</title>
		<author>
			<persName><forename type="first">S.-M</forename><surname>Moosavi-Dezfooli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Fawzi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Frossard</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
				<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="2574" to="2582" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b71">
	<analytic>
		<title level="a" type="main">Towards evaluating the robustness of neural networks</title>
		<author>
			<persName><forename type="first">N</forename><surname>Carlini</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Wagner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Security and Privacy (S&amp;P), 2017 IEEE Symposium</title>
				<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="39" to="57" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b72">
	<monogr>
		<title level="m" type="main">Zoo: Zeroth order optimization based black-box attacks to deep neural networks without training substitute models</title>
		<author>
			<persName><forename type="first">P.-Y</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Sharma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Yi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C.-J</forename><surname>Hsieh</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1708.03999</idno>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b73">
	<analytic>
		<title level="a" type="main">Universal adversarial perturbations</title>
		<author>
			<persName><forename type="first">S.-M</forename><surname>Moosavi-Dezfooli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Fawzi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Fawzi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Frossard</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
				<meeting>the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b74">
	<monogr>
		<title level="m" type="main">One pixel attack for fooling deep neural networks</title>
		<author>
			<persName><forename type="first">J</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">V</forename><surname>Vargas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Kouichi</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1710.08864</idno>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b75">
	<analytic>
		<title level="a" type="main">Adversarial manipulation of deep representations</title>
		<author>
			<persName><forename type="first">S</forename><surname>Sabour</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Faghri</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">J</forename><surname>Fleet</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Conference on Learning Representations (ICLR)</title>
				<meeting>the International Conference on Learning Representations (ICLR)</meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b76">
	<monogr>
		<title level="m" type="main">Generating natural adversarial examples</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Dua</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Singh</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1710.11342</idno>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b77">
	<analytic>
		<title level="a" type="main">Delving into transferable adversarial examples and black-box attacks</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Song</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Conference on Learning Representations (ICLR)</title>
				<meeting>the International Conference on Learning Representations (ICLR)</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b78">
	<monogr>
		<title level="m" type="main">Ground-truth adversarial examples</title>
		<author>
			<persName><forename type="first">N</forename><surname>Carlini</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Katz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Barrett</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">L</forename><surname>Dill</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1709.10207</idno>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b79">
	<analytic>
		<title level="a" type="main">Exploring the space of adversarial images</title>
		<author>
			<persName><forename type="first">P</forename><surname>Tabacof</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Valle</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Neural Networks (IJCNN), 2016 International Joint Conference</title>
				<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="426" to="433" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b80">
	<analytic>
		<title level="a" type="main">Adversarial machine learning at scale</title>
		<author>
			<persName><forename type="first">A</forename><surname>Kurakin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Conference on Learning Representations (ICLR)</title>
				<meeting>the International Conference on Learning Representations (ICLR)</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b81">
	<monogr>
		<title level="m" type="main">Boosting adversarial attacks with momentum</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Liao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Pang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Zhu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1710.06081</idno>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b82">
	<analytic>
		<title level="a" type="main">Deep neural networks are easily fooled: High confidence predictions for unrecognizable images</title>
		<author>
			<persName><forename type="first">A</forename><surname>Nguyen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Yosinski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Clune</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
				<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="427" to="436" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b83">
	<monogr>
		<title level="m" type="main">Ensemble adversarial training: Attacks and defenses</title>
		<author>
			<persName><forename type="first">F</forename><surname>Tramèr</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Kurakin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Papernot</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Boneh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Mcdaniel</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1705.07204</idno>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b84">
	<analytic>
		<title level="a" type="main">Robots that can adapt like animals</title>
		<author>
			<persName><forename type="first">A</forename><surname>Cully</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Clune</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Tarapore</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J.-B</forename><surname>Mouret</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nature</title>
		<imprint>
			<biblScope unit="volume">521</biblScope>
			<biblScope unit="issue">7553</biblScope>
			<biblScope unit="page" from="503" to="507" />
			<date type="published" when="2015-05">may 2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b85">
	<monogr>
		<title level="m" type="main">Adversarial examples are not easily detected: Bypassing ten detection methods</title>
		<author>
			<persName><forename type="first">N</forename><surname>Carlini</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Wagner</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017">2017</date>
			<publisher>AISEC</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b86">
	<monogr>
		<title level="m" type="main">Magnet and efficient defenses against adversarial attacks are not robust to adversarial examples</title>
		<idno type="arXiv">arXiv:1711.08478</idno>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b87">
	<monogr>
		<title level="m" type="main">Striving for simplicity: The all convolutional net</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">T</forename><surname>Springenberg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Dosovitskiy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Brox</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Riedmiller</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1412.6806</idno>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b88">
	<monogr>
		<title level="m" type="main">Network in network</title>
		<author>
			<persName><forename type="first">M</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Yan</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1312.4400</idno>
		<imprint>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b89">
	<analytic>
		<title level="a" type="main">Parametric image alignment using enhanced correlation coefficient maximization</title>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">D</forename><surname>Evangelidis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">Z</forename><surname>Psarakis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page" from="1858" to="1865" />
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b90">
	<analytic>
		<title level="a" type="main">Image quality assessment using the ssim and the just noticeable difference paradigm</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">R</forename><surname>Flynn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Ward</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Abich</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Poole</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Engineering Psychology and Cognitive Ergonomics</title>
				<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2013">2013</date>
			<biblScope unit="page" from="23" to="30" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b91">
	<monogr>
		<title level="m" type="main">Reluplex: An efficient smt solver for verifying deep neural networks</title>
		<author>
			<persName><forename type="first">G</forename><surname>Katz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Barrett</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Dill</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Julian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Kochenderfer</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1702.01135</idno>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b92">
	<monogr>
		<author>
			<persName><forename type="first">S</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Papernot</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Duan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Abbeel</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1702.02284</idno>
		<title level="m">Adversarial attacks on neural network policies</title>
				<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b93">
	<analytic>
		<title level="a" type="main">Delving into adversarial attacks on deep policies</title>
		<author>
			<persName><forename type="first">J</forename><surname>Kos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Song</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR Workshop</title>
				<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b94">
	<monogr>
		<title level="m" type="main">Adversarial examples for generative models</title>
		<author>
			<persName><forename type="first">J</forename><surname>Kos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Fischer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Song</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1702.06832</idno>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b95">
	<analytic>
		<title level="a" type="main">Adversarial images for variational autoencoders</title>
		<author>
			<persName><forename type="first">P</forename><surname>Tabacof</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Tavares</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Valle</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS Workshop</title>
				<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b96">
	<monogr>
		<title level="m" type="main">Adversarial examples for semantic image segmentation</title>
		<author>
			<persName><forename type="first">V</forename><surname>Fischer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">C</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">H</forename><surname>Metzen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Brox</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017">2017 workshop, 2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b97">
	<analytic>
		<title level="a" type="main">On detecting adversarial perturbations</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">H</forename><surname>Metzen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Genewein</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Fischer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Bischoff</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of 5th International Conference on Learning Representations (ICLR)</title>
				<meeting>5th International Conference on Learning Representations (ICLR)</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b98">
	<analytic>
		<title level="a" type="main">Adversarial examples for evaluating reading comprehension systems</title>
		<author>
			<persName><forename type="first">R</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Liang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Conference on Empirical Methods in Natural Language Processing</title>
				<meeting>the Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b99">
	<monogr>
		<title level="m" type="main">Understanding neural networks through representation erasure</title>
		<author>
			<persName><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Monroe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Jurafsky</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1612.08220</idno>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b100">
	<analytic>
		<title level="a" type="main">Adversarial examples for malware detection</title>
		<author>
			<persName><forename type="first">K</forename><surname>Grosse</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Papernot</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Manoharan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Backes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Mcdaniel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Symposium on Research in Computer Security</title>
				<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="62" to="79" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b101">
	<monogr>
		<title level="m" type="main">Evading machine learning malware detection</title>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">S</forename><surname>Anderson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Kharkar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Filar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Roth</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017">2017</date>
			<publisher>Black Hat</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b102">
	<monogr>
		<title level="m" type="main">Generating adversarial malware examples for black-box attacks based on gan</title>
		<author>
			<persName><forename type="first">W</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Tan</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1702.05983</idno>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b103">
	<analytic>
		<title level="a" type="main">Deepdga: Adversariallytuned domain generation and detection</title>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">S</forename><surname>Anderson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Woodbridge</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Filar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2016 ACM Workshop on Artificial Intelligence and Security (AISec)</title>
				<meeting>the 2016 ACM Workshop on Artificial Intelligence and Security (AISec)</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="13" to="21" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b104">
	<analytic>
		<title level="a" type="main">Automatically evading classifiers</title>
		<author>
			<persName><forename type="first">W</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Evans</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Network and Distributed System Security Symposium (NDSS)</title>
				<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b105">
	<analytic>
		<title level="a" type="main">Asynchronous methods for deep reinforcement learning</title>
		<author>
			<persName><forename type="first">V</forename><surname>Mnih</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">P</forename><surname>Badia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Mirza</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Graves</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Lillicrap</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Harley</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Silver</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Kavukcuoglu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
				<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="1928" to="1937" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b106">
	<monogr>
		<title level="m" type="main">Full resolution image compression with recurrent neural networks</title>
		<author>
			<persName><forename type="first">G</forename><surname>Toderici</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Vincent</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Johnston</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">J</forename><surname>Hwang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Minnen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Shor</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Covell</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1608.05148</idno>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b107">
	<analytic>
		<title level="a" type="main">Photo-realistic single image super-resolution using a generative adversarial network</title>
		<author>
			<persName><forename type="first">C</forename><surname>Ledig</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Theis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Huszár</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Caballero</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Cunningham</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Acosta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Aitken</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Tejani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Totz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conference on Computer Vision and Pattern Recognition</title>
				<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b108">
	<monogr>
		<title level="m" type="main">Autoencoding beyond pixels using a learned similarity metric</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">B L</forename><surname>Larsen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">K</forename><surname>Sønderby</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Larochelle</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Winther</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1512.09300</idno>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b109">
	<analytic>
		<title level="a" type="main">Deep face recognition</title>
		<author>
			<persName><forename type="first">O</forename><forename type="middle">M</forename><surname>Parkhi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Vedaldi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">BMVC</title>
				<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b110">
	<monogr>
		<title level="m" type="main">Standard detectors aren&apos;t (currently) fooled by physical adversarial stop signs</title>
		<author>
			<persName><forename type="first">J</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Sibai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Fabry</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Forsyth</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1710.03337</idno>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b111">
	<analytic>
		<title level="a" type="main">Universal adversarial perturbations against semantic image segmentation</title>
		<author>
			<persName><forename type="first">J</forename><surname>Hendrik Metzen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Brox</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Fischer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
				<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="2755" to="2764" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b112">
	<monogr>
		<title level="m" type="main">Squad: 100,000+ questions for machine comprehension of text</title>
		<author>
			<persName><forename type="first">P</forename><surname>Rajpurkar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Lopyrev</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Liang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1606.05250</idno>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b113">
	<monogr>
		<title level="m" type="main">Dimensionality reduction as a defense against evasion attacks on machine learning classifiers</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">N</forename><surname>Bhagoji</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Cullina</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Mittal</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1704.02654</idno>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b114">
	<monogr>
		<title level="m" type="main">Detecting adversarial samples from artifacts</title>
		<author>
			<persName><forename type="first">R</forename><surname>Feinman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">R</forename><surname>Curtin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Shintre</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">B</forename><surname>Gardner</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1703.00410</idno>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b115">
	<monogr>
		<title level="m" type="main">Adversarial and clean data are not twins</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W.-S</forename><surname>Ku</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1704.04960</idno>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b116">
	<monogr>
		<title level="m" type="main">On the (statistical) detection of adversarial examples</title>
		<author>
			<persName><forename type="first">K</forename><surname>Grosse</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Manoharan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Papernot</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Backes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Mcdaniel</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1702.06280</idno>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b117">
	<analytic>
		<title level="a" type="main">Early methods for detecting adversarial images</title>
		<author>
			<persName><forename type="first">D</forename><surname>Hendrycks</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Gimpel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR Workshop</title>
				<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b118">
	<analytic>
		<title level="a" type="main">Magnet: a two-pronged defense against adversarial examples</title>
		<author>
			<persName><forename type="first">D</forename><surname>Meng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CCS</title>
				<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b119">
	<monogr>
		<title level="m" type="main">Towards robust detection of adversarial examples</title>
		<author>
			<persName><forename type="first">T</forename><surname>Pang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Zhu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1706.00633</idno>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b120">
	<monogr>
		<title level="m" type="main">Detecting adversarial attacks on neural network policies with visual foresight</title>
		<author>
			<persName><forename type="first">Y.-C</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M.-Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J.-B</forename><surname>Huang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1710.00814</idno>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b121">
	<analytic>
		<title level="a" type="main">Towards deep neural network architectures robust to adversarial examples</title>
		<author>
			<persName><forename type="first">S</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Rigazio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Conference on Learning Representations (ICLR)</title>
				<meeting>the International Conference on Learning Representations (ICLR)</meeting>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b122">
	<monogr>
		<title level="m" type="main">Pixeldefend: Leveraging generative models to understand and defend against adversarial examples</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Nowozin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Ermon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Kushman</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1710.10766</idno>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b123">
	<monogr>
		<title level="m" type="main">Deepsafe: A data-driven approach for checking adversarial robustness in neural networks</title>
		<author>
			<persName><forename type="first">D</forename><surname>Gopinath</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Katz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">S</forename><surname>Pasareanu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Barrett</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1710.00486</idno>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b124">
	<monogr>
		<title level="m" type="main">Towards proving the adversarial robustness of deep neural networks</title>
		<author>
			<persName><forename type="first">G</forename><surname>Katz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Barrett</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">L</forename><surname>Dill</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Julian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">J</forename><surname>Kochenderfer</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1709.02802</idno>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b125">
	<analytic>
		<title level="a" type="main">Distillation as a defense to adversarial perturbations against deep neural networks</title>
		<author>
			<persName><forename type="first">N</forename><surname>Papernot</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Mcdaniel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Jha</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Swami</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Security and Privacy (SP), 2016 IEEE Symposium</title>
				<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="582" to="597" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b126">
	<monogr>
		<title level="m" type="main">Learning with a strong adversary</title>
		<author>
			<persName><forename type="first">R</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Schuurmans</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Szepesvári</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1511.03034</idno>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b127">
	<monogr>
		<title level="m" type="main">Adversarial examples, uncertainty, and transfer testing robustness in gaussian process hybrid deep networks</title>
		<author>
			<persName><forename type="first">J</forename><surname>Bradshaw</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">G D G</forename><surname>Matthews</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Ghahramani</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1707.02476</idno>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b128">
	<monogr>
		<title level="m" type="main">Robustness to adversarial examples through an ensemble of specialists</title>
		<author>
			<persName><forename type="first">M</forename><surname>Abbasi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Gagné</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1702.06856</idno>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b129">
	<analytic>
		<title level="a" type="main">Do deep nets really need to be deep?</title>
		<author>
			<persName><forename type="first">J</forename><surname>Ba</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Caruana</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
				<imprint>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="2654" to="2662" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b130">
	<monogr>
		<title level="m" type="main">Distilling the knowledge in a neural network</title>
		<author>
			<persName><forename type="first">G</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Dean</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1503.02531</idno>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b131">
	<monogr>
		<title level="m" type="main">Pixelcnn++: Improving the pixelcnn with discretized logistic mixture likelihood and other modifications</title>
		<author>
			<persName><forename type="first">T</forename><surname>Salimans</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Karpathy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">P</forename><surname>Kingma</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017">2017</date>
			<publisher>ICLR Poster</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b132">
	<analytic>
		<title level="a" type="main">Adversarial example defense: Ensembles of weak defenses are not strong</title>
		<author>
			<persName><forename type="first">W</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Carlini</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Song</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">11th USENIX Workshop on Offensive Technologies (WOOT 17)</title>
				<meeting><address><addrLine>Vancouver, BC</addrLine></address></meeting>
		<imprint>
			<publisher>USENIX Association</publisher>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b133">
	<monogr>
		<title level="m" type="main">The space of transferable adversarial examples</title>
		<author>
			<persName><forename type="first">F</forename><surname>Tramèr</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Papernot</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Boneh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Mcdaniel</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1704.03453</idno>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b134">
	<analytic>
		<title level="a" type="main">Deepxplore: Automated whitebox testing of deep learning systems</title>
		<author>
			<persName><forename type="first">K</forename><surname>Pei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Jana</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the ACM SIGOPS 26th symposium on Operating systems principles</title>
				<meeting>the ACM SIGOPS 26th symposium on Operating systems principles</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b135">
	<monogr>
		<title level="m" type="main">Adversarially robust generalization requires more data</title>
		<author>
			<persName><forename type="first">L</forename><surname>Schmidt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Santurkar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Tsipras</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Talwar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Ądry</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1804.11285</idno>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b136">
	<analytic>
		<title level="a" type="main">Fundamental limits on adversarial robustness</title>
		<author>
			<persName><forename type="first">A</forename><surname>Fawzi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Fawzi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Frossard</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ICML, Workshop on Deep Learning</title>
				<meeting>ICML, Workshop on Deep Learning</meeting>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b137">
	<monogr>
		<title level="m" type="main">A boundary tilting persepective on the phenomenon of adversarial examples</title>
		<author>
			<persName><forename type="first">T</forename><surname>Tanay</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Griffin</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1608.07690</idno>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b138">
	<monogr>
		<title level="m" type="main">Adversarial vulnerability for any classifier</title>
		<author>
			<persName><forename type="first">A</forename><surname>Fawzi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Fawzi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Fawzi</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1802.08686</idno>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b139">
	<monogr>
		<title level="m" type="main">Adversarial spheres</title>
		<author>
			<persName><forename type="first">J</forename><surname>Gilmer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Metz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Faghri</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">S</forename><surname>Schoenholz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Raghu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Wattenberg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Goodfellow</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1801.02774</idno>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b140">
	<monogr>
		<title level="m" type="main">Analysis of classifiers&apos; robustness to adversarial perturbations</title>
		<author>
			<persName><forename type="first">A</forename><surname>Fawzi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Fawzi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Frossard</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1502.02590</idno>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b141">
	<analytic>
		<title level="a" type="main">Measuring neural net robustness with constraints</title>
		<author>
			<persName><forename type="first">O</forename><surname>Bastani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Ioannou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Lampropoulos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Vytiniotis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Nori</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Criminisi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
				<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="2613" to="2621" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b142">
	<analytic>
		<title level="a" type="main">Safety verification of deep neural networks</title>
		<author>
			<persName><forename type="first">X</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Kwiatkowska</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Wu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Aided Verification: 29th International Conference (CAV)</title>
				<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="3" to="29" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b143">
	<monogr>
		<author>
			<persName><forename type="first">I</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Papernot</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Mcdaniel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Feinman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Faghri</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Matyasko</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Hambardzumyan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y.-L</forename><surname>Juang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Kurakin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Sheatsley</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Garg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y.-C</forename><surname>Lin</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1610.00768</idno>
		<title level="m">cleverhans v2.0.0: an adversarial machine learning library</title>
				<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b144">
	<monogr>
		<title level="m" type="main">Foolbox v0.8.0: A python toolbox to benchmark the robustness of machine learning models</title>
		<author>
			<persName><forename type="first">J</forename><surname>Rauber</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Brendel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Bethge</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1707.04131</idno>
		<ptr target="http://arxiv.org/abs/1707.04131" />
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b145">
	<monogr>
		<title level="m" type="main">Adversarial attacks and defences competition</title>
		<author>
			<persName><forename type="first">A</forename><surname>Kurakin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Liao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Pang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Xie</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1804.00097</idno>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
