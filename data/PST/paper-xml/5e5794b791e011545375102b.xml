<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">DNN-CHIP PREDICTOR: AN ANALYTICAL PERFORMANCE PREDICTOR FOR DNN ACCELERATORS WITH VARIOUS DATAFLOWS AND HARDWARE ARCHITECTURES</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date type="published" when="2020-02-26">26 Feb 2020</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Yang</forename><surname>Zhao</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Electrical and Computer Engineering</orgName>
								<orgName type="institution">Rice University</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Chaojian</forename><surname>Li</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Electrical and Computer Engineering</orgName>
								<orgName type="institution">Rice University</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Yue</forename><surname>Wang</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Electrical and Computer Engineering</orgName>
								<orgName type="institution">Rice University</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Pengfei</forename><surname>Xu</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Electrical and Computer Engineering</orgName>
								<orgName type="institution">Rice University</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Yongan</forename><surname>Zhang</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Electrical and Computer Engineering</orgName>
								<orgName type="institution">Rice University</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Yingyan</forename><surname>Lin</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Electrical and Computer Engineering</orgName>
								<orgName type="institution">Rice University</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">DNN-CHIP PREDICTOR: AN ANALYTICAL PERFORMANCE PREDICTOR FOR DNN ACCELERATORS WITH VARIOUS DATAFLOWS AND HARDWARE ARCHITECTURES</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2020-02-26">26 Feb 2020</date>
						</imprint>
					</monogr>
					<idno type="arXiv">arXiv:2002.11270v1[cs.LG]</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.2" ident="GROBID" when="2022-12-25T12:51+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>DNN accelerator</term>
					<term>ASIC</term>
					<term>FPGA</term>
					<term>design simulator</term>
					<term>design automation</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>The recent breakthroughs in deep neural networks (DNNs) have spurred a tremendously increased demand for DNN accelerators. However, designing DNN accelerators is non-trivial as it often takes months/years and requires cross-disciplinary knowledge. To enable fast and effective DNN accelerator development, we propose DNN-Chip Predictor, an analytical performance predictor which can accurately predict DNN accelerators' energy, throughput, and latency prior to their actual implementation. Our Predictor features two highlights: (1) its analytical performance formulation of DNN ASIC/FPGA accelerators facilitates fast design space exploration and optimization; and (2) it supports DNN accelerators with different algorithm-to-hardware mapping methods (i.e., dataflows) and hardware architectures. Experiment results based on 2 DNN models and 3 different ASIC/FPGA implementations show that our DNN-Chip Predictor's predicted performance differs from those of chip measurements of FPGA/ASIC implementation by no more than 17.66% when using different DNN models, hardware architectures, and dataflows. We will release code upon acceptance.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">INTRODUCTION</head><p>Deep Neural Networks (DNNs) have achieved record-breaking performance in various applications, such as image classification <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b1">2]</ref> and natural language processing <ref type="bibr" target="#b2">[3]</ref>. However, their powerful performance often comes with a prohibitive complexity <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b4">5,</ref><ref type="bibr" target="#b5">6,</ref><ref type="bibr" target="#b6">7,</ref><ref type="bibr" target="#b7">8]</ref>. Moreover, DNN-based applications often require not only high accuracy, but also aggressive hardware performance, including high throughput, low latency, and high energy efficiency. As such, there has been intensive research on DNN accelerators in order to take advantage of different hardware platforms, such as FPGAs and ASICs, for improving DNN acceleration efficiency <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b9">10,</ref><ref type="bibr" target="#b10">11,</ref><ref type="bibr" target="#b11">12,</ref><ref type="bibr" target="#b12">13,</ref><ref type="bibr" target="#b13">14]</ref>.</p><p>While DNN accelerators can be 1000Ã— more efficient than general purpose computing platforms <ref type="bibr" target="#b15">[15]</ref>, developing DNN accelerators presents significant challenges, because: <ref type="bibr" target="#b0">(1)</ref> mainstream DNNs have millions of parameters and billions of operations; <ref type="bibr" target="#b1">(2)</ref> the design space of DNN accelerator is large due to numerous design choices of architectures, hardware IPs, DNN-to-accelerator-mappings, etc.; and (3) there is an algorithm/hardware co-design need for the same DNN functionality to have a different decomposition that would require different hardware IPs and thus correspond to dramatically different hardware performance/energy/area trade-offs. Therefore, high-quality DNN accelerators often take months/years to design and require a large team of cross-disciplinary experts with knowledge in DNN algorithms, micro-architectures, and physical chip design. Such a barrier makes it difficult to scientifically explore innovative DNN accelerator design and thus limits DNNs' more extensive applications.</p><p>To address the aforementioned challenges, we propose DNN-Chip Predictor, an analytical performance predictor which can efficiently and accurately predict DNN accelerators' performance prior to time-consuming ASIC/FPGA hardware implementation. Specifically, our Predictor formulates DNN accelerators' energy, throughput, and latency based on parameters that characterize the DNN models and corresponding accelerators' architectures and algorithm-to-hardware mapping methods (i.e., dataflows). Such a generic Predictor (1) enables fast evaluation of DNN accelerator innovations and (2) can be used as an efficient design exploration and optimization tool for DNN accelerators, given their large design space. To the best of our knowledge, our proposed Predictor is the first that highlights the following three features simultaneously for practical and wide adoption: (1) analytical and thus fast; (2) covering both ASIC and FPGA DNN accelerators; (3) are validated using different DNN models and accelerator designs (i.e., architectures, dataflows, and process technologies).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">BACKGROUND DNN Accelerators.</head><p>There have been intensive studies of DNN accelerators. For example, the first well-optimized FPGA DNN accelerator <ref type="bibr" target="#b16">[16]</ref> uses loop tiling; the DianNao series <ref type="bibr" target="#b12">[13,</ref><ref type="bibr" target="#b17">17]</ref> is an early effort on synthesis based ASIC accelerators; Eyeriss proposes a row-stationary dataflow <ref type="bibr" target="#b13">[14]</ref> to reduce expensive DRAM accesses; and Google TPUs <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b11">12]</ref> use a systolic array to achieve high throughput. DNN Accelerator Performance Prediction. DNNs often feature a high complexity while there exists various opportunities for reuse, pipeline, and resource allocation to maximize DNN accelerators' performance. Therefore, an accurate yet fast performance predictor is desired to enable efficient design space exploration and optimization with different performance trade-offs. Various methods have been developed for predict-ing or simulating DNN accelerators' performance. Roofline models <ref type="bibr" target="#b16">[16,</ref><ref type="bibr" target="#b18">18]</ref> and customized analytical models which are closely tied to the specific design attributes <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b19">19,</ref><ref type="bibr" target="#b20">20]</ref> are used. However, the roofline model lack fine-grained estimation and customized models are not general as desired. Timeloop <ref type="bibr" target="#b21">[21]</ref> and Eyeriss <ref type="bibr" target="#b22">[22]</ref> use for and parallel-for to describe the temporal and spatial mapping of DNN accelerators. Specifically, Timeloop obtains the number of memory accesses and estimates the latency by calculating the maximum isolated execution cycle across all hardware IPs based on a double-buffering assumption. Accelergy <ref type="bibr" target="#b23">[23]</ref> proposes a configuration language to describe hardware architectures and depends on plug-ins, e.g., Timeloop, to calculate the energy as in <ref type="bibr" target="#b13">[14]</ref>. The work in <ref type="bibr" target="#b24">[24]</ref> adopts Halide <ref type="bibr" target="#b25">[25]</ref>, a domain-specific language for image processing applications, and proposes a modeling framework which is similar to that of <ref type="bibr" target="#b13">[14]</ref>. MAESTRO <ref type="bibr" target="#b26">[26]</ref> is the very first to adopt a data-centric approach.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">THE PROPOSED DNN-CHIP PREDICTOR</head><p>This section presents the proposed DNN-Chip Predictor which is an analytical modelling framework to formulate DNN inference accelerators' energy cost, latency, and throughput when employing different dataflows and hardware architectures. We first introduce the employed design space description method, and then describe the developed performance models. The advantages of the DNN-Chip Predictor are that it (1) matches well with actual implementation results (&lt;18%); ( <ref type="formula" target="#formula_1">2</ref>) is analytical and intuitive (directly ties to the DNN model and accelerator parameters), facilitating its ease of use for timeefficient design space exploration and optimization; and (3) is programmer friendly and compatible with commonly used DNN frameworks (e.g., Pytorch <ref type="bibr" target="#b27">[27]</ref>) thanks to its adopted generic description of DNN accelerators' design space.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Design Space Description</head><p>For modeling DNN accelerators' performance given their large design space, one critical question is how to describe the whole design space, i.e., cover all possible design choices, in a way that is easy to follow? For ease of use and better visualization, we adopt a nested for-loop description <ref type="bibr" target="#b13">[14]</ref> to describe the design space as shown in Fig. <ref type="figure">1</ref>. Specifically, we employ (1) the primitive, for, to describe the temporal operations of each process element (PE) as well as the temporal data tiling and mapping operations at the DRAM, global buffer (GB) and register file (RF) levels; and (2) the primitive, parallel-for, to describe the spatial data tiling and mapping operations at the network-on-chip (NoC) level (i.e., in the PE array). Without loss of generality, we consider four levels of memory hierarchy, i.e., off-chip DRAM, on-chip GB, NoC in the PE array, and RF within the PEs. The design space of DNN accelerators mainly includes two aspects: hardware architectures and dataflows.</p><p>Hardware architecture. It can be described using a set of architecture-dependent hardware parameters and technologydependent IP parameters. In particular, the architecturedependent hardware parameters includes PE array archi-Fig. <ref type="figure">1:</ref> A nested for-loop description of DNN accelerators' design space, using a CONV layer as an example, where 0,1,2,3 denotes the four memory hierarchies (i.e., RF, NoC, GB, and DRAM, respectively), and M, C, R, S, E, F denote the six dimensions of a CONV layer (i.e., input/output channels, kernel width/height, and output feature map width/height, respectively).</p><p>tectures (e.g., spatial array, systolic array, and adder tree), number of PEs, NoC design (e.g., unicast, multicast, or broadcast), memory hierarchies, and the storage capacity and communication bandwidth of each memory hierarchy; the technology-dependent IP parameters includes unit energy/delay costs of (1) a MAC operation, (2) memory accesses to various memory hierarchies, and (3) the clock frequency.</p><p>Dataflow. This describes how a DNN is temporally and spatially scheduled to be executed in an accelerator. Specifically, a dataflow answers the following questions: (1) how to map and schedule the computations in the PE array and within each PE?; and (2) what are the loop ordering and tiling factors on the DRAM and global buffer levels? The former captures the design choice of holding a certain type of data locally in the PE once being fetched from the memories, e.g., row/weight/output stationary. The latter shows how to store data in SRAM and DRAM to accommodate data stationary effectively. These two questions can be described using three groups of parameters as defined below in the context of the example in Fig. <ref type="figure">1</ref>: Loop ordering factors for the twenty-four nested for-loops associated with the six dimensions of the 3D convolution operation and the four considered memory hierarchies (i.e., DRAM, GB, NoC, and RF); Loop tiling factors for the twenty-four nested for-loops associated with the six dimensions of the 3D convolution operation and the four considered memory hierarchies; and Data access locations in which of the nested for-loops we refresh the on-chip GB and in-PE RFs for the activations and weights. 3.2. The DNN-Chip Predictor 3.2.1. Overview Fig. <ref type="figure" target="#fig_0">2</ref> shows a high-level view of the proposed DNN-Chip Predictor, which accepts DNN models (e.g., number of layers, layer structure, bit-precision, etc.), hardware architectures (e.g., memory hierarchy, number of PEs, NoC design, etc.), dataflows (e.g., row/weight/output stationary, loop tiling/ordering factors, etc.), and technology-dependent unit costs (e.g., unit energy/delay cost of a MAC operation and memory accesses to various memory hierarchies), and then outputs the estimated energy consumption, latency, and throughput when executing the DNN in a target accelerator. It thus can be used to (1) validate DNN accelerator techniques prior to the time-and cost-consuming DNN ASIC/FPGA accelerator implementation, and (2) perform time-efficient design space exploration and optimization.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.2.">The Proposed Analytical Models</head><p>This subsection introduces the Predictor's analytical models.</p><p>Energy Models. DNN accelerators' energy cost include both computational (E comp ) and data movement (E DM ) costs, where E comp = N M AC Ã— e M AC with N M AC denoting the total number of MACs in the DNN. Similarly, the data movement cost can be calculated by multiplying the unit energy cost per access (e DMi,j , j âˆˆ {I, O, W }) with the total number of accesses (N DMi,j , j âˆˆ {I, O, W }) to the i-th memory hierarchy (e.g., GB) using the j-th type of data (i.e., inputs (I), outputs (O), and weights (W )):</p><formula xml:id="formula_0">E DM = iâˆˆS M emory jâˆˆ{I,O,W } N DMi,j , Ã—e DMi,j<label>(1)</label></formula><p>where S M emory = {DRAM ) GB, GB ) N oC, N oC ) RF, RF ) P E} for inputs/weights; and S M emory = {DRAM â†” GB, GB â†” N oC, N oC â†” RF, RF â†” M AC} for outputs.</p><p>The key challenge is to obtain N DMi,j for various memory hierarchies and data types when using different DNN models, hardware architectures, and dataflows. We are the first to find that N DMi,j can be calculated as the product of the j-th data volume (V refi,j ) involved in each refresh and the total number of such refreshes (N refi,j ) for the i-th memory:</p><formula xml:id="formula_1">N DMi,j = N refi,j Ã— V refi,j<label>(2)</label></formula><p>To obtain N refi,j and V refi,j , we propose an intuitive methodology: we first (1) choose a refresh location, which can be straightforwardly decided once the dataflow is known, in the nested for-loops (see Fig. <ref type="figure">1</ref>) for a given data type; (2)</p><p>N refi,j is equal to the product of all the loop bounds in the for-loops above the refresh location; and (3) V refi,j is equal to the product of all the loop bounds in the for-loops below the refresh location and associated with the particular type of data. Once N refi,j and V refi,j are obtained, the energy can be calculated as:</p><formula xml:id="formula_2">E DRAM = jâˆˆ{I,O,W } N ref GB,j Ã— V ref GB,j Ã— e DM DRAM,j<label>(3)</label></formula><formula xml:id="formula_3">E GB = jâˆˆ{I,O,W } N ref RF,j Ã— V ref RF,j Ã— N P E M j Ã— e DM GB,j<label>(4)</label></formula><formula xml:id="formula_4">E N oC = jâˆˆ{I,O,W } N ref RF,j Ã— V ref RF,j Ã— N P E Ã— e DM N oC,j (5) E RF = jâˆˆ{I,O,W } N M AC Ã— e DM RF,j<label>(6)</label></formula><p>where N P E is the number of active PEs and M j is the number of PEs that share the same data.</p><p>Latency Models. Similarly, the latency of DNN accelerators can be formulated as:</p><formula xml:id="formula_5">L = L setup + max{L DRAM , L GB , L comp } (<label>7</label></formula><formula xml:id="formula_6">)</formula><p>where L comp , L DRAM , L GB , and L setup denote the latency of computation in the PE array, accessing the DRAM from the GB, accessing the GB from an RF in the PEs, and setting up the first set of the weights and inputs, respectively. Adopting N j bit -bit precision for inputs/outputs/weights is N j bit , j âˆˆ {I, O, W }, we have:</p><formula xml:id="formula_7">L comp = N M AC Ã— t comp<label>(8)</label></formula><formula xml:id="formula_8">L DRAM = max jâˆˆ{I,O,W } N ref GB,j Ã— V ref GB,j Ã— N j bit min{BW j GB , BW DRAM } (9) L GB = max jâˆˆ{I,O,W } N ref RF,j Ã— N ref RF,j Ã— N j bit Ã— N P E BW j GB (10) L setup = max(L DRAM , L GB )<label>(11)</label></formula><formula xml:id="formula_9">L DRAM = max jâˆˆ{I,W } V ref GB,j Ã— N j bit min{BW j,GB , BW DRAM }<label>(12)</label></formula><formula xml:id="formula_10">L GB = max jâˆˆ{I,W } N ref RF,j Ã— N j bit min{BW j,RF , BW j,GB }<label>(13)</label></formula><p>where BW j i is the memory bandwidth for the i-th memory hierarchy for the data type j âˆˆ {I, O, W }.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">EXPERIMENT RESULTS</head><p>We validate our proposed DNN-Chip Predictor by comparing its predicted performance with actual chip measured ones in <ref type="bibr" target="#b13">[14]</ref>, FPGA implementation results in <ref type="bibr" target="#b28">[28]</ref>, and synthesis results based on a commercial CMOS technology, under the same experiment settings (e.g., unit energy, clock frequency, DNN model, architecture design and dataflow, etc).</p><p>Validation against Chip Measurements. For this set of experiments, we compare our Predictor's predicted performance with Eyeriss's chip measurement results using their Fig. <ref type="figure">3</ref>: The # of (L) DRAM and (R) GB accesses in Eyeriss <ref type="bibr" target="#b29">[29]</ref> and our Predictor for AlexNet's CONV layers. normalized unit energy <ref type="bibr" target="#b13">[14]</ref>. First, Table <ref type="table" target="#tab_0">1</ref> compares the energy breakdown of AlexNet's first and fifth CONV layers (denoted as CONV1 and CONV5, respectively), showing that the maximum difference is 5.15% and 1.64%, respectively. Second, Fig. <ref type="figure">3</ref> compares the number of DRAM/GB accesses. The difference between the predicted number of DRAM accesses and Eyeriss's measured results is between 2.18% and 12.10%, while the difference in terms of GB accesses is between -0.70% and 17.66%. Our Predictor's predicted DRAM access number is smaller than that of Eyeriss because the RLC overhead of sparse activations depends on the input images and we lack the information about which set of images were used in Eyeriss's measurements. Additionally, Fig. <ref type="figure">3</ref> shows that the difference between the predicted number of GB accesses and Eyeriss's results is less than 5% except for the CONV1 layer where the relative larger prediction error is caused by its larger stride, which is 4. Specifically, a larger stride leads to lower utilization of inputs fetched from the GB, whereas our current Predictor considers the generic case where stride is 1 as it is more often seen in recent DNN models. For better prediction accuracy, our Predictor can be adjusted to cover cases with other stride values, i.e., more considered cases for the analytical models in Section 3.2.2.</p><p>Third, Fig. <ref type="figure">4</ref> compares the latency of executing AlexNet's five CONV layers, and shows that the predicted ones and Eye-Fig. <ref type="figure">4</ref>: Comparison on the inference latency from Eyeriss <ref type="bibr" target="#b29">[29]</ref> and our Predictor when running AlexNet. Validation against FPGA Implementation. We compare our Predictor's predicted latency with FPGA measured ones under the same DNN model and hardware configurations <ref type="bibr" target="#b31">[31]</ref>. Specifically, for the FPGA one we use the open source implementation of the award winner <ref type="bibr" target="#b31">[31]</ref> in a state-ofthe-art design contest <ref type="bibr" target="#b32">[32]</ref>. Fig. <ref type="figure" target="#fig_1">5</ref> shows that our Predictor's predicted latency differs from the FPGA-synthesized ones by â‰¤ 16.84%. Note that in FPGA implementations the GB can be partitioned into smaller chunks to be accessed simultaneously for increasing the parallelism and minimizing the latency. Our current models do not include the overhead of this partition, which is larger when the GB is partitioned into more chunks for layers with a larger size, leading to a larger prediction error for the CONV4/CONV5/CONV6 layers in Fig. <ref type="figure" target="#fig_1">5</ref>. Validation against Synthesis Results. Table <ref type="table" target="#tab_1">2</ref> compares the Predictor's energy breakdown with that from the synthesis results for AlexNet's CONV3-CONV5 layers when using an in-house dedicated accelerator using a commercial 65nm CMOS technology. It can be seen from Table <ref type="table" target="#tab_1">2</ref> that the difference between our Predictor's predicted energy breakdown and that from the synthesis results is less than 5.28%.</p><p>5. CONCLUSION To close the gap between the growing demand for dedicated DNN accelerators with various specifications and the timeconsuming and challenging DNN accelerator design, we develop DNN-Chip Predictor, which can efficiently and effectively predict an accelerator's energy, latency, and resource consumption. Such an analytical performance prediction tool will facilitate fast development of innovations for not only DNN accelerators but also hardware-aware efficient DNNs.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 2 :</head><label>2</label><figDesc>Fig. 2: A high-level view of the DNN-Chip Predictor.</figDesc><graphic url="image-1.png" coords="3,70.02,67.02,212.60,86.26" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 5 :</head><label>5</label><figDesc>Fig.5: Our Predictor's predicted latency and the FPGA measured one for the 7 CONV layers of SkyNet<ref type="bibr" target="#b28">[28]</ref>.</figDesc><graphic url="image-4.png" coords="4,55.84,625.23,240.94,81.96" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 :</head><label>1</label><figDesc>The energy breakdown from Eyeriss<ref type="bibr" target="#b29">[29]</ref> and our Predictor, for the CONV1 and CONV5 of AlexNet<ref type="bibr" target="#b30">[30]</ref>.</figDesc><table><row><cell>Layer</cell><cell>comp.</cell><cell>RF</cell><cell>NoC</cell><cell>GB</cell></row><row><cell></cell><cell cols="4">Meas. Pred. Meas. Pred. Meas. Pred. Meas. Pred.</cell></row><row><cell cols="5">CONV1 16.7% 18.7% 79.6% 74.4% 1.7% 4.8% 2.0% 2.0%</cell></row><row><cell>âˆ†</cell><cell>2.08%</cell><cell>-5.15%</cell><cell>3.10%</cell><cell>-0.03%</cell></row><row><cell cols="5">CONV5 7.3% 7.5% 80.3% 79.1% 5.3% 7.0% 7.0% 6.3%</cell></row><row><cell>âˆ†</cell><cell>0.26%</cell><cell>-1.16%</cell><cell>1.64%</cell><cell>-0.74%</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 :</head><label>2</label><figDesc>The energy breakdown from synthesized results and our Predictor for AlexNet's CONV3-CONV5 layers. CONV3 38.76 34.49 4.26 60.99 65.25 4.26 0.24 0.25 0.01 CONV4 39.46 34.28 5.19 60.28 65.45 5.16 0.25 0.27 0.02 CONV5 31.13 25.85 5.28 68.65 73.91 5.26 0.22 0.24 0.02riss's differ by â‰¤ 15.51%. The predicted latency is smaller than the measured one because our Predictor's analytical models do not consider the corner cycles when the memory accesses and computation can not be fully pipelined where processing stalls occur. Finally, the predicted throughput of executing AlexNet is 46.0 GOPS while the one measured by Eyeriss is 51.6 GOPS, showing a prediction error of â‰¤11%.</figDesc><table><row><cell>Layer</cell><cell>comp. (%)</cell><cell>RF(%)</cell><cell>GB(%)</cell></row><row><cell></cell><cell>Syn. Pred. âˆ†</cell><cell>Syn. Pred. âˆ†</cell><cell>Syn. Pred. âˆ†</cell></row></table></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">ACKNOWLEDGEMENT</head><p>The work is supported by the National Science Foundation (NSF) through the ECCS Division Of Electrical, Communication &amp; Cyber System (Award number: 1934767).</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Very deep convolutional networks for large-scale image recognition</title>
		<author>
			<persName><forename type="first">Karen</forename><surname>Simonyan</surname></persName>
		</author>
		<idno>abs/1409.1556</idno>
	</analytic>
	<monogr>
		<title level="j">CoRR</title>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">E2-Train: Training State-of-the-art CNNs with Over 80% Energy Savings</title>
		<author>
			<persName><forename type="first">Yue</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
				<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="5139" to="5151" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">The microsoft 2016 conversational speech recognition system</title>
		<author>
			<persName><forename type="first">Wayne</forename><surname>Xiong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Acoustics, Speech and Signal Processing</title>
				<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="5255" to="5259" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">On-demand deep model compression for mobile devices: A usage-driven model selection framework</title>
		<author>
			<persName><forename type="first">Sicong</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 16th Annual International Conference on Mobile Systems, Applications, and Services</title>
				<meeting>the 16th Annual International Conference on Mobile Systems, Applications, and Services</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="389" to="400" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Energynet: Energy-efficient dynamic inference</title>
		<author>
			<persName><forename type="first">Yue</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems (Workshop)</title>
				<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Fractional Skipping: Towards Finer-Grained Dynamic Inference</title>
		<author>
			<persName><forename type="first">Jianghao</forename><surname>Shen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The Thirty-Forth AAAI Conference on Artificial Intelligence</title>
				<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Deep k-Means: Re-Training and Parameter Sharing with Harder Cluster Assignments for Compressing Deep Convolutions</title>
		<author>
			<persName><forename type="first">Junru</forename><surname>Wu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Thirty-fifth International Conference on Machine Learning</title>
				<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Dual dynamic inference: Enabling more efficient, adaptive and controllable deep inference</title>
		<author>
			<persName><forename type="first">Yue</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Journal of Selected Topics in Signal Processing</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">DNNBuilder: an automated tool for building high-performance dnn hardware accelerators for FPGAs</title>
		<author>
			<persName><forename type="first">Xiaofan</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of ICCAD</title>
				<meeting>of ICCAD</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Predictivenet: An energy-efficient convolutional neural network via zero prediction</title>
		<author>
			<persName><forename type="first">Yingyan</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2017 IEEE International Symposium on Circuits and Systems (ISCAS)</title>
				<imprint>
			<date type="published" when="2017-05">May 2017</date>
			<biblScope unit="page" from="1" to="4" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">Edge TPU</title>
		<author>
			<persName><forename type="first">Google</forename><surname>Inc</surname></persName>
		</author>
		<ptr target="https://cloud.google.com/tpu/" />
		<imprint>
			<date type="published" when="2019-09-01">2019-09-01</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">Edge TPU</title>
		<author>
			<persName><forename type="first">Google</forename><surname>Inc</surname></persName>
		</author>
		<ptr target="https://coral.withgoogle.com/docs/edgetpu/faq/,ac-cessed2019-09-01" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Shidiannao: Shifting vision processing closer to the sensor</title>
		<author>
			<persName><forename type="first">Zidong</forename><surname>Du</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM SIGARCH Computer Architecture News. ACM</title>
		<imprint>
			<biblScope unit="volume">43</biblScope>
			<biblScope unit="page" from="92" to="104" />
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Eyeriss: A spatial architecture for energy-efficient dataflow for convolutional neural networks</title>
		<author>
			<persName><forename type="first">Yu-Hsin</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Architecture (ISCA)</title>
				<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m">ACM/IEEE 43th Annual International Symposium on</title>
				<imprint>
			<publisher>IEEE Press</publisher>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="367" to="379" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Eie: efficient inference engine on compressed deep neural network</title>
		<author>
			<persName><forename type="first">Song</forename><surname>Han</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2016 ACM/IEEE 43rd Annual International Symposium on Computer Architecture (ISCA</title>
				<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="243" to="254" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Optimizing fpga-based accelerator design for deep convolutional neural networks</title>
		<author>
			<persName><forename type="first">Chen</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2015 ACM/SIGDA International Symposium on Field-Programmable Gate Arrays</title>
				<meeting>the 2015 ACM/SIGDA International Symposium on Field-Programmable Gate Arrays</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="161" to="170" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Diannao: A small-footprint highthroughput accelerator for ubiquitous machine-learning</title>
		<author>
			<persName><forename type="first">Tianshi</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Sigplan Notices. ACM</title>
		<imprint>
			<biblScope unit="volume">49</biblScope>
			<biblScope unit="page" from="269" to="284" />
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Mlpat: A power, area, timing modeling framework for machine learning accelerators</title>
		<author>
			<persName><forename type="first">Tianqi</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The Second International Workshop on Domain Specific System Architecture (DOSSA)</title>
				<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Throughput-optimized fpga accelerator for deep convolutional neural networks</title>
		<author>
			<persName><forename type="first">Zhiqiang</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Transactions on Reconfigurable Technology and Systems (TRETS)</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page">17</biblScope>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">Scale-sim: Systolic cnn accelerator simulator</title>
		<author>
			<persName><forename type="first">Ananda</forename><surname>Samajdar</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1811.02883</idno>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Timeloop: A systematic approach to dnn accelerator evaluation</title>
		<author>
			<persName><forename type="first">Angshuman</forename><surname>Parashar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2019 IEEE International Symposium on Performance Analysis of Systems and Software (ISPASS)</title>
				<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="304" to="315" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<author>
			<persName><forename type="first">Yu-Hsin</forename><surname>Chen</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1807.07928</idno>
		<title level="m">Eyeriss v2: A flexible accelerator for emerging deep neural networks on mobile devices</title>
				<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">Accelergy: An architecture-level energy estimation methodology for accelerator designs</title>
		<author>
			<persName><forename type="first">Yannan</forename><surname>Wu</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title level="m" type="main">Dnn dataflow choice is overrated</title>
		<author>
			<persName><forename type="first">Xuan</forename><surname>Yang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1809.04070</idno>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Halide: a language and compiler for optimizing parallelism, locality, and recomputation in image processing pipelines</title>
		<author>
			<persName><forename type="first">Jonathan</forename><surname>Ragan-Kelley</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Acm Sigplan Notices</title>
		<imprint>
			<biblScope unit="volume">48</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="519" to="530" />
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Understanding reuse, performance, and hardware cost of dnn dataflows: A datacentric approach</title>
		<author>
			<persName><forename type="first">Hyoukjun</forename><surname>Kwon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 52nd Annual IEEE/ACM International Symposium on Microarchitecture</title>
				<meeting>the 52nd Annual IEEE/ACM International Symposium on Microarchitecture</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="754" to="768" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<title level="m" type="main">Automatic differentiation in pytorch</title>
		<author>
			<persName><forename type="first">Adam</forename><surname>Paszke</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Fpga/dnn co-design: An efficient design methodology for iot intelligence on the edge</title>
		<author>
			<persName><forename type="first">Cong</forename><surname>Hao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of DAC</title>
				<meeting>of DAC</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Eyeriss: An energy-efficient reconfigurable accelerator for deep convolutional neural networks</title>
		<author>
			<persName><forename type="first">Yu-Hsin</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Journal of Solid-State Circuits</title>
		<imprint>
			<biblScope unit="volume">52</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="127" to="138" />
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Imagenet classification with deep convolutional neural networks</title>
		<author>
			<persName><forename type="first">Alex</forename><surname>Krizhevsky</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
				<editor>
			<persName><forename type="first">C</forename><forename type="middle">J C</forename><surname>Pereira</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">L</forename><surname>Burges</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">K</forename><forename type="middle">Q</forename><surname>Bottou</surname></persName>
		</editor>
		<editor>
			<persName><surname>Weinberger</surname></persName>
		</editor>
		<imprint>
			<publisher>Curran Associates, Inc</publisher>
			<date type="published" when="2012">2012</date>
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="page" from="1097" to="1105" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
		<title level="m" type="main">Skynet: A champion model for dac-sdc on low power object detection</title>
		<author>
			<persName><forename type="first">Xiaofan</forename><surname>Zhang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1906.10327</idno>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
		<title level="m" type="main">Dac 2019 system design contest</title>
		<author>
			<persName><forename type="first">Nvidia</forename><surname>Xilinx</surname></persName>
		</author>
		<author>
			<persName><surname>Dji</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
