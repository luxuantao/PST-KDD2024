<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">EMISSARY: Enhanced Miss Awareness Replacement Policy for L2 Instruction Caching</title>
				<funder ref="#_EJKmArv #_TPtMVjh #_vPGGCpK #_Y8dwHjA">
					<orgName type="full">National Science Foundation</orgName>
				</funder>
				<funder>
					<orgName type="full">Intel</orgName>
				</funder>
				<funder ref="#_mKRaRuD #_FpkPFmM">
					<orgName type="full">U.S. Department of Energy, Office of Science, Office of Advanced Scientific Computing Research</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><roleName>Bhargav</roleName><forename type="first">Nayana</forename><forename type="middle">Prasad</forename><surname>Nagendra</surname></persName>
							<email>nayanaprasad.nagendra@arm.com</email>
						</author>
						<author>
							<persName><forename type="first">Bhargav</forename><forename type="middle">Reddy</forename><surname>Godala</surname></persName>
							<email>bgodala@princeton.edu</email>
						</author>
						<author>
							<persName><forename type="first">Ishita</forename><surname>Chaturvedi</surname></persName>
							<email>ishitac@princeton.edu</email>
						</author>
						<author>
							<persName><forename type="first">Atmn</forename><surname>Patel</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Svilen</forename><surname>Kanev</surname></persName>
							<email>skanev@google.com</email>
						</author>
						<author>
							<persName><forename type="first">Tipp</forename><surname>Moseley</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Jared</forename><surname>Stark</surname></persName>
							<email>jared.w.stark@intel.com</email>
						</author>
						<author>
							<persName><forename type="first">Gilles</forename><forename type="middle">A</forename><surname>Pokam</surname></persName>
							<email>gilles.a.pokam@intel.com</email>
						</author>
						<author>
							<persName><forename type="first">Simone</forename><surname>Campanoni</surname></persName>
							<email>simone.campanoni@northwestern.edu</email>
						</author>
						<author>
							<persName><forename type="first">David</forename><forename type="middle">I</forename><surname>August</surname></persName>
							<email>august@princeton.edu</email>
						</author>
						<author>
							<persName><forename type="first">Reddy</forename><surname>Godala</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Simone</forename><surname>Cam</surname></persName>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="institution">Princeton University</orgName>
								<address>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff1">
								<orgName type="institution">Princeton University</orgName>
								<address>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff2">
								<orgName type="institution">Northwestern University</orgName>
								<address>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff3">
								<orgName type="institution">Intel Corporation</orgName>
								<address>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff4">
								<orgName type="institution">Intel Corporation</orgName>
								<address>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff5">
								<orgName type="institution">Northwestern University</orgName>
								<address>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff6">
								<orgName type="institution">Princeton University</orgName>
								<address>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff7">
								<orgName type="institution">Princeton University</orgName>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff8">
								<orgName type="department">ISCA &apos;23</orgName>
								<address>
									<addrLine>June 17-21</addrLine>
									<postCode>2023</postCode>
									<settlement>Orlando</settlement>
									<region>FL</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">EMISSARY: Enhanced Miss Awareness Replacement Policy for L2 Instruction Caching</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="DOI">10.1145/3579371.3589097</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-01-03T09:04+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Cache Microarchitecture</term>
					<term>Cache Replacement Policy</term>
					<term>Cost-Aware Replacement Policy</term>
					<term>Instruction Caching</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>For decades, architects have designed cache replacement policies to reduce cache misses. Since not all cache misses affect processor performance equally, researchers have also proposed cache replacement policies focused on reducing the total miss cost rather than the total miss count. However, all prior cost-aware replacement policies have been proposed specifically for data caching and are either inappropriate or unnecessarily complex for instruction caching. This paper presents EMISSARY, the first cost-aware cache replacement family of policies specifically designed for instruction caching. Observing that modern architectures entirely tolerate many instruction cache misses, EMISSARY resists evicting those cache lines whose misses cause costly decode starvations. In the context of a modern processor with fetch-directed instruction prefetching and other aggressive front-end features, EMISSARY applied to L2 cache instructions delivers an impressive 3.24% geomean speedup (up to 23.7%) and a geomean energy savings of 2.1% (up to 17.7%) when evaluated on widely used server applications with large code footprints. This speedup is 21.6% of the total speedup obtained by an unrealizable L2 cache with a zero-cycle miss latency for all capacity and conflict instruction misses.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">INTRODUCTION</head><p>Caches play a vital role in improving processor performance. For many decades, research has focused on improving processor performance by reducing cache misses <ref type="bibr" target="#b8">[12,</ref><ref type="bibr" target="#b10">14,</ref><ref type="bibr" target="#b23">27,</ref><ref type="bibr" target="#b25">29,</ref><ref type="bibr" target="#b33">37,</ref><ref type="bibr" target="#b34">38,</ref><ref type="bibr" target="#b37">41,</ref><ref type="bibr" target="#b41">45,</ref><ref type="bibr" target="#b45">49,</ref><ref type="bibr" target="#b48">52]</ref>. One important way to reduce cache misses for a given cache size, line size, and associativity is to improve the cache replacement policy. The classic LRU (least recently used) replacement policy exploits temporal locality by evicting the line least recently accessed <ref type="bibr" target="#b44">[48,</ref><ref type="bibr" target="#b52">56]</ref>. B?l?dy's OPT algorithm achieves the minimum number of misses through ideal cache replacement, but it is not realizable because it requires perfect knowledge of future references <ref type="bibr" target="#b11">[15]</ref>. While theoretical in nature, OPT informs the design of many practical cache replacement policies <ref type="bibr" target="#b23">[27,</ref><ref type="bibr" target="#b25">29,</ref><ref type="bibr" target="#b37">41,</ref><ref type="bibr" target="#b45">49]</ref>. Almost all previously proposed cache replacement policies use only reference information.</p><p>Architects have long recognized that not all cache misses have identical costs <ref type="bibr" target="#b26">[30,</ref><ref type="bibr" target="#b27">31,</ref><ref type="bibr" target="#b31">35,</ref><ref type="bibr" target="#b40">44,</ref><ref type="bibr" target="#b46">50,</ref><ref type="bibr" target="#b51">55]</ref>. Cost-aware cache replacement policies recognize this and attempt to increase performance even at the cost of increased cache misses. For modern architectures, many misses do not impact performance at all. For example, an aggressive out-of-order processor can entirely tolerate many first-level data cache (L1D) misses without any negative performance impact <ref type="bibr" target="#b35">[39]</ref>. Likewise, many modern processors have decoupled front-ends with early fetch engines that can tolerate certain first-level instruction cache (L1I) misses without causing decode starvation, a state in which the head instruction of the queue feeding the decode stage is not yet available <ref type="bibr" target="#b20">[24,</ref><ref type="bibr" target="#b21">25,</ref><ref type="bibr" target="#b47">51]</ref>.</p><p>The optimal cost-aware replacement policy (CSOPT) is the perfectknowledge cost-aware cache replacement algorithm <ref type="bibr" target="#b27">[31]</ref>. Unfortunately, CSOPT, like OPT, is not realizable in practice. Prior research has produced realizable cost-aware data cache replacement policies <ref type="bibr" target="#b31">[35,</ref><ref type="bibr" target="#b40">44,</ref><ref type="bibr" target="#b46">50,</ref><ref type="bibr" target="#b51">55]</ref>. For example, the MLP-aware Linear (LIN) policy and related techniques prioritize lines that miss with lower memory-level parallelism (MLP) <ref type="bibr" target="#b46">[50]</ref>. Other techniques consider load criticality as approximated by a variety of methods <ref type="bibr" target="#b31">[35,</ref><ref type="bibr" target="#b40">44,</ref><ref type="bibr" target="#b51">55]</ref>. Since instruction fetch exhibits a different behavior, known costaware data cache replacement policies cannot be effectively applied to instruction caching.</p><p>This paper presents EMISSARY (Enhanced MISS-Awareness Replacement Policy) to address the lack of cost-aware cache replacement policies for instructions. Observing that modern architectures entirely tolerate many L1I misses, EMISSARY prioritizes instruction lines whose miss caused a decode starvation. These higher-priority lines are preserved in L2 upon eviction from L1I. By avoiding decode starvation, EMISSARY consistently improves performance and saves energy. EMISSARY has a minimal hardware footprint because it does not track history, coordinate with prefetchers, make predictions, or perform complex calculations.</p><p>A simple EMISSARY configuration on a baseline model with a fetch-directed instruction prefetcher (FDIP) front-end, yields a geomean speedup of 3.24% and proportional energy savings on 13 front-end-bound data center workloads. EMISSARY achieves 15% of the speedup obtainable by an unrealizable model with a zero-cycle miss latency for all capacity and conflict L2 instruction cache misses. Furthermore, these results are significant in the context of the aggressive front-end found in modern processors that are designed to tolerate instruction cache misses. The aggressive FDIP model used as the baseline for EMISSARY on its own provides an impressive 33.1% geomean speedup, creating a challenging environment for any further improvements. Prior work shows that achieving performance gains over FDIP's highly effective baseline is difficult <ref type="bibr" target="#b21">[25,</ref><ref type="bibr" target="#b22">26]</ref>. In fact, a non-realizable perfect prefetcher implemented over an FDIP baseline boosts performance by only 5.4% <ref type="bibr" target="#b21">[25,</ref><ref type="bibr" target="#b22">26]</ref>.</p><p>Section 2 gives a high-level overview of the family of EMISSARY techniques. Since EMISSARY is a bimodal technique (misses are treated in one of two ways), this overview is contextualized with prior bimodal techniques. That section also highlights the value of persisting this bimodality over a line's entire lifetime in the cache, a feature the authors believe to be an EMISSARY first. Section 3 quantifies the extent to which different instruction cache lines tend to have different decode starvation behaviors, an important characteristic contributing to EMISSARY's success. Using the observation that bimodal selection and treatment are orthogonal, Section 4 introduces a notation covering the space of EMISSARY techniques and related prior works. Using this notation, it describes the EMIS-SARY algorithm in detail. Section 5 outlines how and by how much these policies outperform prior work in both speed and energy. Since EMISSARY preserves instruction lines in the unified L2 cache, Section 6 explains how it moderates its use to leave sufficient space for data caching. Section 7 reviews the related work in-depth, and Section 8 concludes.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">AN OVERVIEW TOUR OF EMISSARY</head><p>This paper argues for treating instructions bimodally (i.e., treating instruction cache lines whose misses impact performance differently from those that do not). Crafting a performant, low-complexity, and energy-efficient cost-aware bimodal policy requires choosing both a suitable bimodal selection policy (i.e., how to set the mode) and a bimodal treatment policy (i.e., how to treat the modes differently). This section provides an overview tour of these elements using Figure <ref type="figure">1</ref> to illustrate their impact on the performance, L2 instruction MPKI, commit-path decode rate, L2 data MPKI, and commit-path issue rate for the tomcat <ref type="bibr" target="#b5">[8]</ref> benchmark. (Note: While ?4 defines the notation in parentheses and ?5 defines the experimental environment; they are not necessary to understand this section's discussion.) All policies apply only to L2 instruction lines rather than L1I because L1I misses are generally tolerated well in modern processors with aggressive front-ends. Also, the longer average reuse intervals in large programs make the L2 more appropriate ( ?3). All policies may use decode starvation (i.e., the decode stage stalls waiting for the head instruction in the queue to become available), and the issue queue empty condition to make decisions.</p><p>In Figure <ref type="figure">1</ref>, traditional LRU is labeled MRU Insert:Always as insertion is always into MRU (most recently used) position without any bimodality. Contrary to traditional LRU, a previously proposed mechanism, called LIP, is an insertion policy in an LRU replacement policy cache that always inserts lines in the LRU position instead <ref type="bibr" target="#b45">[49]</ref>. The BIP prior work adds bimodality to LIP using a 1/32 probability random signal <ref type="bibr" target="#b45">[49]</ref>. To add bimodality based on miss cost (i.e., decode starvation) to BIP, Figure <ref type="figure">1</ref> includes the MRU Insert:Starvation Decode Only policy. Unfortunately, as shown in Figure <ref type="figure">1</ref>, MRU Insert:Starvation Decode Only performs slightly worse than LRU. While this result suggests that decode starvation is not a valuable bimodal selection signal, we observe that the problem is not the bimodality but that the bimodal treatment of a line is short-lived, as it takes only a few subsequent references to remove any MRU/LRU position differentiation.</p><p>To preserve the effect of the starvation signal longer, EMISSARY cache replacement policies are persistently bimodal. Instead of differentiating solely at insertion, EMISSARY replacement policies treat high-priority lines (e.g., those whose misses caused decode starvation) differently for their entire lifetime in the cache. This is partly done by not allowing insertions of low-priority lines (e.g., from a miss without decode starvation) to evict any of up to ? protected high-priority lines in the set. Only high-priority lines may be protected in this way, but not all high-priority lines are protected (i.e., when more than ? lines in a set are high-priority). EMISSARY marks each line in the L1I cache with a priority bit, setting ? = 1 if high-priority. This bit is preserved in L2 upon eviction from L1I and does not change as long as the line remains in either cache. The EMISSARY configurations in Figure <ref type="figure">1</ref> support up to eight protected lines (? = 8) per set in the L2 cache, leaving eight ways in the 16-way cache available for low-priority instruction and data lines.</p><p>Having low-priority lines bypass the cache was not found to be effective, so all misses in EMISSARY result in an insertion. The performance and decode rate impact of persistence is highlighted by line a in Figure <ref type="figure">1</ref>, which shows the power of persistence (i.e., Persistent:Starvation Decode Only over MRU Insert:Always).</p><p>Since available instructions in the instruction queue can hide decode starvation, requiring both the decode starvation and the empty issue queue signals to be true for bimodal selection will likely result in a more judicious use of persistence. Line b in Figure <ref type="figure">1</ref> confirms this by highlighting the difference between the Persistent:Starvation (Decode + IQ Empty) and Persistent:Starvation Decode Only EMISSARY policies.</p><p>Preserving high-priority lines over longer periods keeps them available for future accesses. Lines used only once, regardless of miss cost, do not benefit from persistence and instead, unnecessarily consume valuable cache resources. Thus, to filter out single-use but high-priority cache lines from being selected for bimodal treatment, the Persistent: Starvation (Decode + IQ Empty) Random EMISSARY policy only marks misses with starvation conditions as high-priority with random probability 1/32. This highly selective EMISSARY policy further improves performance, as evidenced by line c. Interestingly, with this policy, the instruction decode rate is less than Persistent:Starvation (Decode + IQ Empty) since not all high-priority instruction lines are selected for bimodal treatment. However, higher performance and an increased issue rate result from reduced data cache misses from the better allocation of L2 resources between instructions and data.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">DECODE STARVATION BEHAVIOR</head><p>Instruction fetch is responsible for keeping the decode stage fed. If the processor could perfectly predict the target of every controlflow instruction, instruction fetch could issue all of its memory requests early enough to tolerate the latency to main memory without starving decode. Unfortunately, even the best branch predictors are not perfect. They are, however, quite good. Modern processor front-ends incorporate decoupled, aggressive fetch engines guided by excellent branch predictors, large BTBs, and pre-decoders <ref type="bibr" target="#b21">[25]</ref>. Such front-ends accurately fetch several tens or even hundreds of instructions early. Instruction decode queues filled this way can often tolerate L1I misses before emptying and leading to decode starvation; this is especially true when an L1I miss leads to an L2 hit. From the perspective of cost-aware cache replacement policies, keeping lines with tolerated L1I misses in the cache has less utility than keeping lines whose misses cause decode starvation. The key to making this work involves differentiating tolerated L1I misses from those that cause starvation.</p><p>Branch mispredictions invalidate early fetch work, requiring a flush of the processor pipeline. Re-steering the front-end takes time, and more time is necessary for fetch to run far enough ahead of decode to fill the instruction decode queue enough to tolerate L1I misses. This concept suggests a cache replacement policy based on proximity to poorly predicted branch targets. The authors' early explorations in this direction considered cache replacement policies that were either too complex, ineffective, or both. Partially, this was because not all branch mispredictions lead to decode starvation. Often the lines necessary after re-steer are in L1I despite the branch mispredict. For example, this is the case for near-target branches in which the mispredicted path and the committed path share the same L1I cache lines. The mispredicted path fetch (or prefetch) acts as a prefetch in such a scenario.</p><p>Beyond branch prediction, many factors interact to determine whether or not an L1I miss will cause decode starvation. For example, a stalled decode cannot starve. Decode stalls occur when the processor back-end cannot accept more instructions. When  decode stalls, it does not attempt to pull from the instruction queue, a necessary condition for starvation. While predicting starvation by its component factors is hard, observing starvation during execution is easy. Existing signals assert when starvation occurs (likewise for when the issue queue is empty). Furthermore, when decode starvation occurs, the address for which the decode is waiting is also known. All of this information is known many cycles before the line that missed under these circumstances is inserted into the cache. Knowing this information in advance does not mean that it is necessarily of value to a cost-aware cache replacement policy.</p><p>An instruction fetch that causes starvation must be a miss in L1I cache. These L1I misses could be served either from L2 cache, L3 cache, or main memory. Lines served from the L2 cache introduce fewer starvation cycles than the ones served from more remote levels. The first bar in Figure <ref type="figure" target="#fig_0">2</ref> depicts the distribution of reuse distance based on observed cache line accesses in the committed path across the datacenter workloads used in this study. Reuse distance is measured as the number of unique lines accessed between two access to the same line. The same line accessed consecutively is not counted. Reuse distances are categorized into three buckets -Short [0-100), Mid [100-5000), and Long [&gt;5000) Reuse. Short Reuse lines are likely to hit in L1I, Mid Reuse lines are likely to miss in L1I and hit in L2, and Long Reuse lines are likely to miss in L2. This predicted behavior is confirmed in the second bar showing the % of Long Reuse accesses that miss in L2. Overall, more than 90% of L2 misses are from Long Reuse lines.</p><p>The third bar in Figure <ref type="figure" target="#fig_0">2</ref> shows the interplay between these different categories of reuse lines and decode starvation. Interestingly, more than 90% of the starvation cycles are caused by Long Reuse lines, which account for less than 20% of all accesses. Thus, a small number of accesses contribute to the majority of starvation cycles -a property that can be utilized by a replacement policy. Consequently, EMISSARY at the L1I cache may have little value as the lines that cause the majority of starvations are Long Reuse lines, lines which the L1I cannot realistically preserve. Since the majority of starvations are caused by L2 miss lines, in this work, EMISSARY policies are applied only to instructions cached in L2 cache. The L1I does play a role in that only L1I misses causing starvation are treated as high-priority. A line's priority is only communicated to L2 cache once it is evicted from the L1I cache. Instruction lines cached in L2 are then guided by the EMISSARY replacement policy. In this way, Long Reuse lines that have caused starvations are now cached in L2 cache for longer. EMISSARY uses 2 bits per line to record priority and for TPLRU access in the L1I and L2 caches. It is 1024 bits in L1I (32kB cache with 64B line size) and 32,768 bits in L2 (1MB cache with 64B line size), a little over 4kB total.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">THE EMISSARY POLICIES</head><p>EMISSARY instruction cache replacement policies build on the lessons of ?3, namely that only lines that caused starvations will likely cause further more starvations in the future. An EMISSARY cache leverages this by holding on to starvation-causing lines for longer, even if they have been less recently accessed than other starvation-free lines. Thus, EMISSARY cache replacement policies are bimodal. Bimodal techniques have two orthogonal aspects: mode selection and mode treatment. These aspects are discussed in this section.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Mode Selection</head><p>The two modes of a bimodal cache replacement policy are referred to as high and low priority, respectively. Table <ref type="table" target="#tab_3">1</ref> shows the mode selection options for the space of realizable cache replacement algorithms referenced in this paper. These mode selection options are combined in Boolean equations. For example, S&amp;R(1/32) requires a missed line to have caused starvation (S) during the miss AND to have been the lucky one of 32 chosen by pseudo-random selection </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Mode Treatment</head><p>A meaningful bimodal cache replacement policy must treat lines differently based on the selected mode. Thus, the next aspect determines how high-priority lines are treated differently from lowpriority lines. All realizable cache replacement policies discussed here use one of the two bimodal behaviors shown in Table <ref type="table" target="#tab_2">2</ref>.</p><p>In the first, M, bimodality comes from inserting high-priority lines into the cache's MRU position while placing low-priority lines into the cache's LRU position <ref type="bibr" target="#b45">[49]</ref>. In the second, P(? ), is the EMISSARY behavior. It is described by Algorithm 1. P(? ) techniques do not act on priority at insertion. Instead, the priority is recorded as a priority bit (?) associated with each line that impacts eviction. High-priority lines have ? = 1, while low-priority lines have ? = 0. When inserting a line L into a P(? ) cache, if the number of highpriority lines in the set is less than or equal to the maximum ? ; if it is then, the line L to be inserted (regardless of priority) replaces the LRU of the low-priority lines. Thus, the step in line 2 may increase the number of high-priority lines in the cache but cannot reduce it. For insertions where the number of high-priority lines in the set is greater than the maximum ? , the cache evicts the LRU among the high-priority lines. Note that the number of high-priority lines is not reduced less than ? at any point without a separate reset mechanism. Table <ref type="table">3</ref>: Cache replacement policies explored</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Algorithm 1 The EMISSARY Eviction Policy</head><p>The EMISSARY treatment option is orthogonal to the specific LRU algorithm used. For lines 2 and 4 of Algorithm 1, finding the LRU among the low-priority or the high-priority lines can be calculated precisely from a true LRU algorithm. With a pseudo-LRU (PLRU) algorithm, however, keeping separate PLRU's for lowand high-priority lines limits the imprecision. The PLRU-based EMISSARY uses the Tree Pseudo-LRU (TPLRU) algorithms with separate trees for low-and high-priority lines. When a high-priority line is accessed, only the high-priority tree is updated. Likewise, for a low-priority line and tree. For eviction, the appropriate tree is used to find the line to replace, skipping any lines that do not match the priority criteria. TPLRU requires ???? -1 bits per tree. Section 2 explored EMISSARY with true LRU. The evaluations use the TPLRU implementation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Cache Replacement Policies</head><p>The top of Table <ref type="table">3</ref> shows the prior work and proposed bimodal cache replacement policies used in this work. Each policy is a combination of a mode selection option (individually or by combination with a Boolean expression) and a mode treatment option described earlier. The rest of Table <ref type="table">3</ref> lists other advanced policies used in the experimental comparison.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">EXPERIMENTAL EXPLORATION</head><p>This section describes the simulation infrastructure, machine model, and benchmarks used to evaluate EMISSARY in various ways.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Simulation Infrastructure and Machine Model</head><p>This study uses gem5, a popular cycle-accurate simulator <ref type="bibr" target="#b13">[17]</ref>, running a detailed CPU model in FS (Full System) mode, with a full Operating System (OS). Gem5 supports a checkpointing mechanism that creates reusable snapshots for later restarts. For datacenter workloads, collecting gem5 checkpoints itself can be a significant bottleneck in evaluating microarchitectural changes. To reduce this, we used the QEMU <ref type="bibr" target="#b12">[16]</ref> emulator and built a tool, FS_Lapidary, to create gem5-compatible snapshots. Snapshots consist of a dump of the physical memory, disk image, device state, CPU architectural state, and a checkpoint file compatible with gem5. Typically, checkpoint files can be only ported to another environment with the same system board configuration. We extended gem5 to support a hardware board configuration called virt_machine for snapshots created with QEMU. This enables the use of QEMU fast emulation features, like hardware acceleration with KVM <ref type="bibr" target="#b19">[23]</ref>. We used an ARM64 system running Ubuntu 18.04 with Linux kernel 4.15, and an Apple M1 Mac mini to accelerate QEMU emulation.</p><p>As shown in Table <ref type="table" target="#tab_4">4</ref>, we used an Intel's Alderlake-like model for all experiments with the TPLRU config. The next line prefetcher (NLP) is enabled in the Adlerlake-like model for the L1D, L2, and L3 caches. The baseline for all experiments has FDIP enabled. L3 is an exclusive cache with DRRIP. L2 uses an SFL (Served From Last-level) bit to track each line's origin (i.e., L3 or memory). When a line with its SFL bit set is evicted from L2, it is placed at the MRU position in the L3. Each simulation includes a warm-up period of 5 million instructions from the resumed state followed by a measurement period of 100 million instructions in the detailed simulation model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Decoupled Fetch Engine</head><p>We extended the fetch engine of the gem5 simulator to model the aggressive front-end found in modern processors with state-ofthe-art FDIP prefetchers <ref type="bibr" target="#b48">[52]</ref>. FDIP includes a Fetch Target Queue (FTQ) to decouple the fetch pipeline from the rest of the processor, enabling the fetch pipeline to run ahead of the rest of the processor pipeline <ref type="bibr" target="#b21">[25,</ref><ref type="bibr" target="#b48">52,</ref><ref type="bibr" target="#b49">53]</ref>. The fetch pipeline, including the BTB and FTQ, has been modified to operate at the dynamic basic block granularity.</p><p>We modified the BTB such that each entry corresponds to a basic block. In addition to the target, entries contain details pertaining to the basic block -starting address, size, and the type of control-flow instruction that ends the basic block. This enabled the BTB to be indexed based on the branch target or the basic block's starting address instead of the branch PC. We used ARM binaries in this work. ARM's fixed-length encoding made it easier to model the BTB. Specifically, given the starting address and size of the basic block in terms of the number of instructions, it is straightforward to find the address of the control-transfer instruction that ends the respective block. This flexibility helped in reducing the otherwise necessary changes to the branch predictor. Variable-width instructions can be supported with additional hardware.</p><p>The branch predictor and BTB enqueue up to one basic block prediction per cycle to the FTQ. As in the BTB, each entry in the FTQ contains the starting address and size of the dynamic basic block. Naturally, enqueuing stalls on BTB misses. The next two fall-through lines are prefetched in the event of a BTB miss. As an enhancement, the modeled front-end also includes a pre-decoder to update the BTB and minimize such enqueue stalls proactively. Branch re-steers flush the FTQ before resuming predictions on the corrected path. The FTQ along with basic-block level fetch enabled the front-end to run-ahead from the processor pipeline soon after every flush operation.</p><p>The FTQ enhancements allowed for the memory requests to be issued early. This work includes an FTQ size of 24 entries with a 192-instruction buffer. This offered the right balance by having sufficient starvation tolerance for hiding many L1I misses (see ?3) while keeping the front-end from becoming overly aggressive in the presence of branch mispredictions. The extended run-ahead frontend requires the instruction buffer to be able to receive memory responses out-of-order. Overall, our optimized FDIP provides a geomean speedup of 33.1% over a no FDIP model for the 13 datacenter benchmarks as described in Section 5.3.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3">Benchmarks</head><p>To evaluate EMISSARY, we used 13 popular server applications with large code footprints from various benchmark suites: tomcat (Apache's implementation of Jakarta Servlet, Jakarta Expression Language, and WebSocket <ref type="bibr" target="#b5">[8]</ref>, from Dacapo benchmark suite <ref type="bibr" target="#b14">[18]</ref>); kafka (Apache's distributed event streaming application used by companies like LinkedIn <ref type="bibr" target="#b3">[6]</ref>, from Dacapo benchmark suite); tpcc (On-Line Transaction Processing workload <ref type="bibr" target="#b6">[9]</ref>, from OLTP-Bench suite <ref type="bibr" target="#b15">[19]</ref>); wikipedia (MediaWiki application on Wikipedia dataset [3], from OLTP-Bench suite); data-serving (Cassandra NoSQL database application <ref type="bibr" target="#b2">[5]</ref>, from Cloudsuite V4 <ref type="bibr" target="#b17">[21]</ref>); media-streaming (Simulates video traffic, from Cloudsuite V4); web-search (Apache Solr search engine application <ref type="bibr" target="#b4">[7]</ref>, from Cloudsuite V4); xapian (a web-search application, Tailbench suite <ref type="bibr" target="#b28">[32]</ref>); specjbb (A SPEC benchmark to test Java application features <ref type="bibr" target="#b0">[1]</ref>, from Tailbench); finagle-http (Twitter's HTTP  <ref type="bibr" target="#b9">[13]</ref> simulating quick sort code); and speedometer2.0 (a Java Script benchmark runs on a web browser benchmark which tests for the number of threads spawned in a minute <ref type="bibr" target="#b1">[2]</ref>). Benchmarks from the Tailbench suite are compiled using the default flags provided by the suite. verilator benchmark was built from the code provided and also optimized further using Facebook's BOLT <ref type="bibr" target="#b42">[46]</ref> binary optimization tool. All benchmarks were built on the emulated environment described in Section 5.1. speedometer2.0 benchmark is simulated on a Chromium web browser.</p><p>Since all benchmarks except verilator are multithreaded, we scaled them to a single core (? = 1) for the evaluation on the simulator. To ensure that this simulation of a multithreaded workload is meaningful, we looked for performance trend differences between single core (? = 1) and multicore (? &gt; 1) thread scalings on a real x86 Linux host machine using hardware performance monitoring counters. We examined the data at the feature level (e.g., branch misprediction rate), at the overall performance level, and according to the methodology outlined in <ref type="bibr" target="#b53">[57]</ref>. We determined with confidence that the single-core scaling of these applications had the same workload characteristics as the ? = 4 and ? = 8 scalings. Software thread scheduling during simulation is handled by the Linux thread scheduler in Full System mode.</p><p>The benchmarks used exhibit various characteristics, as shown in Figure <ref type="figure">3</ref>. specjbb, kafka, and media-stream have very high L1D MPKI when compared to L1I MPKI. The average L1D MPKI is higher than the average L1I MPKI. media-stream and kafka benchmarks additionally have a higher L2 Data MPKI than L2 Instruction MPKI. However, the average L2 Instruction MPKI (9.63) is much larger than the Data counterpart (2.69). Figure <ref type="figure">4</ref> shows the instruction footprints of all benchmarks. Instruction footprints are measured based on the total number of unique cache lines accessed by the application during the simulation times the cache line size. tomcat has the highest footprint of 2.57 MB and xapian has the lowest footprint of 0.29 MB. The average footprint of the selected workloads is 1.05 MB. The chosen workloads were selected over the more traditional SPEC CPU workloads because they have larger code footprints and do not easily fit into the larger L2 caches of modern processors. Also, these benchmarks have been used in the most related works as well <ref type="bibr" target="#b29">[33,</ref><ref type="bibr" target="#b30">34]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.4">Policy Selection and Parameterization</head><p>Section 4 outlines a large space of possible cache replacement policies. To narrow the design space to a small and meaningful set of policies, using an initial exploration, we first select a small set of desirable policy types and then find a reasonable set of configuration parameters for these policy types. The useful representative policy types chosen are the ones listed in Table <ref type="table">3</ref>. Ideally, we would like to find a single value of ? and ? that works well across all policies. Based on prior work, we expect the best ? to be from 1/2 to 1/64 <ref type="bibr" target="#b45">[49]</ref>. For a 16-way cache, useful values of ? are from 2 to 14.</p><p>Table <ref type="table">5</ref> shows the geometric mean speedup across all programs for a ranch of ? and ? values. The "#Best" row (or column) indicates the number of best configurations found in each column (or row). An ? value of 1/32 consistently gives the best results in many cases. Prior work (M:R(? ), BIP <ref type="bibr" target="#b45">[49]</ref>) also suggests 1/32 or 1/64 as the value for ? . Generally, benchmarks reach peak performance when ? is 8, except verilator which continues to improve as ? goes to 14. Hence, we set ? = 8 and ? =1/32 for the evaluation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.5">Performance</head><p>Figure <ref type="figure" target="#fig_3">5</ref> shows the speedup versus MPKI (odd rows) and speedup versus change in starvation cycles (Decode + IQ Empty) for committed instructions (even rows). For space reasons, tpcc is omitted as its L2 instruction MPKI is quite low. Values of ? shown range Table <ref type="table">5</ref>: Geomean speedup with respect to a LRU + FDIP baseline (Alderlake model) across all configurations for various values of ? and ? when run on a system with EMISSARY Policy at L2 Cache from 0 to 14 by 2. An ? of 0 is equivalent the baseline. Lines connect P(? ) to P(? + 2) for each ? from 0 to 12. Generally, when MPKI is greater than 1.0, performance increases and starvations reduce as ? increases to 8 (i.e., half of the 16way cache is preserved for high-priority instruction lines). As ? increases further, the performance gains decrease despite the consistent starvation reduction. This is because the L2 cache is shared by instructions and data. As more ways get used by high-priority instruction lines, resources are constrained to data lines, leading to more back-end stalls. See ?5.8.</p><p>The results show that higher performance can come without much change in MPKI. This is the central observation of all costaware cache replacement policy proposals and this observation is confirmed for the EMISSARY techniques. Not all cache misses in modern out-of-order processors have the same cost. A significant portion of the misses can be tolerated without degrading processor performance. Similarly, a significant portion of the addresses that are latency-sensitive and cause decode starvations do so every time they are accessed. EMISSARY handles both of these categories very efficiently. It assigns higher priority to starvation-prone addresses, keeping them in the cache longer even if they are not accessed frequently. EMISSARY gives lower priority to latency-tolerant addresses, but it does cache them long enough to capture as much of their (belated) temporal locality.</p><p>The speedup and energy reduction of EMISSARY compared to other techniques over the TPLRU baseline is shown in Figure <ref type="figure">7</ref>. Overall, P(8):S&amp;E&amp;R(1/32), the preferred EMISSARY configuration, yields a geomean speedup of 2.49% on all benchmarks, with gains as high as 11.7% (verilator) and as low as -1% (finagle-chirper). Unlike others, EMISSARY does not show any significant slowdowns.</p><p>Figure <ref type="figure">7</ref> also shows that EMISSARY policies outperform all of the others in terms of speedup and energy savings. The preferred configuration, P(8):S&amp;E&amp;R(1/32), performs consistently better than P(8):S&amp;E. This is because the random filter tends to require lines to prove themselves with multiple starvations before being marked high-priority. This filters single reference lines very effectively, but it also filters single decode starvation lines just as well. This is important because high-priority reservations should be allocated to lines that starve with high probability and frequency, especially since an early single starvation is possible due to branch mispredictions and warm-up as new regions of code are executed.</p><p>Replacement policies such as SRRIP <ref type="bibr" target="#b25">[29]</ref>, BRRIP <ref type="bibr" target="#b25">[29]</ref>, and DR-RIP <ref type="bibr" target="#b25">[29]</ref> are designed to keep reused lines longer in the cache than the ones that are either used less frequently or have no reuse. Figure <ref type="figure">3</ref> shows that the hit rate at L2 is very high compared to the miss rate. In such a scenario, reused lines reach the highest priority state very quickly, and very often, this is the case at L2 in the datacenter workloads studied. When all ways in a cache set reach a high priority state, then the state of all lines is reset to a low priority state. In SRRIP policy, a newly inserted line would stay in the cache longer than in BRRIP policy, where only 3% of lines stay longer. BRRIP policy is detrimental when the newly inserted are evicted before they can be promoted to a higher priority state. A dynamic policy DRRIP is designed to reduce the negative effects of BRRIP and SRRIP. A dynamic policy dedicates a small sample (32 sets) to each policy and decides the winning policy based on the policy that contributes to fewer misses. Since the hit rate is much higher than the miss rate at L2, deciding the winner based on the miss rate is detrimental in the datacenter workloads studied. In a scenario where L2 capacity is limited EMISSARY identifies a small fraction of long reuse instruction lines that caused starvations in the processor pipeline and preserves them in the L2 cache longer.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.6">Contextualizing EMISSARY's Benefits</head><p>EMISSARY's impact is significant given how often the modeled architectures tolerate L1I misses <ref type="bibr" target="#b21">[25]</ref>. Prior work suggests that increasing the front-end performance of a modern processor with a properly tuned FDIP front-end is extremely difficult <ref type="bibr" target="#b21">[25,</ref><ref type="bibr" target="#b22">26]</ref>. These works show that FDIP alone outperforms the latest stand-alone prefetching policies such as EIP (one of the top prefetchers in IPC-1) by 2.2% <ref type="bibr" target="#b22">[26]</ref>. The authors further claim that a non-realizable Perfect prefetcher provides just 5.4% of the performance gains <ref type="bibr" target="#b21">[25,</ref><ref type="bibr" target="#b22">26,</ref><ref type="bibr" target="#b50">54]</ref>. The EIP-128KB prefetcher improves FDIP performance by 4.3% <ref type="bibr" target="#b21">[25,</ref><ref type="bibr" target="#b22">26,</ref><ref type="bibr" target="#b50">54]</ref>. It does this with a significant hardware storage cost of 128KB. In contrast, EMISSARY provides up to 3.24% with only 4KB of additional storage.</p><p>To further contextualize EMISSARY's performance, we compare EMISSARY to a perfect and unattainable model with zero cycle miss latency for all capacity and conflict instruction cache misses in the L2. The aforementioned zero cycle miss latency model achieves a geomean speedup of 15% over the FDIP baseline. EMISSARY achieves 21.6% of this unrealizable gain with only 4KB of additional state.</p><p>Finally, we also compare EMISSARY to DCLIP, DRRIP, and PDP. These techniques achieve geomean speedups of -2.48%, -2.9%, and -3.36%, respectively, when implemented on top of the FDIP baseline for the workloads studied in this work.   </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.7">Persistence, By Itself, Improves Hit Rate</head><p>Figure <ref type="figure" target="#fig_3">5</ref> shows that, in a majority of the programs, to a point, as ? increases, L2 Instruction MPKI proportionately reduces. In other words, EMISSARY techniques not only reduce starvation but MPKI as well. Even as the number of ways available to a significant fraction of low-priority addresses is reduced, misses decline as well. This was observed previously with the BIP technique <ref type="bibr" target="#b45">[49]</ref> as well.</p><p>With the prevalence of single reference (or extremely long time between reference) addresses, dedicating fewer cache resources to such lines makes way for lines that would otherwise miss. In this aspect, starvation acts as a filter, increasing the probability of isolating such lines by assigning them low-priority. Put another way, it helps reduce the extent to which these types of single reference lines can pollute the cache.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.8">Impact on Back-end Stalls</head><p>EMISSARY's impact on commit path front-end and back-end stall cycles is shown in Figure <ref type="figure" target="#fig_5">6</ref>. Specifically, it depicts the reduction in stall cycles of the preferred P(8):S&amp;E&amp;R(1/32) policy when compared to the TPLRU baseline. Across benchmarks, EMISSARY's  impact on front-end stalls is more evident than its impact on backend stalls. This is expected as EMISSARY is applied specifically to instruction lines. Interestingly, many benchmarks show an increase in back-end stalls but there is still an overall reduction in total stalls.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.9">Energy Savings</head><p>We used McPAT <ref type="bibr" target="#b36">[40]</ref> to model the energy consumption of different cache replacement policies explored. Fig. <ref type="figure">7</ref> shows energy savings for each benchmark and configuration. The energy savings are strongly correlated with the speedups achieved because of the relatively small amount of hardware added. In EMISSARY, there are only two bits added per cache line, one to mark the priority set once on insert and an additional one for TPLRU set on access. The EMISSARY P(8):S&amp;E&amp;R(1/32) configuration achieves a geomean reduction in the overall energy of 2.12% (up to 17.67%).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">BALANCING DATA LINES</head><p>The EMISSARY policy advocated for in this work has ? = 8 maximum ways reserved for high-priority instruction lines. In a 16-way L2 cache, this policy reserves up to half of the ways for instructions. As mentioned in ?4, once a cache with an EMISSARY policy reaches ? high-priority lines in a set, the number of high-priority lines can never be reduced. In this section, we study the number of sets in the L2 cache that get saturated by high-priority instruction lines (i.e., 8 lines in a set are dedicated to instructions) and propose methods to minimize their impact on caching data lines. Figure <ref type="figure">8</ref> shows the distribution of the number of cache lines occupied by high-priority lines when using the P(8):S&amp;E and P(8):S&amp;E&amp;R (1/32) policies among all sets in the L2 cache at the end of the simulation averaged over all programs. With P(8):S&amp;E, finagle-chirper, tomcat, tpcc, and verilator saturate (reaches ? ) for all sets. Less than 25% of all sets observe saturation with the highly selective (and more desirable) P(8):S&amp;E&amp;R(1/32) policy. In simulations of 1B instructions, resetting all ? = 1 bits every 128M instructions has a negligible impact on performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">RELATED WORK</head><p>Cache replacement algorithms have been of interest to academia and industry for decades. This section describes several cache replacement policies related to EMISSARY.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.1">Cost-Aware Cache Replacement Policies</head><p>Architects have long recognized that not all cache misses have the same performance cost. In light of this observation, several prior works have proposed cost-aware cache replacement policies that give deference to lines with higher miss costs while selecting a line to evict <ref type="bibr" target="#b31">[35,</ref><ref type="bibr" target="#b40">44,</ref><ref type="bibr" target="#b46">50,</ref><ref type="bibr" target="#b51">55]</ref>. All of these techniques target either data or shared caches. We are not aware of any proposed cost-aware replacement algorithms designed specifically for instruction caches.</p><p>CSOPT <ref type="bibr" target="#b27">[31]</ref> is the ideal cost-aware cache replacement algorithm. It is essentially B?l?dy's OPT augmented with cost awareness. Like OPT, CSOPT is also unrealizable.</p><p>Among the realizable cost-aware policies, MLP-aware replacement policies <ref type="bibr" target="#b46">[50]</ref> identify costly misses by observing memory-level parallelism. These techniques reduce the overall miss cost by attempting to reduce the number of isolated misses (i.e., misses that do not have MLP). The MLP LIN policy utilizes the miss status handling register (MSHR) as input to cost calculation hardware that uses fixed-point calculations. In contrast, there is no cost calculation in EMISSARY other than obtaining the already-existing starvation signal, which contributes to its energy efficiency. MLP LIN does not always outperform its baseline, LRU.</p><p>LACS <ref type="bibr" target="#b31">[35]</ref> is a cost-aware cache replacement policy for last-level caches. It calculates the cost by counting the number of instructions that the processor can execute while the miss is being serviced. After insertion, LACS uses reference information to adjust the priority. Like EMISSARY, LACS requires two bits per line. However, LACS requires a history table, counters, and enhancements to the MSHRs. Such additional design complexity is not required in EMISSARY .</p><p>Critical Cache <ref type="bibr" target="#b51">[55]</ref> proposes the use of a victim cache to give performance-critical loads a second chance. Critical loads are defined with heuristics related to the types of instructions executing near the load. Despite the additional hardware, a Critical Cache does not report performance gains over LRU.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.2">Policies Challenging LRU</head><p>Ignoring cost awareness, many techniques have been designed to keep important lines in the cache, such as the Least Frequently Used (LFU) replacement policy, various cache bypassing techniques <ref type="bibr" target="#b32">[36,</ref><ref type="bibr" target="#b38">42]</ref>, and others. In prior work, lines not meeting a threshold may still be inserted but in a way that has them closer to eviction <ref type="bibr" target="#b45">[49]</ref>.  These techniques proposed inserting the most recently used lines into the LRU position to improve performance even when working sets are thrashed. The policy was further improved by a schema that only inserted new lines into the MRU slot with a low, random probability. The final modification was a dynamic method, which chose between traditional LRU and the aforementioned random insertion policy, depending on which resulted in fewer misses for a given workload. The parallel between this cache replacement policy and EMISSARY is that both decide to prefer some lines over others in cache replacement based on a factor other than recency. Of course, EMISSARY prefers lines that are cost-effective to prefer in addition to randomness. The key difference, though, is that EMISSARY sets the preference with a relevant cost signal in addition to randomness. Thus, it not only benefits from requiring lines to prove themselves as commonly used before being deemed important to keep in the cache but also from making this determination based on a specific cost factor, consistently effective for many workloads.</p><p>GHRP <ref type="bibr" target="#b7">[11]</ref> is an instruction cache replacement policy focused on minimizing the number of misses by identifying dead blocks, which is orthogonal to ours. It uses the access history to identify if a block needs to be bypassed upon insertion. Else, a dead-block predictor is used to select the candidate for eviction. GHRP's deadblock prediction mechanism could be combined with EMISSARY to identify the low-priority dead blocks for eviction. Doing so might further improve the performance of EMISSARY .</p><p>Ripple <ref type="bibr" target="#b30">[34]</ref> is a software-only profile-guided technique to improve the performance of instruction cache. A cache line that is no longer required is identified in the offline analysis, and a cache line eviction instruction is inserted in the binary after the last access along all execution paths. Unlike EMISSARY, Ripple identifies lines that are no longer required and ensures they do not waste cache space. Ripple is complementary to EMISSARY and could be used alongside EMISSARY to get the best of both techniques.</p><p>In <ref type="bibr" target="#b24">[28]</ref>, the authors propose CLIP, a cache replacement policy for instruction lines. By modifying the re-reference predictions of instruction and data lines separately, they dynamically prioritize instructions in a cache when the instructions cause L2 cache contention. Unlike EMISSARY, CLIP prioritizes all instruction lines blindly, without confirming that a future miss would cause frontend stalls. Furthermore, by restricting the number of ways used for instruction lines, EMISSARY prevents contention between data and instruction in the L2.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.3">Special Caching Solutions</head><p>The high-priority lines identified by EMISSARY could be due to branches with a high misprediction rate. A special cache has been proposed (MRC <ref type="bibr" target="#b39">[43]</ref>) to mitigate this cost. It has also been adapted in commercial processors as a Misprediction Recovery Buffer (MRB) <ref type="bibr" target="#b18">[22]</ref>. These solutions were specifically designed to mitigate branch recovery cost on misprediction. These structures need to be small to keep up with the timing constraints as they sit on a critical path. These solutions are applicable when the reuse distance is short but in large code footprints reuse distance is long which cannot be fit into small structures. An MRC/MRB and EMISSARY can likely be used together with success as they address orthogonal problems (short vs. long reuse intervals).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8">CONCLUSION</head><p>This paper presents EMISSARY, a new family of cost-aware cachereplacement policies for instructions that are well-suited for the L2 cache. Observing that modern architectures completely tolerate many instruction cache misses, EMISSARY prioritizes, with persistence, inserted lines whose misses cause decode starvation over those whose misses did not. Without the need to track history, coordinate with prefetchers, make predictions, or perform complex calculations, EMISSARY consistently improves performance and saves energy while remaining simple to implement. This allows EMISSARY to achieve a geomean performance gain of 3.24% (up to 23.7%), and a geomean energy savings of 2.12% (up to 17.7%) over TPLRU on top of a state-of-the-art FDIP prefetcher to model the aggressive front-ends found in modern processors. This speedup is 21.6% of the total speedup obtained by an unrealizable model with an ideal L2 instruction cache, with a mere 4KB hardware budget.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: First bar: distribution of Short, Mid, and Long Reuse access lines. Second Bar: fraction of L2 Instruction Misses by Long Reuse lines. Third Bar: distribution of starvation cycles caused by Short, Mid, and Long Reuse lines.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 3 :Figure 4 :</head><label>34</label><figDesc>Figure 3: Average L1I, L1D, L2 Instruction, and L2 Data Cache MPKI of the benchmarks on the TPLRU + FDIP baseline.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 5 :</head><label>5</label><figDesc>Figure 5: Speedup vs. L2 Instruction MPKI and Speedup vs. Change in Decode Starvation cycles when issue queue is empty for instructions on the committed path. P(? ) techniques are shown as line segments with points corresponding to values of ? from 0 to 14 in increments of 2. Lines connect P(? ) to P(? + 2). TPLRU (? = 0) serves as the baseline.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>s p e c j b b x a p i a n f i n a g l e -h t t p f i n a g l e -c h i r p e</head><label></label><figDesc></figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 6 :</head><label>6</label><figDesc>Figure 6: Reduction in various stall types of P(8):S&amp;E&amp;R(1/32) with respect to the TPLRU + FDIP baseline policy</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 7 :Figure 8 :</head><label>78</label><figDesc>Figure 7: Speedup and Energy Reduction of a range of techniques relative to TPLRU + FDIP baseline policy</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>Figure 1: Speedup vs L2 Instruction MPKI, Decode Rate, L2 Data MPKI, and Issue Rate of various cache replacement policies for tomcat benchmark on a 1M 16-way L2 cache(with true LRU and no prefetchers).</figDesc><table><row><cell></cell><cell cols="3">MRU Insert:Always (LRU; Baseline; M:1)</cell><cell></cell><cell></cell><cell cols="3">Persistent:Starvation Decode Only (EMISSARY; P(8):S)</cell><cell></cell><cell cols="4">Persistent:Starvation (Decode + IQ Empty) Random (EMISSARY; P(8):S&amp;E&amp;R(1/32))</cell></row><row><cell></cell><cell cols="4">MRU Insert:Starvation Decode Only (M:S)</cell><cell></cell><cell cols="4">Persistent:Starvation (Decode + IQ Empty) (EMISSARY; P(8):S&amp;E)</cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>1.5%</cell><cell></cell><cell></cell><cell>1.5%</cell><cell></cell><cell></cell><cell></cell><cell>1.5%</cell><cell></cell><cell></cell><cell>1.5%</cell><cell></cell></row><row><cell></cell><cell>1.0% 1.2%</cell><cell>c</cell><cell></cell><cell>1.0% 1.2%</cell><cell></cell><cell></cell><cell>c</cell><cell>1.0% 1.2%</cell><cell></cell><cell>c</cell><cell>1.0% 1.2%</cell><cell></cell><cell>c</cell></row><row><cell>Speedup</cell><cell>0.2% 0.5% 0.8%</cell><cell>b</cell><cell>a</cell><cell>0.5% 0.8% 0.2%</cell><cell></cell><cell>a</cell><cell>b</cell><cell>0.5% 0.8% 0.2%</cell><cell>a</cell><cell>b</cell><cell>0.5% 0.8% 0.2%</cell><cell>a</cell><cell>b</cell></row><row><cell></cell><cell>0.0%</cell><cell></cell><cell></cell><cell>0.0%</cell><cell></cell><cell></cell><cell></cell><cell>0.0%</cell><cell></cell><cell></cell><cell>0.0%</cell><cell></cell></row><row><cell></cell><cell>-0.2%</cell><cell></cell><cell></cell><cell>-0.2%</cell><cell></cell><cell></cell><cell></cell><cell>-0.2%</cell><cell></cell><cell></cell><cell>-0.2%</cell><cell></cell></row><row><cell></cell><cell>11</cell><cell>12</cell><cell>13</cell><cell>14</cell><cell>1.58</cell><cell>1.60</cell><cell>1.62</cell><cell>5.5</cell><cell>6.0</cell><cell>6.5</cell><cell>1.27</cell><cell>1.28</cell><cell>1.29</cell></row><row><cell></cell><cell cols="4">L2 Instruction MPKI</cell><cell></cell><cell>Decode Rate</cell><cell></cell><cell></cell><cell cols="2">L2 Data MPKI</cell><cell></cell><cell>Issue Rate</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2 :</head><label>2</label><figDesc>Mode Treatment Options</figDesc><table><row><cell cols="2">Notation Description</cell></row><row><cell>1</cell><cell>Always High-Priority</cell></row><row><cell>0</cell><cell>Never High-Priority</cell></row><row><cell>R(? )</cell><cell>High-Priority with random probability ?</cell></row><row><cell>S</cell><cell>High-Priority, line miss causes decode starvation</cell></row><row><cell>E</cell><cell>High-Priority, line miss occurs with an empty issue</cell></row><row><cell></cell><cell>queue</cell></row><row><cell></cell><cell>Table 1: Mode Selection Options</cell></row><row><cell cols="2">Notation Description</cell></row><row><cell>M</cell><cell>Insert High-Priority lines in MRU position, other-</cell></row><row><cell></cell><cell>wise LRU</cell></row><row><cell>P(? )</cell><cell>Protect up to ? MRU High-Priority lines/set from</cell></row><row><cell></cell><cell>eviction</cell></row></table><note><p>(R(1/32)). EMISSARY policies all contain S in their mode selection equations. For all policies in this paper, the mode selection is determined once during cache line insertion. LRU can be thought of as a bimodal predictor degenerated to treat all inserted lines as high-priority by MRU position placement.</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>1 :</head><label>1</label><figDesc>if number of high-priority (? = 1) lines &lt;= ? then</figDesc><table><row><cell></cell><cell>Notation</cell><cell>Description</cell></row><row><cell></cell><cell>M:1</cell><cell>Always insert as MRU; Classic LRU; Baseline</cell></row><row><cell></cell><cell>M:0</cell><cell>Never insert as MRU (only as LRU); LRU In-</cell></row><row><cell></cell><cell></cell><cell>sertion Policy (LIP) [49]</cell></row><row><cell></cell><cell>M:R(? )</cell><cell>MRU insert with probability ? ; Bimodal Inser-</cell></row><row><cell></cell><cell></cell><cell>tion Policy (BIP) [49]</cell></row><row><cell></cell><cell>M:S&amp;E</cell><cell>MRU insert when starvation occurs and issue</cell></row><row><cell></cell><cell></cell><cell>queue is empty</cell></row><row><cell></cell><cell>M:S&amp;E&amp;R(? )</cell><cell>MRU insert when starvation occurs, issue</cell></row><row><cell></cell><cell></cell><cell>queue is empty and with probability ?</cell></row><row><cell></cell><cell>P(? ):R(? )</cell><cell>EMISSARY bimodal behavior only; high-</cell></row><row><cell></cell><cell></cell><cell>priority lines selected with probability ?</cell></row><row><cell></cell><cell>P(? ):S</cell><cell>EMISSARY: high-priority on starvation</cell></row><row><cell></cell><cell>P(? ):S&amp;E</cell><cell>EMISSARY: high-priority on starvation and</cell></row><row><cell></cell><cell></cell><cell>empty issue queue</cell></row><row><cell></cell><cell cols="2">P(? ):S&amp;E&amp;R(? ) EMISSARY: high-priority on starvation,</cell></row><row><cell></cell><cell></cell><cell>empty issue queue, and with probability ?</cell></row><row><cell></cell><cell>SRRIP</cell><cell>Static re-referrence interval prediction [29]</cell></row><row><cell></cell><cell>BRRIP</cell><cell>Bimodal re-referrence interval prediction</cell></row><row><cell></cell><cell></cell><cell>with probability (1/32)[29]</cell></row><row><cell></cell><cell>DRRIP</cell><cell>Dynamic re-referrence interval predic-</cell></row><row><cell></cell><cell></cell><cell>tion [29]</cell></row><row><cell></cell><cell>PDP</cell><cell>Static protective distance policy [20]</cell></row><row><cell></cell><cell>DCLIP</cell><cell>Dynamic Code Line Preservation [28]</cell></row><row><cell>2:</cell><cell>Evict the LRU among the low-priority (? = 0) lines</cell></row><row><cell cols="2">3: else</cell></row><row><cell>4:</cell><cell>Evict the LRU among high-priority lines</cell></row><row><cell cols="2">5: end if</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 4 :</head><label>4</label><figDesc>Processor configurations</figDesc><table><row><cell>Field \ Model</cell><cell>Alderlake-like</cell></row><row><cell>ISA</cell><cell>Aarch64 (64-bit ARM)</cell></row><row><cell>Private L1I, L2D</cell><cell>32kB (I), 64kB (D) NLP, 8-way</cell></row><row><cell>Caches</cell><cell>64B line size, 2 cycle hit TPLRU</cell></row><row><cell>Unified L2 Cache</cell><cell>1MB, 16-way, 64B line size</cell></row><row><cell></cell><cell>12 cycle hit, Inclusive NLP</cell></row><row><cell>Shared L3 Cache</cell><cell>2MB, 16-way, 64B line size 32 cycle</cell></row><row><cell></cell><cell>hit latency Exclusive Victim Cache</cell></row><row><cell></cell><cell>NLP DRRIP + SFL</cell></row><row><cell>Branch Predictor</cell><cell>TAGE, ITTAGE</cell></row><row><cell>BTB size</cell><cell>16K entries</cell></row><row><cell>Fetch Target Queue</cell><cell>24 entry 192-instruction</cell></row><row><cell>Fetch/Decode/</cell><cell>8 wide</cell></row><row><cell>Issue/Commit</cell><cell></cell></row><row><cell>ROB Entries</cell><cell>512</cell></row><row><cell cols="2">Issue/Load/Store Queue 240 / 128/ 72</cell></row><row><cell>Int/FP Registers</cell><cell>280 / 224</cell></row></table></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div><head>ACKNOWLEDGMENTS</head><p>We thank members of the <rs type="institution">Liberty Research Group, Arcana Research Lab, Intel, and Google</rs> for their support and feedback on this work. We also thank the anonymous reviewers for the comments and suggestions that made this work stronger. This material is based upon work supported by <rs type="funder">Intel</rs>. This material is based upon work supported by the <rs type="funder">U.S. Department of Energy, Office of Science, Office of Advanced Scientific Computing Research</rs>, under contract numbers <rs type="grantNumber">DE-SC0022138</rs>, and <rs type="grantNumber">DE-SC0022268</rs>. This material is based upon work supported by the <rs type="funder">National Science Foundation</rs> under Grant <rs type="grantNumber">CCF-2107257</rs>, <rs type="grantNumber">CCF-2118708</rs>, <rs type="grantNumber">CCF-2107042</rs>, and <rs type="grantNumber">CCF-1908488</rs>.</p></div>
			</div>
			<listOrg type="funding">
				<org type="funding" xml:id="_mKRaRuD">
					<idno type="grant-number">DE-SC0022138</idno>
				</org>
				<org type="funding" xml:id="_FpkPFmM">
					<idno type="grant-number">DE-SC0022268</idno>
				</org>
				<org type="funding" xml:id="_EJKmArv">
					<idno type="grant-number">CCF-2107257</idno>
				</org>
				<org type="funding" xml:id="_TPtMVjh">
					<idno type="grant-number">CCF-2118708</idno>
				</org>
				<org type="funding" xml:id="_vPGGCpK">
					<idno type="grant-number">CCF-2107042</idno>
				</org>
				<org type="funding" xml:id="_Y8dwHjA">
					<idno type="grant-number">CCF-1908488</idno>
				</org>
			</listOrg>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">SPECjbb</title>
		<ptr target="https://www.spec.org/jbb" />
		<imprint>
			<date type="published" when="2015">2015. 2015. 2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">Speedometer</title>
		<ptr target="https://browserbench.org/Speedometer2.0/" />
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">Apache cassandra</title>
		<ptr target="http://cassandra.apache.org/" />
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">Apache kafka</title>
		<ptr target="https://kafka.apache.org/" />
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">Apache Solr</title>
		<ptr target="https://solr.apache.org/" />
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">Apache tomcat</title>
		<ptr target="https://tomcat.apache.org/" />
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<ptr target="http://www.tpc.org/tpcc/" />
		<title level="m">TPC-C</title>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Exploring Predictive Replacement Policies for Instruction Cache and Branch Target Buffer</title>
		<author>
			<persName><forename type="first">Samira</forename><surname>Mirbagher Ajorpaz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Elba</forename><surname>Garza</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sangam</forename><surname>Jindal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Daniel</forename><forename type="middle">A</forename><surname>Jim?nez</surname></persName>
		</author>
		<idno type="DOI">10.1109/ISCA.2018.00050</idno>
		<ptr target="https://doi.org/10.1109/ISCA.2018.00050" />
	</analytic>
	<monogr>
		<title level="m">2018 ACM/IEEE 45th Annual International Symposium on Computer Architecture (ISCA)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">A Novel Methodology Using Genetic Algorithms for the Design of Caches and Cache Replacement Policy</title>
		<author>
			<persName><forename type="first">Vinod</forename><forename type="middle">K</forename><surname>Erik R Altman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Guang</forename><forename type="middle">R</forename><surname>Agarwal</surname></persName>
		</author>
		<author>
			<persName><surname>Gao</surname></persName>
		</author>
		<idno type="DOI">10.1109/WorldS450073.2020.9210347</idno>
		<ptr target="https://doi.org/10.1109/WorldS450073.2020.9210347" />
	</analytic>
	<monogr>
		<title level="m">ICGA</title>
		<imprint>
			<date type="published" when="1993">1993</date>
			<biblScope unit="page" from="392" to="399" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">The Rocket Chip Generator</title>
		<author>
			<persName><forename type="first">Krste</forename><surname>Asanovi?</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rimas</forename><surname>Avizienis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jonathan</forename><surname>Bachrach</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Scott</forename><surname>Beamer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><surname>Biancolin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christopher</forename><surname>Celio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Henry</forename><surname>Cook</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Daniel</forename><surname>Dabbelt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">John</forename><surname>Hauser</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Adam</forename><surname>Izraelevitz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sagar</forename><surname>Karandikar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ben</forename><surname>Keller</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Donggyu</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">John</forename><surname>Koenig</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yunsup</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Eric</forename><surname>Love</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Martin</forename><surname>Maas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Albert</forename><surname>Magyar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Howard</forename><surname>Mao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Miquel</forename><surname>Moreto</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Albert</forename><surname>Ou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><forename type="middle">A</forename><surname>Patterson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Brian</forename><surname>Richards</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Colin</forename><surname>Schmidt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stephen</forename><surname>Twigg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Huy</forename><surname>Vo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrew</forename><surname>Waterman</surname></persName>
		</author>
		<idno>UCB/EECS-2016-17</idno>
		<ptr target="http://www2.eecs.berkeley.edu/Pubs/TechRpts/2016/EECS-2016-17.html" />
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
		<respStmt>
			<orgName>EECS Department, University of California, Berkeley</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">Technical Report</note>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">AsmDB: Understanding and Mitigating Front-End Stalls in Warehouse-Scale Computers</title>
		<author>
			<persName><forename type="first">Grant</forename><surname>Ayers</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nayana</forename><forename type="middle">P</forename><surname>Nagendra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><forename type="middle">I</forename><surname>August</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kyu</forename><surname>Hyoun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Svilen</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christos</forename><surname>Kanev</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Trivikram</forename><surname>Kozyrakis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Heiner</forename><surname>Krishnamurthy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tipp</forename><surname>Litz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Parthasarathy</forename><surname>Moseley</surname></persName>
		</author>
		<author>
			<persName><surname>Ranganathan</surname></persName>
		</author>
		<idno type="DOI">10.1145/3307650.3322234</idno>
		<ptr target="https://doi.org/10.1145/3307650.3322234" />
	</analytic>
	<monogr>
		<title level="m">Computer Architecture (ISCA)</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">A study of replacement algorithms for a virtual-storage computer</title>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">A</forename><surname>Belady</surname></persName>
		</author>
		<idno type="DOI">10.1147/sj.52.0078</idno>
		<ptr target="https://doi.org/10.1147/sj.52.0078" />
	</analytic>
	<monogr>
		<title level="m">IBM Systems journal</title>
		<imprint>
			<date type="published" when="1966">1966</date>
			<biblScope unit="page" from="78" to="101" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">QEMU, a Fast and Portable Dynamic Translator</title>
		<author>
			<persName><forename type="first">Fabrice</forename><surname>Bellard</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Annual Conference on USENIX Annual Technical Conference</title>
		<meeting>the Annual Conference on USENIX Annual Technical Conference<address><addrLine>Anaheim, CA</addrLine></address></meeting>
		<imprint>
			<publisher>USENIX Association, USA</publisher>
			<date type="published" when="2005">2005</date>
			<biblScope unit="page">41</biblScope>
		</imprint>
	</monogr>
	<note>ATEC &apos;05)</note>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title/>
		<author>
			<persName><forename type="first">Nathan</forename><surname>Binkert</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bradford</forename><surname>Beckmann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gabriel</forename><surname>Black</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Steven</forename><forename type="middle">K</forename><surname>Reinhardt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ali</forename><surname>Saidi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Arkaprava</forename><surname>Basu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Joel</forename><surname>Hestness</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Derek</forename><forename type="middle">R</forename><surname>Hower</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tushar</forename><surname>Krishna</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Somayeh</forename><surname>Sardashti</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rathijit</forename><surname>Sen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Korey</forename><surname>Sewell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Muhammad</forename><surname>Shoaib</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nilay</forename><surname>Vaish</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mark</forename><forename type="middle">D</forename><surname>Hill</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><forename type="middle">A</forename><surname>Wood</surname></persName>
		</author>
		<idno type="DOI">10.1145/2024716.2024718</idno>
		<ptr target="https://doi.org/10.1145/2024716.2024718" />
	</analytic>
	<monogr>
		<title level="j">The Gem5 Simulator. SIGARCH Comput. Archit. News</title>
		<imprint>
			<date type="published" when="2011">2011. 2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<author>
			<persName><forename type="first">M</forename><surname>Stephen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Robin</forename><surname>Blackburn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chris</forename><surname>Garner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Asjad</forename><forename type="middle">M</forename><surname>Hoffmann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kathryn</forename><forename type="middle">S</forename><surname>Khang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rotem</forename><surname>Mckinley</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Amer</forename><surname>Bentzur</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Daniel</forename><surname>Diwan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Daniel</forename><surname>Feinberg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Samuel</forename><forename type="middle">Z</forename><surname>Frampton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Martin</forename><surname>Guyer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Antony</forename><surname>Hirzel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Maria</forename><surname>Hosking</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Han</forename><surname>Jump</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="middle">B</forename><surname>Eliot</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aashish</forename><surname>Moss</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Darko</forename><surname>Phansalkar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Thomas</forename><surname>Stefanovi?</surname></persName>
		</author>
		<author>
			<persName><surname>Vandrunen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ben</forename><surname>Daniel Von Dincklage</surname></persName>
		</author>
		<author>
			<persName><surname>Wiedermann</surname></persName>
		</author>
		<idno type="DOI">10.1145/1167473.1167488</idno>
		<ptr target="https://doi.org/10.1145/1167473.1167488" />
		<title level="m">Proceedings of the 21st Annual ACM SIGPLAN Conference on Object-Oriented Programming Systems, Languages, and Applications</title>
		<meeting>the 21st Annual ACM SIGPLAN Conference on Object-Oriented Programming Systems, Languages, and Applications<address><addrLine>Portland, Oregon, USA; New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computing Machinery</publisher>
			<date type="published" when="2006">2006</date>
			<biblScope unit="page" from="169" to="190" />
		</imprint>
	</monogr>
	<note>OOPSLA &apos;06)</note>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">OLTP-Bench: An Extensible Testbed for Benchmarking Relational Databases</title>
		<author>
			<persName><forename type="first">Andrew</forename><surname>Djellel Eddine Difallah</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Carlo</forename><surname>Pavlo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Philippe</forename><surname>Curino</surname></persName>
		</author>
		<author>
			<persName><surname>Cudre-Mauroux</surname></persName>
		</author>
		<idno type="DOI">10.14778/2732240.2732246</idno>
		<ptr target="https://doi.org/10.14778/2732240.2732246" />
	</analytic>
	<monogr>
		<title level="m">Proc. VLDB Endow</title>
		<meeting>VLDB Endow</meeting>
		<imprint>
			<date type="published" when="2013-12">2013. dec 2013</date>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page" from="277" to="288" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Improving cache management policies using dynamic reuse distances</title>
		<author>
			<persName><forename type="first">Nam</forename><surname>Duong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dali</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Taesu</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rosario</forename><surname>Cammarota</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mateo</forename><surname>Valero</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alexander</forename><forename type="middle">V</forename><surname>Veidenbaum</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2012 45Th annual IEEE/ACM international symposium on microarchitecture</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2012">2012</date>
			<biblScope unit="page" from="389" to="400" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<author>
			<persName><forename type="first">Almutaz</forename><surname>Michael Ferdman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Onur</forename><surname>Adileh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stavros</forename><surname>Kocberber</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mohammad</forename><surname>Volos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Djordje</forename><surname>Alisafaee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Cansu</forename><surname>Jevdjic</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Adrian</forename><forename type="middle">Daniel</forename><surname>Kaynak</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Anastasia</forename><surname>Popescu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Babak</forename><surname>Ailamaki</surname></persName>
		</author>
		<author>
			<persName><surname>Falsafi</surname></persName>
		</author>
		<idno type="DOI">10.1145/2248487.2150982</idno>
		<ptr target="https://doi.org/10.1145/2248487.2150982" />
		<title level="m">Clearing the Clouds: A Study of Emerging Scale-out Workloads on Modern Hardware. SIGPLAN Not</title>
		<imprint>
			<date type="published" when="2012-03">2012. mar 2012</date>
			<biblScope unit="volume">47</biblScope>
			<biblScope unit="page" from="37" to="48" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Evolution of the samsung exynos cpu microarchitecture</title>
		<author>
			<persName><forename type="first">Brian</forename><surname>Grayson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jeff</forename><surname>Rupley</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gerald</forename><surname>Zuraski Zuraski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Eric</forename><surname>Quinnell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Daniel</forename><forename type="middle">A</forename><surname>Jim?nez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tarun</forename><surname>Nakra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Paul</forename><surname>Kitchin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ryan</forename><surname>Hensley</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Edward</forename><surname>Brekelbaum</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vikas</forename><surname>Sinha</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2020 ACM/IEEE 47th Annual International Symposium on Computer Architecture (ISCA)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="40" to="51" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Virtualization with KVM</title>
		<author>
			<persName><forename type="first">Irfan</forename><surname>Habib</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Linux J</title>
		<imprint>
			<biblScope unit="volume">166</biblScope>
			<biblScope unit="issue">8</biblScope>
			<date type="published" when="2008-02">2008. 2008. feb 2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">Instruction fetch unit with early instruction fetch mechanism</title>
		<author>
			<persName><forename type="first">J</forename><surname>Glenn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Robert M Riches</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName><surname>Jr</surname></persName>
		</author>
		<ptr target="https://patents.google.com/patent/US5423014A/enUSPatent5" />
		<imprint>
			<date type="published" when="1995">1995</date>
			<biblScope unit="volume">423</biblScope>
			<biblScope unit="page">14</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Rebasing Instruction Prefetching: An Industry Perspective</title>
		<author>
			<persName><forename type="first">Yasuo</forename><surname>Ishii</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jaekyu</forename><surname>Lee</surname></persName>
		</author>
		<idno type="DOI">10.1109/LCA.2020.3035068</idno>
		<ptr target="https://doi.org/10.1109/LCA.2020.3035068" />
	</analytic>
	<monogr>
		<title level="j">IEEE Computer Architecture Letters</title>
		<imprint>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="page" from="147" to="150" />
			<date type="published" when="2020">2020. 2020</date>
		</imprint>
	</monogr>
	<note>Krishnendra Nathella, and Dam Sunwoo</note>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Reestablishing Fetch-Directed Instruction Prefetching: An Industry Perspective</title>
		<author>
			<persName><forename type="first">Yasuo</forename><surname>Ishii</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jaekyu</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Krishnendra</forename><surname>Nathella</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dam</forename><surname>Sunwoo</surname></persName>
		</author>
		<idno type="DOI">10.1109/ISPASS51385.2021.00034</idno>
		<ptr target="https://doi.org/10.1109/ISPASS51385.2021.00034" />
	</analytic>
	<monogr>
		<title level="m">2021 IEEE International Symposium on Performance Analysis of Systems and Software (ISPASS)</title>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="172" to="182" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Back to the Future: Leveraging Belady&apos;s Algorithm for Improved Cache Replacement</title>
		<author>
			<persName><forename type="first">Akanksha</forename><surname>Jain</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Calvin</forename><surname>Lin</surname></persName>
		</author>
		<idno type="DOI">10.1109/ISCA.2016.17</idno>
		<ptr target="https://doi.org/10.1109/ISCA.2016.17" />
	</analytic>
	<monogr>
		<title level="m">43rd International Symposium on Computer Architecture (ISCA)</title>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">High performing cache hierarchies for server workloads: Relaxing inclusion to capture the latency benefits of exclusive caches</title>
		<author>
			<persName><forename type="first">Aamer</forename><surname>Jaleel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Joseph</forename><surname>Nuzman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Adrian</forename><surname>Moga</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Simon</forename><forename type="middle">C</forename><surname>Steely</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Joel</forename><surname>Emer</surname></persName>
		</author>
		<idno type="DOI">10.1109/HPCA.2015.7056045</idno>
		<ptr target="https://doi.org/10.1109/HPCA.2015.7056045" />
	</analytic>
	<monogr>
		<title level="m">2015 IEEE 21st International Symposium on High Performance Computer Architecture (HPCA)</title>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="343" to="353" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">High Performance Cache Replacement Using Re-Reference Interval Prediction (RRIP)</title>
		<author>
			<persName><forename type="first">Aamer</forename><surname>Jaleel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kevin</forename><forename type="middle">B</forename><surname>Theobald</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Simon</forename><forename type="middle">C</forename><surname>Steely</surname><genName>Jr</genName></persName>
		</author>
		<author>
			<persName><forename type="first">Joel</forename><surname>Emer</surname></persName>
		</author>
		<idno type="DOI">10.1145/1815961.1815971</idno>
		<ptr target="https://doi.org/10.1145/1815961.1815971" />
	</analytic>
	<monogr>
		<title level="m">37th International Symposium on Computer Architecture (ISCA)</title>
		<imprint>
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Cost-sensitive cache replacement algorithms</title>
		<author>
			<persName><forename type="first">Jaeheon</forename><surname>Jeong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michel</forename><surname>Dubois</surname></persName>
		</author>
		<idno type="DOI">10.1109/HPCA.2003.1183550</idno>
		<ptr target="https://doi.org/10.1109/HPCA.2003.1183550" />
	</analytic>
	<monogr>
		<title level="m">The Ninth International Symposium on High-Performance Computer Architecture</title>
		<imprint>
			<date type="published" when="2003">2003. 2003</date>
			<biblScope unit="page" from="327" to="337" />
		</imprint>
	</monogr>
	<note>HPCA-9 2003</note>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Cache replacement algorithms with nonuniform miss costs</title>
		<author>
			<persName><forename type="first">Jaeheon</forename><surname>Jeong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michel</forename><surname>Dubois</surname></persName>
		</author>
		<idno type="DOI">10.1109/TC.2006.50</idno>
		<ptr target="https://doi.org/10.1109/TC.2006.50" />
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Comput</title>
		<imprint>
			<biblScope unit="volume">55</biblScope>
			<biblScope unit="page" from="353" to="365" />
			<date type="published" when="2006">2006. 2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">TailBench: A benchmark suite and evaluation methodology for latency-critical applications</title>
		<author>
			<persName><forename type="first">Harshad</forename><surname>Kasture</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Daniel</forename><surname>Sanchez</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Workload Characterization</title>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
	<note>IISWC</note>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Twig: Profile-guided btb prefetching for data center applications</title>
		<author>
			<persName><forename type="first">Ahmed</forename><surname>Tanvir</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nathan</forename><surname>Khan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Akshitha</forename><surname>Brown</surname></persName>
		</author>
		<author>
			<persName><surname>Sriraman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rakesh</forename><surname>Niranjan K Soundararajan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Joseph</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sreenivas</forename><surname>Devietti</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gilles</forename><forename type="middle">A</forename><surname>Subramoney</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Heiner</forename><surname>Pokam</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Baris</forename><surname>Litz</surname></persName>
		</author>
		<author>
			<persName><surname>Kasikci</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">MICRO-54: 54th Annual IEEE/ACM International Symposium on Microarchitecture</title>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="816" to="829" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Ripple: Profile-Guided Instruction Cache Replacement for Data Center Applications</title>
		<author>
			<persName><forename type="first">Tanvir</forename><surname>Ahmed Khan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dexin</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Akshitha</forename><surname>Sriraman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Joseph</forename><surname>Devietti</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gilles</forename><surname>Pokam</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Heiner</forename><surname>Litz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Baris</forename><surname>Kasikci</surname></persName>
		</author>
		<idno type="DOI">10.1109/ISCA52012.2021.00063</idno>
		<ptr target="https://doi.org/10.1109/ISCA52012.2021.00063" />
	</analytic>
	<monogr>
		<title level="m">2021 ACM/IEEE 48th Annual International Symposium on Computer Architecture (ISCA)</title>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="734" to="747" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">LACS: A locality-aware cost-sensitive cache replacement algorithm</title>
		<author>
			<persName><forename type="first">Mazen</forename><surname>Kharbutli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rami</forename><surname>Sheikh</surname></persName>
		</author>
		<idno type="DOI">10.1109/TC.2013.61</idno>
		<ptr target="https://doi.org/10.1109/TC.2013.61" />
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Comput</title>
		<imprint>
			<biblScope unit="volume">63</biblScope>
			<biblScope unit="page" from="1975" to="1987" />
			<date type="published" when="2014">2014. 2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Counter-based cache replacement and bypassing algorithms</title>
		<author>
			<persName><forename type="first">Mazen</forename><surname>Kharbutli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yan</forename><surname>Solihin</surname></persName>
		</author>
		<idno type="DOI">10.1109/TC.2007.70816</idno>
		<ptr target="https://doi.org/10.1109/TC.2007.70816" />
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Comput</title>
		<imprint>
			<biblScope unit="volume">57</biblScope>
			<biblScope unit="page" from="433" to="447" />
			<date type="published" when="2008">2008. 2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Blasting through the frontend bottleneck with shotgun</title>
		<author>
			<persName><forename type="first">Rakesh</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Boris</forename><surname>Grot</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vijay</forename><surname>Nagarajan</surname></persName>
		</author>
		<idno type="DOI">10.1145/3173162.3173178</idno>
		<ptr target="https://doi.org/10.1145/3173162.3173178" />
	</analytic>
	<monogr>
		<title level="m">Architectural Support for Programming Languages and Operating Systems (ASPLOS)</title>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Boomerang: A metadata-free architecture for control flow delivery</title>
		<author>
			<persName><forename type="first">Rakesh</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Cheng-Chieh</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Boris</forename><surname>Grot</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vijay</forename><surname>Nagarajan</surname></persName>
		</author>
		<idno type="DOI">10.1109/HPCA.2017.53</idno>
		<ptr target="https://doi.org/10.1109/HPCA.2017.53" />
	</analytic>
	<monogr>
		<title level="m">High Performance Computer Architecture (HPCA)</title>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">A large, fast instruction window for tolerating cache misses</title>
		<author>
			<persName><forename type="first">Jinson</forename><surname>Alvin R Lebeck</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tong</forename><surname>Koppanalil</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jaidev</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Eric</forename><surname>Patwardhan</surname></persName>
		</author>
		<author>
			<persName><surname>Rotenberg</surname></persName>
		</author>
		<idno type="DOI">10.1145/545214.545223</idno>
		<ptr target="https://doi.org/10.1145/545214.545223" />
	</analytic>
	<monogr>
		<title level="j">ACM SIGARCH Computer Architecture News</title>
		<imprint>
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="page" from="59" to="70" />
			<date type="published" when="2002">2002. 2002</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">McPAT: An integrated power, area, and timing modeling framework for multicore and manycore architectures</title>
		<author>
			<persName><forename type="first">S</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">H</forename><surname>Ahn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">D</forename><surname>Strong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">B</forename><surname>Brockman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">M</forename><surname>Tullsen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><forename type="middle">P</forename><surname>Jouppi</surname></persName>
		</author>
		<idno type="DOI">10.1145/1669112.1669172</idno>
		<ptr target="https://doi.org/10.1145/1669112.1669172" />
	</analytic>
	<monogr>
		<title level="m">2009 42nd Annual IEEE/ACM International Symposium on Microarchitecture (MICRO)</title>
		<imprint>
			<date type="published" when="2009">2009</date>
			<biblScope unit="page" from="469" to="480" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<monogr>
		<title level="m" type="main">An Imitation Learning Approach for Cache Replacement</title>
		<author>
			<persName><forename type="first">Evan Zheran</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Milad</forename><surname>Hashemi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kevin</forename><surname>Swersky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Junwhan</forename><surname>Parthasarathy Ranganathan</surname></persName>
		</author>
		<author>
			<persName><surname>Ahn</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2006.16239</idno>
		<ptr target="https://arxiv.org/abs/2006.16239" />
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">A survey of techniques for approximate computing</title>
		<author>
			<persName><forename type="first">Sparsh</forename><surname>Mittal</surname></persName>
		</author>
		<idno type="DOI">10.1145/2893356</idno>
		<ptr target="https://doi.org/10.1145/2893356" />
	</analytic>
	<monogr>
		<title level="j">ACM Computing Surveys (CSUR)</title>
		<imprint>
			<biblScope unit="volume">48</biblScope>
			<biblScope unit="page" from="1" to="33" />
			<date type="published" when="2016">2016. 2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">The misprediction recovery cache</title>
		<author>
			<persName><forename type="first">James</forename><forename type="middle">O</forename><surname>Ashwini K Nanda</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Simonjit</forename><surname>Bondi</surname></persName>
		</author>
		<author>
			<persName><surname>Dutta</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International journal of parallel programming</title>
		<imprint>
			<biblScope unit="volume">26</biblScope>
			<biblScope unit="page" from="383" to="415" />
			<date type="published" when="1998">1998. 1998</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Criticality aware tiered cache hierarchy: a fundamental relook at multi-level cache hierarchies</title>
		<author>
			<persName><forename type="first">Anant</forename><surname>Vithal Nori</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jayesh</forename><surname>Gaur</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Siddharth</forename><surname>Rai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sreenivas</forename><surname>Subramoney</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hong</forename><surname>Wang</surname></persName>
		</author>
		<idno type="DOI">10.1109/ISCA.2018.00019</idno>
		<ptr target="https://doi.org/10.1109/ISCA.2018.00019" />
	</analytic>
	<monogr>
		<title level="m">2018 ACM/IEEE 45th Annual International Symposium on Computer Architecture (ISCA)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="96" to="109" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<monogr>
		<title level="m" type="main">Fido: A cache that learns to fetch</title>
		<author>
			<persName><forename type="first">Mark</forename><surname>Palmer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stanley</forename><forename type="middle">B</forename><surname>Zdonik</surname></persName>
		</author>
		<ptr target="http://vldb.org/conf/1991/P255.PDF" />
		<imprint>
			<date type="published" when="1991">1991</date>
		</imprint>
		<respStmt>
			<orgName>Brown University, Department of Computer Science</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">BOLT: A Practical Binary Optimizer for Data Centers and Beyond</title>
		<author>
			<persName><forename type="first">Maksim</forename><surname>Panchenko</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rafael</forename><surname>Auler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bill</forename><surname>Nell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Guilherme</forename><surname>Ottoni</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2019 IEEE/ACM International Symposium on Code Generation and Optimization</title>
		<meeting>the 2019 IEEE/ACM International Symposium on Code Generation and Optimization<address><addrLine>Washington, DC, USA</addrLine></address></meeting>
		<imprint>
			<publisher>IEEE Press</publisher>
			<date type="published" when="2019">2019. 2019</date>
			<biblScope unit="page" from="2" to="14" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Renaissance: Benchmarking Suite for Parallel Applications on the JVM</title>
		<author>
			<persName><forename type="first">Aleksandar</forename><surname>Prokopec</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrea</forename><surname>Ros?</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><surname>Leopoldseder</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gilles</forename><surname>Duboscq</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Petr</forename><surname>T?ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Martin</forename><surname>Studener</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lubom?r</forename><surname>Bulej</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yudi</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alex</forename><surname>Villaz?n</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Doug</forename><surname>Simon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Thomas</forename><surname>W?rthinger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Walter</forename><surname>Binder</surname></persName>
		</author>
		<idno type="DOI">10.1145/3314221.3314637</idno>
		<ptr target="https://doi.org/10.1145/3314221.3314637" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 40th ACM SIGPLAN Conference on Programming Language Design and Implementation</title>
		<meeting>the 40th ACM SIGPLAN Conference on Programming Language Design and Implementation<address><addrLine>Phoenix, AZ, USA; New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computing Machinery</publisher>
			<date type="published" when="2019">2019. 2019</date>
			<biblScope unit="page" from="31" to="47" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<monogr>
		<title level="m" type="main">ANALYSIS OF CACHE REPLACEMENT-ALGORITHMS</title>
		<author>
			<persName><forename type="first">Thomas</forename><surname>Roberts</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Puzak</forename></persName>
		</author>
		<ptr target="https://scholarworks.umass.edu/dissertations/AAI8509594/" />
		<imprint>
			<date type="published" when="1986">1986. 1986</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Adaptive Insertion Policies for High Performance Caching</title>
		<author>
			<persName><forename type="first">K</forename><surname>Moinuddin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aamer</forename><surname>Qureshi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yale</forename><forename type="middle">N</forename><surname>Jaleel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Simon</forename><forename type="middle">C</forename><surname>Patt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Joel</forename><surname>Steely</surname><genName>Jr</genName></persName>
		</author>
		<author>
			<persName><surname>Emer</surname></persName>
		</author>
		<idno type="DOI">10.1145/1273440.1250709</idno>
		<ptr target="https://doi.org/10.1145/1273440.1250709" />
	</analytic>
	<monogr>
		<title level="m">34th International Symposium on Computer Architecture (ISCA)</title>
		<imprint>
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">A case for MLP-aware cache replacement</title>
		<author>
			<persName><forename type="first">K</forename><surname>Moinuddin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Daniel</forename><forename type="middle">N</forename><surname>Qureshi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Onur</forename><surname>Lynch</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yale</forename><forename type="middle">N</forename><surname>Mutlu</surname></persName>
		</author>
		<author>
			<persName><surname>Patt</surname></persName>
		</author>
		<idno type="DOI">10.1109/ISCA.2006.5</idno>
		<ptr target="https://doi.org/10.1109/ISCA.2006.5" />
	</analytic>
	<monogr>
		<title level="m">33rd International Symposium on Computer Architecture (ISCA&apos;06)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2006">2006</date>
			<biblScope unit="page" from="167" to="178" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">A Scalable Front-End Architecture for Fast Instruction Delivery</title>
		<author>
			<persName><forename type="first">Glenn</forename><surname>Reinman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Todd</forename><surname>Austin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Brad</forename><surname>Calder</surname></persName>
		</author>
		<idno type="DOI">10.1145/307338.300999</idno>
		<ptr target="https://doi.org/10.1145/307338.300999" />
	</analytic>
	<monogr>
		<title level="j">SIGARCH Comput. Archit. News</title>
		<imprint>
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="234" to="245" />
			<date type="published" when="1999-05">1999. May 1999</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Fetch directed instruction prefetching</title>
		<author>
			<persName><forename type="first">Glenn</forename><surname>Reinman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Brad</forename><surname>Calder</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Todd</forename><surname>Austin</surname></persName>
		</author>
		<idno type="DOI">10.1109/MICRO.1999.809439</idno>
		<ptr target="https://doi.org/10.1109/MICRO.1999.809439" />
	</analytic>
	<monogr>
		<title level="m">Microarchitecture (MICRO)</title>
		<imprint>
			<date type="published" when="1999">1999</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Optimizations enabled by a decoupled front-end architecture. Computers</title>
		<author>
			<persName><forename type="first">Glenn</forename><surname>Reinman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Brad</forename><surname>Calder</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Todd</forename><surname>Austin</surname></persName>
		</author>
		<idno type="DOI">10.1109/12.919279</idno>
		<ptr target="https://doi.org/10.1109/12.919279" />
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on</title>
		<imprint>
			<biblScope unit="volume">50</biblScope>
			<biblScope unit="page" from="338" to="355" />
			<date type="published" when="2001-05">2001. 05 2001</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">The Entangling Instruction Prefetcher</title>
		<author>
			<persName><forename type="first">Alberto</forename><surname>Ros</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alexandra</forename><surname>Jimborean</surname></persName>
		</author>
		<idno type="DOI">10.1109/LCA.2020.3002947</idno>
		<ptr target="https://doi.org/10.1109/LCA.2020.3002947" />
	</analytic>
	<monogr>
		<title level="j">IEEE Computer Architecture Letters</title>
		<imprint>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="page" from="84" to="87" />
			<date type="published" when="2020">2020. 2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Locality vs. criticality</title>
		<author>
			<persName><forename type="first">T</forename><surname>Srikanth</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Roy</forename><surname>Srinivasan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alvin</forename><forename type="middle">R</forename><surname>Dz-Ching Ju</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chris</forename><surname>Lebeck</surname></persName>
		</author>
		<author>
			<persName><surname>Wilkerson</surname></persName>
		</author>
		<idno type="DOI">10.1145/379240.379258</idno>
		<ptr target="https://doi.org/10.1145/379240.379258" />
	</analytic>
	<monogr>
		<title level="m">Proceedings 28th Annual International Symposium on Computer Architecture</title>
		<meeting>28th Annual International Symposium on Computer Architecture</meeting>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2001">2001</date>
			<biblScope unit="page" from="132" to="143" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">Benchmark synthesis using the LRU cache hit function</title>
		<author>
			<persName><forename type="first">Shing</forename><surname>Wing</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Robert</forename><forename type="middle">J T</forename><surname>Wong</surname></persName>
		</author>
		<author>
			<persName><surname>Morris</surname></persName>
		</author>
		<idno type="DOI">10.1109/12.2202</idno>
		<ptr target="https://doi.org/10.1109/12.2202" />
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Comput</title>
		<imprint>
			<biblScope unit="volume">37</biblScope>
			<biblScope unit="page" from="637" to="645" />
			<date type="published" when="1988">1988. 1988</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">A top-down method for performance analysis and counters architecture</title>
		<author>
			<persName><forename type="first">Ahmad</forename><surname>Yasin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2014 IEEE International Symposium on Performance Analysis of Systems and Software (ISPASS)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="35" to="44" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
