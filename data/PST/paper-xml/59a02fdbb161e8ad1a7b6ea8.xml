<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Credit-Scheduled Delay-Bounded Congestion Control for Datacenters</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Inho</forename><surname>Cho</surname></persName>
							<email>inho00@kaist.ac.kr</email>
						</author>
						<author>
							<persName><forename type="first">Keon</forename><surname>Jang</surname></persName>
							<email>keonjang@google.com</email>
						</author>
						<author>
							<persName><forename type="first">Dongsu</forename><surname>Han</surname></persName>
							<email>dongsuh@ee.kaist.ac.kr</email>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="institution">Google Inc</orgName>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff1">
								<address>
									<addrLine>SIGCOMM &apos;17, August 21-25</addrLine>
									<postCode>2017</postCode>
									<settlement>Los Angeles</settlement>
									<region>CA</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Credit-Scheduled Delay-Bounded Congestion Control for Datacenters</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">29ECD4A6C21CA65ABB752465451684EC</idno>
					<idno type="DOI">10.1145/3098822.3098840</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.3" ident="GROBID" when="2023-07-28T13:34+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Congestion Control</term>
					<term>Datacenter Network</term>
					<term>Credit-based</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Small RTTs (∼tens of microseconds), bursty flow arrivals, and a large number of concurrent flows (thousands) in datacenters bring fundamental challenges to congestion control as they either force a flow to send at most one packet per RTT or induce a large queue build-up. The widespread use of shallow buffered switches also makes the problem more challenging with hosts generating many flows in bursts. In addition, as link speeds increase, algorithms that gradually probe for bandwidth take a long time to reach the fairshare. An ideal datacenter congestion control must provide 1) zero data loss, 2) fast convergence, 3) low buffer occupancy, and 4) high utilization. However, these requirements present conflicting goals.</p><p>This paper presents a new radical approach, called ExpressPass, an end-to-end credit-scheduled, delay-bounded congestion control for datacenters. ExpressPass uses credit packets to control congestion even before sending data packets, which enables us to achieve bounded delay and fast convergence. It gracefully handles bursty flow arrivals. We implement ExpressPass using commodity switches and provide evaluations using testbed experiments and simulations. ExpressPass converges up to 80 times faster than DCTCP in 10 Gbps links, and the gap increases as link speeds become faster. It greatly improves performance under heavy incast workloads and significantly reduces the flow completion times, especially, for small and medium size flows compared to RCP, DCTCP, HULL, and DX under realistic workloads.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>CCS CONCEPTS</head><p>• Networks → Transport protocols;</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">INTRODUCTION</head><p>Datacenter networks are rapidly growing in terms of size and link speed <ref type="bibr" target="#b51">[52]</ref>. A large datacenter network connects over 100 thousand machines using a Clos network of shallow buffered switches <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b27">28]</ref>. Each server is connected at 10/40 Gbps today with 100 Gbps on the horizon. This evolution enables low latency and high bandwidth communication within a datacenter. At the same time, it poses a unique set of challenges for congestion control.</p><p>In datacenters, short propagation delay makes queuing delay a dominant factor in end-to-end latency <ref type="bibr" target="#b2">[3]</ref>. Thus, with higher link speeds, fast convergence has become much more important <ref type="bibr" target="#b33">[34]</ref>. However, with buffer per port per Gbps getting smaller, ensuring zero loss and rapid convergence with traditional congestion control has become much more challenging. In addition, Remote Direct Memory Access (RDMA), recently deployed in datacenters <ref type="bibr" target="#b39">[40,</ref><ref type="bibr" target="#b40">41,</ref><ref type="bibr" target="#b57">58]</ref>, poses more stringent latency and performance requirements (e.g., zero data loss).</p><p>A large body of work addresses these challenges. One popular approach is to react to early congestion signals in a more accurate fashion, using ECN <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b4">5,</ref><ref type="bibr" target="#b54">55,</ref><ref type="bibr" target="#b57">58]</ref> or network delay <ref type="bibr" target="#b37">[38,</ref><ref type="bibr" target="#b40">41,</ref><ref type="bibr" target="#b46">47]</ref>. These approaches keep queuing lower and handle incast traffic much better than the traditional TCP. However, they are still prone to buffer overflows in bursty and incast traffic patterns. Thus, they rely on priority flow control (PFC) or avoid an aggressive increase to prevent data loss. In fact, small RTTs, bursty flow arrivals, and a large number of concurrent flows in datacenters bring fundamental challenges to this approach because these factors either force a flow to send at most one packet per RTT or induce a large queue build-up. We show in Section 2 that even a hypothetically ideal rate control faces these problems. An alternative is to explicitly determine the bandwidth of a flow or even the packet departure time using a controller or a distributed algorithm <ref type="bibr" target="#b33">[34,</ref><ref type="bibr" target="#b46">47]</ref>. However, this approach incurs signaling latency and is very difficult to scale to large, high-speed (e.g., 100 Gbps) datacenters, and is challenging to make robust against failures and traffic churn <ref type="bibr" target="#b43">[44]</ref>.</p><p>Our approach uses credit packets to control the rate and schedule the arrival of data packets. Receivers send credit packets to senders on a per-flow basis in an end-to-end fashion. Switches then rate-limit the credit packets on each link and determine the available bandwidth for data packets flowing in the reverse direction. By shaping the flow of credit packets in the network, the system proactively controls congestion even before sending data packets. A sender naturally learns the amount of traffic that is safe to send, rather than reacting to the congestion signal after sending data. This allows us to quickly ramp up flows without worrying about data loss. In addition, it effectively solves incast because the arrival order of credit packets at the bottleneck link naturally schedules the arrival of data packets in the reverse path at packet granularity. Realizing the idea of credit-based congestion control, however, is not trivial. One might think that with the credit-based scheme a naïve approach in which a receiver sends credit packets as fast as possible (i.e. the maximum credit rate corresponding to its line rate) can achieve fast convergence, high utilization, and fairness at the same time. In a simple, single bottleneck network, this is true. However, in large networks, the three goals are often at odds, and the naïve approach presents serious problems: (i) it wastes bandwidth when there are multiple bottlenecks, (ii) it does not guarantee fairness, and (iii) the difference in path latency can cause queuing. In addition, in networks with multiple paths, credit and data packets may take asymmetric paths.</p><p>This paper demonstrates credit-based congestion control is a viable alternative for datacenters by addressing the challenges and answers key design questions that arise from a credit-based approach. The resulting design incorporates several components and techniques: (i) rate-limiting credits at switches, (ii) symmetric hashing to achieve path symmetry, (iii) credit feedback control, (iv) random jitter, and (v) network calculus to determine the maximum queuing.</p><p>Our feedback control achieves fast convergence and zero data loss. It effectively mitigates utilization and fairness issues in multibottleneck scenarios. We also demonstrate ExpressPass can be implemented using commodity hardware. Our evaluation shows that ExpressPass converges in just a few RTTs when a new flow starts for both 10 Gbps and 100 Gbps, whereas DCTCP takes over hundreds and thousands of RTTs respectively. In all of our evaluations, ExpressPass did not exhibit any single data packet loss. Express-Pass uses up to eight times less switch buffer than DCTCP, and data buffer is kept close to zero at all times. Our evaluation with realistic workload shows that ExpressPass significantly reduces flow completion time especially for small to medium size flows when compared to RCP <ref type="bibr" target="#b22">[23]</ref>, DCTCP <ref type="bibr" target="#b2">[3]</ref>, HULL <ref type="bibr" target="#b4">[5]</ref>, and DX <ref type="bibr" target="#b37">[38]</ref>, and the gap increases with higher link speeds.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">MOTIVATION</head><p>In large datacenter networks, it is not uncommon for a host to generate more than one thousand concurrent connections <ref type="bibr" target="#b2">[3]</ref>. The number of concurrent flows traversing a bottleneck link can be large, and flow arrival is often bursty <ref type="bibr" target="#b12">[13]</ref> due to the popularity of the partition/aggregate communication pattern <ref type="bibr" target="#b2">[3]</ref>. However, existing congestion control algorithms exhibit fundamental limitations under such workload.</p><p>First, partition/aggregate patterns cause bursty packet arrivals that result in packet drops (i.e., incast). The problem only worsens with shallow buffered commodity switches that only provide 100 KB of packet buffer per 10 Gbps port <ref type="bibr" target="#b11">[12]</ref> as well as in high-speed network (e.g., 100 Gbps) where switch buffer per port per Gbps decreases as link speeds go up 1 . Second, even if congestion control algorithms estimate each flow's fair-share correctly, bursty flow arrival <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b12">13]</ref> still causes unbounded queue build-up.</p><p>To demonstrate that even an ideal rate control can result in unbounded queue build-up, we assume a hypothetical but ideal congestion control that instantly determines the exact fair-share rate for each sender using a per-packet timer. We then simulate a partition/aggregate traffic pattern using ns-2. A single master server continuously generates a 200 B request to multiple workers using persistent connections, and each worker responds with 1, 000 B of data for each request. We increase the fan-out from 32 to 2, 048 in an 8-ary fat tree topology with 10 Gbps links with 5 µs delay, 16 core, 32 aggregator, 32 ToR switches and 128 hosts <ref type="foot" target="#foot_0">2</ref> .</p><p>Figure <ref type="figure" target="#fig_0">1</ref> (a) shows the queue length at the bottleneck link. The bars represent the minimum and maximum queue, and the box shows 25, 50, and 75%-tile values. Even though the senders transmit data at their fair-share rate and packets within the same flow are perfectly paced, the packet queue builds up significantly. This is because while each flow knows how fast it should transmit its own packets, packets from multiple flows may arrive in bursts. In the worst case, the maximum data queue length grows proportionally to the number of flows (depicted by the red line in Figure <ref type="figure" target="#fig_0">1 (a)</ref>). Window or rate-based congestion control is far from the ideal case because the congestion control does not converge to the fair-share rate immediately. Figure <ref type="figure" target="#fig_0">1</ref> (b) demonstrates the queue build-up with DCTCP. The average / maximum data queue lengths are much larger than ideal congestion control because it takes multiple RTTs to react to queue build-up.</p><p>The result suggests that existing window-or rate-based protocols have fundamental limitations on queuing, and the problem cannot be solved even with a proactive approach, such as PERC <ref type="bibr" target="#b33">[34]</ref>. Existing congestion control will exhibit high tail latency. The unbounded queue also forces a packet drop or the use of flow control, such as PFC. Even worse, large queues interact poorly with congestion feedback; when flows are transmitting at the fair-share rate, ECN <ref type="bibr" target="#b2">[3]</ref> or delay-based <ref type="bibr" target="#b37">[38,</ref><ref type="bibr" target="#b40">41]</ref> schemes will signal congestion to some flows, resulting in significant unfairness. Bounded queue build-up: To overcome the fundamental limitation, ExpressPass uses credit-based scheduling in which a sender transmits a data packet only after receiving a credit from the receiver. When a credit packet reaches a sender, the sender transmits a data packet if it has one to send; otherwise, the credit packet is ignored. ExpressPass controls congestion by rate-limiting the credit packets at the switch but without introducing per-flow state. This effectively 1 Even a deep buffer Arista 7280R switch provides 15% less buffer per Gbps in their 100 Gbps switches compared to that in 10 Gbps switches <ref type="bibr" target="#b8">[9]</ref>.  <ref type="bibr" target="#b46">[47]</ref>. To demonstrate this, we simulate the same partition/aggregate workload using our credit-based scheme. Figure <ref type="figure" target="#fig_0">1</ref> (c) shows the queue build-up. It shows regardless of the fan-out, the maximum queue build-up is bounded.</p><p>In a credit-based scheme, the queue only builds up when flows have different round-trip times. Two factors can contribute to RTT differences: 1) the difference in path lengths and 2) the variance in packet processing time at the host. In datacenter environments, both can be bounded. The difference in path lengths is bounded by the network topology. For example, a 3-layered fat tree topology has minimum round-trip path length 4 and maximum 12 between any two pairs of hosts. Note the difference is strictly less than the maximum RTT between any pair of hosts.</p><p>The variance in credit processing time can also be bounded. A host discards credit when it does not have any data to send, thus the variance comes only from that of the credit processing delay (e.g., interrupt or DMA latency in a software implementation). In our software implementation on SoftNIC <ref type="bibr" target="#b30">[31]</ref>, it varies between 0.9 and 6.2 µs (99.99 t h percentile). Our simulation result in Figure <ref type="figure" target="#fig_0">1</ref> (b) accounts for this variance. The red line shows the maximum queue required considering the two delay factors. Note, a hardware implementation on a NIC can further reduce the variance in credit processing times. Fast convergence: Another critical challenge for traditional congestion control is quickly ramping up to the fair-share. Fast ramp-up is at odds with low buffer occupancy and risks buffer overflow and packet drops. Thus, in traditional congestion control, it is often a slow, conservative process, which significantly increases the flow completion time. In contrast, credit drop is not as detrimental as data drop, which allows us to send credit packets more aggressively. To demonstrate its potential, we implement a naïve credit-based scheme where a receiver sends credits at its maximum rate. At the bottleneck link, the switch drops excess credit packets using rate-limiting. We use a Pica8 P-3780 10 GbE switch to configure rate-limiting on credit packets. Figure <ref type="figure" target="#fig_1">2</ref> shows the convergence characteristics of a naïve credit-based scheme compared to TCP cubic and DCTCP. It shows the credit-based design can converge to fairness in just one round-trip time, significantly outperforming the TCP variants. Small RTT and sub-packet regime <ref type="bibr" target="#b15">[16]</ref>: Finally, datacenter networks have a small base RTTs around tens of microseconds. Low latency cut-through switches even achieve sub-microsecond latency <ref type="bibr" target="#b7">[8]</ref>. Small RTT coupled with a large number of concurrent flows means each flow may send less than one packet per RTT on average <ref type="bibr" target="#b13">[14]</ref>even at 100 Gbps (10 Gbps), 416 <ref type="bibr" target="#b41">(42)</ref> flows are enough to reach this  point assuming 50 µs RTT. In this regime, window-based protocols break down <ref type="bibr" target="#b13">[14,</ref><ref type="bibr" target="#b15">16,</ref><ref type="bibr" target="#b31">32]</ref>. Rate-based protocols can increase the probing interval to multiples of RTTs, but this comes at a cost because the ramp-up time also increases significantly. Fundamentally, supporting a high dynamic range of flows efficiently requires a cost-effective mechanism for bandwidth probing. The credit-based approach opens up a new design space in this regard.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">EXPRESSPASS DESIGN</head><p>In ExpressPass, credit packets are sent end-to-end on a per-flow basis. Each switch and the host NIC rate-limit credit packets on a per-link basis to ensure that the returning flow of data does not exceed the link capacity. Symmetric routing ensures data packets follow the reverse path of credit flows (see Section 3.1 for details). Intuitively, our end-to-end credit-based scheme "schedules" the arrival of data packets at packet granularity, in addition to controlling their arrival rate at the bottleneck link. To show how this mechanism works, we illustrate a simple scenario with two flows in Figure <ref type="figure" target="#fig_2">3</ref>, where all links have equal capacity. Consider a time window in which only two packets can be transmitted on the link. Now, receiver R A and R B generate credits (A1, A2) and (B1, B2) respectively at the maximum credit rate. All four credit packets arrive at output port O, where credit packets are rate-limited to match the capacity of the reverse link capacity. Thus, half the credits are dropped at the output port. In this example, each sender gets one credit and sends one data packet back to the receivers. Note, the senders generate exactly two data packets that can go through the bottleneck link during the time window. In addition, in an ideal case where the RTTs of the two flows are the same, the data packets do not experience any queuing because their arrival at the bottleneck link is well-scheduled. Design challenges: While the credit-based design shows promising outlook, it is not without its own challenges. One might think that with the credit-based scheme a naïve approach in which a receiver sends credit packets as fast as possible can achieve fast convergence, high utilization, and fairness all at the same time. However, the naïve approach has serious problems with multiple bottlenecks. First, it does not offer fairness. Consider the multi-bottleneck topology of Figure <ref type="figure" target="#fig_4">4 (a)</ref>. When all flows send credit packets at the maximum rate, the second switch (from the left) will receive twice as many credit packets for Flow 0 than Flow 1 and Flow 2. As a result, Flow 0 occupies twice as much bandwidth on Link 2 than others. Second, multi-bottlenecks may lead to low link utilization. Consider the parking lot topology of Figure <ref type="figure" target="#fig_4">4 (b)</ref>. When credit packets are sent at full speed, link 1's utilization drops to 83.3%. This is because, after 50% of Flow 0's credit passing link 1 (when competing with Flow 1), only 33.3% of credit packets go through Link 2, leaving the reverse path of Link 1 under-utilized by 16.6%. Finally, in large networks, the RTTs of different paths may differ significantly. This may break the scheduling of data packets, which leads to a queue build-up.</p><p>Achieving high utilization, fairness, and fast convergence at the same time is non-trivial and requires careful design of a credit feedback loop. In addition, we must limit the queue build-up to ensure zero data loss. Next, we present the design details and how the end-hosts and switches work together to realize the goals.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Switch and End-Host Design</head><p>Credit rate-limiting (switch and host NIC): For credit packets, we use a minimum size, 84 B Ethernet frame, including the preamble and inter-packet gap. Each credit packet triggers a sender to transmit up to a maximum size Ethernet frame (e.g., 1538 B). Thus, in Ethernet, the credit is rate-limited to 84/(84 + 1538) ≈ 5% of the link capacity, and the remaining 95% is used for transmitting data packets. The host and switch perform credit rate-limiting at each switch port. The host also tags credit packets so that switches can classify them and apply rate-limiting on a separate queue. To pace credit packets and limit their maximum burst size, we apply a leaky bucket available on commodity switch chipsets (e.g., maximum bandwidth metering on Broadcom chipsets). At peak credit-sending rate, credits are spaced apart by exactly 1 MTU in time (last byte to the first byte). Because data packets are not always the full MTU in size, two or more data packets can be transmitted back to back, and by the time a credit is sent there may be additional tokens accumulated for a fraction of a credit packet. Increasing the token bucket to the size of 2 credit packets ensures these fractional amounts are not discarded so that credit sending rate becomes nearly the maximum on average.</p><p>Finally, to limit the credit queue, we apply buffer carving <ref type="bibr" target="#b18">[19]</ref> to assign a fixed buffer of four to eight credit packets to the class of credit packets. Ensuring path symmetry (switch): Our mechanism requires path symmetry-data packet must follow the reverse path of the corresponding credit packet. Datacenter networks with multiple paths (e.g., Clos networks) often utilize equal-cost multiple paths. In this case, two adjacent switches need to hash the credit and data packets of the same flow onto the same link (in different directions) for symmetric routing. This can be done, in commodity switches, by using symmetric hashing with deterministic Equal Cost Multi-Path (ECMP) forwarding. Symmetric hashing provides the same hash value for bi-directional flows <ref type="bibr" target="#b14">[15,</ref><ref type="bibr" target="#b16">17,</ref><ref type="bibr" target="#b44">45]</ref>, and deterministic ECMP sorts next-hop entries in the same order (e.g., by the next hop addresses) on different switches <ref type="bibr" target="#b20">[21,</ref><ref type="bibr" target="#b50">51]</ref>. Finally, it requires a mechanism to exclude links that fail unidirectionally <ref type="bibr" target="#b51">[52]</ref>. Note path symmetry does not affect performance. Even with DCTCP, the utilization and performance on fat tree topology are not affected by path symmetry in our simulations. Ensuring zero data loss (switch): Rate-limiting credit packets controls the rate of data in the reverse path and makes the network congestion-free. However, the difference in RTT can cause transient queue build-up. Fortunately, the maximum queue build-up is bounded in ExpressPass. We apply network calculus <ref type="bibr" target="#b36">[37]</ref> to determine the bound. Note this bound is equivalent to the buffer requirement to ensure zero-data loss. Given a network topology, we calculate the bound for each switch port. Let us denote d p as the delay between receiving a credit packet and observing the corresponding data packet at a switch port p, and ∆d p as the delay spread of d p (i.e., the difference between maximum and minimum of d p ). ∆d p is determined by the network topology and the queuing capacity of the network. ∆d p represents the maximum duration of data buffering required to experience zero loss because, in the worst case, ∆d p time unit worth of data can arrive at the same time at port p. For a given credit ingress port p, its delay, d p , consists of four factors: (1) the delay caused by the credit queue, d cr edit ; (2) the switching, transmission, and propagation delay of credit and data packet to/from ingress port q of the next hop, t (p, q); (3) the delay at ingress port q of the next hop, d q ; and (4) returning data packet's queuing delay at port q, d dat a (q) whose maximum is determined by</p><formula xml:id="formula_0">∆d q . d p = d cr edit + t (p, q) + d q + d dat a (q)</formula><p>Then, given an ingress port p and its set of possible next-hop ingress ports N (p), its delay spread ∆d p becomes:</p><formula xml:id="formula_1">∆d p = max(d cr edit ) + max q ∈N (p ) (t (p, q) + d q + ∆d q ) -min q ∈N (p ) (t (p, q) + d q ),<label>(1)</label></formula><p>In datacenters, switches are often constructed hierarchically (e.g., ToR, aggregator, and core switches). Within a hierarchical structure, the delays can be computed in an iterative fashion. For example, if we know the min/max delay of NIC ports, we can get the min/max delay and the delay spread for uplink ports in a ToR switch. At NIC, ∆d p is the same as the delay spread of host processing delay, ∆d host , which is a constant given a host implementation.</p><p>We can then compute the min/max delay and the delay spread for uplink ports in ToR and aggregator switches.</p><p>The delay spread accumulates along the path. In hierarchical multi-rooted tree topologies, traffic from a downlink is forwarded both up and down, but traffic from an uplink is only forwarded down. Therefore, for uplink ports, the set of next hop's ingress ports N (p) only includes the switch/NIC ports at lower layers, whereas for downlink ports N (p) includes ports at both lower and upper layers. As a result, uplink ports require smaller buffer than downlink ports. ToR downlink has the largest path length variance, thus has the largest buffer requirement.</p><p>Table <ref type="table" target="#tab_0">1</ref> shows the buffer per port requirement for different topologies. We assume a credit queue capacity of 8 credit packets (see Section 3.2) and propagation delay of 5 µs for core links and 1 µs for  others. For the host processing delay spread, we use the measurement result from our testbed (shown in Figure <ref type="figure" target="#fig_14">14 (a)</ref>). To compare buffer requirement of ExpressPass to DCTCP, we also show the value normalized to DCTCP's ECN marking threshold K as recommended in the DCTCP paper <ref type="bibr" target="#b2">[3]</ref>. Note, for ExpressPass, the result shows the buffer requirement for operating the network without any data packet loss, whereas DCTCP's marking threshold represents the average queue size for DCTCP. The maximum buffer requirement is a very conservative bound assuming the worst case.</p><p>It is required when a part network that has the longest delay has a full (credit and data) buffer, while the path with the shortest delay has zero buffer. This is an unlikely event that only happens in a network with significant load imbalance. Under realistic workloads, ExpressPass uses only a fraction of the data queue as we show in Section 6. Finally, ExpressPass's correct operation does not depend on zero loss of data packets (i.e., it operates even with smaller buffers).</p><p>Three main factors impact the required buffer size for zero data loss: delay spread of host processing, credit buffer size, and link speed. Figure <ref type="figure">5</ref> shows the breakdown of maximum buffer for a ToR switch by the each contributing source in 32-ary fat tree (8, 192 hosts) topologies with (10/40), (40/100), and (100/100) Gbps (link/core link) speeds. We use two sets of parameters: a) 8 credit queue and ∆d host = 5.1 µs reflect our testbed setting; b) 4 credit queue and ∆d host = 1 µs represent a case where ExpressPass is implemented in NIC hardware. Smaller credit queue capacity and host delay spread results in a smaller data queue. The required buffer space for zeroloss increases sub-linearly with the link capacity. Note, a ToR switch has the largest buffer requirement among all. Even then, its requirement is modest in all cases. Today shallow buffered 10 GbE switches have 9 to 16 MB of shared packet buffers and 100 GbE switches have 16 MB to 256 MB <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b53">54]</ref>, whereas deep buffered switches have up to 24 GB of shared packet buffer <ref type="bibr" target="#b53">[54]</ref>. To address this issue, we introduce random jitter in sending credit packets at the end-host, instead of perfectly pacing them. To determine how much jitter is required for fairness, we create a number of concurrent flows (between 1 and 1024) that traverse a single bottleneck link. We vary the jitter level, j, from 0.01 to 0.08 relative to the inter-credit gap (e.g., the jitter is between 13 ns and 104 ns for 10 GbE) and measure the Jain's fairness index <ref type="bibr" target="#b32">[33]</ref> over an interval of 1 ms. Figure <ref type="figure">6</ref> (a) shows the result. We observe that perfect pacing causes significant unfairness due to exact ordering (fairness index of 1.0 means perfect fairness). However, even small jitter (tens of nanosecond) is enough to achieve good fairness as it breaks synchronization. We also find, in our prototype implementation, the natural jitter at the host and NIC is sufficient enough to achieve fairness. Figure <ref type="figure">6</ref> (b) plots the CDF of inter-credit gap measured using an Intel X520 10 GbE NIC, when credit packets are sent at the maximum credit rate with pacing performed in SoftNIC. The inter-credit gap has a standard deviation of 772.52 ns, which provides a sufficient degree of randomization.</p><p>However, synchronized credit drop can also arise when credits from multiple switches compete for a single bottleneck. The jitter at the end host is not sufficient when the credit queues at the switches are not drained for a long period of time. In large scale simulations, we observe that some flows experience excessive credit drop while other flows make progress. To ensure fairness across switches, we randomize the credit packet sizes from 84 B to 92 B. This effectively creates jitter at the switches and breaks synchronization. With these mechanisms, ExpressPass provides fair credit drop across flows without any per-flow state or queues at the switches. Starting and stopping credit flow (end-host): Finally, Express-Pass requires a signaling mechanism to start the credit flow at the receiver. For TCP, we piggyback credit request to either SYN and/or SYN+ACK packet depending on data availability. This allows data Algorithm 1 Credit Feedback Control at Receiver. (increasing phase)</p><formula xml:id="formula_2">7:</formula><p>if previous phase was increasing phase then 8: w = (w + w max )/2 (w max = 0.5)</p><formula xml:id="formula_3">9: cur_rate = (1 -w ) • cur_rate + w • max_rate • (1 + target_loss) 10:</formula><p>else 11:</p><p>(decreasing phase)</p><formula xml:id="formula_4">12: cur_rate = cur_rate • (1 -credit_loss) • (1 + target_loss) 13:</formula><p>w = max (w/2, w min ) (0 &lt; w min ≤ w max )</p><p>14: until End of flow transmission to start immediately after the 3-way handshake. For persistent connections or UDP, we send credit requests in a minimum sized packet, but this adds a delay of an RTT. At the end of the flow, if there is no data to send for a small timeout period, the sender transmits a CREDIT_STOP to the receiver, and the receiver stops sending credits after receiving it 3 . Figure <ref type="figure">7</ref> depicts the state transition diagram.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Credit Feedback Control</head><p>Fast convergence, utilization, and fairness present challenging tradeoffs in congestion control. In our credit-based scheme, considering only one (e.g., fast convergence) results in an undesirable outcome as shown in Section 2. To address this, we introduce a credit feedback loop where credit sending rate is dynamically adjusted. Our feedback loop strives to achieve high utilization, fairness, and fast convergence.</p><p>One question is: how does it differ from existing data packet feedback control? We first illustrate two key differences and describe the feedback design.</p><p>First, in the credit-based scheme, one can ramp up a flow's credit sending rate much more aggressively because ramping up the credit sending rate does not result in data loss. In traditional design, ramping up the flow rate had to be done carefully in a conservative fashion because it might introduce data loss, which greatly increases the flow completion time due to retransmission. Our algorithm leverages aggressive ramp-up of credit sending rate to achieve fast convergence.</p><p>Second, we fully leverage the fact that the cost of overshooting the credit sending rate is low. At steady state, overshoot targets a small credit loss rate (e.g., 10%). This allows ExpressPass to instantly detect and fill up additional available bandwidth when bottleneck link starts to free up, which achieves high utilization. Feedback control algorithm: At a high-level, our feedback algorithm uses binary increase, similar to BIC-TCP <ref type="bibr" target="#b55">[56]</ref>, with the target rate set to the link capacity. The aggressive increase ensures fast convergence. Fairness is achieved because the flow with lower rate increases its rate more than the competing flow with the higher rate. 3 This cause some credits to be wasted. We quantify the overhead and discuss mitigation strategies in Section 7. However, the aggressive binary increase does not converge and results in oscillation. To improve the stability, we dynamically adjust the aggressiveness factor, w.</p><p>ExpressPass uses the credit loss rate as the congestion indicator. For credit loss rate measurement, each credit packet carries a sequence number, and the data packet echoes back the credit sequence number. A gap in the sequence number indicates credit packets have been dropped in the network. When congestion is detected, Express-Pass reduces the credit sending rate to the credit rates that passed through the bottleneck during the last RTT period.</p><p>Algorithm 1 describes the credit feedback control. If credit_loss is less than or equal to the target loss rate (target_loss), the feedback control concludes that the network is under-utilized and goes through the increasing phase (line 6 to 9). It increases cur_rate by computing a weighted average of cur_rate and max_rate•(1 + target_loss) using an aggressiveness factor w (0 &lt; w ≤ 0.5) as the weight, where max_rate indicates the maximum credit rate for the link. Otherwise (i.e., when credit_loss &gt; target_loss), the feedback control detects congestion and goes through the decreasing phase, where it reduces the credit sending rate (cur_rate) so that the credit loss rate will match its target loss rate at the next update period assuming all flows adjust in the same manner.</p><p>The feedback loop dynamically adjusts the aggressiveness factor w (between w min and w max or 0.5) to provide a balance between stability and fast convergence. When w is large (small), the credit sending rate ramps up more (less) aggressively. Thus, when congestion is detected, we halve w in the decrease phase. When no congestion is detected for two update periods, we increase w by averaging its current value and the maximum value, 0.5. At steady state, a flow experiences increase and decrease phase alternatively, and as a result, w decreases exponentially. This achieves stability as we show in Section 4. Finally, w min provides a lower bound, which keeps w from becoming infinitesimally small. In all our experiments, we use w min of 0.01. Setting w min involves in a trade-off between a better steady state behavior and fast convergence, as we show through analysis in Section 4.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Parameter choices</head><p>Initial rate and w init determine how aggressively a flow starts. They decide the trade-off between small flow FCT and credit waste. Setting them high allows flows to use as much bandwidth as possible from the beginning. However, a flow with only a single packet to send will waste all-but-one credits in the first RTT. Setting them low reduces the credit waste, but increases the convergence time   As the initial rate decreases, the amount of wasted credits decline as expected. Note that w init does not affect the credit waste for a single packet flow. We further analyze the trade-offs with realistic workloads later in Section 6.3. Target loss: One important role of the target loss factor is to compensate for subtle timing mismatch. For example, a receiver could send N credits over prior RTT and only receives N-1 data packets due to subtle timing mismatch, but we do not want our feedback control to overreact to such cases. While targeting some loss may appear to introduce more credit waste, it usually does not because credits delivered to the sender will be used if data is available. However, with multiple bottlenecks, setting the target loss rate higher risks under-utilization. Thus, we use a small target loss rate of 10%. Credit queue capacity: The size of the credit buffer affects utilization, convergence time, and data queue occupancy. A large credit buffer hurts fast convergence as it delays feedback and increases the delay spread and queue occupancy for data packets. However, too small credit buffer can hurt utilization because credit packets can arrive in bursts across different ports and get dropped. We quantify how much credit queue is necessary to ensure high utilization. For this, we conduct an experiment with our feedback control while varying the number of flows from 2 to 32 which all arrive from different physical ports and depart through the same port. We then vary the credit queue size from 1 to 32 packets in power of 2 and measure the corresponding under-utilization in Figure <ref type="figure">9</ref>. It shows that credit buffer size of eight is sufficient across the different number of flows. Hence, we set the credit buffer to eight for rest of the paper. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Effect of feedback control</head><p>The feedback loop significantly improves utilization with multiple bottlenecks by reducing wasted credits for long flows when flows traverse multiple hops. Here, we quantify this using the topology of Figure <ref type="figure" target="#fig_0">10</ref> (a). We increase the number of bottleneck links, N , from one to six. Figure <ref type="figure" target="#fig_0">10 (b)</ref> shows the utilization of the link with the lowest utilization. To isolate the loss due to multiple bottlenecks, we report the utilization normalized to the maximum data rate excluding the credit packets. Our feedback control achieves high utilization even with multiple bottlenecks. With two bottlenecks, it achieves 98.0% utilization, an improvement from 83.3% the naïve case without feedback control; and with six bottlenecks, it achieves 97.8% utilization, an improvement from 60% in the naïve case.</p><p>Our feedback control also improves fairness with multiple bottlenecks. To demonstrate this, we use the multi-bottleneck scenario in Figure <ref type="figure" target="#fig_11">11</ref> (a) and vary the number of flows (N) that use Link 1. We then measure the throughput of Flow 0. Note, all flows are longrunning flows and Flow 1 through N experience multiple bottlenecks but Flow 0 only has a single bottleneck. Figure <ref type="figure" target="#fig_11">11 (b)</ref> shows the throughput of Flow 0 as the number of competing flows increases. With ideal max-min fairness, Flow 0 should get 1/(N+1) of the link capacity (red line in Figure <ref type="figure" target="#fig_11">11 (b)</ref>). ExpressPass follows the max-min fair-share closely until four flows. But, as the number of flows increases, it shows a small gap. There are many factors that contribute to fairness. One important factor is when there is less than one credit packets per RTT. ExpressPass can still achieve good utilization and bounded queue, but fairness deteriorates in such cases.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">ANALYSIS OF EXPRESSPASS</head><p>We now provide analysis of our feedback control. We prove that it converges to fair-share and analyze stability using a discrete model. For analysis, we assume N (&gt; 1) flows are sharing a single bottleneck link and their update periods are synchronized as in many other studies <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b45">46,</ref><ref type="bibr" target="#b58">59]</ref>. Stability analysis: Let us denote the credit sending rate of flow n at time t by R n (t ), and its aggressiveness factor w by w n (t ). The maximum credit sending rate corresponding to the link capacity is denoted as max_rate. We define C as C = max_rate • (1 + target_loss), which is the maximum credit sending rate for a flow.</p><p>Without loss of generality, assume the bottleneck link is underutilized. Then, credit_loss will be zero for all flows, and they will increase their credit sending rates. Eventually, the aggregate credit sending rate ( N i=1 R i (t )) will exceed the maximum credit sending rate, C. In the next update period t = t 0 , it will reach C according to the decreasing phase <ref type="bibr">(</ref> </p><formula xml:id="formula_5">N i =1 R i (t 0 + 1) = N i =1 {(1 -w i (t 0 + 1)) • R i (t 0 ) + w i (t 0 + 1)C } &gt; N i =1 {(1 -w min ) • R i (t 0 ) + w min C } &gt; C<label>(2)</label></formula><p>The aggregate credit sending rate now exceeds C. Then, at t 0 + 2, it becomes C again. From the time t 0 , all flows will experience increasing phase and decreasing phase alternatively. Thus, we get the following equations from algorithm 1 line 12 and 9:</p><formula xml:id="formula_6">R n (t 0 + 1) =(1 -w n (t 0 ))R n (t 0 ) + w n (t 0 )C R n (t 0 + 2) = C N i =1 R i (t 0 + 1) R n (t 0 + 1) = (1 -w n (t 0 ))R n (t 0 ) + w n (t 0 )C 1 -1 C N i =1 w i (t 0 )R i (t 0 ) + N i =1 w i (t 0 ) = 1 A(t 0 ) {(1 -w n (t 0 ))R n (t 0 ) + w n (t 0 )C } A(t 0 ) := 1 + N i=1 w i (t 0 ){1 - R i (t 0 ) C }</formula><p>Solving the recurrence equations yields (for k &gt; 0):</p><formula xml:id="formula_7">R n (t 0 + 2k ) ≈A -k (t 0 )(1 -w n (t 0 )) k • R n (t 0 ) + w n (t 0 ) • C A(t 0 ) -(1 -w n (t 0 )) (3) R n (t 0 + 2k + 1) ≈(1 -w n (t 0 + 2k )) • R n (t 0 + 2k ) + w n (t 0 + 2k ) • C<label>(4)</label></formula><p>The aggressiveness factor w n (t ) halves every two update periods and eventually it converges to w min (Algorithm 1 line 8, 11-12).</p><formula xml:id="formula_8">w n (t + 1) = w n (t ), w n (t + 2) = max( 1 2 w n (t ), w min )</formula><p>Let us denote the time when w n (t ) have converged to w min by t c ; w n (t c + n) = w min . Equation 3 and 4 still hold at t c :</p><formula xml:id="formula_9">R n (t c + 2k ) =A -k (t c )(1 -w min ) k • R n (t c ) + C N R n (t c + 2k + 1) = (1 -w min )R n (t c + 2k ) + w min C</formula><p>Because 0 &lt; w min ≤ w n (t 0 ) ≤ 0.5, N &gt; 1 and C &gt; 0, A(t 0 ) &gt; 1 and 0.5 ≤ (1 -w min ) &lt; 1. Thus, we get:</p><formula xml:id="formula_10">R n (t c + 2k ) → C N (5) R n (t c + 2k + 1) → C N (1 + (N -1)w min )<label>(6)</label></formula><p>We have shown that regardless of the initial credit sending rate and the initial aggressiveness factor w n , R n (t + 2k ) converges to C/N and R n (t + 2k + 1) is bounded. Thus, our feedback control is stable. Below, we also show the difference between R n (t + 2k ) and R n (t + 2k + 1) is bounded. Convergence to fairness: Now, we show the bandwidth allocation B n converges to fairness. From Equations 5 and 6, we see that all flows have the same credit sending rate. Therefore, the bandwidth allocation converges to fairness.</p><p>Figure <ref type="figure" target="#fig_1">12</ref> illustrates the convergence behavior. Let's denote the fair-share rate of a flow as R * (= C N ). Let us denote the difference of credit sending rate at time t and time t -1 as D (t ). It follows that:</p><formula xml:id="formula_11">D (t 0 + 2k + 1) = |R n (t 0 + 2k + 1) -R n (t 0 + 2k ) | ≈ | max (2 -k w n (t 0 ), w min ) • {C -R n (t 0 + 2k ) } |</formula><p>Similarly, we can compute D (t 0 + 2k + 2). D (t ) is an exponentially decreasing function that eventually converges to:</p><formula xml:id="formula_12">D * = C • w min (1 - 1 N )</formula><p>Note C = max_rate• (1+target_loss). Thus, D * depends on w min and target_loss; a small w min and target_loss results in a small D * value, which improves the steady state behavior. However, a small w min can cause delayed convergence by making (1 -w min ) larger in Equation <ref type="formula" target="#formula_7">4</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">IMPLEMENTATION</head><p>We leverage SoftNIC <ref type="bibr" target="#b30">[31]</ref> to implement ExpressPass. SoftNIC is a framework that allows developers to extend NIC functionality by exploiting high-performance kernel bypass I/O framework (DPDK). SoftNIC enables us to pace credits with microseconds level accuracy and react to the credit packets with a small delay of few microseconds. SoftNIC also enables legacy applications and kernel networking stacks to be used without modification. Applications run transparently on top of SoftNIC using TCP/IP. We implement Ex-pressPass as a module inside SoftNIC. ExpressPass module adjusts the credit sending rate according to the feedback algorithm and paces the transmission of credit packets. Data packets are sent only when the ExpressPass module receives a credit packet. To start and stop the transmission of credit packets, we implement the per-flow state machine in Figure <ref type="figure">7</ref>. For example, when the NIC receives data to transmit from the kernel, it generates credit requests. Upon reception of a credit packet, it offers a data packet if available.</p><p>For rate-limiting credit packets, we leverage the scalable rate limiter implementation of SoftNIC. It is able to handle thousands of concurrent flows while introducing only a few microseconds of jitter <ref type="bibr" target="#b30">[31]</ref>. For rate-limiting in the switch, we leverage maximum bandwidth metering available in Broadcom chipsets. We create separate per-port queues for credits. Then, we set the maximum burst size as 2 credits and the queue occupancy as 8 credit packets. Credit processing and rate-limiting: To validate our implementation and setup, we measure 1) the host credit processing time of our implementation and 2) the accuracy of credit rate-limiting at the switch. We connect two hosts using a ToR switch (Quanta T3048-LY2). Figure <ref type="figure" target="#fig_14">14 (a)</ref> shows the CDF of credit processing latency.   The median is about 0.38 µs, but 99.99 t h percentile is large at 6.2 µs.</p><p>We believe a hardware implementation will have a much smaller variance, reducing the buffer requirement for ExpressPass. RDMA NICs implementing iWARP exhibit a maximum delay spread (jitter) of 1.2 µs <ref type="bibr" target="#b17">[18]</ref>. In addition, all the core components of ExpressPass, including pacers <ref type="bibr" target="#b4">[5]</ref>, rate-limiters <ref type="bibr" target="#b47">[48,</ref><ref type="bibr" target="#b48">49,</ref><ref type="bibr" target="#b57">58]</ref>, and credit-based flow control, have been implemented on NIC. Figure <ref type="figure" target="#fig_14">14 (b)</ref> shows the inter-credit gap at transmission at the sender and reception at the receiver. We timestamp credit packets at SoftNIC right before transmission and right after the reception. We observe that the jitter of the inter packet gap is relatively small (within 0.7 µs) which implies that software implementation is efficient enough to generate and receive credit packets at 10 Gbps. Convergence characteristics: To demonstrate ExpressPass works end-to-end, we run a simple test with five flows that arrive and depart over time. Figure <ref type="figure" target="#fig_13">13</ref> shows the throughput averaged over 10 ms and the queue size measured at the switch every 30 ms. ExpressPass achieves a much more stable steady state behavior and exhibits a very small queue. The maximum ExpressPass data throughput is 94.82% of the link capacity because 5.18% of the bandwidth is reserved for credit. The maximum queue size observed was 18 KB for ExpressPass and 240.7 KB for DCTCP. The maximum credit queue size of ExpressPass was only 672 B (8 packets).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">EVALUATION</head><p>We evaluate three key aspects of ExpressPass using testbed experiments (Section 6.1) and ns-2 simulations <ref type="bibr" target="#b38">[39]</ref> (Section 6.1 -6.3):</p><p>(1) We evaluate the flow scalability of ExpressPass in terms of convergence speed, fairness, utilization, and queuing. (2) We measure the effectiveness of ExpressPass under heavy incast.</p><p>(3) We quantify the benefits and the trade-offs of ExpressPass using realistic datacenter workloads.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.1">Microbenchmark</head><p>In this section, using simple synthetic microbenchmark we verify whether ExpressPass behaves as designed, quantify its benefits, and compare it against the DCTCP <ref type="bibr" target="#b2">[3]</ref> and the RCP <ref type="bibr" target="#b22">[23]</ref>. Flow scalability: Flow scalability is an important problem because datacenters have very small BDP on the order of 100 packets. Yet, it needs to support various scale-out workloads that generate thousands of concurrent connections <ref type="bibr" target="#b24">[25]</ref>. To test the flow scalability of ExpressPass, we run a simple experiment using a dumbbell topology where N pairs of sender and receiver share the same bottleneck link. We vary the number of flows from 4 to 256 (testbed) and 1024 (ns-2), and measure utilization, fairness and queuing. Note, these flows are long running flows whose arrival times are not synchronized. We experiment with both testbed and simulation for cross-validation.</p><p>For testbed experiments, we use 12 machines to generate traffic, where each sender may generate more than one flow.</p><p>First, we measure the utilization. ExpressPass achieves approximately 95% of utilization due to the reserved bandwidth for credits. DCTCP achieves 100% utilization in all cases. RCP has underutilization beyond 256 flows. In the testbed, DCTCP shows slightly lower utilization when the number of flows is small. We suspect high variation in kernel latency as the cause.</p><p>Second, we measure the fairness to evaluate how fairly bandwidth is shared across flows. We compute the Jain's fairness index using the throughput of each flow at every 100 ms interval and report the average. With a large number of flows DCTCP's fairness drops significantly. Because DCTCP cannot handle a congestion window of less than 1, some flows time out and eventually collapse. In contrast, both ExpressPass and RCP achieve very good fairness. Finally, Figure <ref type="figure" target="#fig_16">15</ref> (e) and (f) show the maximum queue occupancy at the bottleneck as the number of flows increases. Express-Pass shows maximum queuing of 10.5 KB and 1.34 KB in the testbed and ns-2 respectively. In contrast, DCTCP's max queue occupancy increases with the number of flows. In our simulation, when the maximum queue hits the queue capacity, packet drop occurs. The experimental results show a similar trend. DCTCP's congestion control starts to break around 64 flows. When the number of concurrent flows is larger than 64, most flows stay at the minimum congestion window of 2 because the queue size is always larger than the marking threshold. However, note the maximum queue length for 16 and 32 flows are higher than that of 64 flows. For 16 and 32 flows, some flows may occasionally ramp up when they do not get an ECN signal, which builds up the queue. RCP exceeds the queue capacity and packet drop occurs even with 32 flows. This is because RCP assigns the same rate for a new flow as existing flows when new flows start. Fast convergence: Figure <ref type="figure" target="#fig_17">16</ref> shows the convergence behavior of ExpressPass, DCTCP, and RCP over time. First, we use testbed experiments to compare ExpressPass and DCTCP at 10 Gbps. Express-Pass's throughput is averaged over 25 µs and DCTCP is averaged over 100 ms due to its high throughput variance. As shown in Figure <ref type="figure" target="#fig_17">16</ref> (a) and (b), ExpressPass converges 700x faster than DCTCP in just 100 µs (four RTTs), while DCTCP took 70 ms to converge. Two factors contribute to the difference. First, convergence is much faster in ExpressPass than the DCTCP which performs AIMD. Second, ExpressPass shows RTT of 10 µs at the minimum and 25 µs on average, measured in SoftNIC. On the other hand, DCTCP's feedback loop runs much slower in the Linux kernel, which adds hundreds of microseconds RTT variation <ref type="bibr" target="#b37">[38]</ref>.</p><p>Next, we use simulation to compare the congestion feedback algorithm of ExpressPass and DCTCP in isolation. We compare the convergence time of ExpressPass and DCTCP on two different link speeds (10 Gbps and 100 Gbps). The base RTT is set to 100 µs. which is consistent with 4 RTTs in experiments. DCTCP takes more than 80 times longer than ExpressPass with 10 Gbps link. As bottleneck link capacity increases, the convergence time gap between ExpressPass and DCTCP becomes larger. At 100 Gbps, Express-Pass's convergence time remains unchanged, while that of DCTCP grows linearly to the bottleneck link capacity. Because of DCTCP's additive increase behavior, its convergence time is proportional to the bandwidth-delay product (BDP).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.2">Heavy incast traffic pattern</head><p>One advantage of ExpressPass is robustness against incast traffic patterns. Such traffic patterns commonly happen in the shuffle step of MapReduce <ref type="bibr" target="#b19">[20]</ref>. It creates an all-to-all traffic pattern, generating incast towards each host running a task. We simulate 40 hosts connected to single top-of-rack (ToR) switch via 10 Gbps links using ns-2. Each host runs 8 tasks, each of which sends 1 MB to all other tasks. Thus, each host sends and receives 2496 (39 × 8 × 8) flows. Figure <ref type="figure" target="#fig_0">17</ref> shows the CDF of flow completion times (FCTs) with DCTCP and ExpressPass. The median FCT of DCTCP is slightly better (2.0 vs. 2.2 s). However, DCTCP has a much longer tail. At 99 t h percentile and tail, ExpressPass outperforms DCTCP by a factor of 1.51 and 6.65 respectively. With DCTCP, when some faster flows complete, the remaining flows often catch up. However, at the tail end, delayed flows tend to be toward a small set of hosts, such that they cannot simply catch up by using all available bandwidth. This drastically increases the tail latency and contributes to the straggler problem in MapReduce <ref type="bibr" target="#b6">[7]</ref>. Our example demonstrates Data Mining <ref type="bibr" target="#b27">[28]</ref> Web Search <ref type="bibr" target="#b2">[3]</ref> Cache Follower <ref type="bibr" target="#b49">[50]</ref> Web Server <ref type="bibr" target="#b49">[50]</ref> 0 </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.3">Performance under realistic workload</head><p>To evaluate the performance of ExpressPass in a more realistic scenario, we run ns-2 simulations with four different workloads shown in Table <ref type="table" target="#tab_4">2</ref>. It shows the flow size distribution and the average flow size for each workload. We have chosen the workloads to cover a wide range of average flow sizes ranging from 64 KB to 7.4 MB. While the data mining workload has a smaller fraction of XL size flows compared to web search, it has a larger cap of 1 GB compared to 30 MB for web search, resulting in higher average flow sizes. We generate 100 thousand flows with exponentially distributed interarrival time. We simulate three target loads of 0.2, 0.4, and 0.6. There is an over-subscription at the ToR uplinks and most of the traffic traverses through the ToR up-links due to random peer selection. Hence, we set the target load for ToR up-links.</p><p>We use a fat tree topology that consists of 8 core switches, 16 aggregator switches, 32 top-of-rack (ToR) switches, and 192 nodes. The topology has an over-subscription ratio of 3:1 at ToR switch layer. We create two networks, one with 10 Gbps links and the other with 40 Gbps links. Maximum queue capacities are set to 384.5 KB (250 MTUs) for the network with 10 Gbps link and 1.54 MB (1, 000 MTUs) for the network with 40 Gbps. All network link delays are set to 4 µs host delays to 1 µs, which results in maximum RTT of 52 µs between nodes excluding queuing delay. To support multipath routing, Equal Cost Multi Path (ECMP) routing is used.</p><p>We measure the flow completion time (FCT) and queue occupancy of ExpressPass and compare them with RCP, DCTCP, DX, and HULL. We set the parameters as recommended in their corresponding papers. Parameter sensitivity: The initial value of credit sending rate (α × max_rate) and aggressiveness factor (w init ) determine the early behavior as described in Section 3.2. To decide appropriate values, we run realistic workloads at target load of 0.6 with different α and w init values. Figure <ref type="figure" target="#fig_9">18</ref> (a) and (b) shows the 99%-ile FCT values for short (S) and large (L) flows respectively. As α and w init decrease, 99%-ile FCT of large flows decreases at the cost of increased FCT for short flows. With α = w init = 1/16, large flow FCT decreases significantly, while short flow FCT increases less than 100% compared to using α = w init = 1/2. Further reducing the values provides an incremental gain in large flow FCT, but at a larger cost in short flow FCT. α = w init = 1/16 provides a sweet spot, and we use the setting in the rest of the experiments. Flow Completion Time: We show the average and 99 t h percentile FCTs across workloads for a target load of 0.6 in Figure <ref type="figure" target="#fig_0">19</ref>. The solid bar at the bottom indicates the average FCT and the upper stripe bar shows the 99 t h percentile value. One clear pattern is that ExpressPass performs better than others for short flows (S and M) across workloads, and DCTCP and RCP perform better on large flows (L and XL). ExpressPass achieves from 1.3x to 5.14x faster average FCT compared to DCTCP for S and M flows, and the gap is larger at 99 t h percentile. For L and XL size flows, its speed ranges from 0.37x to 2.86x of DCTCP. This is expected given that two dominant factors for short flow completion time are low queuing and ramp up time which ExpressPass improves at the cost of lower utilization. Between workloads, ExpressPass performs the worst for Web Server workload relative to the others. This is due to the small average flow size of 64 KB causing more credit waste. Credit Waste: To understand how much credit is wasted, we measure the ratio of credit waste from the sender. Figure <ref type="figure" target="#fig_1">20</ref> shows the result broken down by the workload and the link speed. As the average flow size becomes smaller, the wasted amount of credit increases up to 60% in 40 Gbps and 34% in 10 Gbps in the Web Server workload. Higher link speed also increases the wasted credits. This explains why ExpressPass performs worse than DCTCP for large flows in the Web Server workload. In general, the amount of wasted credit is proportional to the bandwidth delay product (BDP) and inversely proportional to the average flow size. In the worst case, the receiver may send an entire BDP worth of credits to the sender  and yet receive only one data packet. The figure also shows credit waste with two different parameter settings to highlight this tradeoff. Setting α to 1/16 reduces the amount of wasted credits significantly to 31% and 19% for 40 Gbps and 10 Gbps link speed respectively. Link speed scalability: Higher link speed enables flows to push more bits per second but may require longer convergence time, which diminishes the benefit of more bandwidth. To evaluate how well ExpressPass performs, we measure the relative speed-up in FCT when the link speed increases from 10 Gbps to 40 Gbps. Figure <ref type="figure" target="#fig_20">21</ref> shows the average FCT speed-up for the Web Server and Web Search workloads. Data Mining and Cache Follower show similar results as Web Search. For small flows, we observe less speed-up compared to larger flows because RTT dominates the small flow FCT thus increased bandwidth helps less. ExpressPass shows the largest gain (1.5x -3.5x) across all cases except large flows in Web Server workload. RCP has the largest gain in this case due to its aggressive ramp up. It also maintains high utilization, whereas ExpressPass suffers from increased credit waste when the BDP increases. DCTCP has a 2.8x gain for the XL size flows and less than 2x gains for S, M, and L size flows. DX and HULL benefit the least as they're the least aggressive scheme. Overall, this shows increasing benefit ExpressPass's fast convergence and low queuing with the higher link speed. Queue length: Table <ref type="table" target="#tab_7">3</ref> shows the average and maximum queue occupancy observed during the simulation. On average, ExpressPass uses less buffer than other congestion controls. ExpressPass's max queue is not proportional to the load whereas all other transports use more queue with the increased load. ExpressPass's queue bound is a property of the topology, independent to the workload.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">DISCUSSION AND LIMITATION</head><p>Path symmetry: To ensure path symmetry, we have used symmetric routing. Symmetric routing can be achieved as evidenced by prior  work <ref type="bibr" target="#b26">[27]</ref> and our simulation also uses symmetric routing on fat tree. However, it incurs increased complexity to maintain consistent link ordering with ECMP in the forward and reverse directions, especially for handling link failures. Packet spraying <ref type="bibr" target="#b21">[22]</ref> is a viable alternative because it ensures all available paths get the equivalent load. We believe the bounded queuing property of ExpressPass will also limit the amount of packet reordering. Presence of other traffic: In real datacenter networks, some traffic, such as ARP packets and link layer control messages, may not be able to send credits in advance. One solution to limit such traffic and apply "reactive" control to account for it. When traffic is sent without credit, we absorb them in the network queue and send credit packets from the receiver, which will drain the queue. Multiple traffic classes: Datacenter networks typically classify traffic into multiple classes and apply prioritization to ensure the quality of service. Existing schemes use multiple queues for data packets and enforce priority or weighted fair-share across the queues. The same logic can be applied to ExpressPass for credit packets instead of data packets. For example, prioritizing flow A's credits over flow B's credits while throttling the sum of credits from A and B will result in the strict prioritization of A over B. Applying weighted fair-share over multiple credit queues would have a similar effect. Limitation of our feedback algorithm: The credit-based design opens up a new design space for feedback control. We have explored a single instance in this space, but our feedback algorithm leaves much room for improvement.</p><p>Short flows cause credit packets to be wasted. This hurts the flow completion times of long flows that compete with many short flows. One way to overcome this is to use the approach of RC3 <ref type="bibr" target="#b41">[42]</ref>. RC3 uses low priority data packet to quickly ramp up flows without affecting other traffic. Similarly, in ExpressPass, one can allow applications to send low priority data packets without credits. Such low priority traffic would then be transmitted opportunistically to compensate for the bandwidth lost due to wasted credits. However, this approach comes at the cost of rather complex loss recovery logic and requires careful design <ref type="bibr" target="#b41">[42]</ref>. Credit waste can also be reduced if the end of the flow can be reliably estimated in advance. Currently, we assume senders do not know when the flow ends in advance. However, it is possible for the sender to notify the end of the flow in advance and send the credit stop request preemptively with some margin. Some designs <ref type="bibr" target="#b0">[1]</ref> even propose advertising send buffer to the receiver. The sender can then leverage the information to control the amount of credit waste.</p><p>Another limitation is that our feedback currently assumes all hosts have the same link capacity. We leverage this assumption in our feedback design for faster convergence. However, when host link speeds are different, the algorithm does not achieve fairness. One could use other algorithms, such as CUBIC <ref type="bibr" target="#b28">[29]</ref>, to regain fairness by trading-off the convergence time under such a setting, without compromising bounded queuing.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8">RELATED WORK.</head><p>Credit-based flow control: Our approach is inspired by credit based flow control <ref type="bibr" target="#b35">[36]</ref> used in on-chip networks and high-speed system interconnect such as Infiniband, PCIe, Intel QuickPath, or AMD Hypertransport <ref type="bibr" target="#b52">[53]</ref>. However, traditional credit-based flow control is hop-by-hop, which requires switch support, and is difficult to scale to datacenter size. Decongestion control and pFabric pioneered a design where hosts transmit data aggressively and switches allocate bandwidth. The difference is that we allocate bandwidth using credit. Finally, TVA <ref type="bibr" target="#b56">[57]</ref> uses a similar idea to rate-limit requests at the router, but it is designed for DDoS prevention considering the size of response rather than congestion control. Low latency datacenter congestion control: DCQCN <ref type="bibr" target="#b57">[58]</ref> and TIMELY <ref type="bibr" target="#b40">[41]</ref> are designed for datacenters that have RDMA traffic. DCQCN uses ECN as congestion signal and QCN-like rate control. The main goals of DCQCN alleviate the problems caused by PFC by reducing its use while reducing ramp-up time. TIMELY uses delay as feedback, similar to DX <ref type="bibr" target="#b37">[38]</ref>, but incorporates PFC to achieve zero loss and lower the 99 t h percentile latency. PERC <ref type="bibr" target="#b33">[34]</ref> proposes a proactive approach to overcome the problems of reactive congestion control. We believe ExpressPass presents an alternative approach and shows promise in the high-speed environment (e.g., 100 Gbps networks). Flow scheduling in datacenters: A large body of work focuses on flow scheduling <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b10">11,</ref><ref type="bibr" target="#b25">26]</ref> in datacenter networks to minimize flow completion times. Although the goals might overlap, flow scheduling is largely orthogonal to congestion control. We believe congestion control is a more fundamental mechanism for network resource allocation. We also note that some flow scheduling schemes <ref type="bibr" target="#b42">[43]</ref> have been used in conjunction with congestion control to minimize the flow completion times. pFabric treats the network as a single switch and performs shortest job first scheduling assuming the flow size is known in advance. This requires switch modifications. PIAS <ref type="bibr" target="#b10">[11]</ref> makes the approach more practical by approximating shortest-jobfirst by implementing a multi-level feedback queue using priority queuing in commodity switches and does not require knowledge of individual flow size. pHost <ref type="bibr" target="#b25">[26]</ref> shares the idea of using credit (token) packets, but token packets in pHost are used for scheduling packets/flows rather than congestion control. It assumes a congestion-free network by using a network with full bisection bandwidth and packet spraying. In addition, pHost does require knowledge of individual flow size in advance. Router-assisted congestion control: Some congestion control algorithms require in-network support <ref type="bibr" target="#b23">[24,</ref><ref type="bibr" target="#b29">30,</ref><ref type="bibr" target="#b34">35,</ref><ref type="bibr" target="#b41">42]</ref>. These mechanisms introduce a form of in-network feedback with which switches explicitly participate in rate allocation of each flow. To reach fast convergence, PERC <ref type="bibr" target="#b33">[34]</ref> and FCP <ref type="bibr" target="#b29">[30]</ref> employ mechanisms for endhosts to signal their bandwidth demand to the switches in the network, which require changes in the switches. In ExpressPass, we use credit packets to signal demand and merely use rate-limiting, which does not require modification of the switches. Finally, RC3 <ref type="bibr" target="#b41">[42]</ref> uses in-network priority queues to fill up the available bandwidth in one RTT. We believe this technique can be applied to credit packets to achieve similar benefits, but leave it as future work.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="9">CONCLUSION</head><p>In this work, we introduce ExpressPass, an end-to-end credit-based congestion control. We use end-to-end credit transfer for bandwidth allocation and fine-grained packet scheduling. We explore the key benefits of a credit-based design and demonstrate it opens up a new design space for more efficient congestion control. In particular, the use of credit enables 1) low-cost bandwidth probing without queue build-up and 2) scheduling the arrival of data packets at packet granularity. We address key challenges in realizing a creditbased congestion control and demonstrate it can be implemented using commodity switches. By shaping the flow of credit packets at the switch, ExpressPass effectively controls congestion even before sending data packets. By achieving fast convergence, it drastically reduces the FCT for small flows. ExpressPass requires a small amount of buffer. Our evaluation shows that ExpressPass (1) outperforms other congestion control algorithms; (2) ensures high utilization and fairness even with many concurrent flows; and (3) the benefits of ExpressPass over other algorithms become even more favorable as link speeds increase.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Data queue length of window/rate-based protocol, DCTCP and credit-based protocol</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>Convergence Time (testbed experiment)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 :</head><label>3</label><figDesc>Figure3: ExpressPass Overview schedules the response packets in the bottleneck link at packet granularity and thus bounds the queue build-up, without relying on a centralized controller<ref type="bibr" target="#b46">[47]</ref>. To demonstrate this, we simulate the same partition/aggregate workload using our credit-based scheme. Figure1(c) shows the queue build-up. It shows regardless of the fan-out, the maximum queue build-up is bounded.In a credit-based scheme, the queue only builds up when flows have different round-trip times. Two factors can contribute to RTT differences: 1) the difference in path lengths and 2) the variance in packet processing time at the host. In datacenter environments, both can be bounded. The difference in path lengths is bounded by the network topology. For example, a 3-layered fat tree topology has minimum round-trip path length 4 and maximum 12 between any two pairs of hosts. Note the difference is strictly less than the maximum RTT between any pair of hosts.The variance in credit processing time can also be bounded. A host discards credit when it does not have any data to send, thus the variance comes only from that of the credit processing delay (e.g., interrupt or DMA latency in a software implementation). In our software implementation on SoftNIC<ref type="bibr" target="#b30">[31]</ref>, it varies between 0.9 and 6.2 µs (99.99 t h percentile). Our simulation result in Figure1(b) accounts for this variance. The red line shows the maximum queue required considering the two delay factors. Note, a hardware implementation on a NIC can further reduce the variance in credit processing times. Fast convergence: Another critical challenge for traditional congestion control is quickly ramping up to the fair-share. Fast ramp-up is at odds with low buffer occupancy and risks buffer overflow and packet drops. Thus, in traditional congestion control, it is often a slow, conservative process, which significantly increases the flow completion time. In contrast, credit drop is not as detrimental as data drop, which allows us to send credit packets more aggressively. To demonstrate its potential, we implement a naïve credit-based scheme where a receiver sends credits at its maximum rate. At the bottleneck link, the switch drops excess credit packets using rate-limiting. We use a Pica8 P-3780 10 GbE switch to configure rate-limiting on credit packets. Figure2shows the convergence characteristics of a naïve credit-based scheme compared to TCP cubic and DCTCP. It shows the credit-based design can converge to fairness in just one round-trip time, significantly outperforming the TCP variants. Small RTT and sub-packet regime<ref type="bibr" target="#b15">[16]</ref>: Finally, datacenter networks have a small base RTTs around tens of microseconds. Low latency cut-through switches even achieve sub-microsecond latency<ref type="bibr" target="#b7">[8]</ref>. Small RTT coupled with a large number of concurrent flows means each flow may send less than one packet per RTT on average<ref type="bibr" target="#b13">[14]</ref>even at 100 Gbps (10 Gbps), 416<ref type="bibr" target="#b41">(42)</ref> flows are enough to reach this</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 4 :</head><label>4</label><figDesc>Figure 4: Problems with naïve credit-based approach</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head></head><label></label><figDesc>4 credit queue, ∆d host = 1µs</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 5 :Figure 6 :</head><label>56</label><figDesc>Figure 5: Maximum buffer for ToR switch in 32-ary fat tree</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure 8 :</head><label>8</label><figDesc>Figure 8: Convergence time and credit waste trade-offs.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Figure 9 :Figure 10 :</head><label>910</label><figDesc>Figure 9: Credit queue capacity vs. Utilization</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>Figure 8 (</head><label>8</label><figDesc>a) shows that as the initial rate decreases, the convergence time increases from 2 RTTs to 14 RTTs.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head></head><label></label><figDesc>Figure 8 (b) shows the amount of credit waste with single packet flows in an idle network whose RTT is 100 µs.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_11"><head>Figure 11 :</head><label>11</label><figDesc>Figure 11: Fairness in multi-bottleneck topology</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_13"><head>Figure 13 :</head><label>13</label><figDesc>Figure 13: Convergence behavior (Testbed)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_14"><head>Figure 14 :</head><label>14</label><figDesc>Figure 14: Host and switch performance (Testbed)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_15"><head></head><label></label><figDesc>Queue length (ns-2)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_16"><head>Figure 15 :</head><label>15</label><figDesc>Figure 15: Queue length / fairness / utilization with many concurrent flows</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_17"><head>Figure 16 :</head><label>16</label><figDesc>Figure 16: Convergence time of ExpressPass, DCTCP and RCP at 10/100 Gbps bottleneck link</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_18"><head>Figure 16 (Figure 17 :</head><label>1617</label><figDesc>Figure 17: Shuffle workload (ns-2)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_19"><head></head><label></label><figDesc>1MB -(XL)Figure19: Average / 99%-ile flow completion time for realistic workload (10Gbps, load 0.6)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_20"><head>Figure 21 :</head><label>21</label><figDesc>Figure 21: Average speed-up of 40G over 10G with websearch, webserver workload (load 0.6).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 :</head><label>1</label><figDesc>Required buffer size for ToR down ports, ToR up ports, and Core ports with datacenter topology.</figDesc><table><row><cell cols="3">Topology (link/core-link speed) ToR down ToR up</cell><cell>Core</cell></row><row><cell>(Core/Aggr./ToR/Server)</cell><cell cols="3">(norm. by DCTCP K)</cell></row><row><cell>32-ary fat tree (10/40 Gbps)</cell><cell>577.3 KB</cell><cell cols="2">19.0 KB 131.1 KB</cell></row><row><cell>(16 / 512 / 512 / 8,192)</cell><cell>(5.77)</cell><cell>(0.19)</cell><cell>(0.33)</cell></row><row><cell>32-ary fat tree (40/100 Gbps)</cell><cell>1.06 MB</cell><cell cols="2">37.2 KB 221.8 KB</cell></row><row><cell>(16 / 512 / 512 / 8,192)</cell><cell>(2.65)</cell><cell>(0.09)</cell><cell>(0.22)</cell></row><row><cell>3-tier Clos (10/40 Gbps)</cell><cell>577.3 KB</cell><cell cols="2">19.0 KB 131.1 KB</cell></row><row><cell>(16 / 128 / 1024 / 8,192)</cell><cell>(5.77)</cell><cell>(0.19)</cell><cell>(0.33)</cell></row><row><cell>3-tier Clos (40/100 Gbps)</cell><cell>1.06 MB</cell><cell cols="2">37.2 KB 221.8 KB</cell></row><row><cell>(16 / 128 / 1024 / 8,192)</cell><cell>(2.65)</cell><cell>(0.09)</cell><cell>(0.22)</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head></head><label></label><figDesc>1: w ← w init 2: cur_rate ← initial_rate 3: repeat per update period (RTT by default)</figDesc><table><row><cell>4:</cell><cell>credit_loss = #_credit_dropped/#_credit_sent</cell></row><row><cell>5:</cell><cell>if credit_loss ≤ target_loss then</cell></row><row><cell>6:</cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head></head><label></label><figDesc>Algorithm 1 line 12) that reduces cur_rate by</figDesc><table><row><cell>Throughput</cell><cell cols="2">𝐷𝐷(𝑟𝑟 𝑜𝑜 + 1)</cell><cell cols="2">Credit sending rate Bandwidth allocation</cell></row><row><cell>𝒎𝒎𝒎𝒎𝒎𝒎_𝒓𝒓𝒎𝒎𝒓𝒓𝒓𝒓 𝑅𝑅  *</cell><cell></cell><cell></cell><cell></cell><cell>𝑟𝑟𝑟𝑟𝑟𝑟𝑡𝑡𝑟𝑟𝑟𝑟_𝑙𝑙𝑙𝑙𝑙𝑙𝑙𝑙 𝐷𝐷  *</cell></row><row><cell>𝑵𝑵</cell><cell></cell><cell>…</cell><cell></cell></row><row><cell></cell><cell>𝑟𝑟 o</cell><cell>𝑟𝑟 o + 1</cell><cell>𝑟𝑟 c</cell><cell>Time</cell></row><row><cell></cell><cell cols="3">Figure 12: Steady state behavior</cell></row><row><cell cols="5">(1 -credit_loss) • (1 + target_loss). At time t 0 + 1, increasing phase</cell></row><row><cell cols="5">of feedback control is triggered, and the aggregate credit sending</cell></row><row><cell>rate becomes:</cell><cell></cell><cell></cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 2 :</head><label>2</label><figDesc>Flow size distribution of realistic workload Figure 18: 99%-ile FCT of ExpressPass with different α and w init (10 Gbps, load 0.6) congestion control can contribute to the problem, and ExpressPass effectively alleviates this.</figDesc><table><row><cell></cell><cell cols="3">-10KB</cell><cell></cell><cell>(S)</cell><cell cols="2">78%</cell><cell></cell><cell cols="2">49%</cell><cell></cell><cell>50%</cell><cell></cell><cell>63%</cell></row><row><cell cols="6">10KB -100KB (M)</cell><cell></cell><cell>5%</cell><cell></cell><cell cols="2">3%</cell><cell></cell><cell>3%</cell><cell></cell><cell>18%</cell></row><row><cell cols="5">100KB -1MB</cell><cell>(L)</cell><cell></cell><cell>8%</cell><cell></cell><cell cols="2">18%</cell><cell></cell><cell>18%</cell><cell></cell><cell>19%</cell></row><row><cell cols="3">1MB -</cell><cell></cell><cell></cell><cell>(XL)</cell><cell></cell><cell>9%</cell><cell></cell><cell cols="2">20%</cell><cell></cell><cell>29%</cell><cell></cell></row><row><cell></cell><cell cols="5">Average flow size</cell><cell cols="5">7.41MB 1.6MB</cell><cell cols="4">701KB 64KB</cell></row><row><cell>99%-ile FCT</cell><cell>0 0.5 1 1.5 2</cell><cell>ms α</cell><cell>1/2 1/2</cell><cell>1/16 1/2</cell><cell cols="2">1/16 1/16 Data Mining 1/32 1/16 Cache Follower Web Server 1/32 1/32</cell><cell>99%-ile FCT</cell><cell>0 0.1 0.2 0.3 0.4</cell><cell>s α</cell><cell>1/2 1/2</cell><cell>1/16 1/2</cell><cell>1/16 1/16</cell><cell>1/32 1/16</cell><cell>1/32 1/32</cell></row><row><cell></cell><cell></cell><cell cols="4">(a) 0 -10KB (S)</cell><cell></cell><cell></cell><cell cols="6">(b) 100KB -1MB (L)</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 3 :</head><label>3</label><figDesc>Average/maximum queue occupancy (ns-2) @ 10Gbps</figDesc><table /></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_0"><p>Note multiple workers can share the same host, when the number of workers exceeds the number of hosts.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4" xml:id="foot_1"><p>We set the DCTCP parameter K= 65, g= 0.0625 for 10 Gbps link, and K= 650, g= 0.01976 for 100 Gbps link. For ExpressPass, we report the average throughput for each RTT. For DCTCP, we averaged over 10 RTT due to its high variance.</p></note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>ACKNOWLEDGEMENT</head><p>We thank our shepherd Changhoon Kim and anonymous reviewers for their valuable feedback. We also thank Wei Bai for sharing his expertise in switch configurations, Sangjin Han for his advice on using SoftNIC, Yibo Zhu for providing the DCQCN simulator, and Changhyun Lee for providing the DX simulator. We appreciate valuable feedback from David Wetherall, Nandita Dukkipati, Judson Wilson, Aurojit Panda, Seongmin Kim, and Jaehyeong Ha. This work was supported by IITP grants funded by the Korea government (MSIP) (No.R-20160222-002755 and No.2015-0-00164).</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Oh Flow, Are Thou Happy? TCP Sendbuffer Advertising for Make Benefit of Clouds and Tenants</title>
		<author>
			<persName><forename type="first">Alexandru</forename><surname>Agache</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Costin</forename><surname>Raiciu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 7th USENIX Conference on Hot Topics in Cloud Computing</title>
		<meeting>the 7th USENIX Conference on Hot Topics in Cloud Computing</meeting>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">A scalable, commodity data center network architecture</title>
		<author>
			<persName><forename type="first">Mohammad</forename><surname>Al-Fares</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alexander</forename><surname>Loukissas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Amin</forename><surname>Vahdat</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACM SIGCOMM</title>
		<imprint>
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Data center TCP (dctcp)</title>
		<author>
			<persName><forename type="first">Mohammad</forename><surname>Alizadeh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Albert</forename><surname>Greenberg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><forename type="middle">A</forename><surname>Maltz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jitendra</forename><surname>Padhye</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Parveen</forename><surname>Patel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Balaji</forename><surname>Prabhakar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sudipta</forename><surname>Sengupta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Murari</forename><surname>Sridharan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACM SIGCOMM</title>
		<imprint>
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Analysis of DCTCP: stability, convergence, and fairness</title>
		<author>
			<persName><forename type="first">Mohammad</forename><surname>Alizadeh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Adel</forename><surname>Javanmard</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Balaji</forename><surname>Prabhakar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the ACM SIGMET-RICS joint international conference on Measurement and modeling of computer systems</title>
		<meeting>the ACM SIGMET-RICS joint international conference on Measurement and modeling of computer systems</meeting>
		<imprint>
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Less is more: trading a little bandwidth for ultra-low latency in the data center</title>
		<author>
			<persName><forename type="first">Mohammad</forename><surname>Alizadeh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Abdul</forename><surname>Kabbani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tom</forename><surname>Edsall</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Balaji</forename><surname>Prabhakar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">USENIX Symposium on Networked Systems Design and Implementation</title>
		<imprint>
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
	<note>Amin Vahdat, and Masato Yasuda</note>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">pfabric: Minimal near-optimal datacenter transport</title>
		<author>
			<persName><forename type="first">Mohammad</forename><surname>Alizadeh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shuang</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Milad</forename><surname>Sharif</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sachin</forename><surname>Katti</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nick</forename><surname>Mckeown</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Balaji</forename><surname>Prabhakar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Scott</forename><surname>Shenker</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACM SIGCOMM</title>
		<imprint>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Reining in the Outliers in Map-Reduce Clusters using Mantri</title>
		<author>
			<persName><forename type="first">Srikanth</forename><surname>Ganesh Ananthanarayanan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Albert</forename><forename type="middle">G</forename><surname>Kandula</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ion</forename><surname>Greenberg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yi</forename><surname>Stoica</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bikas</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Edward</forename><surname>Saha</surname></persName>
		</author>
		<author>
			<persName><surname>Harris</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">USENIX OSDI</title>
		<imprint>
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<ptr target="https://www.arista.com/assets/data/pdf/CloudNetworkLatency.pdf" />
		<title level="m">Architecting Low Latency Cloud Networks</title>
		<imprint>
			<publisher>Arista Networks</publisher>
			<date type="published" when="2016-01">2016. 2016. Jan-2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<ptr target="https://www.arista.com/assets/data/pdf/Datasheets/7280R-DataSheet.pdf" />
		<title level="m">Arista 7280R Series Data Center Switch Router Data Sheet</title>
		<imprint>
			<publisher>Arista Networks</publisher>
			<date type="published" when="2016-01">2016. 2016. Jan-2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<author>
			<persName><forename type="first">Arista</forename><surname>Networks</surname></persName>
		</author>
		<ptr target="https://www.arista.com/assets/data/pdf/Datasheets/7050SX-128_64_Datasheet.pdf" />
		<title level="m">7050SX Series 10/40G Data Center Switches Data Sheet</title>
		<imprint>
			<date type="published" when="2017-01">2017. 2017. Jan-2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Information-agnostic flow scheduling for commodity data centers</title>
		<author>
			<persName><forename type="first">Wei</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Li</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kai</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dongsu</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chen</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hao</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">12th USENIX Symposium on Networked Systems Design and Implementation</title>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="volume">15</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<author>
			<persName><forename type="first">Andreas</forename><surname>Bechtolsheim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lincoln</forename><surname>Dale</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hugh</forename><surname>Holbrook</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ang</forename><surname>Li</surname></persName>
		</author>
		<ptr target="https://www.arista.com/assets/data/pdf/Whitepapers/BigDataBigBuffers-WP.pdf" />
		<title level="m">Why Big Data Needs Big Buffer Switches</title>
		<imprint>
			<date type="published" when="2016-01">2016. 2016. Jan-2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Network Traffic Characteristics of Data Centers in the Wild</title>
		<author>
			<persName><forename type="first">Theophilus</forename><surname>Benson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aditya</forename><surname>Akella</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><forename type="middle">A</forename><surname>Maltz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. 10th ACM SIGCOMM Conference on Internet Measurement</title>
		<meeting>10th ACM SIGCOMM Conference on Internet Measurement</meeting>
		<imprint>
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Scaling tcp&apos;s congestion window for small round trip times</title>
		<author>
			<persName><forename type="first">Bob</forename><surname>Briscoe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Koen</forename><surname>De</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Schepper</forename></persName>
		</author>
		<idno>TR-TUB8-2015-002</idno>
	</analytic>
	<monogr>
		<title level="j">BT</title>
		<imprint>
			<date type="published" when="2015">2015. 2015</date>
		</imprint>
	</monogr>
	<note type="report_type">Technical report</note>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<author>
			<persName><surname>Broadcom</surname></persName>
		</author>
		<ptr target="https://docs.broadcom.com/docs/12358326" />
		<title level="m">Smart-Hash -Broadcom</title>
		<imprint>
			<date type="published" when="2012-01">2012. 2012. Jan-2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">TCP Behavior in Sub Packet Regimes</title>
		<author>
			<persName><forename type="first">Jay</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Janardhan</forename><surname>Iyengar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lakshminarayanan</forename><surname>Subramanian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bryan</forename><surname>Ford</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ACM SIGMETRICS Joint International Conference on Measurement and Modeling of Computer Systems</title>
		<meeting>ACM SIGMETRICS Joint International Conference on Measurement and Modeling of Computer Systems</meeting>
		<imprint>
			<date type="published" when="2011">2011</date>
			<biblScope unit="volume">2</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<author>
			<persName><surname>Cisco</surname></persName>
		</author>
		<ptr target="http://www.cisco.com/c/en/us/products/collateral/switches/nexus-7000-series-switches/white_paper_c11-687554.html" />
		<title level="m">Nexus 7000 FabricPath</title>
		<imprint>
			<date type="published" when="2013-01">2013. 2013. Jan-2017</date>
		</imprint>
	</monogr>
	<note>Section 7.2.1 Equal-Cost Multipath Forwarding</note>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<ptr target="http://www.chelsio.com/wp-content/uploads/2013/10/Ultra-Low-Latency-Report.pdf" />
		<title level="m">Preliminary Ultra Low Latency Report</title>
		<imprint>
			<publisher>Chelsio Communications</publisher>
			<date type="published" when="2013-01">2013. 2013. Jan-2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<author>
			<persName><forename type="first">Sujal</forename><surname>Das</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rochan</forename><surname>Sankar</surname></persName>
		</author>
		<ptr target="https://docs.broadcom.com/docs/12358325" />
		<title level="m">Broadcom Smart-Buffer Technology in Data Center Switches for Cost-Effective Performance Scaling of Cloud Applications</title>
		<imprint>
			<date type="published" when="2012-01">2012. 2012. Jan-2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">MapReduce: simplified data processing on large clusters</title>
		<author>
			<persName><forename type="first">Jeffrey</forename><surname>Dean</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sanjay</forename><surname>Ghemawat</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Commun. ACM</title>
		<imprint>
			<biblScope unit="volume">51</biblScope>
			<biblScope unit="page">1</biblScope>
			<date type="published" when="2008">2008. 2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<author>
			<persName><surname>Dell</surname></persName>
		</author>
		<ptr target="http://topics-cdn.dell.com/pdf/force10-mxl-blade_Service%20Manual4_en-us.pdf" />
		<title level="m">Dell Networking Configuration Guide for the MXL 10/40GbE Switch I/O Module 9.9(0.0)</title>
		<imprint>
			<date type="published" when="2015-01">2015. 2015. Jan-2017</date>
		</imprint>
	</monogr>
	<note>Enabling Deterministic ECMP Next Hop (pp.329)</note>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">On the impact of packet spraying in data center networks</title>
		<author>
			<persName><forename type="first">Advait</forename><surname>Dixit</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pawan</forename><surname>Prakash</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Charlie</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ramana</forename><surname>Rao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kompella</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">INFOCOM, 2013 Proceedings IEEE</title>
		<imprint>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">Rate Control Protocol (RCP): Congestion control to make flows complete quickly</title>
		<author>
			<persName><forename type="first">Nandita</forename><surname>Dukkipati</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2008">2008</date>
			<pubPlace>Stanford University</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Processor sharing flows in the internet</title>
		<author>
			<persName><forename type="first">Nandita</forename><surname>Dukkipati</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Masayoshi</forename><surname>Kobayashi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rui</forename><surname>Zhang-Shen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nick</forename><surname>Mckeown</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Workshop on Quality of Service</title>
		<imprint>
			<date type="published" when="2005">2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Clearing the Clouds: A Study of Emerging Scale-out Workloads on Modern Hardware</title>
		<author>
			<persName><forename type="first">Almutaz</forename><surname>Michael Ferdman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Onur</forename><surname>Adileh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stavros</forename><surname>Kocberber</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mohammad</forename><surname>Volos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Djordje</forename><surname>Alisafaee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Cansu</forename><surname>Jevdjic</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Adrian</forename><forename type="middle">Daniel</forename><surname>Kaynak</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Anastasia</forename><surname>Popescu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Babak</forename><surname>Ailamaki</surname></persName>
		</author>
		<author>
			<persName><surname>Falsafi</surname></persName>
		</author>
		<idno type="DOI">10.1145/2150976.2150982</idno>
		<ptr target="https://doi.org/10.1145/2150976.2150982" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Seventeenth International Conference on Architectural Support for Programming Languages and Operating Systems (ASPLOS XVII)</title>
		<meeting>the Seventeenth International Conference on Architectural Support for Programming Languages and Operating Systems (ASPLOS XVII)<address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2012">2012</date>
			<biblScope unit="page">12</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">pHost: Distributed near-optimal datacenter transport over commodity network fabric</title>
		<author>
			<persName><forename type="first">Akshay</forename><surname>Peter X Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gautam</forename><surname>Narayan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rachit</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sylvia</forename><surname>Agarwal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Scott</forename><surname>Ratnasamy</surname></persName>
		</author>
		<author>
			<persName><surname>Shenker</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACM CoNEXT</title>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<title level="m" type="main">Modifying Shortest Path Routing Protocols to Create Symmetrical Routes</title>
		<author>
			<persName><forename type="first">Rajib</forename><surname>Ghosh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">George</forename><surname>Varghese</surname></persName>
		</author>
		<idno>UCSD technical report CS2001- 0685</idno>
		<imprint>
			<date type="published" when="2001-09">2001. 2001. September 2001</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">VL2: a scalable and flexible data center network</title>
		<author>
			<persName><forename type="first">Albert</forename><surname>Greenberg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Navendu</forename><surname>James R Hamilton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Srikanth</forename><surname>Jain</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Changhoon</forename><surname>Kandula</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Parantap</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><forename type="middle">A</forename><surname>Lahiri</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Parveen</forename><surname>Maltz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sudipta</forename><surname>Patel</surname></persName>
		</author>
		<author>
			<persName><surname>Sengupta</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In ACM SIG-COMM</title>
		<imprint>
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">CUBIC: a new TCP-friendly high-speed TCP variant</title>
		<author>
			<persName><forename type="first">Sangtae</forename><surname>Ha</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Injong</forename><surname>Rhee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lisong</forename><surname>Xu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM SIGOPS Operating Systems Review</title>
		<imprint>
			<biblScope unit="volume">42</biblScope>
			<date type="published" when="2008">2008. 2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">FCP: A Flexible Transport Framework for Accommodating Diversity</title>
		<author>
			<persName><forename type="first">Dongsu</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Robert</forename><surname>Grandl</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aditya</forename><surname>Akella</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Srinivasan</forename><surname>Seshan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACM SIG-COMM</title>
		<imprint>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
		<title level="m" type="main">SoftNIC: A software NIC to augment hardware</title>
		<author>
			<persName><forename type="first">Sangjin</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Keon</forename><surname>Jang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aurojit</forename><surname>Panda</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shoumik</forename><surname>Palkar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dongsu</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sylvia</forename><surname>Ratnasamy</surname></persName>
		</author>
		<idno>UCB/EECS-2015-155</idno>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
		<respStmt>
			<orgName>EECS Department, University of California, Berkeley</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">Technical Report</note>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Packet slicing for highly concurrent TCPs in data center networks with COTS switches</title>
		<author>
			<persName><forename type="first">Jiawei</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yi</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jianxin</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tian</forename><surname>He</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE ICNP</title>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
		<title level="m" type="main">A quantitative measure of fairness and discrimination for resource allocation in shared computer system</title>
		<author>
			<persName><forename type="first">Raj</forename><surname>Jain</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dah-Ming</forename><surname>Chiu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">William</forename><forename type="middle">R</forename><surname>Hawe</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1984">1984. 1984</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">High speed networks need proactive congestion control</title>
		<author>
			<persName><forename type="first">Lavanya</forename><surname>Jose</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lisa</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mohammad</forename><surname>Alizadeh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">George</forename><surname>Varghese</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nick</forename><surname>Mckeown</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sachin</forename><surname>Katti</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 14th ACM Workshop on Hot Topics in Networks</title>
		<meeting>the 14th ACM Workshop on Hot Topics in Networks</meeting>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Congestion control for high bandwidth-delay product networks</title>
		<author>
			<persName><forename type="first">Dina</forename><surname>Katabi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mark</forename><surname>Handley</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Charlie</forename><surname>Rohrs</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACM SIGCOMM</title>
		<imprint>
			<date type="published" when="2002">2002</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Credit-based flow control for ATM networks: credit update protocol, adaptive credit allocation and statistical multiplexing</title>
		<author>
			<persName><forename type="first">Trevor</forename><surname>Ht Kung</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alan</forename><surname>Blackwell</surname></persName>
		</author>
		<author>
			<persName><surname>Chapman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACM SIGCOMM</title>
		<imprint>
			<date type="published" when="1994">1994</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<monogr>
		<title level="m" type="main">Network Calculus: A Theory of Deterministic Queuing Systems for the Internet</title>
		<author>
			<persName><forename type="first">Jean-Yves</forename></persName>
		</author>
		<author>
			<persName><forename type="first">Le</forename><surname>Boudec</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Patrick</forename><surname>Thiran</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2001">2001</date>
			<publisher>Springer-Verlag</publisher>
			<pubPlace>Berlin, Heidelberg</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Accurate latency-based congestion feedback for datacenters</title>
		<author>
			<persName><forename type="first">Changhyun</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chunjong</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Keon</forename><surname>Jang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sue</forename><surname>Moon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dongsu</forename><surname>Han</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">USENIX Annual Technical Conference</title>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Kevin Fall, Kannan Varadhan, and others</title>
		<author>
			<persName><forename type="first">Steven</forename><surname>Mccanne</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sally</forename><surname>Floyd</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Network</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<date type="published" when="1997">1997. 1997</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<monogr>
		<ptr target="https://azure.microsoft.com/en-us/updates/azure-support-for-linux-rdma" />
		<title level="m">Azure support for Linux RDMA</title>
		<imprint>
			<publisher>Microsoft</publisher>
			<date type="published" when="2015-07">2015. 2015. -July-2016</date>
			<biblScope unit="volume">12</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">TIMELY: RTT-based Congestion Control for the Datacenter</title>
		<author>
			<persName><forename type="first">Radhika</forename><surname>Mittal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nandita</forename><surname>Dukkipati</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Emily</forename><surname>Blem</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hassan</forename><surname>Wassel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Monia</forename><surname>Ghobadi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Amin</forename><surname>Vahdat</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yaogong</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><surname>Wetherall</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">David Zats, and others</title>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
	<note>ACM SIGCOMM</note>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Recursively Cautious Congestion Control</title>
		<author>
			<persName><forename type="first">Radhika</forename><surname>Mittal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Justine</forename><surname>Sherry</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sylvia</forename><surname>Ratnasamy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Scott</forename><surname>Shenker</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">USENIX Conference on Networked Systems Design and Implementation</title>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Friends, not foes: synthesizing existing transport strategies for data center networks</title>
		<author>
			<persName><forename type="first">Ali</forename><surname>Munir</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ghufran</forename><surname>Baig</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ihsan</forename><forename type="middle">A</forename><surname>Syed M Irteza</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alex</forename><forename type="middle">X</forename><surname>Qazi</surname></persName>
		</author>
		<author>
			<persName><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><surname>Fahad R Dogar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACM SIGCOMM</title>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<monogr>
		<title level="m" type="main">NUMFabric: Fast and Flexible Bandwidth Allocation in Datacenters</title>
		<author>
			<persName><forename type="first">Kanthi</forename><surname>Nagaraj</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dinesh</forename><surname>Bharadia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hongzi</forename><surname>Mao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sandeep</forename><surname>Chinchali</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mohammad</forename><surname>Alizadeh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sachin</forename><surname>Katti</surname></persName>
		</author>
		<idno>ACM SIGCOMM. 14</idno>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<monogr>
		<ptr target="https://www.juniper.net/techpubs/en_US/junos15.1/topics/task/configuration/802-3ad-lags-load-balancing-symmetric-hashing-mx-series-pic-level-configuring.html" />
		<title level="m">Configuring PIC-Level Symmetrical Hashing for Load Balancing on 802.3ad LAGs for MX Series Routers</title>
		<imprint>
			<publisher>Juniper Networks</publisher>
			<date type="published" when="2016-01">2016. 2016. Jan-2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Modeling TCP throughput: A simple model and its empirical validation</title>
		<author>
			<persName><forename type="first">Jitendra</forename><surname>Padhye</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Victor</forename><surname>Firoiu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Don</forename><surname>Towsley</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jim</forename><surname>Kurose</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM SIGCOMM Computer Communication Review</title>
		<imprint>
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="page">4</biblScope>
			<date type="published" when="1998">1998. 1998</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Fastpass: A centralized zero-queue datacenter network</title>
		<author>
			<persName><forename type="first">Jonathan</forename><surname>Perry</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Amy</forename><surname>Ousterhout</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hari</forename><surname>Balakrishnan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Devavrat</forename><surname>Shah</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hans</forename><surname>Fugal</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACM SIGCOMM</title>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">SENIC: Scalable NIC for End-Host Rate Limiting</title>
		<author>
			<persName><forename type="first">Yilong</forename><surname>Sivasankar Radhakrishnan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vimalkumar</forename><surname>Geng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Abdul</forename><surname>Jeyakumar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">George</forename><surname>Kabbani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Amin</forename><surname>Porter</surname></persName>
		</author>
		<author>
			<persName><surname>Vahdat</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NSDI</title>
		<imprint>
			<date type="published" when="2014">2014</date>
			<biblScope unit="volume">14</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">NicPic: Scalable and Accurate End-Host Rate Limiting</title>
		<author>
			<persName><forename type="first">Vimalkumar</forename><surname>Sivasankar Radhakrishnan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Abdul</forename><surname>Jeyakumar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">George</forename><surname>Kabbani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Amin</forename><surname>Porter</surname></persName>
		</author>
		<author>
			<persName><surname>Vahdat</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">USENIX HotCloud</title>
		<imprint>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Inside the social network&apos;s (datacenter) network</title>
		<author>
			<persName><forename type="first">Arjun</forename><surname>Roy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hongyi</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jasmeet</forename><surname>Bagga</surname></persName>
		</author>
		<author>
			<persName><forename type="first">George</forename><surname>Porter</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alex</forename><forename type="middle">C</forename><surname>Snoeren</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In ACM SIGCOMM Computer Communication Review</title>
		<imprint>
			<biblScope unit="volume">45</biblScope>
			<date type="published" when="2015">2015</date>
			<publisher>ACM</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<monogr>
		<title level="m" type="main">Method for routing data packets in a fat tree network</title>
		<author>
			<persName><forename type="first">M</forename><surname>Schlansker</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Tourrilhes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Turner</surname></persName>
		</author>
		<ptr target="https://www.google.com/patents/US9007895USPatent9" />
		<imprint>
			<date type="published" when="2015-04-14">2015. April 14 2015</date>
			<biblScope unit="page">895</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Jupiter rising: A decade of clos topologies and centralized control in google&apos;s datacenter network</title>
		<author>
			<persName><forename type="first">Arjun</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Joon</forename><surname>Ong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Amit</forename><surname>Agarwal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Glen</forename><surname>Anderson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ashby</forename><surname>Armistead</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Roy</forename><surname>Bannon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Seb</forename><surname>Boving</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gaurav</forename><surname>Desai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bob</forename><surname>Felderman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACM SIGCOMM</title>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
	<note>Paulie Germano, and others</note>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">A Versatile, Low Latency HyperTransport Core</title>
		<author>
			<persName><forename type="first">David</forename><surname>Slogsnat</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alexander</forename><surname>Giese</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ulrich</forename><surname>Brüning</surname></persName>
		</author>
		<idno type="DOI">10.1145/1216919.1216926</idno>
		<ptr target="https://doi.org/10.1145/1216919.1216926" />
	</analytic>
	<monogr>
		<title level="m">ACM/SIGDA International Symposium on Field Programmable Gate Arrays</title>
		<imprint>
			<date type="published" when="2007">2007</date>
			<biblScope unit="volume">8</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<monogr>
		<title/>
		<author>
			<persName><forename type="first">Jim</forename><surname>Warner</surname></persName>
		</author>
		<ptr target="https://people.ucsc.edu/~warner/buffer.html" />
		<imprint>
			<date type="published" when="2014-01">2014. 2014. Jan-2017</date>
		</imprint>
	</monogr>
	<note>Packet Buffer</note>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">ICTCP: Incast Congestion Control for TCP in Data-Center Networks</title>
		<author>
			<persName><forename type="first">H</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE/ACM Transactions on Networking</title>
		<imprint>
			<biblScope unit="volume">21</biblScope>
			<date type="published" when="2013">2013. 2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<analytic>
		<title level="a" type="main">Binary increase congestion control (BIC) for fast long-distance networks</title>
		<author>
			<persName><forename type="first">Lisong</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Khaled</forename><surname>Harfoush</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Injong</forename><surname>Rhee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">INFOCOM 2004. Twenty-third AnnualJoint Conference of the IEEE Computer and Communications Societies</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2004">2004</date>
			<biblScope unit="volume">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<analytic>
		<title level="a" type="main">A DoS-limiting Network Architecture</title>
		<author>
			<persName><forename type="first">Xiaowei</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><surname>Wetherall</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Thomas</forename><surname>Anderson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACM SIGCOMM</title>
		<imprint>
			<date type="published" when="2005">2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<analytic>
		<title level="a" type="main">Congestion control for large-scale RDMA deployments</title>
		<author>
			<persName><forename type="first">Yibo</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Haggai</forename><surname>Eran</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Daniel</forename><surname>Firestone</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chuanxiong</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Marina</forename><surname>Lipshteyn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yehonatan</forename><surname>Liron</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jitendra</forename><surname>Padhye</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Shachar Raindel, Mohamad Haj Yahia, and Ming Zhang</title>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
	<note>ACM SIGCOMM</note>
</biblStruct>

<biblStruct xml:id="b58">
	<analytic>
		<title level="a" type="main">ECN or Delay: Lessons Learnt from Analysis of DCQCN and TIMELY</title>
		<author>
			<persName><forename type="first">Yibo</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Monia</forename><surname>Ghobadi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vishal</forename><surname>Misra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jitendra</forename><surname>Padhye</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACM CoNEXT</title>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
