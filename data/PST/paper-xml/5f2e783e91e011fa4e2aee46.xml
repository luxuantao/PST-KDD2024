<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Real-World Super-Resolution via Kernel Estimation and Noise Injection</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Ji</forename><surname>Xiaozhong</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Nanjing University</orgName>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="laboratory">Tencent Youtu Lab</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Yun</forename><surname>Cao</surname></persName>
							<email>yuncao@tencent.com</email>
							<affiliation key="aff1">
								<orgName type="laboratory">Tencent Youtu Lab</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Ying</forename><surname>Tai</surname></persName>
							<email>yingtai@tencent.com</email>
							<affiliation key="aff1">
								<orgName type="laboratory">Tencent Youtu Lab</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Chengjie</forename><surname>Wang</surname></persName>
							<affiliation key="aff1">
								<orgName type="laboratory">Tencent Youtu Lab</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Jilin</forename><surname>Li</surname></persName>
							<affiliation key="aff1">
								<orgName type="laboratory">Tencent Youtu Lab</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Feiyue</forename><surname>Huang</surname></persName>
							<affiliation key="aff1">
								<orgName type="laboratory">Tencent Youtu Lab</orgName>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff2">
								<orgName type="institution">Carleton University</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Real-World Super-Resolution via Kernel Estimation and Noise Injection</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">9F04F508EA22B68B31A8E354309E548F</idno>
					<idno type="DOI">10.1109/CVPRW50498.2020.00241</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.3" ident="GROBID" when="2023-07-28T11:44+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Recent state-of-the-art super-resolution methods have achieved impressive performance on ideal datasets regardless of blur and noise. However, these methods always fail in real-world image super-resolution, since most of them adopt simple bicubic downsampling from highquality images to construct Low-Resolution (LR) and High-Resolution (HR) pairs for training which may lose track of frequency-related details. To address this issue, we focus on designing a novel degradation framework for realworld images by estimating various blur kernels as well as real noise distributions. Based on our novel degradation framework, we can acquire LR images sharing a common domain with real-world images. Then, we propose a realworld super-resolution model aiming at better perception.</head><p>Extensive experiments on synthetic noise data and realworld images demonstrate that our method outperforms the state-of-the-art methods, resulting in lower noise and better visual quality. In addition, our method is the winner of NTIRE 2020 Challenge on both tracks of Real-World Super-Resolution, which significantly outperforms other competitors by large margins.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Super-Resolution (SR) task is to increase the resolution of low-quality images, and enhance its clarity <ref type="bibr" target="#b1">[2]</ref>. In recent years, deep learning-based methods <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b34">35,</ref><ref type="bibr" target="#b6">7,</ref><ref type="bibr" target="#b33">34,</ref><ref type="bibr" target="#b19">20,</ref><ref type="bibr" target="#b18">19,</ref><ref type="bibr" target="#b22">23]</ref> have achieved remarkable results with respect to fidelity performance, which mainly focuses on designing network structures to further improve the performance of specific datasets. Most of them use fixed bicubic operation for downsampling to construct training data pairs. Similarly, in test phase, the input image downsampled by bicubic kernel is feed to the designed network. Subsequently, the generated results will be compared with Ground Truth (GT) to calculate PSNR, SSIM and other metrics. Despite the improvement of fidelity, a problem ignored by these methods is that downsampling with the ideal bicubic is unreasonable. Previous methods construct data by ideal downsampling method:</p><formula xml:id="formula_0">I LR = I HR ↓ s , (<label>1</label></formula><formula xml:id="formula_1">)</formula><p>where I LR and I HR indicate the LR and HR image, respectively, and s denotes the scale factor. This makes it easy to obtain paired data for training models. However, with such a known and fixed downsampling kernel, the degraded images may lose high-frequency details but make the lowfrequency content more clear. Based on such constructed paired data, SR model f (•) is trained to minimize the average error of n images:</p><formula xml:id="formula_2">arg min f Σ f (I i LR ) -I i HR , i ∈ {1, 2 • • • n}. (<label>2</label></formula><formula xml:id="formula_3">)</formula><p>If testing on the same downsampling dataset, the generated results are as expected. However, once we directly test on the original image, the results are very blurry with lots of noise. The main reason is that the bicubic downsampled image does not belong to the same domain as the original image. Due to the domain gap, these methods produce unpleasant artifacts and fail on real-world images. For example, EDSR/ZSSR produce unsatisfied result of a real image in Figure <ref type="figure" target="#fig_0">1</ref>. Therefore, the key problem of real-world superresolution is to introduce an accurate degradation method to ensure the generated Low-Resolution (LR) image and the original image with the same domain attributes.</p><p>We first analyze the impact of different kernels on the downsampled image <ref type="bibr" target="#b41">[42,</ref><ref type="bibr" target="#b48">49,</ref><ref type="bibr" target="#b10">11]</ref>. Before our analysis, we define the original real images as the source domain X , and the clean High-Resolution (HR) images as the target domain Y. We found blur kernels with different degrees directly affect the blur of the downsampled images. Bicubic can be regarded as an ideal way of downsampling because it retains the information from X as much as possible. However, the frequency of these downsampled images has changed to another domain X . When training on {X , Y}, the model will try to recover all the details due to all information is important in the domain X . The model works well on I LR but usually fails on I src ∈ X, which is an unprocessed real image. Another problem is the downsampled image has almost no noise, while real-world images in X usually have a lot. Mere estimation of the blurry kernel cannot accurately model the degradation process.</p><p>In this paper, we propose a novel Realistic degradation framework for Super-Resolution (RealSR), which contains kernel estimation and noise injection to preserve the original domain attributes. On one hand, we first use the existing kernel estimation method <ref type="bibr" target="#b2">[3]</ref> to generate more realistic LR images. On the other hand, we propose a simple and effective method to directly collect noise from the original image and add it to the downsampled image. Further, we introduce the patch discriminator <ref type="bibr" target="#b16">[17]</ref> for RealSR to avoid generated artifacts. To verify the effectiveness of the proposed method, we conduct experiments on synthetic dataset and real dataset. The experimental results show that our method produces clearer and cleaner results compared with state-of-the-art methods. Finally, we conduct ablation experiments to verify the effectiveness of the kernel estimation, noise injection, and the patch discriminator for SR generator, respectively. We also participate in the NTIRE 2020 Challenge on Real-World Super-Resolution, and outperform other competitors by large margins on both tracks.</p><p>In summary, our overall contribution is three-fold:</p><p>• We propose a novel degradation framework RealSR under real-world setting, which provides realistic images for super-resolution learning.</p><p>• By estimating the kernel and noise, we explore the specific degradation of blurry and noisy images.</p><p>• We demonstrate that the proposed RealSR achieves state-of-the-art results in terms of visual quality.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Work</head><p>Super-Resolution Recently, many Convolutional Neural Networks (CNN)-based SR networks <ref type="bibr" target="#b22">[23,</ref><ref type="bibr" target="#b29">30,</ref><ref type="bibr" target="#b12">13,</ref><ref type="bibr" target="#b13">14,</ref><ref type="bibr" target="#b21">22,</ref><ref type="bibr" target="#b30">31,</ref><ref type="bibr" target="#b40">41]</ref> achieve strong performance on bicubic downsampling images. Among them, the representative is EDSR <ref type="bibr" target="#b22">[23]</ref>, which uses a deep residual network for training SR model. Zhang et al. <ref type="bibr" target="#b45">[46]</ref> propose a residual in residual structure to form very deep network which achieves better performance than EDSR. Dai et al. <ref type="bibr" target="#b7">[8]</ref> propose a second-order channel attention module to adaptively rescale the channel-wise features by using second-order feature statistics for more discriminative representations. Haris et al. <ref type="bibr" target="#b11">[12]</ref> propose deep back-projection networks to exploit iterative up-and downsampling layers, providing an error feedback mechanism for projection errors at each stage. Although the authors have achieved good performance with respect to fidelity, the generated images have poor visual effects and appear blurry. To address this issue, some researchers enhanced realistic texture via spatial feature transform <ref type="bibr" target="#b46">[47,</ref><ref type="bibr" target="#b47">48,</ref><ref type="bibr" target="#b37">38]</ref>. Soh et al. propose a natural manifold discrimination to classify HR images with blurry or noisy images, which is used to supervise the quality of the generated images. Furthermore, some Generative Adversarial Networks (GAN)-based methods <ref type="bibr" target="#b20">[21,</ref><ref type="bibr" target="#b43">44,</ref><ref type="bibr" target="#b38">39]</ref> pay more attention to visual effects, introducing adversarial losses and perceptual losses. However, these SR models trained on the data generated by bicubic kernel can only work well on clean HR data, because the model has never seen blurry/noisy data during training. This is inconsistent with real-world needs, and real LR images often carry noise and blur. To address this conflict, Xu et al. <ref type="bibr" target="#b39">[40,</ref><ref type="bibr" target="#b4">5,</ref><ref type="bibr" target="#b3">4,</ref><ref type="bibr" target="#b44">45]</ref> collect raw photo pairs directly from nature scene with particular camera equipment. But collecting such paired data requires strict conditions and a lot of manual costs. In this paper, we focus on the training strategy of SR networks in real data by analyzing the degradation in real images.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Real-World Super-Resolution</head><p>To overcome these challenges of real-world super-resolution, recent work <ref type="bibr" target="#b41">[42,</ref><ref type="bibr" target="#b48">49]</ref> combined with denoising or deblurring have been proposed. These methods are trained on the artificially constructed blurry and noise-added data, which further enhanced the robustness of the SR model. However, these explicit modeling methods need sufficient prior about blur/noise, therefore the scope of application is limited.</p><p>Recently, a series of real-world super-resolution challenges <ref type="bibr" target="#b24">[25,</ref><ref type="bibr" target="#b25">26]</ref> have attracted many participants. Many novel methods are proposed to solve this problem. For instance, Fritsche et al. <ref type="bibr" target="#b9">[10]</ref> propose the DSGAN model to generate degraded images. Lugmayr et al. <ref type="bibr" target="#b23">[24]</ref> propose an unsupervised learning method for real-world superresolution. ZSSR <ref type="bibr" target="#b31">[32]</ref> abandon the training process on big data and train a small model for each test image so that specific models pay more attention to the internal information of the image. But the price paid is that the time for inference is greatly increased, which is difficult to apply to the real scene. Different from these methods, we explicitly estimate the kernel degradation in real images, which is very important for generating clear and sharp results. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">The Proposed RealSR</head><p>In this section, we introduce the proposed degradation method as shown in Figure <ref type="figure" target="#fig_1">2</ref>. Our method is mainly divided into two stages. The first stage is to estimate the degradation from real data and generate realistic LR images. The second stage is to train the SR model based on the constructed data.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Realistic Degradation for Super-Resolution</head><p>Here, we introduce a novel method of real image degradation based on kernel estimation and noise injection. Assume that the LR image is obtained by the following degradation method:</p><formula xml:id="formula_4">I LR = (I HR * k)↓ s + n,<label>(3)</label></formula><p>where k and n indicate blurry kernel and noise, respectively. I HR is unknown, indicating that k and n are also unknown. In order to estimate the degradation method more accurately, we explicitly estimate the kernel and noise from the image. After getting the estimated kernel and noise patch, we build a degradation pool, which is used to degrade clean HR images into blurry and noisy images, thereby generating image pairs for training SR models. To describe our method concisely, we formalize this data-constructing pipeline as an algorithm shown in Algorithm 1. Estimate k from I src by solving Eqn. 4</p><p>5:</p><p>Add k to K 6:</p><p>Crop n from I src 7:</p><p>if n meet Eqn. 7 then 8:</p><p>Add n to N </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Kernel Estimation and Downsampling</head><p>We use a kernel estimation algorithm to explicitly estimate kernels from real images. Inspired by KernelGAN <ref type="bibr" target="#b2">[3]</ref>, we adopt a similar kernel estimation method and set appropriate parameters based on real images. The generator of KernelGAN is a linear model without any activation layers, therefore the parameters of all layers can be combined into a fixed kernel. The estimated kernel needs to meet the following constraints:</p><formula xml:id="formula_5">arg min k (I src * k)↓ s -I src ↓ s 1 + |1 -Σ k i,j | +|Σ k i,j • m i,j | + |1 -D((I src * k)↓ s )|.<label>(4)</label></formula><p>(I src * k)↓ s is downsampled LR image with kernel k, and I src ↓ s is downsampled image with ideal kernel, therefore to minimize this error is to encourage the downsampled image to preserve important low-frequency information of the source image. What's more, the second term of the above formula is to constrain k to sum to 1, and the third term is to penalty boundaries of k. Finally, the discriminator D(•) is to ensure the consistency of source domain .</p><p>Clean-Up To get more HR images, we try to generate noise-free images from X . Specifically, we adopt bicubic downsampling on the real image in the source domain to remove noise and make the image sharper. Let I src ∈ X be an image from real source images set, and k bic be the ideal bicubic kernel. Then the image is downsampled with a clean-up scale factor sc.</p><formula xml:id="formula_6">I HR = (I src * k bic )↓ sc .</formula><p>(</p><p>Degradation with Blur Kernels We regard the images after downsampling as clean HR images. Then we perform degradation to these HR images by randomly selecting a blur kernel from the degradation pool. The downsampling process is cross-correlation operations followed by sampling with stride s, which can be formulated as:</p><formula xml:id="formula_8">I D = (I HR * k i )↓ s , i ∈ {1, 2 • • • m}, (<label>6</label></formula><formula xml:id="formula_9">)</formula><p>where I D denotes the downsampled image, and k i refers to the selected specific blur kernel from</p><formula xml:id="formula_10">{k 1 , k 2 • • • k m }.</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Noise Injection</head><p>For noisy images, we explicitly inject noise into the downsampled images to generate realistic LR images. Since the high-frequency information is lost during the downsampling process, the degraded noise distribution changes at the same time. In order to make the degraded image have a similar noise distribution to the source image, we directly collect noise patches from the source dataset X . We observe that patches with richer content have a larger variance. Based on this observation and inspired by <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b48">49]</ref>, we design a filtering rule to collect patches with their variance in a certain range. Simply but effectively, we decouple noise and content by the following rule:</p><formula xml:id="formula_11">σ(n i ) &lt; v, (<label>7</label></formula><formula xml:id="formula_12">)</formula><p>where σ(•) denotes the function to calculate variance, and v is the max value of variance.</p><p>Degradation with Noise Injection Assume that a series of noise patches {n 1 , n 2 • • • n l } are collected and added into the degradation pool. The noise injection process is performed by randomly cropping patches from the noise pool. Similarly, we formalize this process as:</p><formula xml:id="formula_13">I LR = I D + n i , i ∈ {1, 2 • • • l}, (<label>8</label></formula><formula xml:id="formula_14">)</formula><p>where n i is a cropped noise patch from the noise pool con-</p><formula xml:id="formula_15">sisting of {k 1 , k 2 • • • k l }.</formula><p>In detail, we adopt an online noise injection method that the content and the noise are combined during training phase. This makes the noise more diverse and regularizes the SR model to distinguish content with noise. After the degradation with blur kernels and injecting noise, we obtain I LR ∈ X .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4.">Super-Resolution Model</head><p>Based on ESRGAN <ref type="bibr" target="#b38">[39]</ref>, we implement an SR model and train it on constructed paired data {I LR , I HR } ∈ {X , Y}. The generator adopts RRDB <ref type="bibr" target="#b38">[39]</ref> structure, and the resolution of the generated image will be enlarged for 4 times. Several losses are applied to training includes pixel loss, perceptual loss <ref type="bibr" target="#b17">[18]</ref>, and adversarial loss. The pixel loss L 1 uses L1 distance. Perceptual loss L per uses the inactive features of VGG-19 <ref type="bibr" target="#b32">[33]</ref>, which helps to enhance the visual effect of low-frequency features such as edges. Adversarial loss L adv is used to enhance the texture details of the generated image to make it look more realistic. The final loss function is the weighted sum of these three losses:</p><formula xml:id="formula_16">L total = λ 1 • L 1 + λ per • L per + λ adv • L adv , (<label>9</label></formula><formula xml:id="formula_17">)</formula><p>where λ 1 , λ per , and λ adv are set as 0.01, 1, and 0.005 empirically.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.5.">Patch Discriminator in RealSR</head><p>In addition, we observe that the ESRGAN <ref type="bibr" target="#b38">[39]</ref> discriminator may introduce many artifacts. Different from default ESRGAN setting, we use patch discriminator <ref type="bibr" target="#b16">[17,</ref><ref type="bibr" target="#b49">50]</ref> instead of VGG-128 <ref type="bibr" target="#b32">[33]</ref> because of two conveniences: 1) VGG-128 limits the size of the generated image to 128, making multi-scale training inconvenient. 2) VGG-128 contains a deeper network and its fixed fully connected layers make the discriminator pay more attention to global features and ignore local features. In contrast, we use a patch discriminator with fully convolution structure, which has a fixed receptive field. For example, a three-layer network corresponds to a 70×70 patch. That is, each output value by the discriminator is only related to the patch of local fixed area. The patch losses will be fed back to the generator to optimize the gradient of local details. Note that the final error is the average of all local errors to guarantee global consistency.  <ref type="table">1</ref>. Quantitative results on DF2K dataset compared with EDSR, ESRGAN, ZSSR, and K-ZSSR. Note that 'Ours' refers to the proposed RealSR. ↑ and ↓ mean higher or lower is desired.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Experiments 4.1. Datasets</head><p>DF2K The DF2K dataset merges the DIV2K <ref type="bibr" target="#b35">[36]</ref> and Flikr2K <ref type="bibr" target="#b0">[1]</ref> datasets, and contains a total of 3, 450 images. These images are artificially added with Gaussian noise to simulate sensor noise. The validation set contains 100 images with corresponding ground truth, therefore the metrics based on reference can be calculated.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>DPED</head><p>The DPED <ref type="bibr" target="#b14">[15]</ref> dataset contains 5, 614 images taken by the iPhone3 camera. The images in this dataset are unprocessed real images, which are more challenging containing noise, blur, dark light and other low-quality problems. The 100 images in validation set are cropped from original real images. Since there is no corresponding ground truth, we can only provide a visual comparison.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Evaluation Metrics</head><p>For the case of synthetic data, we calculate PSNR, SSIM and LPIPS <ref type="bibr" target="#b42">[43]</ref> of results generated by different methods. Among them, PSNR and SSIM are commonly-used evaluation metrics for image restoration. These two metrics pay more attention to the fidelity of the image rather than visual quality. In contrast, LPIPS pays more attention to whether the visual features of images are similar or not. It uses pretrained Alexnet to extract image features, and then calculates the distance between the two features. Therefore, the smaller the LPIPS is, the closer the generated image is to the ground truth.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Evaluation on Corrupted Images</head><p>First, we compare our RealSR with state-of-the-art SR methods on corrupted DF2K dataset. We evaluate performance on validation set which consists of 100 images. After generating results by these methods, we calculate PSNR, SSIM, and LPIPS according to ground truth. Due to the fact that LPIPS better reflects visual quality, we mainly focus on this metric. The comparing methods include EDSR <ref type="bibr" target="#b22">[23]</ref>, ESRGAN <ref type="bibr" target="#b38">[39]</ref>, ZSSR <ref type="bibr" target="#b31">[32]</ref>, K-ZSSR. We evaluate the EDSR and ESRGAN method using the pre-trained model released by the authors. Since ZSSR doesn't need a training process, we simply run its test code on the val- Quantitative Results on DF2K As shown in Table <ref type="table">1</ref>, our RealSR achieves the best LPIPS performance, indicating our results are much closer to the ground truth in terms of visual characteristics. Note that our method is lower in PSNR than EDSR, and this is because we use perceptual loss that pays more attention to visual quality. Generally, the PSNR and LPIPS metrics are not positively correlated, and even show the opposite relationship within a certain range.</p><p>Qualitative Results on DF2K From Figure <ref type="figure" target="#fig_3">3</ref>, we see the local details of different methods on the same image, where our RealSR produces much less noise. On one hand, compared with EDSR and ZSSR, our results are clearer with richer texture details. On the other hand, compared with ESRGAN and K-ZSSR, our results have almost no artifacts, which is benefit from the accurate degradation estimated from real noise distribution. In particular, K-ZSSR uses a more blurry kernel than bicubic, therefore the image used for training has almost no noise, which leads to many artifacts when feeding with noisy images. The SR model mistakes the noise as the content of input image during test. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.">Evaluation on Real-World Images</head><p>The most concerning problem of our proposed method is real-world super-resolution, so we evaluate our RealSR on the DPED dataset, in which the photos suffer from degradation problems such as blur, noise, etc. Just like the problems encountered in SR training of real images, no ground truth that can be referred to in the validation stage. Therefore, we only show the results of visual comparison. In order to make the details clearer, we enlarge the local area.</p><p>Qualitative Results on DPED As shown in Figure <ref type="figure" target="#fig_4">4</ref>, the EDSR, ESRGAN, and ZSSR methods do not correctly dis-tinguish the noise from the branches and the sky, leading to blurry results. In our results, the trunk and branches are clearer, and the dividing line between the object and the background is sharper. Regarding K-ZSSR, due to wrong processing of the noise, the result produces unnecessary texture details. If we zoom in, this result is unacceptable and cannot be considered as an HR image. When dealing with some solid backgrounds, the advantages of our method are more obvious. As can be seen from the third image, the noise under the eaves has been eliminated, leaving only the important low-frequency features.</p><p>Compared with existing methods, our RealSR produces few noise and artifacts, indicating that the noise estimated by noise injection is closer to the real noise. Our RealSR results are more clear with no ambiguity compared with EDSR, ESRGAN and ZSSR. This reason is that their methods are all trained on bicubic data without estimating blurry kernel from real images. In addition, we use perceptual loss that pays more attention to the visual characteristics of the image. Compared with EDSR using pixel-loss, our results have more clear details. What's more, the cost of training a new ZSSR or K-ZSSR model is much higher than inference, while our method costs only forward time during inference.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5.">NTIRE 2020 Challenge</head><p>Our RealSR is the winner of NTIRE 2020 Challenge on both tracks of Real-World Super-Resolution <ref type="bibr" target="#b25">[26]</ref>, where Track 1 is synthetic corrupted data via image processing artifacts and Track 2 is real data of smartphone images. The data provided by each track includes two domains. One is source domain dataset containing noise and blur, and the other is defined clean HR target dataset. The task is to enlarge the resolution of LR image by 4 times, and keep the clarity and sharpness of the generated SR image consistent with the given target dataset. Since there is no given pair of data for training, participants need to use these two sets of images to construct training data. We applied the proposed method and achieved the best results on both tracks as shown in Tables <ref type="table" target="#tab_2">2</ref> and<ref type="table">3</ref>. Note that the final decision is based on human study, i.e., Mean Opinion Score (MOS) for Track 1 and Mean Opinion Rank (MOR) for Track 2 <ref type="bibr" target="#b25">[26]</ref>. Our method outperforms other approaches by a large margin, and generates SR images with superior sharpness and clarity.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.6.">Ablation Study</head><p>In order to further verify the necessity of estimating kernel, injecting noise during the degradation process, and the patch discriminator during SR training, we conduct ablation experiments on the DPED dataset. We first introduce the settings of each experiment.</p><p>• Bicubic: Under this setting, we adopt bicubic kernel to downsample HR images, and then directly use these paired data to train SR model. Without kernel estimation and noise injection, this setting keeps other parameters as default, which can be understood as fine-tuning ESRGAN on the real dataset to verify its robustness.</p><p>• Noise: This setting is to add noise injection on the basis of bicubic. Because the kernel estimation method is not used, this setting can be observed to verify the validity of the kernel estimation when compared with the proposed complete method.</p><p>• Kernel: This setting only uses the kernel estimation method, but no explicit noise is added, so it can be used to observe the effect of noise injection on the result.</p><p>• VGG-128: As discussed in Section 3.5, this setting uses the default VGG-128 discriminator.</p><p>• Patch: This setting uses a lighter patch discriminator, which is compared with the previous four settings to verify our conclusion.</p><p>Next, we demonstrate three comparative analysis to verify the effectiveness of the three proposed components.  <ref type="table">3</ref>. Quantitative results for NTIRE 2020 Challenge on Real-World Image Super-Resolution: Track 2. The number in () indicates ranking of each metric. Several no-reference based image quality assessment (IQA) is used to provide computed evaluation. The NIQE <ref type="bibr" target="#b28">[29]</ref>, BRISQUE <ref type="bibr" target="#b27">[28]</ref>, and PIQE <ref type="bibr" target="#b36">[37]</ref> metric is calculated using their corresponding MATLAB implementations. NRQM <ref type="bibr" target="#b26">[27]</ref> is a learned IQA score. Moreover, PI <ref type="bibr" target="#b15">[16]</ref> and IQA-Rank indicate summary of the other computed IQA metrics. Note that the final ranking is based on Mean Opinion Rank (MOR). unreal textures, which are partially in conflict with the original image. In contrast, the patch discriminator restores important edge features, and avoids unpleasant artifacts thus generating more realistic details.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Effect of the</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Conclusion</head><p>In this paper, we propose a novel degradation framework RealSR based on kernel estimation and noise injection. By using different combinations of degradation (e.g., blur and noise), we acquire LR images that share a common domain with real images. With those domain-consistent data, we then train a real image super-resolution GAN with a patch discriminator, which can produce HR results with better perception. Experiments on both synthetic noise data and real-world images show our RealSR outperforms the state-of-the-art methods, resulting in lower noise and better visual quality. Furthermore, our RealSR is also the winner of NTIRE 2020 Challenge on both tracks of Real-World Super-Resolution, which significantly outperforms other approaches by large margins in human perception.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 .</head><label>1</label><figDesc>Figure 1. Visualization comparison among EDSR, ZSSR, and our RealSR on a real-world low-resolution image.</figDesc><graphic coords="1,479.66,245.55,59.66,59.78" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 .</head><label>2</label><figDesc>Figure 2. Framework of our proposed RealSR method. The degradation pool provides diverse blur kernels and noise distributions for constructing realistic low-resolution images. During training phase, the SR model is optimized to reconstruct high-resolution images.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Algorithm 1</head><label>1</label><figDesc>Realistic Degradation of our RealSR Input: Real images set X , HR images set Y, downsampling scale factor s Output: Realistic paired images {I LR , I HR } 1: Initialize kernel pool K = ∅ 2: Initialize noise pool N = ∅ 3: for all I src such that I src ∈ X do 4:</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 3 .</head><label>3</label><figDesc>Figure 3. Qualitative results on DF2K dataset compared with EDSR, ESRGAN, ZSSR, and K-ZSSR. GT denotes the original HR ground truth image. The red and yellow area is cropped from different results and enlarged for visual convenient.</figDesc><graphic coords="5,314.64,73.01,233.23,207.33" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 4 .</head><label>4</label><figDesc>Figure 4. Qualitative results on DPED dataset compared with EDSR, ESRGAN, ZSSR, and K-ZSSR. The red and yellow area is cropped from different results and enlarged for visual convenient.</figDesc><graphic coords="6,74.26,73.78,464.06,412.50" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 5 .</head><label>5</label><figDesc>Figure 5. Qualitative results on DPED dataset compared with 'Bicubic', 'Noise', 'Kernel', 'VGG-128' and 'Patch'. The red and yellow area is cropped from different results and enlarged for visual convenient.</figDesc><graphic coords="8,87.30,333.22,438.04,195.12" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>HR such that I HR ∈ Y do LR with k i and n j 14: end for 15: return {I LR , I HR }</figDesc><table><row><cell>9:</cell><cell>end if</cell></row><row><cell cols="2">10: end for</cell></row><row><cell cols="2">11: for all I 12: Randomly select k i ∈ K, n j ∈ N</cell></row><row><cell>13:</cell><cell>Generate I</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2 .</head><label>2</label><figDesc>Quantitative results for the NTIRE 2020 Challenge on Real-World Image Super-Resolution: Track 1. The number in () indicates ranking of each metric. Note that the Mean Opinion Score (MOS) metric is measured according to human study.</figDesc><table><row><cell>Team</cell><cell>PSNR↑</cell><cell>SSIM↑</cell><cell>LPIPS↓</cell><cell>MOS↓</cell></row><row><cell cols="4">Impressionism (ours), winner 24.67 (16) 0.683 (13) 0.232 (1)</cell><cell>2.195</cell></row><row><cell>Samsung-SLSI-MSL</cell><cell cols="2">25.59 (12) 0.727 (9)</cell><cell>0.252 (2)</cell><cell>2.425</cell></row><row><cell>BOE-IOT-AIBD</cell><cell>26.71 (4)</cell><cell>0.761 (4)</cell><cell>0.280 (4)</cell><cell>2.495</cell></row><row><cell>MSMers</cell><cell cols="3">23.20 (18) 0.651 (17) 0.272 (3)</cell><cell>2.530</cell></row><row><cell>KU-ISPL</cell><cell>26.23 (6)</cell><cell>0.747 (7)</cell><cell>0.327 (8)</cell><cell>2.695</cell></row><row><cell>InnoPeak-SR</cell><cell>26.54 (5)</cell><cell>0.746 (8)</cell><cell>0.302 (5)</cell><cell>2.740</cell></row><row><cell>ITS425</cell><cell>27.08 (2)</cell><cell>0.779 (1)</cell><cell>0.325 (6)</cell><cell>2.770</cell></row><row><cell>MLP-SR</cell><cell cols="3">24.87 (15) 0.681 (14) 0.325 (7)</cell><cell>2.905</cell></row><row><cell>Webbzhou</cell><cell>26.10 (9)</cell><cell>0.764 (3)</cell><cell>0.341 (9)</cell><cell>-</cell></row><row><cell>SR-DL</cell><cell cols="3">25.67 (11) 0.718 (10) 0.364 (10)</cell><cell>-</cell></row><row><cell>TeamAY</cell><cell>27.09 (1)</cell><cell>0.773 (2)</cell><cell>0.369 (11)</cell><cell>-</cell></row><row><cell>BIGFEATURE-CAMERA</cell><cell>26.18 (7)</cell><cell>0.750 (6)</cell><cell>0.372 (12)</cell><cell>-</cell></row><row><cell>BMIPL-UNIST-YH-1</cell><cell>26.73 (3)</cell><cell>0.752 (5)</cell><cell>0.379 (13)</cell><cell>-</cell></row><row><cell>SVNIT1-A</cell><cell cols="3">21.22 (19) 0.576 (19) 0.397 (14)</cell><cell>-</cell></row><row><cell>KU-ISPL2</cell><cell cols="3">25.27 (14) 0.680 (15) 0.460 (15)</cell><cell>-</cell></row><row><cell>SuperT</cell><cell cols="3">25.79 (10) 0.699 (12) 0.469 (16)</cell><cell>-</cell></row><row><cell>GDUT-wp</cell><cell>26.11 (8)</cell><cell cols="2">0.706 (11) 0.496 (17)</cell><cell>-</cell></row><row><cell>SVNIT1-B</cell><cell cols="3">24.21 (17) 0.617 (18) 0.562 (18)</cell><cell>-</cell></row><row><cell>SVNIT2</cell><cell cols="3">25.39 (13) 0.674 (16) 0.615 (19)</cell><cell>-</cell></row><row><cell>Bicubic</cell><cell>25.48 (-)</cell><cell>0.680 (-)</cell><cell>0.612 (-)</cell><cell>3.050</cell></row><row><cell>ESRGAN Supervised</cell><cell>24.74 (-)</cell><cell>0.695 (-)</cell><cell>0.207 (-)</cell><cell>2.300</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head></head><label></label><figDesc>Kernel Estimation It can be seen from Figure 5 that the generated results 'Patch' are more clear compared with 'Noise'. This proves that the kernel estimation is important to SR training, which helps SR models produce sharper edges.</figDesc><table><row><cell>Team</cell><cell>NIQE↓</cell><cell cols="2">BRISQUE↓ PIQE↓</cell><cell cols="2">NRQM↑ PI↓</cell><cell>IQA-Rank↓</cell><cell>MOR↓</cell></row><row><cell cols="2">Impressionism (ours), winner 5.00 (1)</cell><cell>24.4 (1)</cell><cell>17.6 (2)</cell><cell>6.50 (1)</cell><cell>4.25 (1)</cell><cell>3.958</cell><cell>1.54 (1)</cell></row><row><cell>AITA-Noah-A</cell><cell>5.63 (4)</cell><cell>33.8 (5)</cell><cell>29.7 (8)</cell><cell>4.23 (8)</cell><cell>5.70 (6)</cell><cell>7.720</cell><cell>3.04 (2)</cell></row><row><cell>ITS425</cell><cell cols="2">8.95 (18) 52.5 (18)</cell><cell cols="3">88.6 (18) 3.08 (18) 7.94 (18)</cell><cell>14.984</cell><cell>3.30 (3)</cell></row><row><cell>AITA-Noah-B</cell><cell cols="2">8.18 (17) 50.1 (12)</cell><cell cols="3">88.0 (17) 3.23 (15) 7.47 (17)</cell><cell>13.386</cell><cell>3.57 (4)</cell></row><row><cell>Webbzhou</cell><cell cols="2">7.88 (15) 51.1 (15)</cell><cell cols="3">87.8 (16) 3.27 (14) 7.30 (15)</cell><cell>12.612</cell><cell>4.44 (5)</cell></row><row><cell>Relbmag-Eht</cell><cell>5.58 (3)</cell><cell>33.1 (3)</cell><cell>12.5 (1)</cell><cell>6.22 (2)</cell><cell>4.68 (2)</cell><cell>4.060</cell><cell>-</cell></row><row><cell>MSMers</cell><cell>5.43 (2)</cell><cell>38.2 (7)</cell><cell>20.5 (3)</cell><cell>5.22 (5)</cell><cell>5.10 (3)</cell><cell>5.420</cell><cell>-</cell></row><row><cell>MLP-SR</cell><cell>6.45 (8)</cell><cell>30.6 (2)</cell><cell>29.0 (6)</cell><cell>6.12 (3)</cell><cell>5.17 (4)</cell><cell>5.926</cell><cell>-</cell></row><row><cell>SR-DL</cell><cell>6.11 (5)</cell><cell>33.5 (4)</cell><cell>29.4 (7)</cell><cell>5.24 (4)</cell><cell>5.43 (5)</cell><cell>6.272</cell><cell>-</cell></row><row><cell>InnoPeak-SR</cell><cell cols="2">7.42 (13) 39.3 (8)</cell><cell>21.5 (4)</cell><cell>5.12 (6)</cell><cell>6.15 (9)</cell><cell>7.716</cell><cell>-</cell></row><row><cell>QCAM</cell><cell>6.21 (6)</cell><cell>44.2 (9)</cell><cell>49.6 (9)</cell><cell cols="2">4.10 (10) 6.05 (8)</cell><cell>8.304</cell><cell>-</cell></row><row><cell>SuperT</cell><cell cols="2">6.94 (10) 50.2 (13)</cell><cell cols="2">75.1 (11) 4.23 (9)</cell><cell>6.35 (10)</cell><cell>9.612</cell><cell>-</cell></row><row><cell>KU-ISPL</cell><cell>6.79 (9)</cell><cell>45.1 (10)</cell><cell cols="3">61.6 (10) 3.60 (13) 6.59 (12)</cell><cell>10.152</cell><cell>-</cell></row><row><cell>BMIPL-UNIST-YH-1</cell><cell cols="2">7.03 (12) 50.2 (14)</cell><cell cols="3">81.5 (13) 3.70 (12) 6.66 (13)</cell><cell>12.218</cell><cell>-</cell></row><row><cell>BIGFEATURE-CAMERA</cell><cell cols="2">7.45 (14) 49.2 (11)</cell><cell cols="3">87.1 (14) 3.23 (16) 7.11 (14)</cell><cell>13.784</cell><cell>-</cell></row><row><cell>Bicubic</cell><cell cols="2">7.97 (16) 52.0 (17)</cell><cell cols="3">87.2 (15) 3.16 (17) 7.40 (16)</cell><cell>14.532</cell><cell>6.04 (6)</cell></row><row><cell>RRDB</cell><cell cols="2">7.01 (11) 51.3 (16)</cell><cell cols="3">76.0 (12) 4.06 (11) 6.48 (11)</cell><cell>10.042</cell><cell>6.06 (7)</cell></row><row><cell>Table</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell cols="5">Effect of the Noise Injection In this comparative experi-</cell></row><row><cell></cell><cell></cell><cell></cell><cell cols="5">ment, we set noise injection as an option to verify if noise</cell></row><row><cell></cell><cell></cell><cell></cell><cell cols="5">injection is necessary. It can be seen from Figure 5 that</cell></row><row><cell></cell><cell></cell><cell></cell><cell cols="5">without explicit noise injection, the results of 'Kernel' have</cell></row><row><cell></cell><cell></cell><cell></cell><cell cols="5">a lot of artifacts, which are very similar to the ESRGAN</cell></row><row><cell></cell><cell></cell><cell></cell><cell cols="5">results trained on clean data. The injected noise is consis-</cell></row><row><cell></cell><cell></cell><cell></cell><cell cols="5">tent with the original noise distribution, thus ensuring SR</cell></row><row><cell></cell><cell></cell><cell></cell><cell cols="4">models robust to noise during testing.</cell><cell></cell></row></table><note><p>Effect of the Patch Discriminator On real data, we use patch discriminator to replace VGG-128. Comparing 'Patch' with 'VGG-128', we show that the VGG-128 discriminator with excessively large receptive field will cause</p></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" xml:id="foot_0"><p>Authorized licensed use limited to: Carleton University. Downloaded on August 06,2020 at 01:42:48 UTC from IEEE Xplore. Restrictions apply.</p></note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Ntire 2017 challenge on single image super-resolution: Dataset and study</title>
		<author>
			<persName><forename type="first">Eirikur</forename><surname>Agustsson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Radu</forename><surname>Timofte</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition Workshops</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition Workshops</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="126" to="135" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Limits on super-resolution and how to break them</title>
		<author>
			<persName><forename type="first">Simon</forename><surname>Baker</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Takeo</forename><surname>Kanade</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="issue">9</biblScope>
			<biblScope unit="page" from="1167" to="1183" />
			<date type="published" when="2002">2002</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Blind super-resolution kernel estimation using an internal-gan</title>
		<author>
			<persName><forename type="first">Sefi</forename><surname>Bell-Kligler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Assaf</forename><surname>Shocher</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michal</forename><surname>Irani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="284" to="293" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Toward real-world single image super-resolution: A new benchmark and a new model</title>
		<author>
			<persName><forename type="first">Jianrui</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hui</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hongwei</forename><surname>Yong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zisheng</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lei</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="3086" to="3095" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Camera lens super-resolution</title>
		<author>
			<persName><forename type="first">Chang</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhiwei</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xinmei</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zheng-Jun</forename><surname>Zha</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Feng</forename><surname>Wu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="1652" to="1660" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Image blind denoising with generative adversarial network based noise modeling</title>
		<author>
			<persName><forename type="first">Jingwen</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiawei</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hongyang</forename><surname>Chao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ming</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="3155" to="3164" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Fsrnet: End-to-end learning face super-resolution with facial priors</title>
		<author>
			<persName><forename type="first">Yu</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ying</forename><surname>Tai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaoming</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chunhua</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jian</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Second-order attention network for single image super-resolution</title>
		<author>
			<persName><forename type="first">Tao</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jianrui</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yongbing</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shu-Tao</forename><surname>Xia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lei</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="11065" to="11074" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Image super-resolution using deep convolutional networks</title>
		<author>
			<persName><forename type="first">Chao</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chen</forename><forename type="middle">Change</forename><surname>Loy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaoou</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE transactions on pattern analysis and machine intelligence</title>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="volume">38</biblScope>
			<biblScope unit="page" from="295" to="307" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">Frequency separation for real-world super-resolution</title>
		<author>
			<persName><forename type="first">Shuhang</forename><surname>Manuel Fritsche</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Radu</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName><surname>Timofte</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1911.07850</idno>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Blind super-resolution with iterative kernel correction</title>
		<author>
			<persName><forename type="first">Jinjin</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hannan</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wangmeng</forename><surname>Zuo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chao</forename><surname>Dong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="1604" to="1613" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Deep back-projection networks for super-resolution</title>
		<author>
			<persName><forename type="first">Muhammad</forename><surname>Haris</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gregory</forename><surname>Shakhnarovich</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Norimichi</forename><surname>Ukita</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="1664" to="1673" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Ode-inspired network design for single image super-resolution</title>
		<author>
			<persName><forename type="first">Xiangyu</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zitao</forename><surname>Mo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Peisong</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mingyuan</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jian</forename><surname>Cheng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="1732" to="1741" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Meta-sr: a magnification-arbitrary network for super-resolution</title>
		<author>
			<persName><forename type="first">Xuecai</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Haoyuan</forename><surname>Mu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zilei</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tieniu</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="1575" to="1584" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Dslr-quality photos on mobile devices with deep convolutional networks</title>
		<author>
			<persName><forename type="first">Andrey</forename><surname>Ignatov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nikolay</forename><surname>Kobyshev</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Radu</forename><surname>Timofte</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kenneth</forename><surname>Vanhoey</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Luc</forename><surname>Van Gool</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="3277" to="3285" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Pirm challenge on perceptual image enhancement on smartphones: Report</title>
		<author>
			<persName><forename type="first">Andrey</forename><surname>Ignatov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Radu</forename><surname>Timofte</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Thang</forename><surname>Van Vu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tung</forename></persName>
		</author>
		<author>
			<persName><forename type="first">Minh</forename><surname>Luu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Trung</forename><forename type="middle">X</forename><surname>Pham</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Cao</forename><surname>Van Nguyen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yongwoo</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jae-Seok</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Munchurl</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jie</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV)</title>
		<meeting>the European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="0" to="0" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Image-to-image translation with conditional adversarial networks</title>
		<author>
			<persName><forename type="first">Phillip</forename><surname>Isola</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jun-Yan</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tinghui</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alexei</forename><forename type="middle">A</forename><surname>Efros</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="1125" to="1134" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Perceptual losses for real-time style transfer and super-resolution</title>
		<author>
			<persName><forename type="first">Justin</forename><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alexandre</forename><surname>Alahi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Li</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European conference on computer vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="694" to="711" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Accurate image super-resolution using very deep convolutional networks</title>
		<author>
			<persName><forename type="first">Jiwon</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jung</forename><surname>Kwon Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kyoung</forename><surname>Mu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lee</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="1646" to="1654" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Deep laplacian pyramid networks for fast and accurate super-resolution</title>
		<author>
			<persName><forename type="first">Wei-Sheng</forename><surname>Lai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jia-Bin</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Narendra</forename><surname>Ahuja</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ming-Hsuan</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="624" to="632" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Photorealistic single image super-resolution using a generative adversarial network</title>
		<author>
			<persName><forename type="first">Christian</forename><surname>Ledig</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lucas</forename><surname>Theis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ferenc</forename><surname>Huszár</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jose</forename><surname>Caballero</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrew</forename><surname>Cunningham</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alejandro</forename><surname>Acosta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrew</forename><surname>Aitken</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alykhan</forename><surname>Tejani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Johannes</forename><surname>Totz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zehan</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="4681" to="4690" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Feedback network for image superresolution</title>
		<author>
			<persName><forename type="first">Zhen</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jinglei</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zheng</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaomin</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gwanggil</forename><surname>Jeon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wei</forename><surname>Wu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="3867" to="3876" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Enhanced deep residual networks for single image super-resolution</title>
		<author>
			<persName><forename type="first">Bee</forename><surname>Lim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sanghyun</forename><surname>Son</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Heewon</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Seungjun</forename><surname>Nah</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kyoung</forename><surname>Mu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lee</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition workshops</title>
		<meeting>the IEEE conference on computer vision and pattern recognition workshops</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="136" to="144" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Unsupervised learning for real-world super-resolution</title>
		<author>
			<persName><forename type="first">Andreas</forename><surname>Lugmayr</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Martin</forename><surname>Danelljan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Radu</forename><surname>Timofte</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV Workshops</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Aim 2019 challenge on real-world image super-resolution: Methods and results</title>
		<author>
			<persName><forename type="first">Andreas</forename><surname>Lugmayr</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Martin</forename><surname>Danelljan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Radu</forename><surname>Timofte</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV Workshops</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title level="m" type="main">Ntire 2020 challenge on real-world image super-resolution: Methods and results</title>
		<author>
			<persName><forename type="first">Andreas</forename><surname>Lugmayr</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Martin</forename><surname>Danelljan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Radu</forename><surname>Timofte</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020">2020</date>
			<publisher>CVPR Workshops</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Learning a no-reference quality metric for single-image super-resolution</title>
		<author>
			<persName><forename type="first">Chao</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chih-Yuan</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaokang</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ming-Hsuan</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computer Vision and Image Understanding</title>
		<imprint>
			<biblScope unit="volume">158</biblScope>
			<biblScope unit="page" from="1" to="16" />
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Referenceless image spatial quality evaluation engine</title>
		<author>
			<persName><surname>Mittal</surname></persName>
		</author>
		<author>
			<persName><surname>Moorthy</surname></persName>
		</author>
		<author>
			<persName><surname>Bovik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">45th Asilomar Conference on Signals, Systems and Computers</title>
		<imprint>
			<date type="published" when="2011">2011</date>
			<biblScope unit="volume">38</biblScope>
			<biblScope unit="page" from="53" to="54" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Making a &quot;completely blind&quot; image quality analyzer</title>
		<author>
			<persName><forename type="first">Anish</forename><surname>Mittal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rajiv</forename><surname>Soundararajan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alan</forename><forename type="middle">C</forename><surname>Bovik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Signal Processing Letters</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="209" to="212" />
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Learning dual convolutional neural networks for low-level vision</title>
		<author>
			<persName><forename type="first">Jinshan</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sifei</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Deqing</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiawei</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jimmy</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zechao</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jinhui</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Huchuan</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yu-Wing</forename><surname>Tai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="3070" to="3079" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Embedded block residual network: A recursive restoration model for single-image super-resolution</title>
		<author>
			<persName><forename type="first">Yajun</forename><surname>Qiu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ruxin</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dapeng</forename><surname>Tao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jun</forename><surname>Cheng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="4180" to="4189" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">zero-shot&quot; super-resolution using deep internal learning</title>
		<author>
			<persName><forename type="first">Assaf</forename><surname>Shocher</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nadav</forename><surname>Cohen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michal</forename><surname>Irani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="3118" to="3126" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
		<title level="m" type="main">Very deep convolutional networks for large-scale image recognition</title>
		<author>
			<persName><forename type="first">Karen</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrew</forename><surname>Zisserman</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1409.1556</idno>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Image superresolution via deep recursive residual network</title>
		<author>
			<persName><forename type="first">Ying</forename><surname>Tai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jian</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaoming</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Memnet: A persistent memory network for image restoration</title>
		<author>
			<persName><forename type="first">Ying</forename><surname>Tai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jian</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaoming</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chunyan</forename><surname>Xu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE international conference on computer vision</title>
		<meeting>the IEEE international conference on computer vision</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="4539" to="4547" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Ntire 2017 challenge on single image super-resolution: Methods and results</title>
		<author>
			<persName><forename type="first">Radu</forename><surname>Timofte</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Eirikur</forename><surname>Agustsson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Luc</forename><surname>Van Gool</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ming-Hsuan</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lei</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition workshops</title>
		<meeting>the IEEE conference on computer vision and pattern recognition workshops</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="114" to="125" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Blind image quality evaluation using perception based features</title>
		<author>
			<persName><surname>Venkatanath</surname></persName>
		</author>
		<author>
			<persName><surname>Praneeth</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chandrasekhar</forename><surname>Maruthi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sumohana</forename><forename type="middle">S</forename><surname>Bh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Swarup</forename><forename type="middle">S</forename><surname>Channappayya</surname></persName>
		</author>
		<author>
			<persName><surname>Medasani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2015 Twenty First National Conference on Communications (NCC)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="1" to="6" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Recovering realistic texture in image super-resolution by deep spatial feature transform</title>
		<author>
			<persName><forename type="first">Xintao</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ke</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chao</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chen</forename><forename type="middle">Change</forename><surname>Loy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="606" to="615" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Esrgan: Enhanced super-resolution generative adversarial networks</title>
		<author>
			<persName><forename type="first">Xintao</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ke</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shixiang</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jinjin</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yihao</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chao</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yu</forename><surname>Qiao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chen</forename><forename type="middle">Change</forename><surname>Loy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV)</title>
		<meeting>the European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="0" to="0" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Towards real scene super-resolution with raw images</title>
		<author>
			<persName><forename type="first">Xiangyu</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yongrui</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wenxiu</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="1723" to="1731" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<monogr>
		<title level="m" type="main">Fan: Feature adaptation network for surveillance face recognition and normalization</title>
		<author>
			<persName><forename type="first">Xi</forename><surname>Yin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ying</forename><surname>Tai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuge</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaoming</forename><surname>Liu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1911.11680v1</idno>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Deep plug-andplay super-resolution for arbitrary blur kernels</title>
		<author>
			<persName><forename type="first">Kai</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wangmeng</forename><surname>Zuo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lei</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="1671" to="1681" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">The unreasonable effectiveness of deep features as a perceptual metric</title>
		<author>
			<persName><forename type="first">Richard</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Phillip</forename><surname>Isola</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alexei</forename><forename type="middle">A</forename><surname>Efros</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Eli</forename><surname>Shechtman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Oliver</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="586" to="595" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Ranksrgan: Generative adversarial networks with ranker for image super-resolution</title>
		<author>
			<persName><forename type="first">Wenlong</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yihao</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chao</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yu</forename><surname>Qiao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="3096" to="3105" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Zoom to learn, learn to zoom</title>
		<author>
			<persName><forename type="first">Xuaner</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qifeng</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ren</forename><surname>Ng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vladlen</forename><surname>Koltun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="3762" to="3770" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Image super-resolution using very deep residual channel attention networks</title>
		<author>
			<persName><forename type="first">Yulun</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kunpeng</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kai</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lichen</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bineng</forename><surname>Zhong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yun</forename><surname>Fu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV)</title>
		<meeting>the European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="286" to="301" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Image super-resolution by neural texture transfer</title>
		<author>
			<persName><forename type="first">Zhifei</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhaowen</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhe</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hairong</forename><surname>Qi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="7982" to="7991" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Crossnet: An end-to-end reference-based super resolution network using cross-scale warping</title>
		<author>
			<persName><forename type="first">Haitian</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mengqi</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Haoqian</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yebin</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lu</forename><surname>Fang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV)</title>
		<meeting>the European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="88" to="104" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Kernel modeling superresolution on real low-resolution images</title>
		<author>
			<persName><forename type="first">Ruofan</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sabine</forename><surname>Susstrunk</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="2433" to="2443" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Unpaired image-to-image translation using cycleconsistent adversarial networks</title>
		<author>
			<persName><forename type="first">Jun-Yan</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Taesung</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Phillip</forename><surname>Isola</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alexei</forename><forename type="middle">A</forename><surname>Efros</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE international conference on computer vision</title>
		<meeting>the IEEE international conference on computer vision</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="2223" to="2232" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
