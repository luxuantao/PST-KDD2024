<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">An Incremental Ant Colony Algorithm with Local Search for Continuous Optimization</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Tianjun</forename><surname>Liao</surname></persName>
							<email>tliao@ulb.ac.be</email>
						</author>
						<author>
							<persName><forename type="first">Marco</forename><forename type="middle">A</forename><surname>Montes De Oca</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Thomas</forename><surname>Stützle</surname></persName>
							<email>stuetzle@ulb.ac.be</email>
						</author>
						<author>
							<persName><forename type="first">Marco</forename><surname>Dorigo</surname></persName>
							<email>mdorigo@ulb.ac.be</email>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="department">CoDE</orgName>
								<orgName type="institution" key="instit1">IRIDIA</orgName>
								<orgName type="institution" key="instit2">Université Libre de Bruxelles</orgName>
								<address>
									<settlement>Brussels</settlement>
									<country key="BE">Belgium</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff1">
								<orgName type="department">CoDE</orgName>
								<orgName type="institution" key="instit1">IRIDIA</orgName>
								<orgName type="institution" key="instit2">Université Libre de Bruxelles</orgName>
								<address>
									<settlement>Brussels</settlement>
									<country key="BE">Belgium</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff2">
								<orgName type="department">Dept. of Computer Engineering</orgName>
								<orgName type="institution">Ege University</orgName>
								<address>
									<settlement>Izmir</settlement>
									<country key="TR">Turkey</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff3">
								<orgName type="department">CoDE</orgName>
								<orgName type="institution" key="instit1">IRIDIA</orgName>
								<orgName type="institution" key="instit2">Université Libre de Bruxelles</orgName>
								<address>
									<settlement>Brussels</settlement>
									<country key="BE">Belgium</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff4">
								<orgName type="department">CoDE</orgName>
								<orgName type="institution" key="instit1">IRIDIA</orgName>
								<orgName type="institution" key="instit2">Université Libre de Bruxelles</orgName>
								<address>
									<settlement>Brussels</settlement>
									<country key="BE">Belgium</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">An Incremental Ant Colony Algorithm with Local Search for Continuous Optimization</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">96779A7E15A992481AD07516654EB9A0</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.3" ident="GROBID" when="2023-07-27T07:48+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>I.2.8 [Artificial Intelligence]: Problem Solving</term>
					<term>Control Methods</term>
					<term>and Search-Heuristic methods; G.1.6 [Numerical Analysis]: Optimization Ant Colony Optimization</term>
					<term>Continuous Optimization</term>
					<term>Local Search</term>
					<term>Automatic Parameter Tuning</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>ACO R is one of the most popular ant colony optimization algorithms for tackling continuous optimization problems. In this paper, we propose IACO R -LS, which is a variant of ACO R that uses local search and that features a growing solution archive. We experiment with Powell's conjugate directions set, Powell's BOBYQA, and Lin-Yu Tseng's Mtsls1 methods as local search procedures. Automatic parameter tuning results show that IACO R -LS with Mtsls1 (IACO R -Mtsls1) is not only a significant improvement over ACO R , but that it is also competitive with the state-of-theart algorithms described in a recent special issue of the Soft Computing journal. Further experimentation with IACO R -Mtsls1 on an extended benchmark functions suite, which includes functions from both the special issue of Soft Computing and the IEEE 2005 Congress on Evolutionary Computation, demonstrates its good performance on continuous optimization problems.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">INTRODUCTION</head><p>Several algorithms based on or inspired by the ant colony optimization (ACO) metaheuristic <ref type="bibr" target="#b4">[4]</ref> have been proposed to tackle continuous optimization problems <ref type="bibr" target="#b5">[5,</ref><ref type="bibr" target="#b9">9,</ref><ref type="bibr" target="#b12">12,</ref><ref type="bibr" target="#b14">14,</ref><ref type="bibr" target="#b18">18]</ref>. One of the most popular ACO-based algorithms for continuous domains is ACO R <ref type="bibr" target="#b21">[21]</ref><ref type="bibr" target="#b22">[22]</ref><ref type="bibr" target="#b23">[23]</ref>. Recently, Leguizamón and Coello <ref type="bibr" target="#b11">[11]</ref> proposed a variant of ACO R that performs better than the original ACO R on six benchmark functions. However, the results obtained with Leguizamón and Coello's variant are far from being competitive with the results obtained by state-of-the-art continuous optimization algorithms recently featured in a special issue of the Soft Computing journal <ref type="bibr" target="#b13">[13]</ref> (Throughout the rest of the paper, we will refer to this special issue as SOCO). The set of algorithms described in SOCO consists of differential evolution algorithms, memetic algorithms, particle swarm optimization algorithms and other types of optimization algorithms <ref type="bibr" target="#b13">[13]</ref>. In SOCO, the differential evolution algorithm (DE) <ref type="bibr" target="#b24">[24]</ref>, the covariance matrix adaptation evolution strategy with increasing population size (G-CMA-ES) <ref type="bibr" target="#b1">[1]</ref>, and the realcoded CHC algorithm (CHC) <ref type="bibr" target="#b6">[6]</ref> are used as the reference algorithms. It should be noted that no ACO-based algorithms are featured in SOCO.</p><p>In this paper, we propose an improved ACO R algorithm, called IACO R -LS, that is competitive with the state of the art in continuous optimization. We first present IACO R , which is an ACO R with an extra search diversification mechanism that consists of a growing solution archive. Then, we hybridize IACO R with a local search procedure in order to enhance its search intensification abilities. We experiment with three local search procedures: Powell's conjugate directions set <ref type="bibr" target="#b19">[19]</ref>, Powell's BOBYQA <ref type="bibr" target="#b20">[20]</ref>, and Lin-Yu Tseng's Mtsls1 <ref type="bibr" target="#b27">[27]</ref>. An automatic parameter tuning procedure, Iterated F-race <ref type="bibr" target="#b2">[2,</ref><ref type="bibr" target="#b3">3]</ref>, is used for the configuration of the investigated algorithms. The best algorithm found after tuning, IACO R -Mtsls1, obtains results that are as good as the best of the 16 algorithms featured in SOCO. To assess the quality of IACO R -Mtsls1 and the best SOCO algorithms on problems not seen during their design phase, we compare their performance using an extended benchmark functions suite that includes functions from SOCO and the Special Session on Continuous Optimization of the IEEE 2005 Congress on Evolutionary Computation <ref type="bibr">(CEC 2005)</ref>. The results show that IACO R -Mtsls1 can be considered to be a state-of-theart continuous optimization algorithm.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">THE ACO R ALGORITHM</head><p>The ACO R algorithm stores a set of k solutions, called solution archive, which represents the algorithm's "pheromone model." The solution archive is used to create a probability distribution of promising solutions over the search space. Solutions are generated on a coordinate-per-coordinate basis using mixtures of weighted Gaussian functions. Initially, the solution archive is filled with randomly generated solutions. The algorithm iteratively refines the solution archive by generating m new solutions and then keeping only the best k solutions of the k + m solutions that are available. The k solutions in the archive are always sorted according to their quality (from best to worst).</p><p>The core of the solution construction procedure is the estimation of multimodal one-dimensional probability density functions (PDF). The mechanism to do that in ACO R is based on a Gaussian kernel, which is defined as a weighted sum of several Gaussian functions g i j , where j is a solution index and i is a coordinate index. The Gaussian kernel for coordinate i is:</p><formula xml:id="formula_0">G i (x) = k j=1 ωjg i j (x) = k j=1 ωj 1 σ i j √ 2π e - (x-µ i j ) 2 2σ i j 2 ,<label>(1)</label></formula><p>where j ∈ {1, ..., k}, i ∈ {1, ..., D} with D being the problem dimensionality, and ωj is a weight associated with the ranking of solution j in the archive, rank(j). The weight is calculated using a Gaussian function:</p><formula xml:id="formula_1">ωj = 1 qk √ 2π e -(rank(j)-1) 2 2q 2 k 2 , (<label>2</label></formula><formula xml:id="formula_2">)</formula><p>where q is a parameter of the algorithm.</p><p>During the solution generation process, each coordinate is treated independently. First, an archive solution is chosen with a probability proportional to its weight. Then, the algorithm samples around the selected solution component s i j using a Gaussian PDF with µ i j = s i j , and σ i j equal to</p><formula xml:id="formula_3">σ i j = ξ k r=1 |s i r -s i j | k -1 ,<label>(3)</label></formula><p>which is the average distance between the i-th variable of the solution sj and the i-th variable of the other solutions in the archive, multiplied by a parameter ξ. The solution generation process is repeated m times for each dimension i = 1, ..., D. An outline of ACO R is given in Algorithm 1.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">THE IACO R ALGORITHM</head><p>IACO R is an ACO R algorithm with a solution archive whose size increases over time. This modification is based on the incremental social learning framework <ref type="bibr" target="#b15">[15,</ref><ref type="bibr">17]</ref>. A parameter Growth controls the rate at which the archive grows. Fast growth rates encourage search diversification while slow ones encourage intensification <ref type="bibr" target="#b15">[15]</ref>. In IACO R the optimization process begins with a small archive, a parameter InitArchiveSize defines its size. A new solution is added to it every Growth iterations until a maximum archive size, Algorithm 1 Outline of ACO R Input: k, m, D, q, ξ, and termination criterion. Output: The best solution found Initialize and evaluate k solutions // Sort solutions and store them in the archive T = Sort(S1 </p><formula xml:id="formula_4">S new = Snew + rand(0, 1)(S best -Snew) ,<label>(4)</label></formula><p>where rand(0, 1) is a random number in the range [0, 1). IACO R also features a mechanism different from the one used in the original ACO R for selecting the solution that guides the generation of new solutions. The new procedure depends on a parameter p ∈ [0, 1], which controls the probability of using only the best solution in the archive as a guiding solution. With a probability 1 -p, all the solutions in the archive are used to generate new solutions. Once a guiding solution is selected, and a new one is generated (in exactly the same way as in ACO R ), they are compared. If the newly generated solution is better than the guiding solution, it replaces it in the archive. This replacement strategy is different from the one used in ACO R in which all the solutions in the archive and all the newly generated ones compete.</p><p>We include an algorithm-level diversification mechanism for fighting stagnation. The mechanism consists in restarting the algorithm and initializing the new initial archive with the best-so-far solution. The restart criterion is the number of consecutive iterations, M axStagIter, with a relative solution improvement lower than a certain threshold.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">IACO R WITH LOCAL SEARCH</head><p>The IACO R -LS algorithm is a hybridization of IACO R with a local search procedure. IACO R provides the exploration needed to locate promising solutions and the local search procedure enables a fast convergence toward good solutions. In our experiments, we considered Powell's conjugate directions set <ref type="bibr" target="#b19">[19]</ref>, Powell's BOBYQA <ref type="bibr" target="#b20">[20]</ref> and Lin-Yu Tseng's Mtsls1 <ref type="bibr" target="#b27">[27]</ref> methods as local search procedures. We used the NLopt library <ref type="bibr" target="#b10">[10]</ref> implementation of the first two methods and implemented Mtsls1 following the pseudocode found in <ref type="bibr" target="#b27">[27]</ref>.</p><p>In IACO R -LS, the local search procedure is called using the best solution in the archive as initial point. The local search methods terminate after a maximum number of iterations, MaxITER, have been reached, or when the tolerance, that is the relative change between solutions found in two consecutive iterations, is lower than a parameter FTOL.</p><p>Like <ref type="bibr" target="#b16">[16]</ref>, we use an adaptive step size for the local search procedures. This is achieved as follows: a solution in the archive, different from the best solution, is chosen at random. The maximum norm (|| • ||∞) of the vector that separates this random solution from the best solution is used as the local search step size. Hence, step sizes tend to decrease over time due to the convergence tendency of the solutions in the archive. This phenomenon in turn makes the search focus around the best-so-far solution.</p><p>For fighting stagnation at the level of the local search, we call the local search procedure from different solutions from time to time. A parameter, MaxFailures, determines the maximum number of repeated calls to the local search method from the same initial solution that does not result in a solution improvement. We maintain a failures counter for each solution in the archive. When a solution's failures counter is greater than or equal to MaxFailures, the local search procedure is not called again from this solution. Instead, the local search procedure is called from a random solution whose failures counter is less than MaxFailures.</p><p>Finally, we use a simple mechanism to enforce boundary constraints in IACO R -LS. We use the following penalty function in Powell's conjugate directions method as well as in Mtsls1:</p><formula xml:id="formula_5">P (x) = fes • D i=1 Bound(xi) ,<label>(5)</label></formula><p>where Bound(xi) is defined as</p><formula xml:id="formula_6">Bound(xi) =      0, if xmin ≤ xi ≤ xmax (xmin -xi) 2 , if xi &lt; xmin (xmax -xi) 2 , if xi &gt; xmax (6)</formula><p>where xmin and xmax are the minimum and maximum limits of the search range, respectively, and fes is the number of function evaluations that have been used so far. BOBYQA has its own mechanism for dealing with bound constraints. IACO R -LS is shown in Algorithm 2. The C++ implementation of IACO R -LS is available in http://iridia.ulb.ac. be/supp/IridiaSupp2011-008/.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">EXPERIMENTAL STUDY</head><p>Our study is carried out in two stages. First, we evaluate the performance of ACO R , IACO R -BOBYQA, IACO R -Powell and IACO R -Mtsls1 by comparing their performance with that of the 16 algorithms featured in SOCO. For this purpose, we use the same 19 benchmark functions suite (functions labeled as fsoco * ). Second, we include 21<ref type="foot" target="#foot_0">1</ref> of the benchmark functions proposed for the special session on continuous optimization organized for the IEEE 2005 Congress on Evolutionary Computation (CEC 2005) <ref type="bibr" target="#b25">[25]</ref> (functions labeled as fcec * ).</p><p>In the first stage of the study, we used the 50-and 100dimensional versions of the 19 SOCO functions. Functions   <ref type="table" target="#tab_1">1</ref>. The detailed description is available in <ref type="bibr">[8,</ref><ref type="bibr" target="#b25">25]</ref>. We applied the termination conditions used for SOCO and CEC 2005 were used, that is, the maximum number of function evaluations was 5000 × D for the SOCO functions, and 10000 × D for the CEC 2005 functions. All the investigated algorithms were run 25 times on each function. We report error values defined as f (x) -f (x * ), where x is a candidate solution and x * is the optimal solution. Error values lower than 10 -14 (this value is referred to as 0-threshold ) are approximated to 0. Our analysis is based on either the whole solution quality distribution, or on the median and average errors.</p><formula xml:id="formula_7">M N N fsoco4 Shift.Rastrigin M Y N fsoco5 Shift.Griewank M N N fsoco6 Shift.Ackley M Y N fsoco7 Shift.Schwefel 2.22 U Y N fsoco8 Shift.Schwefel 1.2 U N N fsoco9 Shift.Extended f10 U N N fsoco10Shift.Bohachevsky U N N fsoco11Shift.Schaffer U N N fsoco12fsoco9 ⊕0.25 fsoco1 M N N fsoco13fsoco9 ⊕0.25 fsoco3 M N N fsoco14fsoco9 ⊕0.25 fsoco4 M N N fsoco15fsoco10 ⊕0.25 fsoco7 M N N fsoco16fsoco9 ⊕0.5 fsoco1 M N N fsoco17fsoco9 ⊕0.75 fsoco3 M N N fsoco18fsoco9 ⊕0.75 fsoco4 M N N fsoco19fsoco10 ⊕0.75 fsoco7 M N N fcec3 Shift.Ro.Elliptic U N Y fcec4 Shift.</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Parameter Settings</head><p>We used Iterated F-race <ref type="bibr" target="#b2">[2,</ref><ref type="bibr" target="#b3">3]</ref> to automatically tune algorithm parameters. The 10-dimensional versions of the 19 SOCO functions were randomly sampled as training in-stances. A maximum of 50,000 algorithm runs were used as tuning budget for ACO R , IACO R -BOBYQA, IACO R -Powell and IACO R -Mtsls1. The number of function evaluations used in each run is equal to 50,000. The best set of parameters, for each algorithm found with this process is given in Table <ref type="table" target="#tab_3">2</ref>. The only parameter that we set manually was MaxArchiveSize, which we set to 1,000.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Experimental Results and Comparison</head><p>Figure <ref type="figure" target="#fig_0">1</ref> shows the distribution of median and average errors across the 19 SOCO benchmark functions obtained with ACO R , IACO R -BOBYQA, IACO R -Powell, IACO R -Mtsls1 and the 16 algorithms featured in SOCO. <ref type="foot" target="#foot_2">2</ref> We marked with a + symbol those cases in which there is a statistically significant difference at the 0.05 α-level with a Wilcoxon test with respect to IACO R -Mtsls1 (in favor of IACO R -Mtsls1). Also at the top of each plot, a count of the number of optima found by each algorithm (or an objective function value lower than 10 -14 ) is given.</p><p>In all cases, IACO R -Mtsls1 significantly outperforms ACO R , and is in general more effective than IACO R -BOBYQA, and IACO R -Powell. IACO R -Mtsls1 is also competitive with the best algorithms in SOCO. If we consider medians only, IACO R -Mtsls1 significantly outperforms G-CMA-ES, CHC, DE, EVoPROpt, VXQR1, EM323, and RPSO-vm in both 50 and 100 dimensions. In 100 dimensions, IACO R -Mtsls1 also significantly outperforms MA-SSW and GODE. Moreover, the median error of IACO R -Mtsls1 is below the 0-threshold 14 times out of the 19 possible of the SOCO benchmark functions suite. Only MOS-DE matches such a performance.</p><p>If one considers mean values, the performance of IACO R -Mtsls1 degrades slightly. This is an indication that IACO R -Mtsls1 still stagnates with some low probability. However, IACO R -Mtsls1 still outperforms G-CMA-ES, CHC, GODE, EVoPROpt, RPSO-vm, and EM323. Even though IACO R -Mtsls1 does not significantly outperform DE and other algorithms, its performance is very competitive. The mean error of IACO R -Mtsls1 is below the 0-threshold 13 and 11 times in problems of 50 and 100 dimensions, respectively.</p><p>We note that although G-CMA-ES has difficulties in dealing with multimodal or unimodal shifted separable functions, such as fsoco4 , fsoco6 and fsoco7, G-CMA-ES showed impressive results on function fsoco8, which is a hyperellipsoid rotated in all directions. None of the other investigated algorithms can find the optimum of this function except G-CMA-ES. This result is interesting considering that G-CMA-ES showed an impressive performance in the CEC 2005 special session on continuous optimization. This fact suggests that releasing details about the problems that will be used to compare algorithms induces an undesired "overfitting" effect. In other words, authors may use the released problems to design algorithms that perform well on them but that may perform poorly on another unknown set of problems. This motivated us to carry out the second stage of our study, which consists in carrying out a more comprehensive comparison that includes G-CMA-ES and some of the best algorithms in SOCO. For this comparison, we use 40 benchmark functions as discussed above. From SOCO, we include in our study IPSO-Powell given its good performance as shown in Figure <ref type="figure" target="#fig_0">1</ref>. To discard the possibility that   (10 -14 ). A + symbol on top of a box-plot denotes a statistically significant difference at the 0.05 α-level detected with a Wilcoxon test between the results obtained with the indicated algorithm and those obtained with IACO R -Mtsls1. The absence of a symbol means that the difference is not significant with IACO R -Mtsls1. The numbers on top of a box-plot denotes the number of optima found by the corresponding algorithm.</p><p>the local search procedure is the main responsible for the obtained results, we also use Mtsls1 with IPSO, thus generating IPSO-Mtsls1. In this second stage, IPSO-Powell and IPSO-Mtsls1 were tuned as described in Section 5.1.</p><p>Table <ref type="table">3</ref> shows the median and average errors obtained by the compared algorithm on each of the 40 benchmark functions. Two facts can be noticed from these results. First, Mtsls1 seems to be indeed responsible for most of the good performance of the algorithms that use it as a local search procedure. Regarding median results, the SOCO functions for which IPSO-Mtsls1 finds the optimum, IACO R -Mtsls1 does it as well. However, IACO R -Mtsls1 seems to be more robust given the fact that it finds more optima than IPSO-Mtsls1 if functions from the CEC 2005 special session or Table <ref type="table">3</ref>: The median and average errors of objective function values obtained with G-CMA-ES, IPSO-Powell, IPSO-Mtsls1, and IACO R -Mtsls1 on 40 functions with D = 50. The lowest values were highlighted in boldface. The values below 10 -14 are approximated to 0. The results of fcec1, fcec2, fcec6, fcec9 are not presented to avoid repeated test on the similar functions such as fsoco1, fsoco3, fsoco4, fsoco8. At the bottom of the table, we report the number of times an algorithm found the lowest error.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Median errors Mean errors Function</head><formula xml:id="formula_8">G-CMA-ESIPSO-PowellIPSO-Mtsls1IACO R -Mtsls1 Function G-CMA-ESIPSO-PowellIPSO-Mtsls1IACO R -Mtsls1</formula><p>fsoco1 0.00E+00 0.00E+00 0.00E+00 0.00E+00 fsoco1 0.00E+00 0.00E+00 0.00E+00 0.00E+00 fsoco2 2.64E-11 1.42E-14 4.12E-13 4.41E-13 fsoco2 2.75E-11 2.56E-14 4.80E-13 5.50E-13 fsoco3 0.00E+00 0.00E+00 6.38E+00 4.83E+01 fsoco3 7.97E-01 0.00E+00 7.29E+01 8.17E+01 fsoco4 1.08E+02 0.00E+00 0.00E+00 0.00E+00 fsoco4 1.05E+02 0.00E+00 1.31E+00 0.00E+00 fsoco5 0.00E+00 0.00E+00 0.00E+00 0.00E+00 fsoco5 2.96E-04 6.72E-03 5.92E-04 0.00E+00 fsoco6 2.11E+01 0.00E+00 0.00E+00 0.00E+00 fsoco6 2.09E+01 0.00E+00 0.00E+00 0.00E+00 fsoco7 7.67E-11 0.00E+00 0.00E+00 0.00E+00 fsoco7 1.01E-10 4.98E-12 0.00E+00 0.00E+00 fsoco8 0.00E+00 1.75E-09 2.80E-10 2.66E-05 fsoco8 0.00E+00 4.78E-09 4.29E-10 2.94E-05 fsoco9 1.61E+01 0.00E+00 0.00E+00 0.00E+00 fsoco9 1.66E+01 4.95E-06 0.00E+00 0.00E+00 fsoco10 6.71E+00 0.00E+00 0.00E+00 0.00E+00 fsoco10 6.81E+00 0.00E+00 0.00E+00 0.00E+00 fsoco11 2.83E+01 0.00E+00 0.00E+00 0.00E+00 fsoco11 3.01E+01 8.19E-02 7.74E-02 0.00E+00 fsoco12 1.87E+02 1.02E-12 0.00E+00 0.00E+00 fsoco12 1.88E+02 1.17E-11 7.27E-03 0.00E+00 fsoco13 1.97E+02 2.00E-10 5.39E-01 6.79E-01 fsoco13 1.97E+02 2.65E-10 2.75E+00 3.03E+00 fsoco14 1.05E+02 1.77E-12 0.00E+00 0.00E+00 fsoco14 1.09E+02 1.18E+00 5.26E-01</p><p>3.04E-01 fsoco15 8.12E-04 1.07E-11 0.00E+00 0.00E+00 fsoco15 9.79E-04 2.62E-11 0.00E+00 0.00E+00 fsoco16 4.22E+02 3.08E-12 0.00E+00 0.00E+00 fsoco16 4.27E+02 2.80E+00 2.46E+00 0.00E+00 fsoco17 6.71E+02 4.35E-08 1.47E+01 6.50E+00 fsoco17 6.89E+02 3.10E+00 7.27E+01 6.19E+01 fsoco18 1.27E+02 8.06E-12 0.00E+00 0.00E+00 fsoco18 1.31E+02 1.24E+00 1.68E+00 0.00E+00 fsoco19 4.03E+00 1.83E-12 0.00E+00 0.00E+00 fsoco19 4.76E+00 1.19E-11 0.00E+00 0.00E+00 fcec3 0.00E+00 Figure <ref type="figure" target="#fig_1">2</ref> shows correlation plots that illustrate the relative performance between IACO R -Mtsls1 and G-CMA-ES, IPSO-Powell and IPSO-Mtsls1. On the x-axis, the coordinates are the results obtained with IACO R -Mtsls1; on the yaxis, the coordinates are the results obtained with the other algorithms for each of the 40 functions. Thus, points that appear on the left part of the correlation plot correspond to functions for which IACO R -Mtsls1 has better results than the other algorithm.</p><p>Table <ref type="table" target="#tab_6">4</ref> shows a detailed comparison presented in form of (win, draw, lose) according to different properties of the 40 functions used. The two-sided p-values of Wilcoxon matched-pairs signed-ranks test of IACO R -Mtsls1 with other algorithms across 40 functions are also presented. In gen-eral, IACO R -Mtsls1 performs better more often than all the other compared algorithms. IACO R -Mtsls1 wins more often against G-CMA-ES; however, G-CMA-ES performs clearly better than IACO R -Mtsls1 on rotated functions, which can be explained by the covariance matrix adaptation mechanism <ref type="bibr" target="#b7">[7]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">CONCLUSIONS</head><p>In this paper, we have introduced IACO R -LS, an ACO R algorithm with growing solution archive hybridized with a local search procedure. Three different local search procedures, Powell's conjugate directions set, Powell's BOBYQA, and Mtsls1, were tested with IACO R -LS. Through automatic tuning across 19 functions, IACO R -Mtsls1 proved to be superior to the other two variants.</p><p>The results of a comprehensive experimental comparison with 16 algorithms featured in a recent special issue of the  Soft Computing journal show that IACO R -Mtsls1 significantly outperforms the original ACO R and that IACO R -Mtsls1 is competitive with the state of the art. We also conducted a second comparison that included 21 extra functions from the special session on continuous optimization of the IEEE 2005 Congress on Evolutionary Computation. From this additional comparison we can conclude that IACO R -Mtsls1 remains very competitive. It mainly shows slightly worse results than G-CMA-ES on functions that are rotated w.r.t. the usual coordinate system. In fact, this is maybe not surprising as G-CMA-ES is the only algorithm of the 20 compared ones that performs very well on these rotated functions. In further work we may test ACO R in the version that includes the mechanism for adjusting for rotated functions <ref type="bibr" target="#b23">[23]</ref> to check whether these potential improvements transfer to IACO R -Mtsls1. Nevertheless, the very good per- </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>The box-plots show the distribution of the median (left) and average (right) errors obtained on the 19 SOCO benchmark functions of 50 (top) and 100 (bottom) dimensions. The results obtained with the three reference algorithms in SOCO are shown on the left part of each plot. The results of 13 algorithms published in SOCO are shown in the middle part of each plot. The results obtained with ACO R , IACO R -BOBYQA, IACO R -Powell, and IACO R -Mtsls1 are shown on the right part of each plot. The line at the bottom of each plot represents the 0-threshold</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: The correlation plot between IACO R -Mtsls1 and G-CMA-ES, IPSO-Powell and IPSO-Mtsls1 over 40 functions. Each point represents a function. The points on the left part of correlation plot illustrate that on those represented functions, IACO R -Mtsls1 obtains better results than the other algorithm.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 1 :</head><label>1</label><figDesc>Benchmark functions ID Name/DescriptionUni./Multi.Sep.Ro.</figDesc><table><row><cell>fsoco1 Shift.Sphere</cell><cell>U</cell><cell>Y N</cell></row><row><cell>fsoco2 Shift.Schwefel 2.21</cell><cell>U</cell><cell>N N</cell></row><row><cell>fsoco3 Shift.Rosenbrock</cell><cell></cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 2 :</head><label>2</label><figDesc>Best parameter settings found through iterated F-Race for ACO R , IACO R -BOBYQA, IACO R -Powell and IACO R -Mtsls1. The parameter FTOL is first transformed as 10 FTOL before using it in the algorithms.</figDesc><table><row><cell></cell><cell>ACO R</cell><cell></cell><cell></cell><cell></cell><cell cols="8">q 0.04544 0.8259 ξ</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>m 10</cell><cell></cell><cell></cell><cell>k 85</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="4">IACO R -BOBYQA</cell><cell></cell><cell cols="8">p 0.6979 0.8643 ξ</cell><cell cols="9">InitArchiveSize Growth 4 1</cell><cell cols="3">FTOL -3.13</cell><cell></cell><cell cols="6">MaxITER 240</cell><cell></cell><cell cols="8">MaxFailures MaxStagIter 5 20</cell></row><row><cell cols="3">IACO R -Powell</cell><cell></cell><cell></cell><cell cols="8">p 0.3586 0.9040 ξ</cell><cell cols="9">InitArchiveSize Growth 1 7</cell><cell cols="3">FTOL -1</cell><cell></cell><cell cols="6">MaxITER 20</cell><cell></cell><cell cols="8">MaxFailures MaxStagIter 6 8</cell></row><row><cell cols="3">IACO R -Mtsls1</cell><cell></cell><cell></cell><cell cols="8">p 0.6475 0.7310 ξ</cell><cell cols="26">InitArchiveSize Growth MaxITER MaxFailures MaxStagIter 14 1 85 4 13</cell><cell></cell></row><row><cell></cell><cell cols="20">Optima 6 0 4 9 12 7 10 12 12 14 11 5 9 4 5 6 3 5 6 14</cell><cell cols="20">Optima 6 0 2 8 9 6 9 12 12 14 9 4 5 0 5 6 2 5 6 13</cell></row><row><cell></cell><cell cols="3">+ + +</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="7">+ + + + + +</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">+ +</cell><cell></cell><cell></cell><cell>+</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="7">+ + + + +</cell></row><row><cell>Median Errors of Fitness Value</cell><cell>1e-14 1e-09 1e-04 1e+01 1e+06</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>Average Errors of Fitness Value</cell><cell>1e-14 1e-09 1e-04 1e+01 1e+06</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>DE</cell><cell>CHC</cell><cell>G-CMA-ES</cell><cell>SOUPDE</cell><cell>DE-D40-Mm</cell><cell>GODE</cell><cell>GaDE</cell><cell>jDElscop</cell><cell>SaDE-MMTS</cell><cell>MOS-DE</cell><cell>MA-SSW</cell><cell>RPSO-vm</cell><cell>IPSO-Powell</cell><cell>EvoPROpt</cell><cell>EM323</cell><cell>VXQR1</cell><cell>ACOr</cell><cell>IACOr-Bobyqa</cell><cell>IACOr-Powell</cell><cell>IACOr-Mtsls1</cell><cell></cell><cell>DE</cell><cell>CHC</cell><cell>G-CMA-ES</cell><cell>SOUPDE</cell><cell>DE-D40-Mm</cell><cell>GODE</cell><cell>GaDE</cell><cell>jDElscop</cell><cell>SaDE-MMTS</cell><cell>MOS-DE</cell><cell>MA-SSW</cell><cell>RPSO-vm</cell><cell>IPSO-Powell</cell><cell>EvoPROpt</cell><cell>EM323</cell><cell>VXQR1</cell><cell>ACOr</cell><cell>IACOr-Bobyqa</cell><cell>IACOr-Powell</cell><cell>IACOr-Mtsls1</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="9">(a) 50 dimensions</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="9">(b) 50 dimensions</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell cols="20">Optima 6 0 3 9 11 6 11 12 12 14 10 5 8 3 6 6 3 5 6 14</cell><cell cols="20">Optima 6 0 2 8 9 6 9 10 12 13 8 4 5 0 4 5 2 5 6 11</cell></row><row><cell></cell><cell cols="3">+ + +</cell><cell></cell><cell></cell><cell>+</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="8">+ + + + + + +</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">+ +</cell><cell></cell><cell></cell><cell>+</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="8">+ + + + + +</cell></row><row><cell>Median Errors of Fitness Value</cell><cell>1e-14 1e-09 1e-04 1e+01 1e+06</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>Average Errors of Fitness Value</cell><cell>1e-14 1e-09 1e-04 1e+01 1e+06</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>DE</cell><cell>CHC</cell><cell>G-CMA-ES</cell><cell>SOUPDE</cell><cell>DE-D40-Mm</cell><cell>GODE</cell><cell>GaDE</cell><cell>jDElscop</cell><cell>SaDE-MMTS</cell><cell>MOS-DE</cell><cell>MA-SSW</cell><cell>RPSO-vm</cell><cell>IPSO-Powell</cell><cell>EvoPROpt</cell><cell>EM323</cell><cell>VXQR1</cell><cell>ACOr</cell><cell>IACOr-Bobyqa</cell><cell>IACOr-Powell</cell><cell>IACOr-Mtsls1</cell><cell></cell><cell>DE</cell><cell>CHC</cell><cell>G-CMA-ES</cell><cell>SOUPDE</cell><cell>DE-D40-Mm</cell><cell>GODE</cell><cell>GaDE</cell><cell>jDElscop</cell><cell>SaDE-MMTS</cell><cell>MOS-DE</cell><cell>MA-SSW</cell><cell>RPSO-vm</cell><cell>IPSO-Powell</cell><cell>EvoPROpt</cell><cell>EM323</cell><cell>VXQR1</cell><cell>ACOr</cell><cell>IACOr-Bobyqa</cell><cell>IACOr-Powell</cell><cell>IACOr-Mtsls1</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 4 :</head><label>4</label><figDesc>The comparison is conducted based on median and average errors of objective value and the results of IACO R -Mtsls1 are presented in form of (win, draw, lose), respectively. The tested 40 functions were divided into different properties for details. The two-sided p-values of Wilcoxon matchedpairs signed-rank test of IACO R -Mtsls1 at a 0.05 αlevel with other algorithms are also presented -Mtsls1 on most of the Soft Computing benchmark functions is a clear indication of the high potential ACO algorithms have for this problem domain. In fact, IACO R -Mtsls1 is clearly competitive with state-of-theart continuous optimizers.</figDesc><table><row><cell></cell><cell cols="2">Median Errors</cell><cell></cell></row><row><cell cols="4">Properties IACO R -Mtsls1IACO R -Mtsls1IACO R -Mtsls1</cell></row><row><cell>of</cell><cell>vs</cell><cell>vs</cell><cell>vs</cell></row><row><cell>Functions</cell><cell cols="3">G-CMA-ES IPSO-Powell IPSO-Mtsls1</cell></row><row><cell>Separable</cell><cell>(3, 1, 0)</cell><cell>(0, 4, 0)</cell><cell>(0, 4, 0)</cell></row><row><cell cols="2">Non-Separable (18, 2, 16)</cell><cell>(22, 7, 7)</cell><cell>(16, 13, 7)</cell></row><row><cell cols="2">Non-Separable (7, 2, 8) (Non-Hybrid)</cell><cell>(6, 6, 5)</cell><cell>(6, 6, 5)</cell></row><row><cell cols="2">Non-Separable (11, 0, 8) (Hybrid)</cell><cell>(16, 1, 2)</cell><cell>(10, 7, 2)</cell></row><row><cell>Unimodal</cell><cell>(6, 1, 3)</cell><cell>(1, 5, 4)</cell><cell>(1, 5, 4)</cell></row><row><cell>Multimodal</cell><cell>(15, 2, 13)</cell><cell>(21, 6, 3)</cell><cell>(15, 12, 3)</cell></row><row><cell>Non-rotated</cell><cell>(16, 2, 6)</cell><cell>(10, 8, 6)</cell><cell>(10, 8, 6)</cell></row><row><cell>Rotated</cell><cell>(5, 1, 10)</cell><cell>(12, 3, 1)</cell><cell>(12, 3, 1)</cell></row><row><cell>SOCO</cell><cell>(15, 2, 2 )</cell><cell>(6, 8, 5)</cell><cell>(1, 14, 4)</cell></row><row><cell>CEC 2005</cell><cell>(6, 1, 14)</cell><cell>(16, 3, 2)</cell><cell>(15, 3, 3)</cell></row><row><cell>In total</cell><cell>(21, 3, 16)</cell><cell>(22, 11, 7)</cell><cell>(16, 17, 7)</cell></row><row><cell>p-value</cell><cell>8.33E-01</cell><cell>6.03E-03</cell><cell>1.32E-02</cell></row><row><cell></cell><cell cols="2">Average Errors</cell><cell></cell></row><row><cell cols="4">Properties IACO R -Mtsls1IACO R -Mtsls1IACO R -Mtsls1</cell></row><row><cell>of</cell><cell>vs</cell><cell>vs</cell><cell>vs</cell></row><row><cell>Functions</cell><cell cols="3">G-CMA-ES IPSO-Powell IPSO-Mtsls1</cell></row><row><cell>Separable</cell><cell>(3, 1, 0)</cell><cell>(1, 3, 0)</cell><cell>(1, 3, 0)</cell></row><row><cell cols="2">Non-Separable (21, 0, 15)</cell><cell>(26, 3, 7)</cell><cell>(23, 6, 7)</cell></row><row><cell cols="2">Non-Separable (10, 0, 7) (Non-Hybrid)</cell><cell>(9, 3, 5)</cell><cell>(8, 4, 5)</cell></row><row><cell cols="2">Non-Separable (11, 0, 8) (Hybrid)</cell><cell>(17, 0, 2)</cell><cell>(15, 2, 2)</cell></row><row><cell>Unimodal</cell><cell>(6, 1, 3)</cell><cell>(4, 2, 4)</cell><cell>(2, 4, 4)</cell></row><row><cell>Multimodal</cell><cell>(18, 0, 12)</cell><cell>(23, 4, 3)</cell><cell>(22, 5, 3)</cell></row><row><cell>Non-rotated</cell><cell>(20, 1, 3)</cell><cell>(13, 5, 6)</cell><cell>(11, 7, 6)</cell></row><row><cell>Rotated</cell><cell>(4, 0, 12)</cell><cell>(14, 1, 1)</cell><cell>(13, 2, 1)</cell></row><row><cell>SOCO</cell><cell>(16, 1, 2 )</cell><cell>(10, 4, 5)</cell><cell>(8, 7, 4)</cell></row><row><cell>CEC 2005</cell><cell>(8, 0, 13)</cell><cell>(17, 2, 2)</cell><cell>(16, 2, 3)</cell></row><row><cell>In total</cell><cell>(24, 1, 15)</cell><cell>(27, 6, 7)</cell><cell>(24, 9, 7)</cell></row><row><cell>p-value</cell><cell>4.22E-01</cell><cell>1.86E-03</cell><cell>1.66E-03</cell></row><row><cell cols="2">formance of IACO R</cell><cell></cell><cell></cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0"><p>From the original</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="25" xml:id="foot_1"><p>functions, we decided to omit fcec1, fcec2, fcec6, and fcec9 because they are the same as fsoco1, fsoco3, fsoco4, fsoco8.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_2"><p>For information about these 16 algorithms please go to http://sci2s.ugr.es/eamhco/CFP.php</p></note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.">ACKNOWLEDGMENTS</head><p>This work was supported by the E-SWARM project, funded by an ERC Advanced Grant, and by the Meta-X project, funded by the Scientific Research Directorate of the French Community of Belgium. Thomas Stützle and Marco Dorigo acknowledge support from the Belgian F.R.S.-FNRS, of which they are a Research Associate and a Research Director, respectively.</p></div>
			</div>

			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"> <ref type="bibr" target="#b26">[26]</ref><p>. Functions fsoco7-fsoco11 were proposed at the</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title/>
		<author>
			<persName><surname>References</surname></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">A restart CMA evolution strategy with increasing population size</title>
		<author>
			<persName><forename type="first">A</forename><surname>Auger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Hansen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of CEC 2005</title>
		<meeting>of CEC 2005<address><addrLine>Piscataway, NJ</addrLine></address></meeting>
		<imprint>
			<publisher>IEEE Press</publisher>
			<date type="published" when="2005">2005</date>
			<biblScope unit="page" from="1769" to="1776" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Improvement strategies for the F-Race algorithm: Sampling design and iterative refinement</title>
		<author>
			<persName><forename type="first">P</forename><surname>Balaprakash</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Birattari</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Stützle</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of HM 2007</title>
		<meeting>of HM 2007<address><addrLine>Germany</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2007">2007</date>
			<biblScope unit="volume">4771</biblScope>
			<biblScope unit="page" from="108" to="122" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">F-Race and iterated F-Race: An overview. Experimental Methods for the Analysis of Optimization Algorithms</title>
		<author>
			<persName><forename type="first">M</forename><surname>Birattari</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Balaprakash</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Stützle</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2010">2010</date>
			<biblScope unit="page" from="311" to="336" />
			<pubPlace>Germany, Springer</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">Ant Colony Optimization</title>
		<author>
			<persName><forename type="first">M</forename><surname>Dorigo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Stützle</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2004">2004</date>
			<publisher>MIT Press</publisher>
			<pubPlace>Cambridge, MA</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Continuous interacting ant colony algorithm based on dense heterarchy</title>
		<author>
			<persName><forename type="first">J</forename><surname>Dréo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Siarry</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Future Generation Computer Systems</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="841" to="856" />
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Real-coded genetic algorithms and interval-schemata</title>
		<author>
			<persName><forename type="first">L</forename><surname>Eshelman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Schaffer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Foundations of Genetic Algorithms</title>
		<imprint>
			<date type="published" when="1993">1993. 1993</date>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="187" to="202" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">On the adaptation of arbitrary normal mutation distributions in evolution strategies: The generating set adaptation</title>
		<author>
			<persName><forename type="first">N</forename><surname>Hansen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Ostermeier</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Gawelczyk</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of 6th ICGA</title>
		<meeting>of 6th ICGA<address><addrLine>San Francisco, CA</addrLine></address></meeting>
		<imprint>
			<publisher>Morgan Kaufmann</publisher>
			<date type="published" when="1995">1995</date>
			<biblScope unit="page" from="57" to="64" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">Test suite for the special issue of soft computing on scalability of evolutionary algorithms and other metaheuristics for large scale continuous optimization problems</title>
		<author>
			<persName><forename type="first">F</forename><surname>Herrera</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Lozano</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Molina</surname></persName>
		</author>
		<ptr target="http://sci2s.ugr.es/eamhco/updated-functions1-19.pdf" />
		<imprint>
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Orthogonal methods based ant colony search for solving continuous optimization problems</title>
		<author>
			<persName><forename type="first">X</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Computer Science and Technology</title>
		<imprint>
			<biblScope unit="volume">23</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="2" to="18" />
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">The NLopt nonlinear-optimization package</title>
		<author>
			<persName><forename type="first">S</forename><surname>Johnson</surname></persName>
		</author>
		<ptr target="http://ab-initio.mit.edu/wiki/index.php/NLopt" />
		<imprint>
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">An alternative ACO R algorithm for continuous optimization problems</title>
		<author>
			<persName><forename type="first">G</forename><surname>Leguizamón</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Coello</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc of ANTS 2010</title>
		<editor>
			<persName><forename type="first">M</forename><surname>Dorigo</surname></persName>
		</editor>
		<meeting>of ANTS 2010<address><addrLine>Germany</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2010">2010</date>
			<biblScope unit="volume">6234</biblScope>
			<biblScope unit="page" from="48" to="59" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">A method for solving optimization problems in continuous space using ant colony algorithm</title>
		<author>
			<persName><forename type="first">C</forename><surname>Ling</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Jie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Ling</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Hongjian</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of ANTS 2002</title>
		<editor>
			<persName><forename type="first">M</forename><surname>Dorigo</surname></persName>
		</editor>
		<meeting>of ANTS 2002<address><addrLine>Germany</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2002">2002</date>
			<biblScope unit="volume">2463</biblScope>
			<biblScope unit="page" from="288" to="289" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">Editorial: Scalability of evolutionary algorithms and other metaheuristics for large-scale continuous optimization problems</title>
		<author>
			<persName><forename type="first">M</forename><surname>Lozano</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Molina</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Herrera</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2011">2011</date>
			<publisher>In Press</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">On how Pachycondyla apicalis ants suggest a new search algorithm</title>
		<author>
			<persName><forename type="first">N</forename><surname>Monmarché</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Venturini</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Slimane</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Future Generation Computer Systems</title>
		<imprint>
			<biblScope unit="volume">16</biblScope>
			<biblScope unit="issue">9</biblScope>
			<biblScope unit="page" from="937" to="946" />
			<date type="published" when="2000">2000</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Incremental social learning in particle swarms</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">A</forename><surname>Montes De Oca</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Stützle</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Van Den Enden</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Dorigo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Systems, Man, and Cybernetics -Part B: Cybernetics</title>
		<imprint>
			<biblScope unit="volume">41</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="368" to="384" />
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">An incremental particle swarm for large-scale optimization problems: An example of tuning-in-the-loop (re)design of optimization algorithms</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">A</forename><surname>Montes De Oca</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Aydın</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Stützle</surname></persName>
		</author>
		<idno type="DOI">10.1007/s00500-010-0649-0</idno>
		<imprint>
			<date type="published" when="2011">2011</date>
			<publisher>Soft Computing</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Towards incremental social learning in optimization and multiagent systems</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">A</forename><surname>Montes De Oca</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Stützle</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECoMASS Workshop of the Genetic and Evolutionary Computation Conference</title>
		<editor>
			<persName><forename type="first">W</forename><surname>Rand</surname></persName>
		</editor>
		<meeting><address><addrLine>New York</addrLine></address></meeting>
		<imprint>
			<publisher>ACM Press</publisher>
			<date type="published" when="2008">2008. 2008</date>
			<biblScope unit="page" from="1939" to="1944" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">An extension of ant colony system to continuous optimization problems</title>
		<author>
			<persName><forename type="first">S</forename><surname>Pourtakdoust</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Nobahari</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of ANTS 2004</title>
		<editor>
			<persName><forename type="first">M</forename><surname>Dorigo</surname></persName>
		</editor>
		<meeting>of ANTS 2004<address><addrLine>Germany</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2004">2004</date>
			<biblScope unit="volume">3172</biblScope>
			<biblScope unit="page" from="158" to="173" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">An efficient method for finding the minimum of a function of several variables without calculating derivatives</title>
		<author>
			<persName><forename type="first">M</forename><surname>Powell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The Computer Journal</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page">155</biblScope>
			<date type="published" when="1964">1964</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">The BOBYQA algorithm for bound constrained optimization without derivatives</title>
		<author>
			<persName><forename type="first">M</forename><surname>Powell</surname></persName>
		</author>
		<idno>NA2009/06</idno>
		<imprint>
			<date type="published" when="2009">2009</date>
			<pubPlace>University of Cambridge, UK</pubPlace>
		</imprint>
	</monogr>
	<note type="report_type">Cambridge NA Report</note>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">ACO for continuous and mixed-variable optimization</title>
		<author>
			<persName><forename type="first">K</forename><surname>Socha</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of ANTS 2004</title>
		<editor>
			<persName><forename type="first">M</forename><surname>Dorigo</surname></persName>
		</editor>
		<meeting>of ANTS 2004<address><addrLine>Germany</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2004">2004</date>
			<biblScope unit="volume">3172</biblScope>
			<biblScope unit="page" from="25" to="36" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">An ant colony optimization algorithm for continuous optimization: application to feed-forward neural network training</title>
		<author>
			<persName><forename type="first">K</forename><surname>Socha</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Blum</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural Computing &amp; Applications</title>
		<imprint>
			<biblScope unit="volume">16</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="235" to="247" />
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Ant colony optimization for continuous domains</title>
		<author>
			<persName><forename type="first">K</forename><surname>Socha</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Dorigo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">European Journal of Operational Research</title>
		<imprint>
			<biblScope unit="volume">185</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="1155" to="1173" />
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Differential evolution-a simple and efficient heuristic for global optimization over continuous spaces</title>
		<author>
			<persName><forename type="first">R</forename><surname>Storn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Price</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Global Optimization</title>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="341" to="359" />
			<date type="published" when="1997">1997</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title level="m" type="main">Problem definitions and evaluation criteria for the CEC 2005 special session on real-parameter optimization</title>
		<author>
			<persName><forename type="first">P</forename><surname>Suganthan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Hansen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Deb</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Auger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Tiwari</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2005">2005005. 2005</date>
		</imprint>
		<respStmt>
			<orgName>Nanyang Technological University</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">Technical Report</note>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Benchmark functions for the CEC 2008 special session and competition on large scale global optimization</title>
		<author>
			<persName><forename type="first">K</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Suganthan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Macnish</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Yang</surname></persName>
		</author>
		<ptr target="http://nical.ustc.edu.cn/cec08ss.php" />
	</analytic>
	<monogr>
		<title level="j">Nature Inspired Computation and Applications Laboratory</title>
		<imprint>
			<date type="published" when="2007">2007</date>
			<pubPlace>USTC, China</pubPlace>
		</imprint>
	</monogr>
	<note type="report_type">Technical report</note>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Multiple trajectory search for large scale global optimization</title>
		<author>
			<persName><forename type="first">L</forename><surname>Tseng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of CEC 2008</title>
		<meeting>of CEC 2008<address><addrLine>Piscataway, NJ</addrLine></address></meeting>
		<imprint>
			<publisher>IEEE Press</publisher>
			<date type="published" when="2008">2008</date>
			<biblScope unit="page" from="3052" to="3059" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
