<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Document Clustering using Word Clusters via the Information Bottleneck Method</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Noam</forename><surname>Slonim</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">School of Computer Science and Engineering and The Interdisciplinary Center for Neural Computation</orgName>
								<orgName type="institution">The Hebrew University</orgName>
								<address>
									<postCode>91904</postCode>
									<settlement>Jerusalem</settlement>
									<country key="IL">Israel</country>
								</address>
							</affiliation>
						</author>
						<author role="corresp">
							<persName><forename type="first">Naftali</forename><surname>Tishby</surname></persName>
							<email>tishby¡@cs.huji.ac.il</email>
							<affiliation key="aff0">
								<orgName type="department">School of Computer Science and Engineering and The Interdisciplinary Center for Neural Computation</orgName>
								<orgName type="institution">The Hebrew University</orgName>
								<address>
									<postCode>91904</postCode>
									<settlement>Jerusalem</settlement>
									<country key="IL">Israel</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Document Clustering using Word Clusters via the Information Bottleneck Method</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">59EA927CC4B95D375126FFC39918BECE</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.3" ident="GROBID" when="2023-07-28T10:44+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>We present a novel implementation of the recently introduced information bottleneck method for unsupervised document clustering. Given a joint empirical distribution of words and documents, ¢ ¤£ ¦¥ ¨ § © , we first cluster the words, , so that the obtained word clusters, , maximally preserve the information on the documents. The resulting joint distribution, ¢ ¤£ ¦ § , contains most of the o- riginal information about the documents, £ ¦ ! £ ¦ , but it is much less sparse and noisy. Using the same procedure we then cluster the documents, , so that the information about the word-clusters is preserved. Thus, we first find word-clusters that capture most of the mutual information about the set of documents, and then find document clusters, that preserve the information about the word clusters. We tested this procedure over several document collections based on subsets taken from the standard " $# $% &amp; (' 0) 21 43 65 $7 ¢ ) corpus. The results were assessed by calculating the correlation between the document clusters and the correct labels for these documents. Finding from our experiments show that this double clustering procedure, which uses the information bottleneck method, yields significantly superior performance compared to other common document distributional clustering algorithms. Moreover, the double clustering procedure improves all the distributional clustering methods examined here.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Document clustering has long been an important problem in information retrieval. Early works suggested improving the efficiency and increasing the effectiveness of document retrieval systems by first grouping the documents into clusters (cf. <ref type="bibr" target="#b26">[27]</ref> and the references therein). Recently, document clustering has been put forward as an important tool for Web search engines <ref type="bibr" target="#b14">[15]</ref>  <ref type="bibr" target="#b15">[16]</ref> [18] <ref type="bibr" target="#b29">[30]</ref>, navigating and browsing document collections <ref type="bibr" target="#b4">[5]</ref> [6] <ref type="bibr" target="#b7">[8]</ref> [9] <ref type="bibr" target="#b22">[23]</ref> and distributed retrieval <ref type="bibr" target="#b28">[29]</ref>. Two types of clustering have been studied in the context of information retrieval systems: clustering the documents on the basis of the distributions of words that co-occur in the documents, and clustering the words using the distributions of the documents in which they occur (see <ref type="bibr" target="#b27">[28]</ref> for in-depth review). In this paper we propose a new method for document clustering, which combines these two approaches under a single information theoretic framework. A recently introduced principle, termed the information bottleneck method <ref type="bibr" target="#b25">[26]</ref> is based on the following simple idea. Given the empirical joint distribution of two variables, one variable is compressed so that the mutual information about the other is preserved as much as possible. In our case these two variables correspond to the set of documents and the set of words. Thus, we may find word-clusters that capture most of the information about the document corpus, or we may extract document clusters that capture most of the information about the words that occur. In this work we combine the two alternatives. We approach this problem using a two stage algorithm. First, we extract word-clusters that capture most of the information about the documents. In the second stage we replace the original representation of the documents, the co-occurrence matrix of documents versus words, by a much more compact representation based on the co-occurrences of the word-clusters in the documents. Using this new document representation, we reapply the same clustering procedure to obtain the desired document clusters. The main advantage of this double-clustering procedure lies in a significant reduction of the inevitable noise of the original co-occurrence matrix, due to its very high dimension. The reduced matrix, based on the word-clusters, is denser and more robust, providing a better reflection of the inherent structure of the document corpus.</p><p>Our main concern is how well this method actually discovers this inherent structure. Therefore, instead of evaluating our procedure by its effectiveness for an IR system (e.g. <ref type="bibr" target="#b29">[30]</ref>), we evaluate the method on a standard labeled corpus, commonly used to evaluate supervised text classification algorithms. In this way we circumvent the bias caused by the use of a specific IR system. In addition, we view the 'correct' labels of the documents as objective knowledge on the inherent structure of the dataset. Specifically we used the " $# 8% 9&amp; @' A) 21 43 $5 67 ¢ ) dataset, collected by Lang <ref type="bibr" target="#b11">[12]</ref>, which contains about " $# § # $# 8# articles evenly distributed over " $# UseNet discussion groups. From this corpus we generated several subsets and measured clustering performance via the correlation between the obtained document clusters and the original newsgroups. We compared several clustering algorithms including the single-stage information bottleneck algorithm <ref type="bibr" target="#b23">[24]</ref>, Ward's method <ref type="bibr" target="#b0">[1]</ref> and complete-linkage <ref type="bibr" target="#b27">[28]</ref> using the standard tf-idf term weights <ref type="bibr" target="#b19">[20]</ref>. We found that double-clustering, using the information bottleneck method, was significantly superior to all the other examined algorithms. In addition, the double-clustering procedure improved performance over other algorithms in all our experiments. In other words, clustering the documents by their words was always inferior to clustering by word-clusters.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">The Information Bottleneck Method</head><p>Most clustering algorithms start either from pairwise 'distances' between points (pairwise clustering) or with a distortion measure between a data point and a class centroid (vector quantization).</p><p>Given the distance matrix or the distortion measure, the clustering task can be adapted in various ways into an optimization problem consisting of finding a small number of classes with low intraclass distortion or with high intraclass connectivity. The main problem with this approach is in the choice of the distance or distortion measures. Too often this is an arbitrary choice, sensitive to the specific representation, which may not accurately reflect the structure of the various components in the high dimensional data.</p><p>In the context of document clustering, a natural measure of similarity of two documents is the similarity between their word conditional distributions. Specifically, let be the set of docu- ments and let be the set of words, then for every document we can define</p><formula xml:id="formula_0">¢ ¤£ ¦© ¡ ¥ £¢ ¤ £ ¦© ¥ ¥ ¦ ¨ § © ¤ £ ¦© ¡ ¥ §<label>(1)</label></formula><p>where ¤ £ ¦© ¡ ¥ is the number of occurrences of the word © in the document ¥ . Roughly speaking, we would like documents with similar conditional word distributions to belong to the same cluster. This formulation of finding a cluster hierarchy of the members of one set (e.g. documents), based on the similarity of their conditional distributions w.r.t the members of another set (e.g. words), was first introduced in <ref type="bibr" target="#b16">[17]</ref> and was called "distributional clustering". The issue of selecting the 'right' distance measure between distributions remains, however, unresolved in that earlier work. Recently, Tishby, Pereira, and Bialek <ref type="bibr" target="#b25">[26]</ref> proposed a principled approach to this problem, which avoids the arbitrary choice of a distortion or a distance measures. In this new approach, given the empirical joint distribution of two random variables ¢ ¤£ ¦¥ ¨ § © , one looks for a compact representation of , which preserves as much information as possible about the relevant variable . This simple intuitive idea has a natural information theoretic formulation: find clusters of the members of the set , denoted here by , such that the mutual information £ is maximized, under a constraint on the information extracted from , £ .</p><p>The mutual information, £ ¦ , between the random vari- ables and is given by (e.g. <ref type="bibr" target="#b3">[4]</ref>)</p><formula xml:id="formula_1">£ ¦ ¢ © § © ¢ ¤£ ¦¥ ¢ ¤£ ¦© ¡ ¥ ! #" $ ¢ ¤£ ¦© ¡ ¥ ¢ ¤£ ¦© § (2)</formula><p>and is the only consistent statistical measure of the information that variable contains about variable . </p><formula xml:id="formula_2">@ ¥ I¢ V X b ah X i aqp sr ut £ wv x D E yG H ¢ ¤£ ¦© ¡ ¥ QP ¢ ¤£ ¦© ¡ ¥ SR ¢ ¤£ ¦© ¡ ¥ I¢ V X b a`¦ ¢ ¤£ ¥ @ ¥ ¢ ¤£ ¦¥ ¢ ¤£ ¦© ¡ ¥ ¢ ¤£ ¥ I¢ ¦ ¢ ¤£ ¥ A ¥ ¢ ¨£ ¦¥ §<label>(3)</label></formula><p>where £ x § ¥ is a normalization factor, and the single positive (Lagrange) parameter x determines the "softness" of the classifi- cation. Intuitively, in this procedure the information contained in about is 'squeezed' through a compact 'bottleneck' of clusters , that is forced to represent the 'relevant' part in w.r.t. to .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Relation to previous work</head><p>An important information theoretic based approach to word clustering was carried out by Brown et al <ref type="bibr" target="#b2">[3]</ref> who used n-gram models, and about the same time by Pereira, Tishby and Lee <ref type="bibr" target="#b16">[17]</ref> who introduced an early version of the bottleneck method, using verbobject tagging for word sense disambiguation. Hofmann <ref type="bibr" target="#b9">[10]</ref> has recently proposed another procedure, called probabilistic latent semantic indexing (PLSI) for automated document indexing, motivated and based upon our earlier work. Using this procedure one can represent documents (and words) in a low-dimensional 'latent semantic space'. The latent variables defined in this scheme are somewhat analogous to our variable. However, there are im- portant differences between these approaches. First, while PLSI assumes a generative hidden variable model for the data and uses maximum likelihood for estimating the latent variables, the information bottleneck method makes no assumption about the structure of the data distribution (it is not a hidden variable model) and uses a variational principle to optimize directly the relevant information in the co-occurrence data to extract the new representation.</p><p>Second, the PLSI model is based on a conditional-independence assumption, i.e. given the latent variables the words and documents are independent, which is not needed in our approach. Another important advantage of our method is that it has a complete (formal) analytic solution, enabling better understanding of the resulting classification.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">The Agglomerative Information Bottleneck Algorithm</head><p>As has been shown in <ref type="bibr" target="#b23">[24]</ref> [25], there is a simple implementation of the information bottleneck method, restricted to the case of 'hard' clusters. In this case every ¥ 8 belongs to precisely one cluster ¥ 8</p><p>. This restriction, which corresponds to the lim- it x in Eqs. (3), yields a natural distance measure between distributions which can be easily implemented in an agglomerative hierarchical clustering procedure.</p><p>Let ¥ 8 denote a specific (hard) cluster, then following <ref type="bibr" target="#b24">[25]</ref> we define, d e e e e e f e e e e e g ¢ ¤£</p><formula xml:id="formula_3">¥ @ ¥ I¢ ¡ £¢ if ¥ 8 ¥ # otherwise ¢ ¤£ ¦© ¡ ¥ I¢ V X b a`¦ © b ¢ ¤£ ¦¥ ¢ ¤£ ¦© ¡ ¥ ¢ ¤£ ¥ I¢ ¦ © b ¢ ¤£ ¦¥ ¥¤<label>(4)</label></formula><p>Using these distributions one can easily evaluate the mutual information between the set of clusters and using Eq.( <ref type="formula">2</ref>). As stated earlier, the objective of the information bottleneck method is to extract partitions of , defined by the mapping ¢ ¤£ ¥ @ ¥ , that maximize the mutual information functional. Note that the roles of and in this scenario can be switched. We may extrac- t clusters of words which capture maximum mutual information about the documents, or find clusters of documents that capture the mutual information about the words. We utilize this symmetry in the double-clustering procedure (see section 5 </p><formula xml:id="formula_4">¥ I¢ ¡ ¢ if ¥ 8 ¥ §¦ or ¥ 8 ¥ ©# otherwise ¢ ¨£ ¦© ¥ ¥ ¢ V X b VX b Q`¢ ¤£ ¦© ¡ ¥ ¦ V X b VX b s`¢ ¤£ ¦© ¡ ¥ ¨ ¢ ¨£ ¥ 2 ¢ ¢ ¨£ ¥ §¦ ! ¢ ¤£ ¥ ©@ "¤<label>(5)</label></formula><p>The decrease in the mutual information £ due to this merg- er is defined by # $ £ ¥ §¦ § ¥ ©6 %$ £ '&amp; )( 10 2 ¥3 4( 6 7v £ 65 70 8 9( @3 $ , where £ &amp; )( 10 2 "3 4( and £ 5 70 8 9( @3 are the information values before and after the merger, respectively. After a little algebra <ref type="bibr" target="#b24">[25]</ref> one can see that</p><formula xml:id="formula_5"># 6 £ ¥ ¦ § ¥ ¨ ¢ £ ¢ ¤£ ¥ ¦ ! ¢ ¤£ ¥ ¨ BA D 'C ED @H ¢ ¨£ ¦© ¥ ¥ ¦ § ¢ ¤£ ¦© ¡ ¥ ¨ SR (6)</formula><p>where the functional D 'C FD is the Jensen-Shannon (G IH ) divergence (see <ref type="bibr" target="#b12">[13]</ref>  <ref type="bibr" target="#b6">[7]</ref>) defined as</p><formula xml:id="formula_6">D C FD H ¢ §¦ § ¦¢ ©QR ¢ QP §¦ D FE 7G IH ¢ §¦ P SR ¢ R T UP ©¨D FE yG £H ¢ ©¨P VR ¢ WR § (7)</formula><p>where in our case d e e e e f e e e e g</p><formula xml:id="formula_7">W ¢ X¦ § ¢ ©ËY V$ W ¢ ¤£ ¦© ¡ ¥ X¦ § ¢ ¤£ ¦© ¡ ¥ `6 4Y W P ¦ § 4P ¨Y V$ W V X b VX b Q` § V X b VX b `Y R ¢ ¢ QP §¦ ¢ ¤£ ¦© ¡ ¥ §¦ ! UP ©¨¢ ¤£ ¦© ¡ ¥ ©$ a¤<label>(8)</label></formula><p>The G IH -divergence is non-negative and equals zero if and only if both arguments are identical. It is upper bounded (by ¢</p><p>) and symmetric though it is not a metric. Note that the "merger cost", # 6 £ ¥ ¦ § ¥ ¨ , can now be interpreted as the multiplication of the 'weight' of the merged elements, ¢ ¤£</p><formula xml:id="formula_8">¥ §¦ I ¢ ¤£ ¥ ©6 , by their 'dis- tance', D %C FD ¥H ¢ ¤£ ¦© ¡ ¥ ¦ § ¦¢ ¤£ ¦© ¡ ¥ ¨ SR .</formula><p>By introducing the information optimization criterion the resulting similarity measure directly emerges from the analysis. The</p><formula xml:id="formula_9">Input: Joint probability distribution ¢ ¨£ ¦¥ ¨ § © Output: A partition of into b clusters, c §b 8 W ¢ ¤ d¤ e¤ 2 Y Initialization: f Construct g$ f c §h § )i ¢ ¢ ¤ d¤ e¤ 2 § h qp i , calculate r ¦ ¨¢ £ ¢ ¤£ ¥ §¦ ¢ ¤£ ¥ `$ D C FD H ¢ ¤£ ¦© ¡ ¥ §¦ § ¦¢ ¤£ ¦© ¡ ¥ ©6 SR Loop: f For b ¢ v ¢ ¤ d¤ e¤ ¢ -Find the indices W h § )i ©Y for which r ¦ ¨is mini- mized -Merge W ¥ §¦ § ¥ ©sY t ¥ -Update ¢ W v W ¥ §¦ § ¥ ©sY EY vu W ¥ Y -Update r ¦ ¨costs w.r.t.</formula><p>¥ f algorithm is now very simple, where at each step we perform "the best possible merge", i.e. merge the clusters W ¥ w¦ § ¥ `FY which min- imize # 6 £ ¥ §¦ § ¥ ©6 . In figure <ref type="figure" target="#fig_0">1</ref> we provide the pseudo code of this agglomerative procedure.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>End For</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Other Clustering Methods</head><p>In the same general framework of the agglomerative clustering algorithm, we applied two other similarity criteria to construct two other algorithms for purposes of comparison. First, a common natural distance measure between probability distributions is the</p><p>x ¢ norm (or the variational distance), defined as,</p><formula xml:id="formula_10">x £ ¢ X¦ § ¢ ©$ v$ § © ¢ X¦ £ ¦© Av ¢ ©4£ ¦© Q ¤<label>(9)</label></formula><p>Unlike the G IH -divergence, the x ¢ norm is a distance measure satisfying all the metric properties, including triangle inequality. It also approximates the G !H -divergence for close distributions <ref type="bibr" target="#b12">[13]</ref>.</p><p>Our second clustering algorithm therefore used the following distributional similarity measure</p><formula xml:id="formula_11">r ¦ ¨¢ £ ¢ ¤£ ¥ §¦ ¢ ¤£ ¥ ©@ aA x £ ¢ ¨£ ¦© ¥ ¥ §¦ § ¢ ¤£ ¦© ¡ ¥ `$ a¤ (10)</formula><p>Note that multiplication by the 'weight' of the clusters to be merged is crucial. Otherwise there is a strong bias for assigning all objects into one cluster. Besides these two algorithms, which are motivated by probability theory, our third comparison algorithm is the standard Ward's method which is based on the Euclidean distance <ref type="bibr" target="#b0">[1]</ref>. The similarity measure for this algorithm is thus given by</p><formula xml:id="formula_12">r ¦ ¨¢ ¢ ¤£ ¥ §¦ ¢ ¤£ ¥ ©@ ¢ ¤£ ¥ ¦ ¢ ¤£ ¥ ¨ A § © ! £ ¢ ¤£ ¦© ¡ ¥ ¦ Av 9¢ ¨£ ¦© ¥ ¥ ¨ 1y ¤ (11)</formula><p>In addition we also implemented a complete-linkage (agglomerative) algorithm (see e.g. <ref type="bibr" target="#b27">[28]</ref>) which uses the conventional tfidf term-weights <ref type="bibr" target="#b19">[20]</ref> to represent the documents in a vector space model. In this method the least similar pair of documents, one of each cluster, determines the similarity between clusters. Specifically,</p><formula xml:id="formula_13">H ¤ ¡ £ ¥ §¦ § ¥ ©$ ¢ b h ¤ © b £¢ © b £ H ¤ ¡ £ ¦¥ ¨ § ¥ ¥¤ § (12)</formula><p>where for H ¤ ¡ £ ¦¥ ¨ § ¥ ¤ we used the cosine of the angle between the two tf-idf vectors representing the documents. We also implemented a single-linkage algorithm for which the most similar pair of documents, one from each cluster, determines the similarity between clusters. However, the results for this method were significantly inferior to the complete-linkage algorithm (due to a strong tendency to cluster all documents into one huge cluster), thus we do not report these data here.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">The Double Clustering Procedure</head><p>The three criteria described in Eqs. <ref type="bibr" target="#b5">(6,</ref><ref type="bibr" target="#b9">10,</ref><ref type="bibr" target="#b10">11)</ref> are essentially symmetric with regard to the roles of and . In other words, there are no prior requirements regarding which variable should be compressed. In this work we suggest a combination of these two options. In order to do that we introduce a two-stage clustering procedure. In the first stage we represent each word © by its con- ditional distribution over the set of documents, ¢ ¤£ ¦¥ £ © . We then use a distributional clustering algorithm to obtain word-clusters, denoted by , with §¦ . In the second stage we use these word-clusters to replace the original representation of the documents. Instead of representing a document by its conditional distribution of words, ¢ ¤£ ¦© ¡ ¥ , we represent it by its conditional distribution of word-clusters, ¢ ¤£ © ¥ , defined by</p><formula xml:id="formula_14">¢ ¤£ © ¥ I¢ ¤ £ © ¡ ¥ ¦ Cb § a© b ¤ £ © ¡ ¥ ¢ ¦ § © ¡b § ¤ £ ¦© ¡ ¥ ¦ b § a© b ¦ ¨ § © ¡b § ¤ £ ¦© ¥ ¥ ¤ (13)</formula><p>Using this compact representation, we re-apply the distributional clustering algorithm to extract the desired document clusters, .</p><p>In figure <ref type="figure" target="#fig_1">2</ref> we outline the double clustering procedure. Using the information bottleneck method in this double-clustering framework provides clear-cut information on the nature of the clusters obtained. In the first stage the algorithm extracts wordclusters which capture most of the relevant information about the given documents. More formally stated, in the first stage the algorithm finds a set of clusters such that £ ¦ ©¨ £ ¦ . In the second stage, the algorithm extracts document clusters, , that capture most of the relevant information about the word-clusters.</p><p>Therefore we obtain significant reduction in both dimensions of the original variables, without losing too much of their mutual information: £ ¨ £ ¦ ¨ £ ¦ .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">The Experimental Design</head><p>In this section we describe our experiments and present a new objective method for evaluating document clustering procedures. In addition we describe the datasets used in our experiments, which are all based on a standard IR corpus, the " $# $% &amp; (' 0) 21 43 65 $7 ¢ ) corpus.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.1">The evaluation method</head><p>In general, measuring clustering effectiveness is not a trivial issue. Standard measures such as the average distance between data points and candidate class centroids are rather abstract for our needs, and furthermore, as already mentioned, it is not clear what distance measure should be used. In most of the previous work on document clustering the performance of the clustering has been measured in terms of its effectiveness over some information retrieval system. Specifically, the clustering results are used to reorder the list of documents returned by the IR system, under the assumption that the user is able to select the clusters with the highest relevant document density <ref type="bibr" target="#b8">[9]</ref> [22] <ref type="bibr" target="#b29">[30]</ref>. There are several problems in this evaluation method. First, as noted in <ref type="bibr" target="#b29">[30]</ref>, empirical tests have shown that users fail to choose the best cluster about " 6# of the time <ref type="bibr" target="#b8">[9]</ref>. Second, generating the document collection- s by the results obtained by an IR system w.r.t. some queries is sensitive to the specific IR system and queries being used, which may result in some unclear bias over the datasets. Third, this evaluation method does not measure directly how well the inherent structure of the document corpus is revealed by the clustering procedure, but rather provide indirect estimates, through the IR system performance. To overcome these problems we propose a simple solution, which is essentially estimating document clustering performance by tools used for supervised text classification tasks. In other words, since our interest is in measuring how well the clustering process can reveal the inherent structure of a given document collection, we use a standard labeled text classification corpus to construct our datasets, while using the labels as clear objective knowledge reflecting the dataset inherent structure. In addition, we adopt the accuracy measure used by supervised learning algorithms to our needs. Specifically, we measure clustering performance by the accuracy given by the contingency table of the obtained clusters and the 'real' document categories.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.2">The datasets</head><p>We constructed ¢ # different document subsets of the " 6# 8% &amp; (' A) 21 43 $5 67 ¢ ) corpus collected by Lang <ref type="bibr" target="#b11">[12]</ref>. This corpus con- tains about " $# § # 8# 8# articles evenly distributed among " $# UseNet discussion groups, and is usually employed for evaluating supervised text classification techniques (e.g. <ref type="bibr">[2] [21]</ref>). Many of these groups have similar topics (e.g. five groups discuss different issues concerning computers). In addition, as pointed out by Schapire and Singer <ref type="bibr" target="#b20">[21]</ref> about ¤ of the documents in this corpus are present in more than one group (since people tend to post articles to multiple newsgroups). Therefore, the 'real' clusters are inherently fuzzy. For our tests we used ¢ # different randomly chosen subsets from this corpus. The details of these subsets are given in table ¢</p><p>. Our pre-processing included ignoring all file headers, lowering the upper case characters and ignoring all words that contained digits or non alpha-numeric characters. We did not use a stop-list or any stemming procedure. However, we included a standard feature selection mechanism, where for each dataset we selected the " $# $# 8# words with the highest contribution to the mutual information between the words and the documents. More formally stated, for each dataset, we sorted all words by £ ¦© B$ ¢ ¤£ ¦© ¦ © ¢ ¤£ ¦¥ A © 0 #" $ V X Y § VX a`and selected the top " $# $# 8# . </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Dataset</head><formula xml:id="formula_15">£ h ¤ ¥¤ 3 © § £ h ¤ ¥¤ 3 © y and £ h ¤ ¥¤ 3 © ¦</formula><p>, each of which consisted of # 8# documents.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">Experimental Results</head><p>For each of our ¢ # document collections we tested the following clustering algorithms: f £ 2 &amp; ( : The double-clustering procedure using the dis- tance measure derived from the information bottleneck method (Eq. 6). f x ¢ 2 &amp; ( : The double-clustering procedure using the x ¢ norm distance measure (Eq. 10). f ¤ 3 r 2 s&amp; ( : The double-clustering procedure using Ward's distance measure, i.e. the Euclid norm (Eq. 11). f £ ! ¦ #" %$ ( : Clustering the documents based on the original co-occurrence matrix of documents versus words, using the distance measure derived from the information bottleneck method (Eq. 6). f x ¢ ¦ " &amp;$ ( : Clustering the documents based on the original co-occurrence matrix of documents versus words, using the x ¢ -norm distance measure (Eq. 10). f ¤ 3 r ¦ #" %$ ( : Clustering the documents based on the origi- nal co-occurrence matrix of documents versus words, using Ward's distance measure (Eq. 11). f ' 5 b ¢ § &amp; (© &amp; 8 0 0) ¦ 0 : Clustering the documents based on the original co-occurrence matrix of documents versus words, using a complete-linkage algorithm and the tf-idf term weights. The similarity between documents was estimated by the cosine of the angle between the two tf-idf vectors representing the documents.</p><p>To avoid bias due to the number of word-clusters used by the double-clustering procedures, we tested the performance of these algorithms for various numbers of word-clusters. Specifically we tested performance using ¢ # § " 6# § 1 # § # and # word- clusters. Performance was estimated as the accuracy given by the contingency table of the obtained clusters and the real document categories. £ 2 &amp; ( algorithm over the 7 ¨ § © h y dataset using ¢ # word-clusters. The accuracy is # ¤ 5 06 . 7 ¨ § © h y dataset using ¢ # word-clusters. Note that this accura- cy was obtained in an unsupervised manner, without using any of the document labels. The number of document clusters used for evaluating the contingency table was generally set to be identical to the number of 'real' categories (except for the Binary datasets for which we used document clusters instead of " ). This is equivalent to a simplifying assumption that a user is approximately aware of the number of 'real' categories in the document collection. Choosing the appropriate number of document clusters without any prior knowledge about the data is a question of model selection which is beyond the scope of this work. We note, however, that this problem could be addressed using standard techniques such as cross-validation.</p><p>Detailed results for all three double-clustering algorithms in all ¢ # datasets are given in figure <ref type="figure" target="#fig_4">3</ref>. In table <ref type="table" target="#tab_2">1</ref> we list the re- sults for the three single-clustering procedures, as well as for the </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>¦</head><p>the accuracy measure we used in this work. For purposes of comparison, we also present the averaged results using the mutual information as the quality measure in figure <ref type="figure" target="#fig_4">3</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>7</head><p>To gain some perspective we also tested the performance of Rainbow software package <ref type="bibr" target="#b13">[14]</ref>     The best performance of all algorithms was over the three Binary datasets. For the Multi and Science datasets per- formance was usually similar. The weakest performance was obtained consistently for the Multi ¢ # datasets. In oth- er words, as expected, increasing the number of categories resulted in poorer performance, regardless of the algorithm used.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8">Discussion and Further Work</head><p>In this paper we presented a novel principled approach to the clustering of documents, which outperformed several other commonly used algorithms. The combination of two novel ingredients contributed to this work. The first is the information bottleneck method, which is a principled information theoretic approach to distributional clustering. It provides an objective measure of the quality of the obtained clusters -the extracted relevant information -as well as a well justified distributional similarity measure. This measure, which emerges directly from first principles, is the KL-divergence to the mean, or the Jensen-Shannon divergence, and is in this information theoretic sense the optimal similarity measure. The second ingredient is the double clustering procedure. This mechanism, which can be used with any distributional dataset was set to ¢ ¤£ ¥£ documents (evenly distributed). The test sets were identical to those listed in table ¦ . Averaging over § runs, the averaged performance over all data sets attained £ ©¨ ¦ accuracy. Specifically for the three © ¦ !£ datasets, Rainbow averaged performance attained £ ©¨" ¤# accuracy while the unsupervised $ &amp;% (' !) 10 32 4 65 average performance attained £ ©¨" ¥ § accuracy, which is definitely comparable. clustering algorithm, amounts to clustering in both dimensionsfirst, words based on their document distribution, and seconddocuments based on their word-clusters distribution. We demonstrated that the double-clustering procedure is useful for all the similarity measures we examined. When combined with the information bottleneck, the results were clearly better. The method is shown to provide good document classification accuracy for the " 6# 8% &amp; (' A) 21 43 $5 67 ¢ ) dataset, in a fully unsupervised manner, without using any training labels. We argue that these results demonstrate both the validity of the information bottleneck method and the power of the double-clustering procedure for this problem.</p><p>The agglomerative procedure used in this work has time complexity of 7 £ ¦ , which is not suitable for very large datasets.</p><p>Several techniques have been proposed for dealing with this issue, which can also be employed here. For example, a somewhat similar clustering procedure was recently used by Baker and M-cCallum <ref type="bibr" target="#b1">[2]</ref> for finding word clusters in supervised text categorization. To avoid high complexity they suggested using a fixed small number of clusters, where in each step the two most similar clusters are joined and another new word is added as a new cluster. Their work pointed out that distributional clustering can be useful for significant reduction of the feature dimensionality with minor decrease in classification accuracy. The present work, in contrast, shows that for unsupervised document classification, using word clusters is not only more efficient, but also leads to significant improvement in performance.</p><p>The double-clustering procedure used here is a two stage process. First we find word-clusters and then use them to obtain document clusters. A natural generalization is to try and compress both dimensions of the original co-occurrence matrix simultaneously, and we are working in this direction. In addition the agglomerative information bottleneck algorithm used here is a special case of a more general algorithm which yields even better performance on the same data. This more general approach is beyond the scope of this work and will be presented elsewhere <ref type="bibr" target="#b24">[25]</ref>.  </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: Pseudo-code of the agglomerative information bottleneck algorithm.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Input:Figure 2 :</head><label>2</label><figDesc>Figure 2: The double-clustering procedure.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>' 5 b</head><label>5</label><figDesc>¢ § &amp; (© &amp; 8 0 0) ¦ 0 algorithm. For purposes of comparison we al- so include the average results of the double-clustering procedures. Several results should be noted specifically: f Using the information bottleneck algorithm with the double clustering procedure (i.e. algorithm £ 2 &amp; ( ) resulted in superior performance compared to the other algorithms. Specifically the average performance over all datasets attained # ¤ accuracy, while the second best result was # ¤ 6 accuracy (for the ' 5 b ¢ § &amp; (© &amp; 8 0 0) ¦ 0 algorithms).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head></head><label></label><figDesc>using a naive Bayes supervised classifier. The training set for each Data/algorithm£ 2 &amp; ( £ ¦ " &amp;$ ( x ¢ 2 &amp; ( x ¢ ¦ " &amp;$ ( ¤ 3 r 2 s&amp; ( ¤ 3 r ¦ #" %$ ( ' 5 b ¢ § &amp; (© &amp; 8 0 0) ¦ 0 H ¢¡ h &amp; ¤ ¡ &amp;</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>¤ 3 r</head><label>3</label><figDesc>2 s&amp; ( . In addition, in 5 out of these # runs, double-clustering improved the performance for all the distance measures used in the clustering process. In other words, £ 2 &amp; ( @ § x ¢ 2 &amp; ( and ¤ 3 r 2 &amp; ( were almost always superior to£ ¦ #" %$ ( § x ¢ ¦ #" %$ ( and ¤ 3 r ¦ #" %$ ( re- spectively. The most significant improvement was for the information bottleneck algorithms. (£ 2 &amp; ( versus £ ! ¦ " &amp;$ ( ). fThe single-stage procedures were tested once for each dataset, i.e. ¢ # runs. Averaging over these runs, the ' 5 b ¢ § &amp; (© &amp; 8 0 0) ¦ 0 algorithm was slightly better than the £ ¦ #" %$ ( algorithm, which on its own was significantly better than the x ¢ ¦ " &amp;$ ( algorithm. The ¤ 3 r ¦ " &amp;$ ( algorithm exhibited the weak- est performance for almost all datasets.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head></head><label></label><figDesc>f</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: Results for all three double-clustering procedures over all data sets. The horizontal axis corresponds to the number of wordclusters used to cluster the documents. The top left figure presents averaged results in terms of the mutual information given in the contingency table of the document clusters and the real categories. Note the similarity with the accuracy averaged results.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 1 :</head><label>1</label><figDesc>Datasets details. For example, for each of the three £ h ¤ ¥¤ 3 © datasets we randomly chose # 8# documents, evenly distributed between the news groups talk.politics.mideast and talk.politics.misc. This resulted in three document collections,</figDesc><table><row><cell>Newsgroups included</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>y</head><label></label><figDesc>For example, in table " we present the contin- gency table and the accuracy for the £ ! 2 &amp; ( algorithm over the 2 Another possible measure for the quality of the obtained clusters is the mutual information given in the contingency table. This measure is highly correlated with</figDesc><table><row><cell>graphics motorcycles baseball space mideast ¥ 78 3 11 6 10 ¥ y 3 68 7 5 5 ¥ ¦ 4 5 59 8 9 ¥ ¨3 6 14 13 68 13 ¥ ¥4 9 10 10 13 63</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 2 :</head><label>2</label><figDesc>Contingency table for the</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 3 :</head><label>3</label><figDesc>Averaged results for all clustering procedures in all datasets. For the double-clustering algorithms the results for every dataset are averaged over the different numbers of word-clusters used in the process.</figDesc><table><row><cell>0.59 0.70 £ h ¤ ¤ 3 © y 0.68 0.60 0.60 0.57 0.49 0.41 0.34 £ h ¤ ¤ 3 © 0.71 0.61 0.62 £ h ¤ ¤ 3 © ¦ 0.75 0.70 0.66 0.65 7 ¨ § #© h 0.59 0.42 0.43 0.38 7 ¨ § #© h y 0.58 0.40 0.43 0.36 7 ¨ § #© h ¦ 0.53 0.50 0.46 0.34 7 ¨ § © h ¢ # 0.35 0.24 0.31 0.24 7 ¨ § © h ¢ # y 0.35 0.26 0.28 0.27 0.35 0.29 0.27 0.28 7 ¨ § © h ¢ # ¦ Average 0.55 0.46 0.45 0.40</cell><cell>0.33 0.59 0.56 0.60 0.34 0.29 0.34 0.20 0.21 0.20 0.37</cell><cell>0.29 0.56 0.51 0.60 0.27 0.28 0.29 0.19 0.20 0.17 0.34</cell><cell>0.47 0.67 0.61 0.52 0.51 0.34 0.63 0.27 0.33 0.34 0.47</cell></row><row><cell>f The double-clustering algorithms were tested using ¢ #  § " 6#  § 1 #  § # and # word-clusters for every dataset, i.e. # run-s ( for each dataset). In almost all runs ( ¡ out of # ) the £ 2 s&amp; ( performance was superior to the other double-clustering algorithms. Of these, the x ¢ 2 &amp; ( was usually</cell><cell></cell><cell></cell><cell></cell></row><row><cell>better than the</cell><cell></cell><cell></cell><cell></cell></row></table></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgments</head><p>Useful discussions with Yoram Singer and Fernando Pereira are greatly appreciated. This research was supported by grants from the Israeli Ministry of Science, and by the US-Israel Bi-national Science Foundation (BSF).</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">R</forename><surname>Anderberg</surname></persName>
		</author>
		<title level="m">Cluster Analysis for Applications</title>
		<imprint>
			<publisher>Academic Press</publisher>
			<date type="published" when="1973">1973</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">D</forename><surname>Baker</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">K</forename><surname>Mccallum</surname></persName>
		</author>
		<title level="m">Distributional Clustering of Words for Text Classification In ACM SIGIR 98</title>
		<imprint>
			<date type="published" when="1998">1998</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Class-based n-gram models of natual language</title>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">F</forename><surname>Brown</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><forename type="middle">J</forename><surname>Della Pietra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">V</forename><surname>Desouza</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">C</forename><surname>Lai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">L</forename><surname>Mercer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computational Linguistics</title>
		<imprint>
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="467" to="477" />
			<date type="published" when="1992">1992</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">M</forename><surname>Cover</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">A</forename><surname>Thomas</surname></persName>
		</author>
		<title level="m">Elements of Information Theory</title>
		<meeting><address><addrLine>New York</addrLine></address></meeting>
		<imprint>
			<publisher>John Wiley &amp; Sons</publisher>
			<date type="published" when="1991">1991</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Scatter/Gother: A Cluster Based Approach to Browsing Large Document Collections</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">R</forename><surname>Cutting</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">R</forename><surname>Karger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">O</forename><surname>Pedersen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">W</forename><surname>Tukey</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACM SIGIR 92</title>
		<imprint>
			<date type="published" when="1992">1992</date>
			<biblScope unit="page" from="318" to="329" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Constant Interaction-Time Scatter/Gother Browsing of Very Large Document Collections</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">R</forename><surname>Cutting</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">R</forename><surname>Karger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">O</forename><surname>Pedersen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACM SIGIR 93</title>
		<imprint>
			<date type="published" when="1993">1993</date>
			<biblScope unit="page" from="126" to="134" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Agnostic classification of Markovian sequences</title>
		<author>
			<persName><forename type="first">R</forename><surname>El-Yaniv</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Fine</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Tishby</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing (NIPS-97)</title>
		<imprint>
			<date type="published" when="1997">1997</date>
			<biblScope unit="page" from="465" to="471" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Adaptive Cluster-based Browsing Using Incrementally Expanded Queries and Its Effects</title>
		<author>
			<persName><forename type="first">K</forename><surname>Eguchi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACM SIGIR 99</title>
		<imprint>
			<date type="published" when="1999">1999</date>
			<biblScope unit="page" from="265" to="266" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Reexamining the Cluster Hypothesis: Scatter/Gather on Retrieval Results</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">A</forename><surname>Hearst</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">O</forename><surname>Pedersen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACM SIGIR 96</title>
		<imprint>
			<date type="published" when="1996">1996</date>
			<biblScope unit="page" from="76" to="84" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Probabilistic Latent Semantic Indexing</title>
		<author>
			<persName><forename type="first">T</forename><surname>Hofmann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACM SIGIR 99</title>
		<imprint>
			<date type="published" when="1999">1999</date>
			<biblScope unit="page" from="50" to="57" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Cluster-Based Text Categorization: A Comparison of Category Search Strategies</title>
		<author>
			<persName><forename type="first">M</forename><surname>Iwayama</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Tokunaga</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACM SIGIR 95</title>
		<imprint>
			<date type="published" when="1995">1995</date>
			<biblScope unit="page" from="273" to="280" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Learning to filter netnews</title>
		<author>
			<persName><forename type="first">K</forename><surname>Lang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of the 12th Int. Conf. on Machine Learning</title>
		<meeting>of the 12th Int. Conf. on Machine Learning</meeting>
		<imprint>
			<date type="published" when="1995">1995</date>
			<biblScope unit="page" from="331" to="339" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Divergence Measures Based on the Shannon Entropy</title>
		<author>
			<persName><forename type="first">J</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Information theory</title>
		<imprint>
			<biblScope unit="volume">37</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="145" to="151" />
			<date type="published" when="1991">1991</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">Bow: A toolkit for statistical language modeling, text retrieval, classification and clustering</title>
		<author>
			<persName><forename type="first">Andrew</forename><surname>Mccallum</surname></persName>
		</author>
		<author>
			<persName><surname>Kachites</surname></persName>
		</author>
		<ptr target="http://www.cs.cmu.edu/mccallum/bow" />
		<imprint>
			<date type="published" when="1996">1996</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">The WebCluster Project: Using Clustering for Mediating Access to the WWW</title>
		<author>
			<persName><forename type="first">M</forename><surname>Mechkour</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">J</forename><surname>Harper</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Muresan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACM SIGIR 98</title>
		<imprint>
			<date type="published" when="1998">1998</date>
			<biblScope unit="page" from="357" to="358" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">WebCluster, a Tool for Mediated Information Access</title>
		<author>
			<persName><forename type="first">G</forename><surname>Muresan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">J</forename><surname>Harper</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Mechkour</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACM SIGIR 99</title>
		<imprint>
			<date type="published" when="1999">1999</date>
			<biblScope unit="page">337</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Distributional clustering of English words</title>
		<author>
			<persName><forename type="first">F</forename><forename type="middle">C</forename><surname>Pereira</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Tishby</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">30th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting><address><addrLine>Columbus, Ohio</addrLine></address></meeting>
		<imprint>
			<date type="published" when="1993">1993</date>
			<biblScope unit="page" from="183" to="190" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">INteractive Internet Search through Automatic Clustering: an Empirical Study</title>
		<author>
			<persName><forename type="first">D</forename><surname>Roussinov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Tolle</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Ramsey</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACM SIGIR 99</title>
		<imprint>
			<date type="published" when="1999">1999</date>
			<biblScope unit="page" from="289" to="290" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">The SMART retrieval system</title>
		<author>
			<persName><forename type="first">G</forename><surname>Salton</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1971">1971</date>
			<publisher>Prentice-Hall</publisher>
			<pubPlace>Englewood Cliffs, NJ</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title/>
		<author>
			<persName><forename type="first">G</forename><surname>Salton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Developments in Automatic Text Retrieval. Science</title>
		<imprint>
			<biblScope unit="volume">253</biblScope>
			<biblScope unit="page" from="974" to="980" />
			<date type="published" when="1990">1990</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">E</forename><surname>Schapire</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><forename type="middle">E</forename><surname>Singer</surname></persName>
		</author>
		<title level="m">BoosTexter: A System for Multiclass Multi-label Text Categorization</title>
		<imprint>
			<date type="published" when="1998">1998</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Projections for Efficient Documents Clustering</title>
		<author>
			<persName><forename type="first">H</forename><surname>Schutze</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Silverstein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACM SIGIR 97</title>
		<imprint>
			<date type="published" when="1997">1997</date>
			<biblScope unit="page" from="74" to="81" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Almost-Constant-Time Clustering for Arbitrary Corpus Subsets</title>
		<author>
			<persName><forename type="first">C</forename><surname>Silverstein</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">O</forename><surname>Pedersen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACM SIGIR 97</title>
		<imprint>
			<date type="published" when="1997">1997</date>
			<biblScope unit="page" from="60" to="66" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Agglomerative Information Bottleneck</title>
		<author>
			<persName><forename type="first">N</forename><surname>Slonim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Tishby</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of Neural Information Processing Systems (NIPS-99)</title>
		<meeting>of Neural Information essing Systems (NIPS-99)</meeting>
		<imprint>
			<date type="published" when="1999">1999</date>
			<biblScope unit="page" from="617" to="623" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title level="m" type="main">The Hard Clustering Limit of the Information Bottleneck Method</title>
		<author>
			<persName><forename type="first">N</forename><surname>Slonim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Tishby</surname></persName>
		</author>
		<imprint/>
	</monogr>
	<note>In preperation</note>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">The Information Bottlencek Method</title>
		<author>
			<persName><forename type="first">N</forename><surname>Tishby</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><forename type="middle">C</forename><surname>Pereira</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Bialek</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of the 37-th Allerton Conference on Communication and Computation</title>
		<meeting>of the 37-th Allerton Conference on Communication and Computation</meeting>
		<imprint>
			<date type="published" when="1999">1999</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title/>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">J</forename><surname>Van Rijsbergen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Information Retrieval. London: Butterworths</title>
		<imprint>
			<date type="published" when="1979">1979</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Recent Trends in Hierarchic Document Clustering: A Crtical Review</title>
		<author>
			<persName><forename type="first">P</forename><surname>Willett</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Information Processing &amp; Management</title>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="577" to="597" />
			<date type="published" when="1988">1988</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Cluster-based Language Models for Distributed Retrieval</title>
		<author>
			<persName><forename type="first">J</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">B</forename><surname>Croft</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACM SIGIR 99</title>
		<imprint>
			<date type="published" when="1999">1999</date>
			<biblScope unit="page" from="254" to="261" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Web Document Clustering: A Feasibility Demonstration</title>
		<author>
			<persName><forename type="first">O</forename><surname>Zamir</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Etzioni</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACM SIGIR 98</title>
		<imprint>
			<date type="published" when="1998">1998</date>
			<biblScope unit="page" from="46" to="54" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
