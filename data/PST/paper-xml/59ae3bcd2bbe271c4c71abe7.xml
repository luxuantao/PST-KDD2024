<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Multi-target support vector regression via correlation regressor chains</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date type="published" when="2017-06-19">19 June 2017</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Gabriella</forename><surname>Melki</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science</orgName>
								<orgName type="institution">Virginia Commonwealth University</orgName>
								<address>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author role="corresp">
							<persName><forename type="first">Alberto</forename><surname>Cano</surname></persName>
							<email>acano@vcu.edu</email>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science</orgName>
								<orgName type="institution">Virginia Commonwealth University</orgName>
								<address>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Vojislav</forename><surname>Kecman</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science</orgName>
								<orgName type="institution">Virginia Commonwealth University</orgName>
								<address>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Sebastián</forename><surname>Ventura</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">Department of Computer Science and Numerical Analysis</orgName>
								<orgName type="institution">University of Cordoba</orgName>
								<address>
									<country key="ES">Spain</country>
								</address>
							</affiliation>
							<affiliation key="aff2">
								<orgName type="department">Department of Computer Science</orgName>
								<orgName type="institution">King Abdulaziz University</orgName>
								<address>
									<country>Saudi Arabia Kingdom</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Multi-target support vector regression via correlation regressor chains</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2017-06-19">19 June 2017</date>
						</imprint>
					</monogr>
					<idno type="MD5">B94CE37A4C467C51B521BC815E4EF80F</idno>
					<idno type="DOI">10.1016/j.ins.2017.06.017</idno>
					<note type="submission">Received 10 August 2016 Revised 18 January 2017 Accepted 18 June 2017</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.3" ident="GROBID" when="2023-07-27T08:06+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>Multi-target regression Multi-output regression Regressor chains Support vector regressor</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Multi-target regression is a challenging task that consists of creating predictive models for problems with multiple continuous target outputs. Despite the increasing attention on multi-label classification, there are fewer studies concerning multi-target (MT) regression. The current leading MT models are based on ensembles of regressor chains, where random, differently ordered chains of the target variables are created and used to build separate regression models, using the previous target predictions in the chain. The challenges of building MT models stem from trying to capture and exploit possible correlations among the target variables during training. This paper presents three multi-target support vector regression models. The first involves building independent, single-target Support Vector Regression (SVR) models for each output variable. The second builds an ensemble of random chains using the first method as a base model. The third calculates the targets' correlations and forms a maximum correlation chain, which is used to build a single chained support vector regression model, improving the models' prediction performance while reducing the computational complexity. The experimental study evaluates and compares the performance of the three approaches with seven other state-of-the-art multi-target regressors on 24 multi-target datasets. The experimental results are then analyzed using nonparametric statistical tests. The results show that the maximum correlation SVR approach improves the performance of using ensembles of random chains.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>In supervised learning, single-target (ST) models are trained to predict the value of a single, categorical or numeric, target attribute of a given example. In some cases, more than one target, or output, can be associated with a single sample input. These situations are handled by a generalization of ST learning, which involves predicting these multiple outputs concurrently, and is known as multi-target (MT) learning <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b4">5]</ref> . Specifically, MT learning includes multi-target regression (MTR), which addresses the prediction of continuous targets, multi-label classification <ref type="bibr" target="#b47">[48]</ref> which focuses on binary targets, and multi-dimensional classification which describes the prediction of discrete targets <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b37">38]</ref> .</p><p>Multi-target prediction has the capacity to generate models representing a wide variety of real-world applications, ranging from natural language processing <ref type="bibr" target="#b24">[25]</ref> to bioinformatics <ref type="bibr" target="#b33">[34]</ref> . Other application areas include ecology <ref type="bibr" target="#b0">[1]</ref> , gene function prediction <ref type="bibr" target="#b26">[27]</ref> , predicting the quality of vegetation <ref type="bibr" target="#b21">[22,</ref><ref type="bibr" target="#b27">28]</ref> , stock price index forecasting <ref type="bibr" target="#b45">[46]</ref> , and operations research <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b22">23]</ref> .</p><p>Constructing models for these types of real-world problems presents many challenges, such as missing data, (due to targets not being observed or recorded), and noisy data (due to instrument, experimental or human error), and the curse of high dimensionality. Along with these challenges, the most difficult task is identifying relationships between the input data and its corresponding output value. In the context of multi-target modeling, multiple outputs must now be trained against, which inherently adds computational complexity. The targets may or may not be correlated, and the corresponding model must accommodate for both scenarios. However, a characteristic of the MT datasets used in these applications and elsewhere, is that they are generated by a single system, most likely indicating that the nature of the outputs captured has some structure <ref type="bibr" target="#b22">[23]</ref> . Even though modeling the multi-variate nature and complex relationships between the target variables is challenging <ref type="bibr" target="#b4">[5]</ref> , they are more accurately represented by an MT model.</p><p>Several base-line methods have been proposed for solving multi-target tasks such as Multi-Objective Random Forests <ref type="bibr" target="#b27">[28]</ref> , Boosted Neural Networks <ref type="bibr" target="#b21">[22]</ref> , Ensembles of Trees <ref type="bibr" target="#b29">[30]</ref> , and many others. Support Vector Machines are a popular set of linear and non-linear supervised machine learning algorithms with a strong theoretical basis on Vapnik-Chervonenkis theory <ref type="bibr" target="#b12">[13]</ref> . It has previously been shown that they outperform most algorithms in terms of performance, scalability, and the ability to efficiently deal with outliers <ref type="bibr" target="#b15">[16,</ref><ref type="bibr" target="#b25">26]</ref> . Input space dimensionality does not have an adverse effect on the model training time, and furthermore, the final model produced is sparse, allowing for quick predictions.</p><p>There are two main approaches for using such base-line methods in the context of MT learning. The first being problem transformation methods, or local methods, in which the multi-target problem is transformed into multiple single-target problems, each solved separately using classical methods, as described above. The second being algorithm adaptation methods, or global , or big-bang methods, that adapt existing single-target methods to predict all the target variables simultaneously <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b26">27]</ref> . Using problem transformation algorithms for a domain of t target variables, t predictive models must be constructed, each predicting a single-target variable <ref type="bibr" target="#b26">[27]</ref> . Prediction for an unseen sample would be obtained by running each of the t single-target models and concatenating their results. Conversely, when using algorithm adaptation algorithms for the same domain of t target variables, only one model would need to be constructed which would output all t predictions.</p><p>Literature shows that algorithm adaptation methods perform better than problem transformation methods <ref type="bibr" target="#b26">[27,</ref><ref type="bibr" target="#b38">39]</ref> . The most valuable advantage of using multi-target techniques is that, not only are the relationships between the sample variables and the targets exploited, but the relationships between the targets amongst themselves are as well <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b8">9]</ref> . Single-target techniques, on the other hand, eliminate any possibility of learning from the possible relationships between the target variables because a single, independent model is trained for each target separately <ref type="bibr" target="#b3">[4]</ref> . Another advantage of MT techniques is model interpretability <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b45">46]</ref> . A single multi-target model is highly more interpretable than a series of single-target models because it not only exploits the relationship between the data and targets, but also the targets amongst themselves. Not only is a single MT model more interpretable, but it could also be considerably more computationally efficient to train, rather than training multiple single-target models individually <ref type="bibr" target="#b1">[2]</ref> .</p><p>This paper proposes three novel approaches to solving multi-target regression problems. The objective of this research topic is to investigate the performance changes when building a regression model using two distinct algorithm adaptation chaining methods versus building independent single-target models for each target variable using a novel framework. The main contributions presented in this paper include:</p><p>• Evaluating the performance of a Support Vector Regressor (SVR) as a multi-target to single-target problem transformation method to determine whether it outperforms current state-of-the-art ST algorithms. We analyze its performance as a base-line model for MT chaining methods due to the fact that ST methods do not account for any correlation among the target variables.</p><p>• Building an MT ensemble of randomly chained SVR models (SVRRC), an algorithm adaptation approach, inspired by the state-of-the-art chaining classification method, Ensemble of Random Chains Corrected (ERCC) <ref type="bibr" target="#b45">[46]</ref> , to investigate the effects and advantages of exploiting correlations among target variables during model training, in the context of regression problems. The main issues to be investigated with this approach are the randomness of the created chains because they might not capture of correlations between the targets, as well as the time taken to build all the regressors in the ensemble.</p><p>• Proposing an MT algorithm adaptation model of SVRs that builds a unique chain, capturing the maximum correlation among target outputs, named SVR Correlation Chains (SVRCC). The advantages of using this maximum correlation chain approach include exploiting the correlations among the targets which leads to an improvement in model prediction performance, and a reduction in computational complexity because a single SVR-chain model is trained, rather than building an ensemble of 10 base regressors.</p><p>The experimental study evaluates and compares the performance of the three approaches, together with 7 other stateof-the-art multi-target regressors, on a set of 24 datasets with varied input size, dimensionality, and output targets. The results of the experiments are analyzed using non-parametric statistical analysis, namely the Bonferroni-Dunn, Holm, and Wilcoxon tests <ref type="bibr" target="#b18">[19]</ref> . These post-hoc tests involve multiple comparisons among the algorithms, where they show significant differences in model performances across all datasets. The statistical analysis of the experiments presented in this paper shows the increase in performance of the support vector regressors, specifically, the maximum correlation chain, SVRCC.  (1)   1 , y (1)   1 ) , . . . , (x (N )   d , y (N )   m ) }</p><formula xml:id="formula_0">X = { X 1 , . . . , X i , . . . , X d } ∈ R N ×d , 1 ≤ i ≤ d Input instance x (l) = (x (l) 1 , . . . , x (l) d ) ∈ X, 1 ≤ l ≤ N Number of dataset targets/Outputs m Target space Y = { Y 1 , . . . , Y j , . . . , Y m } ∈ R N ×m , 1 ≤ j ≤ m Predicted target space ˆ Y = { ˆ Y 1 , . . . , ˆ Y j , . . . , ˆ Y m } ∈ R N ×m , 1 ≤ j ≤ m Target instance y (l) = (y (l) 1 , . . . , y (l) m ) ∈ Y , 1 ≤ l ≤ N Full multi-target (MT) training dataset D = { (x</formula><p>Single-target (ST) dataset with j th target D j = { (x (1)   1 , y (1)   j ) , . . . , (x (N )</p><formula xml:id="formula_1">d , y (N ) j ) } ∈ D, 1 ≤ j ≤ m</formula><p>Number of cross-validation (CV) sets k ST test dataset with j th target, i th CV fold</p><formula xml:id="formula_2">D (i ) j = { (x (i ) 1 , y (i ) j ) , . . . , (x (i ) d , y (i ) j ) ∈ D j , i ∈ { 1 , . . . , N }</formula><p>ST training dataset with j th target, excluding the i th CV fold</p><formula xml:id="formula_3">D (k -i ) j = D j \ D (i ) j MT regression model h : X × Y ST regression model h j : X × Y j , 1 ≤ j ≤ m Unknown sample x (N ) = { x (N +1) , . . . , x ( N ) }</formula><p>Predicted values for unknown sample</p><formula xml:id="formula_4">y (N ) = { y (N +1) , . . . , y (N ) } Table 2</formula><p>Transformation to m single-target datasets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Dataset</head><p>Values Output D 1</p><p>{ (x (1)   1 , y (1)   1 ) , . . . , (x (N )   d , y (N ) (1)   1 , y (1)   2 ) , . . . , (x (N )    (1)   1 , y (1)   j ) , . . . , (x (N )    (1)   1 , y (1)   m ) , . . . , (x (N )   </p><formula xml:id="formula_5">1 ) } h 1 : D 1 → R D 2 { (x</formula><formula xml:id="formula_6">d , y (N ) 2 ) } h 2 : D 2 → R D j { (x</formula><formula xml:id="formula_7">d , y (N ) j ) } h j : D j → R D m { (x</formula><formula xml:id="formula_8">d , y (N ) m ) } h m : D m → R</formula><p>The paper is structured as follows. Section 2 reviews related works on multi-target regression. Section 3 presents the three multi-target support vector regression approaches. Section 4 presents the experimental environment and study. Section 5 shows the results and statistical analysis. Finally, Section 6 shows the main conclusions of this work.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Background</head><p>This section defines first the notation that will be used in the paper and formally describes the multi-target regression problem along with the relevant state-of-the-art algorithms.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.">Notation</head><p>Let D be a training dataset of N instances and their continuous outputs. Let X ∈ D be a matrix consisting of d input variables and N samples, sometimes called input space, having a domain of X ∈ R N ×d . Let Y ∈ D be a matrix consisting of m target variables for the N input samples, sometimes called output space, having a domain of Y ∈ R N ×m . For each sample (x (l) , y (l) </p><formula xml:id="formula_9">) ∈ D, x (l) = (x (l) 1 , . . . , x (l)</formula><p>d ) and y (l) = (y (l)   1 , . . . , y (l)   m ) are the input and output vectors respectively, where l ∈ { 1 , . . . , N } . Using the training dataset D = { (x (1) , y (1) ) , . . . , (x (N ) , y (N ) ) } , the goal is to learn a multi-target regression model h : X × Y , that assigns a vector y with m target values, for each input instance x . The model will then be used to predict the values of { y (N +1) , . . . , y (N ) } for new unlabeled input vectors { x (N +1) , . . . , x (N ) } . Table <ref type="table" target="#tab_0">1</ref> summarizes the notation used in this paper.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.">Multi-target regression methods</head><p>In the context of problem transformation for multi-target models, m single-target models will be trained on the dataset D j = { (x (1)   1 , y (1)   j ) , . . . , (x (N)   d , y (N ) j ) } , where j ∈ { 1 , . . . , m } . This way there are m independent, single-target models, one model for each target variable. This is described as the baseline Single-Target (ST) model in <ref type="bibr" target="#b38">[39]</ref> . A generalized visualization of the problem transformation method is shown in Table <ref type="table">2</ref> . Many problem transformation methods have been proposed to solve multi-target problems, which are detailed next.</p><p>Many authors have proposed using Support Vector Machines (SVM) for multi-target learning <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b45">46,</ref><ref type="bibr" target="#b46">47]</ref> . SVMs have some similarities to various classification and regression problems, but research has shown that in terms of scalability, robustness against outliers, and their efficiency in learning form large and sparse datasets, they are a very useful machine learning tool that can be used in diverse real world applications <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b25">26,</ref><ref type="bibr" target="#b34">35,</ref><ref type="bibr" target="#b50">51]</ref> .</p><p>Zhang et al. <ref type="bibr" target="#b48">[49]</ref> presented a multi-output support vector regression approach based on problem transformation. It builds a multi-output model that considers the correlations between all the targets using the vector virtualization method. Basically, it extends the original feature space and expresses the multi-output problem as an equivalent single-output problem, so that it can then be solved using the single-output least squares support vector regression machines (LS-SVR) algorithm. Xiong et. al. presented a support vector regression method with a firefly heuristic in <ref type="bibr" target="#b45">[46]</ref> , in the context of interval forecasting of a stock price index. Originally proposed in <ref type="bibr" target="#b35">[36]</ref> , the algorithm was then modified with a firefly heuristic, named FA-MSVR, to intelligently identify the appropriate SVR hyper-parameters. To produce attractive results using support vector machines, appropriate hyper-parameters must be selected. This is a crucial and computationally expensive step to ensure the SVR model is performing to the best of its capabilities <ref type="bibr" target="#b49">[50]</ref> . The results obtained by <ref type="bibr">Xiong et al.</ref> indicate that FA-MSVR proved to be a promising alternate method for time series forecasting, thus highlighting the importance of setting the SVR hyper-parameters.</p><p>Moreover, other approaches based on Linear Target Combinations for MT Regression <ref type="bibr" target="#b41">[42]</ref> , and Multi-Objective Random Forests (MORF) <ref type="bibr" target="#b28">[29]</ref> have been proposed. Most commonly investigated issues for MT learning problems include dimensionality reduction for high-dimensional multi-labeled data. The curse of dimensionality is problematic due to the possible correlations between the data and the targets <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b14">15,</ref><ref type="bibr" target="#b23">24]</ref> . This multi-collinearity may cause the learning task to be more difficult and complex <ref type="bibr" target="#b36">[37]</ref> . To reduce the dimensionality of the data, without losing the possible relationships to the targets, careful feature selection could be performed, which is a difficult task as well <ref type="bibr" target="#b30">[31,</ref><ref type="bibr" target="#b31">32]</ref> . Another issue would be processing large datasets quickly and feasibly (because of memory constraints), while providing insightful information <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b7">8]</ref> .</p><p>The RC, MTS, MTSC, ERC, and ERCC methods are introduced by Spyromitros-Xioufis et al. <ref type="bibr" target="#b38">[39]</ref> . The idea behind these algorithms was to investigate whether advances in multi-label learning can be successfully used and implemented in a multi-target regression setting, as well as shedding light on modeling target dependencies. They first describe the RC, MTS, and ERC models, which are inspired by multi-label classification algorithms, and then introduce their corrected versions. These methods involve two stages of learning, the first being building ST models and the second uses the knowledge gained by the first step to predict the target variables while using possible relationships the target variables might have with one another.</p><p>The two stages of training in MTS involve firstly, training m independent single-target models, like in ST. In the second step, a second set of m meta models are learned for each target variable, Y j , 1 ≤ j ≤ m . These meta models are learned on a transformed dataset, where the input attributes space is expanded by adding the approximated target variables obtained in the first stage, excluding the j th target being predicted. Each m meta model, h * j :</p><formula xml:id="formula_10">X × R m -1 → R , is learned by the modi- fied dataset D * j = { (x * (1) 1 , y * (1) j ) , . . . , (x * (N ) d , y * (N ) j ) } , where x * (l) j = { x (l) 1 , . . . , x (l) d , ˆ y (l) 1 , . . . , ˆ y (l) j-1 , ˆ y (l) j+1 , . . . , ˆ y (l) m } , l ∈ { 1 , . . . , N } are</formula><p>the input vectors along with the m -1 predicted target variables, represented by ˆ y , obtained by the first step. To predict the output for a new input vector x ( q ) , the models trained in the first stage are applied and an output vector, ˆ</p><formula xml:id="formula_11">y (q ) = { ˆ y (q ) 1 , . . . , ˆ y (q )</formula><p>m } = { h 1 (x (q ) ) , . . . , h m (x (q ) ) } , is obtained. The second stage models are then applied on the transformed input vector x * (q ) j , as shown above, to produce a final output vector. The ERC method is somewhat similar to the MTS method. In the training of a Regression Chain (RC) model, a random chain, or sequence, of the set of target variables is selected and for each target in the chain, models are built sequentially by using the output of the previous model as input for the next <ref type="bibr" target="#b39">[40]</ref> . If the default, ordered chain is</p><formula xml:id="formula_12">C = { Y 1 , Y 2 , . . . , Y m } , the first model h 1 : X → R is trained for Y 1 , as in ST. For the subsequent models h j, j &gt; 1 , the dataset is transformed into D * j = { (x * (1) j , y (1) j ) , . . . , (x * (N ) j , y (N ) j ) } , where x * (l) j = { x (l) 1 , . . . , x (l) d , y (l) 1 , . . . , y (l)</formula><p>j-1 } are the input vectors transformed by sequentially appending the true values of each of the previous targets in the chain. For a new input vector x ( q ) , the target values are unknown. So once the models are trained, the unseen input vector x ( q ) will be appended with the approximated target values, making the models dependent on the approximated values obtained in each step. One of the issues associated with this method is that, if a single random chain is used, the possible relationships between the targets at the head of the chain and the end of the chain are not exploited due to the algorithm's sequential nature. Also, prediction error in the earlier stages of the models will be propagated as the rest of the models are trained, which is why the Ensemble of Regressor Chains was proposed in <ref type="bibr" target="#b38">[39]</ref> . Instead of a single chain, k chains are created at random, and the final prediction values are obtained by taking the mean values of the k predicted values for each target.</p><p>In the methods described above, the estimated target variables (meta-variables) are used as input in the second stage of training. In both methods, the models are trained using these meta-variables that become noisy at prediction time, and thus the relationship between the meta-variables and target variable is muddied. Dividing the training set into sets, one for each stage, would not help this situation because both methods would be trained on training sets of decreasing size. Due to these issues, Spyromitros-Xioufis et al. proposed modifications, in <ref type="bibr" target="#b38">[39]</ref> , to both methods that resembles k -fold crossvalidation (CV) to be able to obtain unbiased estimates of the meta-variables. These methods are called Regression Chains Corrected (RCC) and Multi-Target Stacking Corrected (MTSC).</p><p>The ERCC and MTSC procedures involve repeating the RCC and MTS procedures k times, respectively, with k randomly ordered chains for ERCC, and k different modified training sets for MTSC. The algorithms were tested and compared using Bagging of 100 regression trees as their base regression algorithm with ERC and ERCC ensemble size of 10, and 10-fold cross-validation. The corrected methods exhibited better performance than their original variants, as well as ST models. The ERCC algorithm had the best overall performance, as well as being statistically significantly more accurate of all the methods tested. These methods can be found and used through the open-source Java library, Mulan <ref type="bibr" target="#b40">[41]</ref> ; to replicate the results found in <ref type="bibr" target="#b38">[39]</ref> .</p><p>The following section presents our approaches to solving the multi-target regression problem inspired by the techniques presented in <ref type="bibr" target="#b38">[39]</ref> . It will show that exploiting and making use of the possible correlations in the target variables produces better results than not.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Multi-target SVR proposal</head><p>Three novel models have been implemented for the purposes of multi-target regression. The base-line model is the problem transformation method, named SVR, where m single-target soft-margin non-linear support vector regressors (NL-SVR) are built for each target variable Y j . For NL-SVR, the regularized soft-margin loss function given in Eq. ( <ref type="formula">1</ref>) is minimized <ref type="bibr" target="#b15">[16,</ref><ref type="bibr" target="#b25">26]</ref> , minimize</p><formula xml:id="formula_13">w,ξ ,ξ * 1 2 || w|| 2 + C N l=1 ξ (l) + ξ * (l) (1) subject to ⎧ ⎨ ⎩ y (l) -w, φ x (l) ≤ + ξ (l) (1a) w, φ x (l) -y (l) ≤ + ξ * (l) (1b) ξ (l) , ξ * (l) ≥ 0 (1c)</formula><p>where w represents the SVR weight vector, represents how precise the approximations are, C &gt; 0 determines how to penalize deviations from , φ( • ) represents a feature mapping function, ξ ( l ) and ξ * ( l ) are slack variables, and y ( l ) is the label corresponding to the input vector x ( l ) . For simplicity, the bias SVR term has been excluded. In our algorithm implementation, the dual of this formulation <ref type="bibr" target="#b12">[13,</ref><ref type="bibr" target="#b15">16]</ref> given by ( <ref type="formula" target="#formula_14">2</ref>) is solved, maximize</p><formula xml:id="formula_14">α, α * - 1 2 N l,k =1 α (l) -α * (l) α (k ) -α * (k ) K x (l) , x (k ) - N l=1 α (l) + α * (l) + N l=1 y (l) α (l) -α * (l)<label>(2)</label></formula><p>subject to</p><formula xml:id="formula_15">N l=1 α (l) -α * (l) = 0 , α (l) , α * (l) ∈ [ 0 , C ]<label>(2a)</label></formula><p>where the α and α * vectors correspond to the SVR dual variables and K x (l) , x (k ) = φ(x (l) ) * φ(x (k ) ) is an ( N × N ) Gaussian kernel matrix which is dependent on the γ parameter for its broadness. Using the SVR optimization problem described, the multi-target problem is solved by transforming it into m single-target problems, as shown in Algorithm 1 and Fig. <ref type="figure" target="#fig_0">1</ref> . This Algorithm 1 MT support vector regression (SVR).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Input: Training dataset D, number of cross-validation folds k</head><p>Output: ST models h j , j = 1 , . . . , m 1: for j = 1 to m do 2:</p><formula xml:id="formula_16">D j = { (x (1) 1 , y (1) j ) , . . . , (x (N ) d , y (N ) j ) } Create the CV training D (k -i ) and test D (i ) sets 3: for i = 1 to k do 4: D (k -i ) j = { (x (k -i ) 1 , y (k -i ) j ) , . . . , (x (k -i ) d , y (k -i ) j ) } 5:</formula><formula xml:id="formula_17">D (i ) j = { (x (i ) 1 , y (i ) j ) , . . . , (x (i ) d , y (i ) j ) } Train the jth model on the training set D (k -i ) 6: h (k -i ) j : D (k -i ) j → R Test the jth model on the test set D (i ) 7: ˆ Y (i ) j = h (k -i ) j (X (i ) ) 8:</formula><p>end for Calculate and store aRRMSE error for the jth model 9: end for 10: return h j , j = 1 , . . . , m algorithm will output m single-target models, h j , ∀ j = 1 , . . . , m, for a given dataset D. It first splits the dataset into m separate ones, each with a single-target variable Y j , and then builds a distinct SVR model for each of the datasets. For predicting  the output values for a new and unseen instance, each of the m models would have to be run on the same sample, and then aggregate their independent outputs.</p><p>There are many advantages of using an SVM as a base-line model. Firstly, the optimization problem defining it contains a regularization parameter, which prevents model overfitting to the training dataset <ref type="bibr" target="#b5">[6]</ref> . Secondly, the optimization problem is convex, indicating a unique global minimum, which can be found efficiently. Furthermore, it also allows for use of the kernel trick which allows for more flexibility and knowledge in model training as well as a final sparse model, based only on the number of support vectors <ref type="bibr" target="#b25">[26]</ref> .</p><p>Building m ST models was a good base-line model, but as mentioned previously, it does not use any of the possible correlations between the target attributes during training. If these correlations are not exploited, it could retract from the model's potential performance. Therefore, we also proposed to construct a series of random chains and create an ensemble model, as done in <ref type="bibr" target="#b38">[39]</ref> , but using our base-line SVR method, named SVR Random Chains (SVRRC). For SVRRC, ensembles of at most 10 random chains are built, with length m , of different and distinct permutations of the target variable indices. For each chain, we train m chained models using the targets true values. An illustration of this process is shown in Fig. <ref type="figure" target="#fig_1">2</ref> . Due to the computational complexity of building m ! distinct chains and training ( m !) × m models, the number of ensembles and chains are limited to a maximum of 10 <ref type="bibr" target="#b38">[39]</ref> . However, if the number of target variables is less than 3, i.e. m ! ≤ 10, we construct all m ! random chains. For each of the random chains, the model is trained by predicting the first target variable in the chain. Next, the first target's true value, Y j , is appended to the end of the training set as such, D</p><formula xml:id="formula_18">* j+1 = { X * j+1 , Y j } ,</formula><p>where</p><formula xml:id="formula_19">X * j+1 is the appended training set, X * j+1 = { x (l) 1 , . . . , x (l) d , y (l) 1 , . . . , y (l) j } , l = { 1 , . . . , N } .</formula><p>The chaining process is repeated for all the target indices in the chains. When chaining target values, there are two main options: using the predicted value as input for the following target, or using the true value of the target variable as input of the subsequent targets. The main problem with the former approach is that errors are propagated throughout the training process. As each estimated, predicted value gets appended to the train set, additional noise now exists in the input space when training for the next target. However, the latter approach, minimizes error propagation, resulting in a more accurate model. Our approach employs chaining of the true values, rather than appending the targets estimation produced while training. Given the ensemble of SVRs, the predicted values for a given instance are calculated by taking the mean of the multiple models generated using different random chains. For predicting unseen inputs that have no target values, the predicted value at each step of the chain ˆ y j (l) is appended to the input as shown in Algorithm 4 . For SVRRC described in Algorithm 2 , at most 10 models will be built (one for each chain RC , RC c = { RC (1)   c , . . . , </p><formula xml:id="formula_20">RC (m ) c } , c ≤ 10 ),</formula><formula xml:id="formula_21">2: D * 1 = { (x (1)</formula><p>1 , y (1)   1 ) , . . . , (x (N )   d , y (N )</p><p>1 ) } 3: for j = 1 to m do 4:</p><formula xml:id="formula_22">h j : D * j → R 5: if j &lt; m then 6: D * j+1 ← ∅ 7:</formula><p>for i = 1 to k do 8:</p><formula xml:id="formula_23">h (i ) j : D (k -i ) j → R</formula><p>Test the model on the test set D * (i ) j 9:</p><formula xml:id="formula_24">for x * (i ) ∈ D * (i ) j do 10: ˆ y (i ) j = h (i ) j (x * (i ) )</formula><p>Append x * (i ) with the true value y (i ) j , and add it to set D * j+1 11:</p><formula xml:id="formula_25">D * j+1 = D * j+1 ∪ [ x * (i ) , y (i ) j ] 12:</formula><p>end for 13:</p><p>end for 14:</p><p>end if 15: end for 16: return h j , j = 1 , . . . , m and training m chained models. For each of the models in the chain, we use 10-fold cross-validation to ensure the hyperparameters chosen best describe the data.</p><p>When the number of output variables increases, the number of possible chains increases factorially. Therefore, there is no guarantee that the random chains generated will truly reflect the relationships among the target variables. Moreover, building an ensemble of regressors is computationally expensive. Finding a heuristic that allows the identification of a single, most appropriate chain, which fully reflects the output variable interrelations would improve the computational complexity of training the ensemble. Our third proposal builds a single chain based on the maximization of the correlations among the target variables. By calculating the correlation of the target variables and imposing that on the order of the chain, we ensure that each appended target provides some additional knowledge on the training of the next. With SVRRC, there is no proof or reasoning behind the generation of these chains, and since the number of random chains generated is limited to 10, there is no way of ensuring that the 10 chains fully represent the targets' dependencies. Therefore, calculating and using the correlations of the targets would break this uncertainty, as done in the SVR Correlation Chain (SVRCC) method, shown in Algorithm 3 and Fig. <ref type="figure" target="#fig_2">3</ref> . The computational complexity and hardware constraints (memory size) of constructing the targets' ( m × m ) correlation matrix are only dependent on the number of targets. This eliminates the additional complexity of creating and aggregating at most m randomly chained models. To calculate the correlation coefficients of the targets, we first construct the targets' co-variance matrix,</p><formula xml:id="formula_26">i, j = cov (Y i , Y j ) = E (Y i -μ i )(Y j -μ j ) , where μ i = E (Y i ) , and E ( Y i ) is the expected value of Y i , ∀ i, j ∈ { 1 , . . . , m } . The correlation coefficient matrix is then calculated as follows, ρ Y = corrcoe f (Y ) = cov (Y i ,Y j ) cov (Y i ,Y i ) cov (Y j ,Y j )</formula><p>, ∀ i, j ∈ { 1 , . . . , m } . The correlation coefficient matrix will describe the linear relationship among the target Algorithm 3 MT SVR with max-correlation chain (SVRCC).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Input: Training dataset D, number of cross-validation folds k</head><formula xml:id="formula_27">Output: Chained model h j , j = { 1 , . . . , m } 1: correlation ( m ×m ) = corrcoe f (Y ) 2: sums m = sum ( correlation ( l, j ) ) , l = 1 , . . . , m</formula><p>Create the maximum correlation chain c of size m (1)   1 , y (1)   c 1 ) , . . . , (x (N )   d , y (N ) c 1 ) } 5: for j = 1 to m do 6:</p><formula xml:id="formula_28">3: c m = sort ( sums m , decreasing ) 4: D * c 1 = { (x</formula><formula xml:id="formula_29">h c j : D * c j → R 7:</formula><p>if j &lt; m then 8:</p><formula xml:id="formula_30">D * c j+1 ← ∅ Split D * c j into k disjoint parts D * (i ) c j , i = 1 , . .</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>. , k for training and testing</head><p>9:</p><p>for i = 1 to k do 10:</p><formula xml:id="formula_31">h (i ) c j : D * (k -i ) c j → R</formula><p>Test the model on the test set D * (i ) c j 11:</p><formula xml:id="formula_32">for x * (i ) ∈ D * (i ) c j do 12: ˆ y i c j = h (i ) c j (x * (i ) )</formula><p>Append x * (i ) with the true value y (i ) c j , and add it to set D * c ( j+1)</p><p>13:</p><formula xml:id="formula_33">D * c ( j+1) = D * c ( j+1) ∪ [ x * (i ) , y (i ) c j ] 14:</formula><p>end for 15:</p><p>end for 16:</p><p>end if 17: end for 18: return h j , j = { 1 , . . . , m } variables and then they can be sorted in decreasing order, creating the maximum correlation chain. Having a single chain reduces the computational complexity, while providing a competitive model. For an unknown sample, the prediction method follows the same procedure as Algorithm 4 , where the predicted value ˆ y j (l) is appended to the input.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Algorithm 4 SVR chained prediction.</head><p>Input: Unknown sample x (q ) , chained model h j , j = { 1 . . . m } Output: Output vector ˆ y (q ) of size m Initialize the output vector 1: ˆ y (q ) = 0 Initialize the input space for the first target 2: D * 1 = x (q ) 3: for j = 1 to m do 4:</p><formula xml:id="formula_34">x * ∈ D * j 5: ˆ y (q ) j = h j (x * ) 6: D * j+1 = D * j ∪ [ x * , ˆ y (q ) j ] 7: end for</formula><p>Create ST transformation of D with 1 st index in chain c 8: return ˆ y (q )   4. Experiments</p><p>This section presents the experimental comparison of the three models proposed, along with seven others from state-ofthe-art. It introduces the datasets, algorithms, performance measures, and statistical analysis.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Datasets</head><p>This section presents a description of the datasets used in the experiments. Although there are many interesting applications of multi-target regression, there are few publicly available datasets to use. The datasets used in the experimental study were collected from the Mulan website <ref type="bibr" target="#b40">[41]</ref> , as well as the UCI Machine Learning Repository <ref type="bibr" target="#b32">[33]</ref> . Information on the 24 datasets used is summarized in Table <ref type="table" target="#tab_2">3</ref> , where the number of samples, attributes (dimensionality), and targets are shown. Note that the datasets used in this experiment are the same datasets used in <ref type="bibr" target="#b38">[39]</ref> to properly perform our model comparisons, as well as additional datasets to ensure thorough investigation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Algorithms</head><p>This section presents the algorithms that we compare our proposals' performances to; namely RC, ST, MTS, MTSC, ERC, ERCC, and MORF which has been used in the experimental study conducted in <ref type="bibr" target="#b38">[39]</ref> . Our contributions are compared to these algorithms because they have shown considerable performance in training multi-target models. The have also made their framework readily available for reproducing their results, allowing proper comparisons to be made with their methods. Moreover, note that all three SVR algorithms are implemented within the general framework of Mulan's MTRegressor<ref type="foot" target="#foot_0">1</ref>  <ref type="bibr" target="#b40">[41]</ref> , which was built on top of Weka<ref type="foot" target="#foot_1">2</ref>  <ref type="bibr" target="#b43">[44]</ref> , and we also used LIBSVM's Epsilon-SVR <ref type="bibr" target="#b9">[10]</ref> implementation of Support Vector Regressors as the base SVR model.</p><p>In order to train a model that accurately describes a dataset and is not over-fit, experimenting with different model hyper-parameters is required. In the case of SVMs, specifically the SVM regression task, these parameters are the penalty parameter C , the Gaussian kernel parameter γ , and the error or tube parameter . We experimented with a range of parameters given in Eqs. (3a) -(3c) , referred to as (3), to ensure the final model is best representative of the dataset the model is being trained on. We have compared the models trained on the different hyper-parameters using k -fold cross-validation, which ensures that the models' performances are accurately assessed, and the model built is not biased towards the full dataset. This is a crucial step in evaluating the generalizability and efficiency of the trained models on an independent dataset.</p><formula xml:id="formula_35">C ∈ { 1 , 10 , 100 } (3a) γ ∈ { 1 -9 , 1 -7 , 1 -5 , 1 -3 , 1 -1 , 1 , 5 , 10 } (3b) ∈ { 0 . 01 , 0 . 1 , 0 . 2 } (3c)</formula><p>To ensure having a controlled environment when conducting our performance comparisons, the experimental environment for running the competing algorithms was the same as what was done in <ref type="bibr" target="#b38">[39]</ref> . This includes the following. The ST base-line model used was Bagging <ref type="bibr" target="#b6">[7]</ref> of 100 regression trees <ref type="bibr" target="#b44">[45]</ref> . The MTSC and ERCC methods are run using 10-fold cross-validation, and the ensemble size for the ERC and ERCC methods was set to 10. The ensemble size of 100 trees was used for MORF, and the rest of its parameters were set as recommended by Kocev et al. <ref type="bibr" target="#b29">[30]</ref> .</p><p>The main preprocessing step used for the experiments was normalization. The input variables were scaled to have a mean value of 0 and standard deviation of 1. To ensure that each sample is used when building the final model, we used sampling without replacement when creating our training and testing sets. The preprocessed and final datasets used in the experiments, along with the code for the algorithms presented in this paper, can be publicly found here 3 .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Performance evaluation</head><p>The performance metrics used to analyze our contributions' performances are shown in Eqs. ( <ref type="formula" target="#formula_36">4</ref>) - <ref type="bibr" target="#b6">( 7 )</ref>. For unseen or test datasets of size N test , the performances are evaluated by taking the the run time (seconds) each algorithm takes to build a classifier, as well as the following metrics <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b17">18]</ref> , where the upwards arrow ↑ indicates maximizing the metric and the downwards arrow ↓ indicates minimizing the metric.</p><p>• The average correlation coefficient (aCC ↑ ):</p><formula xml:id="formula_36">1 m m j=1 N test l=1 (y (l) j -ȳ j )( ˆ y (l) j -¯ y j ) N test l=1 (y (l) j -ȳ j ) 2 N test l=1 ( ˆ y (l) j -¯ y j ) 2<label>(4)</label></formula><p>• The mean squared error (MSE ↓ ):</p><formula xml:id="formula_37">1 m m j=1 1 N test N test l=1 (y (l) j -ˆ y (l) j ) 2</formula><p>(5)</p><p>• The average root mean squared error (aRMSE ↓ ):</p><formula xml:id="formula_38">1 m m j=1 N test l=1 (y (l) j -ˆ y (l) j ) 2 N test<label>(6)</label></formula><p>• The average relative root mean squared error (aRRMSE ↓ ):</p><formula xml:id="formula_39">1 m m j=1 N test l=1 (y (l) j -ˆ y (l) j ) 2 N test l=1 (y (l) j -ȳ j ) 2<label>(7)</label></formula><p>The predicted output is represented by ˆ y , the average of the predicted output is ¯ y , and the average of the true output target variable is ȳ . The test dataset is the hold-out set during cross validation. This ensures our model is evaluated on data that it has not been trained on, and thus unbiased towards the training datasets. It also contributes to the generalizability and robustness of the model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Results &amp; statistical analysis</head><p>This section presents the results obtained by each metric of the 10 algorithms (columns) on 24 multi-target datasets (rows), along with the averaged results, average rank according to Friedman <ref type="bibr" target="#b18">[19]</ref> , and statistical analysis. Tables <ref type="table" target="#tab_3">4 ,</ref><ref type="table" target="#tab_5">6</ref> , 8 , 10 , and 12 show the results of the metrics, described in Section 4.3 , of our algorithm implementations compared with those of RC, MORF, ST, MTS, MTSC, ERC, and ERCC . Each subsection discusses a single metric along with the statistical analysis of the results. The best metric value obtained on each dataset is typeset in bold.</p><p>In order to compare the performances of the multiple models, non-parametric statistical tests are used to validate the experiments results obtained <ref type="bibr" target="#b13">[14,</ref><ref type="bibr" target="#b18">19]</ref> . To determine whether significant differences exist among the performance and results of the algorithms, the Iman-Davenport non-parametric test is run <ref type="bibr" target="#b19">[20]</ref> . It is applied to rank the algorithms over the datasets used, according to the Friedman test, where the best performing algorithm is given rank 1, the next best given rank 2, and so on. The average ranks are presented in the last row of the results tables, and the lowest rank value is typeset in bold.</p><p>Having the rank values helps in determining the best performing algorithm across all datasets and metrics.</p><p>The Iman-Davenport test indicates statistically significant differences, then the Bonferroni-Dunn post-hoc test <ref type="bibr" target="#b16">[17]</ref> is used to find these differences that occur between the algorithms. The test assumes that the two classifiers performances are significantly different if their average ranks differ by at least some critical value <ref type="bibr" target="#b20">[21]</ref> . Below each result table, a diagram highlighting the critical distance (in gray) between each algorithm is shown.</p><p>The Wilcoxon, Nemenyi, and Holm <ref type="bibr" target="#b42">[43]</ref> tests were run for each of the result metrics to compute multiple pairwise comparisons among the proposed algorithms and the state-of-the-art methods. The tests determine whether significant Post Hoc (Friedman) comparison for α = 0 . 05 differences between pairs of algorithms exist. Tables <ref type="table" target="#tab_4">5 ,</ref><ref type="table" target="#tab_6">7</ref> , 9 , 11 , and 13 show the sum of ranks R + and R -of the Wilcoxon rank-sum test, and the p -values for the 3 tests, which show the statistical confidence rather than using a fixed α value.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.">Average correlation coefficient</head><p>Table <ref type="table" target="#tab_3">4</ref> shows that our proposed methods perform the best on 15 out of the 24 datasets. Specifically, the maximum correlation chain method, SVRCC, performs the best on 11, which is better than the total number of datasets the state-ofthe-art methods performed better at <ref type="bibr" target="#b8">(9)</ref>. The Iman-Davenport statistic, distributed according to the F-distribution with 9 and 207 degrees of freedom is 6.72, with a p -value of 1 . 9 E -8 which is significantly less than 0.01, implying a statistical confidence larger than 99%. Therefore, we can conclude that there exist statistically significant differences between the aCC results of the algorithms. Fig. <ref type="figure" target="#fig_3">4</ref> shows the mean rank values of each algorithm along with the critical difference value, 2.4236, for α = 0 . 05 . The algorithms that are to the right of the critical difference rectangle are the ones with significantly different results. Therefore, the 6 out of 10 algorithms beyond the critical difference perform significantly worse than our control algorithm, SVRCC.</p><p>Table <ref type="table" target="#tab_4">5</ref> provides complementary analysis of the results. According to the Wilcoxon test, SVRCC is shown to have significantly better performance over all algorithms with p -value &lt; 0.05. The Nemenyi and Holm tests show that SVRCC performs significantly better than 6 out of the 9 algorithms with p -value ≤ 5 . 6 E -<ref type="foot" target="#foot_2">3</ref> and ≤ 1 . 7 E -2 , respectively. The exact confidence for algorithm SVRCC against all others is 0.95. </p><formula xml:id="formula_40">3 . 4 E -2 1 . 7 E -2 1 . 7 E -2 SVR 262.0 38.0 7 . 4 E -4 6 . 3 E -2 2 . 5 E -2 SVRRC 245.0 55.0 5 . 3 E -3 5 . 1 E -1 5 . 0 E -2</formula><p>Post Hoc (Friedman) comparison for α = 0 . 05 .  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.">Mean square error</head><p>Table <ref type="table" target="#tab_5">6</ref> shows that our proposed methods perform the best on 15 out of the 24 datasets. In this case, SVRCC also performs the best on 11 versus the 9 that the state-of-the-art methods performed better at. The Iman-Davenport statistic, distributed according to the F-distribution with 9 and 207 degrees of freedom is 6.57, with a p -value of 3 . 1 E -8 , implying statistically significant differences among the MSE results. Post Hoc (Friedman) comparison for α = 0 . 05 . Fig. <ref type="figure" target="#fig_4">5</ref> shows the mean rank values of each algorithm along with the critical difference value, 2.4236, for α = 0 . 05 . According to the critical difference bar, there are 6 out of 10 algorithms beyond that perform significantly worse than our control algorithm, SVRCC.</p><p>According to the Wilcoxon test, shown in Table <ref type="table" target="#tab_6">7</ref> , SVRCC is shown to have significantly better performance over all algorithms with p -value &lt; 0.05. The Nemenyi and Holm tests show that SVRCC performs significantly better than 6 out of the 9 algorithms with p -values ≤ 5 . 6 E -3 and ≤ 1 . 7 E -2 respectively, and has an exact confidence of 0.95 against all others.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3.">Average root mean square error</head><p>Table <ref type="table" target="#tab_7">8</ref> shows that our proposed methods perform the best on 18 out of the 24 datasets. In this case, SVRCC performs the best on 15 versus the 6 that the state-of-the-art methods performed better at. The Iman-Davenport statistic is 7.6, with a p -value of 1 . 3 E -9 , implying statistically significant differences in the aRMSE results. Fig. <ref type="figure" target="#fig_5">6</ref> shows the mean rank values of each algorithm along with the critical difference value, 2.4236, for α = 0 . 05 . According to the critical difference bar, there are 7 out of 10 algorithms that perform significantly worse than our control algorithm, SVRCC. Post Hoc (Friedman) comparison for α = 0 . 05 . According to the Wilcoxon test, shown in Table <ref type="table" target="#tab_8">9</ref> , SVRCC is shown to have significantly better performance over all algorithms with p -value &lt; 0.01. The Nemenyi test shows that SVRCC performs significantly better than 7 out of the 9 algorithms with p -value ≤ 5 . 6 E -3 , while the stricter Holm test shows that it performs significantly better than 8 out of the 9 algorithms with p -value ≤ 0.05.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.4.">Average relative root mean square error</head><p>Table <ref type="table" target="#tab_9">10</ref> shows that our proposed methods perform the best on 16 out of the 24 datasets. In this case, SVRCC performs the best on 11 versus the 6 that the state-of-the-art methods performed better at. The Iman-Davenport statistic is 8.54, with a p -value of 7 . 6 E -11 . Fig. <ref type="figure" target="#fig_6">7</ref> shows the mean rank values of each algorithm along with the critical difference value, 2.4236, for α = 0 . 05 . According to the critical difference bar, there are 6 out of 10 algorithms beyond that perform significantly worse than our control algorithm, SVRCC.</p><p>According to the Wilcoxon test, shown in Table <ref type="table" target="#tab_0">11</ref> , SVRCC is shown to have significantly better performance over all algorithms with p -value &lt; 0.05, and 8 out of the 9 algorithms for p -value &lt; 0.01. The Nemenyi test shows that SVRCC Post Hoc (Friedman) comparison for α = 0 . 05 . performs significantly better than 6 out of the 9 algorithms with p -value ≤ 5 . 6 E -3 , and the Holm test shows its performance is significantly better than 8 out of the 9 algorithms with p -value ≤ 0.05.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.5.">Run time</head><p>Table <ref type="table" target="#tab_10">12</ref> shows that our proposed methods perform faster on 16 out of the 24 datasets. In this case, SVR performs the best on 12 versus the 6 of the state-of-the-art methods. The Iman-Davenport statistic 64.41, with a p -value of 0.0 which implies a statistical confidence of 100%. Fig. <ref type="figure" target="#fig_7">8</ref> shows the mean rank values of each algorithm along with the critical difference value, 2.4236, for α = 0 . 05 . According to the critical difference bar, there are 6 out of 10 algorithms beyond that perform significantly worse than our control algorithm, SVR. According to the Wilcoxon test, shown in Table <ref type="table" target="#tab_11">13</ref> , SVR is shown to have significantly better performance over all algorithms with p -value &lt; 0.01. The Nemenyi and Holm tests show that SVRCC performs significantly better than 6 out of the 9 algorithms and 8 out of the 9 algorithms with p -value ≤ 5 . 6 E -3 and p -value ≤ 1 . 6 E -2 , respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.6.">Discussion</head><p>Results indicate that our proposed methods perform competitively against the current state-of-the-art, specifically SVRCC which exploits relationships among the targets. Firstly, they show that using SVR as a base-line method for multi-target chaining causes a performance improvement in model prediction, compared to other ST base-line models, as well as most MT methods. This demonstrates the advantages of using the SVR method as a base-line for multi-target learning, thus increasing the performance of the ensemble of regressor chains, SVRRC, compared to the state-of-the-art ERCC. More importantly, the results highlight the major advantage of capturing and exploiting the targets' relationships during model training. Using an ensemble of randomly generated chains does not ensure the targets' correlations are fully captured; however, using a maximum correlation chain improves the performance in terms of quality metrics as well as run time. The run time of SVR was shown to be the fastest, due to the fact that its complexity is mostly dependent on the number of targets. However, this method does not consider any of the correlations that might exist among the target variables, but SVRCC does take them into account and does not have a significant impact on run time. The most noteworthy finding that highlights advantage of using the base-line SVR and the maximum correlation method, SVRCC, rather than random chaining as done in ERCC, are the run time results and their analysis. ERCC had the worst run time across all datasets, whereas our proposals, SVR and SVRCC, performed the fastest. This emphasizes the advantage of using a single chain rather an ensemble of random chains, especially when the single chain is ordered in the direction of the targets maximum correlation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Conclusion</head><p>This paper proposed three novel methods for solving multi-target regression problems. The first method takes a problem transformation approach, which generates m ST models, each trained independently. This base-line approach was shown to perform the best in terms of run time, but its drawback is that it does not take the possible correlations between the target variables into account during training. The second implements SVR as an ensemble model of randomly generated chains, inspired by the state-of-the-art classification method ERCC. This was done to investigate the effects of exploiting correlations among the target variables during model training. Due to the random nature of this method, capturing target correlations is not guaranteed. The third proposal, SVRCC, generates a single chain that is ordered in the direction of the targets' maximum correlation, ensuring the correlations among targets are taken into account within the learning process.</p><p>The experimental study compared the proposed methods' performances to 7 state-of-the-art methods on 24 MT regression datasets. Firstly, the results show the superior performance of using the SVR method as a base-line model, rather than regression trees as used in MORF. The results for SVRRC show an increase in performance when random chaining is used to develop an ensemble model. This indicates the importance of the relationship among the target variables during training. Finally, the results show the superiority of using the SVRCC method, which was ranked the best in all quality metrics and second best in terms of run time. SVRCC performed better than the single-target SVR model and the randomly chained ensemble model SVRRC, showing that the targets' maximum correlation does positively contribute toward model training. The statistical analysis supports and shows the significance of the results obtained by our experiments. They demonstrated that statistically significant differences exist between the proposed algorithms against the state-of-the-art methods. SVRCCs competitive performance, as well as speed, shows that it is a powerful learning algorithm for multi-target problems.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 1 .</head><label>1</label><figDesc>Fig. 1. SVR flow diagram. Note: firstly, the SVR method separates the MT dataset into m ST datasets, D 1 , D 2 , . . . , D m . It then independently trains models, h 1 , h 2 , . . . , h m , for each ST dataset.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 2 .</head><label>2</label><figDesc>Fig. 2. SVRRC flow diagram. Note: This figure illustrates how the SVRRC algorithm trains a dataset with 3 target variables. It first builds the 6 random chains of the target's indices (3 examples are shown). It then constructs a chained model by proceeding recursively over the chain, building a model, and appending the current target to the input space to predict the next target in the chain.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 3 .</head><label>3</label><figDesc>Fig. 3. SVRCC flow diagram. Note: This figure shows how the SVRCC algorithm trains a sample dataset with 3 target variables. It first finds the direction of maximum correlation among the targets and uses that order as the only chain. It then constructs the chained model as done in SVRRC.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Fig. 4 .</head><label>4</label><figDesc>Fig. 4. Bonferroni-Dunn test for aCC.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Fig. 5 .</head><label>5</label><figDesc>Fig. 5. Bonferroni-Dunn test for MSE.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Fig. 6 .</head><label>6</label><figDesc>Fig. 6. Bonferroni-Dunn test for aRMSE.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Fig. 7 .</head><label>7</label><figDesc>Fig. 7. Bonferroni-Dunn test for aRRMSE.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Fig. 8 .</head><label>8</label><figDesc>Fig. 8. Bonferroni-Dunn test for Run Time.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1</head><label>1</label><figDesc>Notation.</figDesc><table><row><cell>Definition</cell><cell>Notation</cell></row><row><cell>Number of samples</cell><cell>N</cell></row><row><cell>Number of input attributes</cell><cell>d</cell></row><row><cell>Input space</cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 3</head><label>3</label><figDesc>Multi-target regression datasets.</figDesc><table><row><cell>Dataset</cell><cell>Samples (N )</cell><cell>Attributes ( d )</cell><cell>Targets ( m )</cell></row><row><cell>EDM</cell><cell>145</cell><cell>16</cell><cell>2</cell></row><row><cell>Enb</cell><cell>768</cell><cell>8</cell><cell>2</cell></row><row><cell>Jura</cell><cell>359</cell><cell>11</cell><cell>7</cell></row><row><cell>Osales</cell><cell>639</cell><cell>413</cell><cell>12</cell></row><row><cell>Scpf</cell><cell>1137</cell><cell>23</cell><cell>3</cell></row><row><cell>Slump</cell><cell>103</cell><cell>7</cell><cell>3</cell></row><row><cell>Solar flare 1</cell><cell>323</cell><cell>10</cell><cell>3</cell></row><row><cell>Solar flare 2</cell><cell>1066</cell><cell>10</cell><cell>3</cell></row><row><cell>Water quality</cell><cell>1060</cell><cell>16</cell><cell>14</cell></row><row><cell>OES97</cell><cell>323</cell><cell>263</cell><cell>16</cell></row><row><cell>OES10</cell><cell>403</cell><cell>298</cell><cell>16</cell></row><row><cell>ATP1d</cell><cell>201</cell><cell>411</cell><cell>6</cell></row><row><cell>ATP7d</cell><cell>188</cell><cell>411</cell><cell>6</cell></row><row><cell>Andro</cell><cell>49</cell><cell>30</cell><cell>6</cell></row><row><cell>Wisconsin cancer</cell><cell>198</cell><cell>34</cell><cell>2</cell></row><row><cell>Stock</cell><cell>950</cell><cell>10</cell><cell>3</cell></row><row><cell>California housing</cell><cell>20640</cell><cell>7</cell><cell>2</cell></row><row><cell>Puma8NH</cell><cell>8192</cell><cell>8</cell><cell>3</cell></row><row><cell>Puma32H</cell><cell>8192</cell><cell>32</cell><cell>6</cell></row><row><cell>Friedman</cell><cell>500</cell><cell>25</cell><cell>6</cell></row><row><cell>Polymer</cell><cell>41</cell><cell>10</cell><cell>4</cell></row><row><cell>M5SPEC</cell><cell>80</cell><cell>700</cell><cell>3</cell></row><row><cell>MP5SPEC</cell><cell>80</cell><cell>700</cell><cell>3</cell></row><row><cell>MP6SPEC</cell><cell>80</cell><cell>700</cell><cell>3</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 4</head><label>4</label><figDesc>Average correlation coefficient (aCC) results &amp; algorithm rank.</figDesc><table><row><cell>Datasets</cell><cell>MORF</cell><cell>ST</cell><cell>MTS</cell><cell>MTSC</cell><cell>RC</cell><cell>ERC</cell><cell>ERCC</cell><cell>SVR</cell><cell>SVRRC</cell><cell>SVRCC</cell></row><row><cell>Slump</cell><cell>0.6965</cell><cell>0.7062</cell><cell>0.7163</cell><cell>0.6977</cell><cell>0.6956</cell><cell>0.6977</cell><cell>0.7023</cell><cell>0.7245</cell><cell>0.7339</cell><cell>0.7457</cell></row><row><cell>Polymer</cell><cell>0.7305</cell><cell>0.7336</cell><cell>0.7371</cell><cell>0.7228</cell><cell>0.7015</cell><cell>0.7029</cell><cell>0.7222</cell><cell>0.7634</cell><cell>0.7857</cell><cell>0.7905</cell></row><row><cell>Andro</cell><cell>0.7349</cell><cell>0.6454</cell><cell>0.6793</cell><cell>0.6581</cell><cell>0.6915</cell><cell>0.6806</cell><cell>0.6653</cell><cell>0.6880</cell><cell>0.6951</cell><cell>0.7056</cell></row><row><cell>EDM</cell><cell>0.6722</cell><cell>0.6352</cell><cell>0.6412</cell><cell>0.6354</cell><cell>0.6355</cell><cell>0.6379</cell><cell>0.6354</cell><cell>0.6484</cell><cell>0.6565</cell><cell>0.6567</cell></row><row><cell>Solar Flare 1</cell><cell>0.1083</cell><cell>0.1258</cell><cell>0.1034</cell><cell>0.1193</cell><cell>0.1492</cell><cell>0.1387</cell><cell>0.1292</cell><cell>0.1066</cell><cell>0.0857</cell><cell>0.1152</cell></row><row><cell>Jura</cell><cell>0.7854</cell><cell>0.7907</cell><cell>0.7880</cell><cell>0.7882</cell><cell>0.7877</cell><cell>0.7884</cell><cell>0.7897</cell><cell>0.7789</cell><cell>0.7921</cell><cell>0.7983</cell></row><row><cell>Enb</cell><cell>0.9828</cell><cell>0.9832</cell><cell>0.9822</cell><cell>0.9829</cell><cell>0.9813</cell><cell>0.9823</cell><cell>0.9837</cell><cell>0.9858</cell><cell>0.9867</cell><cell>0.9868</cell></row><row><cell>Solar Flare 2</cell><cell>0.2357</cell><cell>0.2295</cell><cell>0.2375</cell><cell>0.2343</cell><cell>0.2302</cell><cell>0.2351</cell><cell>0.2432</cell><cell>0.1470</cell><cell>0.1648</cell><cell>0.1656</cell></row><row><cell>Wisconsin Cancer</cell><cell>0.3362</cell><cell>0.3587</cell><cell>0.3652</cell><cell>0.3588</cell><cell>0.3628</cell><cell>0.3609</cell><cell>0.3590</cell><cell>0.3187</cell><cell>0.3208</cell><cell>0.3373</cell></row><row><cell>California Housing</cell><cell>0.7705</cell><cell>0.7720</cell><cell>0.7149</cell><cell>0.7451</cell><cell>0.7007</cell><cell>0.7844</cell><cell>0.8065</cell><cell>0.7847</cell><cell>0.7949</cell><cell>0.8007</cell></row><row><cell>Stock</cell><cell>0.9785</cell><cell>0.9747</cell><cell>0.9755</cell><cell>0.9752</cell><cell>0.9753</cell><cell>0.9757</cell><cell>0.9763</cell><cell>0.9825</cell><cell>0.9829</cell><cell>0.9822</cell></row><row><cell>SCPF</cell><cell>0.5827</cell><cell>0.5508</cell><cell>0.5503</cell><cell>0.5477</cell><cell>0.5569</cell><cell>0.5656</cell><cell>0.5515</cell><cell>0.5891</cell><cell>0.5975</cell><cell>0.5946</cell></row><row><cell>Puma8NH</cell><cell>0.5424</cell><cell>0.4828</cell><cell>0.4942</cell><cell>0.4205</cell><cell>0.4677</cell><cell>0.4656</cell><cell>0.4650</cell><cell>0.6041</cell><cell>0.5975</cell><cell>0.6038</cell></row><row><cell>Friedman</cell><cell>0.1507</cell><cell>0.1609</cell><cell>0.1548</cell><cell>0.1667</cell><cell>0.1558</cell><cell>0.1608</cell><cell>0.1632</cell><cell>0.1710</cell><cell>0.1748</cell><cell>0.1752</cell></row><row><cell>Puma32H</cell><cell>0.3085</cell><cell>0.2934</cell><cell>0.2890</cell><cell>0.2504</cell><cell>0.2754</cell><cell>0.2870</cell><cell>0.2797</cell><cell>0.3358</cell><cell>0.3351</cell><cell>0.3385</cell></row><row><cell>Water Quality</cell><cell>0.4303</cell><cell>0.4063</cell><cell>0.4019</cell><cell>0.4051</cell><cell>0.3992</cell><cell>0.4052</cell><cell>0.4147</cell><cell>0.3545</cell><cell>0.3828</cell><cell>0.3857</cell></row><row><cell>M5SPEC</cell><cell>0.8161</cell><cell>0.8346</cell><cell>0.8134</cell><cell>0.8228</cell><cell>0.8333</cell><cell>0.8340</cell><cell>0.8308</cell><cell>0.9451</cell><cell>0.9452</cell><cell>0.9472</cell></row><row><cell>MP5SPEC</cell><cell>0.8315</cell><cell>0.8536</cell><cell>0.8244</cell><cell>0.8535</cell><cell>0.8524</cell><cell>0.8526</cell><cell>0.8542</cell><cell>0.9560</cell><cell>0.9602</cell><cell>0.9633</cell></row><row><cell>MP6SPEC</cell><cell>0.8317</cell><cell>0.8531</cell><cell>0.8231</cell><cell>0.8531</cell><cell>0.8507</cell><cell>0.8515</cell><cell>0.8541</cell><cell>0.94 4 4</cell><cell>0.9500</cell><cell>0.9528</cell></row><row><cell>ATP7d</cell><cell>0.8260</cell><cell>0.8408</cell><cell>0.8422</cell><cell>0.8474</cell><cell>0.8273</cell><cell>0.8351</cell><cell>0.8464</cell><cell>0.8305</cell><cell>0.8407</cell><cell>0.8400</cell></row><row><cell>OES97</cell><cell>0.7829</cell><cell>0.7995</cell><cell>0.7990</cell><cell>0.8001</cell><cell>0.7986</cell><cell>0.7990</cell><cell>0.7999</cell><cell>0.8116</cell><cell>0.8134</cell><cell>0.8137</cell></row><row><cell>Osales</cell><cell>0.7186</cell><cell>0.6912</cell><cell>0.7104</cell><cell>0.7076</cell><cell>0.6357</cell><cell>0.7136</cell><cell>0.7193</cell><cell>0.6511</cell><cell>0.6433</cell><cell>0.6677</cell></row><row><cell>ATP1d</cell><cell>0.8961</cell><cell>0.9066</cell><cell>0.9051</cell><cell>0.9075</cell><cell>0.9048</cell><cell>0.9081</cell><cell>0.9071</cell><cell>0.9092</cell><cell>0.9130</cell><cell>0.9100</cell></row><row><cell>OES10</cell><cell>0.8708</cell><cell>0.8808</cell><cell>0.8805</cell><cell>0.8806</cell><cell>0.8804</cell><cell>0.8804</cell><cell>0.8809</cell><cell>0.8911</cell><cell>0.8924</cell><cell>0.8963</cell></row><row><cell>Average</cell><cell>0.6508</cell><cell>0.6462</cell><cell>0.6429</cell><cell>0.6409</cell><cell>0.6396</cell><cell>0.6476</cell><cell>0.6492</cell><cell>0.6634</cell><cell>0.6685</cell><cell>0.6739</cell></row><row><cell>Ranks</cell><cell>6.4167</cell><cell>5.8958</cell><cell>6.6042</cell><cell>6.4792</cell><cell>7.5208</cell><cell>5.8958</cell><cell>4.8542</cell><cell>4.7917</cell><cell>3.7083</cell><cell>2.8333</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 5</head><label>5</label><figDesc>Wilcoxon, Nemenyi, and Holm tests for aCC.</figDesc><table><row><cell>SVRCC vs.</cell><cell>Wilcoxon R +</cell><cell>Wilcoxon R -</cell><cell>Wilcoxon p -value</cell><cell>Nemenyi p -value</cell><cell>Holm p -value</cell></row><row><cell>MORF</cell><cell>224.0</cell><cell>76.0</cell><cell>3 . 4 E -2</cell><cell>4 . 1 E -5</cell><cell>8 . 3 E -3</cell></row><row><cell>ST</cell><cell>239.0</cell><cell>61.0</cell><cell>9 . 6 E -3</cell><cell>4 . 6 E -4</cell><cell>1 . 3 E -2</cell></row><row><cell>MTS</cell><cell>242.0</cell><cell>58.0</cell><cell>7 . 2 E -3</cell><cell>1 . 6 E -5</cell><cell>6 . 3 E -3</cell></row><row><cell>MTSC</cell><cell>238.0</cell><cell>62.0</cell><cell>1 . 1 E -2</cell><cell>3 . 0 E -5</cell><cell>7 . 1 E -3</cell></row><row><cell>RC</cell><cell>250.0</cell><cell>50.0</cell><cell>3 . 1 E -3</cell><cell>0.0 0 0 0</cell><cell>5 . 6 E -3</cell></row><row><cell>ERC</cell><cell>229.0</cell><cell>71.0</cell><cell>2 . 3 E -2</cell><cell>4 . 6 E -4</cell><cell>1 . 0 E -2</cell></row><row><cell>ERCC</cell><cell>221.0</cell><cell>79.0</cell><cell>4 . 3 E -2</cell><cell>2 . 1 E -2</cell><cell>1 . 7 E -2</cell></row><row><cell>SVR</cell><cell>297.0</cell><cell>3.00</cell><cell>6 . 0 E -7</cell><cell>2 . 5 E -2</cell><cell>2 . 5 E -2</cell></row><row><cell>SVRRC</cell><cell>266.5</cell><cell>33.5</cell><cell>4 . 0 E -4</cell><cell>3 . 2 E -1</cell><cell>5 . 0 E -2</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 6</head><label>6</label><figDesc>Mean square error (MSE) results &amp; algorithm rank.</figDesc><table><row><cell>Datasets</cell><cell>MORF</cell><cell>ST</cell><cell>MTS</cell><cell>MTSC</cell><cell>RC</cell><cell>ERC</cell><cell>ERCC</cell><cell>SVR</cell><cell>SVRRC</cell><cell>SVRCC</cell></row><row><cell>Slump</cell><cell>1.4388</cell><cell>1.4161</cell><cell>1.3667</cell><cell>1.4414</cell><cell>1.4602</cell><cell>1.4727</cell><cell>1.4183</cell><cell>1.2991</cell><cell>1.1726</cell><cell>1.1614</cell></row><row><cell>Polymer</cell><cell>1.6718</cell><cell>1.8120</cell><cell>1.5446</cell><cell>1.6726</cell><cell>1.8259</cell><cell>1.9999</cell><cell>1.6873</cell><cell>1.1874</cell><cell>1.1068</cell><cell>1.0796</cell></row><row><cell>Andro</cell><cell>1.4930</cell><cell>2.1467</cell><cell>1.4714</cell><cell>1.7525</cell><cell>2.2603</cell><cell>2.0812</cell><cell>1.8707</cell><cell>1.5406</cell><cell>1.2847</cell><cell>1.2187</cell></row><row><cell>EDM</cell><cell>0.8342</cell><cell>0.9373</cell><cell>0.9352</cell><cell>0.9418</cell><cell>0.9389</cell><cell>0.9326</cell><cell>0.9393</cell><cell>0.9092</cell><cell>0.8650</cell><cell>0.8817</cell></row><row><cell>Solar Flare 1</cell><cell>3.3458</cell><cell>3.1196</cell><cell>3.1193</cell><cell>3.0524</cell><cell>3.0357</cell><cell>3.0381</cell><cell>3.0594</cell><cell>2.9912</cell><cell>3.0176</cell><cell>3.0129</cell></row><row><cell>Jura</cell><cell>1.0973</cell><cell>1.0595</cell><cell>1.0732</cell><cell>1.0695</cell><cell>1.0744</cell><cell>1.0694</cell><cell>1.0632</cell><cell>1.1167</cell><cell>1.0435</cell><cell>1.0315</cell></row><row><cell>Enb</cell><cell>0.0381</cell><cell>0.0361</cell><cell>0.0407</cell><cell>0.0377</cell><cell>0.0452</cell><cell>0.0403</cell><cell>0.0343</cell><cell>0.0255</cell><cell>0.0216</cell><cell>0.0214</cell></row><row><cell>Solar Flare 2</cell><cell>2.9619</cell><cell>2.8532</cell><cell>2.7732</cell><cell>2.8282</cell><cell>2.8510</cell><cell>2.8273</cell><cell>2.8110</cell><cell>2.9518</cell><cell>2.9204</cell><cell>2.8713</cell></row><row><cell>Wisconsin Cancer</cell><cell>1.7666</cell><cell>1.7155</cell><cell>1.7156</cell><cell>1.7256</cell><cell>1.7119</cell><cell>1.7146</cell><cell>1.7195</cell><cell>1.8171</cell><cell>1.7915</cell><cell>1.7692</cell></row><row><cell>California Housing</cell><cell>0.8665</cell><cell>0.8221</cell><cell>0.9642</cell><cell>0.8673</cell><cell>1.0125</cell><cell>0.8952</cell><cell>0.7513</cell><cell>0.7477</cell><cell>0.6987</cell><cell>0.6726</cell></row><row><cell>Stock</cell><cell>0.0841</cell><cell>0.1039</cell><cell>0.0990</cell><cell>0.1008</cell><cell>0.0998</cell><cell>0.0987</cell><cell>0.0949</cell><cell>0.0578</cell><cell>0.0596</cell><cell>0.0554</cell></row><row><cell>SCPF</cell><cell>2.2244</cell><cell>2.3173</cell><cell>2.3661</cell><cell>2.3517</cell><cell>2.3923</cell><cell>2.3025</cell><cell>2.3295</cell><cell>2.2960</cell><cell>2.2510</cell><cell>2.3179</cell></row><row><cell>Puma8NH</cell><cell>1.9678</cell><cell>2.1133</cell><cell>2.0989</cell><cell>2.2024</cell><cell>2.1413</cell><cell>2.1473</cell><cell>2.1467</cell><cell>1.8242</cell><cell>1.8728</cell><cell>1.8299</cell></row><row><cell>Friedman</cell><cell>5.4573</cell><cell>5.3357</cell><cell>5.3478</cell><cell>5.3260</cell><cell>5.3482</cell><cell>5.3253</cell><cell>5.3210</cell><cell>5.3038</cell><cell>5.2942</cell><cell>5.2812</cell></row><row><cell>Puma32H</cell><cell>5.3419</cell><cell>4.9499</cell><cell>4.9627</cell><cell>5.0405</cell><cell>4.9905</cell><cell>4.9662</cell><cell>4.9805</cell><cell>5.2711</cell><cell>5.2749</cell><cell>5.1306</cell></row><row><cell>Water Quality</cell><cell>11.3143</cell><cell>11.5621</cell><cell>11.6276</cell><cell>11.5931</cell><cell>11.6495</cell><cell>11.6022</cell><cell>11.5004</cell><cell>12.2974</cell><cell>12.2042</cell><cell>12.0593</cell></row><row><cell>M5SPEC</cell><cell>1.0081</cell><cell>0.8754</cell><cell>1.0336</cell><cell>0.9421</cell><cell>0.8847</cell><cell>0.8824</cell><cell>0.8903</cell><cell>0.2578</cell><cell>0.2597</cell><cell>0.2575</cell></row><row><cell>MP5SPEC</cell><cell>1.1483</cell><cell>0.9817</cell><cell>1.1953</cell><cell>0.9970</cell><cell>0.9886</cell><cell>0.9880</cell><cell>0.9882</cell><cell>0.2261</cell><cell>0.1979</cell><cell>0.2136</cell></row><row><cell>MP6SPEC</cell><cell>1.1626</cell><cell>0.9928</cell><cell>1.1906</cell><cell>0.9992</cell><cell>1.0115</cell><cell>1.0045</cell><cell>0.9905</cell><cell>0.2926</cell><cell>0.2903</cell><cell>0.2954</cell></row><row><cell>ATP7d</cell><cell>1.7859</cell><cell>1.7348</cell><cell>1.6435</cell><cell>1.6460</cell><cell>1.8521</cell><cell>1.7888</cell><cell>1.6739</cell><cell>1.7820</cell><cell>1.7433</cell><cell>1.7098</cell></row><row><cell>OES97</cell><cell>4.6331</cell><cell>4.8340</cell><cell>4.8379</cell><cell>4.8082</cell><cell>4.8573</cell><cell>4.8591</cell><cell>4.8187</cell><cell>3.1440</cell><cell>3.0633</cell><cell>3.0499</cell></row><row><cell>Osales</cell><cell>7.3631</cell><cell>6.6850</cell><cell>5.8848</cell><cell>6.0850</cell><cell>7.8575</cell><cell>6.4746</cell><cell>5.9155</cell><cell>7.0727</cell><cell>7.3153</cell><cell>7.1374</cell></row><row><cell>ATP1d</cell><cell>1.0589</cell><cell>0.9056</cell><cell>0.9053</cell><cell>0.8982</cell><cell>0.9125</cell><cell>0.8783</cell><cell>0.9004</cell><cell>0.9091</cell><cell>0.8837</cell><cell>0.8922</cell></row><row><cell>OES10</cell><cell>3.6471</cell><cell>3.8931</cell><cell>3.8952</cell><cell>3.8909</cell><cell>3.9031</cell><cell>3.9063</cell><cell>3.8869</cell><cell>2.2623</cell><cell>2.1608</cell><cell>2.1320</cell></row><row><cell>Average</cell><cell>2.6546</cell><cell>2.6334</cell><cell>2.5872</cell><cell>2.5946</cell><cell>2.7127</cell><cell>2.6373</cell><cell>2.5747</cell><cell>2.3993</cell><cell>2.3664</cell><cell>2.3368</cell></row><row><cell>Ranks</cell><cell>6.5833</cell><cell>5.6667</cell><cell>6.0833</cell><cell>6.2500</cell><cell>7.8333</cell><cell>6.1250</cell><cell>5.1250</cell><cell>4.6667</cell><cell>3.6250</cell><cell>3.0417</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 7</head><label>7</label><figDesc>Wilcoxon, Nemenyi, and Holm tests for MSE.</figDesc><table><row><cell>SVRCC vs.</cell><cell>Wilcoxon R +</cell><cell>Wilcoxon R -</cell><cell>Wilcoxon p -value</cell><cell>Nemenyi p -value</cell><cell>Holm p -value</cell></row><row><cell>MORF</cell><cell>268.0</cell><cell>32.0</cell><cell>3 . 2 E -4</cell><cell>5 . 1 E -5</cell><cell>6 . 3 E -3</cell></row><row><cell>ST</cell><cell>241.0</cell><cell>59.0</cell><cell>7 . 9 E -3</cell><cell>2 . 7 E -3</cell><cell>1 . 3 E -2</cell></row><row><cell>MTS</cell><cell>224.0</cell><cell>76.0</cell><cell>3 . 4 E -2</cell><cell>5 . 0 E -4</cell><cell>1 . 0 E -2</cell></row><row><cell>MTSC</cell><cell>226.0</cell><cell>74.0</cell><cell>2 . 9 E -2</cell><cell>2 . 4 E -4</cell><cell>7 . 1 E -3</cell></row><row><cell>RC</cell><cell>263.0</cell><cell>37.0</cell><cell>6 . 5 E -4</cell><cell>0.0 0 0 0</cell><cell>5 . 6 E -3</cell></row><row><cell>ERC</cell><cell>234.0</cell><cell>66.0</cell><cell>1 . 5 E -2</cell><cell>4 . 2 E -4</cell><cell>8 . 3 E -3</cell></row><row><cell>ERCC</cell><cell>224.0</cell><cell>76.0</cell><cell></cell><cell></cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 8</head><label>8</label><figDesc>Average root mean square error (aRMSE) results &amp; algorithm rank.</figDesc><table><row><cell>Datasets</cell><cell>MORF</cell><cell>ST</cell><cell>MTS</cell><cell>MTSC</cell><cell>RC</cell><cell>ERC</cell><cell>ERCC</cell><cell>SVR</cell><cell>SVRRC</cell><cell>SVRCC</cell></row><row><cell>Slump</cell><cell>0.6711</cell><cell>0.6652</cell><cell>0.6456</cell><cell>0.6699</cell><cell>0.6787</cell><cell>0.6793</cell><cell>0.6649</cell><cell>0.5561</cell><cell>0.5345</cell><cell>0.5337</cell></row><row><cell>Polymer</cell><cell>0.5277</cell><cell>0.5409</cell><cell>0.5042</cell><cell>0.5336</cell><cell>0.5536</cell><cell>0.5803</cell><cell>0.5319</cell><cell>0.4403</cell><cell>0.4062</cell><cell>0.4060</cell></row><row><cell>Andro</cell><cell>0.4649</cell><cell>0.5420</cell><cell>0.4414</cell><cell>0.4871</cell><cell>0.5390</cell><cell>0.5317</cell><cell>0.5039</cell><cell>0.4326</cell><cell>0.4061</cell><cell>0.3989</cell></row><row><cell>EDM</cell><cell>0.6372</cell><cell>0.6715</cell><cell>0.6705</cell><cell>0.6729</cell><cell>0.6722</cell><cell>0.6704</cell><cell>0.6721</cell><cell>0.6449</cell><cell>0.6411</cell><cell>0.6366</cell></row><row><cell>Solar Flare 1</cell><cell>0.9777</cell><cell>0.9274</cell><cell>0.9271</cell><cell>0.9089</cell><cell>0.8921</cell><cell>0.9016</cell><cell>0.9121</cell><cell>0.8856</cell><cell>0.8844</cell><cell>0.8801</cell></row><row><cell>Jura</cell><cell>0.5800</cell><cell>0.5686</cell><cell>0.5720</cell><cell>0.5706</cell><cell>0.5726</cell><cell>0.5712</cell><cell>0.5693</cell><cell>0.5794</cell><cell>0.5687</cell><cell>0.5622</cell></row><row><cell>Enb</cell><cell>0.1212</cell><cell>0.1166</cell><cell>0.1237</cell><cell>0.1214</cell><cell>0.1272</cell><cell>0.1253</cell><cell>0.1140</cell><cell>0.0981</cell><cell>0.0914</cell><cell>0.0903</cell></row><row><cell>Solar Flare 2</cell><cell>0.8725</cell><cell>0.8420</cell><cell>0.8127</cell><cell>0.8305</cell><cell>0.8313</cell><cell>0.8300</cell><cell>0.8304</cell><cell>0.8418</cell><cell>0.8349</cell><cell>0.8345</cell></row><row><cell>Wisconsin Cancer</cell><cell>0.9290</cell><cell>0.9163</cell><cell>0.9158</cell><cell>0.9187</cell><cell>0.9153</cell><cell>0.9160</cell><cell>0.9173</cell><cell>0.9422</cell><cell>0.9362</cell><cell>0.9306</cell></row><row><cell>California Housing</cell><cell>0.6541</cell><cell>0.6366</cell><cell>0.6889</cell><cell>0.6530</cell><cell>0.7053</cell><cell>0.6632</cell><cell>0.6079</cell><cell>0.6038</cell><cell>0.5859</cell><cell>0.5755</cell></row><row><cell>Stock</cell><cell>0.1643</cell><cell>0.1830</cell><cell>0.1774</cell><cell>0.1790</cell><cell>0.1790</cell><cell>0.1777</cell><cell>0.1739</cell><cell>0.1357</cell><cell>0.1329</cell><cell>0.1308</cell></row><row><cell>SCPF</cell><cell>0.7113</cell><cell>0.7235</cell><cell>0.7342</cell><cell>0.7255</cell><cell>0.7285</cell><cell>0.7143</cell><cell>0.7227</cell><cell>0.7155</cell><cell>0.7081</cell><cell>0.7048</cell></row><row><cell>Puma8NH</cell><cell>0.7855</cell><cell>0.8139</cell><cell>0.8114</cell><cell>0.8307</cell><cell>0.8196</cell><cell>0.8202</cell><cell>0.8203</cell><cell>0.7650</cell><cell>0.7740</cell><cell>0.7671</cell></row><row><cell>Friedman</cell><cell>0.9382</cell><cell>0.9203</cell><cell>0.9219</cell><cell>0.9199</cell><cell>0.9219</cell><cell>0.9197</cell><cell>0.9193</cell><cell>0.9203</cell><cell>0.9195</cell><cell>0.9183</cell></row><row><cell>Puma32H</cell><cell>0.9395</cell><cell>0.8700</cell><cell>0.8713</cell><cell>0.8778</cell><cell>0.8739</cell><cell>0.8716</cell><cell>0.8727</cell><cell>0.9353</cell><cell>0.9356</cell><cell>0.9331</cell></row><row><cell>Water Quality</cell><cell>0.8921</cell><cell>0.9015</cell><cell>0.9041</cell><cell>0.9025</cell><cell>0.9051</cell><cell>0.9030</cell><cell>0.8990</cell><cell>0.9284</cell><cell>0.9293</cell><cell>0.9271</cell></row><row><cell>M5SPEC</cell><cell>0.5707</cell><cell>0.5324</cell><cell>0.5761</cell><cell>0.5515</cell><cell>0.5347</cell><cell>0.5339</cell><cell>0.5376</cell><cell>0.2745</cell><cell>0.2744</cell><cell>0.2740</cell></row><row><cell>MP5SPEC</cell><cell>0.5315</cell><cell>0.4914</cell><cell>0.5426</cell><cell>0.4947</cell><cell>0.4930</cell><cell>0.4928</cell><cell>0.4928</cell><cell>0.2337</cell><cell>0.2176</cell><cell>0.2177</cell></row><row><cell>MP6SPEC</cell><cell>0.5344</cell><cell>0.4939</cell><cell>0.5416</cell><cell>0.4943</cell><cell>0.4982</cell><cell>0.4967</cell><cell>0.4927</cell><cell>0.2627</cell><cell>0.2460</cell><cell>0.2497</cell></row><row><cell>ATP7d</cell><cell>0.5216</cell><cell>0.4956</cell><cell>0.4752</cell><cell>0.4765</cell><cell>0.5194</cell><cell>0.5024</cell><cell>0.4824</cell><cell>0.5141</cell><cell>0.5066</cell><cell>0.5018</cell></row><row><cell>OES97</cell><cell>0.4652</cell><cell>0.4634</cell><cell>0.4635</cell><cell>0.4622</cell><cell>0.4643</cell><cell>0.4644</cell><cell>0.4627</cell><cell>0.3794</cell><cell>0.3768</cell><cell>0.3749</cell></row><row><cell>Osales</cell><cell>0.7190</cell><cell>0.6912</cell><cell>0.6496</cell><cell>0.6615</cell><cell>0.7591</cell><cell>0.6772</cell><cell>0.6515</cell><cell>0.7212</cell><cell>0.7343</cell><cell>0.7121</cell></row><row><cell>ATP1d</cell><cell>0.4053</cell><cell>0.3608</cell><cell>0.3587</cell><cell>0.3591</cell><cell>0.3653</cell><cell>0.3562</cell><cell>0.3596</cell><cell>0.3693</cell><cell>0.3638</cell><cell>0.3507</cell></row><row><cell>OES10</cell><cell>0.3954</cell><cell>0.3896</cell><cell>0.3897</cell><cell>0.3892</cell><cell>0.3901</cell><cell>0.3903</cell><cell>0.3889</cell><cell>0.3085</cell><cell>0.3039</cell><cell>0.3038</cell></row><row><cell>Average</cell><cell>0.6254</cell><cell>0.6149</cell><cell>0.6133</cell><cell>0.6121</cell><cell>0.6225</cell><cell>0.6162</cell><cell>0.6083</cell><cell>0.5620</cell><cell>0.5547</cell><cell>0.5506</cell></row><row><cell>Ranks</cell><cell>7.3333</cell><cell>5.7708</cell><cell>5.8125</cell><cell>6.0625</cell><cell>7.6250</cell><cell>6.0208</cell><cell>4.8542</cell><cell>5.0625</cell><cell>3.9167</cell><cell>2.5417</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>Table 9</head><label>9</label><figDesc>Wilcoxon, Nemenyi, and Holm tests for aRMSE.</figDesc><table><row><cell>SVRCC vs.</cell><cell>Wilcoxon R +</cell><cell>Wilcoxon R -</cell><cell>Wilcoxon p -value</cell><cell>Nemenyi p -value</cell><cell>Holm p -value</cell></row><row><cell>MORF</cell><cell>286.0</cell><cell>14.0</cell><cell>1 . 3 E -5</cell><cell>0.0 0 0 0</cell><cell>6 . 3 E -3</cell></row><row><cell>ST</cell><cell>259.0</cell><cell>41.0</cell><cell>1 . 1 E -3</cell><cell>2 . 2 E -4</cell><cell>1 . 3 E -2</cell></row><row><cell>MTS</cell><cell>247.0</cell><cell>53.0</cell><cell>4 . 3 E -3</cell><cell>1 . 8 E -5</cell><cell>1 . 0 E -2</cell></row><row><cell>MTSC</cell><cell>251.0</cell><cell>49.0</cell><cell>2 . 8 E -3</cell><cell>5 . 6 E -5</cell><cell>7 . 1 E -3</cell></row><row><cell>RC</cell><cell>270.0</cell><cell>30.0</cell><cell>2 . 4 E -4</cell><cell>0.0 0 0 0</cell><cell>5 . 6 E -3</cell></row><row><cell>ERC</cell><cell>255.0</cell><cell>45.0</cell><cell>1 . 8 E -3</cell><cell>6 . 9 E -5</cell><cell>8 . 3 E -3</cell></row><row><cell>ERCC</cell><cell>246.0</cell><cell>54.0</cell><cell>4 . 8 E -3</cell><cell>8 . 2 E -3</cell><cell>2 . 5 E -2</cell></row><row><cell>SVR</cell><cell>296.0</cell><cell>4.00</cell><cell>8 . 3 E -7</cell><cell>3 . 9 E -3</cell><cell>1 . 7 E -2</cell></row><row><cell>SVRRC</cell><cell>284.0</cell><cell>16.0</cell><cell>2 . 0 E -5</cell><cell>1 . 2 E -1</cell><cell>5 . 0 E -2</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head>Table 10</head><label>10</label><figDesc>Average relative root mean square error (aRRMSE) results &amp; algorithm rank.</figDesc><table><row><cell>Datasets</cell><cell cols="2">MORF</cell><cell>ST</cell><cell>MTS</cell><cell>MTSC</cell><cell></cell><cell>RC</cell><cell>ERC</cell><cell></cell><cell>ERCC</cell><cell>SVR</cell><cell>SVRRC</cell><cell>SVRCC</cell></row><row><cell>Slump</cell><cell cols="2">0.6939</cell><cell>0.6886</cell><cell>0.6690</cell><cell cols="2">0.6938</cell><cell>0.7019</cell><cell cols="2">0.7022</cell><cell>0.6886</cell><cell>0.5765</cell><cell>0.5545</cell><cell>0.5560</cell></row><row><cell>Polymer</cell><cell cols="2">0.6159</cell><cell>0.5971</cell><cell>0.5778</cell><cell cols="2">0.6493</cell><cell>0.6270</cell><cell cols="2">0.6544</cell><cell>0.6131</cell><cell>0.5573</cell><cell>0.5253</cell><cell>0.5116</cell></row><row><cell>Andro</cell><cell cols="2">0.5097</cell><cell>0.5979</cell><cell>0.5155</cell><cell cols="2">0.5633</cell><cell>0.5924</cell><cell cols="2">0.5885</cell><cell>0.5666</cell><cell>0.4856</cell><cell>0.4651</cell><cell>0.4455</cell></row><row><cell>EDM</cell><cell cols="2">0.7337</cell><cell>0.7442</cell><cell>0.7413</cell><cell cols="2">0.7446</cell><cell>0.7449</cell><cell cols="2">0.7452</cell><cell>0.7443</cell><cell>0.7058</cell><cell>0.7070</cell><cell>0.6978</cell></row><row><cell>Solar Flare 1</cell><cell cols="2">1.3046</cell><cell>1.1357</cell><cell>1.1168</cell><cell cols="2">1.0758</cell><cell>0.9951</cell><cell cols="2">1.0457</cell><cell>1.0887</cell><cell>0.9917</cell><cell>0.9455</cell><cell>0.9320</cell></row><row><cell>Jura</cell><cell cols="2">0.5969</cell><cell>0.5874</cell><cell>0.5906</cell><cell cols="2">0.5892</cell><cell>0.5910</cell><cell cols="2">0.5896</cell><cell>0.5880</cell><cell>0.5952</cell><cell>0.5764</cell><cell>0.5885</cell></row><row><cell>Enb</cell><cell cols="2">0.1210</cell><cell>0.1165</cell><cell>0.1231</cell><cell cols="2">0.1211</cell><cell>0.1268</cell><cell cols="2">0.1250</cell><cell>0.1139</cell><cell>0.0977</cell><cell>0.0910</cell><cell>0.0899</cell></row><row><cell>Solar Flare 2</cell><cell cols="2">1.4167</cell><cell>1.1503</cell><cell>0.9483</cell><cell cols="2">1.0840</cell><cell>1.0092</cell><cell cols="2">1.0522</cell><cell>1.0928</cell><cell>1.0385</cell><cell>1.0253</cell><cell>1.0298</cell></row><row><cell>Wisconsin Cancer</cell><cell cols="2">0.9413</cell><cell>0.9314</cell><cell>0.9308</cell><cell cols="2">0.9336</cell><cell>0.9305</cell><cell cols="2">0.9313</cell><cell>0.9323</cell><cell>0.9555</cell><cell>0.9483</cell><cell>0.9427</cell></row><row><cell>California Housing</cell><cell cols="2">0.6611</cell><cell>0.6447</cell><cell>0.6974</cell><cell cols="2">0.6630</cell><cell>0.7131</cell><cell cols="2">0.6690</cell><cell>0.6146</cell><cell>0.6130</cell><cell>0.5945</cell><cell>0.5852</cell></row><row><cell>Stock</cell><cell cols="2">0.1653</cell><cell>0.1844</cell><cell>0.1787</cell><cell cols="2">0.1803</cell><cell>0.1802</cell><cell cols="2">0.1789</cell><cell>0.1752</cell><cell>0.1364</cell><cell>0.1337</cell><cell>0.1388</cell></row><row><cell>SCPF</cell><cell cols="2">0.8273</cell><cell>0.8348</cell><cell>0.8436</cell><cell cols="2">0.8308</cell><cell>0.8263</cell><cell cols="2">0.8105</cell><cell>0.8290</cell><cell>0.8164</cell><cell>0.8037</cell><cell>0.8013</cell></row><row><cell>Puma8NH</cell><cell cols="2">0.7858</cell><cell>0.8142</cell><cell>0.8118</cell><cell cols="2">0.8311</cell><cell>0.8199</cell><cell cols="2">0.8205</cell><cell>0.8207</cell><cell>0.7655</cell><cell>0.7744</cell><cell>0.7676</cell></row><row><cell>Friedman</cell><cell cols="2">0.9394</cell><cell>0.9214</cell><cell>0.9231</cell><cell cols="2">0.9210</cell><cell>0.9231</cell><cell cols="2">0.9209</cell><cell>0.9204</cell><cell>0.9218</cell><cell>0.9208</cell><cell>0.9196</cell></row><row><cell>Puma32H</cell><cell cols="2">0.9406</cell><cell>0.8713</cell><cell>0.8727</cell><cell cols="2">0.8791</cell><cell>0.8752</cell><cell cols="2">0.8729</cell><cell>0.8740</cell><cell>0.9364</cell><cell>0.9367</cell><cell>0.9319</cell></row><row><cell>Water Quality</cell><cell cols="2">0.8994</cell><cell>0.9085</cell><cell>0.9109</cell><cell cols="2">0.9093</cell><cell>0.9121</cell><cell cols="2">0.9097</cell><cell>0.9057</cell><cell>0.9343</cell><cell>0.9310</cell><cell>0.9045</cell></row><row><cell>M5SPEC</cell><cell cols="2">0.5910</cell><cell>0.5523</cell><cell>0.5974</cell><cell cols="2">0.5671</cell><cell>0.5552</cell><cell cols="2">0.5542</cell><cell>0.5558</cell><cell>0.2951</cell><cell>0.2935</cell><cell>0.2925</cell></row><row><cell>MP5SPEC</cell><cell cols="2">0.5522</cell><cell>0.5120</cell><cell>0.5683</cell><cell cols="2">0.5133</cell><cell>0.5145</cell><cell cols="2">0.5143</cell><cell>0.5119</cell><cell>0.2484</cell><cell>0.2323</cell><cell>0.2358</cell></row><row><cell>MP6SPEC</cell><cell cols="2">0.5553</cell><cell>0.5152</cell><cell>0.5686</cell><cell cols="2">0.5119</cell><cell>0.5198</cell><cell cols="2">0.5187</cell><cell>0.5109</cell><cell>0.2850</cell><cell>0.2669</cell><cell>0.2623</cell></row><row><cell>ATP7d</cell><cell cols="2">0.5563</cell><cell>0.5308</cell><cell>0.5141</cell><cell cols="2">0.5142</cell><cell>0.5558</cell><cell cols="2">0.5397</cell><cell>0.5182</cell><cell>0.5455</cell><cell>0.5371</cell><cell>0.5342</cell></row><row><cell>OES97</cell><cell cols="2">0.5490</cell><cell>0.5230</cell><cell>0.5229</cell><cell cols="2">0.5217</cell><cell>0.5239</cell><cell cols="2">0.5237</cell><cell>0.5222</cell><cell>0.4641</cell><cell>0.4618</cell><cell>0.4635</cell></row><row><cell>Osales</cell><cell cols="2">0.7596</cell><cell>0.7471</cell><cell>0.7086</cell><cell cols="2">0.7268</cell><cell>0.8318</cell><cell cols="2">0.7258</cell><cell>0.7101</cell><cell>0.7924</cell><cell>0.7924</cell><cell>0.7811</cell></row><row><cell>ATP1d</cell><cell cols="2">0.4173</cell><cell>0.3732</cell><cell>0.3733</cell><cell cols="2">0.3712</cell><cell>0.3790</cell><cell cols="2">0.3696</cell><cell>0.3721</cell><cell>0.3773</cell><cell>0.3707</cell><cell>0.3775</cell></row><row><cell>OES10</cell><cell cols="2">0.4518</cell><cell>0.4174</cell><cell>0.4176</cell><cell cols="2">0.4171</cell><cell>0.4178</cell><cell cols="2">0.4180</cell><cell>0.4166</cell><cell>0.3570</cell><cell>0.3555</cell><cell>0.3538</cell></row><row><cell>Average</cell><cell cols="2">0.6910</cell><cell>0.6625</cell><cell>0.6551</cell><cell cols="2">0.6589</cell><cell>0.6611</cell><cell cols="2">0.6575</cell><cell>0.6536</cell><cell>0.6039</cell><cell>0.5935</cell><cell>0.5893</cell></row><row><cell>Ranks</cell><cell cols="2">7.50 0 0</cell><cell>5.7708</cell><cell>5.9375</cell><cell cols="2">6.1667</cell><cell>7.4375</cell><cell cols="2">6.3750</cell><cell>4.9792</cell><cell>4.7708</cell><cell>3.2708</cell><cell>2.7917</cell></row><row><cell>Table 11</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="6">Wilcoxon, Nemenyi, and Holm tests for aRRMSE.</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="2">SVRCC vs.</cell><cell cols="2">Wilcoxon R +</cell><cell cols="2">Wilcoxon R -</cell><cell cols="3">Wilcoxon p -value</cell><cell cols="3">Nemenyi p -value</cell><cell>Holm p -value</cell></row><row><cell>MORF</cell><cell></cell><cell>290.0</cell><cell></cell><cell>10.0</cell><cell></cell><cell cols="2">5 . 1 E -6</cell><cell></cell><cell cols="2">0.0 0 0 0</cell><cell>5 . 6 E -3</cell></row><row><cell>ST</cell><cell></cell><cell>261.0</cell><cell></cell><cell>39.0</cell><cell></cell><cell cols="2">8 . 5 E -4</cell><cell></cell><cell cols="2">6 . 5 E -4</cell><cell>1 . 3 E -2</cell></row><row><cell>MTS</cell><cell></cell><cell>239.0</cell><cell></cell><cell>61.0</cell><cell></cell><cell cols="2">9 . 6 E -3</cell><cell></cell><cell cols="2">3 . 2 E -3</cell><cell>1 . 0 E -2</cell></row><row><cell>MTSC</cell><cell></cell><cell>261.0</cell><cell></cell><cell>39.0</cell><cell></cell><cell cols="2">8 . 5 E -4</cell><cell></cell><cell cols="2">1 . 1 E -3</cell><cell>8 . 3 E -3</cell></row><row><cell>RC</cell><cell></cell><cell>275.0</cell><cell></cell><cell>25.0</cell><cell></cell><cell cols="2">1 . 1 E -4</cell><cell></cell><cell cols="2">0.0 0 0 0</cell><cell>6 . 3 E -3</cell></row><row><cell>ERC</cell><cell></cell><cell>261.0</cell><cell></cell><cell>39.0</cell><cell></cell><cell cols="2">8 . 5 E -4</cell><cell></cell><cell cols="2">4 . 1 E -5</cell><cell>7 . 1 E -3</cell></row><row><cell>ERCC</cell><cell></cell><cell>254.0</cell><cell></cell><cell>46.0</cell><cell></cell><cell cols="2">2 . 0 E -3</cell><cell></cell><cell cols="2">1 . 2 E -2</cell><cell>1 . 7 E -2</cell></row><row><cell>SVR</cell><cell></cell><cell>291.0</cell><cell></cell><cell>9.00</cell><cell></cell><cell cols="2">3 . 9 E -6</cell><cell></cell><cell cols="2">2 . 4 E -2</cell><cell>2 . 5 E -2</cell></row><row><cell>SVRRC</cell><cell></cell><cell>222.5</cell><cell></cell><cell>77.5</cell><cell></cell><cell cols="2">3 . 8 E -2</cell><cell></cell><cell cols="2">5 . 8 E -1</cell><cell>5 . 0 E -2</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_10"><head>Table 12</head><label>12</label><figDesc>Run time results (s) &amp; algorithm rank.</figDesc><table><row><cell>Datasets</cell><cell>MORF</cell><cell>ST</cell><cell>MTS</cell><cell>MTSC</cell><cell>RC</cell><cell>ERC</cell><cell>ERCC</cell><cell>SVR</cell><cell>SVRRC</cell><cell>SVRCC</cell></row><row><cell>Slump</cell><cell>38.1</cell><cell>2.6</cell><cell>9.9</cell><cell>15.9</cell><cell>1.8</cell><cell>11.1</cell><cell>50.5</cell><cell>0.6</cell><cell>1.9</cell><cell>0.7</cell></row><row><cell>Polymer</cell><cell>7.6</cell><cell>2.7</cell><cell>9.1</cell><cell>15.5</cell><cell>1.9</cell><cell>14.9</cell><cell>80.5</cell><cell>0.5</cell><cell>2.6</cell><cell>0.5</cell></row><row><cell>Andro</cell><cell>25.7</cell><cell>4.4</cell><cell>15.0</cell><cell>34.2</cell><cell>3.4</cell><cell>33.2</cell><cell>197.9</cell><cell>1.1</cell><cell>6.2</cell><cell>1.1</cell></row><row><cell>EDM</cell><cell>24.8</cell><cell>2.8</cell><cell>9.4</cell><cell>18.1</cell><cell>2.1</cell><cell>5.8</cell><cell>19.0</cell><cell>0.9</cell><cell>1.0</cell><cell>0.9</cell></row><row><cell>Solar Flare 1</cell><cell>34.1</cell><cell>3.5</cell><cell>13.6</cell><cell>26.7</cell><cell>2.7</cell><cell>17.7</cell><cell>86.9</cell><cell>2.3</cell><cell>9.3</cell><cell>2.6</cell></row><row><cell>Jura</cell><cell>64.3</cell><cell>7.9</cell><cell>31.8</cell><cell>74.3</cell><cell>6.4</cell><cell>43.5</cell><cell>254.2</cell><cell>4.7</cell><cell>18.7</cell><cell>5.3</cell></row><row><cell>Enb</cell><cell>71.4</cell><cell>6.6</cell><cell>26.1</cell><cell>63.6</cell><cell>5.4</cell><cell>15.6</cell><cell>69.6</cell><cell>11.3</cell><cell>17.7</cell><cell>15.9</cell></row><row><cell>Solar Flare 2</cell><cell>55.4</cell><cell>7.4</cell><cell>30.7</cell><cell>68.0</cell><cell>6.3</cell><cell>42.9</cell><cell>241.5</cell><cell>9.4</cell><cell>53.5</cell><cell>15.6</cell></row><row><cell>Wisconsin Cancer</cell><cell>51.4</cell><cell>6.1</cell><cell>21.9</cell><cell>53.7</cell><cell>4.9</cell><cell>14.8</cell><cell>61.6</cell><cell>2.0</cell><cell>2.4</cell><cell>2.0</cell></row><row><cell>California Housing</cell><cell>93.0</cell><cell>9.7</cell><cell>34.8</cell><cell>75.9</cell><cell>8.2</cell><cell>21.3</cell><cell>102.0</cell><cell>15.8</cell><cell>25.2</cell><cell>23.6</cell></row><row><cell>Stock</cell><cell>93.7</cell><cell>11.7</cell><cell>46.8</cell><cell>96.7</cell><cell>11.0</cell><cell>75.4</cell><cell>427.3</cell><cell>18.5</cell><cell>90.5</cell><cell>26.3</cell></row><row><cell>SCPF</cell><cell>66.3</cell><cell>19.3</cell><cell>65.9</cell><cell>176.3</cell><cell>15.0</cell><cell>104.2</cell><cell>734.2</cell><cell>32.8</cell><cell>162.8</cell><cell>48.8</cell></row><row><cell>Puma8NH</cell><cell>130.4</cell><cell>29.7</cell><cell>106.7</cell><cell>288.6</cell><cell>27.9</cell><cell>201.6</cell><cell>1227.7</cell><cell>94.1</cell><cell>516.6</cell><cell>177.1</cell></row><row><cell>Friedman</cell><cell>79.5</cell><cell>27.0</cell><cell>81.2</cell><cell>258.3</cell><cell>25.0</cell><cell>273.7</cell><cell>2871.6</cell><cell>12.3</cell><cell>322.3</cell><cell>18.8</cell></row><row><cell>Puma32H</cell><cell>93.9</cell><cell>68.1</cell><cell>181.0</cell><cell>635.0</cell><cell>87.7</cell><cell>667.9</cell><cell>6087.0</cell><cell>32.2</cell><cell>1018.7</cell><cell>53.1</cell></row><row><cell>Water Quality</cell><cell>108.4</cell><cell>93.1</cell><cell>262.1</cell><cell>912.3</cell><cell>127.2</cell><cell>925.4</cell><cell>10993.3</cell><cell>110.2</cell><cell>2567.9</cell><cell>189.5</cell></row><row><cell>M5SPEC</cell><cell>89.8</cell><cell>68.9</cell><cell>166.3</cell><cell>604.6</cell><cell>73.7</cell><cell>262.3</cell><cell>3132.1</cell><cell>39.2</cell><cell>546.7</cell><cell>45.1</cell></row><row><cell>MP5SPEC</cell><cell>84.5</cell><cell>94.6</cell><cell>221.2</cell><cell>888.3</cell><cell>91.5</cell><cell>557.0</cell><cell>6864.1</cell><cell>49.3</cell><cell>1132.1</cell><cell>58.4</cell></row><row><cell>MP6SPEC</cell><cell>90.3</cell><cell>93.4</cell><cell>212.6</cell><cell>871.0</cell><cell>89.1</cell><cell>557.6</cell><cell>6761.3</cell><cell>47.2</cell><cell>1227.1</cell><cell>58.5</cell></row><row><cell>ATP7d</cell><cell>70.5</cell><cell>262.6</cell><cell>452.1</cell><cell>2319.8</cell><cell>242.1</cell><cell>1779.2</cell><cell>24373.8</cell><cell>80.0</cell><cell>1897.4</cell><cell>136.5</cell></row><row><cell>OES97</cell><cell>83.4</cell><cell>485.3</cell><cell>1146.6</cell><cell>4928.9</cell><cell>499.8</cell><cell>5315.0</cell><cell>58072.1</cell><cell>148.2</cell><cell>3759.1</cell><cell>342.6</cell></row><row><cell>Osales</cell><cell>92.0</cell><cell>1094.8</cell><cell>2340.7</cell><cell>8322.2</cell><cell>986.5</cell><cell>11361.2</cell><cell>122265.3</cell><cell>437.0</cell><cell>4830.1</cell><cell>843.6</cell></row><row><cell>ATP1d</cell><cell>70.7</cell><cell>272.9</cell><cell>476.5</cell><cell>2568.9</cell><cell>261.9</cell><cell>2138.9</cell><cell>26768.9</cell><cell>95.0</cell><cell>2127.8</cell><cell>174.4</cell></row><row><cell>OES10</cell><cell>90.0</cell><cell>738.9</cell><cell>1633.6</cell><cell>6682.9</cell><cell>688.5</cell><cell>7150.8</cell><cell>83533.1</cell><cell>229.1</cell><cell>5419.4</cell><cell>577.1</cell></row><row><cell>Average</cell><cell>71.2</cell><cell>142.2</cell><cell>316.5</cell><cell>1250.0</cell><cell>136.2</cell><cell>1316.3</cell><cell>14803.2</cell><cell>61.4</cell><cell>1073.2</cell><cell>117.4</cell></row><row><cell>Ranks</cell><cell>5.5</cell><cell>3.71</cell><cell>6.0</cell><cell>8.29</cell><cell>3.0</cell><cell>7.08</cell><cell>9.92</cell><cell>1.88</cell><cell>6.71</cell><cell>2.92</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_11"><head>Table 13</head><label>13</label><figDesc>Wilcoxon, Nemenyi, and Holm tests for run time.</figDesc><table><row><cell>SVR vs.</cell><cell>Wilcoxon R +</cell><cell>Wilcoxon R -</cell><cell>Wilcoxon p -value</cell><cell>Nemenyi p -value</cell><cell>Holm p -value</cell></row><row><cell>SVRCC</cell><cell>295.0</cell><cell>5.00</cell><cell>1 . 2 E -6</cell><cell>2 . 3 E -1</cell><cell>5 . 0 E -2</cell></row><row><cell>MORF</cell><cell>225.0</cell><cell>75.0</cell><cell>3 . 2 E -2</cell><cell>3 . 4 E -5</cell><cell>1 . 3 E -2</cell></row><row><cell>ST</cell><cell>221.5</cell><cell>78.5</cell><cell>4 . 1 E -2</cell><cell>3 . 6 E -2</cell><cell>1 . 7 E -2</cell></row><row><cell>MTS</cell><cell>300.0</cell><cell>0.00</cell><cell>1 . 2 E -7</cell><cell>2 . 0 E -6</cell><cell>1 . 0 E -2</cell></row><row><cell>MTSC</cell><cell>300.0</cell><cell>0.00</cell><cell>1 . 2 E -7</cell><cell>0.0 0 0 0</cell><cell>6 . 3 E -3</cell></row><row><cell>RC</cell><cell>229.0</cell><cell>71.0</cell><cell>2 . 3 E -2</cell><cell>2 . 0 E -1</cell><cell>2 . 5 E -2</cell></row><row><cell>ERC</cell><cell>300.0</cell><cell>0.00</cell><cell>1 . 2 E -7</cell><cell>0.0 0 0 0</cell><cell>7 . 1 E -3</cell></row><row><cell>ERCC</cell><cell>300.0</cell><cell>0.00</cell><cell>1 . 2 E -7</cell><cell>0.0 0 0 0</cell><cell>5 . 6 E -3</cell></row><row><cell>SVRRC</cell><cell>300.0</cell><cell>0.00</cell><cell>1 . 2 E -7</cell><cell>0.0 0 0 0</cell><cell>8 . 3 E -3</cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0"><p>http://mulan.sourceforge.net .</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_1"><p>http://www.cs.waikato.ac.nz/ml/weka .</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3" xml:id="foot_2"><p>http://people.vcu.edu/ ∼ acano/MTR-SVRCC</p></note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgments</head><p>This research was supported by the Spanish Ministry of Economy and Competitiveness , project TIN2014-55252-P , and by FEDER funds.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Multi-target regression with rule ensembles</title>
		<author>
			<persName><forename type="first">T</forename><surname>Aho</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Zenko</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Dzeroski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Elomaa</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Mach. Learn. Res</title>
		<imprint>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="page" from="2267" to="2407" />
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Stepwise induction of multi-target model trees</title>
		<author>
			<persName><forename type="first">A</forename><surname>Appice</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Dzeroski</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference of Machine Learning</title>
		<title level="s">Lecture Notes on Artificial Intelligence</title>
		<imprint>
			<date type="published" when="2007">2007</date>
			<biblScope unit="volume">4701</biblScope>
			<biblScope unit="page" from="502" to="509" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">A Bayesian/information theoretic model of learning to learn via multiple task sampling</title>
		<author>
			<persName><forename type="first">J</forename><surname>Baxter</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Mach. Learn</title>
		<imprint>
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="page" from="7" to="39" />
			<date type="published" when="1997">1997</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Exploiting task relatedness for multiple task learning</title>
		<author>
			<persName><forename type="first">S</forename><surname>Ben-David</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Schuller</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Sixteenth Annual Conference on Learning Theory</title>
		<meeting>the Sixteenth Annual Conference on Learning Theory</meeting>
		<imprint>
			<date type="published" when="2003">2003</date>
			<biblScope unit="page" from="567" to="580" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">A survey on multi-output regression</title>
		<author>
			<persName><forename type="first">H</forename><surname>Borchani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Varando</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Bielza</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Larrañaga</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">WIREs Data Min. Knowl. Discov</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page" from="216" to="233" />
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<author>
			<persName><forename type="first">S</forename><surname>Boyd</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Vandenberghe</surname></persName>
		</author>
		<title level="m">Convex Optimization</title>
		<imprint>
			<publisher>Cambridge University Press</publisher>
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Bagging predictors</title>
		<author>
			<persName><forename type="first">L</forename><surname>Breiman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Mach. Learn</title>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="page" from="123" to="140" />
			<date type="published" when="1996">1996</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">LAIM discretization for multi-label data</title>
		<author>
			<persName><forename type="first">A</forename><surname>Cano</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Luna</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Gibaja</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Ventura</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Inf. Sci</title>
		<imprint>
			<biblScope unit="volume">330</biblScope>
			<biblScope unit="page" from="370" to="384" />
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Multitask learning</title>
		<author>
			<persName><forename type="first">R</forename><surname>Caruana</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Mach. Learn</title>
		<imprint>
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="page" from="41" to="75" />
			<date type="published" when="1997">1997</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">LIBSVM: a library for support vector machines</title>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">C</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">J</forename><surname>Lin</surname></persName>
		</author>
		<ptr target="http://www.csie.ntu.edu.tw/∼cjlin/libsvm" />
	</analytic>
	<monogr>
		<title level="j">ACM Trans. Intell. Syst. Technol</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="1" to="27" />
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
	<note>27 . Software available at</note>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">LI-MLC: a label inference methodology for addressing high dimensionality in the label space for multilabel classification</title>
		<author>
			<persName><forename type="first">F</forename><surname>Charte</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Rivera</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">D</forename><surname>Jesus</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Herrera</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Neural Netw. Learn Syst</title>
		<imprint>
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="page" from="1842" to="1854" />
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Set-valued samples based support vector regression and its applications</title>
		<author>
			<persName><forename type="first">J</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Pedrycz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Ha</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Ma</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Expert Syst. Appl</title>
		<imprint>
			<biblScope unit="volume">42</biblScope>
			<biblScope unit="page" from="2502" to="2509" />
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<author>
			<persName><forename type="first">C</forename><surname>Cortes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Vapnik</surname></persName>
		</author>
		<title level="m">The Nature of Statistical Learning Theory</title>
		<meeting><address><addrLine>New York</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="1995">1995</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">A practical tutorial on the use of nonparametric statistical tests as a methodology for comparing evolutionary and swarm intelligence algorithms</title>
		<author>
			<persName><forename type="first">J</forename><surname>Derrac</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>García</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Molina</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Herrera</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Swarm Evol. Comput</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="3" to="18" />
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Global nonlinear kernel prediction for large data set with a particle swarm-optimized interval support vector regression</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Pedrycz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Hao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Neural Netw. Learn. Syst</title>
		<imprint>
			<biblScope unit="volume">26</biblScope>
			<biblScope unit="page" from="2521" to="2534" />
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<author>
			<persName><forename type="first">H</forename><surname>Drucker</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Burges</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Kaufman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Smola</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Vapnik</surname></persName>
		</author>
		<title level="m">Proceedings of the Advances in Neural Information Processing Systems</title>
		<meeting>the Advances in Neural Information Processing Systems</meeting>
		<imprint>
			<date type="published" when="1997">1997</date>
			<biblScope unit="page" from="155" to="161" />
		</imprint>
	</monogr>
	<note>Support vector regression machines</note>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Multiple comparisons among means</title>
		<author>
			<persName><forename type="first">O</forename><surname>Dunn</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Am. Stat. Assoc</title>
		<imprint>
			<biblScope unit="volume">56</biblScope>
			<biblScope unit="page" from="52" to="64" />
			<date type="published" when="1961">1961</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Regularization paths for generalized linear models via coordinate descent</title>
		<author>
			<persName><forename type="first">J</forename><surname>Friedman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Hastie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Tibshirani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Stat. Softw</title>
		<imprint>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page" from="1" to="22" />
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Advanced nonparametric tests for multiple comparisons in the design of experiments in computational intelligence and data mining: experimental analysis of power</title>
		<author>
			<persName><forename type="first">S</forename><surname>García</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Fernández</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Luengo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Herrera</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Inf. Sci</title>
		<imprint>
			<biblScope unit="volume">180</biblScope>
			<biblScope unit="page" from="2044" to="2064" />
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">An extension on statistical comparisons of classifiers over multiple data sets for all pairwise comparisons</title>
		<author>
			<persName><forename type="first">S</forename><surname>García</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Herrera</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Mach. Learn. Res</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="page" from="2677" to="2694" />
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">A study on the use of non-parametric tests for analyzing the evolutionary algorithms&apos; behaviour -a case study on the CEC2005 special session on real parameter optimization</title>
		<author>
			<persName><forename type="first">S</forename><surname>García</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Molina</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Lozano</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Herrera</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Heuristics</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="page" from="617" to="644" />
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">A novel boosted-neural network ensemble for modeling multi-target regression problems</title>
		<author>
			<persName><forename type="first">E</forename><surname>Hadavandi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Shahrabi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Shamishirband</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Eng. Appl. Artif. Intell</title>
		<imprint>
			<biblScope unit="volume">45</biblScope>
			<biblScope unit="page" from="204" to="219" />
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">SPMoe: a novel subspace-projected mixture of experts model for multi-target regression problems</title>
		<author>
			<persName><forename type="first">E</forename><surname>Hadavandi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Shahrabi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Hayashi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Soft Comput</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="page" from="2047" to="2065" />
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Multi-instance multi-label learning based on gaussian process with application to visual mobile robot navigation</title>
		<author>
			<persName><forename type="first">J</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Inf. Sci</title>
		<imprint>
			<biblScope unit="volume">190</biblScope>
			<biblScope unit="page" from="162" to="177" />
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Multi-domain spoken language understanding with transfer learning</title>
		<author>
			<persName><forename type="first">M</forename><surname>Jeong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Speech Commun</title>
		<imprint>
			<biblScope unit="volume">51</biblScope>
			<biblScope unit="page" from="412" to="424" />
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<author>
			<persName><forename type="first">V</forename><surname>Kecman</surname></persName>
		</author>
		<title level="m">Learning and Soft Computing: Support Vector Machines, Neural Networks, and Fuzzy Logic Models</title>
		<imprint>
			<publisher>MIT Press</publisher>
			<date type="published" when="2001">2001</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Ensembles of Extremely Randomized Trees for Multi-target Regression</title>
		<author>
			<persName><forename type="first">D</forename><surname>Kocev</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Ceci</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="s">Lecture Notes in Computer Science</title>
		<imprint>
			<biblScope unit="volume">9356</biblScope>
			<biblScope unit="page" from="86" to="100" />
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Using single-and multi-target regression trees and ensembles to model a compound index of vegetation condition</title>
		<author>
			<persName><forename type="first">D</forename><surname>Kocev</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Dzeroski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>White</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Newell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Griffioen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Ecol. Modell</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="page" from="1159" to="1168" />
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<title level="m" type="main">Ensembles of Multi-Objective Decision Trees</title>
		<author>
			<persName><forename type="first">D</forename><surname>Kocev</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Vens</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Struyf</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Dzeroski</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2007">2007</date>
			<publisher>Springer</publisher>
			<biblScope unit="page" from="86" to="100" />
			<pubPlace>Heidelberg</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Tree ensembles for predicting structured outputs</title>
		<author>
			<persName><forename type="first">D</forename><surname>Kocev</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Vens</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Struyf</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Dzeroski</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pattern Recognit</title>
		<imprint>
			<biblScope unit="volume">43</biblScope>
			<biblScope unit="page" from="817" to="833" />
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Memetic feature selection algorithm for multi-label classification</title>
		<author>
			<persName><forename type="first">J</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D.-W</forename><surname>Kim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Inf. Sci</title>
		<imprint>
			<biblScope unit="volume">293</biblScope>
			<biblScope unit="page" from="80" to="96" />
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">A novel attribute reduction approach for multi-label data based on rough set theory</title>
		<author>
			<persName><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Inf. Sci</title>
		<imprint>
			<biblScope unit="volume">367</biblScope>
			<biblScope unit="issue">368</biblScope>
			<biblScope unit="page" from="827" to="847" />
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
		<author>
			<persName><forename type="first">M</forename><surname>Lichman</surname></persName>
		</author>
		<ptr target="http://archive.ics.uci.edu/ml" />
		<title level="m">UCI Machine Learning repository</title>
		<imprint>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Multi-task learning for cross-platform sirna efficacy prediction: an in-silico study</title>
		<author>
			<persName><forename type="first">Q</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Xue</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">BMC Bioinf</title>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="page" from="181" to="196" />
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<monogr>
		<title level="m" type="main">Speeding up online training of l1 support vector machines</title>
		<author>
			<persName><forename type="first">G</forename><surname>Melki</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Kecman</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016">2016</date>
			<publisher>IEEE SoutheastConf</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Multi-dimensional function approximation and regression estimation, Artif</title>
		<author>
			<persName><forename type="first">F</forename><surname>Pérez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Camps</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Soria</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Pérez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Figueiras</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Artés</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural Netw</title>
		<imprint>
			<biblScope unit="page" from="757" to="762" />
			<date type="published" when="2002">2002</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">A reconstruction error based framework for multi-label and multi-view learning</title>
		<author>
			<persName><forename type="first">B</forename><surname>Qian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Ye</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Davidson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Knowl. Data Eng</title>
		<imprint>
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="page" from="594" to="607" />
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Multi-dimensional classification with super-classes</title>
		<author>
			<persName><forename type="first">J</forename><surname>Read</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Bielza</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Larranaga</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Knowl. Data Eng</title>
		<imprint>
			<biblScope unit="volume">26</biblScope>
			<biblScope unit="page" from="1720" to="1733" />
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<monogr>
		<title level="m" type="main">Multi-Label Classification Methods for Multi-Target Regression</title>
		<author>
			<persName><forename type="first">E</forename><surname>Spyromitros-Xioufis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Tsoumakas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Groves</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Vlahavas</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014">2014</date>
			<publisher>Cornell University Library</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Multi-target regression via input space expansion: treating targets as inputs</title>
		<author>
			<persName><forename type="first">E</forename><surname>Spyromitros-Xioufis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Tsoumakas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Groves</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Vlahavas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Mach. Learn</title>
		<imprint>
			<biblScope unit="volume">104</biblScope>
			<biblScope unit="page" from="55" to="98" />
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Mulan: a java library for multi-label learning</title>
		<author>
			<persName><forename type="first">G</forename><surname>Tsoumakas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Spyromitros-Xioufis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Vilcek</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Vlahavas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Mach. Learn. Res</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="page" from="2411" to="2414" />
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Multi-target regression via random linear target combinations</title>
		<author>
			<persName><forename type="first">G</forename><surname>Tsoumakas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Spyromitros-Xioufis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Vrekou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Vlahavas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Mach. Learn. Knowl. Discov. Databases</title>
		<imprint>
			<biblScope unit="volume">8726</biblScope>
			<biblScope unit="page" from="225" to="240" />
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Individual comparisons by ranking methods</title>
		<author>
			<persName><forename type="first">F</forename><surname>Wilcoxon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Biometr. Bull</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="80" to="83" />
			<date type="published" when="1945">1945</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<monogr>
		<author>
			<persName><forename type="first">I</forename><surname>Witten</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Frank</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Hall</surname></persName>
		</author>
		<title level="m">Data Mining: Practical Machine Learning Tools and Techniques</title>
		<imprint>
			<publisher>Morgan Kaufmann</publisher>
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
	<note>third ed.</note>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">ML-TREE: a tree-structure-based approach to multilabel learning</title>
		<author>
			<persName><forename type="first">Q</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Ye</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Chow</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S.-S</forename><surname>Ho</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Neural Netw. Learn. Syst</title>
		<imprint>
			<biblScope unit="volume">26</biblScope>
			<biblScope unit="page" from="430" to="443" />
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Multiple-output support vector regression with a firefly algorithm for interval-valued stock price index forecasting</title>
		<author>
			<persName><forename type="first">T</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Bao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Hu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Knowl. Based Syst</title>
		<imprint>
			<biblScope unit="volume">55</biblScope>
			<biblScope unit="page" from="87" to="100" />
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Multi-output least-squares support vector regression machines</title>
		<author>
			<persName><forename type="first">S</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>An</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Qiao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pattern Recognit</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="page" from="1078" to="1084" />
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">A review on multi-label learning algorithms</title>
		<author>
			<persName><forename type="first">M.-L</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z.-H</forename><surname>Zhou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Knowl. Data Eng</title>
		<imprint>
			<biblScope unit="volume">26</biblScope>
			<biblScope unit="page" from="1819" to="1837" />
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Multi-output LS-SVR machine in extended feature space</title>
		<author>
			<persName><forename type="first">W</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Shi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2012 IEEE International Conference on Computational Intelligence for Measurement Systems and Applications</title>
		<meeting>the 2012 IEEE International Conference on Computational Intelligence for Measurement Systems and Applications</meeting>
		<imprint>
			<date type="published" when="2012">2012</date>
			<biblScope unit="page" from="130" to="134" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">A pruning method of refining recursive reduced least squares support vector regression</title>
		<author>
			<persName><forename type="first">Y.-P</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K.-K</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Inf. Sci</title>
		<imprint>
			<biblScope unit="volume">296</biblScope>
			<biblScope unit="page" from="160" to="174" />
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Finding the samples near the decision plane for support vector learning</title>
		<author>
			<persName><forename type="first">F</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Gao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Inf. Sci</title>
		<imprint>
			<biblScope unit="volume">382</biblScope>
			<biblScope unit="issue">383</biblScope>
			<biblScope unit="page" from="292" to="307" />
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
