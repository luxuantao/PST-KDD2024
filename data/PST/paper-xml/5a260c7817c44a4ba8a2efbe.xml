<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Look Wider to Match Image Patches with Convolutional Neural Networks</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Haesol</forename><surname>Park</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Kyoung</forename><forename type="middle">Mu</forename><surname>Lee</surname></persName>
						</author>
						<title level="a" type="main">Look Wider to Match Image Patches with Convolutional Neural Networks</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">716CBC55A8B4C43B8F18FCA79E1D118E</idno>
					<idno type="DOI">10.1109/LSP.2016.2637355</idno>
					<note type="submission">This article has been accepted for publication in a future issue of this journal, but has not been fully edited. Content may change prior to final publication. Citation information: DOI 10.1109/LSP.2016.2637355, IEEE Signal Processing Letters IEEE SIGNAL PROCESSING LETTERS 1 This article has been accepted for publication in a future issue of this journal, but has not been fully edited. Content may change prior to final publication. Citation information: DOI 10.1109/LSP.2016.2637355, IEEE Signal Processing Letters</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.3" ident="GROBID" when="2023-07-28T12:41+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>stereo matching</term>
					<term>pooling</term>
					<term>CNN</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>When a human matches two images, the viewer has a natural tendency to view the wide area around the target pixel to obtain clues of right correspondence. However, designing a matching cost function that works on a large window in the same way is difficult. The cost function is typically not intelligent enough to discard the information irrelevant to the target pixel, resulting in undesirable artifacts. In this paper, we propose a novel convolutional neural network (CNN) module to learn a stereo matching cost with a large-sized window. Unlike conventional pooling layers with strides, the proposed per-pixel pyramid-pooling layer can cover a large area without a loss of resolution and detail. Therefore, the learned matching cost function can successfully utilize the information from a large area without introducing the fattening effect. The proposed method is robust despite the presence of weak textures, depth discontinuity, illumination, and exposure difference. The proposed method achieves near-peak performance on the Middlebury benchmark.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>I. INTRODUCTION</head><p>Most stereo matching methods first compute the matching cost of each pixel with a certain disparity, before optimizing the whole cost volume either globally or locally by using specific prior knowledge <ref type="bibr" target="#b0">[1]</ref>. For decades, many researchers have focused on the second step, designing a good prior function and optimizing it <ref type="bibr" target="#b1">[2]</ref>, <ref type="bibr" target="#b2">[3]</ref>, <ref type="bibr" target="#b3">[4]</ref>, <ref type="bibr" target="#b4">[5]</ref>, <ref type="bibr" target="#b5">[6]</ref>. Few studies have been conducted on designing or selecting a better matching cost function.</p><p>One of the most widely used matching cost functions is a pixel-wise matching cost function, such as the one used in <ref type="bibr" target="#b6">[7]</ref>. Along with sophisticated prior models, it sometimes produces good results, especially in preserving the detailed structures near the disparity discontinuities. However, the function fails when the image contains weakly-textured areas or repetitive textures. In such cases, a window-based matching cost, such as CENSUS or SAD <ref type="bibr" target="#b7">[8]</ref>, produces a more reliable and distinctive measurement. The critical shortcoming of window-based matching cost functions is their unreliability around disparity discontinuities. Fig. <ref type="figure" target="#fig_0">1</ref> visually illustrates the characteristics of different matching cost measures.</p><p>One method to handle this trade-off is to make the windowbased versatile to its input patterns <ref type="bibr" target="#b9">[10]</ref>, <ref type="bibr" target="#b10">[11]</ref>, <ref type="bibr" target="#b11">[12]</ref>. The key H. Park and K. M. Lee are with the Department of Electrical Engineering and Computer Science, Automation and Systems Research Institute, Seoul National University, Kwanak, PO Box 34, Seoul 151-600, Korea. E-mail: haesol.park@gmail.com, kyoungmu@snu.ac.kr</p><p>Copyright (c) 2015 IEEE. Personal use of this material is permitted. However, permission to use this material for any other purposes must be obtained from the IEEE by sending a request to pubs-permissions@ieee.org. The pixel positions are marked as blue dots, whereas the red and green boxes represent 37 × 37 and 11 × 11 windows centered on them, respectively. At the bottom, the matching costs for each pixel are visualized as a normalized function of disparity for different matching cost functions. The positions of true disparities are marked as red vertical lines. The pixelwise cost shows the lowest values at the true disparity, but it also gives zero costs for other disparities. The SAD and CENSUS matching cost functions <ref type="bibr" target="#b8">[9]</ref> become less ambiguous as the matching window becomes larger. However, these functions are affected by pixels irrelevant to the target pixel (x 2 ). Even the matching cost learned by using the baseline convolutional neural network (CNN) architecture fails when the surface has a nearly flat texture (x 1 ). On the other hand, the proposed CNN architecture works well both on weakly textured regions and disparity discontinuities.</p><p>idea is making the shape of the matching template adaptive so that it can discard the information from the pixels that are irrelevant to the target pixel. However, knowing the background pixels before the actual matching is difficult, making it a 'chicken-and-egg' problem. Therefore, the use of a CNN <ref type="bibr" target="#b12">[13]</ref>, <ref type="bibr" target="#b13">[14]</ref> is appropriate, as it automatically learns the proper shape of the templates for each input pattern. The existing methods, however, are based on conventional CNN architectures resembling the AlexNet <ref type="bibr" target="#b14">[15]</ref> or VGG <ref type="bibr" target="#b15">[16]</ref> network, which are optimized for image classification task and not for image matching. The architectures comprise several convolution layers, each followed by a rectified linear unit (ReLU) <ref type="bibr" target="#b14">[15]</ref>, and pooling layers with strides. One of the limitations of using these architectures for matching is the difficulty of enlarging the size of the patches that are to be compared. The effective size of the patch is directly related to the spatial extent of the receptive field of CNN, which can be increased by (1) including a few strided pooling/convolution layers, (2) using larger convolution kernels at each layer, or (3) increasing the number of layers. However, use of strided pooling/convolution layers makes the results downsampled, losing fine details. Although the resolution can be recovered by applying fractional-strided convolution <ref type="bibr" target="#b16">[17]</ref>, reconstructing small or thin structures is still difficult if once they are lost after downsampling. Increasing the size of the kernels is also problematic, as the number of feature maps required to represent the larger patterns increases significantly. Furthermore, a previous study <ref type="bibr" target="#b17">[18]</ref> reported that the repetitive usage of small convolutions does not always result in a large receptive field.</p><p>This paper contributes to the literature by proposing a novel CNN module to learn a better matching cost function. The module is an innovative scheme that enables a CNN to view a larger area without losing the fine details and without increasing the computational complexity during test times. The experiments show that the use of the proposed module improves the performance of the baseline network, showing competitive results on the Middlebury <ref type="bibr" target="#b0">[1]</ref>, <ref type="bibr" target="#b18">[19]</ref> benchmark.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>II. RELATED WORKS</head><p>Given the introduction of high-resolution stereo datasets with the ground-truth disparity maps <ref type="bibr" target="#b19">[20]</ref>, <ref type="bibr" target="#b18">[19]</ref>, <ref type="bibr" target="#b20">[21]</ref>, many attempts have been made to learn a matching cost function using machine learning algorithms <ref type="bibr" target="#b12">[13]</ref>, <ref type="bibr" target="#b13">[14]</ref>, <ref type="bibr" target="#b21">[22]</ref>. The most impressive results are obtained by using CNN <ref type="bibr" target="#b12">[13]</ref>, <ref type="bibr" target="#b13">[14]</ref>. The architecture proposed in <ref type="bibr" target="#b12">[13]</ref> takes a small 11 × 11 window and processes it without the use of pooling. The computed cost volume is noisy due to the limited size of the window. Thus, it is post-processed by using the crossbased cost aggregation (CBCA) <ref type="bibr" target="#b22">[23]</ref>, semi-global matching (SGM) <ref type="bibr" target="#b2">[3]</ref>, and additional refinement procedures. On the other hand, the method in <ref type="bibr" target="#b13">[14]</ref> uses multiple pooling layers and spatial-pyramid-pooling (SPP) <ref type="bibr" target="#b23">[24]</ref> to process larger patches. However, the results show a fattening effect owing to the loss of information introduced by pooling.</p><p>The main contribution of this paper is in proposing a novel pooling scheme that can handle information from a large receptive field without losing the fine details. Recently, several attempts have been made to accomplish the same goal in the context of semantic segmentation <ref type="bibr" target="#b24">[25]</ref>, <ref type="bibr" target="#b25">[26]</ref>, <ref type="bibr" target="#b26">[27]</ref>. These methods combine the feature maps from the highlevel layers with those from the lower layers, with the aim of correctly aligning the object-level information along the pixel-level details. While this approach can successfully align the boundaries of the big objects, its inherent limitation is its inability to recover small objects in the final output once they are lost during the abstraction due to multiple uses of pooling. In the same context, the FlowNet <ref type="bibr" target="#b27">[28]</ref> architecture can upsample the coarse-level flow to the original scale by using lower-level feature maps. However, it fails to recover the extreme flow elements that are hidden due to the low resolution of high-level feature maps. The architecture most closely related to the current work has been proposed in <ref type="bibr" target="#b23">[24]</ref>. Unlike the other approaches, the SPP network excludes pooling layers between convolutional layers. Instead, it first computes highly-nonlinear feature maps by cascading convolutional layers several times and then generates high-level and mid-level information by pooling them at different scales. By keeping the original feature maps along with feature maps pooled at multiple scales, the SPP network can combine the features from multiple levels without losing fine details. Although the previously mentioned stereo method in <ref type="bibr" target="#b13">[14]</ref> uses SPP, it also employs conventional pooling layers between convolutional layers, thus losing the detailed information.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>III. ARCHITECTURE OF THE NEURAL NETWORK</head><p>The proposed architecture takes two input patches and produces the corresponding matching cost. In the following subsections, the newly proposed module is first introduced. Then the detailed architecture of the entire network is presented.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Per-pixel Pyramid Pooling (4P)</head><p>The use of pooling layers in CNN has been considered desirable because of its accuracy and efficiency in image classification tasks. While the use of max-pooling layers has been reported to provide an additional invariance in spatial transformation, the most important gain comes from the downsampling of feature maps. By performing pooling with a stride that is larger than one, the output feature maps after the pooling are scaled down. The final scale of the CNN output is decreased exponentially in terms of the number of pooling layers. Given that no parameters related to a pooling operation exist, this method is an effective way to widen the receptive field area of a CNN without increasing the number of parameters. The drawback of strided pooling is that the network loses fine details in the original feature maps as the pooling is applied. Thus, a trade-off exists in seeing a larger area and preserving the small details.</p><p>Inspired by the idea discussed in <ref type="bibr" target="#b23">[24]</ref>, we propose a novel pooling scheme to overcome this trade-off. Instead of using a small pooling window with a stride, a large pooling window is used to achieve the desired size of the receptive field. The use of one large pooling window can lead to the loss of finer details. Thus, multiple poolings with varying window sizes are performed, and the outputs are concatenated to create new feature maps. The resulting feature maps contain the information from coarse-to-fine scales. The multi-scale pooling operation is performed for every pixel without strides. We call this whole procedure, "per-pixel pyramid pooling" (4P), which is formally defined as follows:</p><formula xml:id="formula_0">P 4P (F, s) = [P (F, s 1 ) , • • • , P (F, s M )] ,<label>(1)</label></formula><p>where s is a vector having M number of elements, and P (F, s i ) is the pooling operation with size s i and stride one. The structure of this module is illustrated in Fig. <ref type="figure">2</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Proposed model</head><p>To validate the effect of the proposed module, we trained and tested CNNs with and without the 4P module. The baseline architecture is selected as the 'MC-CNN-acrt' <ref type="bibr" target="#b12">[13]</ref>. The 4P module in the proposed architecture is constructed by using the size vector s = <ref type="bibr" target="#b26">[27,</ref><ref type="bibr" target="#b8">9,</ref><ref type="bibr" target="#b2">3,</ref><ref type="bibr" target="#b0">1]</ref>. The structures of two CNNs are visualized in Fig. <ref type="figure">3</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>IV. IMPLEMENTATION DETAILS</head><p>For a fair comparison, we followed the details in <ref type="bibr" target="#b12">[13]</ref> to train the proposed architecture with a few exceptions mentioned below. First, the size of the training patch became 37×37. Furthermore, we only fine-tuned the parameters of the last three 1 × 1 convolution layers of the proposed architecture in Fig. <ref type="figure">3</ref>. The parameters of the earlier layers are borrowed from the pre-trained 'MC-CNN-acrt' <ref type="bibr" target="#b12">[13]</ref> network. In our experiments, this resulted in a better performance than the end-to-end training of the network with random initializations. Moreover, training a few convolution layers with pre-trained features is easier, making it converge faster. We have run a total of four epochs of training, where the last two epochs were run with a decreased learning rate from 0.003 to 0.0003.</p><p>We also used the same post-processing pipeline as in <ref type="bibr" target="#b12">[13]</ref> during the test phase. The post-processing pipeline includes the use of the CBCA <ref type="bibr" target="#b22">[23]</ref> and SGM <ref type="bibr" target="#b2">[3]</ref>, and the disparity maps are refined to have continuous values and undergo median filtering and bilateral filtering.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>V. EXPERIMENTS</head><p>To verify the effect of the proposed 4P module, we have compared the results of the baseline and proposed network. The performance is measured using the 'training dense' set of the Middlebury benchmark <ref type="bibr" target="#b0">[1]</ref>. The quantitative results are briefly summarized in Table I using the average errors. All experiments are performed by using the Intel core i7 4790K CPU and a single Nvidia Geforce GTX Titan X GPU.</p><p>The proposed method outperforms the baseline architecture regardless of the use of post-processing. The benefit of using the 4P module is clear when the disparity maps are obtained by using the pixel-wise winner-takes-it-all (WTA) rule without any post-processing. Given that the images in the dataset contain many weakly-textured areas, the small-sized 11×11 window cannot distinguish the true matches from false ones without the aid of post-processing. On the other hand, the proposed architecture effectively sees the larger window, 37 × 37, by inserting the 4P module before the final decision layer.</p><p>It is less straightforward to understand why the proposed architecture still outperforms the baseline even after postprocessing. In that sense, it is worth to mention that the best parameter setting for post-processing of the proposed method largely differ from that of the baseline. <ref type="foot" target="#foot_0">1</ref> The most notable changes from the original parameter setting is that we use much less number of CBCA <ref type="bibr" target="#b22">[23]</ref>, and it means that multiple uses of CBCA <ref type="bibr" target="#b22">[23]</ref> become redundant in the proposed architecture. From this fact, we can interpret the role of 4P module as adaptive local feature aggregation. Compared to the true disparity and left image proposed MC-CNN-acrt Fig. <ref type="figure">4</ref>. The results for PlaytableP and Vintage are visualized. For each datum, the upper row shows the disparity map and the bottom row shows the corresponding error maps. While the 'MC-CNN-acrt' <ref type="bibr" target="#b12">[13]</ref> shows errors around the weakly-textured areas, such as the surfaces of the chair and the table in PlaytableP or the white wall in Vintage, the proposed method shows more reliable results.</p><p>hand-designed algorithm such as CBCA <ref type="bibr" target="#b22">[23]</ref>, the influence of neighboring pixels to a certain pixel is automatically learned and it can be jointly trained with the cost function itself. Furthermore, the information exchange among pixels is done in feature space which contains richer contextual information than the final cost volume space. Note that the improvement over the baseline clearly results neither from the use of extra layers nor from the use of more parameters, as the authors of <ref type="bibr" target="#b12">[13]</ref> already have shown that the additional use of fully-connected (FC) layers is less significant. Using two additional FC layers leads to an improvement of approximately 1.90%, whereas using the 4P module results in a 21.42% improvement in terms of average error.</p><p>The main contribution of the proposed method lies in learning a less ambiguous matching cost function by inspecting a larger area. Fig. <ref type="figure">4</ref> shows that the proposed network actually works better around the weakly-textured area than the 'MC-CNN-acrt' <ref type="bibr" target="#b12">[13]</ref>. The quantitative and qualitative results of each dataset, including the ones in the 'test dense' set, are available at the Middlebury benchmark <ref type="bibr" target="#b0">[1]</ref> website.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>VI. CONCLUSIONS</head><p>Viewing a large area to estimate the dense pixel correspondence is necessary to fully utilize the texture information to achieve less ambiguous and more accurate matching. A conventional matching cost function fails because neighboring pixels on the same surface as the target pixel are unknown. In this paper, a novel CNN module is proposed to make the CNN structure handle a large image patch without losing the small details, which enables it to learn an intelligent matching cost function for large-sized windows. The learned cost function can discriminate the false matches for weakly-textured areas or repeating textures, and can also conserve the disparity discontinuities well/. The learned cost function achieves competitive performance on the Middlebury benchmark.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 1 .</head><label>1</label><figDesc>Fig.1. The top image shows the reference image with two interested points, x 1 and x 2 . The pixel positions are marked as blue dots, whereas the red and green boxes represent 37 × 37 and 11 × 11 windows centered on them, respectively. At the bottom, the matching costs for each pixel are visualized as a normalized function of disparity for different matching cost functions. The positions of true disparities are marked as red vertical lines. The pixelwise cost shows the lowest values at the true disparity, but it also gives zero costs for other disparities. The SAD and CENSUS matching cost functions<ref type="bibr" target="#b8">[9]</ref> become less ambiguous as the matching window becomes larger. However, these functions are affected by pixels irrelevant to the target pixel (x 2 ). Even the matching cost learned by using the baseline convolutional neural network (CNN) architecture fails when the surface has a nearly flat texture (x 1 ). On the other hand, the proposed CNN architecture works well both on weakly textured regions and disparity discontinuities.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>4 PFig. 2 .</head><label>42</label><figDesc>Fig.2. The 4P module with pooling size vector s =<ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b2">3,</ref><ref type="bibr" target="#b0">1]</ref> is visualized. This figure shows its action for one channel of the feature maps for brevity; it does the same job for all channels.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>Fig.3. The network structures are visualized for the baseline network, 'MC-CNN-acrt'<ref type="bibr" target="#b12">[13]</ref>, and the proposed network. The parenthesized numbers at each layer represent the number of feature maps after the corresponding operations. Note that this figure is drawn in terms of the fully convolutional network.</figDesc><table><row><cell cols="2">Matching score</cell><cell cols="2">Matching score</cell></row><row><cell cols="2">1x1 Conv. + Sigmoid (1)</cell><cell cols="2">1x1 Conv. + Sigmoid (1)</cell></row><row><cell></cell><cell></cell><cell cols="2">1x1 Conv. + ReLU (384)</cell></row><row><cell></cell><cell></cell><cell>4P (384)</cell><cell></cell></row><row><cell></cell><cell></cell><cell cols="2">1x1 Conv. (96)</cell></row><row><cell cols="2">1x1 Conv. + ReLU (384)</cell><cell cols="2">1x1 Conv. + ReLU (384)</cell></row><row><cell cols="2">1x1 Conv. + ReLU (384)</cell><cell cols="2">1x1 Conv. + ReLU (384)</cell></row><row><cell cols="2">1x1 Conv. + ReLU (384)</cell><cell cols="2">1x1 Conv. + ReLU (384)</cell></row><row><cell cols="2">3x3 Conv. + ReLU (112)</cell><cell cols="2">3x3 Conv. + ReLU (112)</cell></row><row><cell cols="2">3x3 Conv. + ReLU (112)</cell><cell cols="2">3x3 Conv. + ReLU (112)</cell></row><row><cell cols="2">3x3 Conv. + ReLU (112)</cell><cell cols="2">3x3 Conv. + ReLU (112)</cell></row><row><cell cols="2">3x3 Conv. + ReLU (112)</cell><cell cols="2">3x3 Conv. + ReLU (112)</cell></row><row><cell cols="2">3x3 Conv. + ReLU (112)</cell><cell cols="2">3x3 Conv. + ReLU (112)</cell></row><row><cell>L</cell><cell>R</cell><cell>L</cell><cell>R</cell></row><row><cell>Baseline</cell><cell></cell><cell>Proposed</cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>TABLE I THE</head><label>I</label><figDesc>QUANTITATIVE RESULTS ON THE 'TRAINING DENSE' SET OF THE MIDDLEBURY BENCHMARK [1] ARE SHOWN. THE ERROR REPRESENTS THE PERCENTAGE OF BAD PIXELS WITH A DISPARITY THRESHOLD 2.0, AND THE SAME WEIGHTING SCHEME IS APPLIED AS IN [1] WHEN COMPUTING THE AVERAGE.</figDesc><table><row><cell cols="2">Methods</cell><cell>avg. error</cell></row><row><cell>WTA</cell><cell>MC-CNN-acrt [13] proposed</cell><cell>22.91 11.75</cell></row><row><cell></cell><cell>MC-CNN-acrt [13]</cell><cell>10.26</cell></row><row><cell>after post-processing</cell><cell>proposed (w/ parameters in [13])</cell><cell>9.72</cell></row><row><cell></cell><cell>proposed (w/ parameter tuning)</cell><cell>8.45</cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0"><p>Following the conventions in<ref type="bibr" target="#b12">[13]</ref>, the best parameter setting is as follows: cbca_num_iterations_1 = 0, cbca_num_iterations_2 = 1, sgm_P1 = 1.3, sgm_P2 = 17.0, sgm_Q1 = 3.6, sgm_Q2 = 36.0, and sgm_V = 1.4.</p></note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">A taxonomy and evaluation of dense two-frame stereo correspondence algorithms</title>
		<author>
			<persName><forename type="first">D</forename><surname>Scharstein</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Szeliski</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IJCV</title>
		<imprint>
			<biblScope unit="volume">47</biblScope>
			<biblScope unit="issue">1-3</biblScope>
			<biblScope unit="page" from="7" to="42" />
			<date type="published" when="2002">2002</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Computing visual correspondence with occlusions using graph cuts</title>
		<author>
			<persName><forename type="first">V</forename><surname>Kolmogorov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Zabih</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ICCV</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="508" to="515" />
			<date type="published" when="2001">2001</date>
			<publisher>IEEE</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Stereo processing by semiglobal matching and mutual information</title>
		<author>
			<persName><forename type="first">H</forename><surname>Hirschmüller</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">PAMI</title>
		<imprint>
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="328" to="341" />
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Global stereo reconstruction under second-order smoothness priors</title>
		<author>
			<persName><forename type="first">O</forename><surname>Woodford</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Torr</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Reid</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Fitzgibbon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">PAMI</title>
		<imprint>
			<biblScope unit="volume">31</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page" from="2115" to="2128" />
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Fast cost-volume filtering for visual correspondence and beyond</title>
		<author>
			<persName><forename type="first">C</forename><surname>Rhemann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Hosni</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Bleyer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Rother</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Gelautz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2011">2011</date>
			<biblScope unit="page" from="3017" to="3024" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">A non-local cost aggregation method for stereo matching</title>
		<author>
			<persName><forename type="first">Q</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2012">2012</date>
			<biblScope unit="page" from="1402" to="1409" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Depth discontinuities by pixel-to-pixel stereo</title>
		<author>
			<persName><forename type="first">S</forename><surname>Birchfield</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Tomasi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Computer Vision</title>
		<imprint>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="269" to="293" />
			<date type="published" when="1999">1999</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Evaluation of stereo matching costs on images with radiometric differences</title>
		<author>
			<persName><forename type="first">H</forename><surname>Hirschmuller</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Scharstein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE transactions on pattern analysis and machine intelligence</title>
		<imprint>
			<biblScope unit="volume">31</biblScope>
			<biblScope unit="issue">9</biblScope>
			<biblScope unit="page" from="1582" to="1599" />
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Evaluation of cost functions for stereo matching</title>
		<author>
			<persName><forename type="first">H</forename><surname>Hirschmüller</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Scharstein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2007">2007</date>
			<biblScope unit="page" from="1" to="8" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Adaptive stereo matching algorithm based on edge detection</title>
		<author>
			<persName><forename type="first">K</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ICIP</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="1345" to="1348" />
			<date type="published" when="2004">2004</date>
			<publisher>IEEE</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Adaptive support-weight approach for correspondence search</title>
		<author>
			<persName><forename type="first">K.-J</forename><surname>Yoon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><forename type="middle">S</forename><surname>Kweon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">PAMI</title>
		<imprint>
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="650" to="656" />
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Classification and evaluation of cost aggregation methods for stereo correspondence</title>
		<author>
			<persName><forename type="first">F</forename><surname>Tombari</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Mattoccia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">D</forename><surname>Stefano</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Addimanda</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2008">2008</date>
			<biblScope unit="page" from="1" to="8" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Stereo matching by training a convolutional neural network to compare image patches</title>
		<author>
			<persName><forename type="first">J</forename><surname>Žbontar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">17</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="2287" to="2318" />
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Learning to compare image patches via convolutional neural networks</title>
		<author>
			<persName><forename type="first">S</forename><surname>Zagoruyko</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Komodakis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015-06">June 2015</date>
			<biblScope unit="page" from="4353" to="4361" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Imagenet classification with deep convolutional neural networks</title>
		<author>
			<persName><forename type="first">A</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2012">2012</date>
			<biblScope unit="page" from="1097" to="1105" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">Very deep convolutional networks for large-scale image recognition</title>
		<author>
			<persName><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1409.1556</idno>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">Unsupervised representation learning with deep convolutional generative adversarial networks</title>
		<author>
			<persName><forename type="first">A</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Metz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Chintala</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1511.06434</idno>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">Object detectors emerge in deep scene cnns</title>
		<author>
			<persName><forename type="first">B</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Khosla</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Lapedriza</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Oliva</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Torralba</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1412.6856</idno>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">High-resolution stereo datasets with subpixel-accurate ground truth</title>
		<author>
			<persName><forename type="first">D</forename><surname>Scharstein</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Hirschmüller</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Kitajima</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Krathwohl</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Nešić</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Westling</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Pattern Recognition</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="31" to="42" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Are we ready for autonomous driving? the kitti vision benchmark suite</title>
		<author>
			<persName><forename type="first">A</forename><surname>Geiger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Lenz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Urtasun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Object scene flow for autonomous vehicles</title>
		<author>
			<persName><forename type="first">M</forename><surname>Menze</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Geiger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">Learning the matching function</title>
		<author>
			<persName><forename type="first">L</forename><surname>Ladickỳ</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Häne</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Pollefeys</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1502.00652</idno>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Cross-based local stereo matching using orthogonal integral images</title>
		<author>
			<persName><forename type="first">K</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Lafruit</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Circuits and Systems for Video Technology</title>
		<imprint>
			<date type="published" when="2009">2009</date>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="page" from="1073" to="1079" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Spatial pyramid pooling in deep convolutional networks for visual recognition</title>
		<author>
			<persName><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="346" to="361" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Fully convolutional networks for semantic segmentation</title>
		<author>
			<persName><forename type="first">J</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Shelhamer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="3431" to="3440" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Hypercolumns for object segmentation and fine-grained localization</title>
		<author>
			<persName><forename type="first">B</forename><surname>Hariharan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Arbelaez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015-06">June 2015</date>
			<biblScope unit="page" from="447" to="456" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<title level="m" type="main">Learning deconvolution network for semantic segmentation</title>
		<author>
			<persName><forename type="first">H</forename><surname>Noh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Hong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Han</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1505.04366</idno>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<title level="m" type="main">Flownet: Learning optical flow with convolutional networks</title>
		<author>
			<persName><forename type="first">P</forename><surname>Fischer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Dosovitskiy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Ilg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Häusser</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Hazırbas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">¸</forename></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Golkov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Van Der Smagt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Cremers</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Brox</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1504.06852</idno>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
