<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">HyperGCN: A New Method of Training Graph Convolutional Networks on Hypergraphs</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Naganand</forename><surname>Yadati</surname></persName>
							<email>y.naganand@gmail.com</email>
						</author>
						<author>
							<persName><forename type="first">Madhav</forename><surname>Nimishakavi</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Prateek</forename><surname>Yadav</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Vikram</forename><surname>Nitin</surname></persName>
							<email>vikramnitin9@gmail.com</email>
						</author>
						<author>
							<persName><forename type="first">Anand</forename><surname>Louis</surname></persName>
							<email>anandl@iisc.ac.in</email>
						</author>
						<author>
							<persName><forename type="first">Partha</forename><surname>Talukdar</surname></persName>
							<email>partha@talukdar.net</email>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="institution">Indian Institute of Science</orgName>
								<address>
									<settlement>Bangalore</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff1">
								<orgName type="institution">Indian Institute of Science</orgName>
								<address>
									<settlement>Bangalore</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff2">
								<orgName type="institution">Indian Institute of Science</orgName>
								<address>
									<settlement>Bangalore</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff3">
								<orgName type="institution">Birla Institute of Technology and Science</orgName>
								<address>
									<country>Pilani</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff4">
								<orgName type="institution">Indian Institute of Science</orgName>
								<address>
									<settlement>Bangalore</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff5">
								<orgName type="institution">Indian Institute of Science</orgName>
								<address>
									<settlement>Bangalore</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff6">
								<orgName type="laboratory">33rd Conference on Neural Information Processing Systems</orgName>
								<orgName type="institution">Indian Institute of Science</orgName>
								<address>
									<postCode>2019)</postCode>
									<settlement>Bangalore, Vancouver</settlement>
									<region>NeurIPS</region>
									<country key="CA">Canada</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">HyperGCN: A New Method of Training Graph Convolutional Networks on Hypergraphs</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.2" ident="GROBID" when="2022-12-25T13:44+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>In many real-world networks such as co-authorship, co-citation, etc., relationships are complex and go beyond pairwise associations. Hypergraphs provide a flexible and natural modeling tool to model such complex relationships. The obvious existence of such complex relationships in many real-world networks naturally motivates the problem of learning with hypergraphs. A popular learning paradigm is hypergraph-based semi-supervised learning (SSL) where the goal is to assign labels to initially unlabelled vertices in a hypergraph. Motivated by the fact that a graph convolutional network (GCN) has been effective for graph-based SSL, we propose HyperGCN, a novel way of training a GCN for SSL on hypergraphs based on tools from sepctral theory of hypergraphs. We demonstrate HyperGCN's effectiveness through detailed experimentation on real-world hypergraphs for SSL and combinatorial optimisation and analyse when it is going to be more effective than state-of-the art baselines. We have made the source code available.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>In many real-world network datasets such as co-authorship, co-citation, email communication, etc., relationships are complex and go beyond pairwise associations. Hypergraphs provide a flexible and natural modeling tool to model such complex relationships. For example, in a co-authorship network an author (hyperedge) can be a co-author of more than two documents (vertices).</p><p>The obvious existence of such complex relationships in many real-world networks naturaly motivates the problem of learning with hypergraphs <ref type="bibr" target="#b51">[52,</ref><ref type="bibr" target="#b21">22,</ref><ref type="bibr" target="#b48">49,</ref><ref type="bibr" target="#b16">17]</ref>. A popular learning paradigm is graphbased / hypergraph-based semi-supervised learning (SSL) where the goal is to assign labels to initially unlabelled vertices in a graph / hypergraph <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b53">54,</ref><ref type="bibr" target="#b41">42]</ref>. While many techniques have used explicit Laplacian regularisation in the objective <ref type="bibr" target="#b50">[51,</ref><ref type="bibr" target="#b52">53,</ref><ref type="bibr" target="#b10">11,</ref><ref type="bibr" target="#b47">48]</ref>, the state-of-the-art neural methods encode the graph / hypergraph structure G = (V, E) implicitly via a neural network f (G, X) <ref type="bibr" target="#b24">[25,</ref><ref type="bibr" target="#b2">3,</ref><ref type="bibr" target="#b16">17]</ref> (X contains the initial features on the vertices for example, text attributes for documents). While explicit Laplacian regularisation assumes similarity among vertices in each edge / hyperedge, implicit regularisation of graph convolutional networks (GCNs) <ref type="bibr" target="#b24">[25]</ref> avoids this restriction and enables application to a broader range of problems in combinatorial optimisation <ref type="bibr" target="#b18">[19,</ref><ref type="bibr" target="#b25">26,</ref><ref type="bibr" target="#b37">38,</ref><ref type="bibr" target="#b30">31]</ref>, computer vision <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b36">37]</ref>, natural language processing <ref type="bibr" target="#b43">[44,</ref><ref type="bibr" target="#b33">34]</ref>, etc. In this work, we propose, HyperGCN, a novel training scheme for a GCN on hypergraph and show its effectiveness not only in SSL where hyperedges encode similarity but also in combinatorial optimisation where hyperedges do not encode similarity. Combinatorial optimisation on hypergraphs has recently been highlighted as crucial for real-world network analysis <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b35">36]</ref>.</p><p>Methodologically, HyperGCN approximates each hyperedge of the hypergraph by a set of pairwise edges connecting the vertices of the hyperedge and treats the learning problem as a graph learning problem on the approximation. While the state-of-the-art hypergraph neural networks (HGNN) <ref type="bibr" target="#b16">[17]</ref> approximates each hyperedge by a clique and hence requires s C 2 (quadratic number of) edges for each hyperedge of size s, our method, i.e. HyperGCN, requires a linear number of edges (i.e. O(s)) for each hyperedge. The advantage of this linear approximation is evident in Table <ref type="table" target="#tab_0">1</ref> where a faster variant of our method has lower training time on synthetic data (with higher density as well) for densest k-subhypergraph and SSL on real-world hypergraphs (DBLP and Pubmed). In summary, we make the following contributions:</p><p>• We propose HyperGCN, a new method of training a GCN on hypergraph using tools from spectral theory of hypergraphs and introduce FastHyperGCN, its faster variant (Section 4). • We apply our methods to the problems of SSL and combinatorial optimisation on realworld hypergraphs. Through detailed experimentation, we demonstrate their effectiveness compared to the state-of-the art HGNN <ref type="bibr" target="#b16">[17]</ref> and other baselines (Sections 5, and 7). • We thoroughly discuss when we prefer our methods to HGNN (Sections 6, and 8).</p><p>While the motivation of our methods is based on similarity of vertices in a hyperedge, we show they can be effectively used for combinatorial optimisation where hyperedges do not encode similarity.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related work</head><p>In this section, we discuss related work and then the background in the next section.</p><p>Deep learning on graphs: Geometric deep learning <ref type="bibr" target="#b4">[5]</ref> is an umbrella phrase for emerging techniques attempting to generalise (structured) deep neural network models to non-Euclidean domains such as graphs and manifolds. Graph convolutional network (GCN) <ref type="bibr" target="#b24">[25]</ref> defines the convolution using a simple linear function of the graph Laplacian and is shown to be effective on semi-supervised classification on attributed graphs. The reader is referred to a comprehensive literature review <ref type="bibr" target="#b4">[5]</ref> and extensive surveys <ref type="bibr" target="#b19">[20,</ref><ref type="bibr" target="#b3">4]</ref> on this topic of deep learning on graphs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Learning on hypergraphs:</head><p>The clique expansion of a hypergraph was introduced in a seminal work <ref type="bibr" target="#b51">[52]</ref> and has become a popular approach for learning on hypergraph-structured data <ref type="bibr" target="#b38">[39,</ref><ref type="bibr" target="#b15">16,</ref><ref type="bibr" target="#b49">50,</ref><ref type="bibr" target="#b42">43]</ref>. Hypergraph neural networks <ref type="bibr" target="#b16">[17]</ref> and their variants <ref type="bibr" target="#b22">[23,</ref><ref type="bibr" target="#b23">24]</ref> use the clique expansion to extend GCNs for hypergraphs. Powerset convolutional networks <ref type="bibr" target="#b46">[47]</ref> utilise tools from signal processing to define convolution on set functions. Another line of work uses mathematically appealing tensor methods <ref type="bibr" target="#b39">[40,</ref><ref type="bibr" target="#b5">6]</ref>, but they are limited to uniform hypergraphs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Spectral Theory of Hypergraphs:</head><p>The clique expansion of a hypergraph essentially models it as a graph by converting each hyperedge to a clique subgraph <ref type="bibr" target="#b0">[1]</ref>. It has been well established that this approximation causes distortion, fails to utilise higher-order relationships in the data and leads to unreliable learning performance for clustering, SSL, active learning, etc. <ref type="bibr" target="#b27">[28,</ref><ref type="bibr" target="#b12">13]</ref>. A simple yet effective way to overcome the limitations is to introduce hyperedge-dependent vertex weights <ref type="bibr" target="#b13">[14]</ref>.</p><p>Researchers have fully utilised the hypergraph structure also through non-linear Laplacian operators <ref type="bibr" target="#b21">[22,</ref><ref type="bibr" target="#b31">32,</ref><ref type="bibr" target="#b7">8]</ref>. It has been shown that these operators enable Cheeger-type inequality for hypergraphs, relating the second smallest eigenvalue of the operator to hypergraph expansion <ref type="bibr" target="#b31">[32,</ref><ref type="bibr" target="#b7">8]</ref>. One such Laplacian is derived from the notion of total variation on hypergraphs (Lovasz extension of the hypergraph cut) which considers the maximally disparate vertices in each hyperedge <ref type="bibr" target="#b21">[22]</ref>. Recent developements have extended these non-linear operators to several different settings:</p><p>• directed hypergraphs (idea is to consider supremum in tail, infimum in head) <ref type="bibr" target="#b48">[49,</ref><ref type="bibr" target="#b8">9]</ref>.</p><p>• submodular hypergraphs (different submodular weights for different hyperedge cuts) <ref type="bibr" target="#b29">[30]</ref> and submodular function minimisation (generalises hypergraph SSL objective) <ref type="bibr" target="#b26">[27,</ref><ref type="bibr" target="#b28">29]</ref>.</p><p>• Laplacian that considers all vertices in each hyperedge (includes the vertices other than the maximally disparate ones in each hyperedge) <ref type="bibr" target="#b6">[7]</ref>.</p><p>Graph-based SSL: Researchers have shown that using unlabelled data in training can improve learning accuracy significantly. This topic is so popular that it has influential books <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b53">54,</ref><ref type="bibr" target="#b41">42]</ref>.</p><p>Graph neural networks for combinatorial optimisation: Graph-based deep models have recently been shown to be effective as learning-based approaches for NP-hard problems such as maximal independent set, minimum vertex cover, etc. <ref type="bibr" target="#b30">[31]</ref>, the decision version of the traveling salesman problem <ref type="bibr" target="#b37">[38]</ref>, graph colouring <ref type="bibr" target="#b25">[26]</ref>, and clique optimisation <ref type="bibr" target="#b18">[19]</ref>.</p><p>3 Background: Graph convolutional network</p><p>Let G = (V, E), with N = |V|, be a simple undirected graph with adjacency A ∈ R N ×N , and data matrix X ∈ R N ×p . which has p-dimensional real-valued vector representations for each node v ∈ V.</p><p>The basic formulation of graph convolution <ref type="bibr" target="#b24">[25]</ref> stems from the convolution theorem <ref type="bibr" target="#b32">[33]</ref> and it can be shown that the convolution C of a real-valued graph signal S ∈ R N and a filter signal</p><formula xml:id="formula_0">F ∈ R N is approximately C ≈ (w 0 + w 1 L)S</formula><p>where w 0 and w 1 are learned weights, and</p><formula xml:id="formula_1">L = 2L λ N − I is the scaled graph Laplacian, λ N is the largest eigenvalue of the symmetrically-normalised graph Laplacian L = I − D − 1 2 AD − 1 2 where D = diag(d 1 , • • • , d N )</formula><p>is the diagonal degree matrix with elements d i = N j=1,j =i A ji . The filter F depends on the structure of the graph (the graph Laplacian L). The detailed derivation from the convolution theorem uses existing tools from graph signal processing <ref type="bibr" target="#b40">[41,</ref><ref type="bibr" target="#b20">21,</ref><ref type="bibr" target="#b4">5]</ref> and is provided in the supplementary material. The key point here is that the convolution of two graph signals is a linear function of the graph Laplacian L. The graph convolution for p different graph signals contained in the data matrix X ∈ R N ×p with learned weights Θ ∈ R p×r with r hidden units is ĀXΘ , Ā = D− 1 2 Ã D− 1 2 , Ã = A + I, and Dii = N j=1 Ãij . The proof involves a renormalisation trick <ref type="bibr" target="#b24">[25]</ref> and is in the supplementary.</p><p>GCN <ref type="bibr" target="#b24">[25]</ref> The forward model for a simple two-layer GCN takes the following simple form:</p><formula xml:id="formula_2">Z = f GCN (X, A) = softmax Ā ReLU ĀXΘ (1) Θ (2) ,<label>(1)</label></formula><p>where Θ (1) ∈ R p×h is an input-to-hidden weight matrix for a hidden layer with h hidden units and Θ (2) ∈ R h×r is a hidden-to-output weight matrix. The softmax activation function is defined as softmax(x i ) = exp(xi) j exp(xj ) and applied row-wise. GCN training for SSL: For multi-class, classification with q classes, we minimise cross-entropy,</p><formula xml:id="formula_3">L = − i∈V L q j=1 Y ij ln Z ij ,<label>(2)</label></formula><p>over the set of labelled examples V L . Weights Θ (1) and Θ (2) are trained using gradient descent.</p><p>A summary of the notations used throughout our work is shown in Table <ref type="table" target="#tab_1">2</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">HyperGCN: Hypergraph Convolutional Network</head><p>We consider semi-supervised hypernode classification on an undirected hypergraph</p><formula xml:id="formula_4">H = (V, E) with |V | = n, |E| = m and a small set V L of labelled hypernodes. Each hypernode v ∈ V = {1, • • • , n}</formula><p>is also associated with a feature vector x v ∈ R p of dimension p given by X ∈ R n×p . The task is to predict the labels of all the unlabelled hypernodes, that is, all the hypernodes in the set V \ V L .</p><p>Overview: The crucial working principle here is that the hypernodes in the same hyperedge are similar and hence are likely to share the same label <ref type="bibr" target="#b48">[49]</ref>. Suppose we use {h v : v ∈ V } to denote some representation of the hypernodes in V , then, for any e ∈ E, the function max i,j∈e ||h i − h j ||<ref type="foot" target="#foot_0">2</ref> will be "small" only if vectors corresponding to the hypernodes in e are "close" to each other. Therefore, e∈E max i,j∈e ||h i − h j || 2 as a regulariser is likely to achieve the objective of the hypernodes in the same hyperedge having similar representations. However, instead of using it as an explicit regulariser, we can achieve the same goal by using GCN over an appropriately defined Laplacian of the hypergraph. In other words, we use the notion of hypergraph Laplacian as an implicit regulariser which achieves this objective.</p><p>A hypergraph Laplacian with the same underlying motivation as stated above was proposed in prior works <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b31">32]</ref>. We present this Laplacian first. Then we run GCN over the simple graph associated with this hypergraph Laplacian. We call the resulting method 1-HyperGCN (as each hyperedge is approximated by exactly one pairwise edge). One epoch of 1-HyperGCN is shown in figure <ref type="figure" target="#fig_0">1</ref> 4</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>.1 Hypergraph Laplacian</head><p>As explained before, the key element for a GCN is the graph Laplacian of the given graph G. Thus, in order to develop a GCN-based SSL method for hypergraphs, we first need to define a Laplacian for hypergraphs. One such way <ref type="bibr" target="#b7">[8]</ref> (see also <ref type="bibr" target="#b31">[32]</ref>) is a non-linear function L : R n → R n (the Laplacian matrix for graphs can be viewed as a linear function L : R n → R n ).</p><p>Definition 1 (Hypergraph Laplacian <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b31">32]</ref> 2 ) Given a real-valued signal S ∈ R n defined on the hypernodes, L(S) is computed as follows.</p><p>1. For each hyperedge e ∈ E, let (i e , j e ) := argmax i,j∈e |S i − S j |, breaking ties randomly 2 . Our approach requires at most a linear number of edges (1 and 2|e| − 3 respectively) while HGNN <ref type="bibr" target="#b16">[17]</ref> requires a quadratic number of edges for each hyperedge. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">1-HyperGCN</head><p>By following the Laplacian construction steps outlined in Section 4.1, we end up with the simple graph G S with normalized adjacency matrix ĀS . We now perform GCN over this simple graph G S .</p><p>The graph convolution operation in Equation ( <ref type="formula" target="#formula_2">1</ref>), when applied to a hypernode v ∈ V in G S , in the neural message-passing framework <ref type="bibr" target="#b17">[18]</ref> is h</p><formula xml:id="formula_5">(τ +1) v = σ (Θ (τ ) ) T u∈N (v) ([ Ā(τ) S ] v,u • h (τ ) u ) .</formula><p>Here, τ is epoch number, h</p><formula xml:id="formula_6">(τ +1) v</formula><p>is the new hidden layer representation of node v, σ is a non-linear activation function, Θ is a matrix of learned weights, N (u) is the set of neighbours of v, [ Ā(τ) S ] v,u is the weight on the edge {v, u} after normalisation, and h (τ ) u is the previous hidden layer representation of the neighbour u. We note that along with the embeddings of the hypernodes, the adjacency matrix is also re-estimated in each epoch.</p><p>Figure <ref type="figure" target="#fig_0">1</ref> shows a hypernode v with five hyperedges incident on it. We consider exactly one representative simple edge for each hyperedge e ∈ E given by (i e , j e ) where (i e , j e ) = arg max i,j∈e ||(Θ (τ ) ) T (h</p><formula xml:id="formula_7">(τ ) i − h (τ ) j )|| 2 for epoch τ .</formula><p>Because of this consideration, the hypernode v may not be a part of all representative simple edges (only three shown in figure). We then use traditional Graph Convolution Operation on v considering only the simple edges incident on it. Note that we apply the operation on each hypernode v ∈ V in each epoch τ of training until convergence.</p><p>Connection to total variation on hypergraphs: Our 1-HyperGCN model can be seen as performing implicit regularisation based on the total variation on hypergraphs <ref type="bibr" target="#b21">[22]</ref>. In that prior work, explicit regularisation and only the hypergraph structure is used for hypernode classification in the SSL setting. HyperGCN, on the other hand, can use both the hypergraph structure and also exploit any available features on the hypernodes, e.g., text attributes for documents.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">HyperGCN: Enhancing 1-HyperGCN with mediators</head><p>One peculiar aspect of the hypergraph Laplacian discussed is that each hyperedge e is represented by a single pairwise simple edge {i e , j e } (with this simple edge potentially changing from epoch to epoch). This hypergraph Laplacian ignores the hypernodes in K e := {k ∈ e : k = i e , k = j e } in the given epoch. Recently, it has been shown that a generalised hypergraph Laplacian in which the hypernodes in K e act as "mediators" <ref type="bibr" target="#b6">[7]</ref> satisfies all the properties satisfied by the above Laplacian given by <ref type="bibr" target="#b7">[8]</ref>. The two Laplacians are pictorially compared in Figure <ref type="figure" target="#fig_1">2</ref>. Note that if the hyperedge is of size 2, we connect i e and j e with an edge. We also run a GCN on the simple graph associated with the hypergraph Laplacian with mediators <ref type="bibr" target="#b6">[7]</ref> (right in Figure <ref type="figure" target="#fig_1">2</ref>). It has been suggested that the weights on the edges for each hyperedge in the hypergraph Laplacian (with mediators) sum to 1 <ref type="bibr" target="#b6">[7]</ref>.</p><p>We chose each weight to be 1 2|e|−3 as there are 2|e| − 3 edges for a hyperedge e.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">FastHyperGCN</head><p>We use just the initial features X (without the weights) to construct the hypergraph Laplacian matrix (with mediators) and we call this method FastHyperGCN. Because the matrix is computed only once before training (and not in each epoch), the training time of FastHyperGCN is much less than that of other methods. Please see the supplementrary for all the algorithms.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Experiments for semi-supervised learning</head><p>We conducted experiments not only on real-world datasets but also on categorical data (results in supplementary) which are a standard practice in hypergraph-based learning <ref type="bibr" target="#b51">[52,</ref><ref type="bibr" target="#b21">22,</ref><ref type="bibr" target="#b48">49,</ref><ref type="bibr" target="#b29">30,</ref><ref type="bibr" target="#b28">29,</ref><ref type="bibr" target="#b26">27]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Baselines</head><p>We compared HyperGCN, 1-HyperGCN and FastHyperGCN against the following baselines:</p><p>• Hypergraph neural networks (HGNN) <ref type="bibr" target="#b16">[17]</ref> uses the clique expansion <ref type="bibr" target="#b51">[52,</ref><ref type="bibr" target="#b0">1]</ref> to approximate the hypergraph. Each hyperedge of size s is approximated by an s-clique. • Multi-layer perceptron (MLP) treats each instance (hypernode) as an independent and identically distributed (i.i.d) instance. In other words, A = I in equation 1. We note that this baseline does not use the hypergraph structure to make predictions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>• Multi-layer perceptron + explicit hypergraph Laplacian regularisation (MLP + HLR):</head><p>regularises the MLP by training it with the loss given by L = L 0 + λL reg and uses the hypergraph Laplacian with mediators for explicit Laplacian regularisation L reg . We used 10% of the test set used for all the above models for this baseline to get an optimal λ. • Confidence Interval-based method (CI) <ref type="bibr" target="#b48">[49]</ref> uses a subgradient-based method <ref type="bibr" target="#b48">[49]</ref>. We note that this method has consistently been shown to be superior to the primal dual hybrid gradient (PDHG) of <ref type="bibr" target="#b21">[22]</ref> and also <ref type="bibr" target="#b51">[52]</ref>. Hence, we did not use these other previous methods as baselines, and directly compared HyperGCN against CI.</p><p>The task for each dataset is to predict the topic to which a document belongs (multi-class classification).</p><p>Statistics are summarised in Table <ref type="table" target="#tab_2">3</ref>. For more details about datasets, please refer to the supplementary. We trained all methods for 200 epochs and used the same hyperparameters of a prior work <ref type="bibr" target="#b24">[25]</ref>. We report the mean test error and standard deviation over 100 different train-test splits. We sampled sets of same sizes of labelled hypernodes from each class to have a balanced train split.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Analysis of results</head><p>The results on real-world datasets are shown in Table <ref type="table" target="#tab_3">4</ref>. Firstly we note that HyperGCN is superior to 1-HyperGCN. This is expected as all the vertices in a hyperedge participate in the hypergraph Laplacian in HyperGCN while only two in 1-HyperGCN. Interestingly, we note that FastHyperGCN is superior to 1-HyperGCN. This, we believe is because of the large hyperedges (size more than 4) present in all the datasets. FastHyperGCN uses all the mediators while 1-HyperGCN uses only two vertices. We now attempt to explain them. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Comparable performance on Cora and Citeseer co-citation</head><p>We note that HGNN is the most competitive baseline. Also S = X for FastHyperGCN and S = HΘ for HyperGCN. The proposition states that the graphs of HGNN, FastHyperGCN, and HyperGCN are the same irrespective of the signal values whenever the maximum size of a hyperedge is 3. This explains why the three methods have comparable accuracies for Cora co-citaion and Citeseer cocitiation hypergraphs. The mean hyperedge sizes are close to 3 (with comparitively lower deviations) as shown in Table <ref type="table" target="#tab_2">3</ref>. Hence the graphs of the three methods are more or less the same.</p><p>Superior performance on Pubmed, DBLP, and Cora co-authorship We see that HyperGCN performs statistically significantly (p-value of Welch t-test is less than 0.0001) compared to HGNN on the other three datasets. We believe this is due to large noisy hyperedges in real-world hypergraphs. An author can write papers from different topics in a co-authorship network or a paper typically cites papers of different topics in co-citation networks.</p><p>Average sizes in Table <ref type="table" target="#tab_2">3</ref> show the presence of large hyperedges (note the large standard deviations). Clique expansion has edges on all pairs and hence potentially a larger number of hypernode pairs of different labels than the mediator graph of Figure <ref type="figure" target="#fig_1">2</ref>, thus accumulating more noise.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Preference of HyperGCN and FastHyperGCN over HGNN</head><p>To further illustrate superiority over HGNN on noisy hyperedges, we conducted experiments on synthetic hypergraphs each consisting of 1000 hypernodes, randomly sampled 500 hyperedges, and 2 classes with 500 hypernodes in each class. For each synthetic hypergraph, 100 hyperedges (each of size 5) were "pure", i.e., all hypernodes were from the same class while the other 400 hyperedges (each of size 20) contained hypernodes from both classes. The ratio, η, of hypernodes of one class to the other was varied from 0.75 (less noisy) to 0.50 (most noisy) in steps of 0.05.</p><p>Table <ref type="table" target="#tab_4">5</ref> shows the results on synthetic data. We initialise features to random Gaussian of d = 256. We report mean error and deviation over 10 different synthetically generated hypergraphs. We see that for hyperedges with η = 0.75, 0.7 (mostly pure), HGNN is the superior model because it connects more similar vertices. However, as η (noise) increases, our methods begin to outperform HGNN. Interestingly, for η = 0.50, FastHyperGCN even seems to outperform HyperGCN.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Subset of DBLP:</head><p>We also trained all three models on a subset of DBLP (we call it sDBLP) by removing all hyperedges of size 2 and 3. The resulting hypergraph has around 8000 hyperedges with an average size of 8.5 ± 8.8. We report mean error over 10 different train-test splits in Table <ref type="table" target="#tab_4">5</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Conclusion:</head><p>From the above analysis, we conclude that our proposed methods (HyperGCN and FastHyperGCN) should be preferred to HGNN for hypergraphs with large noisy hyperedges. This is also the case on experiments in combinatorial optimisation (Table <ref type="table" target="#tab_5">6</ref>) which we discuss next.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">HyperGCN for combinatorial optimisation</head><p>Inspired by the recent sucesses of deep graph models as learning-based approaches for NP-hard problems <ref type="bibr" target="#b30">[31,</ref><ref type="bibr" target="#b37">38,</ref><ref type="bibr" target="#b25">26,</ref><ref type="bibr" target="#b18">19]</ref>, we have used HyperGCN as a learning-based approach for the densest k-subhypergraph problem <ref type="bibr" target="#b14">[15]</ref>. NP-hard problems on hypergraphs have recently been highlighted as crucial for real-world network analysis <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b35">36]</ref>. Our problem is, given a hypergraph (V, E), to find a subset W ⊆ V of k hypernodes so as to maximise the number of hyperedges contained in V , i.e., we wish to maximise the density given by |e ∈ E : e ⊆ W |.</p><p>A greedy heuristic for the problem is to select the k hypernodes of the maximum degree. We call this "MaxDegree". Another greedy heuristic is to iteratively remove all hyperedges from the current (residual) hypergraph consisting of a hypernode of the minimum degree. We repeat the procedure n−k times and consider the density of the remaining k hypernodes. We call this "RemoveMinDegree".</p><p>Experiments: Table <ref type="table" target="#tab_5">6</ref> shows the results. We trained all the learning-based models with a synthetically generated dataset. More details on the approach and the synthetic data are in the supplementary. As seen in Table <ref type="table" target="#tab_5">6</ref>, our proposed HyperGCN outperforms all the other approaches except for the pubmed dataset which contains a small number of vertices with large degrees and a large number of vertices with small degrees. The RemoveMinDegree baseline is able to recover all the hyperedges here.</p><p>Visualisation: Figure <ref type="figure" target="#fig_5">3</ref> shows the visualisations given by HGNN and HyperGCN on the Cora co-authorship clique-expanded hypergraph. We used Gephi's Force Atlas to space out the vertices. In general, a cluster of nearby vertices has multiple hyperedges connecting them. Clusters of only green vertices are ideal, this means the algorithm has likely included many hyperedges induced by the clusters. The figure of HyperGCN has more dense green clusters than that of HGNN.   </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8">Comparison of training times of FastHyperGCN and HGNN</head><p>We compared the average training times in Table <ref type="table" target="#tab_0">1</ref>. Both were run on a GeForce GTX 1080 Ti GPU machine. FastHyperGCN is faster because it uses a linear number of edges for each hyperedge while HGNN uses quadratic. It is also superior in terms of performance on hypergraphs with large noisy hyperedges (Table <ref type="table" target="#tab_4">5</ref>) and higly competitive on real-world data (Tables <ref type="table" target="#tab_5">4 and 6</ref>). Please see supplementary for the algorithms and time complexities of all the proposed methods and HGNN.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="9">Conclusion</head><p>We have proposed HyperGCN, a new method of training GCN on hypergraph using tools from spectral theory of hypergraphs. We have shown HyperGCN's effectiveness in SSL and combinatorial optimisation. Approaches that assign importance to nodes <ref type="bibr" target="#b45">[46,</ref><ref type="bibr" target="#b34">35,</ref><ref type="bibr" target="#b44">45]</ref> have improved results on SSL.</p><p>HyperGCN may be augmented with such approaches for even more improved performance. One of the limitations of our approach is that the quality of the graph approximation obtained is highly dependent on the weight initialisation. We address this issue as part of future work.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="10">Acknowledgement</head><p>Anand Louis was supported in part by SERB Award ECR/2017/003296 and a Pratiksha Trust Young Investigator Award</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: Graph convolution on a hypernode v using HyperGCN.</figDesc><graphic url="image-1.png" coords="4,108.00,72.99,396.02,108.60" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: Hypergraph Laplacian [8] vs. the generalised hypergraph Laplacian with mediators [7].Our approach requires at most a linear number of edges (1 and 2|e| − 3 respectively) while HGNN<ref type="bibr" target="#b16">[17]</ref> requires a quadratic number of edges for each hyperedge.</figDesc><graphic url="image-2.png" coords="5,108.00,72.00,396.00,104.15" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>2 .</head><label>2</label><figDesc>A weighted graph G S on the vertex set V is constructed by adding edges {{i e , j e } : e ∈ E} with weights w({i e , j e }) := w(e) to G S , where w(e) is the weight of the hyperedge e. Let A S denote the weighted adjacency matrix of the graph G S . 3. The symmetrically normalised hypergraph Laplacian is L(S) := (I − D − 1 2 A S D − 1 2 )S</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Proposition 1 : 1 2|e|− 3 ,= 3 . 2 =</head><label>11332</label><figDesc>Given a hypergraph H = (V, E) with E ⊆ 2 V − ∪ v∈V {v} and signals on the vertices S : V → R d , let, for each hyperedge e ∈ E, (i e , j e ) := arg max i,j∈e ||S i − S j || 2 and K e := {v ∈ e : v = i e , v = j e }. Define• E c := e∈E {u, v} : u ∈ e, v ∈ e, u = v • w c {u, v} := e∈E 1 {u,v}∈Ec • 1 u∈e • 1 v∈e 2 |e|•(|e|−1) , • E m (S) := e∈E {i e , j e } e∈E,|e|≥3 {u, v} : u ∈ {i e , j e }, v ∈ K e • w m S, {u, v} := e∈E 1 {u,v}∈Em(S) • 1 u∈e • 1 v∈e so that G c = (V, E c , w c ) and G m (S) = (V, E m (S), w m (S)) are the normalised clique expansion, i.e., graph of HGNN and mediator expansion, i.e., graph of HyperGCN/FastHyperGCN respectively. A sufficient condition for G c = G m (S), ∀S is max e∈E |e| Proof: Observe that we consider hypergraphs in which the size of each hyperedge is at least 2. It follows from definitions that |E c | = e∈E |e| C 2 and |E m | = e∈E 2|e| − 3 . Clealy, a sufficient condition is when each hyperedge is approximated by the same subgraph in both the expansions. In other words the condition is |e|•(|e|−1) 2|e| − 3 for each e ∈ E. Solving the resulting quadratic eqution x 2 − 5x + 6 = 0 gives us (x − 2)(x − 3) = 0. Hence, |e| = 2 or |e| = 3 for each e ∈ E.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: Green / pink hypernodes denote those the algorithm labels as positive / negative respectively.</figDesc><graphic url="image-3.png" coords="9,117.64,232.98,179.22,134.41" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 :</head><label>1</label><figDesc>average training time of an epoch (lower is better)</figDesc><table><row><cell cols="5">Model↓ Metric → Training time Density Training time (DBLP) Training time (Pubmed)</cell></row><row><cell>HGNN</cell><cell>170s</cell><cell>337</cell><cell>0.115s</cell><cell>0.019s</cell></row><row><cell>FastHyperGCN</cell><cell>143s</cell><cell>352</cell><cell>0.035s</cell><cell>0.016s</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 :</head><label>2</label><figDesc>Summary of symbols used in the paper.</figDesc><table><row><cell>Symbol</cell><cell>Description</cell><cell>Symbol</cell><cell>Description</cell></row><row><cell cols="4">G = (V, E) an undirected simple graph H = (V, E) an undirected hypergraph</cell></row><row><cell>V</cell><cell>set of nodes</cell><cell>V</cell><cell>set of hypernodes</cell></row><row><cell>E</cell><cell>set of edges</cell><cell>E</cell><cell>set of hyperedges</cell></row><row><cell>N = |V|</cell><cell>number of nodes</cell><cell>n = |V |</cell><cell>number of hypernodes</cell></row><row><cell>L</cell><cell>graph Laplacian</cell><cell>L</cell><cell>hypergraph Laplacian</cell></row><row><cell>A</cell><cell>graph adjacency matrix</cell><cell>H</cell><cell>hypergraph incidence matrix</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 3 :</head><label>3</label><figDesc>Real-world hypergraph datasets used in our work. Distribution of hyperedge sizes is not symmetric either side of the mean and has a strong positive skewness.</figDesc><table><row><cell></cell><cell>DBLP</cell><cell>Pubmed</cell><cell>Cora</cell><cell>Cora</cell><cell>Citeseer</cell></row><row><cell></cell><cell cols="5">(co-authorship) (co-citation) (co-authorship) (co-citation) (co-citation)</cell></row><row><cell># hypernodes, |V |</cell><cell>43413</cell><cell>19717</cell><cell>2708</cell><cell>2708</cell><cell>3312</cell></row><row><cell># hyperedges, |E|</cell><cell>22535</cell><cell>7963</cell><cell>1072</cell><cell>1579</cell><cell>1079</cell></row><row><cell cols="2">avg. hyperedge size 4.7 ± 6.1</cell><cell>4.3 ± 5.7</cell><cell>4.2 ± 4.1</cell><cell>3.0 ± 1.1</cell><cell>3.2 ± 2.0</cell></row><row><cell># features, d</cell><cell>1425</cell><cell>500</cell><cell>1433</cell><cell>1433</cell><cell>3703</cell></row><row><cell># classes, q</cell><cell>6</cell><cell>3</cell><cell>7</cell><cell>7</cell><cell>6</cell></row><row><cell>label rate, |VL|/|V |</cell><cell>0.040</cell><cell>0.008</cell><cell>0.052</cell><cell>0.052</cell><cell>0.042</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 4 :</head><label>4</label><figDesc>Results of SSL experiments. We report mean test error ± standard deviation (lower is better) over 100 train-test splits. Please refer to section 5 for details. HyperGCN 24.09 ± 2.0 25.56 ± 1.6 30.08 ± 1.8 32.37 ± 1.7 37.35 ± 1.6</figDesc><table><row><cell cols="2">Data Method</cell><cell>DBLP</cell><cell>Pubmed</cell><cell>Cora</cell><cell>Cora</cell><cell>Citeseer</cell></row><row><cell></cell><cell></cell><cell>co-authorship</cell><cell>co-citation</cell><cell>co-authorship</cell><cell>co-citation</cell><cell>co-citation</cell></row><row><cell>H</cell><cell>CI</cell><cell>54.81 ± 0.9</cell><cell>52.96 ± 0.8</cell><cell>55.45 ± 0.6</cell><cell>64.40 ± 0.8</cell><cell>70.37 ± 0.3</cell></row><row><cell>X</cell><cell>MLP</cell><cell>37.77 ± 2.0</cell><cell>30.70 ± 1.6</cell><cell>41.25 ± 1.9</cell><cell>42.14 ± 1.8</cell><cell>41.12 ± 1.7</cell></row><row><cell cols="2">H, X MLP + HLR</cell><cell>30.42 ± 2.1</cell><cell>30.18 ± 1.5</cell><cell>34.87 ± 1.8</cell><cell>36.98 ± 1.8</cell><cell>37.75 ± 1.6</cell></row><row><cell cols="2">H, X HGNN</cell><cell>25.65 ± 2.1</cell><cell>29.41 ± 1.5</cell><cell>31.90 ± 1.9</cell><cell cols="2">32.41 ± 1.8 37.40 ± 1.6</cell></row><row><cell cols="2">H, X 1-HyperGCN</cell><cell>33.87 ± 2.4</cell><cell>30.08 ± 1.5</cell><cell>36.22 ± 2.2</cell><cell>34.45 ± 2.1</cell><cell>38.87 ± 1.9</cell></row><row><cell cols="2">H, X FastHyperGCN</cell><cell>27.34 ± 2.1</cell><cell>29.48 ± 1.6</cell><cell>32.54 ± 1.8</cell><cell cols="2">32.43 ± 1.8 37.42 ± 1.7</cell></row><row><cell>H, X</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 5 :</head><label>5</label><figDesc>Results (lower is better) on sythetic data and a subset of DBLP showing that our methods are more effective for noisy hyperedges. η is no. of hypernodes of one class divided by that of the other in noisy hyperedges. Best result is in bold and second best is underlined. Please see Section 6. ± 2.4 24.89 ± 2.2 31.32 ± 1.9 39.13 ± 1.78 42.23 ± 1.9 44.25 ± 1.8 45.27 ± 2.4 FastHyperGCN 28.86 ± 2.6 31.56 ± 2.7 33.78 ± 2.1 33.89 ± 2.0 34.56 ± 2.2 35.65 ± 2.1 41.79 ± 2.8 HyperGCN 22.44 ± 2.0 29.33 ± 2.2 33.41 ± 1.9 33.67 ± 1.9 35.05 ± 2.0 37.89 ± 1.9 41.64 ± 2.6</figDesc><table><row><cell>Method</cell><cell>η = 0.75</cell><cell>η = 0.70</cell><cell>η = 0.65</cell><cell>η = 0.60</cell><cell>η = 0.55</cell><cell>η = 0.50</cell><cell>sDBLP</cell></row><row><cell>HGNN</cell><cell>15.92</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 6 :</head><label>6</label><figDesc>Results on the densest k-subhypergraph problem. We report density (higher is better) of the set of vertices obtained by each of the proposed approaches for k = 3|V | 4 . See section 7 for details.</figDesc><table><row><cell>Dataset→</cell><cell>Synthetic</cell><cell>DBLP</cell><cell>Pubmed</cell><cell>Cora</cell><cell>Cora</cell><cell>Citeseer</cell></row><row><cell>Approach↓</cell><cell>test set</cell><cell cols="5">co-authorship co-citation co-authorship co-citation co-citation</cell></row><row><cell>MaxDegree</cell><cell>174 ± 50</cell><cell>4840</cell><cell>1306</cell><cell>194</cell><cell>544</cell><cell>507</cell></row><row><cell cols="2">RemoveMinDegree 147 ± 48</cell><cell>7714</cell><cell>7963</cell><cell>450</cell><cell>1369</cell><cell>843</cell></row><row><cell>MLP</cell><cell>174 ± 56</cell><cell>5580</cell><cell>1206</cell><cell>238</cell><cell>550</cell><cell>534</cell></row><row><cell>MLP + HLR</cell><cell>231 ± 46</cell><cell>5821</cell><cell>3462</cell><cell>297</cell><cell>952</cell><cell>764</cell></row><row><cell>HGNN</cell><cell>337 ± 49</cell><cell>6274</cell><cell>7865</cell><cell>437</cell><cell>1408</cell><cell>969</cell></row><row><cell>1-HyperGCN</cell><cell>207 ± 52</cell><cell>5624</cell><cell>1761</cell><cell>251</cell><cell>563</cell><cell>509</cell></row><row><cell>FastHyperGCN</cell><cell>352 ± 45</cell><cell>7342</cell><cell>7893</cell><cell>452</cell><cell>1419</cell><cell>969</cell></row><row><cell>HyperGCN</cell><cell>359 ± 49</cell><cell>7720</cell><cell>7928</cell><cell>504</cell><cell>1431</cell><cell>971</cell></row><row><cell># hyperedges, |E|</cell><cell>500</cell><cell>22535</cell><cell>7963</cell><cell>1072</cell><cell>1579</cell><cell>1079</cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_0">The problem of breaking ties in choosing ie (resp. je) is a non-trivial problem as shown in<ref type="bibr" target="#b7">[8]</ref>. Breaking ties randomly was proposed in<ref type="bibr" target="#b31">[32]</ref>, but<ref type="bibr" target="#b7">[8]</ref> showed that this might not work for all applications (see<ref type="bibr" target="#b7">[8]</ref> for more details).<ref type="bibr" target="#b7">[8]</ref> gave a way to break ties, and gave a proof of correctness for their tie-breaking rule for the problems they studied. We chose to break ties randomly because of its simplicity and its efficiency.</note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Higher order learning with graphs</title>
		<author>
			<persName><forename type="first">Sameer</forename><surname>Agarwal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kristin</forename><surname>Branson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Serge</forename><surname>Belongie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning (ICML)</title>
				<imprint>
			<date type="published" when="2006">2006</date>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="17" to="24" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">Planted hitting set recovery in hypergraphs</title>
		<author>
			<persName><forename type="first">Ilya</forename><surname>Amburg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jon</forename><surname>Kleinberg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Austin</forename><forename type="middle">R</forename><surname>Benson</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1905.05839</idno>
		<imprint>
			<date type="published" when="1928">2019. 2 and 8</date>
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Diffusion-convolutional neural networks</title>
		<author>
			<persName><forename type="first">James</forename><surname>Atwood</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Don</forename><surname>Towsley</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Neural Information Processing Systems (NIPS)</title>
				<imprint>
			<publisher>Curran Associates, Inc</publisher>
			<date type="published" when="2001">2016. 1</date>
			<biblScope unit="page" from="1993" to="2001" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<author>
			<persName><forename type="first">Peter</forename><forename type="middle">W</forename><surname>Battaglia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jessica</forename><surname>Hamrick</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Victor</forename><surname>Bapst</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alvaro</forename><surname>Sanchez-Gonzalez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vinícius</forename><surname>Flores Zambaldi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mateusz</forename><surname>Malinowski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrea</forename><surname>Tacchetti</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><surname>Raposo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Adam</forename><surname>Santoro</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ryan</forename><surname>Faulkner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Çaglar</forename><surname>Gülçehre</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Francis</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrew</forename><surname>Ballard</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Justin</forename><surname>Gilmer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">George</forename><surname>Dahl</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ashish</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kelsey</forename><surname>Allen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Charles</forename><surname>Nash</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Victoria</forename><surname>Langston</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chris</forename><surname>Dyer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nicolas</forename><surname>Heess</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Daan</forename><surname>Wierstra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pushmeet</forename><surname>Kohli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Matthew</forename><surname>Botvinick</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1806.01261</idno>
		<title level="m">Oriol Vinyals, Yujia Li, and Razvan Pascanu. Relational inductive biases, deep learning, and graph networks</title>
				<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">Geometric deep learning: Beyond euclidean data</title>
		<author>
			<persName><forename type="first">Michael</forename><surname>Bronstein</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Joan</forename><surname>Bruna</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yann</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Arthur</forename><surname>Szlam</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pierre</forename><surname>Vandergheynst</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017">2017</date>
			<publisher>IEEE Signal Process</publisher>
			<biblScope unit="volume">2</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">A game-theoretic approach to hypergraph clustering</title>
		<author>
			<persName><forename type="first">R</forename><surname>Samuel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Marcello</forename><surname>Bulò</surname></persName>
		</author>
		<author>
			<persName><surname>Pelillo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems (NIPS)</title>
				<imprint>
			<publisher>Curran Associates, Inc</publisher>
			<date type="published" when="2009">2009</date>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="page" from="1571" to="1579" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Generalizing the hypergraph laplacian via a diffusion process with mediators</title>
		<author>
			<persName><forename type="first">T.-H</forename><surname>Hubert Chan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhibin</forename><surname>Liang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computing and Combinatorics -24th International Conference, (COCOON)</title>
				<imprint>
			<date type="published" when="2006">2018. 3, 5, and 6</date>
			<biblScope unit="page" from="441" to="453" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Spectral properties of hypergraph laplacian and approximation algorithms</title>
		<author>
			<persName><forename type="first">T.-H</forename><surname>Hubert Chan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Anand</forename><surname>Louis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhihao</forename><surname>Gavin Tang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chenzi</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. ACM</title>
		<imprint>
			<biblScope unit="volume">65</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="1" to="15" />
			<date type="published" when="2005">2018. 2, 3, 4, and 5</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Diffusion operator and spectral analysis for directed hypergraph laplacian</title>
		<author>
			<persName><forename type="first">T.-H</forename><surname>Hubert Chan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhihao</forename><surname>Gavin Tang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaowei</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chenzi</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Theor. Comput. Sci</title>
		<imprint>
			<biblScope unit="page">3</biblScope>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">Semi-Supervised Learning</title>
		<author>
			<persName><forename type="first">Olivier</forename><surname>Chapelle</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bernhard</forename><surname>Scholkopf</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alexander</forename><surname>Zien</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2010">2010</date>
			<publisher>The MIT Press</publisher>
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Cluster kernels for semi-supervised learning</title>
		<author>
			<persName><forename type="first">Olivier</forename><surname>Chapelle</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jason</forename><surname>Weston</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bernhard</forename><surname>Schölkopf</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Neural Information Processing Systems (NIPS)</title>
				<imprint>
			<publisher>MIT</publisher>
			<date type="published" when="2003">2003</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="601" to="608" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Multi-label image recognition with graph convolutional networks</title>
		<author>
			<persName><forename type="first">Xiu-Shen</forename><surname>Zhao-Min Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Peng</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yanwen</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><surname>Guo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
				<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Active learning over hypergraphs with pointwise and pairwise queries</title>
		<author>
			<persName><forename type="first">I</forename></persName>
		</author>
		<author>
			<persName><forename type="first">Eli</forename><forename type="middle">)</forename><surname>Chien</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Huozhi</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pan</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Artificial Intelligence and Statistics (AISTATS)</title>
				<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="2466" to="2475" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Random walks on hypergraphs with edge-dependent vertex weights</title>
		<author>
			<persName><forename type="first">Uthsav</forename><surname>Chitra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Benjamin</forename><forename type="middle">J</forename><surname>Raphael</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 36th International Conference on Machine Learning (ICML)</title>
				<meeting>the 36th International Conference on Machine Learning (ICML)</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">The densest k-subhypergraph problem</title>
		<author>
			<persName><forename type="first">Eden</forename><surname>Chlamtác</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><surname>Dinitz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christian</forename><surname>Konrad</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Guy</forename><surname>Kortsarz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">George</forename><surname>Rabanca</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">SIAM J. Discrete Math</title>
		<imprint>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="1458" to="1477" />
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Learning on partialorder hypergraphs</title>
		<author>
			<persName><forename type="first">Fuli</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiangnan</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yiqun</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Liqiang</forename><surname>Nie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tat-Seng</forename><surname>Chua</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2018 World Wide Web Conference (WWW)</title>
				<meeting>the 2018 World Wide Web Conference (WWW)</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="1523" to="1532" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Hypergraph neural networks</title>
		<author>
			<persName><forename type="first">Yifan</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Haoxuan</forename><surname>You</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zizhao</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rongrong</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yue</forename><surname>Gao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Thirty-Third Conference on Association for the Advancement of Artificial Intelligence (AAAI)</title>
				<meeting>the Thirty-Third Conference on Association for the Advancement of Artificial Intelligence (AAAI)</meeting>
		<imprint>
			<date type="published" when="2006">2019. 1, 2, 5, and 6</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Neural message passing for quantum chemistry</title>
		<author>
			<persName><forename type="first">Justin</forename><surname>Gilmer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Samuel</forename><forename type="middle">S</forename><surname>Schoenholz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Patrick</forename><forename type="middle">F</forename><surname>Riley</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Oriol</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName><forename type="first">George</forename><forename type="middle">E</forename><surname>Dahl</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 34th International Conference on Machine Learning (ICML)</title>
				<meeting>the 34th International Conference on Machine Learning (ICML)</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="1263" to="1272" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Exact-k recommendation via maximal clique optimization</title>
		<author>
			<persName><forename type="first">Yu</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yu</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lu</forename><surname>Duan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qingwen</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ziyu</forename><surname>Guan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fei</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wenwu</forename><surname>Ou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kenny</forename><forename type="middle">Q</forename><surname>Zhu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">KDD</title>
				<imprint>
			<date type="published" when="2008">2019. 2, 3, and 8</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Representation learning on graphs: Methods and applications</title>
		<author>
			<persName><forename type="first">William</forename><forename type="middle">L</forename><surname>Hamilton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rex</forename><surname>Ying</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jure</forename><surname>Leskovec</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Data Eng. Bull</title>
		<imprint>
			<biblScope unit="volume">40</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="52" to="74" />
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Wavelets on graphs via spectral graph theory</title>
		<author>
			<persName><forename type="first">David</forename><forename type="middle">K</forename><surname>Hammond</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pierre</forename><surname>Vandergheynst</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rémi</forename><surname>Gribonval</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Applied and Computational Harmonic Analysis</title>
		<imprint>
			<biblScope unit="issue">3</biblScope>
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">The total variation on hypergraphs -learning on hypergraphs revisited</title>
		<author>
			<persName><forename type="first">Matthias</forename><surname>Hein</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Simon</forename><surname>Setzer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Leonardo</forename><surname>Jost</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Syama</forename><surname>Sundar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rangapuram</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems (NIPS)</title>
				<imprint>
			<publisher>Curran Associates, Inc</publisher>
			<date type="published" when="2006">2013. 1, 2, 3, 5, and 6</date>
			<biblScope unit="volume">26</biblScope>
			<biblScope unit="page" from="2427" to="2435" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Dynamic hypergraph neural networks</title>
		<author>
			<persName><forename type="first">Jianwen</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuxuan</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yifan</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jingxuan</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yue</forename><surname>Gao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Twenty-Eighth International Joint Conference on Artificial Intelligence (IJCAI)</title>
				<meeting>the Twenty-Eighth International Joint Conference on Artificial Intelligence (IJCAI)</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="2635" to="2641" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Hypergraph induced convolutional manifold networks</title>
		<author>
			<persName><forename type="first">Taisong</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Liujuan</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Baochang</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaoshuai</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Cheng</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rongrong</forename><surname>Ji</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Twenty-Eighth International Joint Conference on Artificial Intelligence (IJCAI)</title>
				<meeting>the Twenty-Eighth International Joint Conference on Artificial Intelligence (IJCAI)</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="2670" to="2676" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Semi-supervised classification with graph convolutional networks</title>
		<author>
			<persName><forename type="first">N</forename><surname>Thomas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Max</forename><surname>Kipf</surname></persName>
		</author>
		<author>
			<persName><surname>Welling</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations (ICLR)</title>
				<imprint>
			<date type="published" when="2006">2017. 1, 2, 3, and 6</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Graph colouring meets deep learning: Effective graph neural network models for combinatorial problems</title>
		<author>
			<persName><forename type="first">Henrique</forename><surname>Lemos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Marcelo</forename><surname>Prates</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pedro</forename><surname>Avelar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Luis</forename><surname>Lamb</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 28th International Joint Conference on Artificial Intelligence, (IJCAI)</title>
				<meeting>the 28th International Joint Conference on Artificial Intelligence, (IJCAI)</meeting>
		<imprint>
			<date type="published" when="2008">2019. 2, 3, and 8</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Quadratic decomposable submodular function minimization</title>
		<author>
			<persName><forename type="first">Pan</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Niao</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Olgica</forename><surname>Milenkovic</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems (NeurIPS) 31</title>
				<imprint>
			<publisher>Curran Associates, Inc</publisher>
			<date type="published" when="1936">2018. 3 and 6</date>
			<biblScope unit="page" from="1054" to="1064" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Inhomogeneous hypergraph clustering with applications</title>
		<author>
			<persName><forename type="first">Pan</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Olgica</forename><surname>Milenkovic</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems (NIPS)</title>
				<imprint>
			<publisher>Curran Associates, Inc</publisher>
			<date type="published" when="2017">2017</date>
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="page" from="2308" to="2318" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Revisiting decomposable submodular function minimization with incidence relations</title>
		<author>
			<persName><forename type="first">Pan</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Olgica</forename><surname>Milenkovic</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems (NeurIPS) 31</title>
				<imprint>
			<publisher>Curran Associates, Inc</publisher>
			<date type="published" when="1936">2018. 3 and 6</date>
			<biblScope unit="page" from="2237" to="2247" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Submodular hypergraphs: p-laplacians, Cheeger inequalities and spectral clustering</title>
		<author>
			<persName><forename type="first">Pan</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Olgica</forename><surname>Milenkovic</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 35th International Conference on Machine Learning (ICML)</title>
				<meeting>the 35th International Conference on Machine Learning (ICML)</meeting>
		<imprint>
			<date type="published" when="1936">2018. 3 and 6</date>
			<biblScope unit="page" from="3014" to="3023" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Combinatorial optimization with graph convolutional networks and guided tree search</title>
		<author>
			<persName><forename type="first">Zhuwen</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qifeng</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vladlen</forename><surname>Koltun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems (NIPS) 31</title>
				<imprint>
			<publisher>Curran Associates, Inc</publisher>
			<date type="published" when="2008">2018. 2, 3, and 8</date>
			<biblScope unit="page" from="537" to="546" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Hypergraph markov operators, eigenvalues and approximation algorithms</title>
		<author>
			<persName><forename type="first">Anand</forename><surname>Louis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Forty-Seventh Annual ACM on Symposium on Theory of Computing, (STOC)</title>
				<meeting>the Forty-Seventh Annual ACM on Symposium on Theory of Computing, (STOC)</meeting>
		<imprint>
			<date type="published" when="2004">2015. 2, 3, and 4</date>
			<biblScope unit="page" from="713" to="722" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
		<title level="m" type="main">A Wavelet Tour of Signal Processing</title>
		<author>
			<persName><forename type="first">Stphane</forename><surname>Mallat</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1999">1999</date>
			<publisher>Academic Press</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Encoding sentences with graph convolutional networks for semantic role labeling</title>
		<author>
			<persName><forename type="first">Diego</forename><surname>Marcheggiani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ivan</forename><surname>Titov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing (EMNLP)</title>
				<meeting>the 2017 Conference on Empirical Methods in Natural Language Processing (EMNLP)</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="1506" to="1515" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<monogr>
		<title level="m" type="main">Dual-primal graph convolutional</title>
		<author>
			<persName><forename type="first">Federico</forename><surname>Monti</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Oleksandr</forename><surname>Shchur</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aleksandar</forename><surname>Bojchevski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Or</forename><surname>Litany</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stephan</forename><surname>Günnemann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><surname>Bronstein</surname></persName>
		</author>
		<idno>networks. abs/1806.00770</idno>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<monogr>
		<title level="m" type="main">Approximate k-cover in hypergraphs: Efficient algorithms, and applications</title>
		<author>
			<persName><forename type="first">Hung</forename><surname>Nguyen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Phuc</forename><surname>Thai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">My</forename><surname>Thai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tam</forename><surname>Vu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Thang</forename><surname>Dinh</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1901.07928</idno>
		<imprint>
			<date type="published" when="1928">2019. 2 and 8</date>
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Learning conditioned graph structures for interpretable visual question answering</title>
		<author>
			<persName><forename type="first">Will</forename><surname>Norcliffe-Brown</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Efstathios</forename><surname>Vafeias</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sarah</forename><surname>Parisot</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems (NeurIPS) 31</title>
				<imprint>
			<publisher>Curran Associates, Inc</publisher>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="8344" to="8353" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Learning to solve np-complete problems -a graph neural network for the decision tsp</title>
		<author>
			<persName><forename type="first">O</forename><forename type="middle">R</forename><surname>Marcelo</surname></persName>
		</author>
		<author>
			<persName><surname>Prates</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">C</forename><surname>Pedro</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Henrique</forename><surname>Avelar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Luis</forename><surname>Lemos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Moshe</forename><surname>Lamb</surname></persName>
		</author>
		<author>
			<persName><surname>Vardi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Thirty-Third Conference on Association for the Advancement of Artificial Intelligence (AAAI)</title>
				<meeting>the Thirty-Third Conference on Association for the Advancement of Artificial Intelligence (AAAI)</meeting>
		<imprint>
			<date type="published" when="2008">2019. 2, 3, and 8</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Extended discriminative random walk: A hypergraph approach to multi-view multi-relational transductive learning</title>
		<author>
			<persName><forename type="first">Harini</forename><surname>Sai Nageswar Satchidanand</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Balaraman</forename><surname>Ananthapadmanaban</surname></persName>
		</author>
		<author>
			<persName><surname>Ravindran</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IJCAI</title>
				<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="3791" to="3797" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Multi-way clustering using super-symmetric non-negative tensor factorization</title>
		<author>
			<persName><forename type="first">Amnon</forename><surname>Shashua</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ron</forename><surname>Zass</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tamir</forename><surname>Hazan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 9th European Conference on Computer Vision (ECCV)</title>
				<meeting>the 9th European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2006">2006</date>
			<biblScope unit="page" from="595" to="608" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">The emerging field of signal processing on graphs: Extending high-dimensional data analysis to networks and other irregular domains</title>
		<author>
			<persName><forename type="first">David</forename><forename type="middle">I</forename><surname>Shuman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Sunil</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pascal</forename><surname>Narang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Antonio</forename><surname>Frossard</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pierre</forename><surname>Ortega</surname></persName>
		</author>
		<author>
			<persName><surname>Vandergheynst</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Signal Process. Mag</title>
		<imprint>
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="issue">3</biblScope>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<monogr>
		<title level="m" type="main">Graph-Based Semi-Supervised Learning</title>
		<author>
			<persName><forename type="first">Amarnag</forename><surname>Subramanya</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Partha</forename><surname>Pratim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Talukdar</forename></persName>
		</author>
		<imprint>
			<date type="published" when="2014">2014</date>
			<publisher>Morgan &amp; Claypool Publishers</publisher>
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Structural deep embedding for hypernetworks</title>
		<author>
			<persName><forename type="first">Ke</forename><surname>Tu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Peng</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiao</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fei</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wenwu</forename><surname>Zhu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Thirty-Second Conference on Association for the Advancement of Artificial Intelligence (AAAI)</title>
				<meeting>the Thirty-Second Conference on Association for the Advancement of Artificial Intelligence (AAAI)</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Incorporating syntactic and semantic information in word embeddings using graph convolutional networks</title>
		<author>
			<persName><forename type="first">Shikhar</forename><surname>Vashishth</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Manik</forename><surname>Bhandari</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Prateek</forename><surname>Yadav</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Piyush</forename><surname>Rai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chiranjib</forename><surname>Bhattacharyya</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Partha</forename><surname>Talukdar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics (ACL)</title>
				<meeting>the 57th Annual Meeting of the Association for Computational Linguistics (ACL)</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Confidence-based graph convolutional networks for semi-supervised learning</title>
		<author>
			<persName><forename type="first">Shikhar</forename><surname>Vashishth</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Prateek</forename><surname>Yadav</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Manik</forename><surname>Bhandari</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Partha</forename><surname>Talukdar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Artificial Intelligence and Statistics (AISTATS)</title>
				<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Graph attention networks</title>
		<author>
			<persName><forename type="first">Petar</forename><surname>Veličković</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Guillem</forename><surname>Cucurull</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Arantxa</forename><surname>Casanova</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Adriana</forename><surname>Romero</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pietro</forename><surname>Liò</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
				<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Powerset convolutional neural networks</title>
		<author>
			<persName><forename type="first">Chris</forename><surname>Wendler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dan</forename><surname>Alistarh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Markus</forename><surname>Püschel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems (NeurIPS)</title>
				<imprint>
			<publisher>Curran Associates, Inc</publisher>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page">32</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Deep learning via semi-supervised embedding</title>
		<author>
			<persName><forename type="first">Jason</forename><surname>Weston</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Frédéric</forename><surname>Ratle</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ronan</forename><surname>Collobert</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 25th International Conference on Machine Learning (ICML)</title>
				<meeting>the 25th International Conference on Machine Learning (ICML)</meeting>
		<imprint>
			<date type="published" when="2008">2008</date>
			<biblScope unit="page" from="1168" to="1175" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Re-revisiting learning on hypergraphs: Confidence interval and subgradient method</title>
		<author>
			<persName><forename type="first">Chenzi</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shuguang</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhihao</forename><surname>Gavin Tang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T-H</forename><surname>Hubert Chan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of 34th International Conference on Machine Learning (ICML)</title>
				<meeting>34th International Conference on Machine Learning (ICML)</meeting>
		<imprint>
			<date type="published" when="2006">2017. 1, 3, 4, and 6</date>
			<biblScope unit="page" from="4026" to="4034" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Beyond link prediction: Predicting hyperlinks in adjacency space</title>
		<author>
			<persName><forename type="first">Muhan</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhicheng</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shali</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yixin</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Thirty-Second Conference on Association for the Advancement of Artificial Intelligence (AAAI)</title>
				<meeting>the Thirty-Second Conference on Association for the Advancement of Artificial Intelligence (AAAI)</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Learning with local and global consistency</title>
		<author>
			<persName><forename type="first">Dengyong</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Olivier</forename><surname>Bousquet</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jason</forename><surname>Thomas Navin Lal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bernhard</forename><surname>Weston</surname></persName>
		</author>
		<author>
			<persName><surname>Schölkopf</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
				<imprint>
			<date type="published" when="2003">2003</date>
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Learning with hypergraphs: Clustering, classification, and embedding</title>
		<author>
			<persName><forename type="first">Denny</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiayuan</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bernhard</forename><surname>Schölkopf</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems (NIPS) 19</title>
				<imprint>
			<publisher>MIT Press</publisher>
			<date type="published" when="2006">2007. 1, 2, and 6</date>
			<biblScope unit="page" from="1601" to="1608" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">Semi-supervised learning using gaussian fields and harmonic functions</title>
		<author>
			<persName><forename type="first">Xiaojin</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zoubin</forename><surname>Ghahramani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">John</forename><surname>Lafferty</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
				<imprint>
			<date type="published" when="2003">2003</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<monogr>
		<title level="m" type="main">Introduction to Semi-Supervised Learning</title>
		<author>
			<persName><forename type="first">Xiaojin</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrew</forename><forename type="middle">B</forename><surname>Goldberg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ronald</forename><surname>Brachman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Thomas</forename><surname>Dietterich</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2009">2009</date>
			<publisher>Morgan and Claypool Publishers</publisher>
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
