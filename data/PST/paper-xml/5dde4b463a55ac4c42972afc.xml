<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">One Man&apos;s Trash is Another Man&apos;s Treasure: Resisting Adversarial Examples by Adversarial Examples</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date type="published" when="2019-11-27">27 Nov 2019</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Chang</forename><surname>Xiao</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Changxi Zheng Columbia University</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">One Man&apos;s Trash is Another Man&apos;s Treasure: Resisting Adversarial Examples by Adversarial Examples</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2019-11-27">27 Nov 2019</date>
						</imprint>
					</monogr>
					<idno type="arXiv">arXiv:1911.11219v2[cs.LG]</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.2" ident="GROBID" when="2022-12-25T13:44+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Modern image classification systems are often built on deep neural networks, which suffer from adversarial examples-images with deliberately crafted, imperceptible noise to mislead the network's classification. To defend against adversarial examples, a plausible idea is to obfuscate the network's gradient with respect to the input image. This general idea has inspired a long line of defense methods. Yet, almost all of them have proven vulnerable.</head><p>We revisit this seemingly flawed idea from a radically different perspective. We embrace the omnipresence of adversarial examples and the numerical procedure of crafting them, and turn this harmful attacking process into a useful defense mechanism. Our defense method is conceptually simple: before feeding an input image for classification, transform it by finding an adversarial example on a pretrained external model. We evaluate our method against a wide range of possible attacks. On both CIFAR-10 and Tiny ImageNet datasets, our method is significantly more robust than state-of-the-art methods. Particularly, in comparison to adversarial training, our method offers lower training cost as well as stronger robustness.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Input image</head><p>Adversarially transformed image Bird</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Deep neural networks have vastly improved the performance of image classification systems. Yet they are prone to adversarial examples. Those are natural images with deliberately crafted, imperceptible noise, aiming to mislead the network's decision entirely <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b40">41]</ref>. In numerous applications, from face recognition authorization to autonomous cars <ref type="bibr" target="#b35">[36,</ref><ref type="bibr" target="#b42">43]</ref>, the vulnerability caused by adversarial examples gives rise to serious security concerns and presses for efficient defense mechanisms.</p><p>The defense, unfortunately, remains grim. Recent studies <ref type="bibr" target="#b33">[34,</ref><ref type="bibr" target="#b44">45,</ref><ref type="bibr" target="#b11">12]</ref> suggest that the prevalence of adversarial examples may be an inherent property of high-dimensional natural data distributions. Facing this intrinsic difficulty of eliminating adversarial examples, a plausible thought is to conceal them-making them hard to find. Indeed, a long line of works aims to obfuscate the network model's gradient with respect to its input <ref type="bibr" target="#b49">[50,</ref><ref type="bibr" target="#b13">14,</ref><ref type="bibr" target="#b48">49,</ref><ref type="bibr" target="#b6">7,</ref><ref type="bibr" target="#b38">39,</ref><ref type="bibr" target="#b32">33]</ref>, motivated by the fact that the gradient information is essential for crafting adversarial examples: the gradient indicates how to perturb the input to alter the network's decision.</p><p>Yet, almost all these gradient obfuscation based defenses have proven vulnerable. In their recent seminal work, Athalye et al. <ref type="bibr" target="#b1">[2]</ref> presented a suite of strategies for estimating network gradients in the presence of gradient obfuscation. Adversarial examples crafted by their method have successfully fooled many existing defense models, some of which even yield 0% accuracy under their attack.</p><p>We revisit the idea of gradient obfuscation but take a radically different approach. Instead of expelling adversarial examples, we embrace them. Instead of obstructing the way of finding adversarial examples on a model, we exploit it to strengthen the robustness of another model.</p><p>Our defense is conceptually simple: before feeding an input image to a classification model, we transform it through the process of finding adversarial examples on an external model. Mathematically, if we use f (x) to denote the model that classifies an input image x, our defense model is expressed as f (g(x)), where g(â€¢) represents the process of finding an adversarial example near x on a pre-trained external model (see Fig. <ref type="figure" target="#fig_0">1</ref>).</p><p>The robustness of our defense model f (g(x)) stems from the fundamental difficulties of estimating the gradient of g(x) with respect to x. Finding an adversarial example amounts to searching for a local minimum on a highly fluctuated objective landscape <ref type="bibr" target="#b25">[26]</ref>. As a result, g(x) is not an analytic function, not smooth, not deterministic, but an iterative procedure with random initialization and non-differentiable operators. We show that all these traits together constitute a highly robust defense mechanism.</p><p>We play devil's advocate in attacking our defense model thoroughly. We examine a wide range of possible attacks, including those having successfully circumvented many previous defenses <ref type="bibr" target="#b1">[2]</ref>. Under these attacks, we compare the worst-case robustness of our method with state-of-the-art defense methods on both CIFAR-10 and Tiny ImageNet datasets. Our defense demonstrates superior robustness over those methods. Particularly, in comparison to models optimized with adversarial training-by far the most effective defense against white-box attacks-our method offers simultaneously lower training cost and stronger robustness.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Work</head><p>Adversarial attack. The seminal work of Biggio et al. <ref type="bibr" target="#b4">[5]</ref> and Szegedy et al. <ref type="bibr" target="#b40">[41]</ref> first suggested the existence of adversarial examples that can mislead deep neural networks. The latter also used a constrained L-BFGS to find adversarial examples. Goodfellow et al. <ref type="bibr" target="#b12">[13]</ref> later introduced Fast Gradient Sign Method (FGSM) that generates adversarial examples more efficiently. Madry et al. <ref type="bibr" target="#b25">[26]</ref> further formalized the problem of adversarial attacks and proposed Projected Gradient Descent (PGD) method, which further inspires many subsequent attacking methods <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b7">8,</ref><ref type="bibr" target="#b27">28,</ref><ref type="bibr" target="#b18">19]</ref>. PGDtype methods are considered the strongest attacks based on first-order information, namely the network's gradient with respect to the input <ref type="bibr" target="#b25">[26]</ref>. To compute the gradients, the adversary must have full access to the network structure and parameters. This scenario is referred as the white-box attack.</p><p>When the adversary has no knowledge about the model, the attack, referred as black-box attack, is not as easy as the white-box attack. By far the most popular black-box attack is the so-called transfer attack, which uses adversarial examples generated on a known model (e.g., using PGD) to attack an unknown model <ref type="bibr" target="#b29">[30]</ref>. Several methods (e.g., <ref type="bibr" target="#b43">[44,</ref><ref type="bibr" target="#b2">3,</ref><ref type="bibr" target="#b51">52,</ref><ref type="bibr" target="#b15">16]</ref>) are proposed to improve the transferability of the adversarial examples so that the adversarial examples generated on one model are more likely to fool another model. Another type of black-box attacking methods is query-based <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b36">37,</ref><ref type="bibr" target="#b8">9,</ref><ref type="bibr" target="#b0">1,</ref><ref type="bibr" target="#b19">20]</ref>: they execute the model many times with different input in order to learn the behavior of the model and construct adversarial examples.</p><p>While our defense is motivated by attacks in white-box scenarios, we evaluate our method under a wide range of possibilities, including both white-box and black-box attacks.</p><p>Adversarial defense. The threat of adversarial examples has motivated active studies of defense mechanisms. By far the most successful defense against white-box attacks is adversarial training <ref type="bibr" target="#b25">[26,</ref><ref type="bibr" target="#b12">13,</ref><ref type="bibr" target="#b37">38]</ref>, and a rich set of methods has been proposed to accelerate its training speed or further improve its robustness <ref type="bibr" target="#b50">[51,</ref><ref type="bibr" target="#b53">54,</ref><ref type="bibr" target="#b20">21,</ref><ref type="bibr" target="#b12">13,</ref><ref type="bibr" target="#b45">46,</ref><ref type="bibr" target="#b34">35,</ref><ref type="bibr" target="#b52">53,</ref><ref type="bibr" target="#b28">29,</ref><ref type="bibr" target="#b26">27]</ref>. In comparison to adversarial training, our method offers both stronger robustness and lower training cost.</p><p>To defend against gradient-based attacks (such as the PGD attack), a natural idea is to obfuscate (or mask) network gradients <ref type="bibr" target="#b29">[30,</ref><ref type="bibr" target="#b43">44]</ref>. To this end, there exist a long line of works that apply random transformation to input images <ref type="bibr" target="#b49">[50,</ref><ref type="bibr" target="#b13">14]</ref>, or employ stochastic activation functions <ref type="bibr" target="#b9">[10]</ref> and nondifferentiable operators in the model <ref type="bibr" target="#b48">[49,</ref><ref type="bibr" target="#b6">7,</ref><ref type="bibr" target="#b38">39,</ref><ref type="bibr" target="#b32">33]</ref>.</p><p>Unfortunately, many of these methods have proven vulnerable by Athalye et al. <ref type="bibr" target="#b1">[2]</ref>, who introduced a set of attacking strategies, including a method called Backward Pass Differentiable Approximation (BPDA), to circumvent gradient obfuscation (see further discussion in Sec. 3.1 and 3.3). Since then, a few other gradient obfuscation based defenses have been proposed <ref type="bibr" target="#b22">[23,</ref><ref type="bibr" target="#b30">31,</ref><ref type="bibr" target="#b16">17,</ref><ref type="bibr" target="#b41">42,</ref><ref type="bibr" target="#b21">22]</ref>. But those works either report degraded robustness under BPDA attacks <ref type="bibr" target="#b22">[23,</ref><ref type="bibr" target="#b30">31]</ref> or neglected the evaluation against BPDA attacks <ref type="bibr" target="#b16">[17,</ref><ref type="bibr" target="#b41">42,</ref><ref type="bibr" target="#b21">22]</ref>.</p><p>Thus far, gradient obfuscation is generally considered vulnerable (and at least incomplete) <ref type="bibr" target="#b1">[2]</ref>. We revisit gradient obfuscation, and our defense demonstrates unprecedented robustness against BPDA and other possible attacks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Defense via Adversarial Transformation</head><p>We now present a simple approach to defend against adversarial attacks. We will first motivate and describe our adversarial transformation (Sec. 3.1 and 3.2), and then provide the rationale of why it improves adversarial robustness (Sec. 3.3 and 3.4), backed by empirical evidence (Sec. 4).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Motivation: Input Transformation</head><p>An attempt that has been explored in adversarial defensealbeit unsuccessfully so far-is the defense via input transformation. Consider a neural network model f a that classifies the input image x (i.e., evaluating f a (x)). Instead of feeding x into f a directly, this defense approach transforms the input image through an operator g before presenting it to the classification model (i.e., evaluating f a (g(x))).</p><p>The transformation g is applied in both training and inference. Provided a training dataset X , the network weights Î¸ are optimized by solving</p><formula xml:id="formula_0">Î¸ * = arg min Î¸ E (x,y)âˆˆX [â„“(f a (g(x); Î¸), y)] ,<label>(1)</label></formula><p>where x and y are respectively the image and its corresponding label drawn from the training dataset, and â„“ is the loss function (such as cross entropy for classification tasks). Correspondingly, at inference time, the model predicts the label of an input image x by evaluating f a (g(x)).</p><p>Input transformation g(â€¢) offers an opportunity to implement the idea of gradient obfuscation. For example, by transforming the input image with certain randomness such as random resizing and padding <ref type="bibr" target="#b49">[50]</ref>, the network gradients become hard to estimate.</p><p>Another use of g(â€¢) for defense is to remove the noise (or perturbations) in adversarial examples. For instance, g(â€¢) has been used to restore a natural image from a potentially adversarial input, by projecting it on a GAN-or PixelCNNrepresented image manifold <ref type="bibr" target="#b32">[33,</ref><ref type="bibr" target="#b38">39]</ref> or regularizing the input image through total variation minimization <ref type="bibr" target="#b13">[14]</ref>.</p><p>These input-transformation-based defense mechanisms seem plausible. Yet they are all fragile. As demonstrated by Athalye et al. <ref type="bibr" target="#b1">[2]</ref>, with random input transformation, adversarial examples can still be found using Expectation over Transformation <ref type="bibr" target="#b2">[3]</ref>, which estimates the network gradient by taking the average over multiple trials (more details in Sec. 3.3). The noise-removal transformation is also ineffective. One can use Backward Pass Differentiable Approximation <ref type="bibr" target="#b1">[2]</ref> to easily construct effective adversarial examples. In short, the current consensus is that input transformation as a defense mechanism remains vulnerable.</p><p>We challenge this consensus. We now present a new input transformation method for gradient obfuscation, followed by the explanation of why it is able to avoid the shortcomings of prior work and offer stronger adversarial robustness.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Adversarial Transformation</head><p>Our input transformation operation takes an approach opposite to the intuition behind previous methods <ref type="bibr" target="#b13">[14,</ref><ref type="bibr" target="#b38">39,</ref><ref type="bibr" target="#b32">33]</ref>. In contrast to those aiming to purge input images of the adversarial noise, we embrace adversarial noise. As we will show, our transformation injects noticeably strong adversarial noise into the input image. This seemingly counter-intuitive operation is able to strengthen the network model in training, making it more robust.</p><p>Our transformation operation relies on another network model f b , whose choice will be discussed later in Sec. 3.4. The model f b is pre-trained to perform the same task as f a . Then, given an input image x, the transformation operator g(â€¢) is defined as the process that finds the adversarial example nearby x to fool f b . Formally, this process is meant to reach a local minimum of the optimization problem,</p><formula xml:id="formula_1">g(x) = arg min x â€² âˆˆâˆ†x â„“(f b (x â€² ), y L ),<label>(2)</label></formula><p>where â„“(â€¢) is the loss function as used in network training (1); and y L is the adversarial target, setting to be the input x's least likely class predicted by f b . The adversarial examples are restricted in âˆ† x , an L âˆž -ball at x, defined as x â€² âˆ’ x âˆž &lt; âˆ†. The perturbation range âˆ† is a hyperparameter. Transformation g(x) defined in (2) can be implemented using any gradient-based attacking methods (such as Deepfool <ref type="bibr" target="#b27">[28]</ref> and C&amp;W <ref type="bibr" target="#b7">[8]</ref>). We choose to use the least-likely  class projected gradient descent (LL-PGD) method <ref type="bibr" target="#b18">[19,</ref><ref type="bibr" target="#b43">44]</ref>. LL-PGD is an iterative process, wherein each iteration updates the adversarial example by the rule,</p><formula xml:id="formula_2">x â€² t = Î  x â€² âˆˆâˆ†x x â€² tâˆ’1 âˆ’ Ç« â€¢ sgn(âˆ‡ x â„“(f b (x â€² tâˆ’1 ), y L )) .<label>(3)</label></formula><p>Here x â€² t denotes the adversarial example after t iterations; sgn(â€¢) is the sign function, and Î  x â€² âˆˆâˆ†x [â€¢] projects the image back into the allowed perturbation ball âˆ† x . This iterative process starts from a random perturbation of input image x, namely x + Î´, where each element (pixel</p><formula xml:id="formula_3">) in Î´ is uniformly drawn from [âˆ’âˆ†, âˆ†]. The output x â€² N (after N iterations) is the transformed version of x. In other words, g(x) = x â€²</formula><p>N , which we refer as adversarial transformation.</p><p>After defining the adversarial transformation g(â€¢) based on the pre-trained model f b , we use g(â€¢) to train the model f a as described in <ref type="bibr" target="#b0">(1)</ref>. At inference time, the label of an image x is predicted as f a (g(x)). The consequence of using a fixed external model f b for adversarial transformation is substantial. As we will discuss in Sec. 3.4, f b can be chosen much simpler than f a . As a result, crafting the adversarial examples on f b has lower cost than that on f a , and thus our training process is faster than adversarial training (see experiments in Sec. 5). More remarkably, the adversarial transformation using f b makes the model f a much harder to attack, as explained next.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Rationale behind Adversarial Transformation</head><p>Embracing adversarial noise. Given an input image x, our adversarial transformation effectively adds perturbation noise to x. The perturbation range âˆ† controls how much noise is added. Normally, in adversarial attacks, âˆ† is set small to generate adversarial examples perceptually similar to the input image. But when we use adversarial attacks (on f b ) as a means of input transformation for training f a , we have the freedom to use a much larger âˆ†, thereby adding noticeably stronger adversarial noise (see Fig. <ref type="figure" target="#fig_2">2-c</ref>).</p><p>At training time, the excessively strong adversarial noise forces the network f a to learn how to classify robustly. This is because perturbations crafted on an external model can approximate the adversarial examples of the model under training (an insight inspired the prior work <ref type="bibr" target="#b43">[44]</ref>). This reason, although valid, can not explain how our method is able to avoid the deficiencies of prior defense methods. There exist deeper reasons: Randomness. The adversarial noise added by our g(x) is randomized, since the update rule (3) always starts from the input image with a random perturbation (i.e., x + Î´ with uniformly sampled Î´ i âˆ¼ [âˆ’âˆ†, âˆ†]). Randomization is not new; prior defenses also employ randomized transformations to the input. But they have been circumvented by Expectation Over Transformation (EOT) <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b2">3]</ref>. EOT attack first estimates the gradient of expected f (g(x)) with respect to x using the relationship âˆ‡ E gâˆ¼T f (g(x)) = E gâˆ¼T âˆ‡f (g(x)), where g(â€¢) is a deterministic version of g(â€¢) sampled from the distribution of randomized transformations T . It then uses the estimated gradients in PGD-type attacks to generate adversarial examples. Thus, the feasibility of EOT hinges on a reliable estimation of âˆ‡f (g(x)). In our method, g(x) corresponds to solving the optimization problem (2) starting from a particular sample x + Î´.</p><p>In what follows, we examine a range of strategies that have been successfully used to estimate âˆ‡f (g(x)) in prior defense methods (and thus break them), and show that our method is robust against all those attacking strategies. Automatic differentiation. By chain rule, the estimation of âˆ‡f (g(x)) requires the knowledge of g(x)'s Jacobian (first-order derivatives) Dg(x). A straightforward attempt to this end is by unrolling the iterative steps (3) and using automatic differentiation (AD) <ref type="bibr" target="#b46">[47]</ref> to compute Dg(x). Yet, this is infeasible. As shown in (3), the iterative steps involves non-differentiable operators including sgn(â€¢) and</p><formula xml:id="formula_4">Î  x â€² âˆˆâˆ†x [â€¢].</formula><p>Thus, directly applying AD leads to erroneous estimation of Dg(x), which in turn obstructs the search for adversarial examples. Our early experiments indeed show that virtually no adversarial examples crafted using AD can fool our model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Backward Pass Differentiable Approximation (BPDA).</head><p>To circumvent the defense using non-differentiable operators, Athalye et al. <ref type="bibr" target="#b1">[2]</ref> introduced a strategy called Backward Pass Differentiable Approximation (BPDA) to estimate the defense model's gradients. The idea is to replace the non-differentiable operators in g with differentiable approximations, and estimate the derivatives Dg(x) in AD by will reach a position at the red star. Perturbing x0 toward one side (to the red square), g(x) will still reach the red star, and in this way, the finite difference gradient vanishes. But if x0 is perturbed to the green square, g(x) will reach the green star-an entirely different local minimum, and the finite difference gradient explodes. (right) We plot âˆ‚ gi (x) âˆ‚x j (for particular i and j here) estimated by finite difference method (4) with an increasing hj. When hj is extremely small (&lt; 10 âˆ’5 ), the estimated gradient vanishes; as hj increases, the estimated gradient fluctuates severely, due to the reason illustrated on the left. computing the AD's forward pass using the original g and computing its backward pass using g's differentiable approximation. BPDA has succeed in gradient-based attacks (such as PGD and C&amp;W <ref type="bibr" target="#b7">[8]</ref>) toward many prior defenses, allowing the adversary to craft efficient adversarial examples.</p><p>When applying BPDA to estimate the gradients of our defense model, we replace sgn(â€¢) and Î  x â€² âˆˆâˆ†x [â€¢] in (3) with their differentiable approximations (see Sec. 4.1 for details). We found that if the number of iterations (LL-PGD steps) for applying (3) is low (i.e., â‰¤ 2), BPDA indeed enables the adversary to find valid adversarial examples. But when the number of iterations is set moderately high (i.e., &gt; 4), BPDA is greatly thwarted; the adversary can hardly find any valid adversarial example (see Sec. 4.1). This is because the differentiable approximations must be applied in each iteration, and as the number of iterations increases, the approximation error accumulates rapidly. Finite difference gradients. Another strategy for estimating Dg(x) is the classic finite difference estimation. Each element in the Jacobian matrix Dg(x) can be estimated using</p><formula xml:id="formula_5">âˆ‚g i (x) âˆ‚x j â‰ˆ 1 2h [g i (x + h j ) âˆ’ gi (x âˆ’ h j )] ,<label>(4)</label></formula><p>where gi (x) indicates the i-th element (or pixel) of the transformed image, and h j is a vector with all zeros except the j-th element (or pixel) which has a value h.</p><p>Our defense inherently thwarts this attacking strategy. It causes the adversary to suffer from either exploding or vanishing gradients <ref type="bibr" target="#b1">[2]</ref>. Figure <ref type="figure" target="#fig_4">3</ref>-left shows a 1D depiction illustrating this phenomenon in our method. Indeed, our experiments confirm that it is too unreliable to estimate derivatives using (4) (see Fig. <ref type="figure" target="#fig_4">3-right</ref>). Reparameterization. Vanishing and exploding gradients have been exploited as a defense mechanism <ref type="bibr" target="#b38">[39,</ref><ref type="bibr" target="#b32">33]</ref>. Yet those defenses have been proven vulnerable under a reparameterization strategy <ref type="bibr" target="#b1">[2]</ref>. This strategy aims to find some differentiable function h(â€¢) for a change-of-variable x = h(z) such that g(h(z)) â‰ˆ h(z). If such a function h(â€¢) can be found, then one can compute the gradient of the differentiable function f (h(z)) to launch adversarial attack.</p><p>To break our defense using this strategy, one must find an h(â€¢) that constructs the adversarial examples of f b directly (so that g(h(â€¢)) = h(â€¢)), without solving the optimization problem <ref type="bibr" target="#b1">(2)</ref>. We argue that finding such an h(â€¢) is extremely hard. If h(â€¢) could be constructed, we would have a direct way of crafting adversarial examples; PGD-type iterations would not be needed; and the entire territory of adversarial learning would be redefined-which are unlikely to happen.</p><p>Indeed, we implemented this strategy by training a neural network model h Î¸ that aims to minimize h Î¸ (x) âˆ’ g(x) 2 over the natural image distribution. This attempt is futile. Our experiments show that the generalization error of the trained h Î¸ is too high to launch any valid adversarial attack (see Appendix A.2). This conclusion also echos the prior studies <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b47">48]</ref>, which show that learning-based adversarial attacks usually perform worse than gradient-based attacks.</p><p>Identity mapping approximation. Some prior defense methods also use an optimization process to transform the input image-for example, the optimization that aims to erase adversarial noise from the input image <ref type="bibr" target="#b13">[14,</ref><ref type="bibr" target="#b32">33]</ref>. In those defenses, the transformed image g(x) remain similar to the input x. Consequently, as shown in <ref type="bibr" target="#b1">[2]</ref>, those defenses can be easily circumvented by replacing g(â€¢) with the identity mapping in the backward pass of BPDA attack.</p><p>Similarly, in our defense, if the perturbation range âˆ† in (3) for defining g(â€¢) were set small, g(x) (the adversarial example of f b ) would be close to x, and our defense would be at risk. To prevent this vulnerability, we must ensure that g(â€¢) be far from the identity mapping. This requires us to set a relatively large âˆ†. In practice, we use âˆ† = 0.2 for pixel values ranging in [0, 1] (see details in Sec. 4.1).</p><p>It turns out that a relatively large âˆ† is necessary but not sufficient. The choice of the network model f b also affects how far g(x) is from x statistically, as we will discuss next.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4.">Choosing Pre-trained Model f b</head><p>A large perturbation range âˆ† allows our adversarial transformation g(â€¢) to output an image far from the input. Yet, because of the randomness in g(â€¢), a large âˆ† x can not guarantee that g(x) is statistically different from x. If the expectation over the transformation E gâˆ¼T g(x) remains close to x, our defense method may still suffer from the aforementioned BPDA attack, in which identity mapping can be used to approximate g(â€¢) in the backward pass.</p><p>This intuition is supported by an empirical discovery. We experimented with an input transformation g(â€¢) constructed using an untrained model fb whose weights are assigned  randomly. As shown in Fig. <ref type="figure" target="#fig_6">4</ref> and the first column in Table <ref type="table" target="#tab_0">1</ref>, the expectation over transformation E gâˆ¼T g(x) is indeed close to x (in L âˆž norm), and the BPDA attack with identity mapping approximation can easily fool this defense model. Next, we train a series of models f (i) b , each obtained with an increasing number of training epochs. We found that as the number of training epochs increases, the expectation over transformation E gâˆ¼T g(x) resulted by using each of these f (i) b models drifts further away from x, that is, x âˆ’ E gâˆ¼T g(x) âˆž increases. Meanwhile, the defense model f (g(â€¢)) trained with the corresponding f (i) b becomes more robust, yielding increasingly better robust accuracy under the BPDA attack (see Table <ref type="table" target="#tab_0">1</ref>).</p><p>Remarkably, we discover that an even larger distance x âˆ’ E gâˆ¼T g(x) âˆž can be obtained, if the model f b is adversarially trained. When used in g(â€¢), the adversarially trained model f b further improves the robustness of our defense model. This discovery confirms our intuition. Computational performance. The choice of f b also affects the computational cost of our defense method. A complex network structure of f b makes g(â€¢) expensive, which in turn imposes a large performance overhead on both the training of f a and the inference using f a . Therefore, a simple network structure is preferred.</p><p>The freedom of choosing a simple network f b brings our method a performance advantage over adversarial training. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Devil's Advocate</head><p>We now play devil's advocate in attacking our defense method. In our defense, the network gradient with respect to the input (i.e., âˆ‡f a (g(x))) is intentionally undefined. Thus one can not craft adversarial examples by directly applying PGD-type methods on our defense (recall Sec. 2). We therefore evaluate our defense against a range of other possible attacks, including those discussed in Sec. 3.3. Later in Sec. 5, we will compare the worst-case robustness of our defense under these attacks with various recently proposed defense methods. Common experiment setups. Experiments in this section are conducted on CIFAR-10 dataset <ref type="bibr" target="#b17">[18]</ref> with standard training/test split. We use ResNet18 <ref type="bibr" target="#b14">[15]</ref> as the classification model f a and a small VGG-style network for f b , whose details are given in Appendix A.1. All models are trained for 80 epochs using Stochastic Gradient Descent (SGD) (constant learning rate=0.1, momentum=0.9). Our adversarial transformation g(â€¢) performs LL-PGD update (3) for 13 iterations, each with a stepsize Ç« = âˆ†/6. The perturbation range âˆ† varies in individual experiments, and will be reported therein. Metric. Following prior work, our evaluation uses an accuracy measure defined as the ratio of the number of correctly classified images to the total number of tested images. We refer to this measure as standard accuracy if the tested images include only clean images, and as robust accuracy if the tested images consist of adversarially crafted images.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">BPDA Attack and the Variants</head><p>BPDA attack <ref type="bibr" target="#b1">[2]</ref>, as reviewed in Sec. 3.3, is a powerful way to estimate network gradients that are obfuscated by defense methods. The estimated gradients are then used in Figure <ref type="figure">5</ref>. Robustness under BPDA. We evaluate the robust accuracy of our defense under two versions of BPDA attacks, which replace sgn(â€¢) in ( <ref type="formula" target="#formula_2">3</ref>) with soft sign (in orange) and tanh (in green), respectively. The resulting two robust accuracies are compared with our defense model's standard accuracy (in blue) evaluated with clean (natural) images. Along X-axis, we repeat this evaluation, each time with an increasing number of LL-PGD steps (3) in our adversarial transformation g(â€¢). To highlight only the effect of the smooth approximations of sgn(â€¢) and Î  x â€² âˆˆâˆ†x (â€¢), we factor out the randomness in our defense by disabling the random start at the beginning of (3). After the network gradient is estimated using BPDA, we use PGD to search for adversarial examples with a maximum perturbation size of 0.031 (in Lâˆž norm). The PGD search takes 50 iterations with a stepsize 0.002. PGD-type methods (if the defense is deterministic) or the EOT method (if the defense is randomized) for crafting adversarial examples. BPDA has circumvented a handful of recent defense techniques <ref type="bibr" target="#b13">[14,</ref><ref type="bibr" target="#b49">50,</ref><ref type="bibr" target="#b24">25,</ref><ref type="bibr" target="#b38">39,</ref><ref type="bibr" target="#b6">7]</ref> that implement gradient obfuscation, in many defenses resulting in 0% robust accuracy. We therefore evaluate our defense against it and its possible variants. Differentiable approximation on backward pass. The update rule (3) in our adversarial transformation involves two non-differentiable operators, namely, sgn(â€¢) and Î  x â€² âˆˆâˆ†x (â€¢), whose specific forms are given in Appendix A.3. To launch BPDA attack, we need to replace them with differentiable operators and compute their derivatives. We experimented with two different smooth approximations of sgn(â€¢): the soft sign function x 1+|x| and tanh function e x âˆ’e âˆ’x e x +e âˆ’x . The smooth approximation of Î  x â€² âˆˆâˆ†x (â€¢) is not explicitly defined. Instead, we directly approximate its derivative using</p><formula xml:id="formula_6">d dx Î  x â€² âˆˆâˆ†x (x) â‰ˆ 1, if |x| &lt; âˆ†, 1 (1+|x|) 2 , otherwise.<label>(5)</label></formula><p>Reported in Fig. <ref type="figure">5</ref>, the experiments show that our defense is robust to this attack, as long as the number of LL-PGD steps N in g(â€¢) is not too small (i.e., N &gt; 5). are in orange and green, respectively. In our input transformation, we enable random start of (3). Therefore, in BPDA-I attack, we use 500 samples of g(â€¢) for EOT gradient estimation. The gradientdescent setup is the same as that in the earlier experiments in Fig. <ref type="figure">5</ref>.</p><p>If the number of LL-PGD steps N is set too small, BPDA attack is indeed able to find adversarial examples (Fig. <ref type="figure">5</ref>). Therefore, another attempt one may ponder is to craft adversarial examples on a model trained with a small N (N â‰¤ 3), and use them to transfer attack our defense (which is trained with a larger N ). This attack remains ineffective (see details in Appendix A.3). We conjecture that this is because the adversarial examples for the model with a small N have a different distribution from that with a larger N <ref type="bibr" target="#b29">[30,</ref><ref type="bibr" target="#b39">40]</ref>. Identity mapping approximation. Another possible attack is by replacing the input transformation g(â€¢) with the identity mapping for gradient estimation in BPDA backward pass (recall discussion in Sec. 3.3). We refer this attack as BPDA-I attack. Under this attack, several previous defenses (e.g., <ref type="bibr" target="#b32">[33,</ref><ref type="bibr" target="#b38">39,</ref><ref type="bibr" target="#b13">14]</ref>) have been nullified.</p><p>We applied BPDA-I attack on our defense. The attack setup and results are summarized in Fig. <ref type="figure" target="#fig_7">6</ref>. As the perturbation size âˆ† in the adversarial transformation increases, the robust accuracy of our method increases. The robust accuracy is always upper bounded by the standard accuracy, which decreases gradually as âˆ† increases. If âˆ† is too large, the excessive perturbations to the input make the network f a harder to learn and thus lower the standard and robust accuracies. Empirically, âˆ† = 0.2 offers the best performance.</p><p>Reparameterization. In Sec. 3.3, we described another BPDA strategy, one that uses reparameterization to smoothly approximate our adversarial transformation g(â€¢). As discussed therein, it is extremely hard to directly derive the reparameterization function. Instead, we attempted to train a Fully Convolutional Network <ref type="bibr" target="#b23">[24]</ref> to represent h(z). We denote this network as h(x; Î¸), whose weights are optimized with the loss function, â„“(Î¸) = E xâˆˆX h(x; Î¸) âˆ’ g(x)</p><p>2 .</p><p>Here X represents the distribution of natural images (we use CIFAR-10 as the training dataset).</p><p>Our experiment shows that although we can reach a low  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Gradient-Free Attacks</head><p>Several attacking methods require no gradient information of the model, and they can be employed to potentially threaten our defense. As discussed in Sec. 2, these attacks fall into two categories: transfer attack and query-based attack. Against both types of attacks we evaluate our defense.</p><p>White-box transfer attack. In white-box setting, the adversary has full knowledge of our defense model. A tempting idea is to generate adversarial examples on the classifier model f a , and use them to transfer attack our defense model f a (g(â€¢)). Note that this differs from BPDA-I attack in Sec. 4.1, where f a is used only in the backward pass for gradient estimation while the forward pass still uses the full model f a (g(â€¢)). Here, in contrast, adversarial examples are generated solely on f a . We refer this attack as White-box Transfer (WT) attack, and report the robust accuracies of our defense in Fig. <ref type="figure" target="#fig_7">6</ref>, along with the results under BPDA-I attack. We found that our model's robustness performances under both attacks are similar, and âˆ† = 0.2 is the best choice.</p><p>One may realize another attacking possibility by noticing the way we choose f b (on which we perform adversarial transformation). In Sec. 3.4, we present that f b should be chosen such that for a given natural image x the average transformation E gâˆ¼T g(x) stays far from x. Thus, it seems plausible to first generate adversarial examples on f a using PGD attack starting from the average transformation E gâˆ¼T g(x), and use them to attack our full model f a (g(â€¢)). However, thanks to the large perturbation range âˆ† x we use (recall Sec. 3.3), E gâˆ¼T g(x) is always far from a natural image (see the image in the red box of Fig. <ref type="figure" target="#fig_6">4</ref>). Thus the adversarial examples generated in this way all have easily noticeable artifacts; they are not valid.</p><p>Black-box attacks. We also evaluate our defense against the black-box attacks, including the black-box transfer attack <ref type="bibr" target="#b29">[30]</ref>   <ref type="bibr" target="#b1">[2]</ref>. We evaluate other methods using the code provided in the original papers, training them using the same network and hyperparameters as our method. The perturbation range of all adversarial examples is âˆ† = 0.031. The last column indicates the most efficient attacking method that produces the worst robustness. The second last row indicates the worst-case robustness of our method under all BPDA-type attacks, while the last row indicates our worst-case robustness under all attacks.</p><p>attacks, HopSkipJumpAttack (HSJ) <ref type="bibr" target="#b8">[9]</ref> and GenAttack (GA) <ref type="bibr" target="#b0">[1]</ref>. In Table <ref type="table" target="#tab_2">2</ref>, we summarize the robust accuracies of our model under these attacks, along with two baselines from the same classification model (f a ) optimized respectively using standard training and adversarial training.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Comparisons and Further Evaluation</head><p>Comparisons. We now compare the robustness of our method with other state-of-the-art defense methods under white-box attacks. Unlike many others that can be attacked using PGD-type methods, our defense model is inherently non-differentiable, immune to direct PGD attacks. Therefore it is not possible to compare all these defense methods under exactly the same attacks. Instead, we compare the worst-case robustness of our method under all the attacks described in Sec. 4 with other methods. The comparison results on CIFAR-10 dataset are summarized in Table <ref type="table" target="#tab_3">3</ref>, where A std is the standard accuracy tested with clean images, and A rob is the worst-case robust accuracy under all tested attacks. The methods indicated by a star (*) are those circumvented by Athalye et al. <ref type="bibr" target="#b1">[2]</ref>. We include their results therein as a reference. The other defense methods (including ours) all use ResNet18 as their classification model, trained with SGD (learning rate=0.1, momentum=0.9) for 80 epochs.</p><p>On CIFAR-10 dataset, the most effective attack on our method is the black-box transfer attack (Sec. 4.2), although its severity surpasses BPDA attacks only slightly: the worstcase robust accuracy of our method under BPDA attack and its variants (Sec. 4.1) is 80.2%. Nevertheless, our robustness performance is significantly better than the state-of-the-art methods, as shown in Table <ref type="table" target="#tab_3">3</ref>.</p><p>We also performed the comparisons on Tiny ImageNet Sufficiency of EOT samples. Our defense is randomized, and when using EOT to attack our method we take 500 samples of g(â€¢) (recall experiments in Fig. <ref type="figure" target="#fig_7">6</ref>). Here we conduct additional experiments to ensure the sufficiency of using 500 samples for estimating the expectation. As shown in Fig. <ref type="figure">7</ref>, when the perturbation size âˆ† is small and the number of samples is also small (e.g., &lt; 100), increasing sample size indeed allows EOT to better attack our method. However, when the perturbation size is set to 0.2, the value we consistently use throughout all our evaluations, EOT attacks became persistently inefficient, regardless of the sample size. Therefore, we conclude that 500 samples in EOT allow thorough evaluation of our defense against EOT.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Conclusion</head><p>We have presented a simple defense mechanism against adversarial attacks. Our method takes advantage of the numerical recipe that searches for adversarial examples, and turns this harmful process into a useful input transformation for better robustness of a network model. On CIFAR-10 and Tiny ImageNet datasets, it demonstrates state-of-the-art worst-case robustness under a wide range of attacks. We hope our work can offer other researchers a new perspective to study the adversarial defense mechanisms. In the future, we would like to better understand the theoretical properties of our adversarial transformation and their connections to stronger adversarial robustness.  <ref type="table" target="#tab_3">3</ref> in the main text (i.e., the comparisons on CIFAR-10). The perturbation range of all adversarial examples is âˆ† = 0.031. The last column indicates the most efficient attacking method that produces the worst robustness. The second last row indicates the worst-case robustness of our method under all BPDAtype attacks, while the last row indicates our worst-case robustness under all attacks.</p><p>based on adversarial training <ref type="bibr" target="#b25">[26,</ref><ref type="bibr" target="#b53">54]</ref>. For all those methods, we use the implementation code provided in their original papers. When comparing with these methods, we use the same training protocol: the models are optimized use SGD (learning rate=0.1, momentum=0.9) and trained for 80 epochs.</p><p>As shown in Table <ref type="table" target="#tab_5">6</ref>, our method demonstrates significantly stronger robustness in comparison to previous methods. Our worst-case robust accuracy is 40.2%. In contrast, previous methods have robust accuracies around 18%. Remarkably, the standard accuracy of our method also outperforms previous methods.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.5. Expectation over Transformation Images</head><p>Figure <ref type="figure" target="#fig_6">4</ref> in the main text shows a few examples of the difference between an input image x and its expectation over transformation, that is, the image of normalized x âˆ’ E gâˆ¼T g(x). We now provide more samples of x âˆ’ E gâˆ¼T g(x) images on both CIFAR-10 and Tiny Ima-geNet (see Fig. <ref type="figure" target="#fig_9">9</ref>).</p><p>Discussion. In <ref type="bibr" target="#b44">[45]</ref>, Tsipras et al. presented an interesting finding. They visualized the loss gradient with respect to input pixels, and found that if the model is adversarially trained, such a loss gradient is significantly human-alignedthey align well with perceptually relevant features (e.g., see Figure <ref type="figure" target="#fig_2">2</ref> in their paper). But if the model is not adversarially trained, the loss gradient appears like random noise. Here, we discover that the normalized difference x âˆ’ E gâˆ¼T g(x) is also human-aligned, exhibiting perceptually relevant features, as shown in Fig. <ref type="figure" target="#fig_10">10</ref>. In contrast to the discovery in <ref type="bibr" target="#b44">[45]</ref>, we found that x âˆ’ E gâˆ¼T g(x) is always humanaligned. Even if the model f b is not adversarially trained, the difference image x âˆ’ E gâˆ¼T g(x) still exhibits perceptually relevant features, as along as they are trained with sufficient number of epochs (see Fig. <ref type="figure" target="#fig_9">9</ref>). If the model f b is adversarially trained, those perceptually relevant features become more noticeable.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Discussion on Computational Performance</head><p>Our defense demands lower training cost than the standard adversarial training. For example, on CIFAR-10 dataset, our method takes 82 minutes to train a ResNet18 model for 80 epochs. This time cost is close to the standard (nonadversarial) training, which takes 56 minutes for the same setting. In contrast, the standard adversarial training takes 460 minutes for the same number of epochs and the same network structure. Notice that the lower training cost in our method is obtained without sacrificing its robustness performance. In fact, as shown in Table <ref type="table" target="#tab_3">3</ref> in the main text and Table <ref type="table" target="#tab_5">6</ref> here, our defense offers much stronger robustness.</p><p>The inference cost of our defense is more expensive than adversarially trained models, because the input image x during the inference also needs to be transformed by g(â€¢). In our experiments, our defense takes 17 seconds to predict the labels of 10000 images in CIFAR-10, while the adversarially trained model and the standard model (without adversarial training) both take 4 seconds. This is the cost we have to pay in exchange for stronger robustness. We argue that this is worthy cost to pay because in comparison to network training cost, the inference cost is negligible. In fact, almost all adversarial defense methods that rely on input transformation <ref type="bibr" target="#b13">[14,</ref><ref type="bibr" target="#b38">39,</ref><ref type="bibr" target="#b32">33]</ref> have a performance overhead at inference time. For example, PixelDefend <ref type="bibr" target="#b38">[39]</ref> projects the input to a pre-trained PixelCNN-represented manifold through 100 steps of L-BFGS iterations. Their transformation is about 10Ã— slower than ours even when our method uses the same network structure in f b as their PixelCNN.   </p><note type="other">Tiny ImageNet</note></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 .</head><label>1</label><figDesc>Figure 1. A simple and effective defense mechanism. Given an input image, our defense method first transforms it through the process of crafting adversarial examples on a pre-trained simple model f b , deliberately adding strong adversarial noise. The transformed image is then fed into another model fa for classification. The same pipeline is applied in both training and inference.</figDesc><graphic url="image-1.png" coords="1,310.13,249.79,52.77,52.77" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 2 .</head><label>2</label><figDesc>Figure 2.To launch a valid attack from an input image (a), the adversarial example (b) must be perceptually similar to the original image (e.g., here âˆ† = 0.031). Otherwise, it can be easily pinpointed. In our method, the transformed image is used for training fa, not attacking. We therefore intentionally add much stronger adversarial noise to the input (c) (here âˆ† = 0.2). The strong noise helps to strengthen the robustness of fa and defend against BPDA attacks (see Sec. 3.3).</figDesc><graphic url="image-7.png" coords="3,328.88,74.82,57.76,57.63" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 1</head><label>1</label><figDesc>illustrates the pipeline of our method. Differences from adversarial training. With adversarial transformation, our training process superficially resembles the adversarial training, because both training processes need to search for adversarial examples of the input training data. But fundamental differences exist. In adversarial training, a single model f a is used for crafting adversarial examples and evolving itself at each epoch, whereas our method involves two models: the model f b is pre-trained and stays fixed during both the training of f a and the inference using f a .</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 3 .</head><label>3</label><figDesc>Figure 3. (left) We show a 1D depiction of our input transformation g(x), the process aiming to find the local minimum of the optimization problem (2). Starting from an x0 at the red dot, g(x) will reach a position at the red star. Perturbing x0 toward one side (to the red square), g(x) will still reach the red star, and in this way, the finite difference gradient vanishes. But if x0 is perturbed to the green square, g(x) will reach the green star-an entirely different local minimum, and the finite difference gradient explodes. (right) We plot âˆ‚ gi (x)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 4 .</head><label>4</label><figDesc>Figure 4. We apply the adversarial transformation g(â€¢) defined on different models f b to an input image (top-left). Corresponding to the six images toward the right are the f b models (with the same network structure) untrained, trained with an increasing number of epochs, and adversarially trained. In each of those six images, we visualize the normalized difference between the input x and the expectation over transformation (EOT) image Egâˆ¼T g(x) (estimated using 5000 samples). The Lâˆž norms of the difference images are shown under the images. Image in the red box (bottom-left) is the EOT image Egâˆ¼T g(x) produced using an adversarially trained f b model. Because we intentionally use a large perturbation range âˆ† = 0.2, this image has pronounced artifacts.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure 6 .</head><label>6</label><figDesc>Figure 6. Robustness w.r.t. perturbation size. We test our defense robustness using independently trained defense models with increasing perturbation range âˆ† used in g(â€¢). The standard accuracy (blue curve) is measured using clean images. The robust accuracies under BPDA-I attack (Sec. 4.1) and WT attack (Sec. 4.2) are in orange and green, respectively. In our input transformation, we enable random start of (3). Therefore, in BPDA-I attack, we use 500 samples of g(â€¢) for EOT gradient estimation. The gradientdescent setup is the same as that in the earlier experiments in Fig.5.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>Figure 9 .</head><label>9</label><figDesc>Figure 9. Here we show supplementary examples similar to those in Fig. 4 in the main text. The top four images are the results on CIFAR-10, while the bottom four images are those on Tiny ImageNet. The first column shows the input image x in each example. The other columns show the images generated by adversarial transformations with the f b models that are untrained, trained with an increasing number of epochs, and adversarially trained, as labeled on the top line. Each of those images is a visualization of the normalized difference x âˆ’ Egâˆ¼T g(x), where the expectation is estimated using 5000 samples. It is evident that as the number of training epochs increases, the expectation over transformation Egâˆ¼T g(x) drifts further away from x, and the adversarially trained f b model produces an even larger difference.</figDesc><graphic url="image-81.png" coords="14,97.72,560.59,51.44,51.44" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head>Figure 10 .</head><label>10</label><figDesc>Figure 10. Here we visualize the normalized difference between an input image (shown in column (a)) and its expectation over transformation image Egâˆ¼T g(x) in our defence model. The top three examples are from CIFAR-10, and the bottom three are from Tiny ImageNet. Column (b) shows the results using f b models with standard training, while column (c) are results with adversarial training. The Expectation over transformation in each example is estimated using an increasing number of samples. The ten sub-images (from left to right, top to bottom) in each group of column (b) and (c) are results in which the expectations over transformation are estimated using 1, 10, 50, 100, 200, 500, 1000, 2000, 5000, 10000 samples of g(â€¢), respectively.</figDesc><graphic url="image-105.png" coords="15,172.02,554.02,161.51,69.36" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 .</head><label>1</label><figDesc>In adversarial training, adversarial examples are crafted on the classification network f a for each input image at every epoch. In our training, however, by choosing a model f b Discovery for choosing f b . Corresponding to individual columns are f b models untrained, trained with an increasing number of epochs, and adversarially trained. For the defense model fa(g(â€¢)) equipped with each f b , we evaluate its standard accuracy (first row) and robust accuracy (second row) under the BPDA-I attack (see Sec. 4.1). The third row shows x âˆ’ Egâˆ¼T g(x) âˆž where Egâˆ¼T g(x) is estimated using 5000 samples. Notice the correlation between the increase of the Lâˆž distance and the increase of adversarial robustness. simpler than f a , it becomes faster to find adversarial examples. As shown in our experiments (in Sec. 5), in comparison to adversarial training, our defense requires shorter training time, and at the same time offers stronger robustness. Guiding rules. In summary, we present two guiding rules for choosing f b . 1) f b should be chosen to yield a large x âˆ’ E gâˆ¼T g(x) âˆž value. Given an f b 's network structure, adversarial training on f b (in pre-training step) is preferred. 2) Meanwhile, the structure of f b should be as simple as possible. In Appendix A.1, we report f b 's network structure that we use in our experiments.</figDesc><table><row><cell></cell><cell cols="8">Untrained 1 epoch 2 epochs 5 epochs 10 epochs 15 epochs 20 epochs Adv. trained</cell></row><row><cell>Standard Acc.</cell><cell>81.8%</cell><cell>81.6%</cell><cell>82.4%</cell><cell>83.0%</cell><cell>82.4%</cell><cell>82.2%</cell><cell>82.7%</cell><cell>82.9%</cell></row><row><cell>BPDA-I Acc.</cell><cell>30.6%</cell><cell>30.7%</cell><cell>40.0%</cell><cell>46.4%</cell><cell>62.3%</cell><cell>63.1%</cell><cell>62.9%</cell><cell>80.5%</cell></row><row><cell>Avg. L âˆž dist.</cell><cell>0.005</cell><cell>0.033</cell><cell>0.036</cell><cell>0.067</cell><cell>0.130</cell><cell>0.133</cell><cell>0.136</cell><cell>0.272</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2 .</head><label>2</label><figDesc>Robustness under black-box attacks. In the transfer attack, adversarial examples are crafted on an independently trained ResNet18 model. The query-based attacks are performed using a third-party library foolbox<ref type="bibr" target="#b31">[32]</ref> with default parameters to launch these attacks. All adversarial examples are restricted in the Lâˆž ball with a perturbation size of 0.031. loss value in training h(x; Î¸), the loss on test dataset always stays high, indicating that h(x; Î¸) is always overfitted. As a result, the adversarial examples resulted in this way have almost no effect on our defense model-the accuracy drop under this attack is within 1% from the standard accuracy. See Appendix A.2 for the details of this experiment.</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 3 .</head><label>3</label><figDesc>and two most recently introduced query-based Comparisons on CIFAR-10. Methods indicated by * are those circumvented in</figDesc><table><row><cell>Method</cell><cell>A std</cell><cell>A rob</cell><cell>Best Attack</cell></row><row><cell>No defense</cell><cell>92.9%</cell><cell>0.0%</cell><cell>PGD</cell></row><row><cell>Madry et al. [26]</cell><cell cols="2">81.7% 42.7%</cell><cell>PGD</cell></row><row><cell>Zhang et al. [54]</cell><cell cols="2">80.4% 44.6%</cell><cell>PGD</cell></row><row><cell>Xie et al. [51]</cell><cell cols="2">83.8% 45.2%</cell><cell>PGD</cell></row><row><cell>Guo et al.* [14]</cell><cell>-</cell><cell>0.0%</cell><cell>BPDA</cell></row><row><cell>et al.* [7]</cell><cell>-</cell><cell>0.0%</cell><cell>BPDA</cell></row><row><cell>Dhillon et al.* [10]</cell><cell>-</cell><cell>0.0%</cell><cell>BPDA</cell></row><row><cell>Song et al.* [39]</cell><cell>-</cell><cell>5.0%</cell><cell>BPDA</cell></row><row><cell cols="3">Ours (under BPDA) 82.9% 80.2%</cell><cell>BPDA</cell></row><row><cell>Ours</cell><cell cols="2">82.9% 78.1%</cell><cell>Transfer</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head></head><label></label><figDesc>, and our method demonstrates significantly stronger robustness as well. In short, our worst-case robust accuracy is 40.2%, in stark contrast to previous methods, which all have robust accuracies around 18%. The results are reported in details in Appendix A.<ref type="bibr" target="#b3">4</ref>. Training cost. Our method has significantly lower training cost than the adversarial training<ref type="bibr" target="#b25">[26,</ref><ref type="bibr" target="#b53">54,</ref><ref type="bibr" target="#b50">51]</ref>, while offering stronger robustness. For example, our method takes 82 minutes to train a ResNet18 model on CIFAR-10 for 80 epochs, while the adversarial training takes 460 minutes. As a baseline, the standard training takes 56 minutes. All timings are measured on a NVIDIA RTX 2080Ti GPU.</figDesc><table><row><cell>Figure 7. Robustness w.r.t. EOT samples. When using EOT to</cell></row><row><cell>attack our method, we sample our adversarial transformation with</cell></row><row><cell>different random starts x + Î´ to estimate the expected âˆ‡fa(g(x)).</cell></row><row><cell>When âˆ† = 0.1 (blue curve), increasing the sample size allows EOT</cell></row><row><cell>to better attack our defense, until it plateaus. But when âˆ† = 0.2</cell></row><row><cell>(orange curve), EOT becomes persistently inefficient.</cell></row></table><note>dataset</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 6 .</head><label>6</label><figDesc>Comparisons on Tiny ImageNet. The layout of this table is similar to Table</figDesc><table /></figure>
		</body>
		<back>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Following the guidelines presented at the end of Sec. 3.4, we choose to use a VGG-style small network in f b for defining our adversarial transformation. This network is simple enough to enable fast adversarial transformation, while producing the expectation over transformation image (i.e., E gâˆ¼T g(x)) drastically different from the input image. The structure of f b for experiments on CIFAR-10 dataset is described in Table <ref type="table">4</ref>.</p><p>On Tiny ImageNet dataset, the network structure of f b remains largely the same except two minor changes to accommodate the different resolution of the images in Tiny ImageNet. Namely, the changes are at the 12nd layer (which has a dimension 2048) and the 15th layer (which has a dimension 200).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Layer Module</head><p>Output Size </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.2. Reparameterization Attack</head><p>As discussed in Sec. 3.3 and 4.1, to launch the reparameterization attack, we need to find a forward function h(â€¢) that approximate our adversarial transformation process. To this end, we attempted to train a Fully Convlutional Network (FCN) <ref type="bibr" target="#b23">[24]</ref>, denoted as h(x; Î¸), through the following optimization,</p><p>where X is the given dataset, Î´ is the initial input perturbation in the L âˆž ball of size âˆ† (as described in Sec. 3.3), gÎ´ (â€¢) is the deterministic version of our adversarial transformation g(â€¢): it starts the adversarial search iteration (3) from x + Î´ by using a sampled Î´. However, after optimizing (6), we found that although the FCN model can reach a relatively low training error, the error on test set remains high, as depicted in Fig. <ref type="figure">8</ref>. This suggests that the FCN model is not able to learn a h(x; Î¸) that generalizes well. The inability to generalize is not a surprise: if h(x; Î¸) could generalize well, we would have a direct way of crafting adversarial examples; and PGD-type iterations would not be needed-which are all unlikely.</p><p>Indeed, when we use the trained h(x, Î¸) to launch a reparameterization attack to our model, the attack hardly succeeds. Under this attack (on CIFAR-10), the robust accuracy of our defense is 81.1%, even better than the robust accuracy under BPDA-I attack (80.2%). In fact, this accuracy nearly reaches its upper bound, the standard accuracy (i.e., 82.9%), as reported in Table <ref type="table">3</ref> of the main text.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.3. BPDA Attack Details and Additional Results</head><p>As described in Sec. 4.1, we evaluate our defense model under the BPDA attack. To launch BPDA attack, we need to replace the non-differentiable operators in our adversarial transformation with their smooth approximations. In particular, the non-differentiable operators in the adversarial update Robust Acc. <ref type="bibr">(</ref>  The right most three columns correspond to the models with smaller numbers of LL-PGD steps. We use these models to craft adversarial examples to transfer attack our defense model. It shows that even when the number N of LL-PGD steps in our defense model is moderately large (N â‰¥ 5), the transfer attacks become ineffective. In all the evaluations, the perturbation size âˆ† in our defense model is set as âˆ† = 0.2.</p><p>rule (3) (in the main text) are the sgn(â€¢) function,</p><p>and the L âˆž projection operator,</p><p>In Sec. 4.1, we experimented with two different smooth approximations of the sgn(â€¢) function, namely, the soft sign function x 1+|x| and tanh function e x âˆ’e âˆ’x e x +e âˆ’x , and the projection operator is replaced by directly approximating its derivative using <ref type="bibr" target="#b4">(5)</ref>.</p><p>Also discussed in Sec. 4.1 is an additional transfer attack: First, we craft adversarial examples by setting the number N of LL-PGD steps to be a small value. This is motivated by the observation that, as shown in Fig. <ref type="figure">5</ref>, BPDA attack is able to find effective adversarial examples when N is small. We then use the resulting adversarial examples to transfer attack our defense model, which uses a larger number of LL-PGD steps in the adversarial transformation (in both training and inference). As summarized in Table <ref type="table">5</ref>, our experiment shows that this attack remains ineffective to our defense.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.4. Evaluation on Tiny ImageNet</head><p>We also evaluate our defense model on Tiny ImageNet dataset consisting of 64pxÃ—64px RGB images. These images fall into 200 classes, each has 500 images for training and 50 images for testing. Following the evaluations setups in prior works, the adversarial examples used in the attacks have a maximum perturbation size (in L âˆž norm) of 0.031 for pixel values ranging in [0,1]. We use ResNet18 as our classification network (in f a ) and the network structure of f b is described in Appendix A.1.</p><p>We compare our method with the state-of-the-art method <ref type="bibr" target="#b26">[27]</ref>  </p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Genattack: Practical black-box attacks with gradient-free optimization</title>
		<author>
			<persName><forename type="first">Moustafa</forename><surname>Alzantot</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yash</forename><surname>Sharma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Supriyo</forename><surname>Chakraborty</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Huan</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Cho-Jui</forename><surname>Hsieh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mani B</forename><surname>Srivastava</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Genetic and Evolutionary Computation Conference</title>
				<meeting>the Genetic and Evolutionary Computation Conference</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2019">2019</date>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="1111" to="1119" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Obfuscated gradients give a false sense of security: Circumventing defenses to adversarial examples</title>
		<author>
			<persName><forename type="first">Anish</forename><surname>Athalye</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nicholas</forename><surname>Carlini</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><forename type="middle">A</forename><surname>Wagner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
				<imprint>
			<date type="published" when="2008">2018. 1, 2, 3, 4, 5, 6, 8</date>
			<biblScope unit="page" from="274" to="283" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Synthesizing robust adversarial examples</title>
		<author>
			<persName><forename type="first">Anish</forename><surname>Athalye</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Logan</forename><surname>Engstrom</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrew</forename><surname>Ilyas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kevin</forename><surname>Kwok</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning</title>
				<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Learning to attack: Adversarial transformation networks</title>
		<author>
			<persName><forename type="first">Shumeet</forename><surname>Baluja</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ian</forename><surname>Fischer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of AAAI-2018</title>
				<meeting>AAAI-2018</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Evasion attacks against machine learning at test time</title>
		<author>
			<persName><forename type="first">Battista</forename><surname>Biggio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Igino</forename><surname>Corona</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Davide</forename><surname>Maiorca</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Blaine</forename><surname>Nelson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nedim</forename><surname>Å rndiÄ‡</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pavel</forename><surname>Laskov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Giorgio</forename><surname>Giacinto</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fabio</forename><surname>Roli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Joint European conference on machine learning and knowledge discovery in databases</title>
				<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2013">2013</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="387" to="402" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Decision-based adversarial attacks: Reliable attacks against black-box machine learning models</title>
		<author>
			<persName><forename type="first">Wieland</forename><surname>Brendel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jonas</forename><surname>Rauber</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Matthias</forename><surname>Bethge</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
				<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Colin Raffel, and Ian Goodfellow. Thermometer encoding: One hot way to resist adversarial examples</title>
		<author>
			<persName><forename type="first">Jacob</forename><surname>Buckman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aurko</forename><surname>Roy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
				<imprint>
			<date type="published" when="2008">2018. 1, 2, 6, 8</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Towards evaluating the robustness of neural networks</title>
		<author>
			<persName><forename type="first">Nicholas</forename><surname>Carlini</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><surname>Wagner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2017 IEEE Symposium on Security and Privacy (SP)</title>
				<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">Boundary attack++: Query-efficient decision-based adversarial attack</title>
		<author>
			<persName><forename type="first">Jianbo</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><forename type="middle">I</forename><surname>Jordan</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1904.02144</idno>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="volume">2</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Stochastic activation pruning for robust adversarial defense</title>
		<author>
			<persName><forename type="first">S</forename><surname>Guneet</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kamyar</forename><surname>Dhillon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jeremy</forename><forename type="middle">D</forename><surname>Azizzadenesheli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jean</forename><surname>Bernstein</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aran</forename><surname>Kossaifi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zachary</forename><forename type="middle">C</forename><surname>Khanna</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Animashree</forename><surname>Lipton</surname></persName>
		</author>
		<author>
			<persName><surname>Anandkumar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
				<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="volume">2</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Boosting adversarial attacks with momentum</title>
		<author>
			<persName><forename type="first">Yinpeng</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fangzhou</forename><surname>Liao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tianyu</forename><surname>Pang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hang</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jun</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaolin</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jianguo</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
				<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="9185" to="9193" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Adversarial vulnerability for any classifier</title>
		<author>
			<persName><forename type="first">Alhussein</forename><surname>Fawzi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hamza</forename><surname>Fawzi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Omar</forename><surname>Fawzi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
				<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="1178" to="1187" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Explaining and harnessing adversarial examples</title>
		<author>
			<persName><forename type="first">Ian</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jonathon</forename><surname>Shlens</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christian</forename><surname>Szegedy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
				<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Countering adversarial images using input transformations</title>
		<author>
			<persName><forename type="first">Chuan</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mayank</forename><surname>Rana</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Moustapha</forename><surname>Cisse</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Laurens</forename><surname>Van Der Maaten</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
				<imprint>
			<date type="published" when="2008">2018. 1, 2, 3, 5, 6, 7, 8</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shaoqing</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
				<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="770" to="778" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Feature space perturbations yield more transferable adversarial examples</title>
		<author>
			<persName><forename type="first">Nathan</forename><surname>Inkawhich</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wei</forename><surname>Wen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hai</forename><forename type="middle">(</forename></persName>
		</author>
		<author>
			<persName><forename type="first">Helen</forename><forename type="middle">)</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yiran</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
				<imprint>
			<date type="published" when="2002">June 2019. 2</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Comdefend: An efficient image compression model to defend adversarial examples</title>
		<author>
			<persName><forename type="first">Xiaojun</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xingxing</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaochun</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hassan</forename><surname>Foroosh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
				<imprint>
			<date type="published" when="2002">June 2019. 2</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">Learning multiple layers of features from tiny images</title>
		<author>
			<persName><forename type="first">Alex</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Geoffrey</forename><surname>Hinton</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2009">2009</date>
			<publisher>Citeseer</publisher>
		</imprint>
	</monogr>
	<note type="report_type">Technical report</note>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Adversarial machine learning at scale</title>
		<author>
			<persName><forename type="first">Alexey</forename><surname>Kurakin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ian</forename><forename type="middle">J</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Samy</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
				<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Learning the distributions of adversarial examples for an improved black-box attack on deep neural networks</title>
		<author>
			<persName><forename type="first">Yandong</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lijun</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Liqiang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tong</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Boqing</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName><surname>Nattack</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
				<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="3866" to="3876" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Defense against adversarial attacks using high-level representation guided denoiser</title>
		<author>
			<persName><forename type="first">Fangzhou</forename><surname>Liao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ming</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yinpeng</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tianyu</forename><surname>Pang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaolin</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jun</forename><surname>Zhu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
				<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="1778" to="1787" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Towards robust neural networks via random selfensemble</title>
		<author>
			<persName><forename type="first">Xuanqing</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Minhao</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Huan</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Cho-Jui</forename><surname>Hsieh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV)</title>
				<meeting>the European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="369" to="385" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Feature distillation: Dnn-oriented jpeg compression against adversarial examples</title>
		<author>
			<persName><forename type="first">Zihao</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qi</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tao</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nuo</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xue</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yanzhi</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wujie</forename><surname>Wen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
				<imprint>
			<date type="published" when="2002">June 2019. 2</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Fully convolutional networks for semantic segmentation</title>
		<author>
			<persName><forename type="first">Jonathan</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Evan</forename><surname>Shelhamer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Trevor</forename><surname>Darrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
				<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Characterizing adversarial subspaces using local intrinsic dimensionality</title>
		<author>
			<persName><forename type="first">Xingjun</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bo</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yisen</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sarah</forename><forename type="middle">M</forename><surname>Erfani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sudanthi</forename><surname>Wijewickrema</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Grant</forename><surname>Schoenebeck</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><forename type="middle">E</forename><surname>Houle</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dawn</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName><forename type="first">James</forename><surname>Bailey</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
				<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Towards deep learning models resistant to adversarial attacks</title>
		<author>
			<persName><forename type="first">Aleksander</forename><surname>Madry</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aleksandar</forename><surname>Makelov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ludwig</forename><surname>Schmidt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dimitris</forename><surname>Tsipras</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Adrian</forename><surname>Vladu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
				<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="volume">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<title level="m" type="main">Metric learning for adversarial robustness</title>
		<author>
			<persName><forename type="first">Chengzhi</forename><surname>Mao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ziyuan</forename><surname>Zhong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Junfeng</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Carl</forename><surname>Vondrick</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Baishakhi</forename><surname>Ray</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1909.00900</idno>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Deepfool: a simple and accurate method to fool deep neural networks</title>
		<author>
			<persName><forename type="first">Seyed-Mohsen</forename><surname>Moosavi-Dezfooli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alhussein</forename><surname>Fawzi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pascal</forename><surname>Frossard</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
				<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Adversarial defense by restricting the hidden space of deep neural networks</title>
		<author>
			<persName><forename type="first">Aamir</forename><surname>Mustafa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Salman</forename><surname>Khan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Munawar</forename><surname>Hayat</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Roland</forename><surname>Goecke</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jianbing</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ling</forename><surname>Shao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE International Conference on Computer Vision (ICCV)</title>
				<imprint>
			<date type="published" when="2019-10">October 2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Practical blackbox attacks against machine learning</title>
		<author>
			<persName><forename type="first">Nicolas</forename><surname>Papernot</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Patrick</forename><surname>Mcdaniel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ian</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Somesh</forename><surname>Jha</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><forename type="middle">Berkay</forename><surname>Celik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ananthram</forename><surname>Swami</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2017 ACM on Asia Conference on Computer and Communications Security, ASIA CCS &apos;17</title>
				<meeting>the 2017 ACM on Asia Conference on Computer and Communications Security, ASIA CCS &apos;17<address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2017">2017</date>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Barrage of random transforms for adversarially robust defense</title>
		<author>
			<persName><forename type="first">Edward</forename><surname>Raff</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jared</forename><surname>Sylvester</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Steven</forename><surname>Forsyth</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mark</forename><surname>Mclean</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
				<imprint>
			<date type="published" when="2002">June 2019. 2</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
		<title level="m" type="main">Foolbox: A python toolbox to benchmark the robustness of machine learning models</title>
		<author>
			<persName><forename type="first">Jonas</forename><surname>Rauber</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wieland</forename><surname>Brendel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Matthias</forename><surname>Bethge</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1707.04131</idno>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Defense-GAN: Protecting classifiers against adversarial attacks using generative models</title>
		<author>
			<persName><forename type="first">Pouya</forename><surname>Samangouei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Maya</forename><surname>Kabkab</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rama</forename><surname>Chellappa</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
				<imprint>
			<date type="published" when="2007">2018. 1, 2, 3, 4, 5, 7</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
		<author>
			<persName><forename type="first">Ali</forename><surname>Shafahi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ronny</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christoph</forename><surname>Studer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Soheil</forename><surname>Feizi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tom</forename><surname>Goldstein</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1809.02104</idno>
		<title level="m">Are adversarial examples inevitable?</title>
				<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b34">
	<monogr>
		<author>
			<persName><forename type="first">Ali</forename><surname>Shafahi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mahyar</forename><surname>Najibi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Amin</forename><surname>Ghiasi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zheng</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">John</forename><surname>Dickerson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christoph</forename><surname>Studer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Larry</forename><forename type="middle">S</forename><surname>Davis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gavin</forename><surname>Taylor</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tom</forename><surname>Goldstein</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1904.12843</idno>
		<title level="m">Adversarial training for free!</title>
				<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Accessorize to a crime: Real and stealthy attacks on state-of-the-art face recognition</title>
		<author>
			<persName><forename type="first">Mahmood</forename><surname>Sharif</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sruti</forename><surname>Bhagavatula</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lujo</forename><surname>Bauer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><forename type="middle">K</forename><surname>Reiter</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2016 ACM SIGSAC Conference on Computer and Communications Security</title>
				<meeting>the 2016 ACM SIGSAC Conference on Computer and Communications Security</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="1528" to="1540" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Curls &amp; whey: Boosting black-box adversarial attacks</title>
		<author>
			<persName><forename type="first">Yucheng</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Siyu</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yahong</forename><surname>Han</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
				<imprint>
			<date type="published" when="2002">June 2019. 2</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Certifiable distributional robustness with principled adversarial training</title>
		<author>
			<persName><forename type="first">Aman</forename><surname>Sinha</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hongseok</forename><surname>Namkoong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">John</forename><surname>Duchi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
				<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Pixeldefend: Leveraging generative models to understand and defend against adversarial examples</title>
		<author>
			<persName><forename type="first">Yang</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Taesup</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sebastian</forename><surname>Nowozin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stefano</forename><surname>Ermon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nate</forename><surname>Kushman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
				<imprint>
			<date type="published" when="2008">2018. 1, 2, 3, 4, 6, 7, 8</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Is robustness the cost of accuracy? -a comprehensive study on the robustness of 18 deep image classification models</title>
		<author>
			<persName><forename type="first">Dong</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Huan</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hongge</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jinfeng</forename><surname>Yi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pin-Yu</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yupeng</forename><surname>Gao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The European Conference on Computer Vision (ECCV)</title>
				<imprint>
			<date type="published" when="2007">September 2018. 7</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<monogr>
		<title level="m" type="main">Intriguing properties of neural networks</title>
		<author>
			<persName><forename type="first">Christian</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wojciech</forename><surname>Zaremba</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Joan</forename><surname>Bruna</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dumitru</forename><surname>Erhan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ian</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rob</forename><surname>Fergus</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1312.6199</idno>
		<imprint>
			<date type="published" when="2013">2013</date>
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Defending against adversarial attacks by randomized diversification</title>
		<author>
			<persName><forename type="first">Olga</forename><surname>Taran</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shideh</forename><surname>Rezaeifar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Taras</forename><surname>Holotyak</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Slava</forename><surname>Voloshynovskiy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
				<imprint>
			<date type="published" when="2002">June 2019. 2</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Fooling automated surveillance cameras: adversarial patches to attack person detection</title>
		<author>
			<persName><forename type="first">Simen</forename><surname>Thys</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wiebe</forename><surname>Van Ranst</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Toon</forename><surname>GoedemÃ©</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition Workshops</title>
				<meeting>the IEEE Conference on Computer Vision and Pattern Recognition Workshops</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="0" to="0" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Ensemble adversarial training: Attacks and defenses</title>
		<author>
			<persName><forename type="first">Florian</forename><surname>Tramr</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alexey</forename><surname>Kurakin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nicolas</forename><surname>Papernot</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ian</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dan</forename><surname>Boneh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Patrick</forename><surname>Mcdaniel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
				<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<monogr>
		<author>
			<persName><forename type="first">Dimitris</forename><surname>Tsipras</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shibani</forename><surname>Santurkar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Logan</forename><surname>Engstrom</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alexander</forename><surname>Turner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aleksander</forename><surname>Madry</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1805.12152</idno>
		<title level="m">Robustness may be at odds with accuracy</title>
				<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Bilateral adversarial training: Towards fast training of more robust models against adversarial attacks</title>
		<author>
			<persName><forename type="first">Jianyu</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Haichao</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
				<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="6629" to="6638" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">A simple automatic derivative evaluation program</title>
		<author>
			<persName><forename type="first">Robert</forename><surname>Edwin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wengert</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Communications of the ACM</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="463" to="464" />
			<date type="published" when="1964">1964</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Generating adversarial examples with adversarial networks</title>
		<author>
			<persName><forename type="first">Chaowei</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bo</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jun-Yan</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Warren</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mingyan</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dawn</forename><surname>Song</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 27th International Joint Conference on Artificial Intelligence</title>
				<meeting>the 27th International Joint Conference on Artificial Intelligence</meeting>
		<imprint>
			<publisher>AAAI Press</publisher>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="3905" to="3911" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<monogr>
		<title level="m" type="main">Resisting adversarial attacks by k-winners-take-all</title>
		<author>
			<persName><forename type="first">Chang</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Peilin</forename><surname>Zhong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Changxi</forename><surname>Zheng</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1905.10510</idno>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Mitigating adversarial effects through randomization</title>
		<author>
			<persName><forename type="first">Cihang</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jianyu</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhishuai</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alan</forename><surname>Zhou Ren</surname></persName>
		</author>
		<author>
			<persName><surname>Yuille</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
				<imprint>
			<date type="published" when="2006">2018. 1, 2, 3, 6</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Feature denoising for improving adversarial robustness</title>
		<author>
			<persName><forename type="first">Cihang</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuxin</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Laurens</forename><surname>Van Der Maaten</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alan</forename><forename type="middle">L</forename><surname>Yuille</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
				<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="501" to="509" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Improving transferability of adversarial examples with input diversity</title>
		<author>
			<persName><forename type="first">Cihang</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhishuai</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuyin</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Song</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jianyu</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alan</forename><forename type="middle">L</forename><surname>Zhou Ren</surname></persName>
		</author>
		<author>
			<persName><surname>Yuille</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
				<imprint>
			<date type="published" when="2002">June 2019. 2</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<monogr>
		<title level="m" type="main">Zhanxing Zhu, and Bin Dong. You only propagate once: Painless adversarial training using maximal principle</title>
		<author>
			<persName><forename type="first">Dinghuai</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tianyuan</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yiping</forename><surname>Lu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1905.00877</idno>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">Theoretically principled trade-off between robustness and accuracy</title>
		<author>
			<persName><forename type="first">Hongyang</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yaodong</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiantao</forename><surname>Jiao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Eric</forename><surname>Xing</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Laurent</forename><forename type="middle">El</forename><surname>Ghaoui</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><surname>Jordan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
				<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="7472" to="7482" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
